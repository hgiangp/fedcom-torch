v = 22.697160301531774
a_0 = 27.840057009285058	a_alpha = -0.3425458469693173
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
num_samples = [126 265 124  43 306 146  54 179 130 118]
msize = 502400
xs = [  6.0943416  -20.79968212  15.00902392  18.81129433 -39.02070377
 -26.04359014   2.55680806  -6.32485185  -0.33602315 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  22.54482414   9.35018685
 -17.18584926   7.37501568 -19.17765202  17.56900603  -0.99851822]
dists_uav = [101.71763524 103.31800857 101.12870423 104.22156154 107.7499017
 104.75505717 100.304178   102.01855757 101.5321766  101.44984286]
dists_bs = [239.94522511 221.81113273 257.42563646 246.58750669 214.31339109
 243.15306433 244.18139867 257.20860434 235.14255819 236.47461703]
uav_gains = [9.58312886e-11 9.21631668e-11 9.72326274e-11 9.01785356e-11
 8.29761259e-11 8.90347214e-11 9.92432040e-11 9.51261523e-11
 9.62695134e-11 9.64649612e-11]
bs_gains = [2.39320649e-11 2.98222725e-11 1.96548195e-11 2.21704798e-11
 3.28364367e-11 2.30584863e-11 2.27876148e-11 1.97012919e-11
 2.53260008e-11 2.49285719e-11]
SystemModel __init__!
t_co_uav = [0.06351163 0.06396501 0.06334461 0.06422068 0.06521719 0.06437154
 0.0631106  0.06359693 0.06345904 0.0634357 ]
t_co_bs = [0.08476871 0.08051968 0.08895491 0.08634835 0.0787878  0.08552995
 0.08577462 0.08890235 0.0836346  0.08394849]
difference = [-0.02125708 -0.01655467 -0.0256103  -0.02212767 -0.01357061 -0.0211584
 -0.02266402 -0.02530541 -0.02017555 -0.0205128 ]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 6.796163711028166	bf = 20.32894796997589	zeta = 7.475780082130983	eta = 0.9090909090909091
af = 6.796163711028166	bf = 20.32894796997589	zeta = 230.74360710778967	eta = 0.02945331312192499
af = 6.796163711028166	bf = 20.32894796997589	zeta = 45.629002909774904	eta = 0.1489439452461113
af = 6.796163711028166	bf = 20.32894796997589	zeta = 39.092727771299245	eta = 0.173847262610764
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.999286881910415	eta = 0.17426379440236703
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.99926333319505	eta = 0.17426389962713645
eta_opt = 0.17426389962713645
initialize_feasible_solution eta = 0.17426389962713645, tau = 10.000839233398438
ti_comp = [0.03604337 0.0758055  0.03547125 0.01230051 0.0875339  0.04176454
 0.01544716 0.05120447 0.0371876  0.0337549 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [3.35654071 5.27057237 3.33162034 2.5799458  5.60760928 4.29178968
 2.64860954 3.87057992 4.07357378 3.96842191]
system_model train() tau = 30	t0 = 0.050004196166992185	t_min = 10.000839233398438
Round 0
-------------------------------
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86489914 22.7015787  10.6948057   3.82750737 26.17399534 12.62305496
  4.75731633 15.36016845 11.26073498 10.24387519]
obj_prev = 128.5079361631131
eta_min = 2.1972428731623064e-09	eta_max = 0.9187482009227209
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 50.862560519193266	eta = 0.5344727942639534
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 40.950101804889684	eta = 0.6638482847646219
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.18020327767913	eta = 0.6938364931761265
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094800455198204	eta = 0.6953521830931377
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094588160812094	eta = 0.6953559590470941
eta = 0.6953559590470941
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.02998124 0.06305577 0.02950534 0.01023169 0.07281157 0.03474016
 0.0128491  0.04259239 0.03093302 0.02807767]
ene_total = [3.3202555  6.49821862 3.27523947 1.52191262 7.37393057 3.95636804
 1.75096148 4.4735121  3.5911297  3.33306006]
ti_comp = [0.26476791 0.24775986 0.26493494 0.26405887 0.24949175 0.2427496
 0.26516894 0.26468261 0.24464495 0.24433105]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.40269086e-05 2.55265831e-04 2.28719695e-05 9.60109636e-07
 3.87586715e-04 4.44691157e-05 1.88561461e-06 6.89326925e-05
 3.09082577e-05 2.31742490e-05]
ene_total = [0.58260011 0.75916097 0.58096823 0.58697181 0.75542628 0.78568377
 0.5769119  0.5874834  0.76712376 0.76928555]
optimize_network iter = 0 obj = 6.7516157792990965
eta = 0.6953559590470941
freqs = [5.66179550e+07 1.27251793e+08 5.56841306e+07 1.93738838e+07
 1.45919802e+08 7.15555497e+07 2.42281409e+07 8.04593666e+07
 6.32202327e+07 5.74582420e+07]
eta_min = 0.6595211103034527	eta_max = 0.6953559590470929
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 0.07249356690995133	eta = 0.909090909090909
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 22.43093651594189	eta = 0.002938051320263521
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.424233800512977	eta = 0.027185184296772417
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333929777668074	eta = 0.028237028926918685
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333881523150504	eta = 0.02823761274584649
eta = 0.02823761274584649
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.35890421e-04 2.50613865e-03 2.24551507e-04 9.42612594e-06
 3.80523333e-03 4.36587104e-04 1.85125116e-05 6.76764628e-04
 3.03449855e-04 2.27519213e-04]
ene_total = [0.18871274 0.3024796  0.18790939 0.18425611 0.33473573 0.25754286
 0.18133617 0.20158775 0.24829862 0.24702256]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 1 obj = 6.050058200424808
eta = 0.6595211103034527
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
eta_min = 0.6595211103034622	eta_max = 0.6595211103034516
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 0.07141177968017555	eta = 0.909090909090909
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 22.429905462588092	eta = 0.0028943412096646293
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.4193533145705937	eta = 0.026833534117679286
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3303118142037405	eta = 0.02785884674898469
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3302654452731733	eta = 0.02785940109996355
eta = 0.02785940109996355
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.36690987e-04 2.47126971e-03 2.25349755e-04 9.45165461e-06
 3.75932384e-03 4.28126361e-04 1.85824929e-05 6.79005729e-04
 2.98203014e-04 2.23506966e-04]
ene_total = [0.18866225 0.30136334 0.18785915 0.18418516 0.33329075 0.25720037
 0.18126762 0.2015735  0.24805176 0.24681156]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 2 obj = 6.0500582004249726
eta = 0.6595211103034622
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
Done!
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.62436988e-05 2.74008143e-04 2.49862116e-05 1.04797559e-06
 4.16824332e-04 4.74695695e-05 2.06037988e-06 7.52864401e-05
 3.30639970e-05 2.47818878e-05]
ene_total = [0.00637741 0.00832598 0.00635945 0.00642312 0.0082956  0.00860046
 0.00631312 0.00643498 0.00839652 0.00841963]
At round 0 energy consumption: 0.07394626955167252
At round 0 eta: 0.6595211103034622
At round 0 a_n: 27.840057009285058
At round 0 local rounds: 13.62985484868938
At round 0 global rounds: 81.76735137411414
gradient difference: 1.1429961919784546
train() client id: f_00000-0-0 loss: 1.084069  [   32/  126]
train() client id: f_00000-0-1 loss: 0.988837  [   64/  126]
train() client id: f_00000-0-2 loss: 1.040076  [   96/  126]
train() client id: f_00000-1-0 loss: 1.033728  [   32/  126]
train() client id: f_00000-1-1 loss: 1.100522  [   64/  126]
train() client id: f_00000-1-2 loss: 0.971006  [   96/  126]
train() client id: f_00000-2-0 loss: 1.041789  [   32/  126]
train() client id: f_00000-2-1 loss: 1.011457  [   64/  126]
train() client id: f_00000-2-2 loss: 1.059017  [   96/  126]
train() client id: f_00000-3-0 loss: 0.965931  [   32/  126]
train() client id: f_00000-3-1 loss: 1.041309  [   64/  126]
train() client id: f_00000-3-2 loss: 1.089433  [   96/  126]
train() client id: f_00000-4-0 loss: 0.960311  [   32/  126]
train() client id: f_00000-4-1 loss: 1.094659  [   64/  126]
train() client id: f_00000-4-2 loss: 0.988792  [   96/  126]
train() client id: f_00000-5-0 loss: 1.176470  [   32/  126]
train() client id: f_00000-5-1 loss: 0.989581  [   64/  126]
train() client id: f_00000-5-2 loss: 1.034402  [   96/  126]
train() client id: f_00000-6-0 loss: 0.993060  [   32/  126]
train() client id: f_00000-6-1 loss: 1.107306  [   64/  126]
train() client id: f_00000-6-2 loss: 1.117061  [   96/  126]
train() client id: f_00000-7-0 loss: 1.022403  [   32/  126]
train() client id: f_00000-7-1 loss: 1.130814  [   64/  126]
train() client id: f_00000-7-2 loss: 1.052528  [   96/  126]
train() client id: f_00000-8-0 loss: 0.996806  [   32/  126]
train() client id: f_00000-8-1 loss: 1.120430  [   64/  126]
train() client id: f_00000-8-2 loss: 1.111782  [   96/  126]
train() client id: f_00000-9-0 loss: 0.975012  [   32/  126]
train() client id: f_00000-9-1 loss: 1.017941  [   64/  126]
train() client id: f_00000-9-2 loss: 1.165626  [   96/  126]
train() client id: f_00000-10-0 loss: 1.052696  [   32/  126]
train() client id: f_00000-10-1 loss: 1.011047  [   64/  126]
train() client id: f_00000-10-2 loss: 1.136858  [   96/  126]
train() client id: f_00000-11-0 loss: 1.158116  [   32/  126]
train() client id: f_00000-11-1 loss: 1.101521  [   64/  126]
train() client id: f_00000-11-2 loss: 1.074344  [   96/  126]
train() client id: f_00000-12-0 loss: 1.206193  [   32/  126]
train() client id: f_00000-12-1 loss: 0.988579  [   64/  126]
train() client id: f_00000-12-2 loss: 1.022851  [   96/  126]
train() client id: f_00001-0-0 loss: 1.039282  [   32/  265]
train() client id: f_00001-0-1 loss: 0.924919  [   64/  265]
train() client id: f_00001-0-2 loss: 0.904653  [   96/  265]
train() client id: f_00001-0-3 loss: 1.018117  [  128/  265]
train() client id: f_00001-0-4 loss: 0.899137  [  160/  265]
train() client id: f_00001-0-5 loss: 0.907600  [  192/  265]
train() client id: f_00001-0-6 loss: 1.043826  [  224/  265]
train() client id: f_00001-0-7 loss: 0.912958  [  256/  265]
train() client id: f_00001-1-0 loss: 0.912751  [   32/  265]
train() client id: f_00001-1-1 loss: 0.946341  [   64/  265]
train() client id: f_00001-1-2 loss: 0.958960  [   96/  265]
train() client id: f_00001-1-3 loss: 0.938772  [  128/  265]
train() client id: f_00001-1-4 loss: 1.036275  [  160/  265]
train() client id: f_00001-1-5 loss: 0.991559  [  192/  265]
train() client id: f_00001-1-6 loss: 0.951767  [  224/  265]
train() client id: f_00001-1-7 loss: 0.952685  [  256/  265]
train() client id: f_00001-2-0 loss: 0.927331  [   32/  265]
train() client id: f_00001-2-1 loss: 1.002853  [   64/  265]
train() client id: f_00001-2-2 loss: 0.956313  [   96/  265]
train() client id: f_00001-2-3 loss: 0.898074  [  128/  265]
train() client id: f_00001-2-4 loss: 1.134975  [  160/  265]
train() client id: f_00001-2-5 loss: 0.914035  [  192/  265]
train() client id: f_00001-2-6 loss: 0.999229  [  224/  265]
train() client id: f_00001-2-7 loss: 0.975590  [  256/  265]
train() client id: f_00001-3-0 loss: 0.961685  [   32/  265]
train() client id: f_00001-3-1 loss: 1.002566  [   64/  265]
train() client id: f_00001-3-2 loss: 1.058716  [   96/  265]
train() client id: f_00001-3-3 loss: 0.969528  [  128/  265]
train() client id: f_00001-3-4 loss: 1.005206  [  160/  265]
train() client id: f_00001-3-5 loss: 0.927145  [  192/  265]
train() client id: f_00001-3-6 loss: 1.001090  [  224/  265]
train() client id: f_00001-3-7 loss: 0.964399  [  256/  265]
train() client id: f_00001-4-0 loss: 1.023621  [   32/  265]
train() client id: f_00001-4-1 loss: 1.009863  [   64/  265]
train() client id: f_00001-4-2 loss: 0.994601  [   96/  265]
train() client id: f_00001-4-3 loss: 1.052047  [  128/  265]
train() client id: f_00001-4-4 loss: 1.072633  [  160/  265]
train() client id: f_00001-4-5 loss: 0.926405  [  192/  265]
train() client id: f_00001-4-6 loss: 1.008422  [  224/  265]
train() client id: f_00001-4-7 loss: 0.966893  [  256/  265]
train() client id: f_00001-5-0 loss: 0.975250  [   32/  265]
train() client id: f_00001-5-1 loss: 1.032606  [   64/  265]
train() client id: f_00001-5-2 loss: 1.011493  [   96/  265]
train() client id: f_00001-5-3 loss: 1.053562  [  128/  265]
train() client id: f_00001-5-4 loss: 0.994801  [  160/  265]
train() client id: f_00001-5-5 loss: 0.991576  [  192/  265]
train() client id: f_00001-5-6 loss: 1.057716  [  224/  265]
train() client id: f_00001-5-7 loss: 1.123089  [  256/  265]
train() client id: f_00001-6-0 loss: 1.164358  [   32/  265]
train() client id: f_00001-6-1 loss: 0.987213  [   64/  265]
train() client id: f_00001-6-2 loss: 0.992056  [   96/  265]
train() client id: f_00001-6-3 loss: 1.048420  [  128/  265]
train() client id: f_00001-6-4 loss: 1.028912  [  160/  265]
train() client id: f_00001-6-5 loss: 1.033405  [  192/  265]
train() client id: f_00001-6-6 loss: 1.017241  [  224/  265]
train() client id: f_00001-6-7 loss: 1.049330  [  256/  265]
train() client id: f_00001-7-0 loss: 1.028234  [   32/  265]
train() client id: f_00001-7-1 loss: 0.989107  [   64/  265]
train() client id: f_00001-7-2 loss: 0.995258  [   96/  265]
train() client id: f_00001-7-3 loss: 1.135666  [  128/  265]
train() client id: f_00001-7-4 loss: 1.107617  [  160/  265]
train() client id: f_00001-7-5 loss: 1.006598  [  192/  265]
train() client id: f_00001-7-6 loss: 1.083526  [  224/  265]
train() client id: f_00001-7-7 loss: 1.117153  [  256/  265]
train() client id: f_00001-8-0 loss: 0.994143  [   32/  265]
train() client id: f_00001-8-1 loss: 1.153796  [   64/  265]
train() client id: f_00001-8-2 loss: 1.054463  [   96/  265]
train() client id: f_00001-8-3 loss: 1.030516  [  128/  265]
train() client id: f_00001-8-4 loss: 1.159751  [  160/  265]
train() client id: f_00001-8-5 loss: 1.183121  [  192/  265]
train() client id: f_00001-8-6 loss: 1.012753  [  224/  265]
train() client id: f_00001-8-7 loss: 1.079159  [  256/  265]
train() client id: f_00001-9-0 loss: 1.079282  [   32/  265]
train() client id: f_00001-9-1 loss: 1.083775  [   64/  265]
train() client id: f_00001-9-2 loss: 1.074326  [   96/  265]
train() client id: f_00001-9-3 loss: 1.083247  [  128/  265]
train() client id: f_00001-9-4 loss: 1.199880  [  160/  265]
train() client id: f_00001-9-5 loss: 1.070114  [  192/  265]
train() client id: f_00001-9-6 loss: 1.068580  [  224/  265]
train() client id: f_00001-9-7 loss: 1.141683  [  256/  265]
train() client id: f_00001-10-0 loss: 1.055222  [   32/  265]
train() client id: f_00001-10-1 loss: 1.067455  [   64/  265]
train() client id: f_00001-10-2 loss: 1.093837  [   96/  265]
train() client id: f_00001-10-3 loss: 1.255394  [  128/  265]
train() client id: f_00001-10-4 loss: 1.039596  [  160/  265]
train() client id: f_00001-10-5 loss: 1.085396  [  192/  265]
train() client id: f_00001-10-6 loss: 1.154772  [  224/  265]
train() client id: f_00001-10-7 loss: 1.188689  [  256/  265]
train() client id: f_00001-11-0 loss: 1.060715  [   32/  265]
train() client id: f_00001-11-1 loss: 1.046812  [   64/  265]
train() client id: f_00001-11-2 loss: 1.175475  [   96/  265]
train() client id: f_00001-11-3 loss: 1.210760  [  128/  265]
train() client id: f_00001-11-4 loss: 1.091860  [  160/  265]
train() client id: f_00001-11-5 loss: 1.286144  [  192/  265]
train() client id: f_00001-11-6 loss: 1.062081  [  224/  265]
train() client id: f_00001-11-7 loss: 1.150478  [  256/  265]
train() client id: f_00001-12-0 loss: 1.162941  [   32/  265]
train() client id: f_00001-12-1 loss: 1.115138  [   64/  265]
train() client id: f_00001-12-2 loss: 1.115373  [   96/  265]
train() client id: f_00001-12-3 loss: 1.086505  [  128/  265]
train() client id: f_00001-12-4 loss: 1.159701  [  160/  265]
train() client id: f_00001-12-5 loss: 1.223805  [  192/  265]
train() client id: f_00001-12-6 loss: 1.125590  [  224/  265]
train() client id: f_00001-12-7 loss: 1.226723  [  256/  265]
train() client id: f_00002-0-0 loss: 0.863821  [   32/  124]
train() client id: f_00002-0-1 loss: 0.869858  [   64/  124]
train() client id: f_00002-0-2 loss: 0.860993  [   96/  124]
train() client id: f_00002-1-0 loss: 0.886756  [   32/  124]
train() client id: f_00002-1-1 loss: 0.761841  [   64/  124]
train() client id: f_00002-1-2 loss: 0.869586  [   96/  124]
train() client id: f_00002-2-0 loss: 0.824987  [   32/  124]
train() client id: f_00002-2-1 loss: 0.940420  [   64/  124]
train() client id: f_00002-2-2 loss: 0.860664  [   96/  124]
train() client id: f_00002-3-0 loss: 0.863691  [   32/  124]
train() client id: f_00002-3-1 loss: 0.917235  [   64/  124]
train() client id: f_00002-3-2 loss: 0.815502  [   96/  124]
train() client id: f_00002-4-0 loss: 0.816789  [   32/  124]
train() client id: f_00002-4-1 loss: 0.829999  [   64/  124]
train() client id: f_00002-4-2 loss: 0.958944  [   96/  124]
train() client id: f_00002-5-0 loss: 0.850453  [   32/  124]
train() client id: f_00002-5-1 loss: 0.880019  [   64/  124]
train() client id: f_00002-5-2 loss: 0.916589  [   96/  124]
train() client id: f_00002-6-0 loss: 0.951778  [   32/  124]
train() client id: f_00002-6-1 loss: 0.838104  [   64/  124]
train() client id: f_00002-6-2 loss: 0.891281  [   96/  124]
train() client id: f_00002-7-0 loss: 0.794150  [   32/  124]
train() client id: f_00002-7-1 loss: 0.972091  [   64/  124]
train() client id: f_00002-7-2 loss: 0.805276  [   96/  124]
train() client id: f_00002-8-0 loss: 1.005797  [   32/  124]
train() client id: f_00002-8-1 loss: 0.841373  [   64/  124]
train() client id: f_00002-8-2 loss: 0.814556  [   96/  124]
train() client id: f_00002-9-0 loss: 0.959589  [   32/  124]
train() client id: f_00002-9-1 loss: 0.868071  [   64/  124]
train() client id: f_00002-9-2 loss: 0.885533  [   96/  124]
train() client id: f_00002-10-0 loss: 0.904334  [   32/  124]
train() client id: f_00002-10-1 loss: 0.883090  [   64/  124]
train() client id: f_00002-10-2 loss: 0.903139  [   96/  124]
train() client id: f_00002-11-0 loss: 0.849113  [   32/  124]
train() client id: f_00002-11-1 loss: 0.951249  [   64/  124]
train() client id: f_00002-11-2 loss: 0.932707  [   96/  124]
train() client id: f_00002-12-0 loss: 0.854144  [   32/  124]
train() client id: f_00002-12-1 loss: 0.822680  [   64/  124]
train() client id: f_00002-12-2 loss: 0.868566  [   96/  124]
train() client id: f_00003-0-0 loss: 1.100288  [   32/   43]
train() client id: f_00003-1-0 loss: 1.110786  [   32/   43]
train() client id: f_00003-2-0 loss: 1.080677  [   32/   43]
train() client id: f_00003-3-0 loss: 1.081336  [   32/   43]
train() client id: f_00003-4-0 loss: 1.062977  [   32/   43]
train() client id: f_00003-5-0 loss: 1.083864  [   32/   43]
train() client id: f_00003-6-0 loss: 1.070752  [   32/   43]
train() client id: f_00003-7-0 loss: 1.089263  [   32/   43]
train() client id: f_00003-8-0 loss: 1.082164  [   32/   43]
train() client id: f_00003-9-0 loss: 1.073291  [   32/   43]
train() client id: f_00003-10-0 loss: 1.051338  [   32/   43]
train() client id: f_00003-11-0 loss: 1.124064  [   32/   43]
train() client id: f_00003-12-0 loss: 1.099168  [   32/   43]
train() client id: f_00004-0-0 loss: 1.068999  [   32/  306]
train() client id: f_00004-0-1 loss: 1.109879  [   64/  306]
train() client id: f_00004-0-2 loss: 1.058143  [   96/  306]
train() client id: f_00004-0-3 loss: 1.084677  [  128/  306]
train() client id: f_00004-0-4 loss: 1.128386  [  160/  306]
train() client id: f_00004-0-5 loss: 1.063765  [  192/  306]
train() client id: f_00004-0-6 loss: 1.057795  [  224/  306]
train() client id: f_00004-0-7 loss: 1.040554  [  256/  306]
train() client id: f_00004-0-8 loss: 1.040201  [  288/  306]
train() client id: f_00004-1-0 loss: 1.047792  [   32/  306]
train() client id: f_00004-1-1 loss: 1.032227  [   64/  306]
train() client id: f_00004-1-2 loss: 1.056855  [   96/  306]
train() client id: f_00004-1-3 loss: 1.059521  [  128/  306]
train() client id: f_00004-1-4 loss: 1.128021  [  160/  306]
train() client id: f_00004-1-5 loss: 1.056449  [  192/  306]
train() client id: f_00004-1-6 loss: 1.115975  [  224/  306]
train() client id: f_00004-1-7 loss: 1.084598  [  256/  306]
train() client id: f_00004-1-8 loss: 1.079163  [  288/  306]
train() client id: f_00004-2-0 loss: 1.110580  [   32/  306]
train() client id: f_00004-2-1 loss: 1.081626  [   64/  306]
train() client id: f_00004-2-2 loss: 1.042826  [   96/  306]
train() client id: f_00004-2-3 loss: 1.083670  [  128/  306]
train() client id: f_00004-2-4 loss: 1.081105  [  160/  306]
train() client id: f_00004-2-5 loss: 1.057677  [  192/  306]
train() client id: f_00004-2-6 loss: 1.093739  [  224/  306]
train() client id: f_00004-2-7 loss: 1.028809  [  256/  306]
train() client id: f_00004-2-8 loss: 1.136998  [  288/  306]
train() client id: f_00004-3-0 loss: 1.058365  [   32/  306]
train() client id: f_00004-3-1 loss: 1.095783  [   64/  306]
train() client id: f_00004-3-2 loss: 1.017645  [   96/  306]
train() client id: f_00004-3-3 loss: 1.128225  [  128/  306]
train() client id: f_00004-3-4 loss: 1.143331  [  160/  306]
train() client id: f_00004-3-5 loss: 1.070354  [  192/  306]
train() client id: f_00004-3-6 loss: 1.062921  [  224/  306]
train() client id: f_00004-3-7 loss: 1.099049  [  256/  306]
train() client id: f_00004-3-8 loss: 1.108282  [  288/  306]
train() client id: f_00004-4-0 loss: 1.089438  [   32/  306]
train() client id: f_00004-4-1 loss: 1.182882  [   64/  306]
train() client id: f_00004-4-2 loss: 1.052007  [   96/  306]
train() client id: f_00004-4-3 loss: 1.119102  [  128/  306]
train() client id: f_00004-4-4 loss: 1.068597  [  160/  306]
train() client id: f_00004-4-5 loss: 1.072211  [  192/  306]
train() client id: f_00004-4-6 loss: 1.086726  [  224/  306]
train() client id: f_00004-4-7 loss: 1.085267  [  256/  306]
train() client id: f_00004-4-8 loss: 1.113191  [  288/  306]
train() client id: f_00004-5-0 loss: 1.114742  [   32/  306]
train() client id: f_00004-5-1 loss: 1.107513  [   64/  306]
train() client id: f_00004-5-2 loss: 1.125357  [   96/  306]
train() client id: f_00004-5-3 loss: 1.122375  [  128/  306]
train() client id: f_00004-5-4 loss: 1.047865  [  160/  306]
train() client id: f_00004-5-5 loss: 1.134645  [  192/  306]
train() client id: f_00004-5-6 loss: 1.124303  [  224/  306]
train() client id: f_00004-5-7 loss: 1.113107  [  256/  306]
train() client id: f_00004-5-8 loss: 1.063982  [  288/  306]
train() client id: f_00004-6-0 loss: 1.076169  [   32/  306]
train() client id: f_00004-6-1 loss: 1.177181  [   64/  306]
train() client id: f_00004-6-2 loss: 1.057179  [   96/  306]
train() client id: f_00004-6-3 loss: 1.127782  [  128/  306]
train() client id: f_00004-6-4 loss: 1.171436  [  160/  306]
train() client id: f_00004-6-5 loss: 1.131013  [  192/  306]
train() client id: f_00004-6-6 loss: 1.142152  [  224/  306]
train() client id: f_00004-6-7 loss: 1.096813  [  256/  306]
train() client id: f_00004-6-8 loss: 1.094095  [  288/  306]
train() client id: f_00004-7-0 loss: 1.139474  [   32/  306]
train() client id: f_00004-7-1 loss: 1.113240  [   64/  306]
train() client id: f_00004-7-2 loss: 1.151233  [   96/  306]
train() client id: f_00004-7-3 loss: 1.088243  [  128/  306]
train() client id: f_00004-7-4 loss: 1.146785  [  160/  306]
train() client id: f_00004-7-5 loss: 1.155864  [  192/  306]
train() client id: f_00004-7-6 loss: 1.182474  [  224/  306]
train() client id: f_00004-7-7 loss: 1.102143  [  256/  306]
train() client id: f_00004-7-8 loss: 1.124982  [  288/  306]
train() client id: f_00004-8-0 loss: 1.201054  [   32/  306]
train() client id: f_00004-8-1 loss: 1.112767  [   64/  306]
train() client id: f_00004-8-2 loss: 1.119605  [   96/  306]
train() client id: f_00004-8-3 loss: 1.056399  [  128/  306]
train() client id: f_00004-8-4 loss: 1.148817  [  160/  306]
train() client id: f_00004-8-5 loss: 1.188742  [  192/  306]
train() client id: f_00004-8-6 loss: 1.245171  [  224/  306]
train() client id: f_00004-8-7 loss: 1.100217  [  256/  306]
train() client id: f_00004-8-8 loss: 1.061612  [  288/  306]
train() client id: f_00004-9-0 loss: 1.232541  [   32/  306]
train() client id: f_00004-9-1 loss: 1.168610  [   64/  306]
train() client id: f_00004-9-2 loss: 1.141291  [   96/  306]
train() client id: f_00004-9-3 loss: 1.182063  [  128/  306]
train() client id: f_00004-9-4 loss: 1.255449  [  160/  306]
train() client id: f_00004-9-5 loss: 1.111274  [  192/  306]
train() client id: f_00004-9-6 loss: 1.055750  [  224/  306]
train() client id: f_00004-9-7 loss: 1.130813  [  256/  306]
train() client id: f_00004-9-8 loss: 1.069243  [  288/  306]
train() client id: f_00004-10-0 loss: 1.123247  [   32/  306]
train() client id: f_00004-10-1 loss: 1.348578  [   64/  306]
train() client id: f_00004-10-2 loss: 1.138661  [   96/  306]
train() client id: f_00004-10-3 loss: 1.202646  [  128/  306]
train() client id: f_00004-10-4 loss: 1.104330  [  160/  306]
train() client id: f_00004-10-5 loss: 1.130619  [  192/  306]
train() client id: f_00004-10-6 loss: 1.139526  [  224/  306]
train() client id: f_00004-10-7 loss: 1.106776  [  256/  306]
train() client id: f_00004-10-8 loss: 1.079770  [  288/  306]
train() client id: f_00004-11-0 loss: 1.097205  [   32/  306]
train() client id: f_00004-11-1 loss: 1.218192  [   64/  306]
train() client id: f_00004-11-2 loss: 1.256359  [   96/  306]
train() client id: f_00004-11-3 loss: 1.221731  [  128/  306]
train() client id: f_00004-11-4 loss: 1.153411  [  160/  306]
train() client id: f_00004-11-5 loss: 1.260514  [  192/  306]
train() client id: f_00004-11-6 loss: 1.159096  [  224/  306]
train() client id: f_00004-11-7 loss: 1.068511  [  256/  306]
train() client id: f_00004-11-8 loss: 1.115885  [  288/  306]
train() client id: f_00004-12-0 loss: 1.244325  [   32/  306]
train() client id: f_00004-12-1 loss: 1.254339  [   64/  306]
train() client id: f_00004-12-2 loss: 1.237070  [   96/  306]
train() client id: f_00004-12-3 loss: 1.173534  [  128/  306]
train() client id: f_00004-12-4 loss: 1.190518  [  160/  306]
train() client id: f_00004-12-5 loss: 1.149601  [  192/  306]
train() client id: f_00004-12-6 loss: 1.256386  [  224/  306]
train() client id: f_00004-12-7 loss: 1.119698  [  256/  306]
train() client id: f_00004-12-8 loss: 1.044015  [  288/  306]
train() client id: f_00005-0-0 loss: 1.031802  [   32/  146]
train() client id: f_00005-0-1 loss: 1.036402  [   64/  146]
train() client id: f_00005-0-2 loss: 1.065008  [   96/  146]
train() client id: f_00005-0-3 loss: 1.071941  [  128/  146]
train() client id: f_00005-1-0 loss: 1.046204  [   32/  146]
train() client id: f_00005-1-1 loss: 1.093212  [   64/  146]
train() client id: f_00005-1-2 loss: 1.038115  [   96/  146]
train() client id: f_00005-1-3 loss: 1.013264  [  128/  146]
train() client id: f_00005-2-0 loss: 1.056320  [   32/  146]
train() client id: f_00005-2-1 loss: 1.008858  [   64/  146]
train() client id: f_00005-2-2 loss: 1.073004  [   96/  146]
train() client id: f_00005-2-3 loss: 1.058209  [  128/  146]
train() client id: f_00005-3-0 loss: 1.058469  [   32/  146]
train() client id: f_00005-3-1 loss: 1.136651  [   64/  146]
train() client id: f_00005-3-2 loss: 1.042343  [   96/  146]
train() client id: f_00005-3-3 loss: 1.047785  [  128/  146]
train() client id: f_00005-4-0 loss: 1.148617  [   32/  146]
train() client id: f_00005-4-1 loss: 1.039418  [   64/  146]
train() client id: f_00005-4-2 loss: 1.017017  [   96/  146]
train() client id: f_00005-4-3 loss: 1.063463  [  128/  146]
train() client id: f_00005-5-0 loss: 1.011887  [   32/  146]
train() client id: f_00005-5-1 loss: 1.091676  [   64/  146]
train() client id: f_00005-5-2 loss: 1.163235  [   96/  146]
train() client id: f_00005-5-3 loss: 1.061328  [  128/  146]
train() client id: f_00005-6-0 loss: 1.042393  [   32/  146]
train() client id: f_00005-6-1 loss: 1.070150  [   64/  146]
train() client id: f_00005-6-2 loss: 1.074525  [   96/  146]
train() client id: f_00005-6-3 loss: 1.191068  [  128/  146]
train() client id: f_00005-7-0 loss: 1.170488  [   32/  146]
train() client id: f_00005-7-1 loss: 1.126290  [   64/  146]
train() client id: f_00005-7-2 loss: 1.040519  [   96/  146]
train() client id: f_00005-7-3 loss: 1.123304  [  128/  146]
train() client id: f_00005-8-0 loss: 1.126648  [   32/  146]
train() client id: f_00005-8-1 loss: 1.110606  [   64/  146]
train() client id: f_00005-8-2 loss: 1.124581  [   96/  146]
train() client id: f_00005-8-3 loss: 1.152817  [  128/  146]
train() client id: f_00005-9-0 loss: 1.127992  [   32/  146]
train() client id: f_00005-9-1 loss: 1.131380  [   64/  146]
train() client id: f_00005-9-2 loss: 1.213587  [   96/  146]
train() client id: f_00005-9-3 loss: 1.102722  [  128/  146]
train() client id: f_00005-10-0 loss: 1.008151  [   32/  146]
train() client id: f_00005-10-1 loss: 1.196482  [   64/  146]
train() client id: f_00005-10-2 loss: 1.280980  [   96/  146]
train() client id: f_00005-10-3 loss: 1.241227  [  128/  146]
train() client id: f_00005-11-0 loss: 1.111335  [   32/  146]
train() client id: f_00005-11-1 loss: 1.186548  [   64/  146]
train() client id: f_00005-11-2 loss: 1.091693  [   96/  146]
train() client id: f_00005-11-3 loss: 1.206543  [  128/  146]
train() client id: f_00005-12-0 loss: 1.150352  [   32/  146]
train() client id: f_00005-12-1 loss: 1.244377  [   64/  146]
train() client id: f_00005-12-2 loss: 1.090431  [   96/  146]
train() client id: f_00005-12-3 loss: 1.263890  [  128/  146]
train() client id: f_00006-0-0 loss: 0.994242  [   32/   54]
train() client id: f_00006-1-0 loss: 0.994768  [   32/   54]
train() client id: f_00006-2-0 loss: 1.023007  [   32/   54]
train() client id: f_00006-3-0 loss: 1.064461  [   32/   54]
train() client id: f_00006-4-0 loss: 1.033459  [   32/   54]
train() client id: f_00006-5-0 loss: 1.050805  [   32/   54]
train() client id: f_00006-6-0 loss: 0.994654  [   32/   54]
train() client id: f_00006-7-0 loss: 1.024773  [   32/   54]
train() client id: f_00006-8-0 loss: 1.066726  [   32/   54]
train() client id: f_00006-9-0 loss: 0.987247  [   32/   54]
train() client id: f_00006-10-0 loss: 1.033606  [   32/   54]
train() client id: f_00006-11-0 loss: 0.980955  [   32/   54]
train() client id: f_00006-12-0 loss: 1.005148  [   32/   54]
train() client id: f_00007-0-0 loss: 1.027660  [   32/  179]
train() client id: f_00007-0-1 loss: 1.045885  [   64/  179]
train() client id: f_00007-0-2 loss: 1.080495  [   96/  179]
train() client id: f_00007-0-3 loss: 1.002090  [  128/  179]
train() client id: f_00007-0-4 loss: 0.989830  [  160/  179]
train() client id: f_00007-1-0 loss: 1.014083  [   32/  179]
train() client id: f_00007-1-1 loss: 1.000998  [   64/  179]
train() client id: f_00007-1-2 loss: 0.998660  [   96/  179]
train() client id: f_00007-1-3 loss: 1.050634  [  128/  179]
train() client id: f_00007-1-4 loss: 1.036065  [  160/  179]
train() client id: f_00007-2-0 loss: 1.029181  [   32/  179]
train() client id: f_00007-2-1 loss: 1.002284  [   64/  179]
train() client id: f_00007-2-2 loss: 1.046589  [   96/  179]
train() client id: f_00007-2-3 loss: 1.045978  [  128/  179]
train() client id: f_00007-2-4 loss: 1.067992  [  160/  179]
train() client id: f_00007-3-0 loss: 1.045325  [   32/  179]
train() client id: f_00007-3-1 loss: 1.006780  [   64/  179]
train() client id: f_00007-3-2 loss: 1.081674  [   96/  179]
train() client id: f_00007-3-3 loss: 1.048388  [  128/  179]
train() client id: f_00007-3-4 loss: 1.056878  [  160/  179]
train() client id: f_00007-4-0 loss: 1.065231  [   32/  179]
train() client id: f_00007-4-1 loss: 1.123358  [   64/  179]
train() client id: f_00007-4-2 loss: 1.084241  [   96/  179]
train() client id: f_00007-4-3 loss: 1.083558  [  128/  179]
train() client id: f_00007-4-4 loss: 1.018224  [  160/  179]
train() client id: f_00007-5-0 loss: 1.026494  [   32/  179]
train() client id: f_00007-5-1 loss: 1.109018  [   64/  179]
train() client id: f_00007-5-2 loss: 1.135930  [   96/  179]
train() client id: f_00007-5-3 loss: 1.012252  [  128/  179]
train() client id: f_00007-5-4 loss: 1.102149  [  160/  179]
train() client id: f_00007-6-0 loss: 1.030110  [   32/  179]
train() client id: f_00007-6-1 loss: 1.059292  [   64/  179]
train() client id: f_00007-6-2 loss: 1.117154  [   96/  179]
train() client id: f_00007-6-3 loss: 1.142329  [  128/  179]
train() client id: f_00007-6-4 loss: 1.156171  [  160/  179]
train() client id: f_00007-7-0 loss: 1.175136  [   32/  179]
train() client id: f_00007-7-1 loss: 1.096940  [   64/  179]
train() client id: f_00007-7-2 loss: 1.166389  [   96/  179]
train() client id: f_00007-7-3 loss: 1.101048  [  128/  179]
train() client id: f_00007-7-4 loss: 1.129087  [  160/  179]
train() client id: f_00007-8-0 loss: 1.124899  [   32/  179]
train() client id: f_00007-8-1 loss: 1.178822  [   64/  179]
train() client id: f_00007-8-2 loss: 1.154956  [   96/  179]
train() client id: f_00007-8-3 loss: 1.173624  [  128/  179]
train() client id: f_00007-8-4 loss: 1.230749  [  160/  179]
train() client id: f_00007-9-0 loss: 1.180429  [   32/  179]
train() client id: f_00007-9-1 loss: 1.257806  [   64/  179]
train() client id: f_00007-9-2 loss: 1.248441  [   96/  179]
train() client id: f_00007-9-3 loss: 1.136435  [  128/  179]
train() client id: f_00007-9-4 loss: 1.168797  [  160/  179]
train() client id: f_00007-10-0 loss: 1.192618  [   32/  179]
train() client id: f_00007-10-1 loss: 1.192765  [   64/  179]
train() client id: f_00007-10-2 loss: 1.293933  [   96/  179]
train() client id: f_00007-10-3 loss: 1.178674  [  128/  179]
train() client id: f_00007-10-4 loss: 1.202112  [  160/  179]
train() client id: f_00007-11-0 loss: 1.184325  [   32/  179]
train() client id: f_00007-11-1 loss: 1.231168  [   64/  179]
train() client id: f_00007-11-2 loss: 1.290223  [   96/  179]
train() client id: f_00007-11-3 loss: 1.262354  [  128/  179]
train() client id: f_00007-11-4 loss: 1.232795  [  160/  179]
train() client id: f_00007-12-0 loss: 1.221027  [   32/  179]
train() client id: f_00007-12-1 loss: 1.231377  [   64/  179]
train() client id: f_00007-12-2 loss: 1.207531  [   96/  179]
train() client id: f_00007-12-3 loss: 1.374387  [  128/  179]
train() client id: f_00007-12-4 loss: 1.283887  [  160/  179]
train() client id: f_00008-0-0 loss: 1.048268  [   32/  130]
train() client id: f_00008-0-1 loss: 1.173182  [   64/  130]
train() client id: f_00008-0-2 loss: 1.149721  [   96/  130]
train() client id: f_00008-0-3 loss: 1.125853  [  128/  130]
train() client id: f_00008-1-0 loss: 1.150647  [   32/  130]
train() client id: f_00008-1-1 loss: 1.047530  [   64/  130]
train() client id: f_00008-1-2 loss: 1.177661  [   96/  130]
train() client id: f_00008-1-3 loss: 1.153613  [  128/  130]
train() client id: f_00008-2-0 loss: 1.086680  [   32/  130]
train() client id: f_00008-2-1 loss: 1.218036  [   64/  130]
train() client id: f_00008-2-2 loss: 1.096611  [   96/  130]
train() client id: f_00008-2-3 loss: 1.161043  [  128/  130]
train() client id: f_00008-3-0 loss: 1.165236  [   32/  130]
train() client id: f_00008-3-1 loss: 1.145117  [   64/  130]
train() client id: f_00008-3-2 loss: 1.192188  [   96/  130]
train() client id: f_00008-3-3 loss: 1.092385  [  128/  130]
train() client id: f_00008-4-0 loss: 1.143080  [   32/  130]
train() client id: f_00008-4-1 loss: 1.127947  [   64/  130]
train() client id: f_00008-4-2 loss: 1.179702  [   96/  130]
train() client id: f_00008-4-3 loss: 1.188016  [  128/  130]
train() client id: f_00008-5-0 loss: 1.298206  [   32/  130]
train() client id: f_00008-5-1 loss: 1.108940  [   64/  130]
train() client id: f_00008-5-2 loss: 1.123067  [   96/  130]
train() client id: f_00008-5-3 loss: 1.132544  [  128/  130]
train() client id: f_00008-6-0 loss: 1.057419  [   32/  130]
train() client id: f_00008-6-1 loss: 1.268094  [   64/  130]
train() client id: f_00008-6-2 loss: 1.254896  [   96/  130]
train() client id: f_00008-6-3 loss: 1.127245  [  128/  130]
train() client id: f_00008-7-0 loss: 1.171126  [   32/  130]
train() client id: f_00008-7-1 loss: 1.206858  [   64/  130]
train() client id: f_00008-7-2 loss: 1.179089  [   96/  130]
train() client id: f_00008-7-3 loss: 1.174620  [  128/  130]
train() client id: f_00008-8-0 loss: 1.170524  [   32/  130]
train() client id: f_00008-8-1 loss: 1.170960  [   64/  130]
train() client id: f_00008-8-2 loss: 1.091075  [   96/  130]
train() client id: f_00008-8-3 loss: 1.285839  [  128/  130]
train() client id: f_00008-9-0 loss: 1.193501  [   32/  130]
train() client id: f_00008-9-1 loss: 1.273429  [   64/  130]
train() client id: f_00008-9-2 loss: 1.153864  [   96/  130]
train() client id: f_00008-9-3 loss: 1.160953  [  128/  130]
train() client id: f_00008-10-0 loss: 1.082576  [   32/  130]
train() client id: f_00008-10-1 loss: 1.238885  [   64/  130]
train() client id: f_00008-10-2 loss: 1.203725  [   96/  130]
train() client id: f_00008-10-3 loss: 1.273772  [  128/  130]
train() client id: f_00008-11-0 loss: 1.154912  [   32/  130]
train() client id: f_00008-11-1 loss: 1.209554  [   64/  130]
train() client id: f_00008-11-2 loss: 1.286725  [   96/  130]
train() client id: f_00008-11-3 loss: 1.240587  [  128/  130]
train() client id: f_00008-12-0 loss: 1.208758  [   32/  130]
train() client id: f_00008-12-1 loss: 1.324723  [   64/  130]
train() client id: f_00008-12-2 loss: 1.205780  [   96/  130]
train() client id: f_00008-12-3 loss: 1.188060  [  128/  130]
train() client id: f_00009-0-0 loss: 1.046527  [   32/  118]
train() client id: f_00009-0-1 loss: 1.066082  [   64/  118]
train() client id: f_00009-0-2 loss: 1.118642  [   96/  118]
train() client id: f_00009-1-0 loss: 1.017563  [   32/  118]
train() client id: f_00009-1-1 loss: 1.186348  [   64/  118]
train() client id: f_00009-1-2 loss: 1.059130  [   96/  118]
train() client id: f_00009-2-0 loss: 0.937323  [   32/  118]
train() client id: f_00009-2-1 loss: 1.131120  [   64/  118]
train() client id: f_00009-2-2 loss: 1.180890  [   96/  118]
train() client id: f_00009-3-0 loss: 0.975928  [   32/  118]
train() client id: f_00009-3-1 loss: 1.181790  [   64/  118]
train() client id: f_00009-3-2 loss: 1.161838  [   96/  118]
train() client id: f_00009-4-0 loss: 1.068449  [   32/  118]
train() client id: f_00009-4-1 loss: 1.120014  [   64/  118]
train() client id: f_00009-4-2 loss: 1.175291  [   96/  118]
train() client id: f_00009-5-0 loss: 1.158846  [   32/  118]
train() client id: f_00009-5-1 loss: 1.170570  [   64/  118]
train() client id: f_00009-5-2 loss: 1.078242  [   96/  118]
train() client id: f_00009-6-0 loss: 1.201091  [   32/  118]
train() client id: f_00009-6-1 loss: 1.224258  [   64/  118]
train() client id: f_00009-6-2 loss: 0.992641  [   96/  118]
train() client id: f_00009-7-0 loss: 1.219804  [   32/  118]
train() client id: f_00009-7-1 loss: 1.035675  [   64/  118]
train() client id: f_00009-7-2 loss: 1.107114  [   96/  118]
train() client id: f_00009-8-0 loss: 1.169663  [   32/  118]
train() client id: f_00009-8-1 loss: 1.111188  [   64/  118]
train() client id: f_00009-8-2 loss: 1.170537  [   96/  118]
train() client id: f_00009-9-0 loss: 1.293960  [   32/  118]
train() client id: f_00009-9-1 loss: 1.153699  [   64/  118]
train() client id: f_00009-9-2 loss: 1.081426  [   96/  118]
train() client id: f_00009-10-0 loss: 1.252210  [   32/  118]
train() client id: f_00009-10-1 loss: 1.173519  [   64/  118]
train() client id: f_00009-10-2 loss: 1.202599  [   96/  118]
train() client id: f_00009-11-0 loss: 1.099022  [   32/  118]
train() client id: f_00009-11-1 loss: 1.219091  [   64/  118]
train() client id: f_00009-11-2 loss: 1.355636  [   96/  118]
train() client id: f_00009-12-0 loss: 1.180294  [   32/  118]
train() client id: f_00009-12-1 loss: 1.219054  [   64/  118]
train() client id: f_00009-12-2 loss: 1.208982  [   96/  118]
At round 0 accuracy: 0.3129973474801061
At round 0 training accuracy: 0.29510395707578807
At round 0 training loss: 1.1863729319652
update_location
xs = [  1.0943416  -15.79968212  20.00902392  18.81129433 -34.02070377
 -21.04359014   2.55680806  -6.32485185   4.66397685 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  17.54482414   9.35018685
 -17.18584926   2.37501568 -14.17765202  17.56900603   4.00148178]
dists_uav = [101.5407992  102.42858035 101.99071065 103.25543883 106.04166294
 103.62521942 100.06087131 101.19787334 101.63868679 101.52381707]
dists_bs = [236.19434294 225.31573798 261.13798756 249.70973192 217.52016588
 246.24779551 247.64047589 253.45510377 238.88001683 232.77727406]
uav_gains = [9.62490767e-11 9.41770035e-11 9.51910990e-11 9.23028553e-11
 8.63584788e-11 9.14815864e-11 9.98476140e-11 9.70665613e-11
 9.60174958e-11 9.62893323e-11]
bs_gains = [2.50114868e-11 2.85415667e-11 1.88824323e-11 2.14030060e-11
 3.14989018e-11 2.22562249e-11 2.19075354e-11 2.05291596e-11
 2.42320740e-11 2.60531636e-11]
Round 1
-------------------------------
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86475814 22.70386864 10.69549308  3.82673858 26.17607366 12.62512831
  4.75712203 15.35951406 11.26321529 10.24142851]
obj_prev = 128.51334030366516
eta_min = 2.311289869413348e-09	eta_max = 0.9179068358889176
af = 27.184654844112664	bf = 2.038244896144109	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.038244896144109	zeta = 50.92141160980498	eta = 0.5338550913006942
af = 27.184654844112664	bf = 2.038244896144109	zeta = 40.97475341706718	eta = 0.6634488941863763
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.19821510905128	eta = 0.693517670855259
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11235513228073	eta = 0.6950400903288041
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11214097403846	eta = 0.6950438960157429
eta = 0.6950438960157429
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.03001828 0.06313368 0.0295418  0.01024433 0.07290153 0.03478308
 0.01286498 0.04264501 0.03097124 0.02811235]
ene_total = [3.31978164 6.5061149  3.27744721 1.51901348 7.38134405 3.96296862
 1.74998819 4.47161385 3.59899574 3.32487331]
ti_comp = [0.26460659 0.24673409 0.26447904 0.26412078 0.24854123 0.24180084
 0.26502657 0.26470385 0.24355148 0.24498963]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.41454867e-05 2.58347910e-04 2.30360589e-05 9.63220790e-07
 3.92006301e-04 4.49850892e-05 1.89464606e-06 6.91773975e-05
 3.13021490e-05 2.31353594e-05]
ene_total = [0.58155701 0.76609949 0.58262017 0.58387567 0.76180371 0.79165776
 0.57569161 0.58478017 0.77442671 0.76055195]
optimize_network iter = 0 obj = 6.763064260913875
eta = 0.6950438960157429
freqs = [5.67224657e+07 1.27938699e+08 5.58490306e+07 1.93932720e+07
 1.46658825e+08 7.19250640e+07 2.42711055e+07 8.05523076e+07
 6.35825283e+07 5.73745792e+07]
eta_min = 0.6596845988039914	eta_max = 0.6950438960157337
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 0.07315436494122368	eta = 0.9090909090909091
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 22.49041741434538	eta = 0.0029569912778037635
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.432729834459089	eta = 0.02733717784291988
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3416402766936963	eta = 0.028400591154114474
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3415910621216036	eta = 0.028401188065745816
eta = 0.028401188065745816
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.36378699e-04 2.52916596e-03 2.25517659e-04 9.42970754e-06
 3.83765052e-03 4.40393562e-04 1.85481443e-05 6.77230634e-04
 3.06440760e-04 2.26489788e-04]
ene_total = [0.18861483 0.30552365 0.1886691  0.18350388 0.33783864 0.25980791
 0.18116974 0.20096826 0.2509534  0.24454163]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 1 obj = 6.069316940771649
eta = 0.6596845988039914
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
eta_min = 0.659684598803993	eta_max = 0.6596845988039878
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 0.072022402521563	eta = 0.9090909090909091
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 22.489338538927885	eta = 0.002911375595591902
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.427629129214501	eta = 0.026970722420201213
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.3378597367303393	eta = 0.028006347153576604
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.337812514296872	eta = 0.028006912865265216
eta = 0.028006912865265216
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.37212058e-04 2.49249066e-03 2.26285293e-04 9.45857837e-06
 3.78935809e-03 4.31651518e-04 1.86209531e-05 6.79681002e-04
 3.00945552e-04 2.22781478e-04]
ene_total = [0.18856219 0.30434926 0.18861456 0.18343028 0.3363184  0.25945213
 0.18109834 0.20095692 0.25069421 0.24433622]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 2 obj = 6.069316940771676
eta = 0.659684598803993
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
Done!
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.63403224e-05 2.76769267e-04 2.51270008e-05 1.05029232e-06
 4.20775042e-04 4.79311221e-05 2.06769383e-06 7.54726250e-05
 3.34173689e-05 2.47379328e-05]
ene_total = [0.00637249 0.00841017 0.00638403 0.00639578 0.00837346 0.00867465
 0.00630622 0.0064119  0.00848508 0.00833258]
At round 1 energy consumption: 0.07414635829145368
At round 1 eta: 0.659684598803993
At round 1 a_n: 27.840057009285058
At round 1 local rounds: 13.621738685881681
At round 1 global rounds: 81.80663264560978
gradient difference: 0.4182078242301941
train() client id: f_00000-0-0 loss: 1.216633  [   32/  126]
train() client id: f_00000-0-1 loss: 1.110601  [   64/  126]
train() client id: f_00000-0-2 loss: 1.160155  [   96/  126]
train() client id: f_00000-1-0 loss: 1.090562  [   32/  126]
train() client id: f_00000-1-1 loss: 1.096872  [   64/  126]
train() client id: f_00000-1-2 loss: 1.106446  [   96/  126]
train() client id: f_00000-2-0 loss: 1.097184  [   32/  126]
train() client id: f_00000-2-1 loss: 1.048917  [   64/  126]
train() client id: f_00000-2-2 loss: 1.032234  [   96/  126]
train() client id: f_00000-3-0 loss: 1.020460  [   32/  126]
train() client id: f_00000-3-1 loss: 1.026612  [   64/  126]
train() client id: f_00000-3-2 loss: 0.995694  [   96/  126]
train() client id: f_00000-4-0 loss: 0.995236  [   32/  126]
train() client id: f_00000-4-1 loss: 1.002404  [   64/  126]
train() client id: f_00000-4-2 loss: 0.956957  [   96/  126]
train() client id: f_00000-5-0 loss: 0.980511  [   32/  126]
train() client id: f_00000-5-1 loss: 0.974543  [   64/  126]
train() client id: f_00000-5-2 loss: 0.874704  [   96/  126]
train() client id: f_00000-6-0 loss: 0.889341  [   32/  126]
train() client id: f_00000-6-1 loss: 0.898503  [   64/  126]
train() client id: f_00000-6-2 loss: 0.995040  [   96/  126]
train() client id: f_00000-7-0 loss: 0.921598  [   32/  126]
train() client id: f_00000-7-1 loss: 0.913448  [   64/  126]
train() client id: f_00000-7-2 loss: 0.934698  [   96/  126]
train() client id: f_00000-8-0 loss: 0.867858  [   32/  126]
train() client id: f_00000-8-1 loss: 0.876364  [   64/  126]
train() client id: f_00000-8-2 loss: 0.885305  [   96/  126]
train() client id: f_00000-9-0 loss: 0.858748  [   32/  126]
train() client id: f_00000-9-1 loss: 0.909738  [   64/  126]
train() client id: f_00000-9-2 loss: 0.914287  [   96/  126]
train() client id: f_00000-10-0 loss: 0.802600  [   32/  126]
train() client id: f_00000-10-1 loss: 0.977166  [   64/  126]
train() client id: f_00000-10-2 loss: 0.880737  [   96/  126]
train() client id: f_00000-11-0 loss: 0.836410  [   32/  126]
train() client id: f_00000-11-1 loss: 0.937388  [   64/  126]
train() client id: f_00000-11-2 loss: 0.947746  [   96/  126]
train() client id: f_00000-12-0 loss: 0.837805  [   32/  126]
train() client id: f_00000-12-1 loss: 0.897772  [   64/  126]
train() client id: f_00000-12-2 loss: 0.826000  [   96/  126]
train() client id: f_00001-0-0 loss: 0.933538  [   32/  265]
train() client id: f_00001-0-1 loss: 0.938289  [   64/  265]
train() client id: f_00001-0-2 loss: 0.899826  [   96/  265]
train() client id: f_00001-0-3 loss: 0.888848  [  128/  265]
train() client id: f_00001-0-4 loss: 0.860278  [  160/  265]
train() client id: f_00001-0-5 loss: 0.862399  [  192/  265]
train() client id: f_00001-0-6 loss: 0.873627  [  224/  265]
train() client id: f_00001-0-7 loss: 0.871872  [  256/  265]
train() client id: f_00001-1-0 loss: 0.842679  [   32/  265]
train() client id: f_00001-1-1 loss: 0.843272  [   64/  265]
train() client id: f_00001-1-2 loss: 0.809238  [   96/  265]
train() client id: f_00001-1-3 loss: 0.771805  [  128/  265]
train() client id: f_00001-1-4 loss: 0.792495  [  160/  265]
train() client id: f_00001-1-5 loss: 0.739183  [  192/  265]
train() client id: f_00001-1-6 loss: 0.754488  [  224/  265]
train() client id: f_00001-1-7 loss: 0.764008  [  256/  265]
train() client id: f_00001-2-0 loss: 0.754079  [   32/  265]
train() client id: f_00001-2-1 loss: 0.713160  [   64/  265]
train() client id: f_00001-2-2 loss: 0.755783  [   96/  265]
train() client id: f_00001-2-3 loss: 0.709360  [  128/  265]
train() client id: f_00001-2-4 loss: 0.696772  [  160/  265]
train() client id: f_00001-2-5 loss: 0.668750  [  192/  265]
train() client id: f_00001-2-6 loss: 0.687880  [  224/  265]
train() client id: f_00001-2-7 loss: 0.680388  [  256/  265]
train() client id: f_00001-3-0 loss: 0.725393  [   32/  265]
train() client id: f_00001-3-1 loss: 0.624028  [   64/  265]
train() client id: f_00001-3-2 loss: 0.681503  [   96/  265]
train() client id: f_00001-3-3 loss: 0.609835  [  128/  265]
train() client id: f_00001-3-4 loss: 0.599240  [  160/  265]
train() client id: f_00001-3-5 loss: 0.703777  [  192/  265]
train() client id: f_00001-3-6 loss: 0.625379  [  224/  265]
train() client id: f_00001-3-7 loss: 0.678119  [  256/  265]
train() client id: f_00001-4-0 loss: 0.672839  [   32/  265]
train() client id: f_00001-4-1 loss: 0.614255  [   64/  265]
train() client id: f_00001-4-2 loss: 0.653649  [   96/  265]
train() client id: f_00001-4-3 loss: 0.597913  [  128/  265]
train() client id: f_00001-4-4 loss: 0.576684  [  160/  265]
train() client id: f_00001-4-5 loss: 0.590220  [  192/  265]
train() client id: f_00001-4-6 loss: 0.599141  [  224/  265]
train() client id: f_00001-4-7 loss: 0.605561  [  256/  265]
train() client id: f_00001-5-0 loss: 0.603464  [   32/  265]
train() client id: f_00001-5-1 loss: 0.603252  [   64/  265]
train() client id: f_00001-5-2 loss: 0.604915  [   96/  265]
train() client id: f_00001-5-3 loss: 0.592717  [  128/  265]
train() client id: f_00001-5-4 loss: 0.662139  [  160/  265]
train() client id: f_00001-5-5 loss: 0.558738  [  192/  265]
train() client id: f_00001-5-6 loss: 0.546541  [  224/  265]
train() client id: f_00001-5-7 loss: 0.526189  [  256/  265]
train() client id: f_00001-6-0 loss: 0.555800  [   32/  265]
train() client id: f_00001-6-1 loss: 0.597370  [   64/  265]
train() client id: f_00001-6-2 loss: 0.639503  [   96/  265]
train() client id: f_00001-6-3 loss: 0.565459  [  128/  265]
train() client id: f_00001-6-4 loss: 0.523421  [  160/  265]
train() client id: f_00001-6-5 loss: 0.493037  [  192/  265]
train() client id: f_00001-6-6 loss: 0.583402  [  224/  265]
train() client id: f_00001-6-7 loss: 0.528176  [  256/  265]
train() client id: f_00001-7-0 loss: 0.609413  [   32/  265]
train() client id: f_00001-7-1 loss: 0.534988  [   64/  265]
train() client id: f_00001-7-2 loss: 0.525357  [   96/  265]
train() client id: f_00001-7-3 loss: 0.511456  [  128/  265]
train() client id: f_00001-7-4 loss: 0.503800  [  160/  265]
train() client id: f_00001-7-5 loss: 0.615971  [  192/  265]
train() client id: f_00001-7-6 loss: 0.547224  [  224/  265]
train() client id: f_00001-7-7 loss: 0.521303  [  256/  265]
train() client id: f_00001-8-0 loss: 0.521552  [   32/  265]
train() client id: f_00001-8-1 loss: 0.497992  [   64/  265]
train() client id: f_00001-8-2 loss: 0.597135  [   96/  265]
train() client id: f_00001-8-3 loss: 0.565114  [  128/  265]
train() client id: f_00001-8-4 loss: 0.508431  [  160/  265]
train() client id: f_00001-8-5 loss: 0.515069  [  192/  265]
train() client id: f_00001-8-6 loss: 0.480986  [  224/  265]
train() client id: f_00001-8-7 loss: 0.588487  [  256/  265]
train() client id: f_00001-9-0 loss: 0.462192  [   32/  265]
train() client id: f_00001-9-1 loss: 0.560575  [   64/  265]
train() client id: f_00001-9-2 loss: 0.569055  [   96/  265]
train() client id: f_00001-9-3 loss: 0.496165  [  128/  265]
train() client id: f_00001-9-4 loss: 0.552015  [  160/  265]
train() client id: f_00001-9-5 loss: 0.482707  [  192/  265]
train() client id: f_00001-9-6 loss: 0.540969  [  224/  265]
train() client id: f_00001-9-7 loss: 0.511566  [  256/  265]
train() client id: f_00001-10-0 loss: 0.671549  [   32/  265]
train() client id: f_00001-10-1 loss: 0.482415  [   64/  265]
train() client id: f_00001-10-2 loss: 0.479838  [   96/  265]
train() client id: f_00001-10-3 loss: 0.585031  [  128/  265]
train() client id: f_00001-10-4 loss: 0.435895  [  160/  265]
train() client id: f_00001-10-5 loss: 0.454986  [  192/  265]
train() client id: f_00001-10-6 loss: 0.499976  [  224/  265]
train() client id: f_00001-10-7 loss: 0.553583  [  256/  265]
train() client id: f_00001-11-0 loss: 0.459037  [   32/  265]
train() client id: f_00001-11-1 loss: 0.533304  [   64/  265]
train() client id: f_00001-11-2 loss: 0.526892  [   96/  265]
train() client id: f_00001-11-3 loss: 0.449273  [  128/  265]
train() client id: f_00001-11-4 loss: 0.449682  [  160/  265]
train() client id: f_00001-11-5 loss: 0.556125  [  192/  265]
train() client id: f_00001-11-6 loss: 0.635339  [  224/  265]
train() client id: f_00001-11-7 loss: 0.454966  [  256/  265]
train() client id: f_00001-12-0 loss: 0.433321  [   32/  265]
train() client id: f_00001-12-1 loss: 0.441425  [   64/  265]
train() client id: f_00001-12-2 loss: 0.438876  [   96/  265]
train() client id: f_00001-12-3 loss: 0.604225  [  128/  265]
train() client id: f_00001-12-4 loss: 0.633883  [  160/  265]
train() client id: f_00001-12-5 loss: 0.441730  [  192/  265]
train() client id: f_00001-12-6 loss: 0.482024  [  224/  265]
train() client id: f_00001-12-7 loss: 0.591691  [  256/  265]
train() client id: f_00002-0-0 loss: 0.895676  [   32/  124]
train() client id: f_00002-0-1 loss: 0.780964  [   64/  124]
train() client id: f_00002-0-2 loss: 0.891119  [   96/  124]
train() client id: f_00002-1-0 loss: 0.841578  [   32/  124]
train() client id: f_00002-1-1 loss: 0.891793  [   64/  124]
train() client id: f_00002-1-2 loss: 0.802732  [   96/  124]
train() client id: f_00002-2-0 loss: 0.834879  [   32/  124]
train() client id: f_00002-2-1 loss: 0.805539  [   64/  124]
train() client id: f_00002-2-2 loss: 0.897310  [   96/  124]
train() client id: f_00002-3-0 loss: 0.840173  [   32/  124]
train() client id: f_00002-3-1 loss: 0.827197  [   64/  124]
train() client id: f_00002-3-2 loss: 0.861702  [   96/  124]
train() client id: f_00002-4-0 loss: 0.805371  [   32/  124]
train() client id: f_00002-4-1 loss: 0.821590  [   64/  124]
train() client id: f_00002-4-2 loss: 0.915437  [   96/  124]
train() client id: f_00002-5-0 loss: 0.873360  [   32/  124]
train() client id: f_00002-5-1 loss: 0.777843  [   64/  124]
train() client id: f_00002-5-2 loss: 0.776103  [   96/  124]
train() client id: f_00002-6-0 loss: 0.738019  [   32/  124]
train() client id: f_00002-6-1 loss: 0.825214  [   64/  124]
train() client id: f_00002-6-2 loss: 0.721190  [   96/  124]
train() client id: f_00002-7-0 loss: 0.738485  [   32/  124]
train() client id: f_00002-7-1 loss: 0.842023  [   64/  124]
train() client id: f_00002-7-2 loss: 0.781633  [   96/  124]
train() client id: f_00002-8-0 loss: 0.864406  [   32/  124]
train() client id: f_00002-8-1 loss: 0.730945  [   64/  124]
train() client id: f_00002-8-2 loss: 0.837716  [   96/  124]
train() client id: f_00002-9-0 loss: 0.754554  [   32/  124]
train() client id: f_00002-9-1 loss: 0.779995  [   64/  124]
train() client id: f_00002-9-2 loss: 0.880885  [   96/  124]
train() client id: f_00002-10-0 loss: 0.909031  [   32/  124]
train() client id: f_00002-10-1 loss: 0.810480  [   64/  124]
train() client id: f_00002-10-2 loss: 0.689779  [   96/  124]
train() client id: f_00002-11-0 loss: 0.873262  [   32/  124]
train() client id: f_00002-11-1 loss: 0.686920  [   64/  124]
train() client id: f_00002-11-2 loss: 0.787302  [   96/  124]
train() client id: f_00002-12-0 loss: 0.750897  [   32/  124]
train() client id: f_00002-12-1 loss: 0.901143  [   64/  124]
train() client id: f_00002-12-2 loss: 0.759235  [   96/  124]
train() client id: f_00003-0-0 loss: 1.063747  [   32/   43]
train() client id: f_00003-1-0 loss: 1.041109  [   32/   43]
train() client id: f_00003-2-0 loss: 1.096386  [   32/   43]
train() client id: f_00003-3-0 loss: 1.064947  [   32/   43]
train() client id: f_00003-4-0 loss: 1.076375  [   32/   43]
train() client id: f_00003-5-0 loss: 1.058584  [   32/   43]
train() client id: f_00003-6-0 loss: 1.053368  [   32/   43]
train() client id: f_00003-7-0 loss: 1.062695  [   32/   43]
train() client id: f_00003-8-0 loss: 1.033008  [   32/   43]
train() client id: f_00003-9-0 loss: 1.026336  [   32/   43]
train() client id: f_00003-10-0 loss: 1.046868  [   32/   43]
train() client id: f_00003-11-0 loss: 1.035063  [   32/   43]
train() client id: f_00003-12-0 loss: 1.050590  [   32/   43]
train() client id: f_00004-0-0 loss: 1.172584  [   32/  306]
train() client id: f_00004-0-1 loss: 1.147323  [   64/  306]
train() client id: f_00004-0-2 loss: 1.192035  [   96/  306]
train() client id: f_00004-0-3 loss: 1.179756  [  128/  306]
train() client id: f_00004-0-4 loss: 1.232166  [  160/  306]
train() client id: f_00004-0-5 loss: 1.237253  [  192/  306]
train() client id: f_00004-0-6 loss: 1.141534  [  224/  306]
train() client id: f_00004-0-7 loss: 1.100070  [  256/  306]
train() client id: f_00004-0-8 loss: 1.124943  [  288/  306]
train() client id: f_00004-1-0 loss: 1.214574  [   32/  306]
train() client id: f_00004-1-1 loss: 1.204457  [   64/  306]
train() client id: f_00004-1-2 loss: 1.098269  [   96/  306]
train() client id: f_00004-1-3 loss: 1.115180  [  128/  306]
train() client id: f_00004-1-4 loss: 1.123084  [  160/  306]
train() client id: f_00004-1-5 loss: 1.134978  [  192/  306]
train() client id: f_00004-1-6 loss: 1.135434  [  224/  306]
train() client id: f_00004-1-7 loss: 1.150797  [  256/  306]
train() client id: f_00004-1-8 loss: 1.162186  [  288/  306]
train() client id: f_00004-2-0 loss: 1.124954  [   32/  306]
train() client id: f_00004-2-1 loss: 1.117552  [   64/  306]
train() client id: f_00004-2-2 loss: 1.107054  [   96/  306]
train() client id: f_00004-2-3 loss: 1.096157  [  128/  306]
train() client id: f_00004-2-4 loss: 1.104455  [  160/  306]
train() client id: f_00004-2-5 loss: 1.165413  [  192/  306]
train() client id: f_00004-2-6 loss: 1.143123  [  224/  306]
train() client id: f_00004-2-7 loss: 1.127836  [  256/  306]
train() client id: f_00004-2-8 loss: 1.124531  [  288/  306]
train() client id: f_00004-3-0 loss: 1.120855  [   32/  306]
train() client id: f_00004-3-1 loss: 1.133909  [   64/  306]
train() client id: f_00004-3-2 loss: 1.147507  [   96/  306]
train() client id: f_00004-3-3 loss: 1.105696  [  128/  306]
train() client id: f_00004-3-4 loss: 1.114011  [  160/  306]
train() client id: f_00004-3-5 loss: 1.089904  [  192/  306]
train() client id: f_00004-3-6 loss: 1.128799  [  224/  306]
train() client id: f_00004-3-7 loss: 1.049403  [  256/  306]
train() client id: f_00004-3-8 loss: 1.126449  [  288/  306]
train() client id: f_00004-4-0 loss: 1.140058  [   32/  306]
train() client id: f_00004-4-1 loss: 1.125383  [   64/  306]
train() client id: f_00004-4-2 loss: 1.090617  [   96/  306]
train() client id: f_00004-4-3 loss: 1.090387  [  128/  306]
train() client id: f_00004-4-4 loss: 1.103846  [  160/  306]
train() client id: f_00004-4-5 loss: 1.104061  [  192/  306]
train() client id: f_00004-4-6 loss: 1.100739  [  224/  306]
train() client id: f_00004-4-7 loss: 1.095267  [  256/  306]
train() client id: f_00004-4-8 loss: 1.077430  [  288/  306]
train() client id: f_00004-5-0 loss: 1.123009  [   32/  306]
train() client id: f_00004-5-1 loss: 1.131554  [   64/  306]
train() client id: f_00004-5-2 loss: 1.137563  [   96/  306]
train() client id: f_00004-5-3 loss: 1.097391  [  128/  306]
train() client id: f_00004-5-4 loss: 1.076813  [  160/  306]
train() client id: f_00004-5-5 loss: 1.030160  [  192/  306]
train() client id: f_00004-5-6 loss: 1.078942  [  224/  306]
train() client id: f_00004-5-7 loss: 1.077806  [  256/  306]
train() client id: f_00004-5-8 loss: 1.119673  [  288/  306]
train() client id: f_00004-6-0 loss: 1.120707  [   32/  306]
train() client id: f_00004-6-1 loss: 1.105048  [   64/  306]
train() client id: f_00004-6-2 loss: 1.097411  [   96/  306]
train() client id: f_00004-6-3 loss: 1.110608  [  128/  306]
train() client id: f_00004-6-4 loss: 1.087159  [  160/  306]
train() client id: f_00004-6-5 loss: 1.136578  [  192/  306]
train() client id: f_00004-6-6 loss: 1.072663  [  224/  306]
train() client id: f_00004-6-7 loss: 1.063362  [  256/  306]
train() client id: f_00004-6-8 loss: 1.072561  [  288/  306]
train() client id: f_00004-7-0 loss: 1.107965  [   32/  306]
train() client id: f_00004-7-1 loss: 1.078660  [   64/  306]
train() client id: f_00004-7-2 loss: 1.084711  [   96/  306]
train() client id: f_00004-7-3 loss: 1.098043  [  128/  306]
train() client id: f_00004-7-4 loss: 1.045066  [  160/  306]
train() client id: f_00004-7-5 loss: 1.076544  [  192/  306]
train() client id: f_00004-7-6 loss: 1.128269  [  224/  306]
train() client id: f_00004-7-7 loss: 1.072999  [  256/  306]
train() client id: f_00004-7-8 loss: 1.178995  [  288/  306]
train() client id: f_00004-8-0 loss: 1.179448  [   32/  306]
train() client id: f_00004-8-1 loss: 1.132100  [   64/  306]
train() client id: f_00004-8-2 loss: 1.167564  [   96/  306]
train() client id: f_00004-8-3 loss: 1.139643  [  128/  306]
train() client id: f_00004-8-4 loss: 1.064826  [  160/  306]
train() client id: f_00004-8-5 loss: 1.018604  [  192/  306]
train() client id: f_00004-8-6 loss: 1.022024  [  224/  306]
train() client id: f_00004-8-7 loss: 1.070952  [  256/  306]
train() client id: f_00004-8-8 loss: 1.025911  [  288/  306]
train() client id: f_00004-9-0 loss: 1.140370  [   32/  306]
train() client id: f_00004-9-1 loss: 1.091726  [   64/  306]
train() client id: f_00004-9-2 loss: 1.092200  [   96/  306]
train() client id: f_00004-9-3 loss: 1.099534  [  128/  306]
train() client id: f_00004-9-4 loss: 1.119747  [  160/  306]
train() client id: f_00004-9-5 loss: 1.037381  [  192/  306]
train() client id: f_00004-9-6 loss: 1.128556  [  224/  306]
train() client id: f_00004-9-7 loss: 1.157881  [  256/  306]
train() client id: f_00004-9-8 loss: 1.072156  [  288/  306]
train() client id: f_00004-10-0 loss: 1.084933  [   32/  306]
train() client id: f_00004-10-1 loss: 1.093432  [   64/  306]
train() client id: f_00004-10-2 loss: 1.042145  [   96/  306]
train() client id: f_00004-10-3 loss: 1.097232  [  128/  306]
train() client id: f_00004-10-4 loss: 1.121788  [  160/  306]
train() client id: f_00004-10-5 loss: 1.127349  [  192/  306]
train() client id: f_00004-10-6 loss: 1.090926  [  224/  306]
train() client id: f_00004-10-7 loss: 1.224073  [  256/  306]
train() client id: f_00004-10-8 loss: 1.030546  [  288/  306]
train() client id: f_00004-11-0 loss: 1.099969  [   32/  306]
train() client id: f_00004-11-1 loss: 1.034002  [   64/  306]
train() client id: f_00004-11-2 loss: 1.084496  [   96/  306]
train() client id: f_00004-11-3 loss: 1.121431  [  128/  306]
train() client id: f_00004-11-4 loss: 1.083933  [  160/  306]
train() client id: f_00004-11-5 loss: 1.108460  [  192/  306]
train() client id: f_00004-11-6 loss: 1.223544  [  224/  306]
train() client id: f_00004-11-7 loss: 1.084073  [  256/  306]
train() client id: f_00004-11-8 loss: 1.048130  [  288/  306]
train() client id: f_00004-12-0 loss: 1.042629  [   32/  306]
train() client id: f_00004-12-1 loss: 1.089849  [   64/  306]
train() client id: f_00004-12-2 loss: 1.170545  [   96/  306]
train() client id: f_00004-12-3 loss: 1.139173  [  128/  306]
train() client id: f_00004-12-4 loss: 1.132863  [  160/  306]
train() client id: f_00004-12-5 loss: 1.037277  [  192/  306]
train() client id: f_00004-12-6 loss: 1.150968  [  224/  306]
train() client id: f_00004-12-7 loss: 1.082967  [  256/  306]
train() client id: f_00004-12-8 loss: 1.165957  [  288/  306]
train() client id: f_00005-0-0 loss: 1.166353  [   32/  146]
train() client id: f_00005-0-1 loss: 1.190780  [   64/  146]
train() client id: f_00005-0-2 loss: 1.142377  [   96/  146]
train() client id: f_00005-0-3 loss: 1.142114  [  128/  146]
train() client id: f_00005-1-0 loss: 1.104508  [   32/  146]
train() client id: f_00005-1-1 loss: 1.137942  [   64/  146]
train() client id: f_00005-1-2 loss: 1.092781  [   96/  146]
train() client id: f_00005-1-3 loss: 1.065364  [  128/  146]
train() client id: f_00005-2-0 loss: 1.064602  [   32/  146]
train() client id: f_00005-2-1 loss: 1.087822  [   64/  146]
train() client id: f_00005-2-2 loss: 1.027209  [   96/  146]
train() client id: f_00005-2-3 loss: 1.003414  [  128/  146]
train() client id: f_00005-3-0 loss: 1.025563  [   32/  146]
train() client id: f_00005-3-1 loss: 0.999599  [   64/  146]
train() client id: f_00005-3-2 loss: 0.956510  [   96/  146]
train() client id: f_00005-3-3 loss: 1.022255  [  128/  146]
train() client id: f_00005-4-0 loss: 0.969090  [   32/  146]
train() client id: f_00005-4-1 loss: 0.935415  [   64/  146]
train() client id: f_00005-4-2 loss: 0.950755  [   96/  146]
train() client id: f_00005-4-3 loss: 0.984225  [  128/  146]
train() client id: f_00005-5-0 loss: 0.965454  [   32/  146]
train() client id: f_00005-5-1 loss: 0.932904  [   64/  146]
train() client id: f_00005-5-2 loss: 0.897080  [   96/  146]
train() client id: f_00005-5-3 loss: 0.893598  [  128/  146]
train() client id: f_00005-6-0 loss: 0.898191  [   32/  146]
train() client id: f_00005-6-1 loss: 0.941606  [   64/  146]
train() client id: f_00005-6-2 loss: 0.878834  [   96/  146]
train() client id: f_00005-6-3 loss: 0.866317  [  128/  146]
train() client id: f_00005-7-0 loss: 0.882052  [   32/  146]
train() client id: f_00005-7-1 loss: 0.871418  [   64/  146]
train() client id: f_00005-7-2 loss: 0.857816  [   96/  146]
train() client id: f_00005-7-3 loss: 0.889679  [  128/  146]
train() client id: f_00005-8-0 loss: 0.869424  [   32/  146]
train() client id: f_00005-8-1 loss: 0.836309  [   64/  146]
train() client id: f_00005-8-2 loss: 0.895213  [   96/  146]
train() client id: f_00005-8-3 loss: 0.883580  [  128/  146]
train() client id: f_00005-9-0 loss: 0.795216  [   32/  146]
train() client id: f_00005-9-1 loss: 0.879141  [   64/  146]
train() client id: f_00005-9-2 loss: 0.769792  [   96/  146]
train() client id: f_00005-9-3 loss: 0.914691  [  128/  146]
train() client id: f_00005-10-0 loss: 0.799946  [   32/  146]
train() client id: f_00005-10-1 loss: 0.809499  [   64/  146]
train() client id: f_00005-10-2 loss: 0.874635  [   96/  146]
train() client id: f_00005-10-3 loss: 0.878479  [  128/  146]
train() client id: f_00005-11-0 loss: 0.933028  [   32/  146]
train() client id: f_00005-11-1 loss: 0.850883  [   64/  146]
train() client id: f_00005-11-2 loss: 0.737420  [   96/  146]
train() client id: f_00005-11-3 loss: 0.770891  [  128/  146]
train() client id: f_00005-12-0 loss: 0.863163  [   32/  146]
train() client id: f_00005-12-1 loss: 0.870564  [   64/  146]
train() client id: f_00005-12-2 loss: 0.897301  [   96/  146]
train() client id: f_00005-12-3 loss: 0.744162  [  128/  146]
train() client id: f_00006-0-0 loss: 1.088246  [   32/   54]
train() client id: f_00006-1-0 loss: 1.046934  [   32/   54]
train() client id: f_00006-2-0 loss: 1.059868  [   32/   54]
train() client id: f_00006-3-0 loss: 1.102260  [   32/   54]
train() client id: f_00006-4-0 loss: 1.104770  [   32/   54]
train() client id: f_00006-5-0 loss: 1.124748  [   32/   54]
train() client id: f_00006-6-0 loss: 1.113362  [   32/   54]
train() client id: f_00006-7-0 loss: 1.090902  [   32/   54]
train() client id: f_00006-8-0 loss: 1.051455  [   32/   54]
train() client id: f_00006-9-0 loss: 1.125725  [   32/   54]
train() client id: f_00006-10-0 loss: 1.046682  [   32/   54]
train() client id: f_00006-11-0 loss: 1.147754  [   32/   54]
train() client id: f_00006-12-0 loss: 1.069710  [   32/   54]
train() client id: f_00007-0-0 loss: 1.371239  [   32/  179]
train() client id: f_00007-0-1 loss: 1.307756  [   64/  179]
train() client id: f_00007-0-2 loss: 1.344587  [   96/  179]
train() client id: f_00007-0-3 loss: 1.211721  [  128/  179]
train() client id: f_00007-0-4 loss: 1.250381  [  160/  179]
train() client id: f_00007-1-0 loss: 1.199667  [   32/  179]
train() client id: f_00007-1-1 loss: 1.265355  [   64/  179]
train() client id: f_00007-1-2 loss: 1.174034  [   96/  179]
train() client id: f_00007-1-3 loss: 1.142451  [  128/  179]
train() client id: f_00007-1-4 loss: 1.120216  [  160/  179]
train() client id: f_00007-2-0 loss: 1.132586  [   32/  179]
train() client id: f_00007-2-1 loss: 1.125616  [   64/  179]
train() client id: f_00007-2-2 loss: 1.076872  [   96/  179]
train() client id: f_00007-2-3 loss: 1.055822  [  128/  179]
train() client id: f_00007-2-4 loss: 1.046927  [  160/  179]
train() client id: f_00007-3-0 loss: 1.041971  [   32/  179]
train() client id: f_00007-3-1 loss: 1.020159  [   64/  179]
train() client id: f_00007-3-2 loss: 0.980817  [   96/  179]
train() client id: f_00007-3-3 loss: 0.990754  [  128/  179]
train() client id: f_00007-3-4 loss: 0.997231  [  160/  179]
train() client id: f_00007-4-0 loss: 1.001570  [   32/  179]
train() client id: f_00007-4-1 loss: 0.960512  [   64/  179]
train() client id: f_00007-4-2 loss: 0.905416  [   96/  179]
train() client id: f_00007-4-3 loss: 0.943785  [  128/  179]
train() client id: f_00007-4-4 loss: 0.919822  [  160/  179]
train() client id: f_00007-5-0 loss: 0.948164  [   32/  179]
train() client id: f_00007-5-1 loss: 0.895970  [   64/  179]
train() client id: f_00007-5-2 loss: 0.859150  [   96/  179]
train() client id: f_00007-5-3 loss: 0.886147  [  128/  179]
train() client id: f_00007-5-4 loss: 0.880522  [  160/  179]
train() client id: f_00007-6-0 loss: 0.868958  [   32/  179]
train() client id: f_00007-6-1 loss: 0.835137  [   64/  179]
train() client id: f_00007-6-2 loss: 0.858539  [   96/  179]
train() client id: f_00007-6-3 loss: 0.816159  [  128/  179]
train() client id: f_00007-6-4 loss: 0.848941  [  160/  179]
train() client id: f_00007-7-0 loss: 0.836086  [   32/  179]
train() client id: f_00007-7-1 loss: 0.870912  [   64/  179]
train() client id: f_00007-7-2 loss: 0.785577  [   96/  179]
train() client id: f_00007-7-3 loss: 0.792659  [  128/  179]
train() client id: f_00007-7-4 loss: 0.802237  [  160/  179]
train() client id: f_00007-8-0 loss: 0.786906  [   32/  179]
train() client id: f_00007-8-1 loss: 0.764138  [   64/  179]
train() client id: f_00007-8-2 loss: 0.784563  [   96/  179]
train() client id: f_00007-8-3 loss: 0.875654  [  128/  179]
train() client id: f_00007-8-4 loss: 0.825348  [  160/  179]
train() client id: f_00007-9-0 loss: 0.760554  [   32/  179]
train() client id: f_00007-9-1 loss: 0.836303  [   64/  179]
train() client id: f_00007-9-2 loss: 0.778067  [   96/  179]
train() client id: f_00007-9-3 loss: 0.825710  [  128/  179]
train() client id: f_00007-9-4 loss: 0.731611  [  160/  179]
train() client id: f_00007-10-0 loss: 0.748723  [   32/  179]
train() client id: f_00007-10-1 loss: 0.812895  [   64/  179]
train() client id: f_00007-10-2 loss: 0.812893  [   96/  179]
train() client id: f_00007-10-3 loss: 0.785263  [  128/  179]
train() client id: f_00007-10-4 loss: 0.745450  [  160/  179]
train() client id: f_00007-11-0 loss: 0.841167  [   32/  179]
train() client id: f_00007-11-1 loss: 0.719820  [   64/  179]
train() client id: f_00007-11-2 loss: 0.781150  [   96/  179]
train() client id: f_00007-11-3 loss: 0.704050  [  128/  179]
train() client id: f_00007-11-4 loss: 0.740665  [  160/  179]
train() client id: f_00007-12-0 loss: 0.741439  [   32/  179]
train() client id: f_00007-12-1 loss: 0.749351  [   64/  179]
train() client id: f_00007-12-2 loss: 0.783801  [   96/  179]
train() client id: f_00007-12-3 loss: 0.697048  [  128/  179]
train() client id: f_00007-12-4 loss: 0.863648  [  160/  179]
train() client id: f_00008-0-0 loss: 1.015910  [   32/  130]
train() client id: f_00008-0-1 loss: 1.083983  [   64/  130]
train() client id: f_00008-0-2 loss: 0.965666  [   96/  130]
train() client id: f_00008-0-3 loss: 0.937267  [  128/  130]
train() client id: f_00008-1-0 loss: 1.036064  [   32/  130]
train() client id: f_00008-1-1 loss: 0.953083  [   64/  130]
train() client id: f_00008-1-2 loss: 0.940144  [   96/  130]
train() client id: f_00008-1-3 loss: 1.038290  [  128/  130]
train() client id: f_00008-2-0 loss: 1.037825  [   32/  130]
train() client id: f_00008-2-1 loss: 0.913134  [   64/  130]
train() client id: f_00008-2-2 loss: 0.915707  [   96/  130]
train() client id: f_00008-2-3 loss: 1.036290  [  128/  130]
train() client id: f_00008-3-0 loss: 0.981985  [   32/  130]
train() client id: f_00008-3-1 loss: 0.963384  [   64/  130]
train() client id: f_00008-3-2 loss: 0.999329  [   96/  130]
train() client id: f_00008-3-3 loss: 0.938005  [  128/  130]
train() client id: f_00008-4-0 loss: 0.912028  [   32/  130]
train() client id: f_00008-4-1 loss: 0.996333  [   64/  130]
train() client id: f_00008-4-2 loss: 0.924598  [   96/  130]
train() client id: f_00008-4-3 loss: 1.014759  [  128/  130]
train() client id: f_00008-5-0 loss: 0.875215  [   32/  130]
train() client id: f_00008-5-1 loss: 1.009499  [   64/  130]
train() client id: f_00008-5-2 loss: 0.983886  [   96/  130]
train() client id: f_00008-5-3 loss: 0.975698  [  128/  130]
train() client id: f_00008-6-0 loss: 1.065644  [   32/  130]
train() client id: f_00008-6-1 loss: 1.087622  [   64/  130]
train() client id: f_00008-6-2 loss: 0.871885  [   96/  130]
train() client id: f_00008-6-3 loss: 0.802941  [  128/  130]
train() client id: f_00008-7-0 loss: 0.986763  [   32/  130]
train() client id: f_00008-7-1 loss: 0.906480  [   64/  130]
train() client id: f_00008-7-2 loss: 0.975158  [   96/  130]
train() client id: f_00008-7-3 loss: 0.941634  [  128/  130]
train() client id: f_00008-8-0 loss: 0.856895  [   32/  130]
train() client id: f_00008-8-1 loss: 0.996759  [   64/  130]
train() client id: f_00008-8-2 loss: 0.954108  [   96/  130]
train() client id: f_00008-8-3 loss: 1.000175  [  128/  130]
train() client id: f_00008-9-0 loss: 1.009204  [   32/  130]
train() client id: f_00008-9-1 loss: 0.997082  [   64/  130]
train() client id: f_00008-9-2 loss: 0.945772  [   96/  130]
train() client id: f_00008-9-3 loss: 0.835806  [  128/  130]
train() client id: f_00008-10-0 loss: 0.895440  [   32/  130]
train() client id: f_00008-10-1 loss: 0.944502  [   64/  130]
train() client id: f_00008-10-2 loss: 0.927413  [   96/  130]
train() client id: f_00008-10-3 loss: 1.036628  [  128/  130]
train() client id: f_00008-11-0 loss: 0.904251  [   32/  130]
train() client id: f_00008-11-1 loss: 0.899521  [   64/  130]
train() client id: f_00008-11-2 loss: 0.901620  [   96/  130]
train() client id: f_00008-11-3 loss: 1.081228  [  128/  130]
train() client id: f_00008-12-0 loss: 0.881011  [   32/  130]
train() client id: f_00008-12-1 loss: 0.949537  [   64/  130]
train() client id: f_00008-12-2 loss: 0.996901  [   96/  130]
train() client id: f_00008-12-3 loss: 0.876914  [  128/  130]
train() client id: f_00009-0-0 loss: 1.280542  [   32/  118]
train() client id: f_00009-0-1 loss: 1.223633  [   64/  118]
train() client id: f_00009-0-2 loss: 1.206637  [   96/  118]
train() client id: f_00009-1-0 loss: 1.233054  [   32/  118]
train() client id: f_00009-1-1 loss: 1.178760  [   64/  118]
train() client id: f_00009-1-2 loss: 1.216041  [   96/  118]
train() client id: f_00009-2-0 loss: 1.096871  [   32/  118]
train() client id: f_00009-2-1 loss: 1.166344  [   64/  118]
train() client id: f_00009-2-2 loss: 1.143331  [   96/  118]
train() client id: f_00009-3-0 loss: 1.151521  [   32/  118]
train() client id: f_00009-3-1 loss: 1.078013  [   64/  118]
train() client id: f_00009-3-2 loss: 1.149072  [   96/  118]
train() client id: f_00009-4-0 loss: 1.123962  [   32/  118]
train() client id: f_00009-4-1 loss: 1.027687  [   64/  118]
train() client id: f_00009-4-2 loss: 1.042886  [   96/  118]
train() client id: f_00009-5-0 loss: 1.155853  [   32/  118]
train() client id: f_00009-5-1 loss: 1.002286  [   64/  118]
train() client id: f_00009-5-2 loss: 1.061506  [   96/  118]
train() client id: f_00009-6-0 loss: 1.060004  [   32/  118]
train() client id: f_00009-6-1 loss: 0.988220  [   64/  118]
train() client id: f_00009-6-2 loss: 0.936678  [   96/  118]
train() client id: f_00009-7-0 loss: 0.996085  [   32/  118]
train() client id: f_00009-7-1 loss: 1.015362  [   64/  118]
train() client id: f_00009-7-2 loss: 0.990861  [   96/  118]
train() client id: f_00009-8-0 loss: 1.029874  [   32/  118]
train() client id: f_00009-8-1 loss: 0.878174  [   64/  118]
train() client id: f_00009-8-2 loss: 1.006810  [   96/  118]
train() client id: f_00009-9-0 loss: 0.918840  [   32/  118]
train() client id: f_00009-9-1 loss: 1.021504  [   64/  118]
train() client id: f_00009-9-2 loss: 0.989497  [   96/  118]
train() client id: f_00009-10-0 loss: 0.924326  [   32/  118]
train() client id: f_00009-10-1 loss: 0.917568  [   64/  118]
train() client id: f_00009-10-2 loss: 0.972845  [   96/  118]
train() client id: f_00009-11-0 loss: 0.965023  [   32/  118]
train() client id: f_00009-11-1 loss: 0.983464  [   64/  118]
train() client id: f_00009-11-2 loss: 0.943699  [   96/  118]
train() client id: f_00009-12-0 loss: 0.968775  [   32/  118]
train() client id: f_00009-12-1 loss: 0.952412  [   64/  118]
train() client id: f_00009-12-2 loss: 0.913097  [   96/  118]
At round 1 accuracy: 0.4005305039787798
At round 1 training accuracy: 0.39302481556002683
At round 1 training loss: 1.068555185489278
update_location
xs = [ -3.9056584  -10.79968212  25.00902392  18.81129433 -29.02070377
 -16.04359014   2.55680806  -6.32485185   9.66397685 -12.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  12.54482414   9.35018685
 -17.18584926  -2.62498432  -9.17765202  17.56900603   4.00148178]
dists_uav = [101.60999206 101.77729242 103.08828885 102.52432593 104.54485756
 102.72657981 100.06711653 100.6192479  101.99050162 100.80414995]
dists_bs = [232.490482   228.87591608 264.89269296 252.89227345 220.7936039
 249.40437224 251.15145857 249.74529647 242.66293951 236.19832881]
uav_gains = [9.60853004e-11 9.56909168e-11 9.26774783e-11 9.39572756e-11
 8.94829513e-11 9.34954732e-11 9.98320356e-11 9.84681076e-11
 9.51915868e-11 9.80171753e-11]
bs_gains = [2.61432507e-11 2.73157933e-11 1.81425423e-11 2.06573456e-11
 3.02086918e-11 2.14764606e-11 2.10607635e-11 2.13944731e-11
 2.31891278e-11 2.50103050e-11]
Round 2
-------------------------------
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.73113195 22.42682549 10.56475866  3.77907907 25.85610411 12.47188502
  4.69859503 15.17007359 11.12712296 10.11765294]
obj_prev = 126.94322881666007
eta_min = 1.8621250529904804e-09	eta_max = 0.9180913765363099
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 29.535190418159765	eta = 0.9090909090909091
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 50.41602139462105	eta = 0.5325722332838158
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 40.52128855107027	eta = 0.6626189360582114
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.752941123532246	eta = 0.6928551054183996
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.66719520504412	eta = 0.6943915369355615
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.6669798224976	eta = 0.694395404830551
eta = 0.694395404830551
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.0300953  0.06329568 0.0296176  0.01027062 0.07308859 0.03487234
 0.01289799 0.04275444 0.03105071 0.02818449]
ene_total = [3.27908415 6.43447607 3.23987046 1.49764204 7.29869581 3.92072634
 1.72777469 4.41559147 3.56239059 3.2907282 ]
ti_comp = [0.26882441 0.25014105 0.26840555 0.26856527 0.25202169 0.24528326
 0.26926224 0.26910548 0.24689208 0.24842218]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.35742980e-05 2.53298284e-04 2.25396212e-05 9.38793768e-07
 3.84196142e-04 4.40541962e-05 1.84967284e-06 6.74494733e-05
 3.06958933e-05 2.26740888e-05]
ene_total = [0.57330776 0.76208585 0.57698348 0.57360274 0.75694218 0.78696767
 0.5674136  0.57472659 0.77128999 0.75680081]
optimize_network iter = 0 obj = 6.700120669142431
eta = 0.694395404830551
freqs = [5.59757626e+07 1.26519974e+08 5.51732252e+07 1.91212720e+07
 1.45004571e+08 7.10858434e+07 2.39506050e+07 7.94380680e+07
 6.28831617e+07 5.67270011e+07]
eta_min = 0.6637595643039359	eta_max = 0.6943954048305468
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 0.07059948298279546	eta = 0.909090909090909
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 22.33326679569504	eta = 0.0028738002708385358
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.406693151543204	eta = 0.026667856733220745
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3186248233022884	eta = 0.02768078195366155
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3185795201975767	eta = 0.02768132281299047
eta = 0.02768132281299047
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.31855663e-04 2.49121488e-03 2.21679510e-04 9.23313400e-06
 3.77860887e-03 4.33277584e-04 1.81917241e-05 6.63372559e-04
 3.01897292e-04 2.23002013e-04]
ene_total = [0.18608375 0.30281643 0.18698052 0.18052076 0.33390587 0.25835522
 0.17880307 0.19749234 0.25008995 0.2435316 ]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 1 obj = 6.097170992212053
eta = 0.6637595643039359
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
eta_min = 0.6637595643039447	eta_max = 0.6637595643039276
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 0.06961124159514927	eta = 0.909090909090909
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 22.33232490105157	eta = 0.0028336927384439636
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.4022158004882264	eta = 0.026343572834638552
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.3153036098319895	eta = 0.027332461555343614
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.315259971692997	eta = 0.02733297671898439
eta = 0.02733297671898439
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.32590817e-04 2.45845542e-03 2.22305891e-04 9.26043939e-06
 3.73546925e-03 4.25596504e-04 1.82559506e-05 6.65629235e-04
 2.97007961e-04 2.19712069e-04]
ene_total = [0.18603789 0.30178187 0.18693126 0.18045688 0.33256672 0.25804555
 0.17874085 0.19748541 0.24986216 0.24335138]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 2 obj = 6.09717099221221
eta = 0.6637595643039447
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
Done!
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.56524120e-05 2.71142739e-04 2.45180888e-05 1.02133270e-06
 4.11984434e-04 4.69389849e-05 2.01344649e-06 7.34121646e-05
 3.27569707e-05 2.42320165e-05]
ene_total = [0.00637376 0.00848759 0.00641452 0.00637505 0.00844037 0.00874916
 0.00630634 0.00639342 0.0085741  0.00841257]
At round 2 energy consumption: 0.07452687465895509
At round 2 eta: 0.6637595643039447
At round 2 a_n: 27.49751116231574
At round 2 local rounds: 13.420089837852764
At round 2 global rounds: 81.77931100223806
gradient difference: 0.5406917333602905
train() client id: f_00000-0-0 loss: 1.276965  [   32/  126]
train() client id: f_00000-0-1 loss: 1.075014  [   64/  126]
train() client id: f_00000-0-2 loss: 1.263688  [   96/  126]
train() client id: f_00000-1-0 loss: 0.998499  [   32/  126]
train() client id: f_00000-1-1 loss: 1.188196  [   64/  126]
train() client id: f_00000-1-2 loss: 1.101002  [   96/  126]
train() client id: f_00000-2-0 loss: 1.031637  [   32/  126]
train() client id: f_00000-2-1 loss: 1.100899  [   64/  126]
train() client id: f_00000-2-2 loss: 1.026969  [   96/  126]
train() client id: f_00000-3-0 loss: 1.055035  [   32/  126]
train() client id: f_00000-3-1 loss: 0.960574  [   64/  126]
train() client id: f_00000-3-2 loss: 1.015720  [   96/  126]
train() client id: f_00000-4-0 loss: 0.985594  [   32/  126]
train() client id: f_00000-4-1 loss: 0.957457  [   64/  126]
train() client id: f_00000-4-2 loss: 0.964529  [   96/  126]
train() client id: f_00000-5-0 loss: 0.959294  [   32/  126]
train() client id: f_00000-5-1 loss: 0.943569  [   64/  126]
train() client id: f_00000-5-2 loss: 1.000367  [   96/  126]
train() client id: f_00000-6-0 loss: 0.966026  [   32/  126]
train() client id: f_00000-6-1 loss: 0.943362  [   64/  126]
train() client id: f_00000-6-2 loss: 0.992919  [   96/  126]
train() client id: f_00000-7-0 loss: 0.955551  [   32/  126]
train() client id: f_00000-7-1 loss: 0.971906  [   64/  126]
train() client id: f_00000-7-2 loss: 0.936998  [   96/  126]
train() client id: f_00000-8-0 loss: 0.989569  [   32/  126]
train() client id: f_00000-8-1 loss: 0.934785  [   64/  126]
train() client id: f_00000-8-2 loss: 0.944251  [   96/  126]
train() client id: f_00000-9-0 loss: 1.013469  [   32/  126]
train() client id: f_00000-9-1 loss: 0.914925  [   64/  126]
train() client id: f_00000-9-2 loss: 0.996734  [   96/  126]
train() client id: f_00000-10-0 loss: 0.972499  [   32/  126]
train() client id: f_00000-10-1 loss: 1.017301  [   64/  126]
train() client id: f_00000-10-2 loss: 0.831310  [   96/  126]
train() client id: f_00000-11-0 loss: 1.013268  [   32/  126]
train() client id: f_00000-11-1 loss: 0.940385  [   64/  126]
train() client id: f_00000-11-2 loss: 0.938147  [   96/  126]
train() client id: f_00000-12-0 loss: 0.931280  [   32/  126]
train() client id: f_00000-12-1 loss: 0.925003  [   64/  126]
train() client id: f_00000-12-2 loss: 1.058228  [   96/  126]
train() client id: f_00001-0-0 loss: 0.784176  [   32/  265]
train() client id: f_00001-0-1 loss: 0.808420  [   64/  265]
train() client id: f_00001-0-2 loss: 0.766252  [   96/  265]
train() client id: f_00001-0-3 loss: 0.765677  [  128/  265]
train() client id: f_00001-0-4 loss: 0.789499  [  160/  265]
train() client id: f_00001-0-5 loss: 0.788950  [  192/  265]
train() client id: f_00001-0-6 loss: 0.774633  [  224/  265]
train() client id: f_00001-0-7 loss: 0.730229  [  256/  265]
train() client id: f_00001-1-0 loss: 0.718844  [   32/  265]
train() client id: f_00001-1-1 loss: 0.729897  [   64/  265]
train() client id: f_00001-1-2 loss: 0.720793  [   96/  265]
train() client id: f_00001-1-3 loss: 0.711522  [  128/  265]
train() client id: f_00001-1-4 loss: 0.666844  [  160/  265]
train() client id: f_00001-1-5 loss: 0.777906  [  192/  265]
train() client id: f_00001-1-6 loss: 0.708797  [  224/  265]
train() client id: f_00001-1-7 loss: 0.629210  [  256/  265]
train() client id: f_00001-2-0 loss: 0.658176  [   32/  265]
train() client id: f_00001-2-1 loss: 0.738690  [   64/  265]
train() client id: f_00001-2-2 loss: 0.640001  [   96/  265]
train() client id: f_00001-2-3 loss: 0.647229  [  128/  265]
train() client id: f_00001-2-4 loss: 0.707610  [  160/  265]
train() client id: f_00001-2-5 loss: 0.692213  [  192/  265]
train() client id: f_00001-2-6 loss: 0.638635  [  224/  265]
train() client id: f_00001-2-7 loss: 0.583077  [  256/  265]
train() client id: f_00001-3-0 loss: 0.616513  [   32/  265]
train() client id: f_00001-3-1 loss: 0.629906  [   64/  265]
train() client id: f_00001-3-2 loss: 0.599800  [   96/  265]
train() client id: f_00001-3-3 loss: 0.659179  [  128/  265]
train() client id: f_00001-3-4 loss: 0.584164  [  160/  265]
train() client id: f_00001-3-5 loss: 0.702125  [  192/  265]
train() client id: f_00001-3-6 loss: 0.643546  [  224/  265]
train() client id: f_00001-3-7 loss: 0.589694  [  256/  265]
train() client id: f_00001-4-0 loss: 0.575745  [   32/  265]
train() client id: f_00001-4-1 loss: 0.562216  [   64/  265]
train() client id: f_00001-4-2 loss: 0.562805  [   96/  265]
train() client id: f_00001-4-3 loss: 0.622353  [  128/  265]
train() client id: f_00001-4-4 loss: 0.561223  [  160/  265]
train() client id: f_00001-4-5 loss: 0.640595  [  192/  265]
train() client id: f_00001-4-6 loss: 0.695734  [  224/  265]
train() client id: f_00001-4-7 loss: 0.650289  [  256/  265]
train() client id: f_00001-5-0 loss: 0.606457  [   32/  265]
train() client id: f_00001-5-1 loss: 0.589826  [   64/  265]
train() client id: f_00001-5-2 loss: 0.604529  [   96/  265]
train() client id: f_00001-5-3 loss: 0.635804  [  128/  265]
train() client id: f_00001-5-4 loss: 0.597344  [  160/  265]
train() client id: f_00001-5-5 loss: 0.549221  [  192/  265]
train() client id: f_00001-5-6 loss: 0.640960  [  224/  265]
train() client id: f_00001-5-7 loss: 0.536319  [  256/  265]
train() client id: f_00001-6-0 loss: 0.593348  [   32/  265]
train() client id: f_00001-6-1 loss: 0.596947  [   64/  265]
train() client id: f_00001-6-2 loss: 0.611145  [   96/  265]
train() client id: f_00001-6-3 loss: 0.567401  [  128/  265]
train() client id: f_00001-6-4 loss: 0.646141  [  160/  265]
train() client id: f_00001-6-5 loss: 0.501602  [  192/  265]
train() client id: f_00001-6-6 loss: 0.589596  [  224/  265]
train() client id: f_00001-6-7 loss: 0.517911  [  256/  265]
train() client id: f_00001-7-0 loss: 0.616115  [   32/  265]
train() client id: f_00001-7-1 loss: 0.528882  [   64/  265]
train() client id: f_00001-7-2 loss: 0.604426  [   96/  265]
train() client id: f_00001-7-3 loss: 0.618326  [  128/  265]
train() client id: f_00001-7-4 loss: 0.647909  [  160/  265]
train() client id: f_00001-7-5 loss: 0.550542  [  192/  265]
train() client id: f_00001-7-6 loss: 0.513737  [  224/  265]
train() client id: f_00001-7-7 loss: 0.493316  [  256/  265]
train() client id: f_00001-8-0 loss: 0.557053  [   32/  265]
train() client id: f_00001-8-1 loss: 0.496170  [   64/  265]
train() client id: f_00001-8-2 loss: 0.594647  [   96/  265]
train() client id: f_00001-8-3 loss: 0.645349  [  128/  265]
train() client id: f_00001-8-4 loss: 0.559809  [  160/  265]
train() client id: f_00001-8-5 loss: 0.483913  [  192/  265]
train() client id: f_00001-8-6 loss: 0.628290  [  224/  265]
train() client id: f_00001-8-7 loss: 0.544030  [  256/  265]
train() client id: f_00001-9-0 loss: 0.626407  [   32/  265]
train() client id: f_00001-9-1 loss: 0.542210  [   64/  265]
train() client id: f_00001-9-2 loss: 0.536371  [   96/  265]
train() client id: f_00001-9-3 loss: 0.495912  [  128/  265]
train() client id: f_00001-9-4 loss: 0.527549  [  160/  265]
train() client id: f_00001-9-5 loss: 0.642741  [  192/  265]
train() client id: f_00001-9-6 loss: 0.505458  [  224/  265]
train() client id: f_00001-9-7 loss: 0.606896  [  256/  265]
train() client id: f_00001-10-0 loss: 0.611124  [   32/  265]
train() client id: f_00001-10-1 loss: 0.472465  [   64/  265]
train() client id: f_00001-10-2 loss: 0.543571  [   96/  265]
train() client id: f_00001-10-3 loss: 0.590336  [  128/  265]
train() client id: f_00001-10-4 loss: 0.560331  [  160/  265]
train() client id: f_00001-10-5 loss: 0.502591  [  192/  265]
train() client id: f_00001-10-6 loss: 0.577943  [  224/  265]
train() client id: f_00001-10-7 loss: 0.572382  [  256/  265]
train() client id: f_00001-11-0 loss: 0.584218  [   32/  265]
train() client id: f_00001-11-1 loss: 0.463911  [   64/  265]
train() client id: f_00001-11-2 loss: 0.591212  [   96/  265]
train() client id: f_00001-11-3 loss: 0.519371  [  128/  265]
train() client id: f_00001-11-4 loss: 0.459947  [  160/  265]
train() client id: f_00001-11-5 loss: 0.535183  [  192/  265]
train() client id: f_00001-11-6 loss: 0.611699  [  224/  265]
train() client id: f_00001-11-7 loss: 0.708151  [  256/  265]
train() client id: f_00001-12-0 loss: 0.492304  [   32/  265]
train() client id: f_00001-12-1 loss: 0.615467  [   64/  265]
train() client id: f_00001-12-2 loss: 0.509650  [   96/  265]
train() client id: f_00001-12-3 loss: 0.555070  [  128/  265]
train() client id: f_00001-12-4 loss: 0.504572  [  160/  265]
train() client id: f_00001-12-5 loss: 0.535258  [  192/  265]
train() client id: f_00001-12-6 loss: 0.587645  [  224/  265]
train() client id: f_00001-12-7 loss: 0.624037  [  256/  265]
train() client id: f_00002-0-0 loss: 1.007697  [   32/  124]
train() client id: f_00002-0-1 loss: 0.982245  [   64/  124]
train() client id: f_00002-0-2 loss: 0.999346  [   96/  124]
train() client id: f_00002-1-0 loss: 0.937866  [   32/  124]
train() client id: f_00002-1-1 loss: 0.931319  [   64/  124]
train() client id: f_00002-1-2 loss: 0.914013  [   96/  124]
train() client id: f_00002-2-0 loss: 0.937943  [   32/  124]
train() client id: f_00002-2-1 loss: 0.938128  [   64/  124]
train() client id: f_00002-2-2 loss: 0.919412  [   96/  124]
train() client id: f_00002-3-0 loss: 1.026942  [   32/  124]
train() client id: f_00002-3-1 loss: 0.874056  [   64/  124]
train() client id: f_00002-3-2 loss: 0.824939  [   96/  124]
train() client id: f_00002-4-0 loss: 0.825782  [   32/  124]
train() client id: f_00002-4-1 loss: 0.906099  [   64/  124]
train() client id: f_00002-4-2 loss: 0.915118  [   96/  124]
train() client id: f_00002-5-0 loss: 0.862250  [   32/  124]
train() client id: f_00002-5-1 loss: 0.921754  [   64/  124]
train() client id: f_00002-5-2 loss: 0.926872  [   96/  124]
train() client id: f_00002-6-0 loss: 0.947167  [   32/  124]
train() client id: f_00002-6-1 loss: 0.936887  [   64/  124]
train() client id: f_00002-6-2 loss: 0.856599  [   96/  124]
train() client id: f_00002-7-0 loss: 0.901109  [   32/  124]
train() client id: f_00002-7-1 loss: 0.820799  [   64/  124]
train() client id: f_00002-7-2 loss: 0.853092  [   96/  124]
train() client id: f_00002-8-0 loss: 0.928349  [   32/  124]
train() client id: f_00002-8-1 loss: 0.823737  [   64/  124]
train() client id: f_00002-8-2 loss: 0.845826  [   96/  124]
train() client id: f_00002-9-0 loss: 0.856029  [   32/  124]
train() client id: f_00002-9-1 loss: 0.964055  [   64/  124]
train() client id: f_00002-9-2 loss: 0.803622  [   96/  124]
train() client id: f_00002-10-0 loss: 0.742183  [   32/  124]
train() client id: f_00002-10-1 loss: 1.020920  [   64/  124]
train() client id: f_00002-10-2 loss: 0.873354  [   96/  124]
train() client id: f_00002-11-0 loss: 0.915372  [   32/  124]
train() client id: f_00002-11-1 loss: 0.895619  [   64/  124]
train() client id: f_00002-11-2 loss: 0.790697  [   96/  124]
train() client id: f_00002-12-0 loss: 0.955237  [   32/  124]
train() client id: f_00002-12-1 loss: 0.780518  [   64/  124]
train() client id: f_00002-12-2 loss: 0.920471  [   96/  124]
train() client id: f_00003-0-0 loss: 1.092888  [   32/   43]
train() client id: f_00003-1-0 loss: 1.128777  [   32/   43]
train() client id: f_00003-2-0 loss: 1.100449  [   32/   43]
train() client id: f_00003-3-0 loss: 1.116216  [   32/   43]
train() client id: f_00003-4-0 loss: 1.073585  [   32/   43]
train() client id: f_00003-5-0 loss: 1.065058  [   32/   43]
train() client id: f_00003-6-0 loss: 1.078695  [   32/   43]
train() client id: f_00003-7-0 loss: 1.079829  [   32/   43]
train() client id: f_00003-8-0 loss: 1.111295  [   32/   43]
train() client id: f_00003-9-0 loss: 1.091112  [   32/   43]
train() client id: f_00003-10-0 loss: 1.079428  [   32/   43]
train() client id: f_00003-11-0 loss: 1.092592  [   32/   43]
train() client id: f_00003-12-0 loss: 1.084087  [   32/   43]
train() client id: f_00004-0-0 loss: 1.073919  [   32/  306]
train() client id: f_00004-0-1 loss: 1.017193  [   64/  306]
train() client id: f_00004-0-2 loss: 1.087394  [   96/  306]
train() client id: f_00004-0-3 loss: 1.044755  [  128/  306]
train() client id: f_00004-0-4 loss: 1.051120  [  160/  306]
train() client id: f_00004-0-5 loss: 1.048248  [  192/  306]
train() client id: f_00004-0-6 loss: 1.063613  [  224/  306]
train() client id: f_00004-0-7 loss: 1.037398  [  256/  306]
train() client id: f_00004-0-8 loss: 1.007070  [  288/  306]
train() client id: f_00004-1-0 loss: 1.016110  [   32/  306]
train() client id: f_00004-1-1 loss: 1.017877  [   64/  306]
train() client id: f_00004-1-2 loss: 1.030571  [   96/  306]
train() client id: f_00004-1-3 loss: 1.037473  [  128/  306]
train() client id: f_00004-1-4 loss: 0.975145  [  160/  306]
train() client id: f_00004-1-5 loss: 1.025549  [  192/  306]
train() client id: f_00004-1-6 loss: 1.015662  [  224/  306]
train() client id: f_00004-1-7 loss: 0.984814  [  256/  306]
train() client id: f_00004-1-8 loss: 1.022394  [  288/  306]
train() client id: f_00004-2-0 loss: 0.996147  [   32/  306]
train() client id: f_00004-2-1 loss: 0.950258  [   64/  306]
train() client id: f_00004-2-2 loss: 1.040526  [   96/  306]
train() client id: f_00004-2-3 loss: 0.950200  [  128/  306]
train() client id: f_00004-2-4 loss: 0.995084  [  160/  306]
train() client id: f_00004-2-5 loss: 1.024108  [  192/  306]
train() client id: f_00004-2-6 loss: 1.014636  [  224/  306]
train() client id: f_00004-2-7 loss: 0.989566  [  256/  306]
train() client id: f_00004-2-8 loss: 0.987609  [  288/  306]
train() client id: f_00004-3-0 loss: 0.959958  [   32/  306]
train() client id: f_00004-3-1 loss: 0.939580  [   64/  306]
train() client id: f_00004-3-2 loss: 0.969544  [   96/  306]
train() client id: f_00004-3-3 loss: 0.960066  [  128/  306]
train() client id: f_00004-3-4 loss: 1.021658  [  160/  306]
train() client id: f_00004-3-5 loss: 0.975516  [  192/  306]
train() client id: f_00004-3-6 loss: 0.952423  [  224/  306]
train() client id: f_00004-3-7 loss: 1.017343  [  256/  306]
train() client id: f_00004-3-8 loss: 0.994319  [  288/  306]
train() client id: f_00004-4-0 loss: 0.924675  [   32/  306]
train() client id: f_00004-4-1 loss: 0.977604  [   64/  306]
train() client id: f_00004-4-2 loss: 0.938083  [   96/  306]
train() client id: f_00004-4-3 loss: 0.975935  [  128/  306]
train() client id: f_00004-4-4 loss: 0.953059  [  160/  306]
train() client id: f_00004-4-5 loss: 0.957670  [  192/  306]
train() client id: f_00004-4-6 loss: 0.933037  [  224/  306]
train() client id: f_00004-4-7 loss: 1.090880  [  256/  306]
train() client id: f_00004-4-8 loss: 0.926893  [  288/  306]
train() client id: f_00004-5-0 loss: 0.953978  [   32/  306]
train() client id: f_00004-5-1 loss: 0.944463  [   64/  306]
train() client id: f_00004-5-2 loss: 0.974935  [   96/  306]
train() client id: f_00004-5-3 loss: 1.047608  [  128/  306]
train() client id: f_00004-5-4 loss: 0.939272  [  160/  306]
train() client id: f_00004-5-5 loss: 0.988070  [  192/  306]
train() client id: f_00004-5-6 loss: 0.963484  [  224/  306]
train() client id: f_00004-5-7 loss: 0.910658  [  256/  306]
train() client id: f_00004-5-8 loss: 0.877571  [  288/  306]
train() client id: f_00004-6-0 loss: 0.954211  [   32/  306]
train() client id: f_00004-6-1 loss: 0.999648  [   64/  306]
train() client id: f_00004-6-2 loss: 0.951095  [   96/  306]
train() client id: f_00004-6-3 loss: 0.929276  [  128/  306]
train() client id: f_00004-6-4 loss: 0.979121  [  160/  306]
train() client id: f_00004-6-5 loss: 0.925298  [  192/  306]
train() client id: f_00004-6-6 loss: 0.962791  [  224/  306]
train() client id: f_00004-6-7 loss: 0.934619  [  256/  306]
train() client id: f_00004-6-8 loss: 0.895343  [  288/  306]
train() client id: f_00004-7-0 loss: 0.861095  [   32/  306]
train() client id: f_00004-7-1 loss: 0.968362  [   64/  306]
train() client id: f_00004-7-2 loss: 0.918370  [   96/  306]
train() client id: f_00004-7-3 loss: 0.937957  [  128/  306]
train() client id: f_00004-7-4 loss: 0.974395  [  160/  306]
train() client id: f_00004-7-5 loss: 1.010314  [  192/  306]
train() client id: f_00004-7-6 loss: 0.912766  [  224/  306]
train() client id: f_00004-7-7 loss: 0.952363  [  256/  306]
train() client id: f_00004-7-8 loss: 0.946011  [  288/  306]
train() client id: f_00004-8-0 loss: 0.990804  [   32/  306]
train() client id: f_00004-8-1 loss: 0.865447  [   64/  306]
train() client id: f_00004-8-2 loss: 1.025660  [   96/  306]
train() client id: f_00004-8-3 loss: 0.904208  [  128/  306]
train() client id: f_00004-8-4 loss: 0.908367  [  160/  306]
train() client id: f_00004-8-5 loss: 0.953902  [  192/  306]
train() client id: f_00004-8-6 loss: 0.992553  [  224/  306]
train() client id: f_00004-8-7 loss: 0.880006  [  256/  306]
train() client id: f_00004-8-8 loss: 0.969388  [  288/  306]
train() client id: f_00004-9-0 loss: 0.924695  [   32/  306]
train() client id: f_00004-9-1 loss: 0.966568  [   64/  306]
train() client id: f_00004-9-2 loss: 0.978947  [   96/  306]
train() client id: f_00004-9-3 loss: 0.881883  [  128/  306]
train() client id: f_00004-9-4 loss: 0.936165  [  160/  306]
train() client id: f_00004-9-5 loss: 0.967028  [  192/  306]
train() client id: f_00004-9-6 loss: 0.999771  [  224/  306]
train() client id: f_00004-9-7 loss: 0.949263  [  256/  306]
train() client id: f_00004-9-8 loss: 0.881695  [  288/  306]
train() client id: f_00004-10-0 loss: 0.947113  [   32/  306]
train() client id: f_00004-10-1 loss: 0.954671  [   64/  306]
train() client id: f_00004-10-2 loss: 0.858722  [   96/  306]
train() client id: f_00004-10-3 loss: 0.990072  [  128/  306]
train() client id: f_00004-10-4 loss: 0.984747  [  160/  306]
train() client id: f_00004-10-5 loss: 1.062615  [  192/  306]
train() client id: f_00004-10-6 loss: 0.886267  [  224/  306]
train() client id: f_00004-10-7 loss: 0.813841  [  256/  306]
train() client id: f_00004-10-8 loss: 0.931824  [  288/  306]
train() client id: f_00004-11-0 loss: 1.034125  [   32/  306]
train() client id: f_00004-11-1 loss: 0.905385  [   64/  306]
train() client id: f_00004-11-2 loss: 0.908165  [   96/  306]
train() client id: f_00004-11-3 loss: 0.936451  [  128/  306]
train() client id: f_00004-11-4 loss: 0.831910  [  160/  306]
train() client id: f_00004-11-5 loss: 0.949073  [  192/  306]
train() client id: f_00004-11-6 loss: 1.039341  [  224/  306]
train() client id: f_00004-11-7 loss: 0.882300  [  256/  306]
train() client id: f_00004-11-8 loss: 0.935106  [  288/  306]
train() client id: f_00004-12-0 loss: 0.931059  [   32/  306]
train() client id: f_00004-12-1 loss: 0.901460  [   64/  306]
train() client id: f_00004-12-2 loss: 0.890791  [   96/  306]
train() client id: f_00004-12-3 loss: 0.901971  [  128/  306]
train() client id: f_00004-12-4 loss: 1.033190  [  160/  306]
train() client id: f_00004-12-5 loss: 1.025811  [  192/  306]
train() client id: f_00004-12-6 loss: 1.012605  [  224/  306]
train() client id: f_00004-12-7 loss: 0.898566  [  256/  306]
train() client id: f_00004-12-8 loss: 0.913467  [  288/  306]
train() client id: f_00005-0-0 loss: 1.074403  [   32/  146]
train() client id: f_00005-0-1 loss: 1.117675  [   64/  146]
train() client id: f_00005-0-2 loss: 1.092246  [   96/  146]
train() client id: f_00005-0-3 loss: 1.067658  [  128/  146]
train() client id: f_00005-1-0 loss: 1.069945  [   32/  146]
train() client id: f_00005-1-1 loss: 1.061829  [   64/  146]
train() client id: f_00005-1-2 loss: 1.046237  [   96/  146]
train() client id: f_00005-1-3 loss: 1.080811  [  128/  146]
train() client id: f_00005-2-0 loss: 0.999999  [   32/  146]
train() client id: f_00005-2-1 loss: 1.026800  [   64/  146]
train() client id: f_00005-2-2 loss: 1.022767  [   96/  146]
train() client id: f_00005-2-3 loss: 1.053019  [  128/  146]
train() client id: f_00005-3-0 loss: 1.028455  [   32/  146]
train() client id: f_00005-3-1 loss: 0.985755  [   64/  146]
train() client id: f_00005-3-2 loss: 1.010982  [   96/  146]
train() client id: f_00005-3-3 loss: 1.025221  [  128/  146]
train() client id: f_00005-4-0 loss: 0.977446  [   32/  146]
train() client id: f_00005-4-1 loss: 0.960666  [   64/  146]
train() client id: f_00005-4-2 loss: 1.070898  [   96/  146]
train() client id: f_00005-4-3 loss: 1.052426  [  128/  146]
train() client id: f_00005-5-0 loss: 1.073887  [   32/  146]
train() client id: f_00005-5-1 loss: 0.960051  [   64/  146]
train() client id: f_00005-5-2 loss: 1.048619  [   96/  146]
train() client id: f_00005-5-3 loss: 0.976771  [  128/  146]
train() client id: f_00005-6-0 loss: 0.946839  [   32/  146]
train() client id: f_00005-6-1 loss: 1.044290  [   64/  146]
train() client id: f_00005-6-2 loss: 1.029671  [   96/  146]
train() client id: f_00005-6-3 loss: 1.012099  [  128/  146]
train() client id: f_00005-7-0 loss: 0.968914  [   32/  146]
train() client id: f_00005-7-1 loss: 0.944115  [   64/  146]
train() client id: f_00005-7-2 loss: 1.081487  [   96/  146]
train() client id: f_00005-7-3 loss: 0.948826  [  128/  146]
train() client id: f_00005-8-0 loss: 0.935293  [   32/  146]
train() client id: f_00005-8-1 loss: 1.062112  [   64/  146]
train() client id: f_00005-8-2 loss: 0.994506  [   96/  146]
train() client id: f_00005-8-3 loss: 0.961369  [  128/  146]
train() client id: f_00005-9-0 loss: 1.016876  [   32/  146]
train() client id: f_00005-9-1 loss: 0.908925  [   64/  146]
train() client id: f_00005-9-2 loss: 0.961565  [   96/  146]
train() client id: f_00005-9-3 loss: 1.018513  [  128/  146]
train() client id: f_00005-10-0 loss: 1.008966  [   32/  146]
train() client id: f_00005-10-1 loss: 0.975119  [   64/  146]
train() client id: f_00005-10-2 loss: 1.057437  [   96/  146]
train() client id: f_00005-10-3 loss: 1.040360  [  128/  146]
train() client id: f_00005-11-0 loss: 1.056216  [   32/  146]
train() client id: f_00005-11-1 loss: 0.959498  [   64/  146]
train() client id: f_00005-11-2 loss: 1.088084  [   96/  146]
train() client id: f_00005-11-3 loss: 0.972398  [  128/  146]
train() client id: f_00005-12-0 loss: 1.054828  [   32/  146]
train() client id: f_00005-12-1 loss: 0.956442  [   64/  146]
train() client id: f_00005-12-2 loss: 0.913297  [   96/  146]
train() client id: f_00005-12-3 loss: 1.080133  [  128/  146]
train() client id: f_00006-0-0 loss: 1.088713  [   32/   54]
train() client id: f_00006-1-0 loss: 1.116379  [   32/   54]
train() client id: f_00006-2-0 loss: 1.140154  [   32/   54]
train() client id: f_00006-3-0 loss: 1.075564  [   32/   54]
train() client id: f_00006-4-0 loss: 1.122857  [   32/   54]
train() client id: f_00006-5-0 loss: 1.147057  [   32/   54]
train() client id: f_00006-6-0 loss: 1.056834  [   32/   54]
train() client id: f_00006-7-0 loss: 1.139168  [   32/   54]
train() client id: f_00006-8-0 loss: 1.159891  [   32/   54]
train() client id: f_00006-9-0 loss: 1.072593  [   32/   54]
train() client id: f_00006-10-0 loss: 1.063580  [   32/   54]
train() client id: f_00006-11-0 loss: 1.086525  [   32/   54]
train() client id: f_00006-12-0 loss: 1.153558  [   32/   54]
train() client id: f_00007-0-0 loss: 1.133605  [   32/  179]
train() client id: f_00007-0-1 loss: 1.104422  [   64/  179]
train() client id: f_00007-0-2 loss: 1.087755  [   96/  179]
train() client id: f_00007-0-3 loss: 1.070528  [  128/  179]
train() client id: f_00007-0-4 loss: 1.081289  [  160/  179]
train() client id: f_00007-1-0 loss: 1.003469  [   32/  179]
train() client id: f_00007-1-1 loss: 1.014928  [   64/  179]
train() client id: f_00007-1-2 loss: 1.019738  [   96/  179]
train() client id: f_00007-1-3 loss: 1.015848  [  128/  179]
train() client id: f_00007-1-4 loss: 0.951927  [  160/  179]
train() client id: f_00007-2-0 loss: 0.940527  [   32/  179]
train() client id: f_00007-2-1 loss: 0.966624  [   64/  179]
train() client id: f_00007-2-2 loss: 0.942922  [   96/  179]
train() client id: f_00007-2-3 loss: 0.913499  [  128/  179]
train() client id: f_00007-2-4 loss: 0.895708  [  160/  179]
train() client id: f_00007-3-0 loss: 0.913163  [   32/  179]
train() client id: f_00007-3-1 loss: 0.855167  [   64/  179]
train() client id: f_00007-3-2 loss: 0.879716  [   96/  179]
train() client id: f_00007-3-3 loss: 0.880533  [  128/  179]
train() client id: f_00007-3-4 loss: 0.828325  [  160/  179]
train() client id: f_00007-4-0 loss: 0.809017  [   32/  179]
train() client id: f_00007-4-1 loss: 0.806859  [   64/  179]
train() client id: f_00007-4-2 loss: 0.917447  [   96/  179]
train() client id: f_00007-4-3 loss: 0.809411  [  128/  179]
train() client id: f_00007-4-4 loss: 0.768738  [  160/  179]
train() client id: f_00007-5-0 loss: 0.783361  [   32/  179]
train() client id: f_00007-5-1 loss: 0.762798  [   64/  179]
train() client id: f_00007-5-2 loss: 0.816275  [   96/  179]
train() client id: f_00007-5-3 loss: 0.792490  [  128/  179]
train() client id: f_00007-5-4 loss: 0.827315  [  160/  179]
train() client id: f_00007-6-0 loss: 0.756411  [   32/  179]
train() client id: f_00007-6-1 loss: 0.737948  [   64/  179]
train() client id: f_00007-6-2 loss: 0.789619  [   96/  179]
train() client id: f_00007-6-3 loss: 0.788815  [  128/  179]
train() client id: f_00007-6-4 loss: 0.779736  [  160/  179]
train() client id: f_00007-7-0 loss: 0.774880  [   32/  179]
train() client id: f_00007-7-1 loss: 0.719768  [   64/  179]
train() client id: f_00007-7-2 loss: 0.746614  [   96/  179]
train() client id: f_00007-7-3 loss: 0.778727  [  128/  179]
train() client id: f_00007-7-4 loss: 0.702557  [  160/  179]
train() client id: f_00007-8-0 loss: 0.696714  [   32/  179]
train() client id: f_00007-8-1 loss: 0.735262  [   64/  179]
train() client id: f_00007-8-2 loss: 0.705939  [   96/  179]
train() client id: f_00007-8-3 loss: 0.662340  [  128/  179]
train() client id: f_00007-8-4 loss: 0.777757  [  160/  179]
train() client id: f_00007-9-0 loss: 0.695730  [   32/  179]
train() client id: f_00007-9-1 loss: 0.685619  [   64/  179]
train() client id: f_00007-9-2 loss: 0.712632  [   96/  179]
train() client id: f_00007-9-3 loss: 0.745319  [  128/  179]
train() client id: f_00007-9-4 loss: 0.664351  [  160/  179]
train() client id: f_00007-10-0 loss: 0.733663  [   32/  179]
train() client id: f_00007-10-1 loss: 0.662327  [   64/  179]
train() client id: f_00007-10-2 loss: 0.678180  [   96/  179]
train() client id: f_00007-10-3 loss: 0.739761  [  128/  179]
train() client id: f_00007-10-4 loss: 0.718325  [  160/  179]
train() client id: f_00007-11-0 loss: 0.737844  [   32/  179]
train() client id: f_00007-11-1 loss: 0.663792  [   64/  179]
train() client id: f_00007-11-2 loss: 0.683979  [   96/  179]
train() client id: f_00007-11-3 loss: 0.680475  [  128/  179]
train() client id: f_00007-11-4 loss: 0.716196  [  160/  179]
train() client id: f_00007-12-0 loss: 0.660063  [   32/  179]
train() client id: f_00007-12-1 loss: 0.770030  [   64/  179]
train() client id: f_00007-12-2 loss: 0.697219  [   96/  179]
train() client id: f_00007-12-3 loss: 0.753268  [  128/  179]
train() client id: f_00007-12-4 loss: 0.625212  [  160/  179]
train() client id: f_00008-0-0 loss: 1.009998  [   32/  130]
train() client id: f_00008-0-1 loss: 0.946984  [   64/  130]
train() client id: f_00008-0-2 loss: 1.025953  [   96/  130]
train() client id: f_00008-0-3 loss: 1.085999  [  128/  130]
train() client id: f_00008-1-0 loss: 0.977661  [   32/  130]
train() client id: f_00008-1-1 loss: 1.038388  [   64/  130]
train() client id: f_00008-1-2 loss: 0.987720  [   96/  130]
train() client id: f_00008-1-3 loss: 1.002289  [  128/  130]
train() client id: f_00008-2-0 loss: 1.107440  [   32/  130]
train() client id: f_00008-2-1 loss: 0.981875  [   64/  130]
train() client id: f_00008-2-2 loss: 0.981514  [   96/  130]
train() client id: f_00008-2-3 loss: 0.945307  [  128/  130]
train() client id: f_00008-3-0 loss: 0.994609  [   32/  130]
train() client id: f_00008-3-1 loss: 1.130310  [   64/  130]
train() client id: f_00008-3-2 loss: 0.985372  [   96/  130]
train() client id: f_00008-3-3 loss: 0.918242  [  128/  130]
train() client id: f_00008-4-0 loss: 1.045992  [   32/  130]
train() client id: f_00008-4-1 loss: 1.100975  [   64/  130]
train() client id: f_00008-4-2 loss: 0.924630  [   96/  130]
train() client id: f_00008-4-3 loss: 0.957094  [  128/  130]
train() client id: f_00008-5-0 loss: 0.998101  [   32/  130]
train() client id: f_00008-5-1 loss: 0.955174  [   64/  130]
train() client id: f_00008-5-2 loss: 1.058517  [   96/  130]
train() client id: f_00008-5-3 loss: 1.007420  [  128/  130]
train() client id: f_00008-6-0 loss: 1.031786  [   32/  130]
train() client id: f_00008-6-1 loss: 0.986860  [   64/  130]
train() client id: f_00008-6-2 loss: 1.035795  [   96/  130]
train() client id: f_00008-6-3 loss: 0.969069  [  128/  130]
train() client id: f_00008-7-0 loss: 0.987509  [   32/  130]
train() client id: f_00008-7-1 loss: 1.043397  [   64/  130]
train() client id: f_00008-7-2 loss: 0.972388  [   96/  130]
train() client id: f_00008-7-3 loss: 1.022570  [  128/  130]
train() client id: f_00008-8-0 loss: 0.944877  [   32/  130]
train() client id: f_00008-8-1 loss: 1.077893  [   64/  130]
train() client id: f_00008-8-2 loss: 0.997527  [   96/  130]
train() client id: f_00008-8-3 loss: 1.013855  [  128/  130]
train() client id: f_00008-9-0 loss: 1.044672  [   32/  130]
train() client id: f_00008-9-1 loss: 0.997193  [   64/  130]
train() client id: f_00008-9-2 loss: 1.018123  [   96/  130]
train() client id: f_00008-9-3 loss: 0.913322  [  128/  130]
train() client id: f_00008-10-0 loss: 1.030282  [   32/  130]
train() client id: f_00008-10-1 loss: 0.947080  [   64/  130]
train() client id: f_00008-10-2 loss: 1.096138  [   96/  130]
train() client id: f_00008-10-3 loss: 0.969683  [  128/  130]
train() client id: f_00008-11-0 loss: 1.110878  [   32/  130]
train() client id: f_00008-11-1 loss: 0.991969  [   64/  130]
train() client id: f_00008-11-2 loss: 0.909246  [   96/  130]
train() client id: f_00008-11-3 loss: 1.034336  [  128/  130]
train() client id: f_00008-12-0 loss: 1.003746  [   32/  130]
train() client id: f_00008-12-1 loss: 1.058114  [   64/  130]
train() client id: f_00008-12-2 loss: 0.930508  [   96/  130]
train() client id: f_00008-12-3 loss: 1.079926  [  128/  130]
train() client id: f_00009-0-0 loss: 1.171458  [   32/  118]
train() client id: f_00009-0-1 loss: 1.155861  [   64/  118]
train() client id: f_00009-0-2 loss: 1.206218  [   96/  118]
train() client id: f_00009-1-0 loss: 1.122682  [   32/  118]
train() client id: f_00009-1-1 loss: 1.148668  [   64/  118]
train() client id: f_00009-1-2 loss: 1.274216  [   96/  118]
train() client id: f_00009-2-0 loss: 1.139673  [   32/  118]
train() client id: f_00009-2-1 loss: 1.121570  [   64/  118]
train() client id: f_00009-2-2 loss: 1.096948  [   96/  118]
train() client id: f_00009-3-0 loss: 1.154407  [   32/  118]
train() client id: f_00009-3-1 loss: 1.039425  [   64/  118]
train() client id: f_00009-3-2 loss: 1.152883  [   96/  118]
train() client id: f_00009-4-0 loss: 1.089675  [   32/  118]
train() client id: f_00009-4-1 loss: 1.128236  [   64/  118]
train() client id: f_00009-4-2 loss: 1.111980  [   96/  118]
train() client id: f_00009-5-0 loss: 1.103343  [   32/  118]
train() client id: f_00009-5-1 loss: 1.081057  [   64/  118]
train() client id: f_00009-5-2 loss: 1.045094  [   96/  118]
train() client id: f_00009-6-0 loss: 1.127494  [   32/  118]
train() client id: f_00009-6-1 loss: 1.020638  [   64/  118]
train() client id: f_00009-6-2 loss: 1.065841  [   96/  118]
train() client id: f_00009-7-0 loss: 0.981611  [   32/  118]
train() client id: f_00009-7-1 loss: 1.027613  [   64/  118]
train() client id: f_00009-7-2 loss: 1.049109  [   96/  118]
train() client id: f_00009-8-0 loss: 1.061251  [   32/  118]
train() client id: f_00009-8-1 loss: 1.032549  [   64/  118]
train() client id: f_00009-8-2 loss: 1.040761  [   96/  118]
train() client id: f_00009-9-0 loss: 1.059340  [   32/  118]
train() client id: f_00009-9-1 loss: 1.004555  [   64/  118]
train() client id: f_00009-9-2 loss: 0.937707  [   96/  118]
train() client id: f_00009-10-0 loss: 1.025883  [   32/  118]
train() client id: f_00009-10-1 loss: 1.047758  [   64/  118]
train() client id: f_00009-10-2 loss: 0.999401  [   96/  118]
train() client id: f_00009-11-0 loss: 0.929155  [   32/  118]
train() client id: f_00009-11-1 loss: 1.029411  [   64/  118]
train() client id: f_00009-11-2 loss: 1.090513  [   96/  118]
train() client id: f_00009-12-0 loss: 0.959791  [   32/  118]
train() client id: f_00009-12-1 loss: 1.146777  [   64/  118]
train() client id: f_00009-12-2 loss: 1.012120  [   96/  118]
At round 2 accuracy: 0.5384615384615384
At round 2 training accuracy: 0.4969818913480885
At round 2 training loss: 0.99154214607458
update_location
xs = [ -3.9056584   -5.79968212  30.00902392  18.81129433 -24.02070377
 -11.04359014   2.55680806  -6.32485185  14.66397685  -7.06087855]
ys = [ 22.5879595   15.55583871   1.32061395   7.54482414   9.35018685
 -17.18584926   2.37501568  -4.17765202  17.56900603   4.00148178]
dists_uav = [102.59371366 101.36873498 104.41401026 102.0332748  103.26867968
 102.0652453  100.06087131 100.28687116 102.58509731 100.32879877]
dists_bs = [229.13468488 232.48911402 268.68797708 256.13288296 224.13078434
 252.62047619 247.64047589 246.08115854 246.48923299 239.67486674]
uav_gains = [9.37984848e-11 9.66580426e-11 8.97635667e-11 9.50918527e-11
 9.22732701e-11 9.50174026e-11 9.98476140e-11 9.92860273e-11
 9.38181825e-11 9.91823280e-11]
bs_gains = [2.72295051e-11 2.61436814e-11 1.74340799e-11 1.99338479e-11
 2.89660886e-11 2.07196372e-11 2.19075354e-11 2.22984497e-11
 2.21952385e-11 2.40077301e-11]
Round 3
-------------------------------
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.59821531 22.14976829 10.43417884  3.73162013 25.53613015 12.31863725
  4.64005817 14.98083601 10.99100861  9.99386679]
obj_prev = 125.37431955255585
eta_min = 1.4920442712219943e-09	eta_max = 0.9182779578592448
af = 26.51569137072327	bf = 2.011304995713791	zeta = 29.1672605077956	eta = 0.9090909090909091
af = 26.51569137072327	bf = 2.011304995713791	zeta = 49.923723386827525	eta = 0.531124074325744
af = 26.51569137072327	bf = 2.011304995713791	zeta = 40.07321095605609	eta = 0.6616812263883753
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.31157890911957	eta = 0.692106462999664
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22584319738169	eta = 0.6936587699009733
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22562613755516	eta = 0.6936627087626083
eta = 0.6936627087626083
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.03018242 0.0634789  0.02970333 0.01030035 0.07330016 0.03497328
 0.01293532 0.0428782  0.03114059 0.02826607]
ene_total = [3.24068386 6.36286468 3.20278272 1.47684991 7.21615316 3.87841894
 1.70546401 4.36028936 3.52564504 3.25647446]
ti_comp = [0.27298014 0.25372925 0.27246494 0.27313894 0.25568174 0.24894535
 0.27369854 0.27363436 0.25041517 0.25203535]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.30610989e-05 2.48328765e-04 2.20634935e-05 9.15521600e-07
 3.76526319e-04 4.31400861e-05 1.80578817e-06 6.58033963e-05
 3.00981129e-05 2.22204868e-05]
ene_total = [0.56723758 0.75785384 0.57171611 0.56386684 0.75191015 0.7820715
 0.55898529 0.56522719 0.76788634 0.75282607]
optimize_network iter = 0 obj = 6.639580919785389
eta = 0.6936627087626083
freqs = [5.52831753e+07 1.25091797e+08 5.45085400e+07 1.88555117e+07
 1.43342577e+08 7.02428831e+07 2.36306014e+07 7.83494392e+07
 6.21779230e+07 5.60756135e+07]
eta_min = 0.6679443667432773	eta_max = 0.6936627087626027
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 0.06810536589768883	eta = 0.9090909090909091
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 22.189266299545174	eta = 0.002790266616394072
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.3821060289713065	eta = 0.025991273371083312
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969923954010767	eta = 0.026954363942109684
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969507848071644	eta = 0.02695485223602507
eta = 0.02695485223602507
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.27830406e-04 2.45334550e-03 2.17974637e-04 9.04482732e-06
 3.71986367e-03 4.26199261e-04 1.78401495e-05 6.50099743e-04
 2.97352060e-04 2.19525641e-04]
ene_total = [0.18429415 0.3001259  0.18545689 0.17774529 0.33002202 0.25690445
 0.17642907 0.19425277 0.24920683 0.24251343]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 1 obj = 6.13145379564688
eta = 0.6679443667432773
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
eta_min = 0.6679443667432772	eta_max = 0.6679443667432751
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 0.06726955400122223	eta = 0.9090909090909091
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 22.188469685723824	eta = 0.002756122475650405
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.378298126804526	eta = 0.025713403762074868
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.294165378733591	eta = 0.026656378205336263
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.2941251141107752	eta = 0.02665684605646063
eta = 0.02665684605646063
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.28418763e-04 2.42502502e-03 2.18460004e-04 9.06917459e-06
 3.68255925e-03 4.19660878e-04 1.78950352e-05 6.52071157e-04
 2.93141173e-04 2.16699410e-04]
ene_total = [0.18425414 0.2992439  0.18541365 0.17769155 0.32888024 0.25664338
 0.17637659 0.19424829 0.24901305 0.24236033]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 2 obj = 6.1314537956468795
eta = 0.6679443667432772
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
Done!
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.50181899e-05 2.65607500e-04 2.39274295e-05 9.93326158e-07
 4.03342377e-04 4.59645058e-05 1.96000269e-06 7.14198774e-05
 3.21070889e-05 2.37345958e-05]
ene_total = [0.00640101 0.00856669 0.00645144 0.0063611  0.00850917 0.00882543
 0.00630611 0.00638199 0.00866459 0.0084942 ]
At round 3 energy consumption: 0.07496174437238437
At round 3 eta: 0.6679443667432772
At round 3 a_n: 27.154965315346423
At round 3 local rounds: 13.214290123550223
At round 3 global rounds: 81.77836059884595
gradient difference: 0.3518173098564148
train() client id: f_00000-0-0 loss: 1.483913  [   32/  126]
train() client id: f_00000-0-1 loss: 1.281101  [   64/  126]
train() client id: f_00000-0-2 loss: 1.205866  [   96/  126]
train() client id: f_00000-1-0 loss: 1.228493  [   32/  126]
train() client id: f_00000-1-1 loss: 1.275845  [   64/  126]
train() client id: f_00000-1-2 loss: 1.275004  [   96/  126]
train() client id: f_00000-2-0 loss: 1.133293  [   32/  126]
train() client id: f_00000-2-1 loss: 1.151493  [   64/  126]
train() client id: f_00000-2-2 loss: 1.084592  [   96/  126]
train() client id: f_00000-3-0 loss: 1.045643  [   32/  126]
train() client id: f_00000-3-1 loss: 1.082051  [   64/  126]
train() client id: f_00000-3-2 loss: 1.031976  [   96/  126]
train() client id: f_00000-4-0 loss: 0.995851  [   32/  126]
train() client id: f_00000-4-1 loss: 0.976034  [   64/  126]
train() client id: f_00000-4-2 loss: 0.937044  [   96/  126]
train() client id: f_00000-5-0 loss: 0.946402  [   32/  126]
train() client id: f_00000-5-1 loss: 0.955995  [   64/  126]
train() client id: f_00000-5-2 loss: 0.887441  [   96/  126]
train() client id: f_00000-6-0 loss: 0.878913  [   32/  126]
train() client id: f_00000-6-1 loss: 0.922979  [   64/  126]
train() client id: f_00000-6-2 loss: 0.890356  [   96/  126]
train() client id: f_00000-7-0 loss: 0.882044  [   32/  126]
train() client id: f_00000-7-1 loss: 0.879895  [   64/  126]
train() client id: f_00000-7-2 loss: 0.817526  [   96/  126]
train() client id: f_00000-8-0 loss: 0.857741  [   32/  126]
train() client id: f_00000-8-1 loss: 0.863202  [   64/  126]
train() client id: f_00000-8-2 loss: 0.786849  [   96/  126]
train() client id: f_00000-9-0 loss: 0.785207  [   32/  126]
train() client id: f_00000-9-1 loss: 0.818938  [   64/  126]
train() client id: f_00000-9-2 loss: 0.868424  [   96/  126]
train() client id: f_00000-10-0 loss: 0.759373  [   32/  126]
train() client id: f_00000-10-1 loss: 0.898566  [   64/  126]
train() client id: f_00000-10-2 loss: 0.787948  [   96/  126]
train() client id: f_00000-11-0 loss: 0.782957  [   32/  126]
train() client id: f_00000-11-1 loss: 0.858426  [   64/  126]
train() client id: f_00000-11-2 loss: 0.758022  [   96/  126]
train() client id: f_00000-12-0 loss: 0.767419  [   32/  126]
train() client id: f_00000-12-1 loss: 0.691972  [   64/  126]
train() client id: f_00000-12-2 loss: 0.835728  [   96/  126]
train() client id: f_00001-0-0 loss: 0.716527  [   32/  265]
train() client id: f_00001-0-1 loss: 0.663002  [   64/  265]
train() client id: f_00001-0-2 loss: 0.694486  [   96/  265]
train() client id: f_00001-0-3 loss: 0.731724  [  128/  265]
train() client id: f_00001-0-4 loss: 0.689740  [  160/  265]
train() client id: f_00001-0-5 loss: 0.626083  [  192/  265]
train() client id: f_00001-0-6 loss: 0.674463  [  224/  265]
train() client id: f_00001-0-7 loss: 0.686029  [  256/  265]
train() client id: f_00001-1-0 loss: 0.608453  [   32/  265]
train() client id: f_00001-1-1 loss: 0.629267  [   64/  265]
train() client id: f_00001-1-2 loss: 0.616582  [   96/  265]
train() client id: f_00001-1-3 loss: 0.632644  [  128/  265]
train() client id: f_00001-1-4 loss: 0.741084  [  160/  265]
train() client id: f_00001-1-5 loss: 0.595288  [  192/  265]
train() client id: f_00001-1-6 loss: 0.566204  [  224/  265]
train() client id: f_00001-1-7 loss: 0.653500  [  256/  265]
train() client id: f_00001-2-0 loss: 0.592358  [   32/  265]
train() client id: f_00001-2-1 loss: 0.703143  [   64/  265]
train() client id: f_00001-2-2 loss: 0.588205  [   96/  265]
train() client id: f_00001-2-3 loss: 0.669273  [  128/  265]
train() client id: f_00001-2-4 loss: 0.553429  [  160/  265]
train() client id: f_00001-2-5 loss: 0.531072  [  192/  265]
train() client id: f_00001-2-6 loss: 0.550040  [  224/  265]
train() client id: f_00001-2-7 loss: 0.541908  [  256/  265]
train() client id: f_00001-3-0 loss: 0.510214  [   32/  265]
train() client id: f_00001-3-1 loss: 0.522389  [   64/  265]
train() client id: f_00001-3-2 loss: 0.625359  [   96/  265]
train() client id: f_00001-3-3 loss: 0.630769  [  128/  265]
train() client id: f_00001-3-4 loss: 0.542664  [  160/  265]
train() client id: f_00001-3-5 loss: 0.566096  [  192/  265]
train() client id: f_00001-3-6 loss: 0.557205  [  224/  265]
train() client id: f_00001-3-7 loss: 0.587265  [  256/  265]
train() client id: f_00001-4-0 loss: 0.509646  [   32/  265]
train() client id: f_00001-4-1 loss: 0.608464  [   64/  265]
train() client id: f_00001-4-2 loss: 0.508775  [   96/  265]
train() client id: f_00001-4-3 loss: 0.511881  [  128/  265]
train() client id: f_00001-4-4 loss: 0.542555  [  160/  265]
train() client id: f_00001-4-5 loss: 0.580436  [  192/  265]
train() client id: f_00001-4-6 loss: 0.573229  [  224/  265]
train() client id: f_00001-4-7 loss: 0.563423  [  256/  265]
train() client id: f_00001-5-0 loss: 0.511474  [   32/  265]
train() client id: f_00001-5-1 loss: 0.581806  [   64/  265]
train() client id: f_00001-5-2 loss: 0.540726  [   96/  265]
train() client id: f_00001-5-3 loss: 0.486239  [  128/  265]
train() client id: f_00001-5-4 loss: 0.667274  [  160/  265]
train() client id: f_00001-5-5 loss: 0.493648  [  192/  265]
train() client id: f_00001-5-6 loss: 0.494587  [  224/  265]
train() client id: f_00001-5-7 loss: 0.532922  [  256/  265]
train() client id: f_00001-6-0 loss: 0.578217  [   32/  265]
train() client id: f_00001-6-1 loss: 0.489070  [   64/  265]
train() client id: f_00001-6-2 loss: 0.575104  [   96/  265]
train() client id: f_00001-6-3 loss: 0.564291  [  128/  265]
train() client id: f_00001-6-4 loss: 0.462783  [  160/  265]
train() client id: f_00001-6-5 loss: 0.511593  [  192/  265]
train() client id: f_00001-6-6 loss: 0.455947  [  224/  265]
train() client id: f_00001-6-7 loss: 0.591897  [  256/  265]
train() client id: f_00001-7-0 loss: 0.595112  [   32/  265]
train() client id: f_00001-7-1 loss: 0.463475  [   64/  265]
train() client id: f_00001-7-2 loss: 0.502346  [   96/  265]
train() client id: f_00001-7-3 loss: 0.475766  [  128/  265]
train() client id: f_00001-7-4 loss: 0.608006  [  160/  265]
train() client id: f_00001-7-5 loss: 0.618108  [  192/  265]
train() client id: f_00001-7-6 loss: 0.455525  [  224/  265]
train() client id: f_00001-7-7 loss: 0.458005  [  256/  265]
train() client id: f_00001-8-0 loss: 0.516858  [   32/  265]
train() client id: f_00001-8-1 loss: 0.544762  [   64/  265]
train() client id: f_00001-8-2 loss: 0.533975  [   96/  265]
train() client id: f_00001-8-3 loss: 0.447859  [  128/  265]
train() client id: f_00001-8-4 loss: 0.523247  [  160/  265]
train() client id: f_00001-8-5 loss: 0.516551  [  192/  265]
train() client id: f_00001-8-6 loss: 0.573906  [  224/  265]
train() client id: f_00001-8-7 loss: 0.485177  [  256/  265]
train() client id: f_00001-9-0 loss: 0.446909  [   32/  265]
train() client id: f_00001-9-1 loss: 0.699959  [   64/  265]
train() client id: f_00001-9-2 loss: 0.497837  [   96/  265]
train() client id: f_00001-9-3 loss: 0.544917  [  128/  265]
train() client id: f_00001-9-4 loss: 0.426482  [  160/  265]
train() client id: f_00001-9-5 loss: 0.496372  [  192/  265]
train() client id: f_00001-9-6 loss: 0.548049  [  224/  265]
train() client id: f_00001-9-7 loss: 0.448456  [  256/  265]
train() client id: f_00001-10-0 loss: 0.534330  [   32/  265]
train() client id: f_00001-10-1 loss: 0.495106  [   64/  265]
train() client id: f_00001-10-2 loss: 0.468623  [   96/  265]
train() client id: f_00001-10-3 loss: 0.615103  [  128/  265]
train() client id: f_00001-10-4 loss: 0.485719  [  160/  265]
train() client id: f_00001-10-5 loss: 0.475795  [  192/  265]
train() client id: f_00001-10-6 loss: 0.497351  [  224/  265]
train() client id: f_00001-10-7 loss: 0.448147  [  256/  265]
train() client id: f_00001-11-0 loss: 0.489150  [   32/  265]
train() client id: f_00001-11-1 loss: 0.498639  [   64/  265]
train() client id: f_00001-11-2 loss: 0.537304  [   96/  265]
train() client id: f_00001-11-3 loss: 0.477833  [  128/  265]
train() client id: f_00001-11-4 loss: 0.630669  [  160/  265]
train() client id: f_00001-11-5 loss: 0.570326  [  192/  265]
train() client id: f_00001-11-6 loss: 0.448308  [  224/  265]
train() client id: f_00001-11-7 loss: 0.408470  [  256/  265]
train() client id: f_00001-12-0 loss: 0.476230  [   32/  265]
train() client id: f_00001-12-1 loss: 0.582567  [   64/  265]
train() client id: f_00001-12-2 loss: 0.420587  [   96/  265]
train() client id: f_00001-12-3 loss: 0.473694  [  128/  265]
train() client id: f_00001-12-4 loss: 0.632523  [  160/  265]
train() client id: f_00001-12-5 loss: 0.519477  [  192/  265]
train() client id: f_00001-12-6 loss: 0.445700  [  224/  265]
train() client id: f_00001-12-7 loss: 0.502219  [  256/  265]
train() client id: f_00002-0-0 loss: 1.062786  [   32/  124]
train() client id: f_00002-0-1 loss: 1.063843  [   64/  124]
train() client id: f_00002-0-2 loss: 0.983766  [   96/  124]
train() client id: f_00002-1-0 loss: 1.013640  [   32/  124]
train() client id: f_00002-1-1 loss: 1.069246  [   64/  124]
train() client id: f_00002-1-2 loss: 0.943631  [   96/  124]
train() client id: f_00002-2-0 loss: 0.993696  [   32/  124]
train() client id: f_00002-2-1 loss: 1.012375  [   64/  124]
train() client id: f_00002-2-2 loss: 0.973223  [   96/  124]
train() client id: f_00002-3-0 loss: 0.920915  [   32/  124]
train() client id: f_00002-3-1 loss: 0.977596  [   64/  124]
train() client id: f_00002-3-2 loss: 0.993480  [   96/  124]
train() client id: f_00002-4-0 loss: 0.973970  [   32/  124]
train() client id: f_00002-4-1 loss: 0.900955  [   64/  124]
train() client id: f_00002-4-2 loss: 0.981626  [   96/  124]
train() client id: f_00002-5-0 loss: 0.887585  [   32/  124]
train() client id: f_00002-5-1 loss: 0.982725  [   64/  124]
train() client id: f_00002-5-2 loss: 0.943228  [   96/  124]
train() client id: f_00002-6-0 loss: 0.973698  [   32/  124]
train() client id: f_00002-6-1 loss: 0.879759  [   64/  124]
train() client id: f_00002-6-2 loss: 0.899935  [   96/  124]
train() client id: f_00002-7-0 loss: 1.008769  [   32/  124]
train() client id: f_00002-7-1 loss: 0.905875  [   64/  124]
train() client id: f_00002-7-2 loss: 0.765052  [   96/  124]
train() client id: f_00002-8-0 loss: 0.985234  [   32/  124]
train() client id: f_00002-8-1 loss: 0.794829  [   64/  124]
train() client id: f_00002-8-2 loss: 0.888029  [   96/  124]
train() client id: f_00002-9-0 loss: 0.885804  [   32/  124]
train() client id: f_00002-9-1 loss: 0.888280  [   64/  124]
train() client id: f_00002-9-2 loss: 0.854116  [   96/  124]
train() client id: f_00002-10-0 loss: 0.960425  [   32/  124]
train() client id: f_00002-10-1 loss: 0.893765  [   64/  124]
train() client id: f_00002-10-2 loss: 0.938289  [   96/  124]
train() client id: f_00002-11-0 loss: 0.834051  [   32/  124]
train() client id: f_00002-11-1 loss: 0.884154  [   64/  124]
train() client id: f_00002-11-2 loss: 0.846648  [   96/  124]
train() client id: f_00002-12-0 loss: 0.873290  [   32/  124]
train() client id: f_00002-12-1 loss: 0.904689  [   64/  124]
train() client id: f_00002-12-2 loss: 0.878610  [   96/  124]
train() client id: f_00003-0-0 loss: 1.074345  [   32/   43]
train() client id: f_00003-1-0 loss: 1.071166  [   32/   43]
train() client id: f_00003-2-0 loss: 1.041294  [   32/   43]
train() client id: f_00003-3-0 loss: 1.133906  [   32/   43]
train() client id: f_00003-4-0 loss: 1.085241  [   32/   43]
train() client id: f_00003-5-0 loss: 1.122637  [   32/   43]
train() client id: f_00003-6-0 loss: 1.060705  [   32/   43]
train() client id: f_00003-7-0 loss: 1.086754  [   32/   43]
train() client id: f_00003-8-0 loss: 1.059804  [   32/   43]
train() client id: f_00003-9-0 loss: 1.078434  [   32/   43]
train() client id: f_00003-10-0 loss: 1.084333  [   32/   43]
train() client id: f_00003-11-0 loss: 1.119771  [   32/   43]
train() client id: f_00003-12-0 loss: 1.066203  [   32/   43]
train() client id: f_00004-0-0 loss: 1.083198  [   32/  306]
train() client id: f_00004-0-1 loss: 1.027408  [   64/  306]
train() client id: f_00004-0-2 loss: 1.030408  [   96/  306]
train() client id: f_00004-0-3 loss: 1.042118  [  128/  306]
train() client id: f_00004-0-4 loss: 1.072997  [  160/  306]
train() client id: f_00004-0-5 loss: 1.008474  [  192/  306]
train() client id: f_00004-0-6 loss: 1.084740  [  224/  306]
train() client id: f_00004-0-7 loss: 1.092805  [  256/  306]
train() client id: f_00004-0-8 loss: 1.072337  [  288/  306]
train() client id: f_00004-1-0 loss: 1.061478  [   32/  306]
train() client id: f_00004-1-1 loss: 1.017932  [   64/  306]
train() client id: f_00004-1-2 loss: 1.066026  [   96/  306]
train() client id: f_00004-1-3 loss: 1.033384  [  128/  306]
train() client id: f_00004-1-4 loss: 1.044014  [  160/  306]
train() client id: f_00004-1-5 loss: 1.047091  [  192/  306]
train() client id: f_00004-1-6 loss: 1.019990  [  224/  306]
train() client id: f_00004-1-7 loss: 1.074955  [  256/  306]
train() client id: f_00004-1-8 loss: 1.028423  [  288/  306]
train() client id: f_00004-2-0 loss: 1.024307  [   32/  306]
train() client id: f_00004-2-1 loss: 1.066627  [   64/  306]
train() client id: f_00004-2-2 loss: 1.034093  [   96/  306]
train() client id: f_00004-2-3 loss: 1.071780  [  128/  306]
train() client id: f_00004-2-4 loss: 1.120726  [  160/  306]
train() client id: f_00004-2-5 loss: 0.919243  [  192/  306]
train() client id: f_00004-2-6 loss: 0.989652  [  224/  306]
train() client id: f_00004-2-7 loss: 0.980281  [  256/  306]
train() client id: f_00004-2-8 loss: 1.029897  [  288/  306]
train() client id: f_00004-3-0 loss: 1.008952  [   32/  306]
train() client id: f_00004-3-1 loss: 1.022372  [   64/  306]
train() client id: f_00004-3-2 loss: 1.069084  [   96/  306]
train() client id: f_00004-3-3 loss: 1.000911  [  128/  306]
train() client id: f_00004-3-4 loss: 1.031574  [  160/  306]
train() client id: f_00004-3-5 loss: 0.993364  [  192/  306]
train() client id: f_00004-3-6 loss: 0.991767  [  224/  306]
train() client id: f_00004-3-7 loss: 1.077384  [  256/  306]
train() client id: f_00004-3-8 loss: 1.020355  [  288/  306]
train() client id: f_00004-4-0 loss: 1.040698  [   32/  306]
train() client id: f_00004-4-1 loss: 1.036816  [   64/  306]
train() client id: f_00004-4-2 loss: 1.040723  [   96/  306]
train() client id: f_00004-4-3 loss: 1.044912  [  128/  306]
train() client id: f_00004-4-4 loss: 1.006271  [  160/  306]
train() client id: f_00004-4-5 loss: 1.040428  [  192/  306]
train() client id: f_00004-4-6 loss: 0.977660  [  224/  306]
train() client id: f_00004-4-7 loss: 0.998429  [  256/  306]
train() client id: f_00004-4-8 loss: 0.972049  [  288/  306]
train() client id: f_00004-5-0 loss: 0.960863  [   32/  306]
train() client id: f_00004-5-1 loss: 1.093176  [   64/  306]
train() client id: f_00004-5-2 loss: 0.922522  [   96/  306]
train() client id: f_00004-5-3 loss: 1.078648  [  128/  306]
train() client id: f_00004-5-4 loss: 0.978902  [  160/  306]
train() client id: f_00004-5-5 loss: 1.015243  [  192/  306]
train() client id: f_00004-5-6 loss: 1.044784  [  224/  306]
train() client id: f_00004-5-7 loss: 1.045643  [  256/  306]
train() client id: f_00004-5-8 loss: 1.048624  [  288/  306]
train() client id: f_00004-6-0 loss: 1.001399  [   32/  306]
train() client id: f_00004-6-1 loss: 1.065922  [   64/  306]
train() client id: f_00004-6-2 loss: 0.952456  [   96/  306]
train() client id: f_00004-6-3 loss: 1.047315  [  128/  306]
train() client id: f_00004-6-4 loss: 1.039686  [  160/  306]
train() client id: f_00004-6-5 loss: 1.057415  [  192/  306]
train() client id: f_00004-6-6 loss: 0.980658  [  224/  306]
train() client id: f_00004-6-7 loss: 0.994576  [  256/  306]
train() client id: f_00004-6-8 loss: 1.039238  [  288/  306]
train() client id: f_00004-7-0 loss: 1.058559  [   32/  306]
train() client id: f_00004-7-1 loss: 1.043495  [   64/  306]
train() client id: f_00004-7-2 loss: 1.010785  [   96/  306]
train() client id: f_00004-7-3 loss: 0.955209  [  128/  306]
train() client id: f_00004-7-4 loss: 1.011841  [  160/  306]
train() client id: f_00004-7-5 loss: 1.010663  [  192/  306]
train() client id: f_00004-7-6 loss: 1.019618  [  224/  306]
train() client id: f_00004-7-7 loss: 0.951425  [  256/  306]
train() client id: f_00004-7-8 loss: 1.054751  [  288/  306]
train() client id: f_00004-8-0 loss: 1.016945  [   32/  306]
train() client id: f_00004-8-1 loss: 1.026943  [   64/  306]
train() client id: f_00004-8-2 loss: 1.067478  [   96/  306]
train() client id: f_00004-8-3 loss: 1.041929  [  128/  306]
train() client id: f_00004-8-4 loss: 1.034761  [  160/  306]
train() client id: f_00004-8-5 loss: 1.037593  [  192/  306]
train() client id: f_00004-8-6 loss: 1.037947  [  224/  306]
train() client id: f_00004-8-7 loss: 0.970227  [  256/  306]
train() client id: f_00004-8-8 loss: 0.985647  [  288/  306]
train() client id: f_00004-9-0 loss: 0.984032  [   32/  306]
train() client id: f_00004-9-1 loss: 0.976186  [   64/  306]
train() client id: f_00004-9-2 loss: 1.002723  [   96/  306]
train() client id: f_00004-9-3 loss: 1.063977  [  128/  306]
train() client id: f_00004-9-4 loss: 1.046548  [  160/  306]
train() client id: f_00004-9-5 loss: 1.004639  [  192/  306]
train() client id: f_00004-9-6 loss: 1.063176  [  224/  306]
train() client id: f_00004-9-7 loss: 1.011417  [  256/  306]
train() client id: f_00004-9-8 loss: 0.940091  [  288/  306]
train() client id: f_00004-10-0 loss: 0.968648  [   32/  306]
train() client id: f_00004-10-1 loss: 1.062235  [   64/  306]
train() client id: f_00004-10-2 loss: 1.125295  [   96/  306]
train() client id: f_00004-10-3 loss: 1.024933  [  128/  306]
train() client id: f_00004-10-4 loss: 0.918924  [  160/  306]
train() client id: f_00004-10-5 loss: 0.971331  [  192/  306]
train() client id: f_00004-10-6 loss: 0.999487  [  224/  306]
train() client id: f_00004-10-7 loss: 1.003291  [  256/  306]
train() client id: f_00004-10-8 loss: 1.056440  [  288/  306]
train() client id: f_00004-11-0 loss: 0.957465  [   32/  306]
train() client id: f_00004-11-1 loss: 1.002701  [   64/  306]
train() client id: f_00004-11-2 loss: 1.002336  [   96/  306]
train() client id: f_00004-11-3 loss: 1.007339  [  128/  306]
train() client id: f_00004-11-4 loss: 1.084007  [  160/  306]
train() client id: f_00004-11-5 loss: 0.973929  [  192/  306]
train() client id: f_00004-11-6 loss: 1.016164  [  224/  306]
train() client id: f_00004-11-7 loss: 1.016020  [  256/  306]
train() client id: f_00004-11-8 loss: 1.037086  [  288/  306]
train() client id: f_00004-12-0 loss: 0.972520  [   32/  306]
train() client id: f_00004-12-1 loss: 1.107660  [   64/  306]
train() client id: f_00004-12-2 loss: 0.944021  [   96/  306]
train() client id: f_00004-12-3 loss: 0.988262  [  128/  306]
train() client id: f_00004-12-4 loss: 1.043976  [  160/  306]
train() client id: f_00004-12-5 loss: 1.122595  [  192/  306]
train() client id: f_00004-12-6 loss: 1.057219  [  224/  306]
train() client id: f_00004-12-7 loss: 1.009124  [  256/  306]
train() client id: f_00004-12-8 loss: 0.935445  [  288/  306]
train() client id: f_00005-0-0 loss: 0.981708  [   32/  146]
train() client id: f_00005-0-1 loss: 1.046853  [   64/  146]
train() client id: f_00005-0-2 loss: 0.943691  [   96/  146]
train() client id: f_00005-0-3 loss: 0.936373  [  128/  146]
train() client id: f_00005-1-0 loss: 0.939287  [   32/  146]
train() client id: f_00005-1-1 loss: 0.988556  [   64/  146]
train() client id: f_00005-1-2 loss: 0.885505  [   96/  146]
train() client id: f_00005-1-3 loss: 0.962522  [  128/  146]
train() client id: f_00005-2-0 loss: 0.912378  [   32/  146]
train() client id: f_00005-2-1 loss: 0.894763  [   64/  146]
train() client id: f_00005-2-2 loss: 0.917314  [   96/  146]
train() client id: f_00005-2-3 loss: 0.920352  [  128/  146]
train() client id: f_00005-3-0 loss: 0.919267  [   32/  146]
train() client id: f_00005-3-1 loss: 0.892673  [   64/  146]
train() client id: f_00005-3-2 loss: 0.892798  [   96/  146]
train() client id: f_00005-3-3 loss: 0.878727  [  128/  146]
train() client id: f_00005-4-0 loss: 0.877789  [   32/  146]
train() client id: f_00005-4-1 loss: 0.897010  [   64/  146]
train() client id: f_00005-4-2 loss: 0.858373  [   96/  146]
train() client id: f_00005-4-3 loss: 0.856762  [  128/  146]
train() client id: f_00005-5-0 loss: 0.871003  [   32/  146]
train() client id: f_00005-5-1 loss: 0.942197  [   64/  146]
train() client id: f_00005-5-2 loss: 0.826745  [   96/  146]
train() client id: f_00005-5-3 loss: 0.802907  [  128/  146]
train() client id: f_00005-6-0 loss: 0.773087  [   32/  146]
train() client id: f_00005-6-1 loss: 0.948739  [   64/  146]
train() client id: f_00005-6-2 loss: 0.854882  [   96/  146]
train() client id: f_00005-6-3 loss: 0.737281  [  128/  146]
train() client id: f_00005-7-0 loss: 0.829328  [   32/  146]
train() client id: f_00005-7-1 loss: 0.855365  [   64/  146]
train() client id: f_00005-7-2 loss: 0.852852  [   96/  146]
train() client id: f_00005-7-3 loss: 0.805439  [  128/  146]
train() client id: f_00005-8-0 loss: 0.964284  [   32/  146]
train() client id: f_00005-8-1 loss: 0.797904  [   64/  146]
train() client id: f_00005-8-2 loss: 0.753659  [   96/  146]
train() client id: f_00005-8-3 loss: 0.846452  [  128/  146]
train() client id: f_00005-9-0 loss: 0.798720  [   32/  146]
train() client id: f_00005-9-1 loss: 0.763002  [   64/  146]
train() client id: f_00005-9-2 loss: 0.784206  [   96/  146]
train() client id: f_00005-9-3 loss: 0.884979  [  128/  146]
train() client id: f_00005-10-0 loss: 0.798235  [   32/  146]
train() client id: f_00005-10-1 loss: 0.904581  [   64/  146]
train() client id: f_00005-10-2 loss: 0.864676  [   96/  146]
train() client id: f_00005-10-3 loss: 0.699253  [  128/  146]
train() client id: f_00005-11-0 loss: 0.757565  [   32/  146]
train() client id: f_00005-11-1 loss: 0.685443  [   64/  146]
train() client id: f_00005-11-2 loss: 0.795917  [   96/  146]
train() client id: f_00005-11-3 loss: 0.906913  [  128/  146]
train() client id: f_00005-12-0 loss: 0.654342  [   32/  146]
train() client id: f_00005-12-1 loss: 0.937190  [   64/  146]
train() client id: f_00005-12-2 loss: 0.807925  [   96/  146]
train() client id: f_00005-12-3 loss: 0.787102  [  128/  146]
train() client id: f_00006-0-0 loss: 1.019919  [   32/   54]
train() client id: f_00006-1-0 loss: 0.993402  [   32/   54]
train() client id: f_00006-2-0 loss: 0.980673  [   32/   54]
train() client id: f_00006-3-0 loss: 0.986352  [   32/   54]
train() client id: f_00006-4-0 loss: 0.977505  [   32/   54]
train() client id: f_00006-5-0 loss: 1.037647  [   32/   54]
train() client id: f_00006-6-0 loss: 0.993445  [   32/   54]
train() client id: f_00006-7-0 loss: 1.036191  [   32/   54]
train() client id: f_00006-8-0 loss: 1.003617  [   32/   54]
train() client id: f_00006-9-0 loss: 1.033772  [   32/   54]
train() client id: f_00006-10-0 loss: 0.992271  [   32/   54]
train() client id: f_00006-11-0 loss: 1.029119  [   32/   54]
train() client id: f_00006-12-0 loss: 0.974933  [   32/   54]
train() client id: f_00007-0-0 loss: 0.999090  [   32/  179]
train() client id: f_00007-0-1 loss: 1.031652  [   64/  179]
train() client id: f_00007-0-2 loss: 0.993738  [   96/  179]
train() client id: f_00007-0-3 loss: 0.985647  [  128/  179]
train() client id: f_00007-0-4 loss: 0.972379  [  160/  179]
train() client id: f_00007-1-0 loss: 0.931896  [   32/  179]
train() client id: f_00007-1-1 loss: 0.933923  [   64/  179]
train() client id: f_00007-1-2 loss: 1.006567  [   96/  179]
train() client id: f_00007-1-3 loss: 0.946857  [  128/  179]
train() client id: f_00007-1-4 loss: 0.926303  [  160/  179]
train() client id: f_00007-2-0 loss: 0.956267  [   32/  179]
train() client id: f_00007-2-1 loss: 0.860715  [   64/  179]
train() client id: f_00007-2-2 loss: 0.853617  [   96/  179]
train() client id: f_00007-2-3 loss: 0.930146  [  128/  179]
train() client id: f_00007-2-4 loss: 0.951637  [  160/  179]
train() client id: f_00007-3-0 loss: 0.875646  [   32/  179]
train() client id: f_00007-3-1 loss: 0.863096  [   64/  179]
train() client id: f_00007-3-2 loss: 0.884072  [   96/  179]
train() client id: f_00007-3-3 loss: 0.873099  [  128/  179]
train() client id: f_00007-3-4 loss: 0.887565  [  160/  179]
train() client id: f_00007-4-0 loss: 0.823735  [   32/  179]
train() client id: f_00007-4-1 loss: 0.830790  [   64/  179]
train() client id: f_00007-4-2 loss: 0.873905  [   96/  179]
train() client id: f_00007-4-3 loss: 0.887177  [  128/  179]
train() client id: f_00007-4-4 loss: 0.827451  [  160/  179]
train() client id: f_00007-5-0 loss: 0.862098  [   32/  179]
train() client id: f_00007-5-1 loss: 0.912906  [   64/  179]
train() client id: f_00007-5-2 loss: 0.833280  [   96/  179]
train() client id: f_00007-5-3 loss: 0.872869  [  128/  179]
train() client id: f_00007-5-4 loss: 0.777192  [  160/  179]
train() client id: f_00007-6-0 loss: 0.782947  [   32/  179]
train() client id: f_00007-6-1 loss: 0.780757  [   64/  179]
train() client id: f_00007-6-2 loss: 0.770076  [   96/  179]
train() client id: f_00007-6-3 loss: 0.910943  [  128/  179]
train() client id: f_00007-6-4 loss: 0.910196  [  160/  179]
train() client id: f_00007-7-0 loss: 0.809942  [   32/  179]
train() client id: f_00007-7-1 loss: 0.815085  [   64/  179]
train() client id: f_00007-7-2 loss: 0.745662  [   96/  179]
train() client id: f_00007-7-3 loss: 0.817622  [  128/  179]
train() client id: f_00007-7-4 loss: 0.896591  [  160/  179]
train() client id: f_00007-8-0 loss: 0.799872  [   32/  179]
train() client id: f_00007-8-1 loss: 0.865501  [   64/  179]
train() client id: f_00007-8-2 loss: 0.788847  [   96/  179]
train() client id: f_00007-8-3 loss: 0.819564  [  128/  179]
train() client id: f_00007-8-4 loss: 0.831966  [  160/  179]
train() client id: f_00007-9-0 loss: 0.744027  [   32/  179]
train() client id: f_00007-9-1 loss: 0.813206  [   64/  179]
train() client id: f_00007-9-2 loss: 0.812772  [   96/  179]
train() client id: f_00007-9-3 loss: 0.899715  [  128/  179]
train() client id: f_00007-9-4 loss: 0.844356  [  160/  179]
train() client id: f_00007-10-0 loss: 0.836520  [   32/  179]
train() client id: f_00007-10-1 loss: 0.759608  [   64/  179]
train() client id: f_00007-10-2 loss: 0.949927  [   96/  179]
train() client id: f_00007-10-3 loss: 0.843405  [  128/  179]
train() client id: f_00007-10-4 loss: 0.765500  [  160/  179]
train() client id: f_00007-11-0 loss: 0.828370  [   32/  179]
train() client id: f_00007-11-1 loss: 0.903248  [   64/  179]
train() client id: f_00007-11-2 loss: 0.878118  [   96/  179]
train() client id: f_00007-11-3 loss: 0.762891  [  128/  179]
train() client id: f_00007-11-4 loss: 0.795884  [  160/  179]
train() client id: f_00007-12-0 loss: 0.825632  [   32/  179]
train() client id: f_00007-12-1 loss: 0.848285  [   64/  179]
train() client id: f_00007-12-2 loss: 0.788911  [   96/  179]
train() client id: f_00007-12-3 loss: 0.856422  [  128/  179]
train() client id: f_00007-12-4 loss: 0.838886  [  160/  179]
train() client id: f_00008-0-0 loss: 0.829510  [   32/  130]
train() client id: f_00008-0-1 loss: 0.901847  [   64/  130]
train() client id: f_00008-0-2 loss: 0.829719  [   96/  130]
train() client id: f_00008-0-3 loss: 0.927723  [  128/  130]
train() client id: f_00008-1-0 loss: 0.866931  [   32/  130]
train() client id: f_00008-1-1 loss: 0.867708  [   64/  130]
train() client id: f_00008-1-2 loss: 0.792076  [   96/  130]
train() client id: f_00008-1-3 loss: 0.934995  [  128/  130]
train() client id: f_00008-2-0 loss: 0.872307  [   32/  130]
train() client id: f_00008-2-1 loss: 0.858300  [   64/  130]
train() client id: f_00008-2-2 loss: 0.953841  [   96/  130]
train() client id: f_00008-2-3 loss: 0.747457  [  128/  130]
train() client id: f_00008-3-0 loss: 0.848726  [   32/  130]
train() client id: f_00008-3-1 loss: 0.853871  [   64/  130]
train() client id: f_00008-3-2 loss: 0.913383  [   96/  130]
train() client id: f_00008-3-3 loss: 0.798812  [  128/  130]
train() client id: f_00008-4-0 loss: 0.793052  [   32/  130]
train() client id: f_00008-4-1 loss: 0.786333  [   64/  130]
train() client id: f_00008-4-2 loss: 0.971833  [   96/  130]
train() client id: f_00008-4-3 loss: 0.846000  [  128/  130]
train() client id: f_00008-5-0 loss: 0.752983  [   32/  130]
train() client id: f_00008-5-1 loss: 0.864435  [   64/  130]
train() client id: f_00008-5-2 loss: 0.795198  [   96/  130]
train() client id: f_00008-5-3 loss: 0.968653  [  128/  130]
train() client id: f_00008-6-0 loss: 0.766677  [   32/  130]
train() client id: f_00008-6-1 loss: 0.878128  [   64/  130]
train() client id: f_00008-6-2 loss: 0.852718  [   96/  130]
train() client id: f_00008-6-3 loss: 0.847484  [  128/  130]
train() client id: f_00008-7-0 loss: 0.828918  [   32/  130]
train() client id: f_00008-7-1 loss: 0.933764  [   64/  130]
train() client id: f_00008-7-2 loss: 0.778676  [   96/  130]
train() client id: f_00008-7-3 loss: 0.827005  [  128/  130]
train() client id: f_00008-8-0 loss: 0.922656  [   32/  130]
train() client id: f_00008-8-1 loss: 0.883522  [   64/  130]
train() client id: f_00008-8-2 loss: 0.677258  [   96/  130]
train() client id: f_00008-8-3 loss: 0.870227  [  128/  130]
train() client id: f_00008-9-0 loss: 0.874645  [   32/  130]
train() client id: f_00008-9-1 loss: 0.763987  [   64/  130]
train() client id: f_00008-9-2 loss: 0.871915  [   96/  130]
train() client id: f_00008-9-3 loss: 0.804013  [  128/  130]
train() client id: f_00008-10-0 loss: 0.859063  [   32/  130]
train() client id: f_00008-10-1 loss: 0.846935  [   64/  130]
train() client id: f_00008-10-2 loss: 0.862065  [   96/  130]
train() client id: f_00008-10-3 loss: 0.773988  [  128/  130]
train() client id: f_00008-11-0 loss: 0.824295  [   32/  130]
train() client id: f_00008-11-1 loss: 0.823672  [   64/  130]
train() client id: f_00008-11-2 loss: 0.808919  [   96/  130]
train() client id: f_00008-11-3 loss: 0.873406  [  128/  130]
train() client id: f_00008-12-0 loss: 0.798850  [   32/  130]
train() client id: f_00008-12-1 loss: 0.769700  [   64/  130]
train() client id: f_00008-12-2 loss: 0.847659  [   96/  130]
train() client id: f_00008-12-3 loss: 0.927307  [  128/  130]
train() client id: f_00009-0-0 loss: 1.231094  [   32/  118]
train() client id: f_00009-0-1 loss: 1.196801  [   64/  118]
train() client id: f_00009-0-2 loss: 1.267589  [   96/  118]
train() client id: f_00009-1-0 loss: 1.147738  [   32/  118]
train() client id: f_00009-1-1 loss: 1.205984  [   64/  118]
train() client id: f_00009-1-2 loss: 1.161837  [   96/  118]
train() client id: f_00009-2-0 loss: 1.155662  [   32/  118]
train() client id: f_00009-2-1 loss: 1.100126  [   64/  118]
train() client id: f_00009-2-2 loss: 1.196939  [   96/  118]
train() client id: f_00009-3-0 loss: 1.127601  [   32/  118]
train() client id: f_00009-3-1 loss: 1.098248  [   64/  118]
train() client id: f_00009-3-2 loss: 1.176259  [   96/  118]
train() client id: f_00009-4-0 loss: 1.088042  [   32/  118]
train() client id: f_00009-4-1 loss: 1.137137  [   64/  118]
train() client id: f_00009-4-2 loss: 1.038492  [   96/  118]
train() client id: f_00009-5-0 loss: 1.037671  [   32/  118]
train() client id: f_00009-5-1 loss: 1.082895  [   64/  118]
train() client id: f_00009-5-2 loss: 1.065111  [   96/  118]
train() client id: f_00009-6-0 loss: 1.116834  [   32/  118]
train() client id: f_00009-6-1 loss: 1.003012  [   64/  118]
train() client id: f_00009-6-2 loss: 1.074784  [   96/  118]
train() client id: f_00009-7-0 loss: 0.982585  [   32/  118]
train() client id: f_00009-7-1 loss: 1.090703  [   64/  118]
train() client id: f_00009-7-2 loss: 1.029279  [   96/  118]
train() client id: f_00009-8-0 loss: 1.035032  [   32/  118]
train() client id: f_00009-8-1 loss: 1.008090  [   64/  118]
train() client id: f_00009-8-2 loss: 1.022136  [   96/  118]
train() client id: f_00009-9-0 loss: 0.937425  [   32/  118]
train() client id: f_00009-9-1 loss: 0.931033  [   64/  118]
train() client id: f_00009-9-2 loss: 1.033647  [   96/  118]
train() client id: f_00009-10-0 loss: 1.056098  [   32/  118]
train() client id: f_00009-10-1 loss: 1.030011  [   64/  118]
train() client id: f_00009-10-2 loss: 0.852945  [   96/  118]
train() client id: f_00009-11-0 loss: 0.997078  [   32/  118]
train() client id: f_00009-11-1 loss: 0.906830  [   64/  118]
train() client id: f_00009-11-2 loss: 0.968882  [   96/  118]
train() client id: f_00009-12-0 loss: 0.923211  [   32/  118]
train() client id: f_00009-12-1 loss: 1.025386  [   64/  118]
train() client id: f_00009-12-2 loss: 1.119934  [   96/  118]
At round 3 accuracy: 0.6180371352785146
At round 3 training accuracy: 0.5539906103286385
At round 3 training loss: 0.9498345584057066
update_location
xs = [ -3.9056584   -0.79968212  35.00902392  18.81129433 -19.02070377
  -6.04359014   2.55680806  -6.32485185  19.66397685  -2.06087855]
ys = [ 27.5879595   15.55583871   1.32061395   2.54482414   9.35018685
 -17.18584926  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [103.80919842 101.2058477  105.95931189 101.78575993 102.2213929
 101.64584791 100.06711653 100.2031936  103.41828638 100.10124413]
dists_bs = [225.83972948 236.15289818 272.52214454 259.4293844  227.52890245
 255.89386294 251.15145857 242.46476046 250.35690873 243.20450851]
uav_gains = [9.10767832e-11 9.70474414e-11 8.65263782e-11 9.56710163e-11
 9.46549499e-11 9.60005847e-11 9.98320356e-11 9.94934409e-11
 9.19399105e-11 9.97469659e-11]
bs_gains = [2.83565323e-11 2.50237793e-11 1.67559498e-11 1.92327073e-11
 2.77710106e-11 1.99860260e-11 2.10607635e-11 2.32422371e-11
 2.12484502e-11 2.30448319e-11]
Round 4
-------------------------------
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.46545628 21.87269455 10.30373985  3.68435765 25.21614886 12.16538239
  4.58153103 14.79179615 10.85487015  9.87006751]
obj_prev = 123.8060444419656
eta_min = 1.1887003308068872e-09	eta_max = 0.9184674772800265
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 28.799330597431425	eta = 0.9090909090909091
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 49.438331310801296	eta = 0.5295730850913346
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 39.62791236585137	eta = 0.660675974861339
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.87221877758381	eta = 0.6913038232004766
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78643716203915	eta = 0.6928732000256066
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78621816141451	eta = 0.6928772157665561
eta = 0.6928772157665561
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.03027591 0.06367553 0.02979534 0.01033226 0.07352722 0.03508161
 0.01297539 0.04301102 0.03123705 0.02835363]
ene_total = [3.202787   6.29122282 3.16613857 1.4566637  7.13363507 3.83605127
 1.68315721 4.30566359 3.48877145 3.22212748]
ti_comp = [0.2772398  0.25747119 0.276632   0.27781288 0.25949396 0.25275957
 0.27830054 0.2782619  0.25409312 0.25580165]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.25663118e-05 2.43411145e-04 2.16033125e-05 8.93224701e-07
 3.68952017e-04 4.22380117e-05 1.76284161e-06 6.42261255e-05
 2.95055929e-05 2.17720570e-05]
ene_total = [0.56161072 0.75347478 0.56683292 0.55471554 0.74677556 0.77704532
 0.5505341  0.55632463 0.76429157 0.74870068]
optimize_network iter = 0 obj = 6.580305820795663
eta = 0.6928772157665561
freqs = [5.46023917e+07 1.23655643e+08 5.38537534e+07 1.85957107e+07
 1.41674235e+08 6.93972005e+07 2.33118325e+07 7.72851390e+07
 6.14677254e+07 5.54211297e+07]
eta_min = 0.6721669962857192	eta_max = 0.692877215766553
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 0.06566942832741904	eta = 0.9090909090909091
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 22.05222718616278	eta = 0.002707185981428379
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.358376629903991	eta = 0.02531380252868433
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276155607764052	eta = 0.02622820693542063
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276117460225635	eta = 0.02622864651797696
eta = 0.02622864651797696
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.23933498e-04 2.41545493e-03 2.14377315e-04 8.86378481e-06
 3.66124143e-03 4.19142738e-04 1.74933012e-05 6.37338573e-04
 2.92794440e-04 2.16051827e-04]
ene_total = [0.18267388 0.2974487  0.18408433 0.17517407 0.32618133 0.25545427
 0.17406893 0.19124253 0.24830343 0.241486  ]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 1 obj = 6.169387607456715
eta = 0.6721669962857192
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
eta_min = 0.6721669962857293	eta_max = 0.6721669962857161
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 0.06499207998561081	eta = 0.909090909090909
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 22.05158160424028	eta = 0.0026793411075088985
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.3552731564287246	eta = 0.02508571412048698
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.273849552272878	eta = 0.0259840010165886
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.2738124460945865	eta = 0.025984425047592612
eta = 0.025984425047592612
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.24377690e-04 2.39200478e-03 2.14730208e-04 8.88417650e-06
 3.63033979e-03 4.13806558e-04 1.75382481e-05 6.38962582e-04
 2.89320240e-04 2.13725309e-04]
ene_total = [0.1826403  0.29672859 0.18404788 0.17513071 0.32524889 0.25524333
 0.17402652 0.19123929 0.24814553 0.24136141]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 2 obj = 6.169387607456903
eta = 0.6721669962857293
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
Done!
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44035723e-05 2.60157155e-04 2.33543012e-05 9.66253122e-07
 3.94839877e-04 4.50060710e-05 1.90747978e-06 6.94942958e-05
 3.14667978e-05 2.32450072e-05]
ene_total = [0.00643481 0.00864742 0.00649454 0.00635406 0.00857983 0.00890343
 0.00630624 0.00637769 0.00875654 0.00857746]
At round 4 energy consumption: 0.07543200489545869
At round 4 eta: 0.6721669962857293
At round 4 a_n: 26.812419468377104
At round 4 local rounds: 13.007932941342311
At round 4 global rounds: 81.78682184099438
gradient difference: 0.3636344373226166
train() client id: f_00000-0-0 loss: 1.419709  [   32/  126]
train() client id: f_00000-0-1 loss: 1.234801  [   64/  126]
train() client id: f_00000-0-2 loss: 1.317433  [   96/  126]
train() client id: f_00000-1-0 loss: 1.177458  [   32/  126]
train() client id: f_00000-1-1 loss: 1.297187  [   64/  126]
train() client id: f_00000-1-2 loss: 1.195412  [   96/  126]
train() client id: f_00000-2-0 loss: 1.143954  [   32/  126]
train() client id: f_00000-2-1 loss: 1.126997  [   64/  126]
train() client id: f_00000-2-2 loss: 1.077314  [   96/  126]
train() client id: f_00000-3-0 loss: 1.101872  [   32/  126]
train() client id: f_00000-3-1 loss: 1.014959  [   64/  126]
train() client id: f_00000-3-2 loss: 1.020661  [   96/  126]
train() client id: f_00000-4-0 loss: 0.999917  [   32/  126]
train() client id: f_00000-4-1 loss: 1.007434  [   64/  126]
train() client id: f_00000-4-2 loss: 0.944430  [   96/  126]
train() client id: f_00000-5-0 loss: 0.943078  [   32/  126]
train() client id: f_00000-5-1 loss: 0.917514  [   64/  126]
train() client id: f_00000-5-2 loss: 0.945027  [   96/  126]
train() client id: f_00000-6-0 loss: 0.894594  [   32/  126]
train() client id: f_00000-6-1 loss: 0.965503  [   64/  126]
train() client id: f_00000-6-2 loss: 0.907407  [   96/  126]
train() client id: f_00000-7-0 loss: 0.880004  [   32/  126]
train() client id: f_00000-7-1 loss: 0.917398  [   64/  126]
train() client id: f_00000-7-2 loss: 0.843400  [   96/  126]
train() client id: f_00000-8-0 loss: 0.804485  [   32/  126]
train() client id: f_00000-8-1 loss: 0.903800  [   64/  126]
train() client id: f_00000-8-2 loss: 0.884135  [   96/  126]
train() client id: f_00000-9-0 loss: 0.848257  [   32/  126]
train() client id: f_00000-9-1 loss: 0.801221  [   64/  126]
train() client id: f_00000-9-2 loss: 0.841610  [   96/  126]
train() client id: f_00000-10-0 loss: 0.869931  [   32/  126]
train() client id: f_00000-10-1 loss: 0.845827  [   64/  126]
train() client id: f_00000-10-2 loss: 0.845083  [   96/  126]
train() client id: f_00000-11-0 loss: 0.838914  [   32/  126]
train() client id: f_00000-11-1 loss: 0.911234  [   64/  126]
train() client id: f_00000-11-2 loss: 0.835378  [   96/  126]
train() client id: f_00000-12-0 loss: 0.881522  [   32/  126]
train() client id: f_00000-12-1 loss: 0.763809  [   64/  126]
train() client id: f_00000-12-2 loss: 0.830181  [   96/  126]
train() client id: f_00001-0-0 loss: 0.686236  [   32/  265]
train() client id: f_00001-0-1 loss: 0.653002  [   64/  265]
train() client id: f_00001-0-2 loss: 0.637955  [   96/  265]
train() client id: f_00001-0-3 loss: 0.736382  [  128/  265]
train() client id: f_00001-0-4 loss: 0.660290  [  160/  265]
train() client id: f_00001-0-5 loss: 0.661490  [  192/  265]
train() client id: f_00001-0-6 loss: 0.648689  [  224/  265]
train() client id: f_00001-0-7 loss: 0.630478  [  256/  265]
train() client id: f_00001-1-0 loss: 0.658427  [   32/  265]
train() client id: f_00001-1-1 loss: 0.606490  [   64/  265]
train() client id: f_00001-1-2 loss: 0.688594  [   96/  265]
train() client id: f_00001-1-3 loss: 0.631283  [  128/  265]
train() client id: f_00001-1-4 loss: 0.567354  [  160/  265]
train() client id: f_00001-1-5 loss: 0.606203  [  192/  265]
train() client id: f_00001-1-6 loss: 0.599518  [  224/  265]
train() client id: f_00001-1-7 loss: 0.673726  [  256/  265]
train() client id: f_00001-2-0 loss: 0.529625  [   32/  265]
train() client id: f_00001-2-1 loss: 0.568601  [   64/  265]
train() client id: f_00001-2-2 loss: 0.606635  [   96/  265]
train() client id: f_00001-2-3 loss: 0.556472  [  128/  265]
train() client id: f_00001-2-4 loss: 0.610700  [  160/  265]
train() client id: f_00001-2-5 loss: 0.658766  [  192/  265]
train() client id: f_00001-2-6 loss: 0.637792  [  224/  265]
train() client id: f_00001-2-7 loss: 0.595270  [  256/  265]
train() client id: f_00001-3-0 loss: 0.632948  [   32/  265]
train() client id: f_00001-3-1 loss: 0.569358  [   64/  265]
train() client id: f_00001-3-2 loss: 0.602484  [   96/  265]
train() client id: f_00001-3-3 loss: 0.706426  [  128/  265]
train() client id: f_00001-3-4 loss: 0.556351  [  160/  265]
train() client id: f_00001-3-5 loss: 0.583687  [  192/  265]
train() client id: f_00001-3-6 loss: 0.506636  [  224/  265]
train() client id: f_00001-3-7 loss: 0.526781  [  256/  265]
train() client id: f_00001-4-0 loss: 0.628844  [   32/  265]
train() client id: f_00001-4-1 loss: 0.514808  [   64/  265]
train() client id: f_00001-4-2 loss: 0.605592  [   96/  265]
train() client id: f_00001-4-3 loss: 0.664378  [  128/  265]
train() client id: f_00001-4-4 loss: 0.502930  [  160/  265]
train() client id: f_00001-4-5 loss: 0.555232  [  192/  265]
train() client id: f_00001-4-6 loss: 0.572796  [  224/  265]
train() client id: f_00001-4-7 loss: 0.569453  [  256/  265]
train() client id: f_00001-5-0 loss: 0.521157  [   32/  265]
train() client id: f_00001-5-1 loss: 0.541994  [   64/  265]
train() client id: f_00001-5-2 loss: 0.577797  [   96/  265]
train() client id: f_00001-5-3 loss: 0.581800  [  128/  265]
train() client id: f_00001-5-4 loss: 0.670482  [  160/  265]
train() client id: f_00001-5-5 loss: 0.499216  [  192/  265]
train() client id: f_00001-5-6 loss: 0.555393  [  224/  265]
train() client id: f_00001-5-7 loss: 0.559261  [  256/  265]
train() client id: f_00001-6-0 loss: 0.584064  [   32/  265]
train() client id: f_00001-6-1 loss: 0.521126  [   64/  265]
train() client id: f_00001-6-2 loss: 0.558033  [   96/  265]
train() client id: f_00001-6-3 loss: 0.584160  [  128/  265]
train() client id: f_00001-6-4 loss: 0.620557  [  160/  265]
train() client id: f_00001-6-5 loss: 0.523024  [  192/  265]
train() client id: f_00001-6-6 loss: 0.542989  [  224/  265]
train() client id: f_00001-6-7 loss: 0.502928  [  256/  265]
train() client id: f_00001-7-0 loss: 0.500548  [   32/  265]
train() client id: f_00001-7-1 loss: 0.544896  [   64/  265]
train() client id: f_00001-7-2 loss: 0.530496  [   96/  265]
train() client id: f_00001-7-3 loss: 0.541744  [  128/  265]
train() client id: f_00001-7-4 loss: 0.590265  [  160/  265]
train() client id: f_00001-7-5 loss: 0.585409  [  192/  265]
train() client id: f_00001-7-6 loss: 0.631606  [  224/  265]
train() client id: f_00001-7-7 loss: 0.551242  [  256/  265]
train() client id: f_00001-8-0 loss: 0.681203  [   32/  265]
train() client id: f_00001-8-1 loss: 0.476996  [   64/  265]
train() client id: f_00001-8-2 loss: 0.634426  [   96/  265]
train() client id: f_00001-8-3 loss: 0.515963  [  128/  265]
train() client id: f_00001-8-4 loss: 0.559874  [  160/  265]
train() client id: f_00001-8-5 loss: 0.483592  [  192/  265]
train() client id: f_00001-8-6 loss: 0.540228  [  224/  265]
train() client id: f_00001-8-7 loss: 0.557434  [  256/  265]
train() client id: f_00001-9-0 loss: 0.615813  [   32/  265]
train() client id: f_00001-9-1 loss: 0.473021  [   64/  265]
train() client id: f_00001-9-2 loss: 0.579888  [   96/  265]
train() client id: f_00001-9-3 loss: 0.573369  [  128/  265]
train() client id: f_00001-9-4 loss: 0.600761  [  160/  265]
train() client id: f_00001-9-5 loss: 0.567944  [  192/  265]
train() client id: f_00001-9-6 loss: 0.483216  [  224/  265]
train() client id: f_00001-9-7 loss: 0.549821  [  256/  265]
train() client id: f_00001-10-0 loss: 0.558089  [   32/  265]
train() client id: f_00001-10-1 loss: 0.503484  [   64/  265]
train() client id: f_00001-10-2 loss: 0.661958  [   96/  265]
train() client id: f_00001-10-3 loss: 0.664454  [  128/  265]
train() client id: f_00001-10-4 loss: 0.480269  [  160/  265]
train() client id: f_00001-10-5 loss: 0.529057  [  192/  265]
train() client id: f_00001-10-6 loss: 0.514001  [  224/  265]
train() client id: f_00001-10-7 loss: 0.493199  [  256/  265]
train() client id: f_00001-11-0 loss: 0.776073  [   32/  265]
train() client id: f_00001-11-1 loss: 0.459877  [   64/  265]
train() client id: f_00001-11-2 loss: 0.482649  [   96/  265]
train() client id: f_00001-11-3 loss: 0.575583  [  128/  265]
train() client id: f_00001-11-4 loss: 0.501742  [  160/  265]
train() client id: f_00001-11-5 loss: 0.568292  [  192/  265]
train() client id: f_00001-11-6 loss: 0.573370  [  224/  265]
train() client id: f_00001-11-7 loss: 0.495307  [  256/  265]
train() client id: f_00001-12-0 loss: 0.546729  [   32/  265]
train() client id: f_00001-12-1 loss: 0.492571  [   64/  265]
train() client id: f_00001-12-2 loss: 0.556994  [   96/  265]
train() client id: f_00001-12-3 loss: 0.492774  [  128/  265]
train() client id: f_00001-12-4 loss: 0.599887  [  160/  265]
train() client id: f_00001-12-5 loss: 0.534361  [  192/  265]
train() client id: f_00001-12-6 loss: 0.589061  [  224/  265]
train() client id: f_00001-12-7 loss: 0.583144  [  256/  265]
train() client id: f_00002-0-0 loss: 1.099596  [   32/  124]
train() client id: f_00002-0-1 loss: 1.051881  [   64/  124]
train() client id: f_00002-0-2 loss: 1.083074  [   96/  124]
train() client id: f_00002-1-0 loss: 1.062399  [   32/  124]
train() client id: f_00002-1-1 loss: 1.014764  [   64/  124]
train() client id: f_00002-1-2 loss: 1.113282  [   96/  124]
train() client id: f_00002-2-0 loss: 1.105953  [   32/  124]
train() client id: f_00002-2-1 loss: 1.015653  [   64/  124]
train() client id: f_00002-2-2 loss: 0.981990  [   96/  124]
train() client id: f_00002-3-0 loss: 0.967031  [   32/  124]
train() client id: f_00002-3-1 loss: 1.072312  [   64/  124]
train() client id: f_00002-3-2 loss: 1.038440  [   96/  124]
train() client id: f_00002-4-0 loss: 1.025747  [   32/  124]
train() client id: f_00002-4-1 loss: 0.989114  [   64/  124]
train() client id: f_00002-4-2 loss: 1.041187  [   96/  124]
train() client id: f_00002-5-0 loss: 0.985808  [   32/  124]
train() client id: f_00002-5-1 loss: 0.954185  [   64/  124]
train() client id: f_00002-5-2 loss: 0.974956  [   96/  124]
train() client id: f_00002-6-0 loss: 1.065304  [   32/  124]
train() client id: f_00002-6-1 loss: 0.920179  [   64/  124]
train() client id: f_00002-6-2 loss: 0.997246  [   96/  124]
train() client id: f_00002-7-0 loss: 0.874098  [   32/  124]
train() client id: f_00002-7-1 loss: 1.063305  [   64/  124]
train() client id: f_00002-7-2 loss: 0.944000  [   96/  124]
train() client id: f_00002-8-0 loss: 0.992694  [   32/  124]
train() client id: f_00002-8-1 loss: 0.833146  [   64/  124]
train() client id: f_00002-8-2 loss: 0.998462  [   96/  124]
train() client id: f_00002-9-0 loss: 0.914469  [   32/  124]
train() client id: f_00002-9-1 loss: 0.954572  [   64/  124]
train() client id: f_00002-9-2 loss: 0.931114  [   96/  124]
train() client id: f_00002-10-0 loss: 0.913821  [   32/  124]
train() client id: f_00002-10-1 loss: 0.959263  [   64/  124]
train() client id: f_00002-10-2 loss: 0.957375  [   96/  124]
train() client id: f_00002-11-0 loss: 0.904148  [   32/  124]
train() client id: f_00002-11-1 loss: 0.870606  [   64/  124]
train() client id: f_00002-11-2 loss: 1.005821  [   96/  124]
train() client id: f_00002-12-0 loss: 0.908093  [   32/  124]
train() client id: f_00002-12-1 loss: 1.046806  [   64/  124]
train() client id: f_00002-12-2 loss: 0.842988  [   96/  124]
train() client id: f_00003-0-0 loss: 1.096549  [   32/   43]
train() client id: f_00003-1-0 loss: 1.087252  [   32/   43]
train() client id: f_00003-2-0 loss: 1.036756  [   32/   43]
train() client id: f_00003-3-0 loss: 1.044191  [   32/   43]
train() client id: f_00003-4-0 loss: 1.072723  [   32/   43]
train() client id: f_00003-5-0 loss: 1.053072  [   32/   43]
train() client id: f_00003-6-0 loss: 1.099762  [   32/   43]
train() client id: f_00003-7-0 loss: 1.059811  [   32/   43]
train() client id: f_00003-8-0 loss: 1.066390  [   32/   43]
train() client id: f_00003-9-0 loss: 1.044674  [   32/   43]
train() client id: f_00003-10-0 loss: 1.087078  [   32/   43]
train() client id: f_00003-11-0 loss: 1.050627  [   32/   43]
train() client id: f_00003-12-0 loss: 1.044446  [   32/   43]
train() client id: f_00004-0-0 loss: 1.055229  [   32/  306]
train() client id: f_00004-0-1 loss: 1.018925  [   64/  306]
train() client id: f_00004-0-2 loss: 1.130014  [   96/  306]
train() client id: f_00004-0-3 loss: 0.984142  [  128/  306]
train() client id: f_00004-0-4 loss: 1.060525  [  160/  306]
train() client id: f_00004-0-5 loss: 1.022446  [  192/  306]
train() client id: f_00004-0-6 loss: 1.088335  [  224/  306]
train() client id: f_00004-0-7 loss: 1.073364  [  256/  306]
train() client id: f_00004-0-8 loss: 1.112459  [  288/  306]
train() client id: f_00004-1-0 loss: 1.119475  [   32/  306]
train() client id: f_00004-1-1 loss: 1.065254  [   64/  306]
train() client id: f_00004-1-2 loss: 1.028025  [   96/  306]
train() client id: f_00004-1-3 loss: 1.049692  [  128/  306]
train() client id: f_00004-1-4 loss: 1.014787  [  160/  306]
train() client id: f_00004-1-5 loss: 1.071732  [  192/  306]
train() client id: f_00004-1-6 loss: 0.988042  [  224/  306]
train() client id: f_00004-1-7 loss: 1.067579  [  256/  306]
train() client id: f_00004-1-8 loss: 1.057279  [  288/  306]
train() client id: f_00004-2-0 loss: 1.044006  [   32/  306]
train() client id: f_00004-2-1 loss: 1.067744  [   64/  306]
train() client id: f_00004-2-2 loss: 1.070665  [   96/  306]
train() client id: f_00004-2-3 loss: 1.038723  [  128/  306]
train() client id: f_00004-2-4 loss: 1.049999  [  160/  306]
train() client id: f_00004-2-5 loss: 1.018367  [  192/  306]
train() client id: f_00004-2-6 loss: 1.114046  [  224/  306]
train() client id: f_00004-2-7 loss: 0.970027  [  256/  306]
train() client id: f_00004-2-8 loss: 1.092585  [  288/  306]
train() client id: f_00004-3-0 loss: 1.007374  [   32/  306]
train() client id: f_00004-3-1 loss: 0.996777  [   64/  306]
train() client id: f_00004-3-2 loss: 1.037836  [   96/  306]
train() client id: f_00004-3-3 loss: 1.137466  [  128/  306]
train() client id: f_00004-3-4 loss: 1.036886  [  160/  306]
train() client id: f_00004-3-5 loss: 1.027494  [  192/  306]
train() client id: f_00004-3-6 loss: 1.006031  [  224/  306]
train() client id: f_00004-3-7 loss: 1.051083  [  256/  306]
train() client id: f_00004-3-8 loss: 1.157624  [  288/  306]
train() client id: f_00004-4-0 loss: 1.041833  [   32/  306]
train() client id: f_00004-4-1 loss: 1.148950  [   64/  306]
train() client id: f_00004-4-2 loss: 1.051112  [   96/  306]
train() client id: f_00004-4-3 loss: 0.991833  [  128/  306]
train() client id: f_00004-4-4 loss: 1.102694  [  160/  306]
train() client id: f_00004-4-5 loss: 1.049860  [  192/  306]
train() client id: f_00004-4-6 loss: 1.044772  [  224/  306]
train() client id: f_00004-4-7 loss: 0.999716  [  256/  306]
train() client id: f_00004-4-8 loss: 0.971335  [  288/  306]
train() client id: f_00004-5-0 loss: 1.005252  [   32/  306]
train() client id: f_00004-5-1 loss: 1.089430  [   64/  306]
train() client id: f_00004-5-2 loss: 1.131107  [   96/  306]
train() client id: f_00004-5-3 loss: 1.065792  [  128/  306]
train() client id: f_00004-5-4 loss: 1.070648  [  160/  306]
train() client id: f_00004-5-5 loss: 1.025385  [  192/  306]
train() client id: f_00004-5-6 loss: 1.046038  [  224/  306]
train() client id: f_00004-5-7 loss: 1.003462  [  256/  306]
train() client id: f_00004-5-8 loss: 0.976912  [  288/  306]
train() client id: f_00004-6-0 loss: 1.089270  [   32/  306]
train() client id: f_00004-6-1 loss: 1.048993  [   64/  306]
train() client id: f_00004-6-2 loss: 1.050051  [   96/  306]
train() client id: f_00004-6-3 loss: 1.017111  [  128/  306]
train() client id: f_00004-6-4 loss: 0.981392  [  160/  306]
train() client id: f_00004-6-5 loss: 1.014905  [  192/  306]
train() client id: f_00004-6-6 loss: 1.110983  [  224/  306]
train() client id: f_00004-6-7 loss: 1.073887  [  256/  306]
train() client id: f_00004-6-8 loss: 1.030299  [  288/  306]
train() client id: f_00004-7-0 loss: 1.021498  [   32/  306]
train() client id: f_00004-7-1 loss: 1.084642  [   64/  306]
train() client id: f_00004-7-2 loss: 1.047165  [   96/  306]
train() client id: f_00004-7-3 loss: 1.048041  [  128/  306]
train() client id: f_00004-7-4 loss: 1.071122  [  160/  306]
train() client id: f_00004-7-5 loss: 1.049112  [  192/  306]
train() client id: f_00004-7-6 loss: 1.095378  [  224/  306]
train() client id: f_00004-7-7 loss: 1.006689  [  256/  306]
train() client id: f_00004-7-8 loss: 0.966259  [  288/  306]
train() client id: f_00004-8-0 loss: 1.102011  [   32/  306]
train() client id: f_00004-8-1 loss: 1.047820  [   64/  306]
train() client id: f_00004-8-2 loss: 1.013518  [   96/  306]
train() client id: f_00004-8-3 loss: 1.012626  [  128/  306]
train() client id: f_00004-8-4 loss: 0.964516  [  160/  306]
train() client id: f_00004-8-5 loss: 1.136599  [  192/  306]
train() client id: f_00004-8-6 loss: 1.087832  [  224/  306]
train() client id: f_00004-8-7 loss: 1.005726  [  256/  306]
train() client id: f_00004-8-8 loss: 1.049738  [  288/  306]
train() client id: f_00004-9-0 loss: 1.047353  [   32/  306]
train() client id: f_00004-9-1 loss: 1.093408  [   64/  306]
train() client id: f_00004-9-2 loss: 0.969560  [   96/  306]
train() client id: f_00004-9-3 loss: 1.112508  [  128/  306]
train() client id: f_00004-9-4 loss: 1.064424  [  160/  306]
train() client id: f_00004-9-5 loss: 0.930143  [  192/  306]
train() client id: f_00004-9-6 loss: 1.034226  [  224/  306]
train() client id: f_00004-9-7 loss: 1.118137  [  256/  306]
train() client id: f_00004-9-8 loss: 1.031965  [  288/  306]
train() client id: f_00004-10-0 loss: 1.009933  [   32/  306]
train() client id: f_00004-10-1 loss: 1.068108  [   64/  306]
train() client id: f_00004-10-2 loss: 1.108063  [   96/  306]
train() client id: f_00004-10-3 loss: 1.010894  [  128/  306]
train() client id: f_00004-10-4 loss: 1.009036  [  160/  306]
train() client id: f_00004-10-5 loss: 1.112258  [  192/  306]
train() client id: f_00004-10-6 loss: 1.102053  [  224/  306]
train() client id: f_00004-10-7 loss: 1.011934  [  256/  306]
train() client id: f_00004-10-8 loss: 0.982264  [  288/  306]
train() client id: f_00004-11-0 loss: 1.007092  [   32/  306]
train() client id: f_00004-11-1 loss: 1.010502  [   64/  306]
train() client id: f_00004-11-2 loss: 1.076762  [   96/  306]
train() client id: f_00004-11-3 loss: 1.033725  [  128/  306]
train() client id: f_00004-11-4 loss: 0.905207  [  160/  306]
train() client id: f_00004-11-5 loss: 1.074059  [  192/  306]
train() client id: f_00004-11-6 loss: 1.036496  [  224/  306]
train() client id: f_00004-11-7 loss: 1.099503  [  256/  306]
train() client id: f_00004-11-8 loss: 1.115032  [  288/  306]
train() client id: f_00004-12-0 loss: 1.034845  [   32/  306]
train() client id: f_00004-12-1 loss: 1.069569  [   64/  306]
train() client id: f_00004-12-2 loss: 1.017493  [   96/  306]
train() client id: f_00004-12-3 loss: 1.007191  [  128/  306]
train() client id: f_00004-12-4 loss: 1.049358  [  160/  306]
train() client id: f_00004-12-5 loss: 1.001509  [  192/  306]
train() client id: f_00004-12-6 loss: 0.971438  [  224/  306]
train() client id: f_00004-12-7 loss: 0.985853  [  256/  306]
train() client id: f_00004-12-8 loss: 1.187026  [  288/  306]
train() client id: f_00005-0-0 loss: 0.951614  [   32/  146]
train() client id: f_00005-0-1 loss: 0.918107  [   64/  146]
train() client id: f_00005-0-2 loss: 0.887862  [   96/  146]
train() client id: f_00005-0-3 loss: 1.028690  [  128/  146]
train() client id: f_00005-1-0 loss: 0.884541  [   32/  146]
train() client id: f_00005-1-1 loss: 0.932292  [   64/  146]
train() client id: f_00005-1-2 loss: 0.942854  [   96/  146]
train() client id: f_00005-1-3 loss: 0.971836  [  128/  146]
train() client id: f_00005-2-0 loss: 0.923491  [   32/  146]
train() client id: f_00005-2-1 loss: 0.847496  [   64/  146]
train() client id: f_00005-2-2 loss: 0.931325  [   96/  146]
train() client id: f_00005-2-3 loss: 0.912927  [  128/  146]
train() client id: f_00005-3-0 loss: 0.908093  [   32/  146]
train() client id: f_00005-3-1 loss: 0.857487  [   64/  146]
train() client id: f_00005-3-2 loss: 0.919878  [   96/  146]
train() client id: f_00005-3-3 loss: 0.879873  [  128/  146]
train() client id: f_00005-4-0 loss: 0.809443  [   32/  146]
train() client id: f_00005-4-1 loss: 0.994724  [   64/  146]
train() client id: f_00005-4-2 loss: 0.888155  [   96/  146]
train() client id: f_00005-4-3 loss: 0.898003  [  128/  146]
train() client id: f_00005-5-0 loss: 0.911535  [   32/  146]
train() client id: f_00005-5-1 loss: 0.859530  [   64/  146]
train() client id: f_00005-5-2 loss: 0.955179  [   96/  146]
train() client id: f_00005-5-3 loss: 0.878535  [  128/  146]
train() client id: f_00005-6-0 loss: 0.864027  [   32/  146]
train() client id: f_00005-6-1 loss: 0.789556  [   64/  146]
train() client id: f_00005-6-2 loss: 0.906744  [   96/  146]
train() client id: f_00005-6-3 loss: 1.019790  [  128/  146]
train() client id: f_00005-7-0 loss: 0.882547  [   32/  146]
train() client id: f_00005-7-1 loss: 0.866820  [   64/  146]
train() client id: f_00005-7-2 loss: 0.921888  [   96/  146]
train() client id: f_00005-7-3 loss: 0.865072  [  128/  146]
train() client id: f_00005-8-0 loss: 0.803616  [   32/  146]
train() client id: f_00005-8-1 loss: 0.705816  [   64/  146]
train() client id: f_00005-8-2 loss: 1.030588  [   96/  146]
train() client id: f_00005-8-3 loss: 0.856086  [  128/  146]
train() client id: f_00005-9-0 loss: 0.889410  [   32/  146]
train() client id: f_00005-9-1 loss: 0.810320  [   64/  146]
train() client id: f_00005-9-2 loss: 0.928495  [   96/  146]
train() client id: f_00005-9-3 loss: 0.804704  [  128/  146]
train() client id: f_00005-10-0 loss: 0.864628  [   32/  146]
train() client id: f_00005-10-1 loss: 0.910078  [   64/  146]
train() client id: f_00005-10-2 loss: 0.896377  [   96/  146]
train() client id: f_00005-10-3 loss: 0.844519  [  128/  146]
train() client id: f_00005-11-0 loss: 0.866917  [   32/  146]
train() client id: f_00005-11-1 loss: 0.828899  [   64/  146]
train() client id: f_00005-11-2 loss: 0.973273  [   96/  146]
train() client id: f_00005-11-3 loss: 0.853966  [  128/  146]
train() client id: f_00005-12-0 loss: 0.879750  [   32/  146]
train() client id: f_00005-12-1 loss: 0.968086  [   64/  146]
train() client id: f_00005-12-2 loss: 0.792531  [   96/  146]
train() client id: f_00005-12-3 loss: 0.826386  [  128/  146]
train() client id: f_00006-0-0 loss: 0.968024  [   32/   54]
train() client id: f_00006-1-0 loss: 0.923637  [   32/   54]
train() client id: f_00006-2-0 loss: 0.973144  [   32/   54]
train() client id: f_00006-3-0 loss: 0.987547  [   32/   54]
train() client id: f_00006-4-0 loss: 0.964378  [   32/   54]
train() client id: f_00006-5-0 loss: 0.916789  [   32/   54]
train() client id: f_00006-6-0 loss: 1.016636  [   32/   54]
train() client id: f_00006-7-0 loss: 1.014838  [   32/   54]
train() client id: f_00006-8-0 loss: 1.026566  [   32/   54]
train() client id: f_00006-9-0 loss: 1.021418  [   32/   54]
train() client id: f_00006-10-0 loss: 0.973285  [   32/   54]
train() client id: f_00006-11-0 loss: 0.924246  [   32/   54]
train() client id: f_00006-12-0 loss: 0.980765  [   32/   54]
train() client id: f_00007-0-0 loss: 0.891769  [   32/  179]
train() client id: f_00007-0-1 loss: 0.913879  [   64/  179]
train() client id: f_00007-0-2 loss: 0.882947  [   96/  179]
train() client id: f_00007-0-3 loss: 0.888972  [  128/  179]
train() client id: f_00007-0-4 loss: 0.849960  [  160/  179]
train() client id: f_00007-1-0 loss: 0.863196  [   32/  179]
train() client id: f_00007-1-1 loss: 0.849445  [   64/  179]
train() client id: f_00007-1-2 loss: 0.852119  [   96/  179]
train() client id: f_00007-1-3 loss: 0.849458  [  128/  179]
train() client id: f_00007-1-4 loss: 0.787936  [  160/  179]
train() client id: f_00007-2-0 loss: 0.846836  [   32/  179]
train() client id: f_00007-2-1 loss: 0.783174  [   64/  179]
train() client id: f_00007-2-2 loss: 0.756770  [   96/  179]
train() client id: f_00007-2-3 loss: 0.773518  [  128/  179]
train() client id: f_00007-2-4 loss: 0.840121  [  160/  179]
train() client id: f_00007-3-0 loss: 0.763728  [   32/  179]
train() client id: f_00007-3-1 loss: 0.803198  [   64/  179]
train() client id: f_00007-3-2 loss: 0.730875  [   96/  179]
train() client id: f_00007-3-3 loss: 0.747853  [  128/  179]
train() client id: f_00007-3-4 loss: 0.832817  [  160/  179]
train() client id: f_00007-4-0 loss: 0.767448  [   32/  179]
train() client id: f_00007-4-1 loss: 0.779082  [   64/  179]
train() client id: f_00007-4-2 loss: 0.706716  [   96/  179]
train() client id: f_00007-4-3 loss: 0.836444  [  128/  179]
train() client id: f_00007-4-4 loss: 0.723409  [  160/  179]
train() client id: f_00007-5-0 loss: 0.706930  [   32/  179]
train() client id: f_00007-5-1 loss: 0.773875  [   64/  179]
train() client id: f_00007-5-2 loss: 0.707686  [   96/  179]
train() client id: f_00007-5-3 loss: 0.824766  [  128/  179]
train() client id: f_00007-5-4 loss: 0.719814  [  160/  179]
train() client id: f_00007-6-0 loss: 0.759386  [   32/  179]
train() client id: f_00007-6-1 loss: 0.757864  [   64/  179]
train() client id: f_00007-6-2 loss: 0.702407  [   96/  179]
train() client id: f_00007-6-3 loss: 0.705566  [  128/  179]
train() client id: f_00007-6-4 loss: 0.693338  [  160/  179]
train() client id: f_00007-7-0 loss: 0.762711  [   32/  179]
train() client id: f_00007-7-1 loss: 0.642476  [   64/  179]
train() client id: f_00007-7-2 loss: 0.703875  [   96/  179]
train() client id: f_00007-7-3 loss: 0.845152  [  128/  179]
train() client id: f_00007-7-4 loss: 0.688261  [  160/  179]
train() client id: f_00007-8-0 loss: 0.784274  [   32/  179]
train() client id: f_00007-8-1 loss: 0.699586  [   64/  179]
train() client id: f_00007-8-2 loss: 0.654419  [   96/  179]
train() client id: f_00007-8-3 loss: 0.774662  [  128/  179]
train() client id: f_00007-8-4 loss: 0.691148  [  160/  179]
train() client id: f_00007-9-0 loss: 0.623124  [   32/  179]
train() client id: f_00007-9-1 loss: 0.790969  [   64/  179]
train() client id: f_00007-9-2 loss: 0.682050  [   96/  179]
train() client id: f_00007-9-3 loss: 0.739182  [  128/  179]
train() client id: f_00007-9-4 loss: 0.695830  [  160/  179]
train() client id: f_00007-10-0 loss: 0.715231  [   32/  179]
train() client id: f_00007-10-1 loss: 0.622010  [   64/  179]
train() client id: f_00007-10-2 loss: 0.689143  [   96/  179]
train() client id: f_00007-10-3 loss: 0.625906  [  128/  179]
train() client id: f_00007-10-4 loss: 0.803656  [  160/  179]
train() client id: f_00007-11-0 loss: 0.724290  [   32/  179]
train() client id: f_00007-11-1 loss: 0.673420  [   64/  179]
train() client id: f_00007-11-2 loss: 0.672112  [   96/  179]
train() client id: f_00007-11-3 loss: 0.809132  [  128/  179]
train() client id: f_00007-11-4 loss: 0.692677  [  160/  179]
train() client id: f_00007-12-0 loss: 0.611028  [   32/  179]
train() client id: f_00007-12-1 loss: 0.615321  [   64/  179]
train() client id: f_00007-12-2 loss: 0.622380  [   96/  179]
train() client id: f_00007-12-3 loss: 0.843253  [  128/  179]
train() client id: f_00007-12-4 loss: 0.754787  [  160/  179]
train() client id: f_00008-0-0 loss: 0.946574  [   32/  130]
train() client id: f_00008-0-1 loss: 0.702335  [   64/  130]
train() client id: f_00008-0-2 loss: 0.818293  [   96/  130]
train() client id: f_00008-0-3 loss: 0.790654  [  128/  130]
train() client id: f_00008-1-0 loss: 0.757969  [   32/  130]
train() client id: f_00008-1-1 loss: 0.895147  [   64/  130]
train() client id: f_00008-1-2 loss: 0.880059  [   96/  130]
train() client id: f_00008-1-3 loss: 0.710370  [  128/  130]
train() client id: f_00008-2-0 loss: 0.754620  [   32/  130]
train() client id: f_00008-2-1 loss: 0.821847  [   64/  130]
train() client id: f_00008-2-2 loss: 0.801532  [   96/  130]
train() client id: f_00008-2-3 loss: 0.849939  [  128/  130]
train() client id: f_00008-3-0 loss: 0.723487  [   32/  130]
train() client id: f_00008-3-1 loss: 0.762137  [   64/  130]
train() client id: f_00008-3-2 loss: 0.799548  [   96/  130]
train() client id: f_00008-3-3 loss: 0.894816  [  128/  130]
train() client id: f_00008-4-0 loss: 0.878396  [   32/  130]
train() client id: f_00008-4-1 loss: 0.828389  [   64/  130]
train() client id: f_00008-4-2 loss: 0.704947  [   96/  130]
train() client id: f_00008-4-3 loss: 0.757296  [  128/  130]
train() client id: f_00008-5-0 loss: 0.779193  [   32/  130]
train() client id: f_00008-5-1 loss: 0.762990  [   64/  130]
train() client id: f_00008-5-2 loss: 0.851026  [   96/  130]
train() client id: f_00008-5-3 loss: 0.784395  [  128/  130]
train() client id: f_00008-6-0 loss: 0.817294  [   32/  130]
train() client id: f_00008-6-1 loss: 0.857979  [   64/  130]
train() client id: f_00008-6-2 loss: 0.684169  [   96/  130]
train() client id: f_00008-6-3 loss: 0.772447  [  128/  130]
train() client id: f_00008-7-0 loss: 0.787998  [   32/  130]
train() client id: f_00008-7-1 loss: 0.856454  [   64/  130]
train() client id: f_00008-7-2 loss: 0.764259  [   96/  130]
train() client id: f_00008-7-3 loss: 0.747003  [  128/  130]
train() client id: f_00008-8-0 loss: 0.726662  [   32/  130]
train() client id: f_00008-8-1 loss: 0.818524  [   64/  130]
train() client id: f_00008-8-2 loss: 0.861315  [   96/  130]
train() client id: f_00008-8-3 loss: 0.751661  [  128/  130]
train() client id: f_00008-9-0 loss: 0.709095  [   32/  130]
train() client id: f_00008-9-1 loss: 0.717024  [   64/  130]
train() client id: f_00008-9-2 loss: 0.992346  [   96/  130]
train() client id: f_00008-9-3 loss: 0.724560  [  128/  130]
train() client id: f_00008-10-0 loss: 0.783119  [   32/  130]
train() client id: f_00008-10-1 loss: 0.785450  [   64/  130]
train() client id: f_00008-10-2 loss: 0.792861  [   96/  130]
train() client id: f_00008-10-3 loss: 0.752643  [  128/  130]
train() client id: f_00008-11-0 loss: 0.803305  [   32/  130]
train() client id: f_00008-11-1 loss: 0.802325  [   64/  130]
train() client id: f_00008-11-2 loss: 0.738599  [   96/  130]
train() client id: f_00008-11-3 loss: 0.761334  [  128/  130]
train() client id: f_00008-12-0 loss: 0.815099  [   32/  130]
train() client id: f_00008-12-1 loss: 0.843083  [   64/  130]
train() client id: f_00008-12-2 loss: 0.756540  [   96/  130]
train() client id: f_00008-12-3 loss: 0.709792  [  128/  130]
train() client id: f_00009-0-0 loss: 1.071388  [   32/  118]
train() client id: f_00009-0-1 loss: 1.208753  [   64/  118]
train() client id: f_00009-0-2 loss: 1.145996  [   96/  118]
train() client id: f_00009-1-0 loss: 1.100868  [   32/  118]
train() client id: f_00009-1-1 loss: 1.139919  [   64/  118]
train() client id: f_00009-1-2 loss: 1.040442  [   96/  118]
train() client id: f_00009-2-0 loss: 1.090554  [   32/  118]
train() client id: f_00009-2-1 loss: 1.085759  [   64/  118]
train() client id: f_00009-2-2 loss: 1.030022  [   96/  118]
train() client id: f_00009-3-0 loss: 1.013862  [   32/  118]
train() client id: f_00009-3-1 loss: 1.058544  [   64/  118]
train() client id: f_00009-3-2 loss: 0.991996  [   96/  118]
train() client id: f_00009-4-0 loss: 1.030089  [   32/  118]
train() client id: f_00009-4-1 loss: 1.055494  [   64/  118]
train() client id: f_00009-4-2 loss: 0.912614  [   96/  118]
train() client id: f_00009-5-0 loss: 0.956267  [   32/  118]
train() client id: f_00009-5-1 loss: 0.893878  [   64/  118]
train() client id: f_00009-5-2 loss: 1.015721  [   96/  118]
train() client id: f_00009-6-0 loss: 0.909569  [   32/  118]
train() client id: f_00009-6-1 loss: 0.980232  [   64/  118]
train() client id: f_00009-6-2 loss: 0.934097  [   96/  118]
train() client id: f_00009-7-0 loss: 0.911012  [   32/  118]
train() client id: f_00009-7-1 loss: 0.997172  [   64/  118]
train() client id: f_00009-7-2 loss: 0.860593  [   96/  118]
train() client id: f_00009-8-0 loss: 0.971718  [   32/  118]
train() client id: f_00009-8-1 loss: 0.924850  [   64/  118]
train() client id: f_00009-8-2 loss: 0.889449  [   96/  118]
train() client id: f_00009-9-0 loss: 0.957757  [   32/  118]
train() client id: f_00009-9-1 loss: 0.908015  [   64/  118]
train() client id: f_00009-9-2 loss: 0.809239  [   96/  118]
train() client id: f_00009-10-0 loss: 0.951785  [   32/  118]
train() client id: f_00009-10-1 loss: 0.875609  [   64/  118]
train() client id: f_00009-10-2 loss: 0.825766  [   96/  118]
train() client id: f_00009-11-0 loss: 0.886070  [   32/  118]
train() client id: f_00009-11-1 loss: 0.822811  [   64/  118]
train() client id: f_00009-11-2 loss: 0.875810  [   96/  118]
train() client id: f_00009-12-0 loss: 0.780403  [   32/  118]
train() client id: f_00009-12-1 loss: 0.924603  [   64/  118]
train() client id: f_00009-12-2 loss: 0.779793  [   96/  118]
At round 4 accuracy: 0.623342175066313
At round 4 training accuracy: 0.5674044265593562
At round 4 training loss: 0.9253974240512262
update_location
xs = [ -3.9056584    4.20031788  40.00902392  18.81129433 -14.02070377
  -1.04359014  -2.44319194  -6.32485185  24.66397685   2.93912145]
ys = [ 32.5879595   15.55583871   1.32061395  -2.45517586   9.35018685
 -17.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [105.24841696 101.28981582 107.71474373 101.78355802 101.41008889
 101.47138757 100.06427799 100.28687116 104.48436116 100.1231756 ]
dists_bs = [222.60831747 239.86495054 276.39357718 262.77967434 230.98526882
 259.22236244 247.64185241 246.08115854 254.2640783  246.78497559]
uav_gains = [8.79949509e-11 9.68464333e-11 8.30438549e-11 9.56761907e-11
 9.65595300e-11 9.64137637e-11 9.98391157e-11 9.92860273e-11
 8.96125391e-11 9.96923511e-11]
bs_gains = [2.95242043e-11 2.39544975e-11 1.61070436e-11 1.85539819e-11
 2.66230674e-11 1.92757456e-11 2.19071944e-11 2.22984497e-11
 2.03467974e-11 2.21208434e-11]
Round 5
-------------------------------
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.33284149 21.59560189 10.17342765  3.63728595 24.89615745 12.01211791
  4.52299688 14.602885   10.71870557  9.74625265]
obj_prev = 122.23827245410807
eta_min = 9.414027209246992e-10	eta_max = 0.9186608067299383
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 28.431400687067253	eta = 0.909090909090909
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 48.95841824133416	eta = 0.5279322499743717
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 39.18478015448109	eta = 0.6596114051281231
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.434409794184106	eta = 0.6904537306569067
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.34853700150222	eta = 0.6920412410342677
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.348315836191034	eta = 0.6920453390909805
eta = 0.6920453390909805
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.03037504 0.06388402 0.0298929  0.01036609 0.07376796 0.03519648
 0.01301788 0.04315185 0.03133933 0.02844647]
ene_total = [3.16534696 6.21953031 3.1298897  1.43707177 7.05111483 3.79361781
 1.66080998 4.25148272 3.45176733 3.18768443]
ti_comp = [0.28160493 0.26136622 0.28090865 0.2825856  0.2634578  0.25672527
 0.28307344 0.28301023 0.25792521 0.25972041]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20876875e-05 2.38538142e-04 2.11569937e-05 8.71815183e-07
 3.61460861e-04 4.13466617e-05 1.72068806e-06 6.27009232e-05
 2.89175573e-05 2.13281397e-05]
ene_total = [0.55639501 0.74895917 0.56229982 0.54614218 0.74154691 0.77190053
 0.54202196 0.54780676 0.76051829 0.74443548]
optimize_network iter = 0 obj = 6.522026104246858
eta = 0.6920453390909805
freqs = [5.39320171e+07 1.22211702e+08 5.32075115e+07 1.83414978e+07
 1.39999582e+08 6.85489188e+07 2.29938135e+07 7.62372573e+07
 6.07527495e+07 5.47636387e+07]
eta_min = 0.6764484296472902	eta_max = 0.6920453390909633
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 0.06328991478194519	eta = 0.9090909090909091
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 21.92072085717903	eta = 0.0026247442563715508
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.335363763739976	eta = 0.024636969648472683
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.25597545648311	eta = 0.025503950408706712
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.255940548856704	eta = 0.025504345047817744
eta = 0.025504345047817744
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20148930e-04 2.37751991e-03 2.10872665e-04 8.68941936e-06
 3.60269593e-03 4.12103953e-04 1.71501718e-05 6.24942794e-04
 2.88222535e-04 2.12578485e-04]
ene_total = [0.1812087  0.29478227 0.18284802 0.17280114 0.32238    0.25400243
 0.17170585 0.1883868  0.24737814 0.2404472 ]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 1 obj = 6.211119913967213
eta = 0.6764484296472902
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
eta_min = 0.6764484296473243	eta_max = 0.6764484296472864
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 0.06277694578067981	eta = 0.909090909090909
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 21.92023194550156	eta = 0.002603528596394288
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.332999905489733	eta = 0.024462045872963308
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2542173952325193	eta = 0.02531696846560011
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2541832396466193	eta = 0.02531735207056884
eta = 0.02531735207056884
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20456386e-04 2.35937920e-03 2.11105834e-04 8.70509514e-06
 3.57877769e-03 4.08031791e-04 1.71845703e-05 6.26179946e-04
 2.85544288e-04 2.10788545e-04]
ene_total = [0.18118229 0.29423306 0.18281927 0.17276841 0.32166859 0.25384311
 0.17167384 0.18838425 0.24725795 0.24035246]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 2 obj = 6.2111199139678615
eta = 0.6764484296473243
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
Done!
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.19761204e-05 2.35193919e-04 2.10440137e-05 8.67764468e-07
 3.56749245e-04 4.06745112e-05 1.71303809e-06 6.24205363e-05
 2.84643859e-05 2.10123849e-05]
ene_total = [0.00647308 0.00871016 0.00654177 0.0063539  0.00862256 0.00897974
 0.00630596 0.00637299 0.00884754 0.00866056]
At round 5 energy consumption: 0.07586826097698268
At round 5 eta: 0.6764484296473243
At round 5 a_n: 26.46987362140779
At round 5 local rounds: 12.80002140651856
At round 5 global rounds: 81.81036980458869
gradient difference: 0.4712376594543457
train() client id: f_00000-0-0 loss: 1.199014  [   32/  126]
train() client id: f_00000-0-1 loss: 1.296626  [   64/  126]
train() client id: f_00000-0-2 loss: 1.316485  [   96/  126]
train() client id: f_00000-1-0 loss: 1.302661  [   32/  126]
train() client id: f_00000-1-1 loss: 1.140091  [   64/  126]
train() client id: f_00000-1-2 loss: 1.130696  [   96/  126]
train() client id: f_00000-2-0 loss: 1.127939  [   32/  126]
train() client id: f_00000-2-1 loss: 1.083856  [   64/  126]
train() client id: f_00000-2-2 loss: 1.080040  [   96/  126]
train() client id: f_00000-3-0 loss: 1.098392  [   32/  126]
train() client id: f_00000-3-1 loss: 1.032324  [   64/  126]
train() client id: f_00000-3-2 loss: 1.024887  [   96/  126]
train() client id: f_00000-4-0 loss: 1.040661  [   32/  126]
train() client id: f_00000-4-1 loss: 1.035404  [   64/  126]
train() client id: f_00000-4-2 loss: 0.971752  [   96/  126]
train() client id: f_00000-5-0 loss: 0.992195  [   32/  126]
train() client id: f_00000-5-1 loss: 0.972310  [   64/  126]
train() client id: f_00000-5-2 loss: 0.958265  [   96/  126]
train() client id: f_00000-6-0 loss: 0.966795  [   32/  126]
train() client id: f_00000-6-1 loss: 0.954479  [   64/  126]
train() client id: f_00000-6-2 loss: 0.964809  [   96/  126]
train() client id: f_00000-7-0 loss: 0.923477  [   32/  126]
train() client id: f_00000-7-1 loss: 0.956647  [   64/  126]
train() client id: f_00000-7-2 loss: 0.952619  [   96/  126]
train() client id: f_00000-8-0 loss: 0.954439  [   32/  126]
train() client id: f_00000-8-1 loss: 0.952357  [   64/  126]
train() client id: f_00000-8-2 loss: 0.894715  [   96/  126]
train() client id: f_00000-9-0 loss: 0.970715  [   32/  126]
train() client id: f_00000-9-1 loss: 0.918022  [   64/  126]
train() client id: f_00000-9-2 loss: 0.856396  [   96/  126]
train() client id: f_00000-10-0 loss: 0.896695  [   32/  126]
train() client id: f_00000-10-1 loss: 0.953036  [   64/  126]
train() client id: f_00000-10-2 loss: 0.937810  [   96/  126]
train() client id: f_00000-11-0 loss: 0.964463  [   32/  126]
train() client id: f_00000-11-1 loss: 0.894603  [   64/  126]
train() client id: f_00000-11-2 loss: 0.858770  [   96/  126]
train() client id: f_00001-0-0 loss: 0.578982  [   32/  265]
train() client id: f_00001-0-1 loss: 0.660478  [   64/  265]
train() client id: f_00001-0-2 loss: 0.585479  [   96/  265]
train() client id: f_00001-0-3 loss: 0.544154  [  128/  265]
train() client id: f_00001-0-4 loss: 0.595049  [  160/  265]
train() client id: f_00001-0-5 loss: 0.619690  [  192/  265]
train() client id: f_00001-0-6 loss: 0.611214  [  224/  265]
train() client id: f_00001-0-7 loss: 0.557660  [  256/  265]
train() client id: f_00001-1-0 loss: 0.542256  [   32/  265]
train() client id: f_00001-1-1 loss: 0.672045  [   64/  265]
train() client id: f_00001-1-2 loss: 0.554740  [   96/  265]
train() client id: f_00001-1-3 loss: 0.658146  [  128/  265]
train() client id: f_00001-1-4 loss: 0.494671  [  160/  265]
train() client id: f_00001-1-5 loss: 0.522983  [  192/  265]
train() client id: f_00001-1-6 loss: 0.490866  [  224/  265]
train() client id: f_00001-1-7 loss: 0.610018  [  256/  265]
train() client id: f_00001-2-0 loss: 0.608209  [   32/  265]
train() client id: f_00001-2-1 loss: 0.493735  [   64/  265]
train() client id: f_00001-2-2 loss: 0.533556  [   96/  265]
train() client id: f_00001-2-3 loss: 0.503325  [  128/  265]
train() client id: f_00001-2-4 loss: 0.604995  [  160/  265]
train() client id: f_00001-2-5 loss: 0.518379  [  192/  265]
train() client id: f_00001-2-6 loss: 0.503362  [  224/  265]
train() client id: f_00001-2-7 loss: 0.609830  [  256/  265]
train() client id: f_00001-3-0 loss: 0.518224  [   32/  265]
train() client id: f_00001-3-1 loss: 0.569184  [   64/  265]
train() client id: f_00001-3-2 loss: 0.539912  [   96/  265]
train() client id: f_00001-3-3 loss: 0.454118  [  128/  265]
train() client id: f_00001-3-4 loss: 0.463912  [  160/  265]
train() client id: f_00001-3-5 loss: 0.501265  [  192/  265]
train() client id: f_00001-3-6 loss: 0.687466  [  224/  265]
train() client id: f_00001-3-7 loss: 0.511844  [  256/  265]
train() client id: f_00001-4-0 loss: 0.489049  [   32/  265]
train() client id: f_00001-4-1 loss: 0.513818  [   64/  265]
train() client id: f_00001-4-2 loss: 0.511913  [   96/  265]
train() client id: f_00001-4-3 loss: 0.530626  [  128/  265]
train() client id: f_00001-4-4 loss: 0.722032  [  160/  265]
train() client id: f_00001-4-5 loss: 0.446687  [  192/  265]
train() client id: f_00001-4-6 loss: 0.478773  [  224/  265]
train() client id: f_00001-4-7 loss: 0.459095  [  256/  265]
train() client id: f_00001-5-0 loss: 0.521312  [   32/  265]
train() client id: f_00001-5-1 loss: 0.514470  [   64/  265]
train() client id: f_00001-5-2 loss: 0.538499  [   96/  265]
train() client id: f_00001-5-3 loss: 0.513805  [  128/  265]
train() client id: f_00001-5-4 loss: 0.480733  [  160/  265]
train() client id: f_00001-5-5 loss: 0.535219  [  192/  265]
train() client id: f_00001-5-6 loss: 0.488730  [  224/  265]
train() client id: f_00001-5-7 loss: 0.442335  [  256/  265]
train() client id: f_00001-6-0 loss: 0.516148  [   32/  265]
train() client id: f_00001-6-1 loss: 0.481693  [   64/  265]
train() client id: f_00001-6-2 loss: 0.526758  [   96/  265]
train() client id: f_00001-6-3 loss: 0.415946  [  128/  265]
train() client id: f_00001-6-4 loss: 0.607938  [  160/  265]
train() client id: f_00001-6-5 loss: 0.561099  [  192/  265]
train() client id: f_00001-6-6 loss: 0.495442  [  224/  265]
train() client id: f_00001-6-7 loss: 0.422154  [  256/  265]
train() client id: f_00001-7-0 loss: 0.440864  [   32/  265]
train() client id: f_00001-7-1 loss: 0.527755  [   64/  265]
train() client id: f_00001-7-2 loss: 0.517838  [   96/  265]
train() client id: f_00001-7-3 loss: 0.419450  [  128/  265]
train() client id: f_00001-7-4 loss: 0.550788  [  160/  265]
train() client id: f_00001-7-5 loss: 0.520377  [  192/  265]
train() client id: f_00001-7-6 loss: 0.450381  [  224/  265]
train() client id: f_00001-7-7 loss: 0.518599  [  256/  265]
train() client id: f_00001-8-0 loss: 0.465952  [   32/  265]
train() client id: f_00001-8-1 loss: 0.575121  [   64/  265]
train() client id: f_00001-8-2 loss: 0.483097  [   96/  265]
train() client id: f_00001-8-3 loss: 0.555505  [  128/  265]
train() client id: f_00001-8-4 loss: 0.489334  [  160/  265]
train() client id: f_00001-8-5 loss: 0.523548  [  192/  265]
train() client id: f_00001-8-6 loss: 0.405739  [  224/  265]
train() client id: f_00001-8-7 loss: 0.447458  [  256/  265]
train() client id: f_00001-9-0 loss: 0.622786  [   32/  265]
train() client id: f_00001-9-1 loss: 0.581661  [   64/  265]
train() client id: f_00001-9-2 loss: 0.426817  [   96/  265]
train() client id: f_00001-9-3 loss: 0.519056  [  128/  265]
train() client id: f_00001-9-4 loss: 0.413405  [  160/  265]
train() client id: f_00001-9-5 loss: 0.502945  [  192/  265]
train() client id: f_00001-9-6 loss: 0.457148  [  224/  265]
train() client id: f_00001-9-7 loss: 0.409933  [  256/  265]
train() client id: f_00001-10-0 loss: 0.451358  [   32/  265]
train() client id: f_00001-10-1 loss: 0.504035  [   64/  265]
train() client id: f_00001-10-2 loss: 0.466847  [   96/  265]
train() client id: f_00001-10-3 loss: 0.467067  [  128/  265]
train() client id: f_00001-10-4 loss: 0.470416  [  160/  265]
train() client id: f_00001-10-5 loss: 0.442686  [  192/  265]
train() client id: f_00001-10-6 loss: 0.456310  [  224/  265]
train() client id: f_00001-10-7 loss: 0.664990  [  256/  265]
train() client id: f_00001-11-0 loss: 0.426096  [   32/  265]
train() client id: f_00001-11-1 loss: 0.448741  [   64/  265]
train() client id: f_00001-11-2 loss: 0.434239  [   96/  265]
train() client id: f_00001-11-3 loss: 0.548384  [  128/  265]
train() client id: f_00001-11-4 loss: 0.417419  [  160/  265]
train() client id: f_00001-11-5 loss: 0.491064  [  192/  265]
train() client id: f_00001-11-6 loss: 0.560376  [  224/  265]
train() client id: f_00001-11-7 loss: 0.502638  [  256/  265]
train() client id: f_00002-0-0 loss: 1.131316  [   32/  124]
train() client id: f_00002-0-1 loss: 1.139997  [   64/  124]
train() client id: f_00002-0-2 loss: 1.103354  [   96/  124]
train() client id: f_00002-1-0 loss: 1.140226  [   32/  124]
train() client id: f_00002-1-1 loss: 1.104626  [   64/  124]
train() client id: f_00002-1-2 loss: 1.162665  [   96/  124]
train() client id: f_00002-2-0 loss: 1.135973  [   32/  124]
train() client id: f_00002-2-1 loss: 1.064876  [   64/  124]
train() client id: f_00002-2-2 loss: 1.096509  [   96/  124]
train() client id: f_00002-3-0 loss: 1.081735  [   32/  124]
train() client id: f_00002-3-1 loss: 1.081311  [   64/  124]
train() client id: f_00002-3-2 loss: 1.100642  [   96/  124]
train() client id: f_00002-4-0 loss: 1.031523  [   32/  124]
train() client id: f_00002-4-1 loss: 1.047217  [   64/  124]
train() client id: f_00002-4-2 loss: 1.085438  [   96/  124]
train() client id: f_00002-5-0 loss: 1.087434  [   32/  124]
train() client id: f_00002-5-1 loss: 1.064287  [   64/  124]
train() client id: f_00002-5-2 loss: 0.975585  [   96/  124]
train() client id: f_00002-6-0 loss: 1.007033  [   32/  124]
train() client id: f_00002-6-1 loss: 1.101908  [   64/  124]
train() client id: f_00002-6-2 loss: 1.013963  [   96/  124]
train() client id: f_00002-7-0 loss: 1.091042  [   32/  124]
train() client id: f_00002-7-1 loss: 1.066283  [   64/  124]
train() client id: f_00002-7-2 loss: 0.977666  [   96/  124]
train() client id: f_00002-8-0 loss: 1.006347  [   32/  124]
train() client id: f_00002-8-1 loss: 0.997764  [   64/  124]
train() client id: f_00002-8-2 loss: 1.005464  [   96/  124]
train() client id: f_00002-9-0 loss: 0.952738  [   32/  124]
train() client id: f_00002-9-1 loss: 1.040483  [   64/  124]
train() client id: f_00002-9-2 loss: 1.044411  [   96/  124]
train() client id: f_00002-10-0 loss: 0.938649  [   32/  124]
train() client id: f_00002-10-1 loss: 0.957149  [   64/  124]
train() client id: f_00002-10-2 loss: 1.003033  [   96/  124]
train() client id: f_00002-11-0 loss: 0.956162  [   32/  124]
train() client id: f_00002-11-1 loss: 0.958503  [   64/  124]
train() client id: f_00002-11-2 loss: 1.028772  [   96/  124]
train() client id: f_00003-0-0 loss: 1.043155  [   32/   43]
train() client id: f_00003-1-0 loss: 1.038176  [   32/   43]
train() client id: f_00003-2-0 loss: 1.043461  [   32/   43]
train() client id: f_00003-3-0 loss: 1.048736  [   32/   43]
train() client id: f_00003-4-0 loss: 1.092915  [   32/   43]
train() client id: f_00003-5-0 loss: 1.064797  [   32/   43]
train() client id: f_00003-6-0 loss: 1.052811  [   32/   43]
train() client id: f_00003-7-0 loss: 1.116696  [   32/   43]
train() client id: f_00003-8-0 loss: 1.060338  [   32/   43]
train() client id: f_00003-9-0 loss: 1.091126  [   32/   43]
train() client id: f_00003-10-0 loss: 1.098434  [   32/   43]
train() client id: f_00003-11-0 loss: 1.036069  [   32/   43]
train() client id: f_00004-0-0 loss: 0.852404  [   32/  306]
train() client id: f_00004-0-1 loss: 0.917236  [   64/  306]
train() client id: f_00004-0-2 loss: 0.825360  [   96/  306]
train() client id: f_00004-0-3 loss: 0.848329  [  128/  306]
train() client id: f_00004-0-4 loss: 0.888063  [  160/  306]
train() client id: f_00004-0-5 loss: 0.971894  [  192/  306]
train() client id: f_00004-0-6 loss: 0.856448  [  224/  306]
train() client id: f_00004-0-7 loss: 0.842137  [  256/  306]
train() client id: f_00004-0-8 loss: 0.846948  [  288/  306]
train() client id: f_00004-1-0 loss: 0.902074  [   32/  306]
train() client id: f_00004-1-1 loss: 0.830504  [   64/  306]
train() client id: f_00004-1-2 loss: 0.858658  [   96/  306]
train() client id: f_00004-1-3 loss: 0.897626  [  128/  306]
train() client id: f_00004-1-4 loss: 0.844062  [  160/  306]
train() client id: f_00004-1-5 loss: 0.821015  [  192/  306]
train() client id: f_00004-1-6 loss: 0.799970  [  224/  306]
train() client id: f_00004-1-7 loss: 0.831490  [  256/  306]
train() client id: f_00004-1-8 loss: 0.883149  [  288/  306]
train() client id: f_00004-2-0 loss: 0.904296  [   32/  306]
train() client id: f_00004-2-1 loss: 0.847027  [   64/  306]
train() client id: f_00004-2-2 loss: 0.841382  [   96/  306]
train() client id: f_00004-2-3 loss: 0.818475  [  128/  306]
train() client id: f_00004-2-4 loss: 0.824072  [  160/  306]
train() client id: f_00004-2-5 loss: 0.796465  [  192/  306]
train() client id: f_00004-2-6 loss: 0.857569  [  224/  306]
train() client id: f_00004-2-7 loss: 0.837195  [  256/  306]
train() client id: f_00004-2-8 loss: 0.957655  [  288/  306]
train() client id: f_00004-3-0 loss: 0.815633  [   32/  306]
train() client id: f_00004-3-1 loss: 0.818416  [   64/  306]
train() client id: f_00004-3-2 loss: 0.841284  [   96/  306]
train() client id: f_00004-3-3 loss: 0.820250  [  128/  306]
train() client id: f_00004-3-4 loss: 0.907155  [  160/  306]
train() client id: f_00004-3-5 loss: 0.862625  [  192/  306]
train() client id: f_00004-3-6 loss: 0.831409  [  224/  306]
train() client id: f_00004-3-7 loss: 0.914760  [  256/  306]
train() client id: f_00004-3-8 loss: 0.842000  [  288/  306]
train() client id: f_00004-4-0 loss: 0.947851  [   32/  306]
train() client id: f_00004-4-1 loss: 0.848469  [   64/  306]
train() client id: f_00004-4-2 loss: 0.740003  [   96/  306]
train() client id: f_00004-4-3 loss: 0.781514  [  128/  306]
train() client id: f_00004-4-4 loss: 0.857095  [  160/  306]
train() client id: f_00004-4-5 loss: 0.780147  [  192/  306]
train() client id: f_00004-4-6 loss: 0.885768  [  224/  306]
train() client id: f_00004-4-7 loss: 0.881586  [  256/  306]
train() client id: f_00004-4-8 loss: 0.838147  [  288/  306]
train() client id: f_00004-5-0 loss: 0.922239  [   32/  306]
train() client id: f_00004-5-1 loss: 0.795181  [   64/  306]
train() client id: f_00004-5-2 loss: 0.874636  [   96/  306]
train() client id: f_00004-5-3 loss: 0.769653  [  128/  306]
train() client id: f_00004-5-4 loss: 0.927879  [  160/  306]
train() client id: f_00004-5-5 loss: 0.873993  [  192/  306]
train() client id: f_00004-5-6 loss: 0.778624  [  224/  306]
train() client id: f_00004-5-7 loss: 0.784366  [  256/  306]
train() client id: f_00004-5-8 loss: 0.814946  [  288/  306]
train() client id: f_00004-6-0 loss: 0.748043  [   32/  306]
train() client id: f_00004-6-1 loss: 0.849502  [   64/  306]
train() client id: f_00004-6-2 loss: 0.874801  [   96/  306]
train() client id: f_00004-6-3 loss: 0.845568  [  128/  306]
train() client id: f_00004-6-4 loss: 0.767007  [  160/  306]
train() client id: f_00004-6-5 loss: 0.813977  [  192/  306]
train() client id: f_00004-6-6 loss: 0.837125  [  224/  306]
train() client id: f_00004-6-7 loss: 0.849082  [  256/  306]
train() client id: f_00004-6-8 loss: 0.910799  [  288/  306]
train() client id: f_00004-7-0 loss: 0.838653  [   32/  306]
train() client id: f_00004-7-1 loss: 0.790543  [   64/  306]
train() client id: f_00004-7-2 loss: 0.909759  [   96/  306]
train() client id: f_00004-7-3 loss: 0.854327  [  128/  306]
train() client id: f_00004-7-4 loss: 0.790654  [  160/  306]
train() client id: f_00004-7-5 loss: 0.883892  [  192/  306]
train() client id: f_00004-7-6 loss: 0.725125  [  224/  306]
train() client id: f_00004-7-7 loss: 0.805985  [  256/  306]
train() client id: f_00004-7-8 loss: 0.873712  [  288/  306]
train() client id: f_00004-8-0 loss: 0.837311  [   32/  306]
train() client id: f_00004-8-1 loss: 0.907042  [   64/  306]
train() client id: f_00004-8-2 loss: 0.763484  [   96/  306]
train() client id: f_00004-8-3 loss: 0.793982  [  128/  306]
train() client id: f_00004-8-4 loss: 0.797337  [  160/  306]
train() client id: f_00004-8-5 loss: 0.804127  [  192/  306]
train() client id: f_00004-8-6 loss: 0.841332  [  224/  306]
train() client id: f_00004-8-7 loss: 0.751095  [  256/  306]
train() client id: f_00004-8-8 loss: 0.970631  [  288/  306]
train() client id: f_00004-9-0 loss: 0.856430  [   32/  306]
train() client id: f_00004-9-1 loss: 0.874377  [   64/  306]
train() client id: f_00004-9-2 loss: 0.810703  [   96/  306]
train() client id: f_00004-9-3 loss: 0.783649  [  128/  306]
train() client id: f_00004-9-4 loss: 0.771568  [  160/  306]
train() client id: f_00004-9-5 loss: 0.761538  [  192/  306]
train() client id: f_00004-9-6 loss: 0.835476  [  224/  306]
train() client id: f_00004-9-7 loss: 0.954152  [  256/  306]
train() client id: f_00004-9-8 loss: 0.787180  [  288/  306]
train() client id: f_00004-10-0 loss: 0.856475  [   32/  306]
train() client id: f_00004-10-1 loss: 0.789395  [   64/  306]
train() client id: f_00004-10-2 loss: 0.841648  [   96/  306]
train() client id: f_00004-10-3 loss: 0.829998  [  128/  306]
train() client id: f_00004-10-4 loss: 0.806152  [  160/  306]
train() client id: f_00004-10-5 loss: 0.782738  [  192/  306]
train() client id: f_00004-10-6 loss: 0.999037  [  224/  306]
train() client id: f_00004-10-7 loss: 0.831458  [  256/  306]
train() client id: f_00004-10-8 loss: 0.787036  [  288/  306]
train() client id: f_00004-11-0 loss: 0.822645  [   32/  306]
train() client id: f_00004-11-1 loss: 0.797061  [   64/  306]
train() client id: f_00004-11-2 loss: 0.832147  [   96/  306]
train() client id: f_00004-11-3 loss: 0.847470  [  128/  306]
train() client id: f_00004-11-4 loss: 0.810131  [  160/  306]
train() client id: f_00004-11-5 loss: 0.838282  [  192/  306]
train() client id: f_00004-11-6 loss: 0.797561  [  224/  306]
train() client id: f_00004-11-7 loss: 0.876111  [  256/  306]
train() client id: f_00004-11-8 loss: 0.871561  [  288/  306]
train() client id: f_00005-0-0 loss: 0.897582  [   32/  146]
train() client id: f_00005-0-1 loss: 0.832526  [   64/  146]
train() client id: f_00005-0-2 loss: 0.900784  [   96/  146]
train() client id: f_00005-0-3 loss: 1.029703  [  128/  146]
train() client id: f_00005-1-0 loss: 0.833884  [   32/  146]
train() client id: f_00005-1-1 loss: 0.910417  [   64/  146]
train() client id: f_00005-1-2 loss: 0.945680  [   96/  146]
train() client id: f_00005-1-3 loss: 0.947933  [  128/  146]
train() client id: f_00005-2-0 loss: 0.853779  [   32/  146]
train() client id: f_00005-2-1 loss: 0.861287  [   64/  146]
train() client id: f_00005-2-2 loss: 0.986656  [   96/  146]
train() client id: f_00005-2-3 loss: 0.841700  [  128/  146]
train() client id: f_00005-3-0 loss: 1.022552  [   32/  146]
train() client id: f_00005-3-1 loss: 0.927232  [   64/  146]
train() client id: f_00005-3-2 loss: 0.755353  [   96/  146]
train() client id: f_00005-3-3 loss: 0.730861  [  128/  146]
train() client id: f_00005-4-0 loss: 0.741491  [   32/  146]
train() client id: f_00005-4-1 loss: 0.935170  [   64/  146]
train() client id: f_00005-4-2 loss: 0.944701  [   96/  146]
train() client id: f_00005-4-3 loss: 0.866645  [  128/  146]
train() client id: f_00005-5-0 loss: 0.805704  [   32/  146]
train() client id: f_00005-5-1 loss: 0.941525  [   64/  146]
train() client id: f_00005-5-2 loss: 0.869651  [   96/  146]
train() client id: f_00005-5-3 loss: 0.776888  [  128/  146]
train() client id: f_00005-6-0 loss: 0.847362  [   32/  146]
train() client id: f_00005-6-1 loss: 0.917743  [   64/  146]
train() client id: f_00005-6-2 loss: 0.897892  [   96/  146]
train() client id: f_00005-6-3 loss: 0.867964  [  128/  146]
train() client id: f_00005-7-0 loss: 0.874546  [   32/  146]
train() client id: f_00005-7-1 loss: 0.942968  [   64/  146]
train() client id: f_00005-7-2 loss: 0.805788  [   96/  146]
train() client id: f_00005-7-3 loss: 0.902162  [  128/  146]
train() client id: f_00005-8-0 loss: 0.868928  [   32/  146]
train() client id: f_00005-8-1 loss: 0.839090  [   64/  146]
train() client id: f_00005-8-2 loss: 0.900521  [   96/  146]
train() client id: f_00005-8-3 loss: 0.863394  [  128/  146]
train() client id: f_00005-9-0 loss: 0.899263  [   32/  146]
train() client id: f_00005-9-1 loss: 0.884031  [   64/  146]
train() client id: f_00005-9-2 loss: 0.755636  [   96/  146]
train() client id: f_00005-9-3 loss: 0.920423  [  128/  146]
train() client id: f_00005-10-0 loss: 0.843165  [   32/  146]
train() client id: f_00005-10-1 loss: 0.989530  [   64/  146]
train() client id: f_00005-10-2 loss: 0.870382  [   96/  146]
train() client id: f_00005-10-3 loss: 0.745657  [  128/  146]
train() client id: f_00005-11-0 loss: 0.900783  [   32/  146]
train() client id: f_00005-11-1 loss: 0.856400  [   64/  146]
train() client id: f_00005-11-2 loss: 0.949843  [   96/  146]
train() client id: f_00005-11-3 loss: 0.708128  [  128/  146]
train() client id: f_00006-0-0 loss: 0.891799  [   32/   54]
train() client id: f_00006-1-0 loss: 0.959217  [   32/   54]
train() client id: f_00006-2-0 loss: 0.942643  [   32/   54]
train() client id: f_00006-3-0 loss: 0.949849  [   32/   54]
train() client id: f_00006-4-0 loss: 0.891803  [   32/   54]
train() client id: f_00006-5-0 loss: 0.948183  [   32/   54]
train() client id: f_00006-6-0 loss: 0.884160  [   32/   54]
train() client id: f_00006-7-0 loss: 0.881244  [   32/   54]
train() client id: f_00006-8-0 loss: 0.896137  [   32/   54]
train() client id: f_00006-9-0 loss: 0.892918  [   32/   54]
train() client id: f_00006-10-0 loss: 0.882959  [   32/   54]
train() client id: f_00006-11-0 loss: 0.956378  [   32/   54]
train() client id: f_00007-0-0 loss: 0.830705  [   32/  179]
train() client id: f_00007-0-1 loss: 0.822925  [   64/  179]
train() client id: f_00007-0-2 loss: 0.829504  [   96/  179]
train() client id: f_00007-0-3 loss: 0.741046  [  128/  179]
train() client id: f_00007-0-4 loss: 0.751120  [  160/  179]
train() client id: f_00007-1-0 loss: 0.726623  [   32/  179]
train() client id: f_00007-1-1 loss: 0.719165  [   64/  179]
train() client id: f_00007-1-2 loss: 0.833129  [   96/  179]
train() client id: f_00007-1-3 loss: 0.657881  [  128/  179]
train() client id: f_00007-1-4 loss: 0.786046  [  160/  179]
train() client id: f_00007-2-0 loss: 0.713553  [   32/  179]
train() client id: f_00007-2-1 loss: 0.797606  [   64/  179]
train() client id: f_00007-2-2 loss: 0.714125  [   96/  179]
train() client id: f_00007-2-3 loss: 0.785804  [  128/  179]
train() client id: f_00007-2-4 loss: 0.667564  [  160/  179]
train() client id: f_00007-3-0 loss: 0.688962  [   32/  179]
train() client id: f_00007-3-1 loss: 0.673410  [   64/  179]
train() client id: f_00007-3-2 loss: 0.644557  [   96/  179]
train() client id: f_00007-3-3 loss: 0.773040  [  128/  179]
train() client id: f_00007-3-4 loss: 0.749419  [  160/  179]
train() client id: f_00007-4-0 loss: 0.657341  [   32/  179]
train() client id: f_00007-4-1 loss: 0.763142  [   64/  179]
train() client id: f_00007-4-2 loss: 0.679645  [   96/  179]
train() client id: f_00007-4-3 loss: 0.687526  [  128/  179]
train() client id: f_00007-4-4 loss: 0.628915  [  160/  179]
train() client id: f_00007-5-0 loss: 0.745854  [   32/  179]
train() client id: f_00007-5-1 loss: 0.601992  [   64/  179]
train() client id: f_00007-5-2 loss: 0.704989  [   96/  179]
train() client id: f_00007-5-3 loss: 0.641993  [  128/  179]
train() client id: f_00007-5-4 loss: 0.716130  [  160/  179]
train() client id: f_00007-6-0 loss: 0.657308  [   32/  179]
train() client id: f_00007-6-1 loss: 0.727881  [   64/  179]
train() client id: f_00007-6-2 loss: 0.685664  [   96/  179]
train() client id: f_00007-6-3 loss: 0.626114  [  128/  179]
train() client id: f_00007-6-4 loss: 0.689426  [  160/  179]
train() client id: f_00007-7-0 loss: 0.685578  [   32/  179]
train() client id: f_00007-7-1 loss: 0.581072  [   64/  179]
train() client id: f_00007-7-2 loss: 0.695144  [   96/  179]
train() client id: f_00007-7-3 loss: 0.587665  [  128/  179]
train() client id: f_00007-7-4 loss: 0.781910  [  160/  179]
train() client id: f_00007-8-0 loss: 0.634557  [   32/  179]
train() client id: f_00007-8-1 loss: 0.620868  [   64/  179]
train() client id: f_00007-8-2 loss: 0.661119  [   96/  179]
train() client id: f_00007-8-3 loss: 0.622863  [  128/  179]
train() client id: f_00007-8-4 loss: 0.679825  [  160/  179]
train() client id: f_00007-9-0 loss: 0.722490  [   32/  179]
train() client id: f_00007-9-1 loss: 0.581592  [   64/  179]
train() client id: f_00007-9-2 loss: 0.610280  [   96/  179]
train() client id: f_00007-9-3 loss: 0.637271  [  128/  179]
train() client id: f_00007-9-4 loss: 0.692809  [  160/  179]
train() client id: f_00007-10-0 loss: 0.692310  [   32/  179]
train() client id: f_00007-10-1 loss: 0.680280  [   64/  179]
train() client id: f_00007-10-2 loss: 0.559707  [   96/  179]
train() client id: f_00007-10-3 loss: 0.735297  [  128/  179]
train() client id: f_00007-10-4 loss: 0.566518  [  160/  179]
train() client id: f_00007-11-0 loss: 0.599571  [   32/  179]
train() client id: f_00007-11-1 loss: 0.628517  [   64/  179]
train() client id: f_00007-11-2 loss: 0.607159  [   96/  179]
train() client id: f_00007-11-3 loss: 0.675417  [  128/  179]
train() client id: f_00007-11-4 loss: 0.694962  [  160/  179]
train() client id: f_00008-0-0 loss: 0.935829  [   32/  130]
train() client id: f_00008-0-1 loss: 0.875486  [   64/  130]
train() client id: f_00008-0-2 loss: 0.973687  [   96/  130]
train() client id: f_00008-0-3 loss: 0.970717  [  128/  130]
train() client id: f_00008-1-0 loss: 0.883501  [   32/  130]
train() client id: f_00008-1-1 loss: 0.908367  [   64/  130]
train() client id: f_00008-1-2 loss: 0.954295  [   96/  130]
train() client id: f_00008-1-3 loss: 0.996580  [  128/  130]
train() client id: f_00008-2-0 loss: 0.995999  [   32/  130]
train() client id: f_00008-2-1 loss: 0.921204  [   64/  130]
train() client id: f_00008-2-2 loss: 0.895127  [   96/  130]
train() client id: f_00008-2-3 loss: 0.899594  [  128/  130]
train() client id: f_00008-3-0 loss: 1.094629  [   32/  130]
train() client id: f_00008-3-1 loss: 0.945984  [   64/  130]
train() client id: f_00008-3-2 loss: 0.816150  [   96/  130]
train() client id: f_00008-3-3 loss: 0.870617  [  128/  130]
train() client id: f_00008-4-0 loss: 0.911871  [   32/  130]
train() client id: f_00008-4-1 loss: 0.983583  [   64/  130]
train() client id: f_00008-4-2 loss: 0.854242  [   96/  130]
train() client id: f_00008-4-3 loss: 0.987806  [  128/  130]
train() client id: f_00008-5-0 loss: 0.879759  [   32/  130]
train() client id: f_00008-5-1 loss: 0.893714  [   64/  130]
train() client id: f_00008-5-2 loss: 0.853404  [   96/  130]
train() client id: f_00008-5-3 loss: 1.103912  [  128/  130]
train() client id: f_00008-6-0 loss: 0.993827  [   32/  130]
train() client id: f_00008-6-1 loss: 1.002277  [   64/  130]
train() client id: f_00008-6-2 loss: 0.793647  [   96/  130]
train() client id: f_00008-6-3 loss: 0.940923  [  128/  130]
train() client id: f_00008-7-0 loss: 0.897558  [   32/  130]
train() client id: f_00008-7-1 loss: 1.038948  [   64/  130]
train() client id: f_00008-7-2 loss: 0.963454  [   96/  130]
train() client id: f_00008-7-3 loss: 0.825550  [  128/  130]
train() client id: f_00008-8-0 loss: 0.980205  [   32/  130]
train() client id: f_00008-8-1 loss: 0.957868  [   64/  130]
train() client id: f_00008-8-2 loss: 0.924013  [   96/  130]
train() client id: f_00008-8-3 loss: 0.832251  [  128/  130]
train() client id: f_00008-9-0 loss: 0.944054  [   32/  130]
train() client id: f_00008-9-1 loss: 0.889667  [   64/  130]
train() client id: f_00008-9-2 loss: 1.019199  [   96/  130]
train() client id: f_00008-9-3 loss: 0.880276  [  128/  130]
train() client id: f_00008-10-0 loss: 0.859623  [   32/  130]
train() client id: f_00008-10-1 loss: 0.916448  [   64/  130]
train() client id: f_00008-10-2 loss: 0.959522  [   96/  130]
train() client id: f_00008-10-3 loss: 0.990339  [  128/  130]
train() client id: f_00008-11-0 loss: 1.024364  [   32/  130]
train() client id: f_00008-11-1 loss: 0.984140  [   64/  130]
train() client id: f_00008-11-2 loss: 0.878178  [   96/  130]
train() client id: f_00008-11-3 loss: 0.813125  [  128/  130]
train() client id: f_00009-0-0 loss: 1.267378  [   32/  118]
train() client id: f_00009-0-1 loss: 1.234529  [   64/  118]
train() client id: f_00009-0-2 loss: 1.164366  [   96/  118]
train() client id: f_00009-1-0 loss: 1.179500  [   32/  118]
train() client id: f_00009-1-1 loss: 1.267299  [   64/  118]
train() client id: f_00009-1-2 loss: 1.065451  [   96/  118]
train() client id: f_00009-2-0 loss: 1.102759  [   32/  118]
train() client id: f_00009-2-1 loss: 1.123068  [   64/  118]
train() client id: f_00009-2-2 loss: 1.243745  [   96/  118]
train() client id: f_00009-3-0 loss: 1.199610  [   32/  118]
train() client id: f_00009-3-1 loss: 1.091981  [   64/  118]
train() client id: f_00009-3-2 loss: 1.051988  [   96/  118]
train() client id: f_00009-4-0 loss: 1.106152  [   32/  118]
train() client id: f_00009-4-1 loss: 1.110614  [   64/  118]
train() client id: f_00009-4-2 loss: 1.091966  [   96/  118]
train() client id: f_00009-5-0 loss: 1.097643  [   32/  118]
train() client id: f_00009-5-1 loss: 1.055082  [   64/  118]
train() client id: f_00009-5-2 loss: 1.131477  [   96/  118]
train() client id: f_00009-6-0 loss: 1.001211  [   32/  118]
train() client id: f_00009-6-1 loss: 1.209018  [   64/  118]
train() client id: f_00009-6-2 loss: 0.990457  [   96/  118]
train() client id: f_00009-7-0 loss: 1.093550  [   32/  118]
train() client id: f_00009-7-1 loss: 1.087278  [   64/  118]
train() client id: f_00009-7-2 loss: 1.039457  [   96/  118]
train() client id: f_00009-8-0 loss: 1.102565  [   32/  118]
train() client id: f_00009-8-1 loss: 1.014163  [   64/  118]
train() client id: f_00009-8-2 loss: 1.071568  [   96/  118]
train() client id: f_00009-9-0 loss: 1.052248  [   32/  118]
train() client id: f_00009-9-1 loss: 1.016315  [   64/  118]
train() client id: f_00009-9-2 loss: 1.132414  [   96/  118]
train() client id: f_00009-10-0 loss: 1.043607  [   32/  118]
train() client id: f_00009-10-1 loss: 1.061390  [   64/  118]
train() client id: f_00009-10-2 loss: 1.056733  [   96/  118]
train() client id: f_00009-11-0 loss: 1.074292  [   32/  118]
train() client id: f_00009-11-1 loss: 1.033421  [   64/  118]
train() client id: f_00009-11-2 loss: 1.085079  [   96/  118]
At round 5 accuracy: 0.6259946949602122
At round 5 training accuracy: 0.5674044265593562
At round 5 training loss: 0.9064685989128791
update_location
xs = [-3.9056584   4.20031788 45.00902392 18.81129433 -9.02070377  3.95640986
 -7.44319194 -1.32485185 29.66397685 -2.06087855]
ys = [ 37.5879595   20.55583871   1.32061395  -7.45517586   9.35018685
 -17.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [106.90233331 102.17722435 109.6702159  102.02668495 100.84046356
 101.54312677 100.31097472 100.09599397 105.7762804  100.10124413]
dists_bs = [219.44325599 236.57082002 280.3007309  266.18172178 234.49730782
 262.60387904 244.18419069 249.53494358 258.20894888 243.20450851]
uav_gains = [8.46306925e-11 9.47572782e-11 7.93911850e-11 9.51072087e-11
 9.79289548e-11 9.62435611e-11 9.92263936e-11 9.97600464e-11
 8.69011873e-11 9.97469659e-11]
bs_gains = [3.07320703e-11 2.49001977e-11 1.54862491e-11 1.78976100e-11
 2.55216104e-11 1.85887806e-11 2.27868853e-11 2.14450097e-11
 1.94883232e-11 2.30448319e-11]
Round 6
-------------------------------
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.20035717 21.31407924 10.04322819  3.59039783 24.57615325 11.85884139
  4.4646498  14.41376646 10.58251293  9.61787463]
obj_prev = 120.66186087163511
eta_min = 7.409354424179371e-10	eta_max = 0.91885879376386
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 28.063470776703085	eta = 0.909090909090909
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 48.384419986505044	eta = 0.5272822567213747
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 38.70245321762515	eta = 0.6591893805073021
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.96801720495558	eta = 0.6901167032896544
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88278882583632	eta = 0.6917114180576249
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88256856080987	eta = 0.6917155490018552
eta = 0.6917155490018552
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.03041438 0.06396674 0.02993161 0.01037951 0.07386349 0.03524206
 0.01303473 0.04320773 0.03137991 0.0284833 ]
ene_total = [3.12833392 6.13288912 3.09403261 1.41867893 6.96750532 3.75135634
 1.63958615 4.19625148 3.41497814 3.13895655]
ti_comp = [0.28546116 0.26646804 0.2846808  0.28683999 0.2669565  0.2602256
 0.28732669 0.28738774 0.26129447 0.26489704]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.15785733e-05 2.30384101e-04 2.06801576e-05 8.49436889e-07
 3.53417801e-04 4.03983548e-05 1.67661934e-06 6.10418604e-05
 2.82862087e-05 2.05824434e-05]
ene_total = [0.55252256 0.73118701 0.55906002 0.53908008 0.73747448 0.76799073
 0.53502532 0.53953919 0.7579054  0.72672045]
optimize_network iter = 0 obj = 6.446505230599096
eta = 0.6917155490018552
freqs = [5.32723555e+07 1.20027048e+08 5.25704736e+07 1.80928566e+07
 1.38343675e+08 6.77144291e+07 2.26827740e+07 7.51732237e+07
 6.00470262e+07 5.37629723e+07]
eta_min = 0.6801612059033121	eta_max = 0.6917155490018475
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 0.06073437862890649	eta = 0.9090909090909091
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 21.69496157536947	eta = 0.002544972079761971
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.3026899172665374	eta = 0.023977640700475235
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.226369694175787	eta = 0.024799597131268097
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.2263378919337997	eta = 0.024799951382431865
eta = 0.024799951382431865
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16436107e-04 2.31078474e-03 2.07424872e-04 8.51997075e-06
 3.54482996e-03 4.05201146e-04 1.68167264e-05 6.12258394e-04
 2.83714628e-04 2.06444785e-04]
ene_total = [0.17988639 0.28688359 0.18173569 0.17062183 0.31863717 0.25255426
 0.16954014 0.18532952 0.24643571 0.23471359]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 1 obj = 6.216126697450631
eta = 0.6801612059033121
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
eta_min = 0.6801612059033234	eta_max = 0.680161205903311
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 0.06036643683170785	eta = 0.909090909090909
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 21.694610889380957	eta = 0.0025295949864110306
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.300984648578189	eta = 0.023850041316801688
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.2251002879015918	eta = 0.024663418200206223
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.225068999906738	eta = 0.02466376500693525
eta = 0.02466376500693525
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16630671e-04 2.29842707e-03 2.07561279e-04 8.53123712e-06
 3.52647900e-03 4.02127407e-04 1.68414620e-05 6.13170349e-04
 2.81673164e-04 2.05226390e-04]
ene_total = [0.17986649 0.2865125  0.18171397 0.17059831 0.31810109 0.25243665
 0.16951713 0.18532807 0.24634661 0.23464818]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 2 obj = 6.216126697450847
eta = 0.6801612059033234
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
Done!
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.14422044e-05 2.27499377e-04 2.05445117e-05 8.44425808e-07
 3.49052528e-04 3.98027573e-05 1.66697572e-06 6.06918856e-05
 2.78801405e-05 2.03134033e-05]
ene_total = [0.00651925 0.00862462 0.00659639 0.00636077 0.00869732 0.00906117
 0.00631292 0.00636584 0.00894236 0.00857453]
At round 6 energy consumption: 0.07605515902799734
At round 6 eta: 0.6801612059033234
At round 6 a_n: 26.12732777443847
At round 6 local rounds: 12.620787147614399
At round 6 global rounds: 81.68905166188517
gradient difference: 0.4510670602321625
train() client id: f_00000-0-0 loss: 1.225268  [   32/  126]
train() client id: f_00000-0-1 loss: 1.187363  [   64/  126]
train() client id: f_00000-0-2 loss: 1.126233  [   96/  126]
train() client id: f_00000-1-0 loss: 1.100578  [   32/  126]
train() client id: f_00000-1-1 loss: 1.048913  [   64/  126]
train() client id: f_00000-1-2 loss: 1.102189  [   96/  126]
train() client id: f_00000-2-0 loss: 1.097013  [   32/  126]
train() client id: f_00000-2-1 loss: 1.043269  [   64/  126]
train() client id: f_00000-2-2 loss: 1.057156  [   96/  126]
train() client id: f_00000-3-0 loss: 1.038675  [   32/  126]
train() client id: f_00000-3-1 loss: 1.046493  [   64/  126]
train() client id: f_00000-3-2 loss: 0.999272  [   96/  126]
train() client id: f_00000-4-0 loss: 0.952990  [   32/  126]
train() client id: f_00000-4-1 loss: 0.967238  [   64/  126]
train() client id: f_00000-4-2 loss: 0.984245  [   96/  126]
train() client id: f_00000-5-0 loss: 0.938052  [   32/  126]
train() client id: f_00000-5-1 loss: 0.968765  [   64/  126]
train() client id: f_00000-5-2 loss: 0.991589  [   96/  126]
train() client id: f_00000-6-0 loss: 0.948705  [   32/  126]
train() client id: f_00000-6-1 loss: 0.974525  [   64/  126]
train() client id: f_00000-6-2 loss: 0.930925  [   96/  126]
train() client id: f_00000-7-0 loss: 0.935559  [   32/  126]
train() client id: f_00000-7-1 loss: 0.923181  [   64/  126]
train() client id: f_00000-7-2 loss: 0.949576  [   96/  126]
train() client id: f_00000-8-0 loss: 0.954060  [   32/  126]
train() client id: f_00000-8-1 loss: 0.910513  [   64/  126]
train() client id: f_00000-8-2 loss: 0.941530  [   96/  126]
train() client id: f_00000-9-0 loss: 1.019523  [   32/  126]
train() client id: f_00000-9-1 loss: 0.967925  [   64/  126]
train() client id: f_00000-9-2 loss: 0.908193  [   96/  126]
train() client id: f_00000-10-0 loss: 0.946774  [   32/  126]
train() client id: f_00000-10-1 loss: 0.948881  [   64/  126]
train() client id: f_00000-10-2 loss: 1.005189  [   96/  126]
train() client id: f_00000-11-0 loss: 0.928499  [   32/  126]
train() client id: f_00000-11-1 loss: 0.958561  [   64/  126]
train() client id: f_00000-11-2 loss: 0.955550  [   96/  126]
train() client id: f_00001-0-0 loss: 0.599541  [   32/  265]
train() client id: f_00001-0-1 loss: 0.606735  [   64/  265]
train() client id: f_00001-0-2 loss: 0.633325  [   96/  265]
train() client id: f_00001-0-3 loss: 0.590606  [  128/  265]
train() client id: f_00001-0-4 loss: 0.628619  [  160/  265]
train() client id: f_00001-0-5 loss: 0.665731  [  192/  265]
train() client id: f_00001-0-6 loss: 0.547866  [  224/  265]
train() client id: f_00001-0-7 loss: 0.575525  [  256/  265]
train() client id: f_00001-1-0 loss: 0.601904  [   32/  265]
train() client id: f_00001-1-1 loss: 0.533960  [   64/  265]
train() client id: f_00001-1-2 loss: 0.595351  [   96/  265]
train() client id: f_00001-1-3 loss: 0.574524  [  128/  265]
train() client id: f_00001-1-4 loss: 0.715553  [  160/  265]
train() client id: f_00001-1-5 loss: 0.578028  [  192/  265]
train() client id: f_00001-1-6 loss: 0.558000  [  224/  265]
train() client id: f_00001-1-7 loss: 0.551800  [  256/  265]
train() client id: f_00001-2-0 loss: 0.571217  [   32/  265]
train() client id: f_00001-2-1 loss: 0.515633  [   64/  265]
train() client id: f_00001-2-2 loss: 0.557005  [   96/  265]
train() client id: f_00001-2-3 loss: 0.599245  [  128/  265]
train() client id: f_00001-2-4 loss: 0.534934  [  160/  265]
train() client id: f_00001-2-5 loss: 0.652694  [  192/  265]
train() client id: f_00001-2-6 loss: 0.625218  [  224/  265]
train() client id: f_00001-2-7 loss: 0.546176  [  256/  265]
train() client id: f_00001-3-0 loss: 0.652673  [   32/  265]
train() client id: f_00001-3-1 loss: 0.522936  [   64/  265]
train() client id: f_00001-3-2 loss: 0.551864  [   96/  265]
train() client id: f_00001-3-3 loss: 0.589179  [  128/  265]
train() client id: f_00001-3-4 loss: 0.541765  [  160/  265]
train() client id: f_00001-3-5 loss: 0.558511  [  192/  265]
train() client id: f_00001-3-6 loss: 0.495832  [  224/  265]
train() client id: f_00001-3-7 loss: 0.585897  [  256/  265]
train() client id: f_00001-4-0 loss: 0.527781  [   32/  265]
train() client id: f_00001-4-1 loss: 0.612025  [   64/  265]
train() client id: f_00001-4-2 loss: 0.519414  [   96/  265]
train() client id: f_00001-4-3 loss: 0.555981  [  128/  265]
train() client id: f_00001-4-4 loss: 0.635500  [  160/  265]
train() client id: f_00001-4-5 loss: 0.526581  [  192/  265]
train() client id: f_00001-4-6 loss: 0.480546  [  224/  265]
train() client id: f_00001-4-7 loss: 0.603091  [  256/  265]
train() client id: f_00001-5-0 loss: 0.487798  [   32/  265]
train() client id: f_00001-5-1 loss: 0.473889  [   64/  265]
train() client id: f_00001-5-2 loss: 0.644964  [   96/  265]
train() client id: f_00001-5-3 loss: 0.520816  [  128/  265]
train() client id: f_00001-5-4 loss: 0.541144  [  160/  265]
train() client id: f_00001-5-5 loss: 0.527820  [  192/  265]
train() client id: f_00001-5-6 loss: 0.514826  [  224/  265]
train() client id: f_00001-5-7 loss: 0.718893  [  256/  265]
train() client id: f_00001-6-0 loss: 0.528609  [   32/  265]
train() client id: f_00001-6-1 loss: 0.531565  [   64/  265]
train() client id: f_00001-6-2 loss: 0.512652  [   96/  265]
train() client id: f_00001-6-3 loss: 0.509076  [  128/  265]
train() client id: f_00001-6-4 loss: 0.477854  [  160/  265]
train() client id: f_00001-6-5 loss: 0.573625  [  192/  265]
train() client id: f_00001-6-6 loss: 0.583455  [  224/  265]
train() client id: f_00001-6-7 loss: 0.616295  [  256/  265]
train() client id: f_00001-7-0 loss: 0.535942  [   32/  265]
train() client id: f_00001-7-1 loss: 0.578345  [   64/  265]
train() client id: f_00001-7-2 loss: 0.504623  [   96/  265]
train() client id: f_00001-7-3 loss: 0.634004  [  128/  265]
train() client id: f_00001-7-4 loss: 0.516945  [  160/  265]
train() client id: f_00001-7-5 loss: 0.472272  [  192/  265]
train() client id: f_00001-7-6 loss: 0.499795  [  224/  265]
train() client id: f_00001-7-7 loss: 0.627321  [  256/  265]
train() client id: f_00001-8-0 loss: 0.642608  [   32/  265]
train() client id: f_00001-8-1 loss: 0.467951  [   64/  265]
train() client id: f_00001-8-2 loss: 0.663168  [   96/  265]
train() client id: f_00001-8-3 loss: 0.514925  [  128/  265]
train() client id: f_00001-8-4 loss: 0.471336  [  160/  265]
train() client id: f_00001-8-5 loss: 0.516337  [  192/  265]
train() client id: f_00001-8-6 loss: 0.583041  [  224/  265]
train() client id: f_00001-8-7 loss: 0.514488  [  256/  265]
train() client id: f_00001-9-0 loss: 0.520866  [   32/  265]
train() client id: f_00001-9-1 loss: 0.478525  [   64/  265]
train() client id: f_00001-9-2 loss: 0.622487  [   96/  265]
train() client id: f_00001-9-3 loss: 0.491437  [  128/  265]
train() client id: f_00001-9-4 loss: 0.578235  [  160/  265]
train() client id: f_00001-9-5 loss: 0.522632  [  192/  265]
train() client id: f_00001-9-6 loss: 0.585208  [  224/  265]
train() client id: f_00001-9-7 loss: 0.495026  [  256/  265]
train() client id: f_00001-10-0 loss: 0.642867  [   32/  265]
train() client id: f_00001-10-1 loss: 0.574911  [   64/  265]
train() client id: f_00001-10-2 loss: 0.513034  [   96/  265]
train() client id: f_00001-10-3 loss: 0.537476  [  128/  265]
train() client id: f_00001-10-4 loss: 0.524109  [  160/  265]
train() client id: f_00001-10-5 loss: 0.503578  [  192/  265]
train() client id: f_00001-10-6 loss: 0.435685  [  224/  265]
train() client id: f_00001-10-7 loss: 0.558454  [  256/  265]
train() client id: f_00001-11-0 loss: 0.582944  [   32/  265]
train() client id: f_00001-11-1 loss: 0.622285  [   64/  265]
train() client id: f_00001-11-2 loss: 0.501504  [   96/  265]
train() client id: f_00001-11-3 loss: 0.611866  [  128/  265]
train() client id: f_00001-11-4 loss: 0.550900  [  160/  265]
train() client id: f_00001-11-5 loss: 0.513944  [  192/  265]
train() client id: f_00001-11-6 loss: 0.487676  [  224/  265]
train() client id: f_00001-11-7 loss: 0.496819  [  256/  265]
train() client id: f_00002-0-0 loss: 1.156855  [   32/  124]
train() client id: f_00002-0-1 loss: 1.114588  [   64/  124]
train() client id: f_00002-0-2 loss: 1.161978  [   96/  124]
train() client id: f_00002-1-0 loss: 1.189347  [   32/  124]
train() client id: f_00002-1-1 loss: 1.084506  [   64/  124]
train() client id: f_00002-1-2 loss: 1.061282  [   96/  124]
train() client id: f_00002-2-0 loss: 1.057318  [   32/  124]
train() client id: f_00002-2-1 loss: 1.149432  [   64/  124]
train() client id: f_00002-2-2 loss: 1.055435  [   96/  124]
train() client id: f_00002-3-0 loss: 1.028161  [   32/  124]
train() client id: f_00002-3-1 loss: 1.005282  [   64/  124]
train() client id: f_00002-3-2 loss: 1.138234  [   96/  124]
train() client id: f_00002-4-0 loss: 1.053645  [   32/  124]
train() client id: f_00002-4-1 loss: 0.910221  [   64/  124]
train() client id: f_00002-4-2 loss: 0.977073  [   96/  124]
train() client id: f_00002-5-0 loss: 0.970688  [   32/  124]
train() client id: f_00002-5-1 loss: 1.050922  [   64/  124]
train() client id: f_00002-5-2 loss: 1.004853  [   96/  124]
train() client id: f_00002-6-0 loss: 1.002510  [   32/  124]
train() client id: f_00002-6-1 loss: 0.939424  [   64/  124]
train() client id: f_00002-6-2 loss: 0.931922  [   96/  124]
train() client id: f_00002-7-0 loss: 1.089651  [   32/  124]
train() client id: f_00002-7-1 loss: 0.976812  [   64/  124]
train() client id: f_00002-7-2 loss: 0.889697  [   96/  124]
train() client id: f_00002-8-0 loss: 0.888523  [   32/  124]
train() client id: f_00002-8-1 loss: 0.985038  [   64/  124]
train() client id: f_00002-8-2 loss: 0.945162  [   96/  124]
train() client id: f_00002-9-0 loss: 0.956561  [   32/  124]
train() client id: f_00002-9-1 loss: 0.951247  [   64/  124]
train() client id: f_00002-9-2 loss: 0.985269  [   96/  124]
train() client id: f_00002-10-0 loss: 0.877085  [   32/  124]
train() client id: f_00002-10-1 loss: 1.062744  [   64/  124]
train() client id: f_00002-10-2 loss: 0.836296  [   96/  124]
train() client id: f_00002-11-0 loss: 0.932718  [   32/  124]
train() client id: f_00002-11-1 loss: 1.003519  [   64/  124]
train() client id: f_00002-11-2 loss: 0.880564  [   96/  124]
train() client id: f_00003-0-0 loss: 1.136331  [   32/   43]
train() client id: f_00003-1-0 loss: 1.066496  [   32/   43]
train() client id: f_00003-2-0 loss: 1.017476  [   32/   43]
train() client id: f_00003-3-0 loss: 1.066868  [   32/   43]
train() client id: f_00003-4-0 loss: 1.023393  [   32/   43]
train() client id: f_00003-5-0 loss: 1.021501  [   32/   43]
train() client id: f_00003-6-0 loss: 1.053552  [   32/   43]
train() client id: f_00003-7-0 loss: 1.052501  [   32/   43]
train() client id: f_00003-8-0 loss: 0.994630  [   32/   43]
train() client id: f_00003-9-0 loss: 1.053856  [   32/   43]
train() client id: f_00003-10-0 loss: 1.029021  [   32/   43]
train() client id: f_00003-11-0 loss: 0.991147  [   32/   43]
train() client id: f_00004-0-0 loss: 0.904543  [   32/  306]
train() client id: f_00004-0-1 loss: 0.919005  [   64/  306]
train() client id: f_00004-0-2 loss: 0.949121  [   96/  306]
train() client id: f_00004-0-3 loss: 0.913677  [  128/  306]
train() client id: f_00004-0-4 loss: 0.862255  [  160/  306]
train() client id: f_00004-0-5 loss: 0.865255  [  192/  306]
train() client id: f_00004-0-6 loss: 0.960681  [  224/  306]
train() client id: f_00004-0-7 loss: 0.985852  [  256/  306]
train() client id: f_00004-0-8 loss: 0.938591  [  288/  306]
train() client id: f_00004-1-0 loss: 1.005270  [   32/  306]
train() client id: f_00004-1-1 loss: 0.885864  [   64/  306]
train() client id: f_00004-1-2 loss: 0.949872  [   96/  306]
train() client id: f_00004-1-3 loss: 0.903862  [  128/  306]
train() client id: f_00004-1-4 loss: 0.879659  [  160/  306]
train() client id: f_00004-1-5 loss: 0.801916  [  192/  306]
train() client id: f_00004-1-6 loss: 0.936446  [  224/  306]
train() client id: f_00004-1-7 loss: 0.919459  [  256/  306]
train() client id: f_00004-1-8 loss: 0.973839  [  288/  306]
train() client id: f_00004-2-0 loss: 0.881494  [   32/  306]
train() client id: f_00004-2-1 loss: 0.962072  [   64/  306]
train() client id: f_00004-2-2 loss: 1.022904  [   96/  306]
train() client id: f_00004-2-3 loss: 0.896081  [  128/  306]
train() client id: f_00004-2-4 loss: 0.906577  [  160/  306]
train() client id: f_00004-2-5 loss: 0.923037  [  192/  306]
train() client id: f_00004-2-6 loss: 0.875159  [  224/  306]
train() client id: f_00004-2-7 loss: 0.975897  [  256/  306]
train() client id: f_00004-2-8 loss: 0.855788  [  288/  306]
train() client id: f_00004-3-0 loss: 0.887965  [   32/  306]
train() client id: f_00004-3-1 loss: 0.818028  [   64/  306]
train() client id: f_00004-3-2 loss: 0.985711  [   96/  306]
train() client id: f_00004-3-3 loss: 0.871074  [  128/  306]
train() client id: f_00004-3-4 loss: 0.975137  [  160/  306]
train() client id: f_00004-3-5 loss: 0.989969  [  192/  306]
train() client id: f_00004-3-6 loss: 0.940705  [  224/  306]
train() client id: f_00004-3-7 loss: 0.827431  [  256/  306]
train() client id: f_00004-3-8 loss: 0.895460  [  288/  306]
train() client id: f_00004-4-0 loss: 0.872428  [   32/  306]
train() client id: f_00004-4-1 loss: 0.937711  [   64/  306]
train() client id: f_00004-4-2 loss: 1.027024  [   96/  306]
train() client id: f_00004-4-3 loss: 0.870817  [  128/  306]
train() client id: f_00004-4-4 loss: 0.805985  [  160/  306]
train() client id: f_00004-4-5 loss: 0.947369  [  192/  306]
train() client id: f_00004-4-6 loss: 0.927162  [  224/  306]
train() client id: f_00004-4-7 loss: 0.951962  [  256/  306]
train() client id: f_00004-4-8 loss: 0.886551  [  288/  306]
train() client id: f_00004-5-0 loss: 0.910404  [   32/  306]
train() client id: f_00004-5-1 loss: 0.845317  [   64/  306]
train() client id: f_00004-5-2 loss: 0.878727  [   96/  306]
train() client id: f_00004-5-3 loss: 0.854632  [  128/  306]
train() client id: f_00004-5-4 loss: 0.836227  [  160/  306]
train() client id: f_00004-5-5 loss: 0.958675  [  192/  306]
train() client id: f_00004-5-6 loss: 0.968519  [  224/  306]
train() client id: f_00004-5-7 loss: 1.013617  [  256/  306]
train() client id: f_00004-5-8 loss: 0.927461  [  288/  306]
train() client id: f_00004-6-0 loss: 0.808643  [   32/  306]
train() client id: f_00004-6-1 loss: 1.021553  [   64/  306]
train() client id: f_00004-6-2 loss: 1.061626  [   96/  306]
train() client id: f_00004-6-3 loss: 0.852095  [  128/  306]
train() client id: f_00004-6-4 loss: 0.921091  [  160/  306]
train() client id: f_00004-6-5 loss: 0.876453  [  192/  306]
train() client id: f_00004-6-6 loss: 0.949616  [  224/  306]
train() client id: f_00004-6-7 loss: 0.882263  [  256/  306]
train() client id: f_00004-6-8 loss: 0.913844  [  288/  306]
train() client id: f_00004-7-0 loss: 1.041314  [   32/  306]
train() client id: f_00004-7-1 loss: 0.975891  [   64/  306]
train() client id: f_00004-7-2 loss: 0.884876  [   96/  306]
train() client id: f_00004-7-3 loss: 0.795854  [  128/  306]
train() client id: f_00004-7-4 loss: 0.940630  [  160/  306]
train() client id: f_00004-7-5 loss: 0.968722  [  192/  306]
train() client id: f_00004-7-6 loss: 0.883327  [  224/  306]
train() client id: f_00004-7-7 loss: 0.906677  [  256/  306]
train() client id: f_00004-7-8 loss: 0.932460  [  288/  306]
train() client id: f_00004-8-0 loss: 0.995994  [   32/  306]
train() client id: f_00004-8-1 loss: 0.955698  [   64/  306]
train() client id: f_00004-8-2 loss: 0.874072  [   96/  306]
train() client id: f_00004-8-3 loss: 0.866477  [  128/  306]
train() client id: f_00004-8-4 loss: 0.947804  [  160/  306]
train() client id: f_00004-8-5 loss: 0.916457  [  192/  306]
train() client id: f_00004-8-6 loss: 0.888759  [  224/  306]
train() client id: f_00004-8-7 loss: 0.854354  [  256/  306]
train() client id: f_00004-8-8 loss: 0.973488  [  288/  306]
train() client id: f_00004-9-0 loss: 0.872080  [   32/  306]
train() client id: f_00004-9-1 loss: 0.939504  [   64/  306]
train() client id: f_00004-9-2 loss: 0.917328  [   96/  306]
train() client id: f_00004-9-3 loss: 0.815577  [  128/  306]
train() client id: f_00004-9-4 loss: 0.939757  [  160/  306]
train() client id: f_00004-9-5 loss: 0.893854  [  192/  306]
train() client id: f_00004-9-6 loss: 0.944144  [  224/  306]
train() client id: f_00004-9-7 loss: 0.953570  [  256/  306]
train() client id: f_00004-9-8 loss: 0.955467  [  288/  306]
train() client id: f_00004-10-0 loss: 1.020872  [   32/  306]
train() client id: f_00004-10-1 loss: 0.771579  [   64/  306]
train() client id: f_00004-10-2 loss: 0.827898  [   96/  306]
train() client id: f_00004-10-3 loss: 0.921841  [  128/  306]
train() client id: f_00004-10-4 loss: 1.025996  [  160/  306]
train() client id: f_00004-10-5 loss: 0.926452  [  192/  306]
train() client id: f_00004-10-6 loss: 0.917594  [  224/  306]
train() client id: f_00004-10-7 loss: 0.949487  [  256/  306]
train() client id: f_00004-10-8 loss: 0.920317  [  288/  306]
train() client id: f_00004-11-0 loss: 0.995614  [   32/  306]
train() client id: f_00004-11-1 loss: 0.909030  [   64/  306]
train() client id: f_00004-11-2 loss: 0.949493  [   96/  306]
train() client id: f_00004-11-3 loss: 0.860523  [  128/  306]
train() client id: f_00004-11-4 loss: 0.906836  [  160/  306]
train() client id: f_00004-11-5 loss: 0.959457  [  192/  306]
train() client id: f_00004-11-6 loss: 0.876449  [  224/  306]
train() client id: f_00004-11-7 loss: 0.933065  [  256/  306]
train() client id: f_00004-11-8 loss: 0.895003  [  288/  306]
train() client id: f_00005-0-0 loss: 0.919029  [   32/  146]
train() client id: f_00005-0-1 loss: 0.957442  [   64/  146]
train() client id: f_00005-0-2 loss: 1.014132  [   96/  146]
train() client id: f_00005-0-3 loss: 0.941284  [  128/  146]
train() client id: f_00005-1-0 loss: 1.016537  [   32/  146]
train() client id: f_00005-1-1 loss: 0.970600  [   64/  146]
train() client id: f_00005-1-2 loss: 0.938770  [   96/  146]
train() client id: f_00005-1-3 loss: 0.924762  [  128/  146]
train() client id: f_00005-2-0 loss: 0.852196  [   32/  146]
train() client id: f_00005-2-1 loss: 0.947050  [   64/  146]
train() client id: f_00005-2-2 loss: 0.922861  [   96/  146]
train() client id: f_00005-2-3 loss: 0.996045  [  128/  146]
train() client id: f_00005-3-0 loss: 0.952457  [   32/  146]
train() client id: f_00005-3-1 loss: 1.072827  [   64/  146]
train() client id: f_00005-3-2 loss: 0.834718  [   96/  146]
train() client id: f_00005-3-3 loss: 0.955374  [  128/  146]
train() client id: f_00005-4-0 loss: 0.940106  [   32/  146]
train() client id: f_00005-4-1 loss: 1.008094  [   64/  146]
train() client id: f_00005-4-2 loss: 1.074754  [   96/  146]
train() client id: f_00005-4-3 loss: 0.759696  [  128/  146]
train() client id: f_00005-5-0 loss: 1.023178  [   32/  146]
train() client id: f_00005-5-1 loss: 0.810191  [   64/  146]
train() client id: f_00005-5-2 loss: 1.036534  [   96/  146]
train() client id: f_00005-5-3 loss: 0.891731  [  128/  146]
train() client id: f_00005-6-0 loss: 0.897053  [   32/  146]
train() client id: f_00005-6-1 loss: 1.155362  [   64/  146]
train() client id: f_00005-6-2 loss: 0.793350  [   96/  146]
train() client id: f_00005-6-3 loss: 0.967587  [  128/  146]
train() client id: f_00005-7-0 loss: 0.931474  [   32/  146]
train() client id: f_00005-7-1 loss: 0.993543  [   64/  146]
train() client id: f_00005-7-2 loss: 0.969732  [   96/  146]
train() client id: f_00005-7-3 loss: 0.972170  [  128/  146]
train() client id: f_00005-8-0 loss: 0.984795  [   32/  146]
train() client id: f_00005-8-1 loss: 0.988729  [   64/  146]
train() client id: f_00005-8-2 loss: 0.932855  [   96/  146]
train() client id: f_00005-8-3 loss: 0.843686  [  128/  146]
train() client id: f_00005-9-0 loss: 0.915528  [   32/  146]
train() client id: f_00005-9-1 loss: 1.010667  [   64/  146]
train() client id: f_00005-9-2 loss: 0.790935  [   96/  146]
train() client id: f_00005-9-3 loss: 1.005234  [  128/  146]
train() client id: f_00005-10-0 loss: 0.935767  [   32/  146]
train() client id: f_00005-10-1 loss: 0.748921  [   64/  146]
train() client id: f_00005-10-2 loss: 1.178845  [   96/  146]
train() client id: f_00005-10-3 loss: 1.000160  [  128/  146]
train() client id: f_00005-11-0 loss: 0.859463  [   32/  146]
train() client id: f_00005-11-1 loss: 0.901104  [   64/  146]
train() client id: f_00005-11-2 loss: 0.983030  [   96/  146]
train() client id: f_00005-11-3 loss: 1.073230  [  128/  146]
train() client id: f_00006-0-0 loss: 0.808958  [   32/   54]
train() client id: f_00006-1-0 loss: 0.803857  [   32/   54]
train() client id: f_00006-2-0 loss: 0.794399  [   32/   54]
train() client id: f_00006-3-0 loss: 0.780063  [   32/   54]
train() client id: f_00006-4-0 loss: 0.851316  [   32/   54]
train() client id: f_00006-5-0 loss: 0.813200  [   32/   54]
train() client id: f_00006-6-0 loss: 0.870865  [   32/   54]
train() client id: f_00006-7-0 loss: 0.861701  [   32/   54]
train() client id: f_00006-8-0 loss: 0.855104  [   32/   54]
train() client id: f_00006-9-0 loss: 0.817795  [   32/   54]
train() client id: f_00006-10-0 loss: 0.761384  [   32/   54]
train() client id: f_00006-11-0 loss: 0.869671  [   32/   54]
train() client id: f_00007-0-0 loss: 0.827867  [   32/  179]
train() client id: f_00007-0-1 loss: 0.815266  [   64/  179]
train() client id: f_00007-0-2 loss: 0.795349  [   96/  179]
train() client id: f_00007-0-3 loss: 0.735829  [  128/  179]
train() client id: f_00007-0-4 loss: 0.668451  [  160/  179]
train() client id: f_00007-1-0 loss: 0.726259  [   32/  179]
train() client id: f_00007-1-1 loss: 0.702344  [   64/  179]
train() client id: f_00007-1-2 loss: 0.832779  [   96/  179]
train() client id: f_00007-1-3 loss: 0.703068  [  128/  179]
train() client id: f_00007-1-4 loss: 0.722758  [  160/  179]
train() client id: f_00007-2-0 loss: 0.725092  [   32/  179]
train() client id: f_00007-2-1 loss: 0.666264  [   64/  179]
train() client id: f_00007-2-2 loss: 0.723700  [   96/  179]
train() client id: f_00007-2-3 loss: 0.697480  [  128/  179]
train() client id: f_00007-2-4 loss: 0.681857  [  160/  179]
train() client id: f_00007-3-0 loss: 0.721705  [   32/  179]
train() client id: f_00007-3-1 loss: 0.689263  [   64/  179]
train() client id: f_00007-3-2 loss: 0.634308  [   96/  179]
train() client id: f_00007-3-3 loss: 0.668858  [  128/  179]
train() client id: f_00007-3-4 loss: 0.662549  [  160/  179]
train() client id: f_00007-4-0 loss: 0.620538  [   32/  179]
train() client id: f_00007-4-1 loss: 0.638491  [   64/  179]
train() client id: f_00007-4-2 loss: 0.714029  [   96/  179]
train() client id: f_00007-4-3 loss: 0.745071  [  128/  179]
train() client id: f_00007-4-4 loss: 0.640448  [  160/  179]
train() client id: f_00007-5-0 loss: 0.605253  [   32/  179]
train() client id: f_00007-5-1 loss: 0.700300  [   64/  179]
train() client id: f_00007-5-2 loss: 0.618196  [   96/  179]
train() client id: f_00007-5-3 loss: 0.694515  [  128/  179]
train() client id: f_00007-5-4 loss: 0.645370  [  160/  179]
train() client id: f_00007-6-0 loss: 0.701804  [   32/  179]
train() client id: f_00007-6-1 loss: 0.635946  [   64/  179]
train() client id: f_00007-6-2 loss: 0.586612  [   96/  179]
train() client id: f_00007-6-3 loss: 0.690147  [  128/  179]
train() client id: f_00007-6-4 loss: 0.595853  [  160/  179]
train() client id: f_00007-7-0 loss: 0.696709  [   32/  179]
train() client id: f_00007-7-1 loss: 0.662129  [   64/  179]
train() client id: f_00007-7-2 loss: 0.552707  [   96/  179]
train() client id: f_00007-7-3 loss: 0.686226  [  128/  179]
train() client id: f_00007-7-4 loss: 0.695797  [  160/  179]
train() client id: f_00007-8-0 loss: 0.553343  [   32/  179]
train() client id: f_00007-8-1 loss: 0.611488  [   64/  179]
train() client id: f_00007-8-2 loss: 0.758073  [   96/  179]
train() client id: f_00007-8-3 loss: 0.781021  [  128/  179]
train() client id: f_00007-8-4 loss: 0.573073  [  160/  179]
train() client id: f_00007-9-0 loss: 0.690609  [   32/  179]
train() client id: f_00007-9-1 loss: 0.616273  [   64/  179]
train() client id: f_00007-9-2 loss: 0.551195  [   96/  179]
train() client id: f_00007-9-3 loss: 0.689072  [  128/  179]
train() client id: f_00007-9-4 loss: 0.657004  [  160/  179]
train() client id: f_00007-10-0 loss: 0.602135  [   32/  179]
train() client id: f_00007-10-1 loss: 0.614442  [   64/  179]
train() client id: f_00007-10-2 loss: 0.763960  [   96/  179]
train() client id: f_00007-10-3 loss: 0.625766  [  128/  179]
train() client id: f_00007-10-4 loss: 0.585631  [  160/  179]
train() client id: f_00007-11-0 loss: 0.801824  [   32/  179]
train() client id: f_00007-11-1 loss: 0.674815  [   64/  179]
train() client id: f_00007-11-2 loss: 0.553495  [   96/  179]
train() client id: f_00007-11-3 loss: 0.680727  [  128/  179]
train() client id: f_00007-11-4 loss: 0.532782  [  160/  179]
train() client id: f_00008-0-0 loss: 1.007694  [   32/  130]
train() client id: f_00008-0-1 loss: 0.929010  [   64/  130]
train() client id: f_00008-0-2 loss: 1.044053  [   96/  130]
train() client id: f_00008-0-3 loss: 0.932850  [  128/  130]
train() client id: f_00008-1-0 loss: 0.952626  [   32/  130]
train() client id: f_00008-1-1 loss: 0.993282  [   64/  130]
train() client id: f_00008-1-2 loss: 1.022389  [   96/  130]
train() client id: f_00008-1-3 loss: 0.971936  [  128/  130]
train() client id: f_00008-2-0 loss: 0.920643  [   32/  130]
train() client id: f_00008-2-1 loss: 1.094705  [   64/  130]
train() client id: f_00008-2-2 loss: 0.921137  [   96/  130]
train() client id: f_00008-2-3 loss: 0.975988  [  128/  130]
train() client id: f_00008-3-0 loss: 0.918935  [   32/  130]
train() client id: f_00008-3-1 loss: 1.033846  [   64/  130]
train() client id: f_00008-3-2 loss: 0.945449  [   96/  130]
train() client id: f_00008-3-3 loss: 1.037815  [  128/  130]
train() client id: f_00008-4-0 loss: 0.894714  [   32/  130]
train() client id: f_00008-4-1 loss: 1.041076  [   64/  130]
train() client id: f_00008-4-2 loss: 0.939641  [   96/  130]
train() client id: f_00008-4-3 loss: 1.039458  [  128/  130]
train() client id: f_00008-5-0 loss: 1.120724  [   32/  130]
train() client id: f_00008-5-1 loss: 0.850051  [   64/  130]
train() client id: f_00008-5-2 loss: 1.105702  [   96/  130]
train() client id: f_00008-5-3 loss: 0.869235  [  128/  130]
train() client id: f_00008-6-0 loss: 1.036666  [   32/  130]
train() client id: f_00008-6-1 loss: 1.067531  [   64/  130]
train() client id: f_00008-6-2 loss: 0.884960  [   96/  130]
train() client id: f_00008-6-3 loss: 0.953919  [  128/  130]
train() client id: f_00008-7-0 loss: 0.905126  [   32/  130]
train() client id: f_00008-7-1 loss: 1.031073  [   64/  130]
train() client id: f_00008-7-2 loss: 1.014243  [   96/  130]
train() client id: f_00008-7-3 loss: 1.004547  [  128/  130]
train() client id: f_00008-8-0 loss: 0.936911  [   32/  130]
train() client id: f_00008-8-1 loss: 1.027677  [   64/  130]
train() client id: f_00008-8-2 loss: 1.035991  [   96/  130]
train() client id: f_00008-8-3 loss: 0.950815  [  128/  130]
train() client id: f_00008-9-0 loss: 1.072153  [   32/  130]
train() client id: f_00008-9-1 loss: 1.015578  [   64/  130]
train() client id: f_00008-9-2 loss: 0.871637  [   96/  130]
train() client id: f_00008-9-3 loss: 0.989030  [  128/  130]
train() client id: f_00008-10-0 loss: 0.943053  [   32/  130]
train() client id: f_00008-10-1 loss: 0.952160  [   64/  130]
train() client id: f_00008-10-2 loss: 1.029949  [   96/  130]
train() client id: f_00008-10-3 loss: 1.037235  [  128/  130]
train() client id: f_00008-11-0 loss: 1.002990  [   32/  130]
train() client id: f_00008-11-1 loss: 0.896071  [   64/  130]
train() client id: f_00008-11-2 loss: 0.873142  [   96/  130]
train() client id: f_00008-11-3 loss: 1.160268  [  128/  130]
train() client id: f_00009-0-0 loss: 1.320923  [   32/  118]
train() client id: f_00009-0-1 loss: 1.088656  [   64/  118]
train() client id: f_00009-0-2 loss: 1.169987  [   96/  118]
train() client id: f_00009-1-0 loss: 1.108775  [   32/  118]
train() client id: f_00009-1-1 loss: 1.195642  [   64/  118]
train() client id: f_00009-1-2 loss: 1.097238  [   96/  118]
train() client id: f_00009-2-0 loss: 1.156648  [   32/  118]
train() client id: f_00009-2-1 loss: 1.074252  [   64/  118]
train() client id: f_00009-2-2 loss: 1.120646  [   96/  118]
train() client id: f_00009-3-0 loss: 1.107378  [   32/  118]
train() client id: f_00009-3-1 loss: 1.056379  [   64/  118]
train() client id: f_00009-3-2 loss: 1.096645  [   96/  118]
train() client id: f_00009-4-0 loss: 1.042993  [   32/  118]
train() client id: f_00009-4-1 loss: 0.994578  [   64/  118]
train() client id: f_00009-4-2 loss: 1.151100  [   96/  118]
train() client id: f_00009-5-0 loss: 1.042598  [   32/  118]
train() client id: f_00009-5-1 loss: 1.003737  [   64/  118]
train() client id: f_00009-5-2 loss: 0.986884  [   96/  118]
train() client id: f_00009-6-0 loss: 1.021765  [   32/  118]
train() client id: f_00009-6-1 loss: 1.009995  [   64/  118]
train() client id: f_00009-6-2 loss: 0.979826  [   96/  118]
train() client id: f_00009-7-0 loss: 0.957890  [   32/  118]
train() client id: f_00009-7-1 loss: 0.994480  [   64/  118]
train() client id: f_00009-7-2 loss: 1.061301  [   96/  118]
train() client id: f_00009-8-0 loss: 0.913646  [   32/  118]
train() client id: f_00009-8-1 loss: 1.077173  [   64/  118]
train() client id: f_00009-8-2 loss: 0.932790  [   96/  118]
train() client id: f_00009-9-0 loss: 0.959259  [   32/  118]
train() client id: f_00009-9-1 loss: 0.968403  [   64/  118]
train() client id: f_00009-9-2 loss: 0.989766  [   96/  118]
train() client id: f_00009-10-0 loss: 0.914048  [   32/  118]
train() client id: f_00009-10-1 loss: 1.009209  [   64/  118]
train() client id: f_00009-10-2 loss: 0.928985  [   96/  118]
train() client id: f_00009-11-0 loss: 0.935125  [   32/  118]
train() client id: f_00009-11-1 loss: 0.879373  [   64/  118]
train() client id: f_00009-11-2 loss: 1.060713  [   96/  118]
At round 6 accuracy: 0.6259946949602122
At round 6 training accuracy: 0.5774647887323944
At round 6 training loss: 0.8835022296384227
update_location
xs = [ -3.9056584    4.20031788  50.00902392  18.81129433  -4.02070377
   3.95640986 -12.44319194   3.67514815  34.66397685   2.93912145]
ys = [ 42.5879595   25.55583871   1.32061395 -12.45517586   9.35018685
 -12.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [108.76115328 103.2992912  111.81523373 102.51339522 100.51662575
 100.81739979 100.8053747  100.15467783 107.28588567 100.1231756 ]
dists_bs = [216.34745711 233.33733364 284.24213267 269.63356758 238.06255551
 258.96706121 240.78071123 253.04039114 262.18981874 246.78497559]
uav_gains = [8.10606699e-11 9.22049228e-11 7.56379401e-11 9.39823243e-11
 9.87196326e-11 9.79849732e-11 9.80141981e-11 9.96139759e-11
 8.38762801e-11 9.96923511e-11]
bs_gains = [3.19793078e-11 2.58784487e-11 1.48924591e-11 1.72634266e-11
 2.44657779e-11 1.93290009e-11 2.37002728e-11 2.06235063e-11
 1.86710946e-11 2.21208434e-11]
Round 7
-------------------------------
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.06798939 21.0326556   9.91312762  3.54368464 24.2561337  11.70105956
  4.40648092 14.22483624 10.44629039  9.49400071]
obj_prev = 119.08625877018483
eta_min = 5.793901722590025e-10	eta_max = 0.9200929959618319
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 27.695540866338916	eta = 0.909090909090909
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 47.81923698001299	eta = 0.5265195769323551
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 38.22376664223015	eta = 0.6586939654483899
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.504271658621605	eta = 0.6897210457833629
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41961812780936	eta = 0.6913242290346582
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41939846351146	eta = 0.6913283987699586
eta = 0.6913283987699586
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.03046057 0.06406391 0.02997707 0.01039528 0.07397568 0.03529559
 0.01305453 0.04327336 0.03142758 0.02852657]
ene_total = [3.09168549 6.04664955 3.05847947 1.40079217 6.88392674 3.69460946
 1.61888686 4.14166343 3.37804549 3.10465979]
ti_comp = [0.28943977 0.27173208 0.28857988 0.29120492 0.27061872 0.2656134
 0.2916892  0.29187392 0.26482942 0.26854656]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.10852063e-05 2.22555556e-04 2.02169483e-05 8.27924223e-07
 3.45486619e-04 3.89530298e-05 1.63426776e-06 5.94499024e-05
 2.76617870e-05 2.01182160e-05]
ene_total = [0.54893267 0.71368309 0.55604324 0.53249539 0.73325245 0.74945808
 0.52851731 0.53180388 0.75506382 0.72338265]
optimize_network iter = 0 obj = 6.3726325729552675
eta = 0.6913283987699586
freqs = [5.26198833e+07 1.17880646e+08 5.19389521e+07 1.78487289e+07
 1.36678791e+08 6.64416515e+07 2.23774684e+07 7.41302190e+07
 5.93355081e+07 5.31128927e+07]
eta_min = 0.6839075543866936	eta_max = 0.6913283987699287
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 0.058240867111970324	eta = 0.9090909090909091
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 21.478076657689165	eta = 0.0024651296143925966
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.271073836819586	eta = 0.023313307551114187
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977536965308997	eta = 0.02409107213089359
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977247993541873	eta = 0.0240913888966546
eta = 0.0240913888966546
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12822560e-04 2.24635427e-03 2.04058838e-04 8.35661507e-06
 3.48715330e-03 3.93170615e-04 1.64954064e-05 6.00054856e-04
 2.79202975e-04 2.03062288e-04]
ene_total = [0.17868869 0.2792033  0.18072909 0.16862269 0.31492846 0.24640612
 0.16755818 0.18248854 0.24546632 0.23363341]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 1 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
eta_min = 0.6839075543866936	eta_max = 0.6839075543866819
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 0.05801261670554691	eta = 0.9090909090909091
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 21.4778591118164	eta = 0.0024554934542136467
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.270009752860956	eta = 0.023232826375798606
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196960837665713	eta = 0.024005317507445147
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196932244064364	eta = 0.024005629942423928
eta = 0.024005629942423928
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12924512e-04 2.23906549e-03 2.04121753e-04 8.36352164e-06
 3.47496787e-03 3.91345733e-04 1.65106003e-05 6.00629173e-04
 2.77855860e-04 2.02257286e-04]
ene_total = [0.17867568 0.27898621 0.18071488 0.16860805 0.31457886 0.24633625
 0.16754386 0.18248768 0.24540916 0.23359161]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 2 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
Done!
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.09225246e-05 2.20016485e-04 2.00575422e-05 8.21821712e-07
 3.41459515e-04 3.84546646e-05 1.62237516e-06 5.90194079e-05
 2.73028502e-05 1.98743347e-05]
ene_total = [0.00657115 0.00854102 0.00665628 0.00637454 0.00877379 0.00897132
 0.00632691 0.00636583 0.00903857 0.00865943]
At round 7 energy consumption: 0.07627883514893437
At round 7 eta: 0.6839075543866936
At round 7 a_n: 25.784781927469155
At round 7 local rounds: 12.440921153911754
At round 7 global rounds: 81.57354686993413
gradient difference: 0.3894316554069519
train() client id: f_00000-0-0 loss: 1.205949  [   32/  126]
train() client id: f_00000-0-1 loss: 1.259750  [   64/  126]
train() client id: f_00000-0-2 loss: 1.257720  [   96/  126]
train() client id: f_00000-1-0 loss: 1.081522  [   32/  126]
train() client id: f_00000-1-1 loss: 1.197947  [   64/  126]
train() client id: f_00000-1-2 loss: 1.195411  [   96/  126]
train() client id: f_00000-2-0 loss: 1.151034  [   32/  126]
train() client id: f_00000-2-1 loss: 1.053929  [   64/  126]
train() client id: f_00000-2-2 loss: 0.985722  [   96/  126]
train() client id: f_00000-3-0 loss: 1.039060  [   32/  126]
train() client id: f_00000-3-1 loss: 1.010403  [   64/  126]
train() client id: f_00000-3-2 loss: 0.986755  [   96/  126]
train() client id: f_00000-4-0 loss: 0.935863  [   32/  126]
train() client id: f_00000-4-1 loss: 0.999872  [   64/  126]
train() client id: f_00000-4-2 loss: 0.953680  [   96/  126]
train() client id: f_00000-5-0 loss: 0.993773  [   32/  126]
train() client id: f_00000-5-1 loss: 0.917958  [   64/  126]
train() client id: f_00000-5-2 loss: 0.929545  [   96/  126]
train() client id: f_00000-6-0 loss: 0.897303  [   32/  126]
train() client id: f_00000-6-1 loss: 0.933856  [   64/  126]
train() client id: f_00000-6-2 loss: 0.898067  [   96/  126]
train() client id: f_00000-7-0 loss: 0.890201  [   32/  126]
train() client id: f_00000-7-1 loss: 0.841657  [   64/  126]
train() client id: f_00000-7-2 loss: 0.989421  [   96/  126]
train() client id: f_00000-8-0 loss: 0.953131  [   32/  126]
train() client id: f_00000-8-1 loss: 0.846574  [   64/  126]
train() client id: f_00000-8-2 loss: 0.896673  [   96/  126]
train() client id: f_00000-9-0 loss: 0.906263  [   32/  126]
train() client id: f_00000-9-1 loss: 0.858533  [   64/  126]
train() client id: f_00000-9-2 loss: 0.949150  [   96/  126]
train() client id: f_00000-10-0 loss: 0.905563  [   32/  126]
train() client id: f_00000-10-1 loss: 0.959824  [   64/  126]
train() client id: f_00000-10-2 loss: 0.897581  [   96/  126]
train() client id: f_00000-11-0 loss: 0.908527  [   32/  126]
train() client id: f_00000-11-1 loss: 0.920011  [   64/  126]
train() client id: f_00000-11-2 loss: 0.876554  [   96/  126]
train() client id: f_00001-0-0 loss: 0.574305  [   32/  265]
train() client id: f_00001-0-1 loss: 0.575990  [   64/  265]
train() client id: f_00001-0-2 loss: 0.623683  [   96/  265]
train() client id: f_00001-0-3 loss: 0.633572  [  128/  265]
train() client id: f_00001-0-4 loss: 0.656068  [  160/  265]
train() client id: f_00001-0-5 loss: 0.519401  [  192/  265]
train() client id: f_00001-0-6 loss: 0.603573  [  224/  265]
train() client id: f_00001-0-7 loss: 0.600850  [  256/  265]
train() client id: f_00001-1-0 loss: 0.579102  [   32/  265]
train() client id: f_00001-1-1 loss: 0.628293  [   64/  265]
train() client id: f_00001-1-2 loss: 0.549719  [   96/  265]
train() client id: f_00001-1-3 loss: 0.545491  [  128/  265]
train() client id: f_00001-1-4 loss: 0.665888  [  160/  265]
train() client id: f_00001-1-5 loss: 0.562455  [  192/  265]
train() client id: f_00001-1-6 loss: 0.535420  [  224/  265]
train() client id: f_00001-1-7 loss: 0.579888  [  256/  265]
train() client id: f_00001-2-0 loss: 0.581881  [   32/  265]
train() client id: f_00001-2-1 loss: 0.680579  [   64/  265]
train() client id: f_00001-2-2 loss: 0.630586  [   96/  265]
train() client id: f_00001-2-3 loss: 0.505580  [  128/  265]
train() client id: f_00001-2-4 loss: 0.566572  [  160/  265]
train() client id: f_00001-2-5 loss: 0.513622  [  192/  265]
train() client id: f_00001-2-6 loss: 0.528636  [  224/  265]
train() client id: f_00001-2-7 loss: 0.486811  [  256/  265]
train() client id: f_00001-3-0 loss: 0.561612  [   32/  265]
train() client id: f_00001-3-1 loss: 0.612526  [   64/  265]
train() client id: f_00001-3-2 loss: 0.611614  [   96/  265]
train() client id: f_00001-3-3 loss: 0.506520  [  128/  265]
train() client id: f_00001-3-4 loss: 0.569392  [  160/  265]
train() client id: f_00001-3-5 loss: 0.564737  [  192/  265]
train() client id: f_00001-3-6 loss: 0.483851  [  224/  265]
train() client id: f_00001-3-7 loss: 0.517695  [  256/  265]
train() client id: f_00001-4-0 loss: 0.638439  [   32/  265]
train() client id: f_00001-4-1 loss: 0.535257  [   64/  265]
train() client id: f_00001-4-2 loss: 0.504544  [   96/  265]
train() client id: f_00001-4-3 loss: 0.520200  [  128/  265]
train() client id: f_00001-4-4 loss: 0.545149  [  160/  265]
train() client id: f_00001-4-5 loss: 0.561168  [  192/  265]
train() client id: f_00001-4-6 loss: 0.524021  [  224/  265]
train() client id: f_00001-4-7 loss: 0.587430  [  256/  265]
train() client id: f_00001-5-0 loss: 0.614252  [   32/  265]
train() client id: f_00001-5-1 loss: 0.470229  [   64/  265]
train() client id: f_00001-5-2 loss: 0.624292  [   96/  265]
train() client id: f_00001-5-3 loss: 0.531938  [  128/  265]
train() client id: f_00001-5-4 loss: 0.520458  [  160/  265]
train() client id: f_00001-5-5 loss: 0.548759  [  192/  265]
train() client id: f_00001-5-6 loss: 0.479464  [  224/  265]
train() client id: f_00001-5-7 loss: 0.591615  [  256/  265]
train() client id: f_00001-6-0 loss: 0.532959  [   32/  265]
train() client id: f_00001-6-1 loss: 0.460037  [   64/  265]
train() client id: f_00001-6-2 loss: 0.474289  [   96/  265]
train() client id: f_00001-6-3 loss: 0.611331  [  128/  265]
train() client id: f_00001-6-4 loss: 0.638887  [  160/  265]
train() client id: f_00001-6-5 loss: 0.557264  [  192/  265]
train() client id: f_00001-6-6 loss: 0.453666  [  224/  265]
train() client id: f_00001-6-7 loss: 0.587339  [  256/  265]
train() client id: f_00001-7-0 loss: 0.618625  [   32/  265]
train() client id: f_00001-7-1 loss: 0.693675  [   64/  265]
train() client id: f_00001-7-2 loss: 0.537085  [   96/  265]
train() client id: f_00001-7-3 loss: 0.585002  [  128/  265]
train() client id: f_00001-7-4 loss: 0.458653  [  160/  265]
train() client id: f_00001-7-5 loss: 0.527019  [  192/  265]
train() client id: f_00001-7-6 loss: 0.455886  [  224/  265]
train() client id: f_00001-7-7 loss: 0.464661  [  256/  265]
train() client id: f_00001-8-0 loss: 0.600988  [   32/  265]
train() client id: f_00001-8-1 loss: 0.553532  [   64/  265]
train() client id: f_00001-8-2 loss: 0.476073  [   96/  265]
train() client id: f_00001-8-3 loss: 0.521691  [  128/  265]
train() client id: f_00001-8-4 loss: 0.585195  [  160/  265]
train() client id: f_00001-8-5 loss: 0.509133  [  192/  265]
train() client id: f_00001-8-6 loss: 0.609265  [  224/  265]
train() client id: f_00001-8-7 loss: 0.469323  [  256/  265]
train() client id: f_00001-9-0 loss: 0.510895  [   32/  265]
train() client id: f_00001-9-1 loss: 0.520041  [   64/  265]
train() client id: f_00001-9-2 loss: 0.460101  [   96/  265]
train() client id: f_00001-9-3 loss: 0.680331  [  128/  265]
train() client id: f_00001-9-4 loss: 0.584144  [  160/  265]
train() client id: f_00001-9-5 loss: 0.487812  [  192/  265]
train() client id: f_00001-9-6 loss: 0.442291  [  224/  265]
train() client id: f_00001-9-7 loss: 0.542871  [  256/  265]
train() client id: f_00001-10-0 loss: 0.564532  [   32/  265]
train() client id: f_00001-10-1 loss: 0.507041  [   64/  265]
train() client id: f_00001-10-2 loss: 0.563928  [   96/  265]
train() client id: f_00001-10-3 loss: 0.641805  [  128/  265]
train() client id: f_00001-10-4 loss: 0.450037  [  160/  265]
train() client id: f_00001-10-5 loss: 0.469625  [  192/  265]
train() client id: f_00001-10-6 loss: 0.510459  [  224/  265]
train() client id: f_00001-10-7 loss: 0.503458  [  256/  265]
train() client id: f_00001-11-0 loss: 0.500029  [   32/  265]
train() client id: f_00001-11-1 loss: 0.522655  [   64/  265]
train() client id: f_00001-11-2 loss: 0.583816  [   96/  265]
train() client id: f_00001-11-3 loss: 0.503671  [  128/  265]
train() client id: f_00001-11-4 loss: 0.539478  [  160/  265]
train() client id: f_00001-11-5 loss: 0.563023  [  192/  265]
train() client id: f_00001-11-6 loss: 0.516843  [  224/  265]
train() client id: f_00001-11-7 loss: 0.606087  [  256/  265]
train() client id: f_00002-0-0 loss: 1.090815  [   32/  124]
train() client id: f_00002-0-1 loss: 1.212196  [   64/  124]
train() client id: f_00002-0-2 loss: 1.206188  [   96/  124]
train() client id: f_00002-1-0 loss: 1.080657  [   32/  124]
train() client id: f_00002-1-1 loss: 1.130271  [   64/  124]
train() client id: f_00002-1-2 loss: 1.108279  [   96/  124]
train() client id: f_00002-2-0 loss: 1.094012  [   32/  124]
train() client id: f_00002-2-1 loss: 1.015980  [   64/  124]
train() client id: f_00002-2-2 loss: 1.239759  [   96/  124]
train() client id: f_00002-3-0 loss: 1.090805  [   32/  124]
train() client id: f_00002-3-1 loss: 1.132168  [   64/  124]
train() client id: f_00002-3-2 loss: 1.070650  [   96/  124]
train() client id: f_00002-4-0 loss: 1.047953  [   32/  124]
train() client id: f_00002-4-1 loss: 1.173508  [   64/  124]
train() client id: f_00002-4-2 loss: 0.968548  [   96/  124]
train() client id: f_00002-5-0 loss: 1.077233  [   32/  124]
train() client id: f_00002-5-1 loss: 1.083174  [   64/  124]
train() client id: f_00002-5-2 loss: 0.990008  [   96/  124]
train() client id: f_00002-6-0 loss: 1.100884  [   32/  124]
train() client id: f_00002-6-1 loss: 0.967043  [   64/  124]
train() client id: f_00002-6-2 loss: 1.083734  [   96/  124]
train() client id: f_00002-7-0 loss: 1.026139  [   32/  124]
train() client id: f_00002-7-1 loss: 1.012110  [   64/  124]
train() client id: f_00002-7-2 loss: 1.066406  [   96/  124]
train() client id: f_00002-8-0 loss: 1.080682  [   32/  124]
train() client id: f_00002-8-1 loss: 0.893639  [   64/  124]
train() client id: f_00002-8-2 loss: 1.064117  [   96/  124]
train() client id: f_00002-9-0 loss: 1.058706  [   32/  124]
train() client id: f_00002-9-1 loss: 0.993938  [   64/  124]
train() client id: f_00002-9-2 loss: 1.051289  [   96/  124]
train() client id: f_00002-10-0 loss: 0.913367  [   32/  124]
train() client id: f_00002-10-1 loss: 1.110304  [   64/  124]
train() client id: f_00002-10-2 loss: 0.999735  [   96/  124]
train() client id: f_00002-11-0 loss: 1.068250  [   32/  124]
train() client id: f_00002-11-1 loss: 1.038916  [   64/  124]
train() client id: f_00002-11-2 loss: 1.037166  [   96/  124]
train() client id: f_00003-0-0 loss: 1.062962  [   32/   43]
train() client id: f_00003-1-0 loss: 1.066339  [   32/   43]
train() client id: f_00003-2-0 loss: 1.033965  [   32/   43]
train() client id: f_00003-3-0 loss: 1.072307  [   32/   43]
train() client id: f_00003-4-0 loss: 1.037253  [   32/   43]
train() client id: f_00003-5-0 loss: 1.060327  [   32/   43]
train() client id: f_00003-6-0 loss: 1.062198  [   32/   43]
train() client id: f_00003-7-0 loss: 1.074618  [   32/   43]
train() client id: f_00003-8-0 loss: 0.972489  [   32/   43]
train() client id: f_00003-9-0 loss: 1.121401  [   32/   43]
train() client id: f_00003-10-0 loss: 1.016771  [   32/   43]
train() client id: f_00003-11-0 loss: 0.988801  [   32/   43]
train() client id: f_00004-0-0 loss: 1.023347  [   32/  306]
train() client id: f_00004-0-1 loss: 0.964865  [   64/  306]
train() client id: f_00004-0-2 loss: 0.904120  [   96/  306]
train() client id: f_00004-0-3 loss: 1.007481  [  128/  306]
train() client id: f_00004-0-4 loss: 0.992362  [  160/  306]
train() client id: f_00004-0-5 loss: 0.922411  [  192/  306]
train() client id: f_00004-0-6 loss: 0.954287  [  224/  306]
train() client id: f_00004-0-7 loss: 0.971579  [  256/  306]
train() client id: f_00004-0-8 loss: 1.035129  [  288/  306]
train() client id: f_00004-1-0 loss: 0.894777  [   32/  306]
train() client id: f_00004-1-1 loss: 1.070431  [   64/  306]
train() client id: f_00004-1-2 loss: 0.989105  [   96/  306]
train() client id: f_00004-1-3 loss: 0.953592  [  128/  306]
train() client id: f_00004-1-4 loss: 0.883512  [  160/  306]
train() client id: f_00004-1-5 loss: 0.967817  [  192/  306]
train() client id: f_00004-1-6 loss: 1.033388  [  224/  306]
train() client id: f_00004-1-7 loss: 0.926085  [  256/  306]
train() client id: f_00004-1-8 loss: 1.018391  [  288/  306]
train() client id: f_00004-2-0 loss: 0.860684  [   32/  306]
train() client id: f_00004-2-1 loss: 1.061514  [   64/  306]
train() client id: f_00004-2-2 loss: 0.956951  [   96/  306]
train() client id: f_00004-2-3 loss: 0.861321  [  128/  306]
train() client id: f_00004-2-4 loss: 0.932102  [  160/  306]
train() client id: f_00004-2-5 loss: 0.947694  [  192/  306]
train() client id: f_00004-2-6 loss: 0.822991  [  224/  306]
train() client id: f_00004-2-7 loss: 1.028295  [  256/  306]
train() client id: f_00004-2-8 loss: 1.096623  [  288/  306]
train() client id: f_00004-3-0 loss: 0.861781  [   32/  306]
train() client id: f_00004-3-1 loss: 0.998032  [   64/  306]
train() client id: f_00004-3-2 loss: 0.922248  [   96/  306]
train() client id: f_00004-3-3 loss: 0.988874  [  128/  306]
train() client id: f_00004-3-4 loss: 0.905889  [  160/  306]
train() client id: f_00004-3-5 loss: 0.869247  [  192/  306]
train() client id: f_00004-3-6 loss: 1.091639  [  224/  306]
train() client id: f_00004-3-7 loss: 1.085122  [  256/  306]
train() client id: f_00004-3-8 loss: 0.893680  [  288/  306]
train() client id: f_00004-4-0 loss: 0.977531  [   32/  306]
train() client id: f_00004-4-1 loss: 1.075821  [   64/  306]
train() client id: f_00004-4-2 loss: 0.922522  [   96/  306]
train() client id: f_00004-4-3 loss: 0.891486  [  128/  306]
train() client id: f_00004-4-4 loss: 0.870305  [  160/  306]
train() client id: f_00004-4-5 loss: 1.001518  [  192/  306]
train() client id: f_00004-4-6 loss: 1.011379  [  224/  306]
train() client id: f_00004-4-7 loss: 0.969104  [  256/  306]
train() client id: f_00004-4-8 loss: 0.982911  [  288/  306]
train() client id: f_00004-5-0 loss: 0.970225  [   32/  306]
train() client id: f_00004-5-1 loss: 0.956251  [   64/  306]
train() client id: f_00004-5-2 loss: 0.979025  [   96/  306]
train() client id: f_00004-5-3 loss: 0.915877  [  128/  306]
train() client id: f_00004-5-4 loss: 1.001666  [  160/  306]
train() client id: f_00004-5-5 loss: 1.027567  [  192/  306]
train() client id: f_00004-5-6 loss: 1.022347  [  224/  306]
train() client id: f_00004-5-7 loss: 0.933020  [  256/  306]
train() client id: f_00004-5-8 loss: 0.910321  [  288/  306]
train() client id: f_00004-6-0 loss: 0.925055  [   32/  306]
train() client id: f_00004-6-1 loss: 1.027613  [   64/  306]
train() client id: f_00004-6-2 loss: 1.002685  [   96/  306]
train() client id: f_00004-6-3 loss: 0.921656  [  128/  306]
train() client id: f_00004-6-4 loss: 0.965334  [  160/  306]
train() client id: f_00004-6-5 loss: 0.978313  [  192/  306]
train() client id: f_00004-6-6 loss: 0.929729  [  224/  306]
train() client id: f_00004-6-7 loss: 0.943908  [  256/  306]
train() client id: f_00004-6-8 loss: 0.949371  [  288/  306]
train() client id: f_00004-7-0 loss: 1.051957  [   32/  306]
train() client id: f_00004-7-1 loss: 0.902647  [   64/  306]
train() client id: f_00004-7-2 loss: 0.984678  [   96/  306]
train() client id: f_00004-7-3 loss: 1.060116  [  128/  306]
train() client id: f_00004-7-4 loss: 0.817715  [  160/  306]
train() client id: f_00004-7-5 loss: 0.957406  [  192/  306]
train() client id: f_00004-7-6 loss: 0.956398  [  224/  306]
train() client id: f_00004-7-7 loss: 0.932759  [  256/  306]
train() client id: f_00004-7-8 loss: 0.985177  [  288/  306]
train() client id: f_00004-8-0 loss: 0.917702  [   32/  306]
train() client id: f_00004-8-1 loss: 0.936336  [   64/  306]
train() client id: f_00004-8-2 loss: 1.012202  [   96/  306]
train() client id: f_00004-8-3 loss: 0.941393  [  128/  306]
train() client id: f_00004-8-4 loss: 1.021247  [  160/  306]
train() client id: f_00004-8-5 loss: 1.028298  [  192/  306]
train() client id: f_00004-8-6 loss: 0.988507  [  224/  306]
train() client id: f_00004-8-7 loss: 0.925428  [  256/  306]
train() client id: f_00004-8-8 loss: 0.937445  [  288/  306]
train() client id: f_00004-9-0 loss: 0.888519  [   32/  306]
train() client id: f_00004-9-1 loss: 0.965558  [   64/  306]
train() client id: f_00004-9-2 loss: 1.005883  [   96/  306]
train() client id: f_00004-9-3 loss: 0.918910  [  128/  306]
train() client id: f_00004-9-4 loss: 1.014240  [  160/  306]
train() client id: f_00004-9-5 loss: 0.834312  [  192/  306]
train() client id: f_00004-9-6 loss: 0.968380  [  224/  306]
train() client id: f_00004-9-7 loss: 1.013275  [  256/  306]
train() client id: f_00004-9-8 loss: 1.016399  [  288/  306]
train() client id: f_00004-10-0 loss: 1.019862  [   32/  306]
train() client id: f_00004-10-1 loss: 0.929054  [   64/  306]
train() client id: f_00004-10-2 loss: 0.921742  [   96/  306]
train() client id: f_00004-10-3 loss: 0.943990  [  128/  306]
train() client id: f_00004-10-4 loss: 0.874203  [  160/  306]
train() client id: f_00004-10-5 loss: 1.034448  [  192/  306]
train() client id: f_00004-10-6 loss: 1.119480  [  224/  306]
train() client id: f_00004-10-7 loss: 0.989509  [  256/  306]
train() client id: f_00004-10-8 loss: 0.949975  [  288/  306]
train() client id: f_00004-11-0 loss: 1.019627  [   32/  306]
train() client id: f_00004-11-1 loss: 0.903637  [   64/  306]
train() client id: f_00004-11-2 loss: 0.985778  [   96/  306]
train() client id: f_00004-11-3 loss: 0.930236  [  128/  306]
train() client id: f_00004-11-4 loss: 1.040664  [  160/  306]
train() client id: f_00004-11-5 loss: 1.173722  [  192/  306]
train() client id: f_00004-11-6 loss: 0.889085  [  224/  306]
train() client id: f_00004-11-7 loss: 0.935468  [  256/  306]
train() client id: f_00004-11-8 loss: 0.900634  [  288/  306]
train() client id: f_00005-0-0 loss: 0.792432  [   32/  146]
train() client id: f_00005-0-1 loss: 0.774438  [   64/  146]
train() client id: f_00005-0-2 loss: 0.795894  [   96/  146]
train() client id: f_00005-0-3 loss: 0.720205  [  128/  146]
train() client id: f_00005-1-0 loss: 0.737620  [   32/  146]
train() client id: f_00005-1-1 loss: 0.738354  [   64/  146]
train() client id: f_00005-1-2 loss: 0.673489  [   96/  146]
train() client id: f_00005-1-3 loss: 0.773005  [  128/  146]
train() client id: f_00005-2-0 loss: 0.751996  [   32/  146]
train() client id: f_00005-2-1 loss: 0.653805  [   64/  146]
train() client id: f_00005-2-2 loss: 0.818511  [   96/  146]
train() client id: f_00005-2-3 loss: 0.676130  [  128/  146]
train() client id: f_00005-3-0 loss: 0.695938  [   32/  146]
train() client id: f_00005-3-1 loss: 0.673729  [   64/  146]
train() client id: f_00005-3-2 loss: 0.661917  [   96/  146]
train() client id: f_00005-3-3 loss: 0.728815  [  128/  146]
train() client id: f_00005-4-0 loss: 0.722954  [   32/  146]
train() client id: f_00005-4-1 loss: 0.796179  [   64/  146]
train() client id: f_00005-4-2 loss: 0.700193  [   96/  146]
train() client id: f_00005-4-3 loss: 0.640678  [  128/  146]
train() client id: f_00005-5-0 loss: 0.877196  [   32/  146]
train() client id: f_00005-5-1 loss: 0.558560  [   64/  146]
train() client id: f_00005-5-2 loss: 0.678786  [   96/  146]
train() client id: f_00005-5-3 loss: 0.656013  [  128/  146]
train() client id: f_00005-6-0 loss: 0.728377  [   32/  146]
train() client id: f_00005-6-1 loss: 0.679813  [   64/  146]
train() client id: f_00005-6-2 loss: 0.660689  [   96/  146]
train() client id: f_00005-6-3 loss: 0.648661  [  128/  146]
train() client id: f_00005-7-0 loss: 0.473685  [   32/  146]
train() client id: f_00005-7-1 loss: 0.865700  [   64/  146]
train() client id: f_00005-7-2 loss: 0.536160  [   96/  146]
train() client id: f_00005-7-3 loss: 0.839001  [  128/  146]
train() client id: f_00005-8-0 loss: 0.689331  [   32/  146]
train() client id: f_00005-8-1 loss: 0.731188  [   64/  146]
train() client id: f_00005-8-2 loss: 0.546423  [   96/  146]
train() client id: f_00005-8-3 loss: 0.723790  [  128/  146]
train() client id: f_00005-9-0 loss: 0.579697  [   32/  146]
train() client id: f_00005-9-1 loss: 0.779078  [   64/  146]
train() client id: f_00005-9-2 loss: 0.707028  [   96/  146]
train() client id: f_00005-9-3 loss: 0.583012  [  128/  146]
train() client id: f_00005-10-0 loss: 0.733325  [   32/  146]
train() client id: f_00005-10-1 loss: 0.621627  [   64/  146]
train() client id: f_00005-10-2 loss: 0.732963  [   96/  146]
train() client id: f_00005-10-3 loss: 0.657824  [  128/  146]
train() client id: f_00005-11-0 loss: 0.668993  [   32/  146]
train() client id: f_00005-11-1 loss: 0.722128  [   64/  146]
train() client id: f_00005-11-2 loss: 0.610872  [   96/  146]
train() client id: f_00005-11-3 loss: 0.573445  [  128/  146]
train() client id: f_00006-0-0 loss: 0.727681  [   32/   54]
train() client id: f_00006-1-0 loss: 0.753319  [   32/   54]
train() client id: f_00006-2-0 loss: 0.811264  [   32/   54]
train() client id: f_00006-3-0 loss: 0.812052  [   32/   54]
train() client id: f_00006-4-0 loss: 0.800013  [   32/   54]
train() client id: f_00006-5-0 loss: 0.769633  [   32/   54]
train() client id: f_00006-6-0 loss: 0.830770  [   32/   54]
train() client id: f_00006-7-0 loss: 0.808665  [   32/   54]
train() client id: f_00006-8-0 loss: 0.798311  [   32/   54]
train() client id: f_00006-9-0 loss: 0.729659  [   32/   54]
train() client id: f_00006-10-0 loss: 0.768748  [   32/   54]
train() client id: f_00006-11-0 loss: 0.712156  [   32/   54]
train() client id: f_00007-0-0 loss: 0.865324  [   32/  179]
train() client id: f_00007-0-1 loss: 0.871054  [   64/  179]
train() client id: f_00007-0-2 loss: 0.791008  [   96/  179]
train() client id: f_00007-0-3 loss: 0.784786  [  128/  179]
train() client id: f_00007-0-4 loss: 0.797290  [  160/  179]
train() client id: f_00007-1-0 loss: 0.758885  [   32/  179]
train() client id: f_00007-1-1 loss: 0.794039  [   64/  179]
train() client id: f_00007-1-2 loss: 0.783646  [   96/  179]
train() client id: f_00007-1-3 loss: 0.767935  [  128/  179]
train() client id: f_00007-1-4 loss: 0.796538  [  160/  179]
train() client id: f_00007-2-0 loss: 0.826636  [   32/  179]
train() client id: f_00007-2-1 loss: 0.826686  [   64/  179]
train() client id: f_00007-2-2 loss: 0.717189  [   96/  179]
train() client id: f_00007-2-3 loss: 0.839819  [  128/  179]
train() client id: f_00007-2-4 loss: 0.755991  [  160/  179]
train() client id: f_00007-3-0 loss: 0.713976  [   32/  179]
train() client id: f_00007-3-1 loss: 0.920911  [   64/  179]
train() client id: f_00007-3-2 loss: 0.676082  [   96/  179]
train() client id: f_00007-3-3 loss: 0.760904  [  128/  179]
train() client id: f_00007-3-4 loss: 0.739802  [  160/  179]
train() client id: f_00007-4-0 loss: 0.806926  [   32/  179]
train() client id: f_00007-4-1 loss: 0.737202  [   64/  179]
train() client id: f_00007-4-2 loss: 0.747566  [   96/  179]
train() client id: f_00007-4-3 loss: 0.704112  [  128/  179]
train() client id: f_00007-4-4 loss: 0.807447  [  160/  179]
train() client id: f_00007-5-0 loss: 0.792027  [   32/  179]
train() client id: f_00007-5-1 loss: 0.904636  [   64/  179]
train() client id: f_00007-5-2 loss: 0.765104  [   96/  179]
train() client id: f_00007-5-3 loss: 0.642763  [  128/  179]
train() client id: f_00007-5-4 loss: 0.670967  [  160/  179]
train() client id: f_00007-6-0 loss: 0.757351  [   32/  179]
train() client id: f_00007-6-1 loss: 0.673722  [   64/  179]
train() client id: f_00007-6-2 loss: 0.757186  [   96/  179]
train() client id: f_00007-6-3 loss: 0.822364  [  128/  179]
train() client id: f_00007-6-4 loss: 0.839210  [  160/  179]
train() client id: f_00007-7-0 loss: 0.794440  [   32/  179]
train() client id: f_00007-7-1 loss: 0.787896  [   64/  179]
train() client id: f_00007-7-2 loss: 0.688431  [   96/  179]
train() client id: f_00007-7-3 loss: 0.688699  [  128/  179]
train() client id: f_00007-7-4 loss: 0.812465  [  160/  179]
train() client id: f_00007-8-0 loss: 0.713080  [   32/  179]
train() client id: f_00007-8-1 loss: 0.654618  [   64/  179]
train() client id: f_00007-8-2 loss: 0.804084  [   96/  179]
train() client id: f_00007-8-3 loss: 0.807076  [  128/  179]
train() client id: f_00007-8-4 loss: 0.835533  [  160/  179]
train() client id: f_00007-9-0 loss: 0.713575  [   32/  179]
train() client id: f_00007-9-1 loss: 0.674805  [   64/  179]
train() client id: f_00007-9-2 loss: 0.857025  [   96/  179]
train() client id: f_00007-9-3 loss: 0.789244  [  128/  179]
train() client id: f_00007-9-4 loss: 0.713603  [  160/  179]
train() client id: f_00007-10-0 loss: 0.759546  [   32/  179]
train() client id: f_00007-10-1 loss: 0.727711  [   64/  179]
train() client id: f_00007-10-2 loss: 0.879991  [   96/  179]
train() client id: f_00007-10-3 loss: 0.797543  [  128/  179]
train() client id: f_00007-10-4 loss: 0.679511  [  160/  179]
train() client id: f_00007-11-0 loss: 0.703565  [   32/  179]
train() client id: f_00007-11-1 loss: 0.658284  [   64/  179]
train() client id: f_00007-11-2 loss: 0.758721  [   96/  179]
train() client id: f_00007-11-3 loss: 0.869885  [  128/  179]
train() client id: f_00007-11-4 loss: 0.852717  [  160/  179]
train() client id: f_00008-0-0 loss: 0.654151  [   32/  130]
train() client id: f_00008-0-1 loss: 0.776848  [   64/  130]
train() client id: f_00008-0-2 loss: 0.654236  [   96/  130]
train() client id: f_00008-0-3 loss: 0.728669  [  128/  130]
train() client id: f_00008-1-0 loss: 0.712164  [   32/  130]
train() client id: f_00008-1-1 loss: 0.631854  [   64/  130]
train() client id: f_00008-1-2 loss: 0.714605  [   96/  130]
train() client id: f_00008-1-3 loss: 0.760867  [  128/  130]
train() client id: f_00008-2-0 loss: 0.738317  [   32/  130]
train() client id: f_00008-2-1 loss: 0.573008  [   64/  130]
train() client id: f_00008-2-2 loss: 0.768198  [   96/  130]
train() client id: f_00008-2-3 loss: 0.724853  [  128/  130]
train() client id: f_00008-3-0 loss: 0.717231  [   32/  130]
train() client id: f_00008-3-1 loss: 0.646358  [   64/  130]
train() client id: f_00008-3-2 loss: 0.693838  [   96/  130]
train() client id: f_00008-3-3 loss: 0.738416  [  128/  130]
train() client id: f_00008-4-0 loss: 0.580027  [   32/  130]
train() client id: f_00008-4-1 loss: 0.653870  [   64/  130]
train() client id: f_00008-4-2 loss: 0.735901  [   96/  130]
train() client id: f_00008-4-3 loss: 0.783763  [  128/  130]
train() client id: f_00008-5-0 loss: 0.782384  [   32/  130]
train() client id: f_00008-5-1 loss: 0.645674  [   64/  130]
train() client id: f_00008-5-2 loss: 0.605100  [   96/  130]
train() client id: f_00008-5-3 loss: 0.736281  [  128/  130]
train() client id: f_00008-6-0 loss: 0.675101  [   32/  130]
train() client id: f_00008-6-1 loss: 0.626661  [   64/  130]
train() client id: f_00008-6-2 loss: 0.878039  [   96/  130]
train() client id: f_00008-6-3 loss: 0.580134  [  128/  130]
train() client id: f_00008-7-0 loss: 0.589149  [   32/  130]
train() client id: f_00008-7-1 loss: 0.662961  [   64/  130]
train() client id: f_00008-7-2 loss: 0.814032  [   96/  130]
train() client id: f_00008-7-3 loss: 0.681032  [  128/  130]
train() client id: f_00008-8-0 loss: 0.649926  [   32/  130]
train() client id: f_00008-8-1 loss: 0.694005  [   64/  130]
train() client id: f_00008-8-2 loss: 0.748685  [   96/  130]
train() client id: f_00008-8-3 loss: 0.626817  [  128/  130]
train() client id: f_00008-9-0 loss: 0.651758  [   32/  130]
train() client id: f_00008-9-1 loss: 0.651567  [   64/  130]
train() client id: f_00008-9-2 loss: 0.585991  [   96/  130]
train() client id: f_00008-9-3 loss: 0.804731  [  128/  130]
train() client id: f_00008-10-0 loss: 0.665562  [   32/  130]
train() client id: f_00008-10-1 loss: 0.628303  [   64/  130]
train() client id: f_00008-10-2 loss: 0.719689  [   96/  130]
train() client id: f_00008-10-3 loss: 0.713517  [  128/  130]
train() client id: f_00008-11-0 loss: 0.671663  [   32/  130]
train() client id: f_00008-11-1 loss: 0.661856  [   64/  130]
train() client id: f_00008-11-2 loss: 0.686768  [   96/  130]
train() client id: f_00008-11-3 loss: 0.697175  [  128/  130]
train() client id: f_00009-0-0 loss: 1.211810  [   32/  118]
train() client id: f_00009-0-1 loss: 1.250793  [   64/  118]
train() client id: f_00009-0-2 loss: 1.164762  [   96/  118]
train() client id: f_00009-1-0 loss: 1.075414  [   32/  118]
train() client id: f_00009-1-1 loss: 1.063522  [   64/  118]
train() client id: f_00009-1-2 loss: 1.135586  [   96/  118]
train() client id: f_00009-2-0 loss: 1.245697  [   32/  118]
train() client id: f_00009-2-1 loss: 0.958587  [   64/  118]
train() client id: f_00009-2-2 loss: 0.976452  [   96/  118]
train() client id: f_00009-3-0 loss: 1.027268  [   32/  118]
train() client id: f_00009-3-1 loss: 1.137556  [   64/  118]
train() client id: f_00009-3-2 loss: 1.003552  [   96/  118]
train() client id: f_00009-4-0 loss: 1.047551  [   32/  118]
train() client id: f_00009-4-1 loss: 0.946765  [   64/  118]
train() client id: f_00009-4-2 loss: 0.965722  [   96/  118]
train() client id: f_00009-5-0 loss: 0.966444  [   32/  118]
train() client id: f_00009-5-1 loss: 1.011689  [   64/  118]
train() client id: f_00009-5-2 loss: 0.991955  [   96/  118]
train() client id: f_00009-6-0 loss: 1.010070  [   32/  118]
train() client id: f_00009-6-1 loss: 0.980639  [   64/  118]
train() client id: f_00009-6-2 loss: 0.896254  [   96/  118]
train() client id: f_00009-7-0 loss: 0.909574  [   32/  118]
train() client id: f_00009-7-1 loss: 0.977097  [   64/  118]
train() client id: f_00009-7-2 loss: 0.967574  [   96/  118]
train() client id: f_00009-8-0 loss: 1.028267  [   32/  118]
train() client id: f_00009-8-1 loss: 0.920168  [   64/  118]
train() client id: f_00009-8-2 loss: 0.851931  [   96/  118]
train() client id: f_00009-9-0 loss: 0.857387  [   32/  118]
train() client id: f_00009-9-1 loss: 0.965805  [   64/  118]
train() client id: f_00009-9-2 loss: 0.926532  [   96/  118]
train() client id: f_00009-10-0 loss: 0.929460  [   32/  118]
train() client id: f_00009-10-1 loss: 0.873342  [   64/  118]
train() client id: f_00009-10-2 loss: 0.809074  [   96/  118]
train() client id: f_00009-11-0 loss: 0.877823  [   32/  118]
train() client id: f_00009-11-1 loss: 0.795898  [   64/  118]
train() client id: f_00009-11-2 loss: 0.964992  [   96/  118]
At round 7 accuracy: 0.6286472148541115
At round 7 training accuracy: 0.5720992622401073
At round 7 training loss: 0.8835374873465481
update_location
xs = [ -3.9056584    4.20031788  55.00902392  18.81129433   0.97929623
   3.95640986 -17.44319194  -1.32485185  39.66397685  -2.06087855]
ys = [ 47.5879595   30.55583871   1.32061395 -17.45517586   9.35018685
  -7.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [110.81456609 104.64846845 114.13911132 103.24024389 100.44095288
 100.33588395 101.54385992 100.09599397 109.0041331  100.10124413]
dists_bs = [213.32393628 230.16704729 288.21637743 273.13332372 241.6786571
 255.37635032 237.43374407 249.53494358 266.20507287 243.20450851]
uav_gains = [7.73572375e-11 8.92616175e-11 7.18461074e-11 9.23368231e-11
 9.89056819e-11 9.91648192e-11 9.62418239e-11 9.97600464e-11
 8.06096645e-11 9.97469659e-11]
bs_gains = [3.32646713e-11 2.68889148e-11 1.43245785e-11 1.66511777e-11
 2.34545361e-11 2.00996360e-11 2.46476348e-11 2.14450097e-11
 1.78932145e-11 2.30448319e-11]
Round 8
-------------------------------
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.93572436 20.7513301   9.78311252  3.49713647 23.93609638 11.54337877
  4.34848004 14.03581921 10.31003617  9.36568175]
obj_prev = 117.50679575560986
eta_min = 4.500137844098843e-10	eta_max = 0.9201793140385103
af = 24.843282687249765	bf = 1.924183044657018	zeta = 27.327610955974745	eta = 0.909090909090909
af = 24.843282687249765	bf = 1.924183044657018	zeta = 47.21200862959305	eta = 0.5262068572883742
af = 24.843282687249765	bf = 1.924183044657018	zeta = 37.72761010314322	eta = 0.6584907609925703
af = 24.843282687249765	bf = 1.924183044657018	zeta = 36.02779691629381	eta = 0.689558752231509
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.944047891502834	eta = 0.6911654124832922
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.94383021535565	eta = 0.6911695981870181
eta = 0.6911695981870181
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.03047953 0.06410378 0.02999573 0.01040174 0.07402172 0.03531755
 0.01306266 0.04330029 0.03144714 0.02854432]
ene_total = [3.05537056 5.9604125  3.0232152  1.38370094 6.79982988 3.63829942
 1.59895401 4.0866222  3.34115314 3.05627237]
ti_comp = [0.29321835 0.27683249 0.29228351 0.29535599 0.2741193  0.27083978
 0.29583662 0.2962475  0.2682047  0.2737568 ]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.05836579e-05 2.14830890e-04 1.97446596e-05 8.06319238e-07
 3.37348113e-04 3.75341750e-05 1.59173447e-06 5.78153199e-05
 2.70203220e-05 1.93958759e-05]
ene_total = [0.54608481 0.69707832 0.55371718 0.52684511 0.72952358 0.73184161
 0.52295022 0.52419719 0.75268396 0.70631616]
optimize_network iter = 0 obj = 6.291238143797367
eta = 0.6911695981870181
freqs = [5.19741187e+07 1.15780803e+08 5.13127289e+07 1.76088264e+07
 1.35017343e+08 6.52000814e+07 2.20774830e+07 7.30812690e+07
 5.86252507e+07 5.21344532e+07]
eta_min = 0.6875884070281042	eta_max = 0.6911695981870177
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 0.05581615321591653	eta = 0.909090909090909
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 21.219211967217113	eta = 0.002391321484860486
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.2358034144344674	eta = 0.022695178449689362
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1654164527348314	eta = 0.02343288627225956
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1653901507160183	eta = 0.02343317090097433
eta = 0.02343317090097433
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09174714e-04 2.18314889e-03 2.00648668e-04 8.19395643e-06
 3.42819024e-03 3.81428819e-04 1.61754827e-05 5.87529343e-04
 2.74585215e-04 1.97104266e-04]
ene_total = [0.17760768 0.27172476 0.17982108 0.16680246 0.3112301  0.24039767
 0.16575821 0.17957311 0.24447922 0.22799587]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 1 obj = 6.219849079279121
eta = 0.6875884070281042
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
eta_min = 0.6875884070281044	eta_max = 0.6875884070280862
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 0.055709725023689025	eta = 0.909090909090909
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 21.21911053031574	eta = 0.0023867732106222467
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.23530448131582	eta = 0.022656960154787298
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650443530624273	eta = 0.023392224965439173
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650181860209075	eta = 0.023392507690695646
eta = 0.023392507690695646
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09211545e-04 2.17991631e-03 2.00666013e-04 8.19706147e-06
 3.42210961e-03 3.80614259e-04 1.61823437e-05 5.87801193e-04
 2.73917274e-04 1.96746859e-04]
ene_total = [0.17760125 0.27162923 0.17981404 0.16679559 0.31105873 0.24036644
 0.16575149 0.17957271 0.24445164 0.22797706]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 2 obj = 6.219849079279126
eta = 0.6875884070281044
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
Done!
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.04160455e-05 2.12728559e-04 1.95821241e-05 7.99915605e-07
 3.33948805e-04 3.71424915e-05 1.57916459e-06 5.73609639e-05
 2.67303963e-05 1.91996709e-05]
ene_total = [0.00662848 0.00845938 0.00672113 0.0063951  0.00885192 0.00888306
 0.00634781 0.00636251 0.00913616 0.00857342]
At round 8 energy consumption: 0.07635896655888749
At round 8 eta: 0.6875884070281044
At round 8 a_n: 25.442236080499836
At round 8 local rounds: 12.265156719696218
At round 8 global rounds: 81.4381945256065
gradient difference: 0.3709415793418884
train() client id: f_00000-0-0 loss: 1.562837  [   32/  126]
train() client id: f_00000-0-1 loss: 1.132296  [   64/  126]
train() client id: f_00000-0-2 loss: 1.285977  [   96/  126]
train() client id: f_00000-1-0 loss: 1.380982  [   32/  126]
train() client id: f_00000-1-1 loss: 1.049874  [   64/  126]
train() client id: f_00000-1-2 loss: 1.256592  [   96/  126]
train() client id: f_00000-2-0 loss: 1.153107  [   32/  126]
train() client id: f_00000-2-1 loss: 1.189292  [   64/  126]
train() client id: f_00000-2-2 loss: 1.112496  [   96/  126]
train() client id: f_00000-3-0 loss: 1.062048  [   32/  126]
train() client id: f_00000-3-1 loss: 1.164641  [   64/  126]
train() client id: f_00000-3-2 loss: 1.037951  [   96/  126]
train() client id: f_00000-4-0 loss: 1.064615  [   32/  126]
train() client id: f_00000-4-1 loss: 0.995838  [   64/  126]
train() client id: f_00000-4-2 loss: 1.048007  [   96/  126]
train() client id: f_00000-5-0 loss: 1.029516  [   32/  126]
train() client id: f_00000-5-1 loss: 0.960299  [   64/  126]
train() client id: f_00000-5-2 loss: 0.971654  [   96/  126]
train() client id: f_00000-6-0 loss: 0.940346  [   32/  126]
train() client id: f_00000-6-1 loss: 0.950253  [   64/  126]
train() client id: f_00000-6-2 loss: 0.986283  [   96/  126]
train() client id: f_00000-7-0 loss: 0.979705  [   32/  126]
train() client id: f_00000-7-1 loss: 0.908348  [   64/  126]
train() client id: f_00000-7-2 loss: 0.899651  [   96/  126]
train() client id: f_00000-8-0 loss: 0.982976  [   32/  126]
train() client id: f_00000-8-1 loss: 0.895789  [   64/  126]
train() client id: f_00000-8-2 loss: 0.865017  [   96/  126]
train() client id: f_00000-9-0 loss: 0.886309  [   32/  126]
train() client id: f_00000-9-1 loss: 0.925931  [   64/  126]
train() client id: f_00000-9-2 loss: 0.963275  [   96/  126]
train() client id: f_00000-10-0 loss: 0.864786  [   32/  126]
train() client id: f_00000-10-1 loss: 1.014831  [   64/  126]
train() client id: f_00000-10-2 loss: 0.853723  [   96/  126]
train() client id: f_00000-11-0 loss: 0.866024  [   32/  126]
train() client id: f_00000-11-1 loss: 1.012798  [   64/  126]
train() client id: f_00000-11-2 loss: 0.890421  [   96/  126]
train() client id: f_00001-0-0 loss: 0.465041  [   32/  265]
train() client id: f_00001-0-1 loss: 0.536814  [   64/  265]
train() client id: f_00001-0-2 loss: 0.549891  [   96/  265]
train() client id: f_00001-0-3 loss: 0.500418  [  128/  265]
train() client id: f_00001-0-4 loss: 0.476643  [  160/  265]
train() client id: f_00001-0-5 loss: 0.485813  [  192/  265]
train() client id: f_00001-0-6 loss: 0.505031  [  224/  265]
train() client id: f_00001-0-7 loss: 0.463919  [  256/  265]
train() client id: f_00001-1-0 loss: 0.423575  [   32/  265]
train() client id: f_00001-1-1 loss: 0.425177  [   64/  265]
train() client id: f_00001-1-2 loss: 0.398112  [   96/  265]
train() client id: f_00001-1-3 loss: 0.544276  [  128/  265]
train() client id: f_00001-1-4 loss: 0.551500  [  160/  265]
train() client id: f_00001-1-5 loss: 0.400626  [  192/  265]
train() client id: f_00001-1-6 loss: 0.457776  [  224/  265]
train() client id: f_00001-1-7 loss: 0.509513  [  256/  265]
train() client id: f_00001-2-0 loss: 0.440113  [   32/  265]
train() client id: f_00001-2-1 loss: 0.449410  [   64/  265]
train() client id: f_00001-2-2 loss: 0.416730  [   96/  265]
train() client id: f_00001-2-3 loss: 0.565708  [  128/  265]
train() client id: f_00001-2-4 loss: 0.478653  [  160/  265]
train() client id: f_00001-2-5 loss: 0.497232  [  192/  265]
train() client id: f_00001-2-6 loss: 0.362816  [  224/  265]
train() client id: f_00001-2-7 loss: 0.484491  [  256/  265]
train() client id: f_00001-3-0 loss: 0.470285  [   32/  265]
train() client id: f_00001-3-1 loss: 0.517336  [   64/  265]
train() client id: f_00001-3-2 loss: 0.399045  [   96/  265]
train() client id: f_00001-3-3 loss: 0.500838  [  128/  265]
train() client id: f_00001-3-4 loss: 0.375614  [  160/  265]
train() client id: f_00001-3-5 loss: 0.445341  [  192/  265]
train() client id: f_00001-3-6 loss: 0.428268  [  224/  265]
train() client id: f_00001-3-7 loss: 0.458910  [  256/  265]
train() client id: f_00001-4-0 loss: 0.415210  [   32/  265]
train() client id: f_00001-4-1 loss: 0.474086  [   64/  265]
train() client id: f_00001-4-2 loss: 0.463100  [   96/  265]
train() client id: f_00001-4-3 loss: 0.402869  [  128/  265]
train() client id: f_00001-4-4 loss: 0.439590  [  160/  265]
train() client id: f_00001-4-5 loss: 0.356733  [  192/  265]
train() client id: f_00001-4-6 loss: 0.527658  [  224/  265]
train() client id: f_00001-4-7 loss: 0.397569  [  256/  265]
train() client id: f_00001-5-0 loss: 0.398072  [   32/  265]
train() client id: f_00001-5-1 loss: 0.420947  [   64/  265]
train() client id: f_00001-5-2 loss: 0.404235  [   96/  265]
train() client id: f_00001-5-3 loss: 0.423658  [  128/  265]
train() client id: f_00001-5-4 loss: 0.349770  [  160/  265]
train() client id: f_00001-5-5 loss: 0.389986  [  192/  265]
train() client id: f_00001-5-6 loss: 0.535708  [  224/  265]
train() client id: f_00001-5-7 loss: 0.533684  [  256/  265]
train() client id: f_00001-6-0 loss: 0.438287  [   32/  265]
train() client id: f_00001-6-1 loss: 0.350718  [   64/  265]
train() client id: f_00001-6-2 loss: 0.519334  [   96/  265]
train() client id: f_00001-6-3 loss: 0.520571  [  128/  265]
train() client id: f_00001-6-4 loss: 0.442135  [  160/  265]
train() client id: f_00001-6-5 loss: 0.367459  [  192/  265]
train() client id: f_00001-6-6 loss: 0.409065  [  224/  265]
train() client id: f_00001-6-7 loss: 0.377498  [  256/  265]
train() client id: f_00001-7-0 loss: 0.407226  [   32/  265]
train() client id: f_00001-7-1 loss: 0.457556  [   64/  265]
train() client id: f_00001-7-2 loss: 0.417989  [   96/  265]
train() client id: f_00001-7-3 loss: 0.425836  [  128/  265]
train() client id: f_00001-7-4 loss: 0.372998  [  160/  265]
train() client id: f_00001-7-5 loss: 0.403674  [  192/  265]
train() client id: f_00001-7-6 loss: 0.488476  [  224/  265]
train() client id: f_00001-7-7 loss: 0.412457  [  256/  265]
train() client id: f_00001-8-0 loss: 0.535568  [   32/  265]
train() client id: f_00001-8-1 loss: 0.385542  [   64/  265]
train() client id: f_00001-8-2 loss: 0.341560  [   96/  265]
train() client id: f_00001-8-3 loss: 0.348454  [  128/  265]
train() client id: f_00001-8-4 loss: 0.397590  [  160/  265]
train() client id: f_00001-8-5 loss: 0.439790  [  192/  265]
train() client id: f_00001-8-6 loss: 0.486578  [  224/  265]
train() client id: f_00001-8-7 loss: 0.376841  [  256/  265]
train() client id: f_00001-9-0 loss: 0.407897  [   32/  265]
train() client id: f_00001-9-1 loss: 0.330208  [   64/  265]
train() client id: f_00001-9-2 loss: 0.462096  [   96/  265]
train() client id: f_00001-9-3 loss: 0.480341  [  128/  265]
train() client id: f_00001-9-4 loss: 0.434067  [  160/  265]
train() client id: f_00001-9-5 loss: 0.335574  [  192/  265]
train() client id: f_00001-9-6 loss: 0.378098  [  224/  265]
train() client id: f_00001-9-7 loss: 0.470691  [  256/  265]
train() client id: f_00001-10-0 loss: 0.336030  [   32/  265]
train() client id: f_00001-10-1 loss: 0.386218  [   64/  265]
train() client id: f_00001-10-2 loss: 0.396341  [   96/  265]
train() client id: f_00001-10-3 loss: 0.428941  [  128/  265]
train() client id: f_00001-10-4 loss: 0.454754  [  160/  265]
train() client id: f_00001-10-5 loss: 0.510012  [  192/  265]
train() client id: f_00001-10-6 loss: 0.433964  [  224/  265]
train() client id: f_00001-10-7 loss: 0.341900  [  256/  265]
train() client id: f_00001-11-0 loss: 0.330814  [   32/  265]
train() client id: f_00001-11-1 loss: 0.385348  [   64/  265]
train() client id: f_00001-11-2 loss: 0.328620  [   96/  265]
train() client id: f_00001-11-3 loss: 0.601034  [  128/  265]
train() client id: f_00001-11-4 loss: 0.456596  [  160/  265]
train() client id: f_00001-11-5 loss: 0.387233  [  192/  265]
train() client id: f_00001-11-6 loss: 0.402864  [  224/  265]
train() client id: f_00001-11-7 loss: 0.417759  [  256/  265]
train() client id: f_00002-0-0 loss: 1.126824  [   32/  124]
train() client id: f_00002-0-1 loss: 1.226698  [   64/  124]
train() client id: f_00002-0-2 loss: 1.096534  [   96/  124]
train() client id: f_00002-1-0 loss: 1.100439  [   32/  124]
train() client id: f_00002-1-1 loss: 1.095876  [   64/  124]
train() client id: f_00002-1-2 loss: 1.041171  [   96/  124]
train() client id: f_00002-2-0 loss: 1.031349  [   32/  124]
train() client id: f_00002-2-1 loss: 1.098202  [   64/  124]
train() client id: f_00002-2-2 loss: 1.200621  [   96/  124]
train() client id: f_00002-3-0 loss: 1.073977  [   32/  124]
train() client id: f_00002-3-1 loss: 1.057375  [   64/  124]
train() client id: f_00002-3-2 loss: 1.117861  [   96/  124]
train() client id: f_00002-4-0 loss: 1.053856  [   32/  124]
train() client id: f_00002-4-1 loss: 1.019787  [   64/  124]
train() client id: f_00002-4-2 loss: 1.041926  [   96/  124]
train() client id: f_00002-5-0 loss: 1.016968  [   32/  124]
train() client id: f_00002-5-1 loss: 1.116304  [   64/  124]
train() client id: f_00002-5-2 loss: 0.983554  [   96/  124]
train() client id: f_00002-6-0 loss: 0.886487  [   32/  124]
train() client id: f_00002-6-1 loss: 1.103053  [   64/  124]
train() client id: f_00002-6-2 loss: 1.072481  [   96/  124]
train() client id: f_00002-7-0 loss: 0.992602  [   32/  124]
train() client id: f_00002-7-1 loss: 0.988766  [   64/  124]
train() client id: f_00002-7-2 loss: 0.961085  [   96/  124]
train() client id: f_00002-8-0 loss: 1.023236  [   32/  124]
train() client id: f_00002-8-1 loss: 1.096179  [   64/  124]
train() client id: f_00002-8-2 loss: 0.875782  [   96/  124]
train() client id: f_00002-9-0 loss: 1.021546  [   32/  124]
train() client id: f_00002-9-1 loss: 0.986355  [   64/  124]
train() client id: f_00002-9-2 loss: 0.983478  [   96/  124]
train() client id: f_00002-10-0 loss: 0.910959  [   32/  124]
train() client id: f_00002-10-1 loss: 1.100985  [   64/  124]
train() client id: f_00002-10-2 loss: 1.035355  [   96/  124]
train() client id: f_00002-11-0 loss: 0.892178  [   32/  124]
train() client id: f_00002-11-1 loss: 1.049842  [   64/  124]
train() client id: f_00002-11-2 loss: 0.965584  [   96/  124]
train() client id: f_00003-0-0 loss: 1.000256  [   32/   43]
train() client id: f_00003-1-0 loss: 1.103217  [   32/   43]
train() client id: f_00003-2-0 loss: 0.998976  [   32/   43]
train() client id: f_00003-3-0 loss: 0.966809  [   32/   43]
train() client id: f_00003-4-0 loss: 1.071738  [   32/   43]
train() client id: f_00003-5-0 loss: 1.030452  [   32/   43]
train() client id: f_00003-6-0 loss: 1.030287  [   32/   43]
train() client id: f_00003-7-0 loss: 1.007474  [   32/   43]
train() client id: f_00003-8-0 loss: 1.009117  [   32/   43]
train() client id: f_00003-9-0 loss: 0.980677  [   32/   43]
train() client id: f_00003-10-0 loss: 0.972622  [   32/   43]
train() client id: f_00003-11-0 loss: 1.014215  [   32/   43]
train() client id: f_00004-0-0 loss: 0.991308  [   32/  306]
train() client id: f_00004-0-1 loss: 0.958297  [   64/  306]
train() client id: f_00004-0-2 loss: 1.059167  [   96/  306]
train() client id: f_00004-0-3 loss: 1.038003  [  128/  306]
train() client id: f_00004-0-4 loss: 1.127531  [  160/  306]
train() client id: f_00004-0-5 loss: 1.050426  [  192/  306]
train() client id: f_00004-0-6 loss: 0.938360  [  224/  306]
train() client id: f_00004-0-7 loss: 1.005815  [  256/  306]
train() client id: f_00004-0-8 loss: 0.941881  [  288/  306]
train() client id: f_00004-1-0 loss: 0.907241  [   32/  306]
train() client id: f_00004-1-1 loss: 1.081270  [   64/  306]
train() client id: f_00004-1-2 loss: 1.011841  [   96/  306]
train() client id: f_00004-1-3 loss: 1.066791  [  128/  306]
train() client id: f_00004-1-4 loss: 1.070178  [  160/  306]
train() client id: f_00004-1-5 loss: 1.057795  [  192/  306]
train() client id: f_00004-1-6 loss: 0.901621  [  224/  306]
train() client id: f_00004-1-7 loss: 0.937388  [  256/  306]
train() client id: f_00004-1-8 loss: 1.107826  [  288/  306]
train() client id: f_00004-2-0 loss: 0.985574  [   32/  306]
train() client id: f_00004-2-1 loss: 1.012682  [   64/  306]
train() client id: f_00004-2-2 loss: 0.985123  [   96/  306]
train() client id: f_00004-2-3 loss: 1.076085  [  128/  306]
train() client id: f_00004-2-4 loss: 0.898471  [  160/  306]
train() client id: f_00004-2-5 loss: 1.076447  [  192/  306]
train() client id: f_00004-2-6 loss: 1.012353  [  224/  306]
train() client id: f_00004-2-7 loss: 1.090435  [  256/  306]
train() client id: f_00004-2-8 loss: 1.015842  [  288/  306]
train() client id: f_00004-3-0 loss: 0.912563  [   32/  306]
train() client id: f_00004-3-1 loss: 0.983817  [   64/  306]
train() client id: f_00004-3-2 loss: 1.015636  [   96/  306]
train() client id: f_00004-3-3 loss: 1.061129  [  128/  306]
train() client id: f_00004-3-4 loss: 0.981451  [  160/  306]
train() client id: f_00004-3-5 loss: 1.024310  [  192/  306]
train() client id: f_00004-3-6 loss: 0.998668  [  224/  306]
train() client id: f_00004-3-7 loss: 0.951624  [  256/  306]
train() client id: f_00004-3-8 loss: 1.164929  [  288/  306]
train() client id: f_00004-4-0 loss: 0.914765  [   32/  306]
train() client id: f_00004-4-1 loss: 1.028846  [   64/  306]
train() client id: f_00004-4-2 loss: 0.959751  [   96/  306]
train() client id: f_00004-4-3 loss: 0.883356  [  128/  306]
train() client id: f_00004-4-4 loss: 1.084877  [  160/  306]
train() client id: f_00004-4-5 loss: 1.005701  [  192/  306]
train() client id: f_00004-4-6 loss: 1.075969  [  224/  306]
train() client id: f_00004-4-7 loss: 1.090726  [  256/  306]
train() client id: f_00004-4-8 loss: 1.059522  [  288/  306]
train() client id: f_00004-5-0 loss: 0.862078  [   32/  306]
train() client id: f_00004-5-1 loss: 0.970953  [   64/  306]
train() client id: f_00004-5-2 loss: 1.003306  [   96/  306]
train() client id: f_00004-5-3 loss: 0.900007  [  128/  306]
train() client id: f_00004-5-4 loss: 1.045908  [  160/  306]
train() client id: f_00004-5-5 loss: 0.962460  [  192/  306]
train() client id: f_00004-5-6 loss: 1.192192  [  224/  306]
train() client id: f_00004-5-7 loss: 1.062995  [  256/  306]
train() client id: f_00004-5-8 loss: 1.048377  [  288/  306]
train() client id: f_00004-6-0 loss: 1.074621  [   32/  306]
train() client id: f_00004-6-1 loss: 1.026796  [   64/  306]
train() client id: f_00004-6-2 loss: 1.034013  [   96/  306]
train() client id: f_00004-6-3 loss: 0.891888  [  128/  306]
train() client id: f_00004-6-4 loss: 1.053630  [  160/  306]
train() client id: f_00004-6-5 loss: 0.977901  [  192/  306]
train() client id: f_00004-6-6 loss: 0.950695  [  224/  306]
train() client id: f_00004-6-7 loss: 1.042125  [  256/  306]
train() client id: f_00004-6-8 loss: 1.005829  [  288/  306]
train() client id: f_00004-7-0 loss: 1.005751  [   32/  306]
train() client id: f_00004-7-1 loss: 0.919352  [   64/  306]
train() client id: f_00004-7-2 loss: 0.924785  [   96/  306]
train() client id: f_00004-7-3 loss: 1.074760  [  128/  306]
train() client id: f_00004-7-4 loss: 1.117183  [  160/  306]
train() client id: f_00004-7-5 loss: 1.004185  [  192/  306]
train() client id: f_00004-7-6 loss: 1.006936  [  224/  306]
train() client id: f_00004-7-7 loss: 1.017864  [  256/  306]
train() client id: f_00004-7-8 loss: 0.958150  [  288/  306]
train() client id: f_00004-8-0 loss: 1.042657  [   32/  306]
train() client id: f_00004-8-1 loss: 0.993684  [   64/  306]
train() client id: f_00004-8-2 loss: 1.057008  [   96/  306]
train() client id: f_00004-8-3 loss: 0.987225  [  128/  306]
train() client id: f_00004-8-4 loss: 0.912638  [  160/  306]
train() client id: f_00004-8-5 loss: 0.989567  [  192/  306]
train() client id: f_00004-8-6 loss: 0.942734  [  224/  306]
train() client id: f_00004-8-7 loss: 0.987044  [  256/  306]
train() client id: f_00004-8-8 loss: 1.023662  [  288/  306]
train() client id: f_00004-9-0 loss: 1.082075  [   32/  306]
train() client id: f_00004-9-1 loss: 0.998151  [   64/  306]
train() client id: f_00004-9-2 loss: 0.891968  [   96/  306]
train() client id: f_00004-9-3 loss: 1.011003  [  128/  306]
train() client id: f_00004-9-4 loss: 1.046793  [  160/  306]
train() client id: f_00004-9-5 loss: 0.969747  [  192/  306]
train() client id: f_00004-9-6 loss: 0.962032  [  224/  306]
train() client id: f_00004-9-7 loss: 1.045822  [  256/  306]
train() client id: f_00004-9-8 loss: 1.029858  [  288/  306]
train() client id: f_00004-10-0 loss: 1.055373  [   32/  306]
train() client id: f_00004-10-1 loss: 1.009982  [   64/  306]
train() client id: f_00004-10-2 loss: 1.055515  [   96/  306]
train() client id: f_00004-10-3 loss: 0.946857  [  128/  306]
train() client id: f_00004-10-4 loss: 0.990338  [  160/  306]
train() client id: f_00004-10-5 loss: 0.973253  [  192/  306]
train() client id: f_00004-10-6 loss: 1.066724  [  224/  306]
train() client id: f_00004-10-7 loss: 0.949087  [  256/  306]
train() client id: f_00004-10-8 loss: 0.932144  [  288/  306]
train() client id: f_00004-11-0 loss: 0.908854  [   32/  306]
train() client id: f_00004-11-1 loss: 0.993030  [   64/  306]
train() client id: f_00004-11-2 loss: 0.957664  [   96/  306]
train() client id: f_00004-11-3 loss: 0.954280  [  128/  306]
train() client id: f_00004-11-4 loss: 1.024148  [  160/  306]
train() client id: f_00004-11-5 loss: 1.010523  [  192/  306]
train() client id: f_00004-11-6 loss: 1.126370  [  224/  306]
train() client id: f_00004-11-7 loss: 0.895087  [  256/  306]
train() client id: f_00004-11-8 loss: 1.087808  [  288/  306]
train() client id: f_00005-0-0 loss: 0.827429  [   32/  146]
train() client id: f_00005-0-1 loss: 0.838203  [   64/  146]
train() client id: f_00005-0-2 loss: 0.904949  [   96/  146]
train() client id: f_00005-0-3 loss: 0.844726  [  128/  146]
train() client id: f_00005-1-0 loss: 0.812996  [   32/  146]
train() client id: f_00005-1-1 loss: 0.992176  [   64/  146]
train() client id: f_00005-1-2 loss: 0.859285  [   96/  146]
train() client id: f_00005-1-3 loss: 0.866296  [  128/  146]
train() client id: f_00005-2-0 loss: 0.869899  [   32/  146]
train() client id: f_00005-2-1 loss: 0.912169  [   64/  146]
train() client id: f_00005-2-2 loss: 0.827423  [   96/  146]
train() client id: f_00005-2-3 loss: 0.796802  [  128/  146]
train() client id: f_00005-3-0 loss: 0.773654  [   32/  146]
train() client id: f_00005-3-1 loss: 1.025449  [   64/  146]
train() client id: f_00005-3-2 loss: 0.769661  [   96/  146]
train() client id: f_00005-3-3 loss: 0.851014  [  128/  146]
train() client id: f_00005-4-0 loss: 0.897822  [   32/  146]
train() client id: f_00005-4-1 loss: 0.901891  [   64/  146]
train() client id: f_00005-4-2 loss: 0.849687  [   96/  146]
train() client id: f_00005-4-3 loss: 0.761872  [  128/  146]
train() client id: f_00005-5-0 loss: 0.815006  [   32/  146]
train() client id: f_00005-5-1 loss: 0.709157  [   64/  146]
train() client id: f_00005-5-2 loss: 0.899996  [   96/  146]
train() client id: f_00005-5-3 loss: 0.998248  [  128/  146]
train() client id: f_00005-6-0 loss: 0.895020  [   32/  146]
train() client id: f_00005-6-1 loss: 0.909654  [   64/  146]
train() client id: f_00005-6-2 loss: 0.917796  [   96/  146]
train() client id: f_00005-6-3 loss: 0.754149  [  128/  146]
train() client id: f_00005-7-0 loss: 0.785370  [   32/  146]
train() client id: f_00005-7-1 loss: 0.999889  [   64/  146]
train() client id: f_00005-7-2 loss: 0.920116  [   96/  146]
train() client id: f_00005-7-3 loss: 0.827055  [  128/  146]
train() client id: f_00005-8-0 loss: 0.824799  [   32/  146]
train() client id: f_00005-8-1 loss: 0.790856  [   64/  146]
train() client id: f_00005-8-2 loss: 1.000646  [   96/  146]
train() client id: f_00005-8-3 loss: 0.879537  [  128/  146]
train() client id: f_00005-9-0 loss: 0.862167  [   32/  146]
train() client id: f_00005-9-1 loss: 0.883885  [   64/  146]
train() client id: f_00005-9-2 loss: 0.960824  [   96/  146]
train() client id: f_00005-9-3 loss: 0.833460  [  128/  146]
train() client id: f_00005-10-0 loss: 0.694227  [   32/  146]
train() client id: f_00005-10-1 loss: 0.836236  [   64/  146]
train() client id: f_00005-10-2 loss: 0.816678  [   96/  146]
train() client id: f_00005-10-3 loss: 1.052993  [  128/  146]
train() client id: f_00005-11-0 loss: 0.728057  [   32/  146]
train() client id: f_00005-11-1 loss: 0.736775  [   64/  146]
train() client id: f_00005-11-2 loss: 0.989341  [   96/  146]
train() client id: f_00005-11-3 loss: 0.966391  [  128/  146]
train() client id: f_00006-0-0 loss: 0.875057  [   32/   54]
train() client id: f_00006-1-0 loss: 0.772509  [   32/   54]
train() client id: f_00006-2-0 loss: 0.816748  [   32/   54]
train() client id: f_00006-3-0 loss: 0.823615  [   32/   54]
train() client id: f_00006-4-0 loss: 0.769628  [   32/   54]
train() client id: f_00006-5-0 loss: 0.831620  [   32/   54]
train() client id: f_00006-6-0 loss: 0.807281  [   32/   54]
train() client id: f_00006-7-0 loss: 0.816927  [   32/   54]
train() client id: f_00006-8-0 loss: 0.822037  [   32/   54]
train() client id: f_00006-9-0 loss: 0.759298  [   32/   54]
train() client id: f_00006-10-0 loss: 0.845281  [   32/   54]
train() client id: f_00006-11-0 loss: 0.881553  [   32/   54]
train() client id: f_00007-0-0 loss: 0.639445  [   32/  179]
train() client id: f_00007-0-1 loss: 0.656794  [   64/  179]
train() client id: f_00007-0-2 loss: 0.630190  [   96/  179]
train() client id: f_00007-0-3 loss: 0.628829  [  128/  179]
train() client id: f_00007-0-4 loss: 0.743303  [  160/  179]
train() client id: f_00007-1-0 loss: 0.631917  [   32/  179]
train() client id: f_00007-1-1 loss: 0.609594  [   64/  179]
train() client id: f_00007-1-2 loss: 0.603856  [   96/  179]
train() client id: f_00007-1-3 loss: 0.605914  [  128/  179]
train() client id: f_00007-1-4 loss: 0.658386  [  160/  179]
train() client id: f_00007-2-0 loss: 0.633037  [   32/  179]
train() client id: f_00007-2-1 loss: 0.582337  [   64/  179]
train() client id: f_00007-2-2 loss: 0.728347  [   96/  179]
train() client id: f_00007-2-3 loss: 0.621832  [  128/  179]
train() client id: f_00007-2-4 loss: 0.539655  [  160/  179]
train() client id: f_00007-3-0 loss: 0.499602  [   32/  179]
train() client id: f_00007-3-1 loss: 0.698231  [   64/  179]
train() client id: f_00007-3-2 loss: 0.548435  [   96/  179]
train() client id: f_00007-3-3 loss: 0.694247  [  128/  179]
train() client id: f_00007-3-4 loss: 0.571692  [  160/  179]
train() client id: f_00007-4-0 loss: 0.611884  [   32/  179]
train() client id: f_00007-4-1 loss: 0.560239  [   64/  179]
train() client id: f_00007-4-2 loss: 0.675412  [   96/  179]
train() client id: f_00007-4-3 loss: 0.491051  [  128/  179]
train() client id: f_00007-4-4 loss: 0.606403  [  160/  179]
train() client id: f_00007-5-0 loss: 0.496286  [   32/  179]
train() client id: f_00007-5-1 loss: 0.541871  [   64/  179]
train() client id: f_00007-5-2 loss: 0.538193  [   96/  179]
train() client id: f_00007-5-3 loss: 0.467068  [  128/  179]
train() client id: f_00007-5-4 loss: 0.666582  [  160/  179]
train() client id: f_00007-6-0 loss: 0.557273  [   32/  179]
train() client id: f_00007-6-1 loss: 0.710486  [   64/  179]
train() client id: f_00007-6-2 loss: 0.480341  [   96/  179]
train() client id: f_00007-6-3 loss: 0.528258  [  128/  179]
train() client id: f_00007-6-4 loss: 0.599201  [  160/  179]
train() client id: f_00007-7-0 loss: 0.561218  [   32/  179]
train() client id: f_00007-7-1 loss: 0.569451  [   64/  179]
train() client id: f_00007-7-2 loss: 0.576259  [   96/  179]
train() client id: f_00007-7-3 loss: 0.589383  [  128/  179]
train() client id: f_00007-7-4 loss: 0.540056  [  160/  179]
train() client id: f_00007-8-0 loss: 0.664191  [   32/  179]
train() client id: f_00007-8-1 loss: 0.451264  [   64/  179]
train() client id: f_00007-8-2 loss: 0.589445  [   96/  179]
train() client id: f_00007-8-3 loss: 0.506739  [  128/  179]
train() client id: f_00007-8-4 loss: 0.523876  [  160/  179]
train() client id: f_00007-9-0 loss: 0.458480  [   32/  179]
train() client id: f_00007-9-1 loss: 0.503743  [   64/  179]
train() client id: f_00007-9-2 loss: 0.597429  [   96/  179]
train() client id: f_00007-9-3 loss: 0.565724  [  128/  179]
train() client id: f_00007-9-4 loss: 0.523029  [  160/  179]
train() client id: f_00007-10-0 loss: 0.528136  [   32/  179]
train() client id: f_00007-10-1 loss: 0.502942  [   64/  179]
train() client id: f_00007-10-2 loss: 0.437451  [   96/  179]
train() client id: f_00007-10-3 loss: 0.508958  [  128/  179]
train() client id: f_00007-10-4 loss: 0.573511  [  160/  179]
train() client id: f_00007-11-0 loss: 0.737345  [   32/  179]
train() client id: f_00007-11-1 loss: 0.425749  [   64/  179]
train() client id: f_00007-11-2 loss: 0.565009  [   96/  179]
train() client id: f_00007-11-3 loss: 0.506578  [  128/  179]
train() client id: f_00007-11-4 loss: 0.531775  [  160/  179]
train() client id: f_00008-0-0 loss: 0.900366  [   32/  130]
train() client id: f_00008-0-1 loss: 0.763254  [   64/  130]
train() client id: f_00008-0-2 loss: 0.922331  [   96/  130]
train() client id: f_00008-0-3 loss: 0.808329  [  128/  130]
train() client id: f_00008-1-0 loss: 0.838700  [   32/  130]
train() client id: f_00008-1-1 loss: 0.938302  [   64/  130]
train() client id: f_00008-1-2 loss: 0.749868  [   96/  130]
train() client id: f_00008-1-3 loss: 0.854716  [  128/  130]
train() client id: f_00008-2-0 loss: 0.735285  [   32/  130]
train() client id: f_00008-2-1 loss: 0.889053  [   64/  130]
train() client id: f_00008-2-2 loss: 0.883030  [   96/  130]
train() client id: f_00008-2-3 loss: 0.869817  [  128/  130]
train() client id: f_00008-3-0 loss: 0.857216  [   32/  130]
train() client id: f_00008-3-1 loss: 0.757708  [   64/  130]
train() client id: f_00008-3-2 loss: 0.817950  [   96/  130]
train() client id: f_00008-3-3 loss: 0.925892  [  128/  130]
train() client id: f_00008-4-0 loss: 0.777538  [   32/  130]
train() client id: f_00008-4-1 loss: 0.826426  [   64/  130]
train() client id: f_00008-4-2 loss: 0.943770  [   96/  130]
train() client id: f_00008-4-3 loss: 0.815994  [  128/  130]
train() client id: f_00008-5-0 loss: 0.863965  [   32/  130]
train() client id: f_00008-5-1 loss: 0.908926  [   64/  130]
train() client id: f_00008-5-2 loss: 0.850869  [   96/  130]
train() client id: f_00008-5-3 loss: 0.732114  [  128/  130]
train() client id: f_00008-6-0 loss: 0.932534  [   32/  130]
train() client id: f_00008-6-1 loss: 0.851763  [   64/  130]
train() client id: f_00008-6-2 loss: 0.766690  [   96/  130]
train() client id: f_00008-6-3 loss: 0.801615  [  128/  130]
train() client id: f_00008-7-0 loss: 0.822404  [   32/  130]
train() client id: f_00008-7-1 loss: 0.848548  [   64/  130]
train() client id: f_00008-7-2 loss: 0.829228  [   96/  130]
train() client id: f_00008-7-3 loss: 0.829647  [  128/  130]
train() client id: f_00008-8-0 loss: 0.734417  [   32/  130]
train() client id: f_00008-8-1 loss: 0.903444  [   64/  130]
train() client id: f_00008-8-2 loss: 0.851300  [   96/  130]
train() client id: f_00008-8-3 loss: 0.857140  [  128/  130]
train() client id: f_00008-9-0 loss: 0.836557  [   32/  130]
train() client id: f_00008-9-1 loss: 0.778585  [   64/  130]
train() client id: f_00008-9-2 loss: 0.877080  [   96/  130]
train() client id: f_00008-9-3 loss: 0.793267  [  128/  130]
train() client id: f_00008-10-0 loss: 0.853527  [   32/  130]
train() client id: f_00008-10-1 loss: 0.759153  [   64/  130]
train() client id: f_00008-10-2 loss: 0.874691  [   96/  130]
train() client id: f_00008-10-3 loss: 0.855242  [  128/  130]
train() client id: f_00008-11-0 loss: 0.899340  [   32/  130]
train() client id: f_00008-11-1 loss: 0.736376  [   64/  130]
train() client id: f_00008-11-2 loss: 0.939359  [   96/  130]
train() client id: f_00008-11-3 loss: 0.770499  [  128/  130]
train() client id: f_00009-0-0 loss: 1.321893  [   32/  118]
train() client id: f_00009-0-1 loss: 1.098944  [   64/  118]
train() client id: f_00009-0-2 loss: 1.143984  [   96/  118]
train() client id: f_00009-1-0 loss: 1.148214  [   32/  118]
train() client id: f_00009-1-1 loss: 1.112954  [   64/  118]
train() client id: f_00009-1-2 loss: 1.146084  [   96/  118]
train() client id: f_00009-2-0 loss: 1.143132  [   32/  118]
train() client id: f_00009-2-1 loss: 1.115669  [   64/  118]
train() client id: f_00009-2-2 loss: 1.094520  [   96/  118]
train() client id: f_00009-3-0 loss: 1.147552  [   32/  118]
train() client id: f_00009-3-1 loss: 1.009305  [   64/  118]
train() client id: f_00009-3-2 loss: 0.972114  [   96/  118]
train() client id: f_00009-4-0 loss: 0.937859  [   32/  118]
train() client id: f_00009-4-1 loss: 0.979243  [   64/  118]
train() client id: f_00009-4-2 loss: 1.009231  [   96/  118]
train() client id: f_00009-5-0 loss: 0.937245  [   32/  118]
train() client id: f_00009-5-1 loss: 1.019323  [   64/  118]
train() client id: f_00009-5-2 loss: 0.952810  [   96/  118]
train() client id: f_00009-6-0 loss: 0.926907  [   32/  118]
train() client id: f_00009-6-1 loss: 0.920802  [   64/  118]
train() client id: f_00009-6-2 loss: 0.999765  [   96/  118]
train() client id: f_00009-7-0 loss: 0.878628  [   32/  118]
train() client id: f_00009-7-1 loss: 0.960828  [   64/  118]
train() client id: f_00009-7-2 loss: 1.019287  [   96/  118]
train() client id: f_00009-8-0 loss: 0.904274  [   32/  118]
train() client id: f_00009-8-1 loss: 0.847812  [   64/  118]
train() client id: f_00009-8-2 loss: 0.962216  [   96/  118]
train() client id: f_00009-9-0 loss: 0.879643  [   32/  118]
train() client id: f_00009-9-1 loss: 0.941231  [   64/  118]
train() client id: f_00009-9-2 loss: 0.900809  [   96/  118]
train() client id: f_00009-10-0 loss: 0.902851  [   32/  118]
train() client id: f_00009-10-1 loss: 0.902265  [   64/  118]
train() client id: f_00009-10-2 loss: 0.890486  [   96/  118]
train() client id: f_00009-11-0 loss: 0.927123  [   32/  118]
train() client id: f_00009-11-1 loss: 0.918166  [   64/  118]
train() client id: f_00009-11-2 loss: 0.878570  [   96/  118]
At round 8 accuracy: 0.6286472148541115
At round 8 training accuracy: 0.574111334674715
At round 8 training loss: 0.8770060659361024
update_location
xs = [ -3.9056584    4.20031788  60.00902392  18.81129433   0.97929623
   3.95640986 -22.44319194  -1.32485185  44.66397685   2.93912145]
ys = [ 52.5879595   35.55583871   1.32061395 -22.45517586  14.35018685
  -2.18584926  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [113.05196881 106.21610206 116.63115781 104.20220591 101.02913878
 100.10210345 102.5211559  100.0121567  110.92132708 100.1231756 ]
dists_bs = [210.37580989 227.0626082  292.22212521 276.67917212 238.27940567
 251.83371857 234.14571262 245.9693305  270.25317868 246.78497559]
uav_gains = [7.35860472e-11 8.60043309e-11 6.80689758e-11 9.02204198e-11
 9.74723702e-11 9.97448252e-11 9.39645390e-11 9.99692471e-11
 7.71712154e-11 9.96923511e-11]
bs_gains = [3.45864385e-11 2.79309909e-11 1.37815295e-11 1.60605338e-11
 2.44034856e-11 2.09013928e-11 2.56290612e-11 2.23268472e-11
 1.71528313e-11 2.21208434e-11]
Round 9
-------------------------------
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.80354861 20.47010187  9.65317002  3.45074232 23.61178753 11.38579797
  4.29063581 13.84678522 10.17374854  9.24174878]
obj_prev = 115.9280666591941
eta_min = 3.3564500217132474e-10	eta_max = 0.9202772988105504
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 26.95968104561057	eta = 0.909090909090909
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 46.612772647926946	eta = 0.5257958185768866
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 37.23476446800202	eta = 0.6582236063723965
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.55373220830825	eta = 0.6893453774967627
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.470824829161245	eta = 0.690956612049402
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.47060887359621	eta = 0.690960818797758
eta = 0.690960818797758
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.03050446 0.06415621 0.03002026 0.01041025 0.07408226 0.03534644
 0.01307334 0.0433357  0.03147286 0.02856767]
ene_total = [3.01933378 5.87455757 2.98817258 1.36705087 6.70212565 3.5823017
 1.57947799 4.03156066 3.30411446 3.0219136 ]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 0 obj = 6.211065406906146
eta = 0.690960818797758
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
eta_min = 0.6909608187976134	eta_max = 0.6909608187977386
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 0.05319201668668125	eta = 0.9090909090909091
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 20.968149575428694	eta = 0.002306182461743776
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.200288580777995	eta = 0.021977289355824242
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330781928113622	eta = 0.022669763803801746
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330546285065273	eta = 0.022670014241468774
eta = 0.022670014241468774
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.05846218e-04 2.12451699e-03 1.97533160e-04 8.04588971e-06
 3.33298867e-03 3.70534332e-04 1.58844283e-05 5.75826930e-04
 2.70281154e-04 1.93918678e-04]
ene_total = [0.17661037 0.26448822 0.17897858 0.16512347 0.30228746 0.23450342
 0.1641027  0.17665537 0.24344476 0.22686028]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 1 obj = 6.211065406903268
eta = 0.6909608187976134
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
Done!
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [1.99212129e-05 2.05604726e-04 1.91166988e-05 7.78658377e-07
 3.22557186e-04 3.58592613e-05 1.53724990e-06 5.57268965e-05
 2.61570432e-05 1.87668994e-05]
ene_total = [0.00669092 0.00837971 0.00679064 0.0064223  0.00876002 0.0087964
 0.00637547 0.00635849 0.00923511 0.00865832]
At round 9 energy consumption: 0.07646738157410429
At round 9 eta: 0.6909608187976134
At round 9 a_n: 25.09969023353052
At round 9 local rounds: 12.104944645341616
At round 9 global rounds: 81.21847247936174
gradient difference: 0.4560118615627289
train() client id: f_00000-0-0 loss: 1.215445  [   32/  126]
train() client id: f_00000-0-1 loss: 1.251461  [   64/  126]
train() client id: f_00000-0-2 loss: 1.247757  [   96/  126]
train() client id: f_00000-1-0 loss: 1.135129  [   32/  126]
train() client id: f_00000-1-1 loss: 1.246824  [   64/  126]
train() client id: f_00000-1-2 loss: 1.162347  [   96/  126]
train() client id: f_00000-2-0 loss: 1.122089  [   32/  126]
train() client id: f_00000-2-1 loss: 1.131121  [   64/  126]
train() client id: f_00000-2-2 loss: 1.142151  [   96/  126]
train() client id: f_00000-3-0 loss: 1.078795  [   32/  126]
train() client id: f_00000-3-1 loss: 1.049571  [   64/  126]
train() client id: f_00000-3-2 loss: 1.061142  [   96/  126]
train() client id: f_00000-4-0 loss: 1.034267  [   32/  126]
train() client id: f_00000-4-1 loss: 1.034284  [   64/  126]
train() client id: f_00000-4-2 loss: 1.024780  [   96/  126]
train() client id: f_00000-5-0 loss: 0.967121  [   32/  126]
train() client id: f_00000-5-1 loss: 1.045284  [   64/  126]
train() client id: f_00000-5-2 loss: 0.938280  [   96/  126]
train() client id: f_00000-6-0 loss: 1.013913  [   32/  126]
train() client id: f_00000-6-1 loss: 0.958600  [   64/  126]
train() client id: f_00000-6-2 loss: 0.912884  [   96/  126]
train() client id: f_00000-7-0 loss: 0.931856  [   32/  126]
train() client id: f_00000-7-1 loss: 0.997212  [   64/  126]
train() client id: f_00000-7-2 loss: 0.937179  [   96/  126]
train() client id: f_00000-8-0 loss: 0.940504  [   32/  126]
train() client id: f_00000-8-1 loss: 0.952053  [   64/  126]
train() client id: f_00000-8-2 loss: 0.968086  [   96/  126]
train() client id: f_00000-9-0 loss: 0.963331  [   32/  126]
train() client id: f_00000-9-1 loss: 0.976983  [   64/  126]
train() client id: f_00000-9-2 loss: 0.952108  [   96/  126]
train() client id: f_00000-10-0 loss: 0.875407  [   32/  126]
train() client id: f_00000-10-1 loss: 0.949770  [   64/  126]
train() client id: f_00000-10-2 loss: 1.057006  [   96/  126]
train() client id: f_00000-11-0 loss: 0.870745  [   32/  126]
train() client id: f_00000-11-1 loss: 1.015332  [   64/  126]
train() client id: f_00000-11-2 loss: 0.920218  [   96/  126]
train() client id: f_00001-0-0 loss: 0.504182  [   32/  265]
train() client id: f_00001-0-1 loss: 0.501962  [   64/  265]
train() client id: f_00001-0-2 loss: 0.495761  [   96/  265]
train() client id: f_00001-0-3 loss: 0.590648  [  128/  265]
train() client id: f_00001-0-4 loss: 0.459960  [  160/  265]
train() client id: f_00001-0-5 loss: 0.512565  [  192/  265]
train() client id: f_00001-0-6 loss: 0.480282  [  224/  265]
train() client id: f_00001-0-7 loss: 0.506703  [  256/  265]
train() client id: f_00001-1-0 loss: 0.459503  [   32/  265]
train() client id: f_00001-1-1 loss: 0.553705  [   64/  265]
train() client id: f_00001-1-2 loss: 0.488250  [   96/  265]
train() client id: f_00001-1-3 loss: 0.483079  [  128/  265]
train() client id: f_00001-1-4 loss: 0.449130  [  160/  265]
train() client id: f_00001-1-5 loss: 0.537704  [  192/  265]
train() client id: f_00001-1-6 loss: 0.423079  [  224/  265]
train() client id: f_00001-1-7 loss: 0.591295  [  256/  265]
train() client id: f_00001-2-0 loss: 0.468447  [   32/  265]
train() client id: f_00001-2-1 loss: 0.649500  [   64/  265]
train() client id: f_00001-2-2 loss: 0.466995  [   96/  265]
train() client id: f_00001-2-3 loss: 0.467211  [  128/  265]
train() client id: f_00001-2-4 loss: 0.461136  [  160/  265]
train() client id: f_00001-2-5 loss: 0.426072  [  192/  265]
train() client id: f_00001-2-6 loss: 0.481508  [  224/  265]
train() client id: f_00001-2-7 loss: 0.500026  [  256/  265]
train() client id: f_00001-3-0 loss: 0.450931  [   32/  265]
train() client id: f_00001-3-1 loss: 0.412751  [   64/  265]
train() client id: f_00001-3-2 loss: 0.522484  [   96/  265]
train() client id: f_00001-3-3 loss: 0.524270  [  128/  265]
train() client id: f_00001-3-4 loss: 0.442954  [  160/  265]
train() client id: f_00001-3-5 loss: 0.471935  [  192/  265]
train() client id: f_00001-3-6 loss: 0.520803  [  224/  265]
train() client id: f_00001-3-7 loss: 0.509631  [  256/  265]
train() client id: f_00001-4-0 loss: 0.449578  [   32/  265]
train() client id: f_00001-4-1 loss: 0.436645  [   64/  265]
train() client id: f_00001-4-2 loss: 0.452401  [   96/  265]
train() client id: f_00001-4-3 loss: 0.493390  [  128/  265]
train() client id: f_00001-4-4 loss: 0.454737  [  160/  265]
train() client id: f_00001-4-5 loss: 0.446652  [  192/  265]
train() client id: f_00001-4-6 loss: 0.482476  [  224/  265]
train() client id: f_00001-4-7 loss: 0.551000  [  256/  265]
train() client id: f_00001-5-0 loss: 0.401936  [   32/  265]
train() client id: f_00001-5-1 loss: 0.431949  [   64/  265]
train() client id: f_00001-5-2 loss: 0.549506  [   96/  265]
train() client id: f_00001-5-3 loss: 0.490962  [  128/  265]
train() client id: f_00001-5-4 loss: 0.442602  [  160/  265]
train() client id: f_00001-5-5 loss: 0.430213  [  192/  265]
train() client id: f_00001-5-6 loss: 0.501279  [  224/  265]
train() client id: f_00001-5-7 loss: 0.449106  [  256/  265]
train() client id: f_00001-6-0 loss: 0.524587  [   32/  265]
train() client id: f_00001-6-1 loss: 0.454043  [   64/  265]
train() client id: f_00001-6-2 loss: 0.494073  [   96/  265]
train() client id: f_00001-6-3 loss: 0.437076  [  128/  265]
train() client id: f_00001-6-4 loss: 0.515171  [  160/  265]
train() client id: f_00001-6-5 loss: 0.385575  [  192/  265]
train() client id: f_00001-6-6 loss: 0.437608  [  224/  265]
train() client id: f_00001-6-7 loss: 0.439594  [  256/  265]
train() client id: f_00001-7-0 loss: 0.371880  [   32/  265]
train() client id: f_00001-7-1 loss: 0.467206  [   64/  265]
train() client id: f_00001-7-2 loss: 0.600804  [   96/  265]
train() client id: f_00001-7-3 loss: 0.380623  [  128/  265]
train() client id: f_00001-7-4 loss: 0.529610  [  160/  265]
train() client id: f_00001-7-5 loss: 0.364171  [  192/  265]
train() client id: f_00001-7-6 loss: 0.501214  [  224/  265]
train() client id: f_00001-7-7 loss: 0.411601  [  256/  265]
train() client id: f_00001-8-0 loss: 0.501670  [   32/  265]
train() client id: f_00001-8-1 loss: 0.375185  [   64/  265]
train() client id: f_00001-8-2 loss: 0.537157  [   96/  265]
train() client id: f_00001-8-3 loss: 0.506311  [  128/  265]
train() client id: f_00001-8-4 loss: 0.424924  [  160/  265]
train() client id: f_00001-8-5 loss: 0.487289  [  192/  265]
train() client id: f_00001-8-6 loss: 0.427358  [  224/  265]
train() client id: f_00001-8-7 loss: 0.426504  [  256/  265]
train() client id: f_00001-9-0 loss: 0.441549  [   32/  265]
train() client id: f_00001-9-1 loss: 0.401024  [   64/  265]
train() client id: f_00001-9-2 loss: 0.409018  [   96/  265]
train() client id: f_00001-9-3 loss: 0.454759  [  128/  265]
train() client id: f_00001-9-4 loss: 0.378060  [  160/  265]
train() client id: f_00001-9-5 loss: 0.532525  [  192/  265]
train() client id: f_00001-9-6 loss: 0.389474  [  224/  265]
train() client id: f_00001-9-7 loss: 0.654774  [  256/  265]
train() client id: f_00001-10-0 loss: 0.426565  [   32/  265]
train() client id: f_00001-10-1 loss: 0.556383  [   64/  265]
train() client id: f_00001-10-2 loss: 0.436287  [   96/  265]
train() client id: f_00001-10-3 loss: 0.406740  [  128/  265]
train() client id: f_00001-10-4 loss: 0.426530  [  160/  265]
train() client id: f_00001-10-5 loss: 0.460742  [  192/  265]
train() client id: f_00001-10-6 loss: 0.372356  [  224/  265]
train() client id: f_00001-10-7 loss: 0.516790  [  256/  265]
train() client id: f_00001-11-0 loss: 0.473448  [   32/  265]
train() client id: f_00001-11-1 loss: 0.417161  [   64/  265]
train() client id: f_00001-11-2 loss: 0.375762  [   96/  265]
train() client id: f_00001-11-3 loss: 0.377372  [  128/  265]
train() client id: f_00001-11-4 loss: 0.391397  [  160/  265]
train() client id: f_00001-11-5 loss: 0.636880  [  192/  265]
train() client id: f_00001-11-6 loss: 0.469454  [  224/  265]
train() client id: f_00001-11-7 loss: 0.454518  [  256/  265]
train() client id: f_00002-0-0 loss: 1.095745  [   32/  124]
train() client id: f_00002-0-1 loss: 1.200708  [   64/  124]
train() client id: f_00002-0-2 loss: 1.125855  [   96/  124]
train() client id: f_00002-1-0 loss: 1.128653  [   32/  124]
train() client id: f_00002-1-1 loss: 1.074024  [   64/  124]
train() client id: f_00002-1-2 loss: 1.195639  [   96/  124]
train() client id: f_00002-2-0 loss: 1.114949  [   32/  124]
train() client id: f_00002-2-1 loss: 1.059560  [   64/  124]
train() client id: f_00002-2-2 loss: 1.150584  [   96/  124]
train() client id: f_00002-3-0 loss: 1.053138  [   32/  124]
train() client id: f_00002-3-1 loss: 1.127518  [   64/  124]
train() client id: f_00002-3-2 loss: 1.091306  [   96/  124]
train() client id: f_00002-4-0 loss: 0.992326  [   32/  124]
train() client id: f_00002-4-1 loss: 1.064243  [   64/  124]
train() client id: f_00002-4-2 loss: 1.048947  [   96/  124]
train() client id: f_00002-5-0 loss: 0.975948  [   32/  124]
train() client id: f_00002-5-1 loss: 1.047080  [   64/  124]
train() client id: f_00002-5-2 loss: 1.094474  [   96/  124]
train() client id: f_00002-6-0 loss: 0.934511  [   32/  124]
train() client id: f_00002-6-1 loss: 1.039364  [   64/  124]
train() client id: f_00002-6-2 loss: 1.025397  [   96/  124]
train() client id: f_00002-7-0 loss: 1.023284  [   32/  124]
train() client id: f_00002-7-1 loss: 1.056153  [   64/  124]
train() client id: f_00002-7-2 loss: 0.919933  [   96/  124]
train() client id: f_00002-8-0 loss: 1.049734  [   32/  124]
train() client id: f_00002-8-1 loss: 0.900535  [   64/  124]
train() client id: f_00002-8-2 loss: 0.935464  [   96/  124]
train() client id: f_00002-9-0 loss: 1.077055  [   32/  124]
train() client id: f_00002-9-1 loss: 0.952737  [   64/  124]
train() client id: f_00002-9-2 loss: 0.924806  [   96/  124]
train() client id: f_00002-10-0 loss: 0.986469  [   32/  124]
train() client id: f_00002-10-1 loss: 0.942035  [   64/  124]
train() client id: f_00002-10-2 loss: 0.993355  [   96/  124]
train() client id: f_00002-11-0 loss: 1.132030  [   32/  124]
train() client id: f_00002-11-1 loss: 0.896533  [   64/  124]
train() client id: f_00002-11-2 loss: 1.039015  [   96/  124]
train() client id: f_00003-0-0 loss: 0.975049  [   32/   43]
train() client id: f_00003-1-0 loss: 1.022894  [   32/   43]
train() client id: f_00003-2-0 loss: 0.953363  [   32/   43]
train() client id: f_00003-3-0 loss: 0.958888  [   32/   43]
train() client id: f_00003-4-0 loss: 0.838632  [   32/   43]
train() client id: f_00003-5-0 loss: 1.041873  [   32/   43]
train() client id: f_00003-6-0 loss: 0.996699  [   32/   43]
train() client id: f_00003-7-0 loss: 0.854349  [   32/   43]
train() client id: f_00003-8-0 loss: 1.010739  [   32/   43]
train() client id: f_00003-9-0 loss: 0.934673  [   32/   43]
train() client id: f_00003-10-0 loss: 0.887766  [   32/   43]
train() client id: f_00003-11-0 loss: 0.946549  [   32/   43]
train() client id: f_00004-0-0 loss: 0.873506  [   32/  306]
train() client id: f_00004-0-1 loss: 0.870706  [   64/  306]
train() client id: f_00004-0-2 loss: 0.716760  [   96/  306]
train() client id: f_00004-0-3 loss: 0.860491  [  128/  306]
train() client id: f_00004-0-4 loss: 0.833556  [  160/  306]
train() client id: f_00004-0-5 loss: 0.836180  [  192/  306]
train() client id: f_00004-0-6 loss: 0.712599  [  224/  306]
train() client id: f_00004-0-7 loss: 0.861654  [  256/  306]
train() client id: f_00004-0-8 loss: 0.785570  [  288/  306]
train() client id: f_00004-1-0 loss: 0.737532  [   32/  306]
train() client id: f_00004-1-1 loss: 0.928581  [   64/  306]
train() client id: f_00004-1-2 loss: 0.840836  [   96/  306]
train() client id: f_00004-1-3 loss: 0.804989  [  128/  306]
train() client id: f_00004-1-4 loss: 0.856761  [  160/  306]
train() client id: f_00004-1-5 loss: 0.858665  [  192/  306]
train() client id: f_00004-1-6 loss: 0.746154  [  224/  306]
train() client id: f_00004-1-7 loss: 0.824497  [  256/  306]
train() client id: f_00004-1-8 loss: 0.828226  [  288/  306]
train() client id: f_00004-2-0 loss: 0.893454  [   32/  306]
train() client id: f_00004-2-1 loss: 0.881022  [   64/  306]
train() client id: f_00004-2-2 loss: 0.819818  [   96/  306]
train() client id: f_00004-2-3 loss: 0.814920  [  128/  306]
train() client id: f_00004-2-4 loss: 0.886109  [  160/  306]
train() client id: f_00004-2-5 loss: 0.790471  [  192/  306]
train() client id: f_00004-2-6 loss: 0.726137  [  224/  306]
train() client id: f_00004-2-7 loss: 0.879889  [  256/  306]
train() client id: f_00004-2-8 loss: 0.750966  [  288/  306]
train() client id: f_00004-3-0 loss: 0.808873  [   32/  306]
train() client id: f_00004-3-1 loss: 0.823669  [   64/  306]
train() client id: f_00004-3-2 loss: 0.947409  [   96/  306]
train() client id: f_00004-3-3 loss: 0.767743  [  128/  306]
train() client id: f_00004-3-4 loss: 0.775283  [  160/  306]
train() client id: f_00004-3-5 loss: 0.876597  [  192/  306]
train() client id: f_00004-3-6 loss: 0.760726  [  224/  306]
train() client id: f_00004-3-7 loss: 0.860394  [  256/  306]
train() client id: f_00004-3-8 loss: 0.795336  [  288/  306]
train() client id: f_00004-4-0 loss: 0.867332  [   32/  306]
train() client id: f_00004-4-1 loss: 0.870859  [   64/  306]
train() client id: f_00004-4-2 loss: 0.823170  [   96/  306]
train() client id: f_00004-4-3 loss: 0.813445  [  128/  306]
train() client id: f_00004-4-4 loss: 0.756311  [  160/  306]
train() client id: f_00004-4-5 loss: 0.860204  [  192/  306]
train() client id: f_00004-4-6 loss: 0.855239  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838317  [  256/  306]
train() client id: f_00004-4-8 loss: 0.842763  [  288/  306]
train() client id: f_00004-5-0 loss: 0.801268  [   32/  306]
train() client id: f_00004-5-1 loss: 0.864613  [   64/  306]
train() client id: f_00004-5-2 loss: 0.936488  [   96/  306]
train() client id: f_00004-5-3 loss: 0.832600  [  128/  306]
train() client id: f_00004-5-4 loss: 0.779173  [  160/  306]
train() client id: f_00004-5-5 loss: 0.738500  [  192/  306]
train() client id: f_00004-5-6 loss: 0.908376  [  224/  306]
train() client id: f_00004-5-7 loss: 0.816595  [  256/  306]
train() client id: f_00004-5-8 loss: 0.801983  [  288/  306]
train() client id: f_00004-6-0 loss: 0.793250  [   32/  306]
train() client id: f_00004-6-1 loss: 0.834986  [   64/  306]
train() client id: f_00004-6-2 loss: 0.678873  [   96/  306]
train() client id: f_00004-6-3 loss: 0.819448  [  128/  306]
train() client id: f_00004-6-4 loss: 0.822223  [  160/  306]
train() client id: f_00004-6-5 loss: 0.780608  [  192/  306]
train() client id: f_00004-6-6 loss: 0.770549  [  224/  306]
train() client id: f_00004-6-7 loss: 0.847967  [  256/  306]
train() client id: f_00004-6-8 loss: 0.993923  [  288/  306]
train() client id: f_00004-7-0 loss: 0.743089  [   32/  306]
train() client id: f_00004-7-1 loss: 0.888524  [   64/  306]
train() client id: f_00004-7-2 loss: 0.849380  [   96/  306]
train() client id: f_00004-7-3 loss: 0.846196  [  128/  306]
train() client id: f_00004-7-4 loss: 0.915802  [  160/  306]
train() client id: f_00004-7-5 loss: 0.842661  [  192/  306]
train() client id: f_00004-7-6 loss: 0.810464  [  224/  306]
train() client id: f_00004-7-7 loss: 0.820204  [  256/  306]
train() client id: f_00004-7-8 loss: 0.776050  [  288/  306]
train() client id: f_00004-8-0 loss: 0.832449  [   32/  306]
train() client id: f_00004-8-1 loss: 0.850979  [   64/  306]
train() client id: f_00004-8-2 loss: 0.958544  [   96/  306]
train() client id: f_00004-8-3 loss: 0.933826  [  128/  306]
train() client id: f_00004-8-4 loss: 0.833302  [  160/  306]
train() client id: f_00004-8-5 loss: 0.684280  [  192/  306]
train() client id: f_00004-8-6 loss: 0.895394  [  224/  306]
train() client id: f_00004-8-7 loss: 0.776209  [  256/  306]
train() client id: f_00004-8-8 loss: 0.772833  [  288/  306]
train() client id: f_00004-9-0 loss: 0.914407  [   32/  306]
train() client id: f_00004-9-1 loss: 0.781672  [   64/  306]
train() client id: f_00004-9-2 loss: 0.750355  [   96/  306]
train() client id: f_00004-9-3 loss: 0.801505  [  128/  306]
train() client id: f_00004-9-4 loss: 0.844660  [  160/  306]
train() client id: f_00004-9-5 loss: 0.861144  [  192/  306]
train() client id: f_00004-9-6 loss: 0.851986  [  224/  306]
train() client id: f_00004-9-7 loss: 0.731897  [  256/  306]
train() client id: f_00004-9-8 loss: 0.906533  [  288/  306]
train() client id: f_00004-10-0 loss: 0.771220  [   32/  306]
train() client id: f_00004-10-1 loss: 0.867522  [   64/  306]
train() client id: f_00004-10-2 loss: 0.909753  [   96/  306]
train() client id: f_00004-10-3 loss: 0.958160  [  128/  306]
train() client id: f_00004-10-4 loss: 0.792479  [  160/  306]
train() client id: f_00004-10-5 loss: 0.820744  [  192/  306]
train() client id: f_00004-10-6 loss: 0.799130  [  224/  306]
train() client id: f_00004-10-7 loss: 0.818690  [  256/  306]
train() client id: f_00004-10-8 loss: 0.786887  [  288/  306]
train() client id: f_00004-11-0 loss: 0.868470  [   32/  306]
train() client id: f_00004-11-1 loss: 0.837987  [   64/  306]
train() client id: f_00004-11-2 loss: 0.752643  [   96/  306]
train() client id: f_00004-11-3 loss: 0.882705  [  128/  306]
train() client id: f_00004-11-4 loss: 0.913482  [  160/  306]
train() client id: f_00004-11-5 loss: 0.753750  [  192/  306]
train() client id: f_00004-11-6 loss: 0.860935  [  224/  306]
train() client id: f_00004-11-7 loss: 0.933912  [  256/  306]
train() client id: f_00004-11-8 loss: 0.753700  [  288/  306]
train() client id: f_00005-0-0 loss: 0.645323  [   32/  146]
train() client id: f_00005-0-1 loss: 0.757932  [   64/  146]
train() client id: f_00005-0-2 loss: 0.633539  [   96/  146]
train() client id: f_00005-0-3 loss: 0.836189  [  128/  146]
train() client id: f_00005-1-0 loss: 0.649059  [   32/  146]
train() client id: f_00005-1-1 loss: 0.695461  [   64/  146]
train() client id: f_00005-1-2 loss: 0.742590  [   96/  146]
train() client id: f_00005-1-3 loss: 0.568630  [  128/  146]
train() client id: f_00005-2-0 loss: 0.747094  [   32/  146]
train() client id: f_00005-2-1 loss: 0.572447  [   64/  146]
train() client id: f_00005-2-2 loss: 0.647385  [   96/  146]
train() client id: f_00005-2-3 loss: 0.704019  [  128/  146]
train() client id: f_00005-3-0 loss: 0.528056  [   32/  146]
train() client id: f_00005-3-1 loss: 0.766133  [   64/  146]
train() client id: f_00005-3-2 loss: 0.615602  [   96/  146]
train() client id: f_00005-3-3 loss: 0.626347  [  128/  146]
train() client id: f_00005-4-0 loss: 0.567250  [   32/  146]
train() client id: f_00005-4-1 loss: 0.617422  [   64/  146]
train() client id: f_00005-4-2 loss: 0.608855  [   96/  146]
train() client id: f_00005-4-3 loss: 0.797776  [  128/  146]
train() client id: f_00005-5-0 loss: 0.777019  [   32/  146]
train() client id: f_00005-5-1 loss: 0.724439  [   64/  146]
train() client id: f_00005-5-2 loss: 0.486879  [   96/  146]
train() client id: f_00005-5-3 loss: 0.662244  [  128/  146]
train() client id: f_00005-6-0 loss: 0.682880  [   32/  146]
train() client id: f_00005-6-1 loss: 0.488486  [   64/  146]
train() client id: f_00005-6-2 loss: 0.641566  [   96/  146]
train() client id: f_00005-6-3 loss: 0.692855  [  128/  146]
train() client id: f_00005-7-0 loss: 0.444424  [   32/  146]
train() client id: f_00005-7-1 loss: 0.669662  [   64/  146]
train() client id: f_00005-7-2 loss: 0.668811  [   96/  146]
train() client id: f_00005-7-3 loss: 0.797075  [  128/  146]
train() client id: f_00005-8-0 loss: 0.689756  [   32/  146]
train() client id: f_00005-8-1 loss: 0.959320  [   64/  146]
train() client id: f_00005-8-2 loss: 0.440332  [   96/  146]
train() client id: f_00005-8-3 loss: 0.491479  [  128/  146]
train() client id: f_00005-9-0 loss: 0.782821  [   32/  146]
train() client id: f_00005-9-1 loss: 0.657253  [   64/  146]
train() client id: f_00005-9-2 loss: 0.455401  [   96/  146]
train() client id: f_00005-9-3 loss: 0.739242  [  128/  146]
train() client id: f_00005-10-0 loss: 0.592522  [   32/  146]
train() client id: f_00005-10-1 loss: 0.769122  [   64/  146]
train() client id: f_00005-10-2 loss: 0.547144  [   96/  146]
train() client id: f_00005-10-3 loss: 0.670107  [  128/  146]
train() client id: f_00005-11-0 loss: 0.556984  [   32/  146]
train() client id: f_00005-11-1 loss: 0.797819  [   64/  146]
train() client id: f_00005-11-2 loss: 0.669383  [   96/  146]
train() client id: f_00005-11-3 loss: 0.497294  [  128/  146]
train() client id: f_00006-0-0 loss: 0.737009  [   32/   54]
train() client id: f_00006-1-0 loss: 0.644650  [   32/   54]
train() client id: f_00006-2-0 loss: 0.747809  [   32/   54]
train() client id: f_00006-3-0 loss: 0.695348  [   32/   54]
train() client id: f_00006-4-0 loss: 0.701152  [   32/   54]
train() client id: f_00006-5-0 loss: 0.685246  [   32/   54]
train() client id: f_00006-6-0 loss: 0.688029  [   32/   54]
train() client id: f_00006-7-0 loss: 0.704014  [   32/   54]
train() client id: f_00006-8-0 loss: 0.689927  [   32/   54]
train() client id: f_00006-9-0 loss: 0.694663  [   32/   54]
train() client id: f_00006-10-0 loss: 0.705621  [   32/   54]
train() client id: f_00006-11-0 loss: 0.744214  [   32/   54]
train() client id: f_00007-0-0 loss: 0.697767  [   32/  179]
train() client id: f_00007-0-1 loss: 0.747113  [   64/  179]
train() client id: f_00007-0-2 loss: 0.722938  [   96/  179]
train() client id: f_00007-0-3 loss: 0.852784  [  128/  179]
train() client id: f_00007-0-4 loss: 0.729403  [  160/  179]
train() client id: f_00007-1-0 loss: 0.720134  [   32/  179]
train() client id: f_00007-1-1 loss: 0.738477  [   64/  179]
train() client id: f_00007-1-2 loss: 0.692641  [   96/  179]
train() client id: f_00007-1-3 loss: 0.739388  [  128/  179]
train() client id: f_00007-1-4 loss: 0.674278  [  160/  179]
train() client id: f_00007-2-0 loss: 0.742613  [   32/  179]
train() client id: f_00007-2-1 loss: 0.600411  [   64/  179]
train() client id: f_00007-2-2 loss: 0.796533  [   96/  179]
train() client id: f_00007-2-3 loss: 0.717972  [  128/  179]
train() client id: f_00007-2-4 loss: 0.661306  [  160/  179]
train() client id: f_00007-3-0 loss: 0.688896  [   32/  179]
train() client id: f_00007-3-1 loss: 0.669200  [   64/  179]
train() client id: f_00007-3-2 loss: 0.653100  [   96/  179]
train() client id: f_00007-3-3 loss: 0.745714  [  128/  179]
train() client id: f_00007-3-4 loss: 0.721283  [  160/  179]
train() client id: f_00007-4-0 loss: 0.606279  [   32/  179]
train() client id: f_00007-4-1 loss: 0.663651  [   64/  179]
train() client id: f_00007-4-2 loss: 0.727192  [   96/  179]
train() client id: f_00007-4-3 loss: 0.870627  [  128/  179]
train() client id: f_00007-4-4 loss: 0.565887  [  160/  179]
train() client id: f_00007-5-0 loss: 0.600875  [   32/  179]
train() client id: f_00007-5-1 loss: 0.666202  [   64/  179]
train() client id: f_00007-5-2 loss: 0.862424  [   96/  179]
train() client id: f_00007-5-3 loss: 0.785303  [  128/  179]
train() client id: f_00007-5-4 loss: 0.559901  [  160/  179]
train() client id: f_00007-6-0 loss: 0.792568  [   32/  179]
train() client id: f_00007-6-1 loss: 0.653349  [   64/  179]
train() client id: f_00007-6-2 loss: 0.817570  [   96/  179]
train() client id: f_00007-6-3 loss: 0.598881  [  128/  179]
train() client id: f_00007-6-4 loss: 0.578942  [  160/  179]
train() client id: f_00007-7-0 loss: 0.557457  [   32/  179]
train() client id: f_00007-7-1 loss: 0.642508  [   64/  179]
train() client id: f_00007-7-2 loss: 0.800254  [   96/  179]
train() client id: f_00007-7-3 loss: 0.703599  [  128/  179]
train() client id: f_00007-7-4 loss: 0.638979  [  160/  179]
train() client id: f_00007-8-0 loss: 0.755180  [   32/  179]
train() client id: f_00007-8-1 loss: 0.613835  [   64/  179]
train() client id: f_00007-8-2 loss: 0.784424  [   96/  179]
train() client id: f_00007-8-3 loss: 0.652909  [  128/  179]
train() client id: f_00007-8-4 loss: 0.623500  [  160/  179]
train() client id: f_00007-9-0 loss: 0.745027  [   32/  179]
train() client id: f_00007-9-1 loss: 0.716158  [   64/  179]
train() client id: f_00007-9-2 loss: 0.600529  [   96/  179]
train() client id: f_00007-9-3 loss: 0.642713  [  128/  179]
train() client id: f_00007-9-4 loss: 0.712371  [  160/  179]
train() client id: f_00007-10-0 loss: 0.755607  [   32/  179]
train() client id: f_00007-10-1 loss: 0.602293  [   64/  179]
train() client id: f_00007-10-2 loss: 0.638241  [   96/  179]
train() client id: f_00007-10-3 loss: 0.699769  [  128/  179]
train() client id: f_00007-10-4 loss: 0.717531  [  160/  179]
train() client id: f_00007-11-0 loss: 0.571703  [   32/  179]
train() client id: f_00007-11-1 loss: 0.573610  [   64/  179]
train() client id: f_00007-11-2 loss: 0.724886  [   96/  179]
train() client id: f_00007-11-3 loss: 0.848321  [  128/  179]
train() client id: f_00007-11-4 loss: 0.620980  [  160/  179]
train() client id: f_00008-0-0 loss: 0.913363  [   32/  130]
train() client id: f_00008-0-1 loss: 0.870201  [   64/  130]
train() client id: f_00008-0-2 loss: 0.902038  [   96/  130]
train() client id: f_00008-0-3 loss: 0.901360  [  128/  130]
train() client id: f_00008-1-0 loss: 0.928481  [   32/  130]
train() client id: f_00008-1-1 loss: 0.907102  [   64/  130]
train() client id: f_00008-1-2 loss: 0.944651  [   96/  130]
train() client id: f_00008-1-3 loss: 0.792527  [  128/  130]
train() client id: f_00008-2-0 loss: 0.983456  [   32/  130]
train() client id: f_00008-2-1 loss: 0.838573  [   64/  130]
train() client id: f_00008-2-2 loss: 0.882037  [   96/  130]
train() client id: f_00008-2-3 loss: 0.864564  [  128/  130]
train() client id: f_00008-3-0 loss: 0.889627  [   32/  130]
train() client id: f_00008-3-1 loss: 0.906500  [   64/  130]
train() client id: f_00008-3-2 loss: 0.814159  [   96/  130]
train() client id: f_00008-3-3 loss: 0.933231  [  128/  130]
train() client id: f_00008-4-0 loss: 0.846702  [   32/  130]
train() client id: f_00008-4-1 loss: 0.899809  [   64/  130]
train() client id: f_00008-4-2 loss: 0.850205  [   96/  130]
train() client id: f_00008-4-3 loss: 0.975572  [  128/  130]
train() client id: f_00008-5-0 loss: 0.794295  [   32/  130]
train() client id: f_00008-5-1 loss: 0.959152  [   64/  130]
train() client id: f_00008-5-2 loss: 0.916328  [   96/  130]
train() client id: f_00008-5-3 loss: 0.897050  [  128/  130]
train() client id: f_00008-6-0 loss: 0.853162  [   32/  130]
train() client id: f_00008-6-1 loss: 0.872425  [   64/  130]
train() client id: f_00008-6-2 loss: 0.956528  [   96/  130]
train() client id: f_00008-6-3 loss: 0.888032  [  128/  130]
train() client id: f_00008-7-0 loss: 0.907192  [   32/  130]
train() client id: f_00008-7-1 loss: 0.879291  [   64/  130]
train() client id: f_00008-7-2 loss: 0.813032  [   96/  130]
train() client id: f_00008-7-3 loss: 0.932806  [  128/  130]
train() client id: f_00008-8-0 loss: 1.015994  [   32/  130]
train() client id: f_00008-8-1 loss: 0.851493  [   64/  130]
train() client id: f_00008-8-2 loss: 0.806808  [   96/  130]
train() client id: f_00008-8-3 loss: 0.857598  [  128/  130]
train() client id: f_00008-9-0 loss: 0.987603  [   32/  130]
train() client id: f_00008-9-1 loss: 0.862642  [   64/  130]
train() client id: f_00008-9-2 loss: 0.875870  [   96/  130]
train() client id: f_00008-9-3 loss: 0.838822  [  128/  130]
train() client id: f_00008-10-0 loss: 0.944056  [   32/  130]
train() client id: f_00008-10-1 loss: 0.820521  [   64/  130]
train() client id: f_00008-10-2 loss: 0.909813  [   96/  130]
train() client id: f_00008-10-3 loss: 0.892294  [  128/  130]
train() client id: f_00008-11-0 loss: 0.827563  [   32/  130]
train() client id: f_00008-11-1 loss: 0.907061  [   64/  130]
train() client id: f_00008-11-2 loss: 0.871641  [   96/  130]
train() client id: f_00008-11-3 loss: 0.963327  [  128/  130]
train() client id: f_00009-0-0 loss: 1.199803  [   32/  118]
train() client id: f_00009-0-1 loss: 1.306450  [   64/  118]
train() client id: f_00009-0-2 loss: 1.062035  [   96/  118]
train() client id: f_00009-1-0 loss: 1.267335  [   32/  118]
train() client id: f_00009-1-1 loss: 1.122164  [   64/  118]
train() client id: f_00009-1-2 loss: 1.080803  [   96/  118]
train() client id: f_00009-2-0 loss: 1.090680  [   32/  118]
train() client id: f_00009-2-1 loss: 1.175460  [   64/  118]
train() client id: f_00009-2-2 loss: 1.048804  [   96/  118]
train() client id: f_00009-3-0 loss: 1.143777  [   32/  118]
train() client id: f_00009-3-1 loss: 1.022209  [   64/  118]
train() client id: f_00009-3-2 loss: 1.078814  [   96/  118]
train() client id: f_00009-4-0 loss: 1.021240  [   32/  118]
train() client id: f_00009-4-1 loss: 1.036336  [   64/  118]
train() client id: f_00009-4-2 loss: 1.094195  [   96/  118]
train() client id: f_00009-5-0 loss: 1.092173  [   32/  118]
train() client id: f_00009-5-1 loss: 0.964741  [   64/  118]
train() client id: f_00009-5-2 loss: 0.987745  [   96/  118]
train() client id: f_00009-6-0 loss: 1.000984  [   32/  118]
train() client id: f_00009-6-1 loss: 0.911209  [   64/  118]
train() client id: f_00009-6-2 loss: 1.080925  [   96/  118]
train() client id: f_00009-7-0 loss: 1.034229  [   32/  118]
train() client id: f_00009-7-1 loss: 1.014200  [   64/  118]
train() client id: f_00009-7-2 loss: 0.918206  [   96/  118]
train() client id: f_00009-8-0 loss: 1.008200  [   32/  118]
train() client id: f_00009-8-1 loss: 0.929863  [   64/  118]
train() client id: f_00009-8-2 loss: 0.951832  [   96/  118]
train() client id: f_00009-9-0 loss: 0.903081  [   32/  118]
train() client id: f_00009-9-1 loss: 1.065937  [   64/  118]
train() client id: f_00009-9-2 loss: 0.968345  [   96/  118]
train() client id: f_00009-10-0 loss: 1.052801  [   32/  118]
train() client id: f_00009-10-1 loss: 0.953949  [   64/  118]
train() client id: f_00009-10-2 loss: 1.003738  [   96/  118]
train() client id: f_00009-11-0 loss: 0.949446  [   32/  118]
train() client id: f_00009-11-1 loss: 1.003724  [   64/  118]
train() client id: f_00009-11-2 loss: 0.999239  [   96/  118]
At round 9 accuracy: 0.6286472148541115
At round 9 training accuracy: 0.5747820254862508
At round 9 training loss: 0.8674956796484695
update_location
xs = [ -3.9056584    4.20031788  65.00902392  18.81129433   0.97929623
   3.95640986 -27.44319194  -6.32485185  49.66397685   2.93912145]
ys = [ 57.5879595   40.55583871   1.32061395 -27.45517586  19.35018685
   2.81415074  -2.62498432   0.82234798  17.56900603  -0.99851822]
dists_uav = [115.46266603 107.99267903 119.28083338 105.39284357 101.85965223
 100.11779374 103.730513   100.2031936  113.02734434 100.04816577]
dists_bs = [207.50629142 224.02675383 296.25809812 280.26936337 234.93738961
 248.34122355 230.91913446 242.46476046 274.3326819  250.27546695]
uav_gains = [6.98045343e-11 8.25105385e-11 6.43507590e-11 8.76937845e-11
 9.54975988e-11 9.97057494e-11 9.12496048e-11 9.94934409e-11
 7.36261378e-11 9.98793178e-11]
bs_gains = [3.59423553e-11 2.90037668e-11 1.32622570e-11 1.54911026e-11
 2.53879770e-11 2.17348876e-11 2.66444222e-11 2.32422371e-11
 1.64481458e-11 2.12678163e-11]
Round 10
-------------------------------
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.67144915 20.18897006  9.52328793  3.40449035 23.28757639 11.22831614
  4.23293599 13.65794809 10.03742584  9.11771254]
obj_prev = 114.35011248363482
eta_min = 2.485131023397881e-10	eta_max = 0.9203875705067922
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 26.5917511352464	eta = 0.909090909090909
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 46.02197555438805	eta = 0.5252777379209097
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 36.745409717816976	eta = 0.6578867782263103
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.08220734696584	eta = 0.6890763450194114
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.00007514490989	eta = 0.6906933517648766
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 34.99986062549486	eta = 0.690697585128414
eta = 0.690697585128414
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.0305359  0.06422234 0.03005121 0.01042098 0.07415862 0.03538287
 0.01308682 0.04338037 0.0315053  0.02859712]
ene_total = [2.98353072 5.7890883  2.95330941 1.35079969 6.60483527 3.5266156
 1.5604156  3.97716561 3.26691924 2.98718119]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 0 obj = 6.132529161591935
eta = 0.690697585128414
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
eta_min = 0.6906975851284028	eta_max = 0.6906975851284034
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 0.050678425323249086	eta = 0.9090909090909091
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 20.72563143256374	eta = 0.002222913974819694
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.1660187348301907	eta = 0.02127003566846808
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101859368191834	eta = 0.021919304614581324
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101838286505997	eta = 0.02191952446779107
eta = 0.02191952446779107
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [2.02550289e-04 2.06745903e-03 1.94438128e-04 7.90109671e-06
 3.24046959e-03 3.59959346e-04 1.55998826e-05 5.64483901e-04
 2.65926276e-04 1.90664962e-04]
ene_total = [0.17569726 0.2574448  0.17820355 0.16359333 0.29359605 0.22873723
 0.16259842 0.17396099 0.24238129 0.22562536]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 1 obj = 6.132529161591719
eta = 0.6906975851284028
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
Done!
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.94295065e-05 1.98319681e-04 1.86513526e-05 7.57907632e-07
 3.10839966e-04 3.45288693e-05 1.49640873e-06 5.41477560e-05
 2.55088073e-05 1.82894141e-05]
ene_total = [0.00675815 0.00830173 0.0068645  0.00645594 0.00866947 0.00871129
 0.00640967 0.00636234 0.00933534 0.00874141]
At round 10 energy consumption: 0.07660983999268792
At round 10 eta: 0.6906975851284028
At round 10 a_n: 24.7571443865612
At round 10 local rounds: 12.11742183622639
At round 10 global rounds: 80.0418722784263
gradient difference: 0.42101359367370605
train() client id: f_00000-0-0 loss: 1.086689  [   32/  126]
train() client id: f_00000-0-1 loss: 1.343254  [   64/  126]
train() client id: f_00000-0-2 loss: 1.198717  [   96/  126]
train() client id: f_00000-1-0 loss: 1.066959  [   32/  126]
train() client id: f_00000-1-1 loss: 1.205711  [   64/  126]
train() client id: f_00000-1-2 loss: 1.123338  [   96/  126]
train() client id: f_00000-2-0 loss: 1.224774  [   32/  126]
train() client id: f_00000-2-1 loss: 1.048646  [   64/  126]
train() client id: f_00000-2-2 loss: 1.069801  [   96/  126]
train() client id: f_00000-3-0 loss: 1.040042  [   32/  126]
train() client id: f_00000-3-1 loss: 1.023772  [   64/  126]
train() client id: f_00000-3-2 loss: 0.998804  [   96/  126]
train() client id: f_00000-4-0 loss: 0.984387  [   32/  126]
train() client id: f_00000-4-1 loss: 0.995217  [   64/  126]
train() client id: f_00000-4-2 loss: 0.913973  [   96/  126]
train() client id: f_00000-5-0 loss: 0.932646  [   32/  126]
train() client id: f_00000-5-1 loss: 1.003891  [   64/  126]
train() client id: f_00000-5-2 loss: 0.910400  [   96/  126]
train() client id: f_00000-6-0 loss: 0.913479  [   32/  126]
train() client id: f_00000-6-1 loss: 0.945897  [   64/  126]
train() client id: f_00000-6-2 loss: 0.993260  [   96/  126]
train() client id: f_00000-7-0 loss: 0.918198  [   32/  126]
train() client id: f_00000-7-1 loss: 0.955324  [   64/  126]
train() client id: f_00000-7-2 loss: 0.961569  [   96/  126]
train() client id: f_00000-8-0 loss: 0.960223  [   32/  126]
train() client id: f_00000-8-1 loss: 0.943867  [   64/  126]
train() client id: f_00000-8-2 loss: 0.919033  [   96/  126]
train() client id: f_00000-9-0 loss: 0.959226  [   32/  126]
train() client id: f_00000-9-1 loss: 0.864193  [   64/  126]
train() client id: f_00000-9-2 loss: 0.957667  [   96/  126]
train() client id: f_00000-10-0 loss: 0.894872  [   32/  126]
train() client id: f_00000-10-1 loss: 0.910032  [   64/  126]
train() client id: f_00000-10-2 loss: 0.987109  [   96/  126]
train() client id: f_00000-11-0 loss: 0.866904  [   32/  126]
train() client id: f_00000-11-1 loss: 0.903013  [   64/  126]
train() client id: f_00000-11-2 loss: 0.944866  [   96/  126]
train() client id: f_00001-0-0 loss: 0.536456  [   32/  265]
train() client id: f_00001-0-1 loss: 0.590087  [   64/  265]
train() client id: f_00001-0-2 loss: 0.527458  [   96/  265]
train() client id: f_00001-0-3 loss: 0.594091  [  128/  265]
train() client id: f_00001-0-4 loss: 0.509604  [  160/  265]
train() client id: f_00001-0-5 loss: 0.588026  [  192/  265]
train() client id: f_00001-0-6 loss: 0.529335  [  224/  265]
train() client id: f_00001-0-7 loss: 0.537684  [  256/  265]
train() client id: f_00001-1-0 loss: 0.470416  [   32/  265]
train() client id: f_00001-1-1 loss: 0.480945  [   64/  265]
train() client id: f_00001-1-2 loss: 0.613194  [   96/  265]
train() client id: f_00001-1-3 loss: 0.599815  [  128/  265]
train() client id: f_00001-1-4 loss: 0.474526  [  160/  265]
train() client id: f_00001-1-5 loss: 0.590601  [  192/  265]
train() client id: f_00001-1-6 loss: 0.461967  [  224/  265]
train() client id: f_00001-1-7 loss: 0.551466  [  256/  265]
train() client id: f_00001-2-0 loss: 0.452900  [   32/  265]
train() client id: f_00001-2-1 loss: 0.520303  [   64/  265]
train() client id: f_00001-2-2 loss: 0.510143  [   96/  265]
train() client id: f_00001-2-3 loss: 0.466469  [  128/  265]
train() client id: f_00001-2-4 loss: 0.477657  [  160/  265]
train() client id: f_00001-2-5 loss: 0.458130  [  192/  265]
train() client id: f_00001-2-6 loss: 0.588838  [  224/  265]
train() client id: f_00001-2-7 loss: 0.675731  [  256/  265]
train() client id: f_00001-3-0 loss: 0.509900  [   32/  265]
train() client id: f_00001-3-1 loss: 0.556557  [   64/  265]
train() client id: f_00001-3-2 loss: 0.473007  [   96/  265]
train() client id: f_00001-3-3 loss: 0.503361  [  128/  265]
train() client id: f_00001-3-4 loss: 0.593196  [  160/  265]
train() client id: f_00001-3-5 loss: 0.521298  [  192/  265]
train() client id: f_00001-3-6 loss: 0.559367  [  224/  265]
train() client id: f_00001-3-7 loss: 0.448927  [  256/  265]
train() client id: f_00001-4-0 loss: 0.617676  [   32/  265]
train() client id: f_00001-4-1 loss: 0.426574  [   64/  265]
train() client id: f_00001-4-2 loss: 0.513024  [   96/  265]
train() client id: f_00001-4-3 loss: 0.549275  [  128/  265]
train() client id: f_00001-4-4 loss: 0.413873  [  160/  265]
train() client id: f_00001-4-5 loss: 0.564345  [  192/  265]
train() client id: f_00001-4-6 loss: 0.431207  [  224/  265]
train() client id: f_00001-4-7 loss: 0.605688  [  256/  265]
train() client id: f_00001-5-0 loss: 0.435189  [   32/  265]
train() client id: f_00001-5-1 loss: 0.531303  [   64/  265]
train() client id: f_00001-5-2 loss: 0.538786  [   96/  265]
train() client id: f_00001-5-3 loss: 0.432743  [  128/  265]
train() client id: f_00001-5-4 loss: 0.534976  [  160/  265]
train() client id: f_00001-5-5 loss: 0.580842  [  192/  265]
train() client id: f_00001-5-6 loss: 0.490195  [  224/  265]
train() client id: f_00001-5-7 loss: 0.466871  [  256/  265]
train() client id: f_00001-6-0 loss: 0.420429  [   32/  265]
train() client id: f_00001-6-1 loss: 0.488617  [   64/  265]
train() client id: f_00001-6-2 loss: 0.620458  [   96/  265]
train() client id: f_00001-6-3 loss: 0.495638  [  128/  265]
train() client id: f_00001-6-4 loss: 0.534460  [  160/  265]
train() client id: f_00001-6-5 loss: 0.522682  [  192/  265]
train() client id: f_00001-6-6 loss: 0.460267  [  224/  265]
train() client id: f_00001-6-7 loss: 0.518916  [  256/  265]
train() client id: f_00001-7-0 loss: 0.510968  [   32/  265]
train() client id: f_00001-7-1 loss: 0.480389  [   64/  265]
train() client id: f_00001-7-2 loss: 0.487624  [   96/  265]
train() client id: f_00001-7-3 loss: 0.581592  [  128/  265]
train() client id: f_00001-7-4 loss: 0.473205  [  160/  265]
train() client id: f_00001-7-5 loss: 0.517793  [  192/  265]
train() client id: f_00001-7-6 loss: 0.532995  [  224/  265]
train() client id: f_00001-7-7 loss: 0.448711  [  256/  265]
train() client id: f_00001-8-0 loss: 0.500733  [   32/  265]
train() client id: f_00001-8-1 loss: 0.700112  [   64/  265]
train() client id: f_00001-8-2 loss: 0.441782  [   96/  265]
train() client id: f_00001-8-3 loss: 0.528656  [  128/  265]
train() client id: f_00001-8-4 loss: 0.443511  [  160/  265]
train() client id: f_00001-8-5 loss: 0.448943  [  192/  265]
train() client id: f_00001-8-6 loss: 0.466448  [  224/  265]
train() client id: f_00001-8-7 loss: 0.507382  [  256/  265]
train() client id: f_00001-9-0 loss: 0.410809  [   32/  265]
train() client id: f_00001-9-1 loss: 0.569449  [   64/  265]
train() client id: f_00001-9-2 loss: 0.486057  [   96/  265]
train() client id: f_00001-9-3 loss: 0.530532  [  128/  265]
train() client id: f_00001-9-4 loss: 0.518286  [  160/  265]
train() client id: f_00001-9-5 loss: 0.415593  [  192/  265]
train() client id: f_00001-9-6 loss: 0.582212  [  224/  265]
train() client id: f_00001-9-7 loss: 0.460075  [  256/  265]
train() client id: f_00001-10-0 loss: 0.475831  [   32/  265]
train() client id: f_00001-10-1 loss: 0.563261  [   64/  265]
train() client id: f_00001-10-2 loss: 0.607326  [   96/  265]
train() client id: f_00001-10-3 loss: 0.421088  [  128/  265]
train() client id: f_00001-10-4 loss: 0.419512  [  160/  265]
train() client id: f_00001-10-5 loss: 0.478839  [  192/  265]
train() client id: f_00001-10-6 loss: 0.442261  [  224/  265]
train() client id: f_00001-10-7 loss: 0.560609  [  256/  265]
train() client id: f_00001-11-0 loss: 0.654039  [   32/  265]
train() client id: f_00001-11-1 loss: 0.570235  [   64/  265]
train() client id: f_00001-11-2 loss: 0.429878  [   96/  265]
train() client id: f_00001-11-3 loss: 0.418978  [  128/  265]
train() client id: f_00001-11-4 loss: 0.441932  [  160/  265]
train() client id: f_00001-11-5 loss: 0.414944  [  192/  265]
train() client id: f_00001-11-6 loss: 0.417939  [  224/  265]
train() client id: f_00001-11-7 loss: 0.546842  [  256/  265]
train() client id: f_00002-0-0 loss: 1.121084  [   32/  124]
train() client id: f_00002-0-1 loss: 1.238325  [   64/  124]
train() client id: f_00002-0-2 loss: 1.236359  [   96/  124]
train() client id: f_00002-1-0 loss: 1.067367  [   32/  124]
train() client id: f_00002-1-1 loss: 1.156954  [   64/  124]
train() client id: f_00002-1-2 loss: 1.168246  [   96/  124]
train() client id: f_00002-2-0 loss: 1.145112  [   32/  124]
train() client id: f_00002-2-1 loss: 1.080458  [   64/  124]
train() client id: f_00002-2-2 loss: 1.059052  [   96/  124]
train() client id: f_00002-3-0 loss: 0.930284  [   32/  124]
train() client id: f_00002-3-1 loss: 1.209533  [   64/  124]
train() client id: f_00002-3-2 loss: 1.008808  [   96/  124]
train() client id: f_00002-4-0 loss: 0.964315  [   32/  124]
train() client id: f_00002-4-1 loss: 1.030861  [   64/  124]
train() client id: f_00002-4-2 loss: 1.069505  [   96/  124]
train() client id: f_00002-5-0 loss: 0.991943  [   32/  124]
train() client id: f_00002-5-1 loss: 0.954177  [   64/  124]
train() client id: f_00002-5-2 loss: 1.001358  [   96/  124]
train() client id: f_00002-6-0 loss: 0.950244  [   32/  124]
train() client id: f_00002-6-1 loss: 1.019388  [   64/  124]
train() client id: f_00002-6-2 loss: 0.987043  [   96/  124]
train() client id: f_00002-7-0 loss: 0.926455  [   32/  124]
train() client id: f_00002-7-1 loss: 0.969852  [   64/  124]
train() client id: f_00002-7-2 loss: 1.019907  [   96/  124]
train() client id: f_00002-8-0 loss: 1.021275  [   32/  124]
train() client id: f_00002-8-1 loss: 0.981093  [   64/  124]
train() client id: f_00002-8-2 loss: 0.829410  [   96/  124]
train() client id: f_00002-9-0 loss: 0.863244  [   32/  124]
train() client id: f_00002-9-1 loss: 1.007370  [   64/  124]
train() client id: f_00002-9-2 loss: 0.884443  [   96/  124]
train() client id: f_00002-10-0 loss: 0.967682  [   32/  124]
train() client id: f_00002-10-1 loss: 0.862690  [   64/  124]
train() client id: f_00002-10-2 loss: 0.878901  [   96/  124]
train() client id: f_00002-11-0 loss: 0.970255  [   32/  124]
train() client id: f_00002-11-1 loss: 0.917827  [   64/  124]
train() client id: f_00002-11-2 loss: 0.898228  [   96/  124]
train() client id: f_00003-0-0 loss: 0.862691  [   32/   43]
train() client id: f_00003-1-0 loss: 0.983996  [   32/   43]
train() client id: f_00003-2-0 loss: 0.880253  [   32/   43]
train() client id: f_00003-3-0 loss: 0.945510  [   32/   43]
train() client id: f_00003-4-0 loss: 0.876507  [   32/   43]
train() client id: f_00003-5-0 loss: 0.862276  [   32/   43]
train() client id: f_00003-6-0 loss: 0.961224  [   32/   43]
train() client id: f_00003-7-0 loss: 0.837838  [   32/   43]
train() client id: f_00003-8-0 loss: 1.018090  [   32/   43]
train() client id: f_00003-9-0 loss: 0.814147  [   32/   43]
train() client id: f_00003-10-0 loss: 0.866292  [   32/   43]
train() client id: f_00003-11-0 loss: 0.915229  [   32/   43]
train() client id: f_00004-0-0 loss: 0.996387  [   32/  306]
train() client id: f_00004-0-1 loss: 1.023247  [   64/  306]
train() client id: f_00004-0-2 loss: 1.148144  [   96/  306]
train() client id: f_00004-0-3 loss: 0.979597  [  128/  306]
train() client id: f_00004-0-4 loss: 1.149636  [  160/  306]
train() client id: f_00004-0-5 loss: 1.110161  [  192/  306]
train() client id: f_00004-0-6 loss: 1.024724  [  224/  306]
train() client id: f_00004-0-7 loss: 1.015242  [  256/  306]
train() client id: f_00004-0-8 loss: 1.037811  [  288/  306]
train() client id: f_00004-1-0 loss: 1.009620  [   32/  306]
train() client id: f_00004-1-1 loss: 1.007636  [   64/  306]
train() client id: f_00004-1-2 loss: 1.038269  [   96/  306]
train() client id: f_00004-1-3 loss: 1.004764  [  128/  306]
train() client id: f_00004-1-4 loss: 1.038215  [  160/  306]
train() client id: f_00004-1-5 loss: 1.090080  [  192/  306]
train() client id: f_00004-1-6 loss: 1.108024  [  224/  306]
train() client id: f_00004-1-7 loss: 1.025102  [  256/  306]
train() client id: f_00004-1-8 loss: 1.163527  [  288/  306]
train() client id: f_00004-2-0 loss: 0.997212  [   32/  306]
train() client id: f_00004-2-1 loss: 1.016952  [   64/  306]
train() client id: f_00004-2-2 loss: 0.971017  [   96/  306]
train() client id: f_00004-2-3 loss: 1.087437  [  128/  306]
train() client id: f_00004-2-4 loss: 1.168245  [  160/  306]
train() client id: f_00004-2-5 loss: 1.139228  [  192/  306]
train() client id: f_00004-2-6 loss: 0.937999  [  224/  306]
train() client id: f_00004-2-7 loss: 1.019692  [  256/  306]
train() client id: f_00004-2-8 loss: 1.069352  [  288/  306]
train() client id: f_00004-3-0 loss: 1.011454  [   32/  306]
train() client id: f_00004-3-1 loss: 1.115881  [   64/  306]
train() client id: f_00004-3-2 loss: 0.973053  [   96/  306]
train() client id: f_00004-3-3 loss: 1.062327  [  128/  306]
train() client id: f_00004-3-4 loss: 1.049989  [  160/  306]
train() client id: f_00004-3-5 loss: 1.041637  [  192/  306]
train() client id: f_00004-3-6 loss: 1.093541  [  224/  306]
train() client id: f_00004-3-7 loss: 1.123514  [  256/  306]
train() client id: f_00004-3-8 loss: 0.906449  [  288/  306]
train() client id: f_00004-4-0 loss: 1.095730  [   32/  306]
train() client id: f_00004-4-1 loss: 1.140890  [   64/  306]
train() client id: f_00004-4-2 loss: 1.051700  [   96/  306]
train() client id: f_00004-4-3 loss: 0.968104  [  128/  306]
train() client id: f_00004-4-4 loss: 1.058128  [  160/  306]
train() client id: f_00004-4-5 loss: 0.991350  [  192/  306]
train() client id: f_00004-4-6 loss: 1.079073  [  224/  306]
train() client id: f_00004-4-7 loss: 0.992443  [  256/  306]
train() client id: f_00004-4-8 loss: 0.951690  [  288/  306]
train() client id: f_00004-5-0 loss: 1.056579  [   32/  306]
train() client id: f_00004-5-1 loss: 0.927528  [   64/  306]
train() client id: f_00004-5-2 loss: 1.115517  [   96/  306]
train() client id: f_00004-5-3 loss: 0.979274  [  128/  306]
train() client id: f_00004-5-4 loss: 1.051396  [  160/  306]
train() client id: f_00004-5-5 loss: 1.043998  [  192/  306]
train() client id: f_00004-5-6 loss: 1.022377  [  224/  306]
train() client id: f_00004-5-7 loss: 1.091874  [  256/  306]
train() client id: f_00004-5-8 loss: 1.060012  [  288/  306]
train() client id: f_00004-6-0 loss: 1.085138  [   32/  306]
train() client id: f_00004-6-1 loss: 1.093781  [   64/  306]
train() client id: f_00004-6-2 loss: 1.068126  [   96/  306]
train() client id: f_00004-6-3 loss: 1.007807  [  128/  306]
train() client id: f_00004-6-4 loss: 1.087206  [  160/  306]
train() client id: f_00004-6-5 loss: 0.970443  [  192/  306]
train() client id: f_00004-6-6 loss: 0.969780  [  224/  306]
train() client id: f_00004-6-7 loss: 0.960405  [  256/  306]
train() client id: f_00004-6-8 loss: 1.084226  [  288/  306]
train() client id: f_00004-7-0 loss: 1.008983  [   32/  306]
train() client id: f_00004-7-1 loss: 1.220977  [   64/  306]
train() client id: f_00004-7-2 loss: 0.993206  [   96/  306]
train() client id: f_00004-7-3 loss: 1.100157  [  128/  306]
train() client id: f_00004-7-4 loss: 1.031305  [  160/  306]
train() client id: f_00004-7-5 loss: 1.003774  [  192/  306]
train() client id: f_00004-7-6 loss: 0.970474  [  224/  306]
train() client id: f_00004-7-7 loss: 1.013466  [  256/  306]
train() client id: f_00004-7-8 loss: 0.919480  [  288/  306]
train() client id: f_00004-8-0 loss: 0.934552  [   32/  306]
train() client id: f_00004-8-1 loss: 1.178553  [   64/  306]
train() client id: f_00004-8-2 loss: 1.001860  [   96/  306]
train() client id: f_00004-8-3 loss: 1.089399  [  128/  306]
train() client id: f_00004-8-4 loss: 1.057407  [  160/  306]
train() client id: f_00004-8-5 loss: 1.060984  [  192/  306]
train() client id: f_00004-8-6 loss: 0.954246  [  224/  306]
train() client id: f_00004-8-7 loss: 0.951872  [  256/  306]
train() client id: f_00004-8-8 loss: 0.989212  [  288/  306]
train() client id: f_00004-9-0 loss: 1.031624  [   32/  306]
train() client id: f_00004-9-1 loss: 1.046532  [   64/  306]
train() client id: f_00004-9-2 loss: 1.068434  [   96/  306]
train() client id: f_00004-9-3 loss: 0.964864  [  128/  306]
train() client id: f_00004-9-4 loss: 1.116332  [  160/  306]
train() client id: f_00004-9-5 loss: 1.153545  [  192/  306]
train() client id: f_00004-9-6 loss: 0.934035  [  224/  306]
train() client id: f_00004-9-7 loss: 0.876123  [  256/  306]
train() client id: f_00004-9-8 loss: 1.020355  [  288/  306]
train() client id: f_00004-10-0 loss: 0.980103  [   32/  306]
train() client id: f_00004-10-1 loss: 0.976552  [   64/  306]
train() client id: f_00004-10-2 loss: 0.994189  [   96/  306]
train() client id: f_00004-10-3 loss: 1.051055  [  128/  306]
train() client id: f_00004-10-4 loss: 1.077860  [  160/  306]
train() client id: f_00004-10-5 loss: 1.164370  [  192/  306]
train() client id: f_00004-10-6 loss: 1.045737  [  224/  306]
train() client id: f_00004-10-7 loss: 1.025326  [  256/  306]
train() client id: f_00004-10-8 loss: 0.936052  [  288/  306]
train() client id: f_00004-11-0 loss: 0.922499  [   32/  306]
train() client id: f_00004-11-1 loss: 0.953936  [   64/  306]
train() client id: f_00004-11-2 loss: 1.019390  [   96/  306]
train() client id: f_00004-11-3 loss: 1.057523  [  128/  306]
train() client id: f_00004-11-4 loss: 1.109842  [  160/  306]
train() client id: f_00004-11-5 loss: 0.922350  [  192/  306]
train() client id: f_00004-11-6 loss: 1.197587  [  224/  306]
train() client id: f_00004-11-7 loss: 1.016093  [  256/  306]
train() client id: f_00004-11-8 loss: 1.045732  [  288/  306]
train() client id: f_00005-0-0 loss: 0.788216  [   32/  146]
train() client id: f_00005-0-1 loss: 0.866247  [   64/  146]
train() client id: f_00005-0-2 loss: 0.795175  [   96/  146]
train() client id: f_00005-0-3 loss: 1.092975  [  128/  146]
train() client id: f_00005-1-0 loss: 0.868575  [   32/  146]
train() client id: f_00005-1-1 loss: 0.935051  [   64/  146]
train() client id: f_00005-1-2 loss: 0.886285  [   96/  146]
train() client id: f_00005-1-3 loss: 0.758164  [  128/  146]
train() client id: f_00005-2-0 loss: 0.959592  [   32/  146]
train() client id: f_00005-2-1 loss: 0.886794  [   64/  146]
train() client id: f_00005-2-2 loss: 0.753233  [   96/  146]
train() client id: f_00005-2-3 loss: 0.833396  [  128/  146]
train() client id: f_00005-3-0 loss: 0.789986  [   32/  146]
train() client id: f_00005-3-1 loss: 0.780936  [   64/  146]
train() client id: f_00005-3-2 loss: 0.966392  [   96/  146]
train() client id: f_00005-3-3 loss: 0.926507  [  128/  146]
train() client id: f_00005-4-0 loss: 0.957307  [   32/  146]
train() client id: f_00005-4-1 loss: 0.810103  [   64/  146]
train() client id: f_00005-4-2 loss: 0.680734  [   96/  146]
train() client id: f_00005-4-3 loss: 1.019014  [  128/  146]
train() client id: f_00005-5-0 loss: 0.837392  [   32/  146]
train() client id: f_00005-5-1 loss: 0.947485  [   64/  146]
train() client id: f_00005-5-2 loss: 0.886157  [   96/  146]
train() client id: f_00005-5-3 loss: 0.775460  [  128/  146]
train() client id: f_00005-6-0 loss: 0.762041  [   32/  146]
train() client id: f_00005-6-1 loss: 0.885861  [   64/  146]
train() client id: f_00005-6-2 loss: 0.765266  [   96/  146]
train() client id: f_00005-6-3 loss: 0.970959  [  128/  146]
train() client id: f_00005-7-0 loss: 0.905233  [   32/  146]
train() client id: f_00005-7-1 loss: 0.703734  [   64/  146]
train() client id: f_00005-7-2 loss: 0.828453  [   96/  146]
train() client id: f_00005-7-3 loss: 1.115710  [  128/  146]
train() client id: f_00005-8-0 loss: 0.711498  [   32/  146]
train() client id: f_00005-8-1 loss: 0.956440  [   64/  146]
train() client id: f_00005-8-2 loss: 0.922256  [   96/  146]
train() client id: f_00005-8-3 loss: 0.930026  [  128/  146]
train() client id: f_00005-9-0 loss: 0.783852  [   32/  146]
train() client id: f_00005-9-1 loss: 0.969710  [   64/  146]
train() client id: f_00005-9-2 loss: 0.915636  [   96/  146]
train() client id: f_00005-9-3 loss: 0.917199  [  128/  146]
train() client id: f_00005-10-0 loss: 0.951458  [   32/  146]
train() client id: f_00005-10-1 loss: 0.771058  [   64/  146]
train() client id: f_00005-10-2 loss: 0.672182  [   96/  146]
train() client id: f_00005-10-3 loss: 0.970759  [  128/  146]
train() client id: f_00005-11-0 loss: 0.820520  [   32/  146]
train() client id: f_00005-11-1 loss: 1.103050  [   64/  146]
train() client id: f_00005-11-2 loss: 0.773462  [   96/  146]
train() client id: f_00005-11-3 loss: 0.758002  [  128/  146]
train() client id: f_00006-0-0 loss: 0.648623  [   32/   54]
train() client id: f_00006-1-0 loss: 0.606663  [   32/   54]
train() client id: f_00006-2-0 loss: 0.641511  [   32/   54]
train() client id: f_00006-3-0 loss: 0.702760  [   32/   54]
train() client id: f_00006-4-0 loss: 0.632101  [   32/   54]
train() client id: f_00006-5-0 loss: 0.599881  [   32/   54]
train() client id: f_00006-6-0 loss: 0.709838  [   32/   54]
train() client id: f_00006-7-0 loss: 0.649337  [   32/   54]
train() client id: f_00006-8-0 loss: 0.701104  [   32/   54]
train() client id: f_00006-9-0 loss: 0.674079  [   32/   54]
train() client id: f_00006-10-0 loss: 0.646212  [   32/   54]
train() client id: f_00006-11-0 loss: 0.606696  [   32/   54]
train() client id: f_00007-0-0 loss: 0.795855  [   32/  179]
train() client id: f_00007-0-1 loss: 0.720152  [   64/  179]
train() client id: f_00007-0-2 loss: 0.801962  [   96/  179]
train() client id: f_00007-0-3 loss: 0.619889  [  128/  179]
train() client id: f_00007-0-4 loss: 0.648179  [  160/  179]
train() client id: f_00007-1-0 loss: 0.628762  [   32/  179]
train() client id: f_00007-1-1 loss: 0.920834  [   64/  179]
train() client id: f_00007-1-2 loss: 0.638533  [   96/  179]
train() client id: f_00007-1-3 loss: 0.663393  [  128/  179]
train() client id: f_00007-1-4 loss: 0.786585  [  160/  179]
train() client id: f_00007-2-0 loss: 0.636746  [   32/  179]
train() client id: f_00007-2-1 loss: 0.784808  [   64/  179]
train() client id: f_00007-2-2 loss: 0.657668  [   96/  179]
train() client id: f_00007-2-3 loss: 0.714844  [  128/  179]
train() client id: f_00007-2-4 loss: 0.647581  [  160/  179]
train() client id: f_00007-3-0 loss: 0.886071  [   32/  179]
train() client id: f_00007-3-1 loss: 0.674963  [   64/  179]
train() client id: f_00007-3-2 loss: 0.685817  [   96/  179]
train() client id: f_00007-3-3 loss: 0.610325  [  128/  179]
train() client id: f_00007-3-4 loss: 0.602370  [  160/  179]
train() client id: f_00007-4-0 loss: 0.700824  [   32/  179]
train() client id: f_00007-4-1 loss: 0.679546  [   64/  179]
train() client id: f_00007-4-2 loss: 0.734208  [   96/  179]
train() client id: f_00007-4-3 loss: 0.687441  [  128/  179]
train() client id: f_00007-4-4 loss: 0.657228  [  160/  179]
train() client id: f_00007-5-0 loss: 0.677565  [   32/  179]
train() client id: f_00007-5-1 loss: 0.646582  [   64/  179]
train() client id: f_00007-5-2 loss: 0.571556  [   96/  179]
train() client id: f_00007-5-3 loss: 0.736146  [  128/  179]
train() client id: f_00007-5-4 loss: 0.637386  [  160/  179]
train() client id: f_00007-6-0 loss: 0.648429  [   32/  179]
train() client id: f_00007-6-1 loss: 0.657505  [   64/  179]
train() client id: f_00007-6-2 loss: 0.720612  [   96/  179]
train() client id: f_00007-6-3 loss: 0.629207  [  128/  179]
train() client id: f_00007-6-4 loss: 0.769614  [  160/  179]
train() client id: f_00007-7-0 loss: 0.622951  [   32/  179]
train() client id: f_00007-7-1 loss: 0.584458  [   64/  179]
train() client id: f_00007-7-2 loss: 0.635312  [   96/  179]
train() client id: f_00007-7-3 loss: 0.725444  [  128/  179]
train() client id: f_00007-7-4 loss: 0.733722  [  160/  179]
train() client id: f_00007-8-0 loss: 0.615105  [   32/  179]
train() client id: f_00007-8-1 loss: 0.653665  [   64/  179]
train() client id: f_00007-8-2 loss: 0.835100  [   96/  179]
train() client id: f_00007-8-3 loss: 0.566164  [  128/  179]
train() client id: f_00007-8-4 loss: 0.620312  [  160/  179]
train() client id: f_00007-9-0 loss: 0.808102  [   32/  179]
train() client id: f_00007-9-1 loss: 0.604875  [   64/  179]
train() client id: f_00007-9-2 loss: 0.615165  [   96/  179]
train() client id: f_00007-9-3 loss: 0.697996  [  128/  179]
train() client id: f_00007-9-4 loss: 0.623608  [  160/  179]
train() client id: f_00007-10-0 loss: 0.554213  [   32/  179]
train() client id: f_00007-10-1 loss: 0.647152  [   64/  179]
train() client id: f_00007-10-2 loss: 0.585474  [   96/  179]
train() client id: f_00007-10-3 loss: 0.709761  [  128/  179]
train() client id: f_00007-10-4 loss: 0.602886  [  160/  179]
train() client id: f_00007-11-0 loss: 0.619276  [   32/  179]
train() client id: f_00007-11-1 loss: 0.776833  [   64/  179]
train() client id: f_00007-11-2 loss: 0.610648  [   96/  179]
train() client id: f_00007-11-3 loss: 0.600907  [  128/  179]
train() client id: f_00007-11-4 loss: 0.638858  [  160/  179]
train() client id: f_00008-0-0 loss: 0.578928  [   32/  130]
train() client id: f_00008-0-1 loss: 0.723604  [   64/  130]
train() client id: f_00008-0-2 loss: 0.887317  [   96/  130]
train() client id: f_00008-0-3 loss: 0.875326  [  128/  130]
train() client id: f_00008-1-0 loss: 0.743279  [   32/  130]
train() client id: f_00008-1-1 loss: 0.710698  [   64/  130]
train() client id: f_00008-1-2 loss: 0.764569  [   96/  130]
train() client id: f_00008-1-3 loss: 0.796849  [  128/  130]
train() client id: f_00008-2-0 loss: 0.819253  [   32/  130]
train() client id: f_00008-2-1 loss: 0.746404  [   64/  130]
train() client id: f_00008-2-2 loss: 0.766635  [   96/  130]
train() client id: f_00008-2-3 loss: 0.722013  [  128/  130]
train() client id: f_00008-3-0 loss: 0.728618  [   32/  130]
train() client id: f_00008-3-1 loss: 0.797082  [   64/  130]
train() client id: f_00008-3-2 loss: 0.704552  [   96/  130]
train() client id: f_00008-3-3 loss: 0.785608  [  128/  130]
train() client id: f_00008-4-0 loss: 0.831250  [   32/  130]
train() client id: f_00008-4-1 loss: 0.726354  [   64/  130]
train() client id: f_00008-4-2 loss: 0.795413  [   96/  130]
train() client id: f_00008-4-3 loss: 0.691585  [  128/  130]
train() client id: f_00008-5-0 loss: 0.806842  [   32/  130]
train() client id: f_00008-5-1 loss: 0.797837  [   64/  130]
train() client id: f_00008-5-2 loss: 0.716524  [   96/  130]
train() client id: f_00008-5-3 loss: 0.688733  [  128/  130]
train() client id: f_00008-6-0 loss: 0.847159  [   32/  130]
train() client id: f_00008-6-1 loss: 0.689120  [   64/  130]
train() client id: f_00008-6-2 loss: 0.757264  [   96/  130]
train() client id: f_00008-6-3 loss: 0.738125  [  128/  130]
train() client id: f_00008-7-0 loss: 0.739315  [   32/  130]
train() client id: f_00008-7-1 loss: 0.858280  [   64/  130]
train() client id: f_00008-7-2 loss: 0.755865  [   96/  130]
train() client id: f_00008-7-3 loss: 0.648879  [  128/  130]
train() client id: f_00008-8-0 loss: 0.855940  [   32/  130]
train() client id: f_00008-8-1 loss: 0.659850  [   64/  130]
train() client id: f_00008-8-2 loss: 0.822129  [   96/  130]
train() client id: f_00008-8-3 loss: 0.699360  [  128/  130]
train() client id: f_00008-9-0 loss: 0.846574  [   32/  130]
train() client id: f_00008-9-1 loss: 0.628290  [   64/  130]
train() client id: f_00008-9-2 loss: 0.807224  [   96/  130]
train() client id: f_00008-9-3 loss: 0.754729  [  128/  130]
train() client id: f_00008-10-0 loss: 0.879594  [   32/  130]
train() client id: f_00008-10-1 loss: 0.672923  [   64/  130]
train() client id: f_00008-10-2 loss: 0.809614  [   96/  130]
train() client id: f_00008-10-3 loss: 0.674169  [  128/  130]
train() client id: f_00008-11-0 loss: 0.713366  [   32/  130]
train() client id: f_00008-11-1 loss: 0.827272  [   64/  130]
train() client id: f_00008-11-2 loss: 0.772020  [   96/  130]
train() client id: f_00008-11-3 loss: 0.714172  [  128/  130]
train() client id: f_00009-0-0 loss: 1.257288  [   32/  118]
train() client id: f_00009-0-1 loss: 1.260521  [   64/  118]
train() client id: f_00009-0-2 loss: 1.128490  [   96/  118]
train() client id: f_00009-1-0 loss: 1.123356  [   32/  118]
train() client id: f_00009-1-1 loss: 1.076662  [   64/  118]
train() client id: f_00009-1-2 loss: 1.206180  [   96/  118]
train() client id: f_00009-2-0 loss: 1.087839  [   32/  118]
train() client id: f_00009-2-1 loss: 1.120040  [   64/  118]
train() client id: f_00009-2-2 loss: 1.123759  [   96/  118]
train() client id: f_00009-3-0 loss: 0.940673  [   32/  118]
train() client id: f_00009-3-1 loss: 1.041103  [   64/  118]
train() client id: f_00009-3-2 loss: 1.008897  [   96/  118]
train() client id: f_00009-4-0 loss: 0.989925  [   32/  118]
train() client id: f_00009-4-1 loss: 1.062090  [   64/  118]
train() client id: f_00009-4-2 loss: 1.000583  [   96/  118]
train() client id: f_00009-5-0 loss: 1.065939  [   32/  118]
train() client id: f_00009-5-1 loss: 0.980282  [   64/  118]
train() client id: f_00009-5-2 loss: 0.877873  [   96/  118]
train() client id: f_00009-6-0 loss: 0.865269  [   32/  118]
train() client id: f_00009-6-1 loss: 1.036464  [   64/  118]
train() client id: f_00009-6-2 loss: 1.024030  [   96/  118]
train() client id: f_00009-7-0 loss: 0.871925  [   32/  118]
train() client id: f_00009-7-1 loss: 0.986495  [   64/  118]
train() client id: f_00009-7-2 loss: 0.945482  [   96/  118]
train() client id: f_00009-8-0 loss: 0.898702  [   32/  118]
train() client id: f_00009-8-1 loss: 0.965635  [   64/  118]
train() client id: f_00009-8-2 loss: 0.930781  [   96/  118]
train() client id: f_00009-9-0 loss: 0.857750  [   32/  118]
train() client id: f_00009-9-1 loss: 0.904696  [   64/  118]
train() client id: f_00009-9-2 loss: 1.019452  [   96/  118]
train() client id: f_00009-10-0 loss: 0.945258  [   32/  118]
train() client id: f_00009-10-1 loss: 0.886556  [   64/  118]
train() client id: f_00009-10-2 loss: 0.853525  [   96/  118]
train() client id: f_00009-11-0 loss: 0.818170  [   32/  118]
train() client id: f_00009-11-1 loss: 0.798819  [   64/  118]
train() client id: f_00009-11-2 loss: 1.009737  [   96/  118]
At round 10 accuracy: 0.6286472148541115
At round 10 training accuracy: 0.5707578806170356
At round 10 training loss: 0.8705799226308267
update_location
xs = [ -3.9056584    4.20031788  70.00902392  18.81129433   0.97929623
   3.95640986 -32.44319194 -11.32485185  54.66397685  -2.06087855]
ys = [ 62.5879595   45.55583871   1.32061395 -32.45517586  24.35018685
   7.81415074  -2.62498432   0.82234798  17.56900603  -0.99851822]
dists_uav = [118.03604044 109.96807314 122.07787453 106.80450943 102.92662736
 100.38283783 105.16392559 100.64257809 115.31183954 100.02621786]
dists_bs = [204.71868644 221.06230981 300.3230776  283.90221521 231.65508607
 244.90101025 227.75662137 239.01340671 278.44220249 246.74565476]
uav_gains = [6.60612004e-11 7.88546405e-11 6.07267938e-11 8.48246225e-11
 9.30418280e-11 9.90488970e-11 8.81718085e-11 9.84110509e-11
 7.00330537e-11 9.99341172e-11]
bs_gains = [3.73295817e-11 3.01059902e-11 1.27657316e-11 1.49424392e-11
 2.64080865e-11 2.26006261e-11 2.76933346e-11 2.41942270e-11
 1.57774174e-11 2.21307151e-11]
Round 11
-------------------------------
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.53941363 19.90793381  9.39345483  3.35836813 22.96346205 11.07093228
  4.17536772 13.46928103  9.90106646  8.98947369]
obj_prev = 112.76875362310263
eta_min = 1.8259794231621306e-10	eta_max = 0.9205107351620903
af = 23.839837477165666	bf = 1.854557298226717	zeta = 26.223821224882236	eta = 0.909090909090909
af = 23.839837477165666	bf = 1.854557298226717	zeta = 45.394101441493795	eta = 0.5251747852723034
af = 23.839837477165666	bf = 1.854557298226717	zeta = 36.24067924685511	eta = 0.6578198304391449
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.59948643670541	eta = 0.6890228709254712
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51842070411875	eta = 0.6906410255994443
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51820885585075	eta = 0.6906452642639038
eta = 0.6906452642639038
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.03054215 0.06423548 0.03005736 0.01042312 0.0741738  0.03539011
 0.01308949 0.04338925 0.03151175 0.02860297]
ene_total = [2.94794512 5.70363107 2.91862696 1.33519898 6.50745954 3.47131979
 1.5419756  3.92315854 3.2297488  2.93914446]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 0 obj = 6.04741367166706
eta = 0.6906452642639038
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
eta_min = 0.6906452642638832	eta_max = 0.6906452642639023
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 0.04826057494319857	eta = 0.909090909090909
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 20.44612752124682	eta = 0.0021457975307437047
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.1286929945616637	eta = 0.020610416842846153
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.067483129885282	eta = 0.02122060843649869
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.0674642356817103	eta = 0.021220802368025433
eta = 0.021220802368025433
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.99188226e-04 2.01117734e-03 1.91271134e-04 7.75548441e-06
 3.14943156e-03 3.49577711e-04 1.53136962e-05 5.53174501e-04
 2.61453995e-04 1.85124181e-04]
ene_total = [0.17486013 0.25058139 0.17748901 0.16220541 0.28513266 0.22310299
 0.16123818 0.17146061 0.24129522 0.22009864]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 1 obj = 6.047413671666661
eta = 0.6906452642638832
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
Done!
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.89463426e-05 1.91298731e-04 1.81932864e-05 7.37684489e-07
 2.99566950e-04 3.32510571e-05 1.45660485e-06 5.26167325e-05
 2.48689246e-05 1.76086018e-05]
ene_total = [0.00682989 0.00822591 0.00694244 0.00649578 0.00858108 0.00862785
 0.00645017 0.00637328 0.00943692 0.00865622]
At round 11 energy consumption: 0.0766195440375612
At round 11 eta: 0.6906452642638832
At round 11 a_n: 24.414598539591886
At round 11 local rounds: 12.119902394804324
At round 11 global rounds: 78.921043447086
gradient difference: 0.4101265072822571
train() client id: f_00000-0-0 loss: 1.420595  [   32/  126]
train() client id: f_00000-0-1 loss: 1.161998  [   64/  126]
train() client id: f_00000-0-2 loss: 1.400183  [   96/  126]
train() client id: f_00000-1-0 loss: 1.399896  [   32/  126]
train() client id: f_00000-1-1 loss: 1.068822  [   64/  126]
train() client id: f_00000-1-2 loss: 1.182254  [   96/  126]
train() client id: f_00000-2-0 loss: 1.186253  [   32/  126]
train() client id: f_00000-2-1 loss: 1.115193  [   64/  126]
train() client id: f_00000-2-2 loss: 1.103305  [   96/  126]
train() client id: f_00000-3-0 loss: 1.183309  [   32/  126]
train() client id: f_00000-3-1 loss: 1.036622  [   64/  126]
train() client id: f_00000-3-2 loss: 0.978746  [   96/  126]
train() client id: f_00000-4-0 loss: 1.034810  [   32/  126]
train() client id: f_00000-4-1 loss: 1.044643  [   64/  126]
train() client id: f_00000-4-2 loss: 0.977846  [   96/  126]
train() client id: f_00000-5-0 loss: 1.006339  [   32/  126]
train() client id: f_00000-5-1 loss: 1.024097  [   64/  126]
train() client id: f_00000-5-2 loss: 0.896171  [   96/  126]
train() client id: f_00000-6-0 loss: 1.038424  [   32/  126]
train() client id: f_00000-6-1 loss: 0.952183  [   64/  126]
train() client id: f_00000-6-2 loss: 0.867771  [   96/  126]
train() client id: f_00000-7-0 loss: 0.916200  [   32/  126]
train() client id: f_00000-7-1 loss: 1.020788  [   64/  126]
train() client id: f_00000-7-2 loss: 0.899379  [   96/  126]
train() client id: f_00000-8-0 loss: 0.888947  [   32/  126]
train() client id: f_00000-8-1 loss: 0.930049  [   64/  126]
train() client id: f_00000-8-2 loss: 0.950998  [   96/  126]
train() client id: f_00000-9-0 loss: 0.899857  [   32/  126]
train() client id: f_00000-9-1 loss: 0.920077  [   64/  126]
train() client id: f_00000-9-2 loss: 0.881135  [   96/  126]
train() client id: f_00000-10-0 loss: 0.846487  [   32/  126]
train() client id: f_00000-10-1 loss: 0.999313  [   64/  126]
train() client id: f_00000-10-2 loss: 0.920177  [   96/  126]
train() client id: f_00000-11-0 loss: 0.890923  [   32/  126]
train() client id: f_00000-11-1 loss: 0.941740  [   64/  126]
train() client id: f_00000-11-2 loss: 0.910962  [   96/  126]
train() client id: f_00001-0-0 loss: 0.471491  [   32/  265]
train() client id: f_00001-0-1 loss: 0.463619  [   64/  265]
train() client id: f_00001-0-2 loss: 0.495970  [   96/  265]
train() client id: f_00001-0-3 loss: 0.401480  [  128/  265]
train() client id: f_00001-0-4 loss: 0.470549  [  160/  265]
train() client id: f_00001-0-5 loss: 0.570077  [  192/  265]
train() client id: f_00001-0-6 loss: 0.586639  [  224/  265]
train() client id: f_00001-0-7 loss: 0.435040  [  256/  265]
train() client id: f_00001-1-0 loss: 0.559609  [   32/  265]
train() client id: f_00001-1-1 loss: 0.410465  [   64/  265]
train() client id: f_00001-1-2 loss: 0.573019  [   96/  265]
train() client id: f_00001-1-3 loss: 0.453002  [  128/  265]
train() client id: f_00001-1-4 loss: 0.455853  [  160/  265]
train() client id: f_00001-1-5 loss: 0.495612  [  192/  265]
train() client id: f_00001-1-6 loss: 0.492450  [  224/  265]
train() client id: f_00001-1-7 loss: 0.428893  [  256/  265]
train() client id: f_00001-2-0 loss: 0.519114  [   32/  265]
train() client id: f_00001-2-1 loss: 0.549384  [   64/  265]
train() client id: f_00001-2-2 loss: 0.483889  [   96/  265]
train() client id: f_00001-2-3 loss: 0.431999  [  128/  265]
train() client id: f_00001-2-4 loss: 0.462859  [  160/  265]
train() client id: f_00001-2-5 loss: 0.485303  [  192/  265]
train() client id: f_00001-2-6 loss: 0.393871  [  224/  265]
train() client id: f_00001-2-7 loss: 0.478133  [  256/  265]
train() client id: f_00001-3-0 loss: 0.546525  [   32/  265]
train() client id: f_00001-3-1 loss: 0.447364  [   64/  265]
train() client id: f_00001-3-2 loss: 0.514378  [   96/  265]
train() client id: f_00001-3-3 loss: 0.396683  [  128/  265]
train() client id: f_00001-3-4 loss: 0.386217  [  160/  265]
train() client id: f_00001-3-5 loss: 0.448410  [  192/  265]
train() client id: f_00001-3-6 loss: 0.567496  [  224/  265]
train() client id: f_00001-3-7 loss: 0.434117  [  256/  265]
train() client id: f_00001-4-0 loss: 0.422718  [   32/  265]
train() client id: f_00001-4-1 loss: 0.439351  [   64/  265]
train() client id: f_00001-4-2 loss: 0.496541  [   96/  265]
train() client id: f_00001-4-3 loss: 0.506951  [  128/  265]
train() client id: f_00001-4-4 loss: 0.398133  [  160/  265]
train() client id: f_00001-4-5 loss: 0.415748  [  192/  265]
train() client id: f_00001-4-6 loss: 0.453818  [  224/  265]
train() client id: f_00001-4-7 loss: 0.539729  [  256/  265]
train() client id: f_00001-5-0 loss: 0.402075  [   32/  265]
train() client id: f_00001-5-1 loss: 0.441691  [   64/  265]
train() client id: f_00001-5-2 loss: 0.520887  [   96/  265]
train() client id: f_00001-5-3 loss: 0.460666  [  128/  265]
train() client id: f_00001-5-4 loss: 0.449319  [  160/  265]
train() client id: f_00001-5-5 loss: 0.400774  [  192/  265]
train() client id: f_00001-5-6 loss: 0.433127  [  224/  265]
train() client id: f_00001-5-7 loss: 0.453795  [  256/  265]
train() client id: f_00001-6-0 loss: 0.443336  [   32/  265]
train() client id: f_00001-6-1 loss: 0.439895  [   64/  265]
train() client id: f_00001-6-2 loss: 0.360713  [   96/  265]
train() client id: f_00001-6-3 loss: 0.456373  [  128/  265]
train() client id: f_00001-6-4 loss: 0.530439  [  160/  265]
train() client id: f_00001-6-5 loss: 0.558473  [  192/  265]
train() client id: f_00001-6-6 loss: 0.362989  [  224/  265]
train() client id: f_00001-6-7 loss: 0.395377  [  256/  265]
train() client id: f_00001-7-0 loss: 0.356412  [   32/  265]
train() client id: f_00001-7-1 loss: 0.441944  [   64/  265]
train() client id: f_00001-7-2 loss: 0.425746  [   96/  265]
train() client id: f_00001-7-3 loss: 0.488750  [  128/  265]
train() client id: f_00001-7-4 loss: 0.383372  [  160/  265]
train() client id: f_00001-7-5 loss: 0.395105  [  192/  265]
train() client id: f_00001-7-6 loss: 0.508271  [  224/  265]
train() client id: f_00001-7-7 loss: 0.577304  [  256/  265]
train() client id: f_00001-8-0 loss: 0.456212  [   32/  265]
train() client id: f_00001-8-1 loss: 0.588622  [   64/  265]
train() client id: f_00001-8-2 loss: 0.405220  [   96/  265]
train() client id: f_00001-8-3 loss: 0.541738  [  128/  265]
train() client id: f_00001-8-4 loss: 0.345034  [  160/  265]
train() client id: f_00001-8-5 loss: 0.387557  [  192/  265]
train() client id: f_00001-8-6 loss: 0.361766  [  224/  265]
train() client id: f_00001-8-7 loss: 0.458287  [  256/  265]
train() client id: f_00001-9-0 loss: 0.467760  [   32/  265]
train() client id: f_00001-9-1 loss: 0.383048  [   64/  265]
train() client id: f_00001-9-2 loss: 0.588272  [   96/  265]
train() client id: f_00001-9-3 loss: 0.355962  [  128/  265]
train() client id: f_00001-9-4 loss: 0.334069  [  160/  265]
train() client id: f_00001-9-5 loss: 0.435403  [  192/  265]
train() client id: f_00001-9-6 loss: 0.468920  [  224/  265]
train() client id: f_00001-9-7 loss: 0.447016  [  256/  265]
train() client id: f_00001-10-0 loss: 0.421179  [   32/  265]
train() client id: f_00001-10-1 loss: 0.480139  [   64/  265]
train() client id: f_00001-10-2 loss: 0.463194  [   96/  265]
train() client id: f_00001-10-3 loss: 0.498798  [  128/  265]
train() client id: f_00001-10-4 loss: 0.538177  [  160/  265]
train() client id: f_00001-10-5 loss: 0.397041  [  192/  265]
train() client id: f_00001-10-6 loss: 0.348759  [  224/  265]
train() client id: f_00001-10-7 loss: 0.378496  [  256/  265]
train() client id: f_00001-11-0 loss: 0.357019  [   32/  265]
train() client id: f_00001-11-1 loss: 0.449543  [   64/  265]
train() client id: f_00001-11-2 loss: 0.341560  [   96/  265]
train() client id: f_00001-11-3 loss: 0.503404  [  128/  265]
train() client id: f_00001-11-4 loss: 0.457973  [  160/  265]
train() client id: f_00001-11-5 loss: 0.433366  [  192/  265]
train() client id: f_00001-11-6 loss: 0.580323  [  224/  265]
train() client id: f_00001-11-7 loss: 0.400416  [  256/  265]
train() client id: f_00002-0-0 loss: 1.252460  [   32/  124]
train() client id: f_00002-0-1 loss: 1.131797  [   64/  124]
train() client id: f_00002-0-2 loss: 1.360873  [   96/  124]
train() client id: f_00002-1-0 loss: 1.161549  [   32/  124]
train() client id: f_00002-1-1 loss: 1.302034  [   64/  124]
train() client id: f_00002-1-2 loss: 1.035955  [   96/  124]
train() client id: f_00002-2-0 loss: 1.088353  [   32/  124]
train() client id: f_00002-2-1 loss: 1.158013  [   64/  124]
train() client id: f_00002-2-2 loss: 1.198365  [   96/  124]
train() client id: f_00002-3-0 loss: 1.130378  [   32/  124]
train() client id: f_00002-3-1 loss: 1.129490  [   64/  124]
train() client id: f_00002-3-2 loss: 1.104199  [   96/  124]
train() client id: f_00002-4-0 loss: 1.152265  [   32/  124]
train() client id: f_00002-4-1 loss: 1.075390  [   64/  124]
train() client id: f_00002-4-2 loss: 1.052437  [   96/  124]
train() client id: f_00002-5-0 loss: 1.075926  [   32/  124]
train() client id: f_00002-5-1 loss: 1.139616  [   64/  124]
train() client id: f_00002-5-2 loss: 1.093167  [   96/  124]
train() client id: f_00002-6-0 loss: 1.125359  [   32/  124]
train() client id: f_00002-6-1 loss: 0.994143  [   64/  124]
train() client id: f_00002-6-2 loss: 1.133399  [   96/  124]
train() client id: f_00002-7-0 loss: 1.144308  [   32/  124]
train() client id: f_00002-7-1 loss: 0.960347  [   64/  124]
train() client id: f_00002-7-2 loss: 0.988756  [   96/  124]
train() client id: f_00002-8-0 loss: 0.948251  [   32/  124]
train() client id: f_00002-8-1 loss: 1.078099  [   64/  124]
train() client id: f_00002-8-2 loss: 1.028783  [   96/  124]
train() client id: f_00002-9-0 loss: 1.079717  [   32/  124]
train() client id: f_00002-9-1 loss: 0.964538  [   64/  124]
train() client id: f_00002-9-2 loss: 1.038057  [   96/  124]
train() client id: f_00002-10-0 loss: 1.173623  [   32/  124]
train() client id: f_00002-10-1 loss: 0.997564  [   64/  124]
train() client id: f_00002-10-2 loss: 0.979355  [   96/  124]
train() client id: f_00002-11-0 loss: 1.034013  [   32/  124]
train() client id: f_00002-11-1 loss: 0.999688  [   64/  124]
train() client id: f_00002-11-2 loss: 0.915592  [   96/  124]
train() client id: f_00003-0-0 loss: 0.842932  [   32/   43]
train() client id: f_00003-1-0 loss: 0.853119  [   32/   43]
train() client id: f_00003-2-0 loss: 0.850443  [   32/   43]
train() client id: f_00003-3-0 loss: 0.891114  [   32/   43]
train() client id: f_00003-4-0 loss: 0.874219  [   32/   43]
train() client id: f_00003-5-0 loss: 0.760667  [   32/   43]
train() client id: f_00003-6-0 loss: 0.826565  [   32/   43]
train() client id: f_00003-7-0 loss: 0.923028  [   32/   43]
train() client id: f_00003-8-0 loss: 0.883639  [   32/   43]
train() client id: f_00003-9-0 loss: 0.851436  [   32/   43]
train() client id: f_00003-10-0 loss: 0.858717  [   32/   43]
train() client id: f_00003-11-0 loss: 0.923384  [   32/   43]
train() client id: f_00004-0-0 loss: 0.854359  [   32/  306]
train() client id: f_00004-0-1 loss: 0.858344  [   64/  306]
train() client id: f_00004-0-2 loss: 0.880038  [   96/  306]
train() client id: f_00004-0-3 loss: 0.843052  [  128/  306]
train() client id: f_00004-0-4 loss: 0.887534  [  160/  306]
train() client id: f_00004-0-5 loss: 0.957827  [  192/  306]
train() client id: f_00004-0-6 loss: 0.879446  [  224/  306]
train() client id: f_00004-0-7 loss: 0.915148  [  256/  306]
train() client id: f_00004-0-8 loss: 0.977879  [  288/  306]
train() client id: f_00004-1-0 loss: 0.922483  [   32/  306]
train() client id: f_00004-1-1 loss: 0.898715  [   64/  306]
train() client id: f_00004-1-2 loss: 1.069401  [   96/  306]
train() client id: f_00004-1-3 loss: 0.834557  [  128/  306]
train() client id: f_00004-1-4 loss: 0.740076  [  160/  306]
train() client id: f_00004-1-5 loss: 0.893974  [  192/  306]
train() client id: f_00004-1-6 loss: 0.912340  [  224/  306]
train() client id: f_00004-1-7 loss: 0.916035  [  256/  306]
train() client id: f_00004-1-8 loss: 0.901169  [  288/  306]
train() client id: f_00004-2-0 loss: 0.919934  [   32/  306]
train() client id: f_00004-2-1 loss: 0.765233  [   64/  306]
train() client id: f_00004-2-2 loss: 1.040070  [   96/  306]
train() client id: f_00004-2-3 loss: 0.923198  [  128/  306]
train() client id: f_00004-2-4 loss: 0.936706  [  160/  306]
train() client id: f_00004-2-5 loss: 0.836021  [  192/  306]
train() client id: f_00004-2-6 loss: 0.923225  [  224/  306]
train() client id: f_00004-2-7 loss: 0.922137  [  256/  306]
train() client id: f_00004-2-8 loss: 0.825880  [  288/  306]
train() client id: f_00004-3-0 loss: 0.937485  [   32/  306]
train() client id: f_00004-3-1 loss: 0.987996  [   64/  306]
train() client id: f_00004-3-2 loss: 0.881564  [   96/  306]
train() client id: f_00004-3-3 loss: 0.781164  [  128/  306]
train() client id: f_00004-3-4 loss: 0.941491  [  160/  306]
train() client id: f_00004-3-5 loss: 0.883889  [  192/  306]
train() client id: f_00004-3-6 loss: 0.952281  [  224/  306]
train() client id: f_00004-3-7 loss: 0.934908  [  256/  306]
train() client id: f_00004-3-8 loss: 0.825768  [  288/  306]
train() client id: f_00004-4-0 loss: 0.862077  [   32/  306]
train() client id: f_00004-4-1 loss: 0.907203  [   64/  306]
train() client id: f_00004-4-2 loss: 0.847679  [   96/  306]
train() client id: f_00004-4-3 loss: 0.922561  [  128/  306]
train() client id: f_00004-4-4 loss: 0.963231  [  160/  306]
train() client id: f_00004-4-5 loss: 0.891999  [  192/  306]
train() client id: f_00004-4-6 loss: 0.967964  [  224/  306]
train() client id: f_00004-4-7 loss: 0.857646  [  256/  306]
train() client id: f_00004-4-8 loss: 0.880779  [  288/  306]
train() client id: f_00004-5-0 loss: 0.851607  [   32/  306]
train() client id: f_00004-5-1 loss: 0.790295  [   64/  306]
train() client id: f_00004-5-2 loss: 0.936667  [   96/  306]
train() client id: f_00004-5-3 loss: 0.865628  [  128/  306]
train() client id: f_00004-5-4 loss: 0.916128  [  160/  306]
train() client id: f_00004-5-5 loss: 0.889904  [  192/  306]
train() client id: f_00004-5-6 loss: 0.934726  [  224/  306]
train() client id: f_00004-5-7 loss: 1.025633  [  256/  306]
train() client id: f_00004-5-8 loss: 0.909816  [  288/  306]
train() client id: f_00004-6-0 loss: 0.968034  [   32/  306]
train() client id: f_00004-6-1 loss: 0.920935  [   64/  306]
train() client id: f_00004-6-2 loss: 0.892883  [   96/  306]
train() client id: f_00004-6-3 loss: 0.854706  [  128/  306]
train() client id: f_00004-6-4 loss: 0.939701  [  160/  306]
train() client id: f_00004-6-5 loss: 0.937193  [  192/  306]
train() client id: f_00004-6-6 loss: 0.803830  [  224/  306]
train() client id: f_00004-6-7 loss: 0.930997  [  256/  306]
train() client id: f_00004-6-8 loss: 0.963910  [  288/  306]
train() client id: f_00004-7-0 loss: 0.927163  [   32/  306]
train() client id: f_00004-7-1 loss: 0.891686  [   64/  306]
train() client id: f_00004-7-2 loss: 0.943259  [   96/  306]
train() client id: f_00004-7-3 loss: 0.839638  [  128/  306]
train() client id: f_00004-7-4 loss: 1.004155  [  160/  306]
train() client id: f_00004-7-5 loss: 0.944882  [  192/  306]
train() client id: f_00004-7-6 loss: 0.828666  [  224/  306]
train() client id: f_00004-7-7 loss: 0.891002  [  256/  306]
train() client id: f_00004-7-8 loss: 0.877910  [  288/  306]
train() client id: f_00004-8-0 loss: 0.979772  [   32/  306]
train() client id: f_00004-8-1 loss: 0.950380  [   64/  306]
train() client id: f_00004-8-2 loss: 0.919783  [   96/  306]
train() client id: f_00004-8-3 loss: 0.940563  [  128/  306]
train() client id: f_00004-8-4 loss: 0.899786  [  160/  306]
train() client id: f_00004-8-5 loss: 0.785568  [  192/  306]
train() client id: f_00004-8-6 loss: 0.979278  [  224/  306]
train() client id: f_00004-8-7 loss: 0.830964  [  256/  306]
train() client id: f_00004-8-8 loss: 0.860607  [  288/  306]
train() client id: f_00004-9-0 loss: 0.933999  [   32/  306]
train() client id: f_00004-9-1 loss: 0.910215  [   64/  306]
train() client id: f_00004-9-2 loss: 0.901754  [   96/  306]
train() client id: f_00004-9-3 loss: 0.912265  [  128/  306]
train() client id: f_00004-9-4 loss: 0.890165  [  160/  306]
train() client id: f_00004-9-5 loss: 0.836819  [  192/  306]
train() client id: f_00004-9-6 loss: 0.938307  [  224/  306]
train() client id: f_00004-9-7 loss: 0.949849  [  256/  306]
train() client id: f_00004-9-8 loss: 0.881068  [  288/  306]
train() client id: f_00004-10-0 loss: 0.981210  [   32/  306]
train() client id: f_00004-10-1 loss: 0.938769  [   64/  306]
train() client id: f_00004-10-2 loss: 0.896419  [   96/  306]
train() client id: f_00004-10-3 loss: 0.825556  [  128/  306]
train() client id: f_00004-10-4 loss: 0.835571  [  160/  306]
train() client id: f_00004-10-5 loss: 0.893673  [  192/  306]
train() client id: f_00004-10-6 loss: 1.038737  [  224/  306]
train() client id: f_00004-10-7 loss: 0.846566  [  256/  306]
train() client id: f_00004-10-8 loss: 0.876559  [  288/  306]
train() client id: f_00004-11-0 loss: 0.932427  [   32/  306]
train() client id: f_00004-11-1 loss: 0.912934  [   64/  306]
train() client id: f_00004-11-2 loss: 0.875500  [   96/  306]
train() client id: f_00004-11-3 loss: 0.884186  [  128/  306]
train() client id: f_00004-11-4 loss: 0.844712  [  160/  306]
train() client id: f_00004-11-5 loss: 0.961032  [  192/  306]
train() client id: f_00004-11-6 loss: 0.937336  [  224/  306]
train() client id: f_00004-11-7 loss: 0.909342  [  256/  306]
train() client id: f_00004-11-8 loss: 0.900387  [  288/  306]
train() client id: f_00005-0-0 loss: 0.824497  [   32/  146]
train() client id: f_00005-0-1 loss: 0.742722  [   64/  146]
train() client id: f_00005-0-2 loss: 0.993051  [   96/  146]
train() client id: f_00005-0-3 loss: 0.749524  [  128/  146]
train() client id: f_00005-1-0 loss: 0.804762  [   32/  146]
train() client id: f_00005-1-1 loss: 0.725455  [   64/  146]
train() client id: f_00005-1-2 loss: 1.011695  [   96/  146]
train() client id: f_00005-1-3 loss: 0.867591  [  128/  146]
train() client id: f_00005-2-0 loss: 0.747895  [   32/  146]
train() client id: f_00005-2-1 loss: 0.757099  [   64/  146]
train() client id: f_00005-2-2 loss: 0.805972  [   96/  146]
train() client id: f_00005-2-3 loss: 0.791282  [  128/  146]
train() client id: f_00005-3-0 loss: 0.865952  [   32/  146]
train() client id: f_00005-3-1 loss: 1.001626  [   64/  146]
train() client id: f_00005-3-2 loss: 0.841286  [   96/  146]
train() client id: f_00005-3-3 loss: 0.733385  [  128/  146]
train() client id: f_00005-4-0 loss: 0.706420  [   32/  146]
train() client id: f_00005-4-1 loss: 0.845297  [   64/  146]
train() client id: f_00005-4-2 loss: 0.805977  [   96/  146]
train() client id: f_00005-4-3 loss: 0.918597  [  128/  146]
train() client id: f_00005-5-0 loss: 0.700311  [   32/  146]
train() client id: f_00005-5-1 loss: 0.927844  [   64/  146]
train() client id: f_00005-5-2 loss: 0.848639  [   96/  146]
train() client id: f_00005-5-3 loss: 0.775007  [  128/  146]
train() client id: f_00005-6-0 loss: 0.781053  [   32/  146]
train() client id: f_00005-6-1 loss: 0.966869  [   64/  146]
train() client id: f_00005-6-2 loss: 0.925208  [   96/  146]
train() client id: f_00005-6-3 loss: 0.643942  [  128/  146]
train() client id: f_00005-7-0 loss: 0.894137  [   32/  146]
train() client id: f_00005-7-1 loss: 0.854835  [   64/  146]
train() client id: f_00005-7-2 loss: 0.756761  [   96/  146]
train() client id: f_00005-7-3 loss: 0.677518  [  128/  146]
train() client id: f_00005-8-0 loss: 0.713090  [   32/  146]
train() client id: f_00005-8-1 loss: 0.858657  [   64/  146]
train() client id: f_00005-8-2 loss: 0.635860  [   96/  146]
train() client id: f_00005-8-3 loss: 0.957751  [  128/  146]
train() client id: f_00005-9-0 loss: 0.835060  [   32/  146]
train() client id: f_00005-9-1 loss: 0.746980  [   64/  146]
train() client id: f_00005-9-2 loss: 0.831234  [   96/  146]
train() client id: f_00005-9-3 loss: 0.995876  [  128/  146]
train() client id: f_00005-10-0 loss: 0.682268  [   32/  146]
train() client id: f_00005-10-1 loss: 0.926290  [   64/  146]
train() client id: f_00005-10-2 loss: 0.888797  [   96/  146]
train() client id: f_00005-10-3 loss: 0.863608  [  128/  146]
train() client id: f_00005-11-0 loss: 0.800774  [   32/  146]
train() client id: f_00005-11-1 loss: 0.883340  [   64/  146]
train() client id: f_00005-11-2 loss: 0.778527  [   96/  146]
train() client id: f_00005-11-3 loss: 0.723072  [  128/  146]
train() client id: f_00006-0-0 loss: 0.787993  [   32/   54]
train() client id: f_00006-1-0 loss: 0.780154  [   32/   54]
train() client id: f_00006-2-0 loss: 0.729795  [   32/   54]
train() client id: f_00006-3-0 loss: 0.680096  [   32/   54]
train() client id: f_00006-4-0 loss: 0.722608  [   32/   54]
train() client id: f_00006-5-0 loss: 0.785386  [   32/   54]
train() client id: f_00006-6-0 loss: 0.783712  [   32/   54]
train() client id: f_00006-7-0 loss: 0.738835  [   32/   54]
train() client id: f_00006-8-0 loss: 0.734385  [   32/   54]
train() client id: f_00006-9-0 loss: 0.788281  [   32/   54]
train() client id: f_00006-10-0 loss: 0.733407  [   32/   54]
train() client id: f_00006-11-0 loss: 0.735753  [   32/   54]
train() client id: f_00007-0-0 loss: 0.715466  [   32/  179]
train() client id: f_00007-0-1 loss: 0.616057  [   64/  179]
train() client id: f_00007-0-2 loss: 0.793325  [   96/  179]
train() client id: f_00007-0-3 loss: 0.656724  [  128/  179]
train() client id: f_00007-0-4 loss: 0.635599  [  160/  179]
train() client id: f_00007-1-0 loss: 0.709575  [   32/  179]
train() client id: f_00007-1-1 loss: 0.697068  [   64/  179]
train() client id: f_00007-1-2 loss: 0.649862  [   96/  179]
train() client id: f_00007-1-3 loss: 0.639406  [  128/  179]
train() client id: f_00007-1-4 loss: 0.650496  [  160/  179]
train() client id: f_00007-2-0 loss: 0.544686  [   32/  179]
train() client id: f_00007-2-1 loss: 0.723847  [   64/  179]
train() client id: f_00007-2-2 loss: 0.702505  [   96/  179]
train() client id: f_00007-2-3 loss: 0.709794  [  128/  179]
train() client id: f_00007-2-4 loss: 0.528802  [  160/  179]
train() client id: f_00007-3-0 loss: 0.626329  [   32/  179]
train() client id: f_00007-3-1 loss: 0.671738  [   64/  179]
train() client id: f_00007-3-2 loss: 0.602463  [   96/  179]
train() client id: f_00007-3-3 loss: 0.540906  [  128/  179]
train() client id: f_00007-3-4 loss: 0.736802  [  160/  179]
train() client id: f_00007-4-0 loss: 0.662289  [   32/  179]
train() client id: f_00007-4-1 loss: 0.596613  [   64/  179]
train() client id: f_00007-4-2 loss: 0.681787  [   96/  179]
train() client id: f_00007-4-3 loss: 0.599931  [  128/  179]
train() client id: f_00007-4-4 loss: 0.566240  [  160/  179]
train() client id: f_00007-5-0 loss: 0.691891  [   32/  179]
train() client id: f_00007-5-1 loss: 0.672679  [   64/  179]
train() client id: f_00007-5-2 loss: 0.565950  [   96/  179]
train() client id: f_00007-5-3 loss: 0.642956  [  128/  179]
train() client id: f_00007-5-4 loss: 0.524442  [  160/  179]
train() client id: f_00007-6-0 loss: 0.669164  [   32/  179]
train() client id: f_00007-6-1 loss: 0.591278  [   64/  179]
train() client id: f_00007-6-2 loss: 0.606956  [   96/  179]
train() client id: f_00007-6-3 loss: 0.581012  [  128/  179]
train() client id: f_00007-6-4 loss: 0.734151  [  160/  179]
train() client id: f_00007-7-0 loss: 0.590468  [   32/  179]
train() client id: f_00007-7-1 loss: 0.618114  [   64/  179]
train() client id: f_00007-7-2 loss: 0.658667  [   96/  179]
train() client id: f_00007-7-3 loss: 0.719595  [  128/  179]
train() client id: f_00007-7-4 loss: 0.498978  [  160/  179]
train() client id: f_00007-8-0 loss: 0.577304  [   32/  179]
train() client id: f_00007-8-1 loss: 0.745844  [   64/  179]
train() client id: f_00007-8-2 loss: 0.566456  [   96/  179]
train() client id: f_00007-8-3 loss: 0.527349  [  128/  179]
train() client id: f_00007-8-4 loss: 0.659612  [  160/  179]
train() client id: f_00007-9-0 loss: 0.611702  [   32/  179]
train() client id: f_00007-9-1 loss: 0.822312  [   64/  179]
train() client id: f_00007-9-2 loss: 0.512521  [   96/  179]
train() client id: f_00007-9-3 loss: 0.565747  [  128/  179]
train() client id: f_00007-9-4 loss: 0.579797  [  160/  179]
train() client id: f_00007-10-0 loss: 0.493019  [   32/  179]
train() client id: f_00007-10-1 loss: 0.575600  [   64/  179]
train() client id: f_00007-10-2 loss: 0.680740  [   96/  179]
train() client id: f_00007-10-3 loss: 0.663671  [  128/  179]
train() client id: f_00007-10-4 loss: 0.662369  [  160/  179]
train() client id: f_00007-11-0 loss: 0.566625  [   32/  179]
train() client id: f_00007-11-1 loss: 0.643039  [   64/  179]
train() client id: f_00007-11-2 loss: 0.595041  [   96/  179]
train() client id: f_00007-11-3 loss: 0.515383  [  128/  179]
train() client id: f_00007-11-4 loss: 0.577251  [  160/  179]
train() client id: f_00008-0-0 loss: 0.938030  [   32/  130]
train() client id: f_00008-0-1 loss: 0.874822  [   64/  130]
train() client id: f_00008-0-2 loss: 0.825439  [   96/  130]
train() client id: f_00008-0-3 loss: 0.761887  [  128/  130]
train() client id: f_00008-1-0 loss: 0.788565  [   32/  130]
train() client id: f_00008-1-1 loss: 0.916774  [   64/  130]
train() client id: f_00008-1-2 loss: 0.738891  [   96/  130]
train() client id: f_00008-1-3 loss: 0.966271  [  128/  130]
train() client id: f_00008-2-0 loss: 0.864879  [   32/  130]
train() client id: f_00008-2-1 loss: 0.809894  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846261  [   96/  130]
train() client id: f_00008-2-3 loss: 0.840987  [  128/  130]
train() client id: f_00008-3-0 loss: 0.824952  [   32/  130]
train() client id: f_00008-3-1 loss: 0.911328  [   64/  130]
train() client id: f_00008-3-2 loss: 0.836158  [   96/  130]
train() client id: f_00008-3-3 loss: 0.799646  [  128/  130]
train() client id: f_00008-4-0 loss: 0.751033  [   32/  130]
train() client id: f_00008-4-1 loss: 0.830810  [   64/  130]
train() client id: f_00008-4-2 loss: 0.957351  [   96/  130]
train() client id: f_00008-4-3 loss: 0.865641  [  128/  130]
train() client id: f_00008-5-0 loss: 0.884179  [   32/  130]
train() client id: f_00008-5-1 loss: 0.819548  [   64/  130]
train() client id: f_00008-5-2 loss: 0.894729  [   96/  130]
train() client id: f_00008-5-3 loss: 0.772151  [  128/  130]
train() client id: f_00008-6-0 loss: 0.985271  [   32/  130]
train() client id: f_00008-6-1 loss: 0.804353  [   64/  130]
train() client id: f_00008-6-2 loss: 0.805534  [   96/  130]
train() client id: f_00008-6-3 loss: 0.777062  [  128/  130]
train() client id: f_00008-7-0 loss: 0.828916  [   32/  130]
train() client id: f_00008-7-1 loss: 0.743880  [   64/  130]
train() client id: f_00008-7-2 loss: 0.850156  [   96/  130]
train() client id: f_00008-7-3 loss: 0.940430  [  128/  130]
train() client id: f_00008-8-0 loss: 0.838038  [   32/  130]
train() client id: f_00008-8-1 loss: 0.861024  [   64/  130]
train() client id: f_00008-8-2 loss: 0.917717  [   96/  130]
train() client id: f_00008-8-3 loss: 0.785087  [  128/  130]
train() client id: f_00008-9-0 loss: 0.789804  [   32/  130]
train() client id: f_00008-9-1 loss: 0.877377  [   64/  130]
train() client id: f_00008-9-2 loss: 0.825679  [   96/  130]
train() client id: f_00008-9-3 loss: 0.911730  [  128/  130]
train() client id: f_00008-10-0 loss: 0.783164  [   32/  130]
train() client id: f_00008-10-1 loss: 0.836558  [   64/  130]
train() client id: f_00008-10-2 loss: 0.884255  [   96/  130]
train() client id: f_00008-10-3 loss: 0.870802  [  128/  130]
train() client id: f_00008-11-0 loss: 0.831552  [   32/  130]
train() client id: f_00008-11-1 loss: 0.831629  [   64/  130]
train() client id: f_00008-11-2 loss: 0.829491  [   96/  130]
train() client id: f_00008-11-3 loss: 0.893312  [  128/  130]
train() client id: f_00009-0-0 loss: 1.184664  [   32/  118]
train() client id: f_00009-0-1 loss: 1.004371  [   64/  118]
train() client id: f_00009-0-2 loss: 1.008887  [   96/  118]
train() client id: f_00009-1-0 loss: 0.964691  [   32/  118]
train() client id: f_00009-1-1 loss: 1.044541  [   64/  118]
train() client id: f_00009-1-2 loss: 0.970542  [   96/  118]
train() client id: f_00009-2-0 loss: 1.052452  [   32/  118]
train() client id: f_00009-2-1 loss: 0.850855  [   64/  118]
train() client id: f_00009-2-2 loss: 0.989550  [   96/  118]
train() client id: f_00009-3-0 loss: 0.909924  [   32/  118]
train() client id: f_00009-3-1 loss: 0.974474  [   64/  118]
train() client id: f_00009-3-2 loss: 0.884855  [   96/  118]
train() client id: f_00009-4-0 loss: 0.902208  [   32/  118]
train() client id: f_00009-4-1 loss: 0.942053  [   64/  118]
train() client id: f_00009-4-2 loss: 0.873049  [   96/  118]
train() client id: f_00009-5-0 loss: 0.989221  [   32/  118]
train() client id: f_00009-5-1 loss: 0.914515  [   64/  118]
train() client id: f_00009-5-2 loss: 0.794336  [   96/  118]
train() client id: f_00009-6-0 loss: 0.907935  [   32/  118]
train() client id: f_00009-6-1 loss: 0.756454  [   64/  118]
train() client id: f_00009-6-2 loss: 0.827601  [   96/  118]
train() client id: f_00009-7-0 loss: 0.866242  [   32/  118]
train() client id: f_00009-7-1 loss: 0.944694  [   64/  118]
train() client id: f_00009-7-2 loss: 0.737677  [   96/  118]
train() client id: f_00009-8-0 loss: 0.745256  [   32/  118]
train() client id: f_00009-8-1 loss: 0.887539  [   64/  118]
train() client id: f_00009-8-2 loss: 0.834619  [   96/  118]
train() client id: f_00009-9-0 loss: 0.902156  [   32/  118]
train() client id: f_00009-9-1 loss: 0.808106  [   64/  118]
train() client id: f_00009-9-2 loss: 0.793210  [   96/  118]
train() client id: f_00009-10-0 loss: 0.716812  [   32/  118]
train() client id: f_00009-10-1 loss: 0.833913  [   64/  118]
train() client id: f_00009-10-2 loss: 0.809837  [   96/  118]
train() client id: f_00009-11-0 loss: 0.786476  [   32/  118]
train() client id: f_00009-11-1 loss: 0.831279  [   64/  118]
train() client id: f_00009-11-2 loss: 0.790545  [   96/  118]
At round 11 accuracy: 0.6312997347480106
At round 11 training accuracy: 0.5694164989939637
At round 11 training loss: 0.8682103709617359
update_location
xs = [ -3.9056584    4.20031788  75.00902392  18.81129433   0.97929623
   3.95640986 -37.44319194 -16.32485185  59.66397685  -2.06087855]
ys = [ 67.5879595   50.55583871   1.32061395 -37.45517586  29.35018685
  12.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [120.76169275 112.13177738 125.01239015 108.42857093 104.22280215
 100.89527064 106.81237365 101.32707952 117.76442632 100.10124413]
dists_bs = [202.0163859  218.17218706 304.41590165 287.5761109  228.43506905
 241.51531283 224.66087888 235.6176078  282.58043084 243.20450851]
uav_gains = [6.23955507e-11 7.51052065e-11 5.72241493e-11 8.16837356e-11
 9.01758519e-11 9.77960164e-11 8.48090092e-11 9.67574158e-11
 6.64428547e-11 9.97469659e-11]
bs_gains = [3.87446385e-11 3.12360282e-11 1.22909527e-11 1.44140568e-11
 2.74636534e-11 2.34989796e-11 2.87751248e-11 2.51832865e-11
 1.51389674e-11 2.30448319e-11]
Round 12
-------------------------------
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.40743039 19.62699225  9.26366007  3.31236285 22.63944362 10.91364539
  4.11791779 13.28077427  9.76466884  8.86129599]
obj_prev = 111.18819146740302
eta_min = 1.3309903798194387e-10	eta_max = 0.9206473861818304
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 25.855891314518058	eta = 0.9090909090909091
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 44.77490344422677	eta = 0.5249672010962586
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 35.73954375539702	eta = 0.6576848294802706
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.11938254858583	eta = 0.6889150384535667
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039315064938776	eta = 0.6905355085914164
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039105597683566	eta = 0.6905397579562299
eta = 0.6905397579562299
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.03055476 0.064262   0.03006976 0.01042742 0.07420442 0.03540472
 0.0130949  0.04340716 0.03152475 0.02861478]
ene_total = [2.91251884 5.6185508  2.88405621 1.31991488 6.41048895 3.41632264
 1.52386453 3.86970026 3.19241569 2.89127279]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 0 obj = 5.96397047673148
eta = 0.6905397579562299
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
eta_min = 0.6905397579563017	eta_max = 0.6905397579562269
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 0.04594401078389732	eta = 0.909090909090909
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 20.175396261636216	eta = 0.002070208782478131
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0926044394311156	eta = 0.019959473345172857
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.034227485351597	eta = 0.020532257494100595
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0342105771628316	eta = 0.02053242815651352
eta = 0.02053242815651352
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.95847449e-04 1.95635877e-03 1.88115503e-04 7.61251424e-06
 3.06087387e-03 3.39489881e-04 1.50327361e-05 5.42150909e-04
 2.56937143e-04 1.79723710e-04]
ene_total = [0.17408175 0.24390138 0.17681862 0.16093954 0.27690584 0.21759158
 0.16000151 0.16914252 0.24017663 0.21465121]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 1 obj = 5.963970476732851
eta = 0.6905397579563017
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
Done!
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.84704838e-05 1.84505302e-04 1.77412796e-05 7.17940527e-07
 2.88672747e-04 3.20174828e-05 1.41774584e-06 5.11305590e-05
 2.42318873e-05 1.69498448e-05]
ene_total = [0.00690583 0.00815225 0.0070242  0.00654157 0.00849482 0.00854611
 0.00649669 0.00639122 0.00953984 0.00857117]
At round 12 energy consumption: 0.07666369174488584
At round 12 eta: 0.6905397579563017
At round 12 a_n: 24.072052692622567
At round 12 local rounds: 12.124905073660063
At round 12 global rounds: 77.78722246725123
gradient difference: 0.3905566334724426
train() client id: f_00000-0-0 loss: 1.470991  [   32/  126]
train() client id: f_00000-0-1 loss: 1.337248  [   64/  126]
train() client id: f_00000-0-2 loss: 1.357089  [   96/  126]
train() client id: f_00000-1-0 loss: 1.276042  [   32/  126]
train() client id: f_00000-1-1 loss: 1.240027  [   64/  126]
train() client id: f_00000-1-2 loss: 1.254412  [   96/  126]
train() client id: f_00000-2-0 loss: 1.237386  [   32/  126]
train() client id: f_00000-2-1 loss: 1.147232  [   64/  126]
train() client id: f_00000-2-2 loss: 1.039852  [   96/  126]
train() client id: f_00000-3-0 loss: 1.110543  [   32/  126]
train() client id: f_00000-3-1 loss: 1.086053  [   64/  126]
train() client id: f_00000-3-2 loss: 1.074431  [   96/  126]
train() client id: f_00000-4-0 loss: 0.990802  [   32/  126]
train() client id: f_00000-4-1 loss: 1.052462  [   64/  126]
train() client id: f_00000-4-2 loss: 1.030469  [   96/  126]
train() client id: f_00000-5-0 loss: 1.027928  [   32/  126]
train() client id: f_00000-5-1 loss: 0.974573  [   64/  126]
train() client id: f_00000-5-2 loss: 1.022678  [   96/  126]
train() client id: f_00000-6-0 loss: 0.970427  [   32/  126]
train() client id: f_00000-6-1 loss: 0.984716  [   64/  126]
train() client id: f_00000-6-2 loss: 0.911493  [   96/  126]
train() client id: f_00000-7-0 loss: 0.916368  [   32/  126]
train() client id: f_00000-7-1 loss: 0.901436  [   64/  126]
train() client id: f_00000-7-2 loss: 0.944647  [   96/  126]
train() client id: f_00000-8-0 loss: 1.012481  [   32/  126]
train() client id: f_00000-8-1 loss: 0.849766  [   64/  126]
train() client id: f_00000-8-2 loss: 0.907768  [   96/  126]
train() client id: f_00000-9-0 loss: 0.928342  [   32/  126]
train() client id: f_00000-9-1 loss: 0.875032  [   64/  126]
train() client id: f_00000-9-2 loss: 0.909844  [   96/  126]
train() client id: f_00000-10-0 loss: 0.786632  [   32/  126]
train() client id: f_00000-10-1 loss: 0.964778  [   64/  126]
train() client id: f_00000-10-2 loss: 0.909171  [   96/  126]
train() client id: f_00000-11-0 loss: 0.833323  [   32/  126]
train() client id: f_00000-11-1 loss: 0.948716  [   64/  126]
train() client id: f_00000-11-2 loss: 0.858306  [   96/  126]
train() client id: f_00001-0-0 loss: 0.556339  [   32/  265]
train() client id: f_00001-0-1 loss: 0.659564  [   64/  265]
train() client id: f_00001-0-2 loss: 0.520453  [   96/  265]
train() client id: f_00001-0-3 loss: 0.531796  [  128/  265]
train() client id: f_00001-0-4 loss: 0.508884  [  160/  265]
train() client id: f_00001-0-5 loss: 0.590412  [  192/  265]
train() client id: f_00001-0-6 loss: 0.518871  [  224/  265]
train() client id: f_00001-0-7 loss: 0.460118  [  256/  265]
train() client id: f_00001-1-0 loss: 0.511361  [   32/  265]
train() client id: f_00001-1-1 loss: 0.510572  [   64/  265]
train() client id: f_00001-1-2 loss: 0.607775  [   96/  265]
train() client id: f_00001-1-3 loss: 0.562947  [  128/  265]
train() client id: f_00001-1-4 loss: 0.497325  [  160/  265]
train() client id: f_00001-1-5 loss: 0.548501  [  192/  265]
train() client id: f_00001-1-6 loss: 0.498035  [  224/  265]
train() client id: f_00001-1-7 loss: 0.524723  [  256/  265]
train() client id: f_00001-2-0 loss: 0.450762  [   32/  265]
train() client id: f_00001-2-1 loss: 0.516418  [   64/  265]
train() client id: f_00001-2-2 loss: 0.539042  [   96/  265]
train() client id: f_00001-2-3 loss: 0.616000  [  128/  265]
train() client id: f_00001-2-4 loss: 0.438645  [  160/  265]
train() client id: f_00001-2-5 loss: 0.650011  [  192/  265]
train() client id: f_00001-2-6 loss: 0.461720  [  224/  265]
train() client id: f_00001-2-7 loss: 0.525596  [  256/  265]
train() client id: f_00001-3-0 loss: 0.452434  [   32/  265]
train() client id: f_00001-3-1 loss: 0.576631  [   64/  265]
train() client id: f_00001-3-2 loss: 0.525918  [   96/  265]
train() client id: f_00001-3-3 loss: 0.599696  [  128/  265]
train() client id: f_00001-3-4 loss: 0.490928  [  160/  265]
train() client id: f_00001-3-5 loss: 0.436822  [  192/  265]
train() client id: f_00001-3-6 loss: 0.443584  [  224/  265]
train() client id: f_00001-3-7 loss: 0.585496  [  256/  265]
train() client id: f_00001-4-0 loss: 0.554646  [   32/  265]
train() client id: f_00001-4-1 loss: 0.442373  [   64/  265]
train() client id: f_00001-4-2 loss: 0.527885  [   96/  265]
train() client id: f_00001-4-3 loss: 0.463813  [  128/  265]
train() client id: f_00001-4-4 loss: 0.495272  [  160/  265]
train() client id: f_00001-4-5 loss: 0.539488  [  192/  265]
train() client id: f_00001-4-6 loss: 0.435371  [  224/  265]
train() client id: f_00001-4-7 loss: 0.656909  [  256/  265]
train() client id: f_00001-5-0 loss: 0.601362  [   32/  265]
train() client id: f_00001-5-1 loss: 0.543943  [   64/  265]
train() client id: f_00001-5-2 loss: 0.560917  [   96/  265]
train() client id: f_00001-5-3 loss: 0.505915  [  128/  265]
train() client id: f_00001-5-4 loss: 0.530263  [  160/  265]
train() client id: f_00001-5-5 loss: 0.480845  [  192/  265]
train() client id: f_00001-5-6 loss: 0.435532  [  224/  265]
train() client id: f_00001-5-7 loss: 0.433033  [  256/  265]
train() client id: f_00001-6-0 loss: 0.606277  [   32/  265]
train() client id: f_00001-6-1 loss: 0.556833  [   64/  265]
train() client id: f_00001-6-2 loss: 0.468503  [   96/  265]
train() client id: f_00001-6-3 loss: 0.606964  [  128/  265]
train() client id: f_00001-6-4 loss: 0.474322  [  160/  265]
train() client id: f_00001-6-5 loss: 0.410799  [  192/  265]
train() client id: f_00001-6-6 loss: 0.522225  [  224/  265]
train() client id: f_00001-6-7 loss: 0.431581  [  256/  265]
train() client id: f_00001-7-0 loss: 0.411297  [   32/  265]
train() client id: f_00001-7-1 loss: 0.421414  [   64/  265]
train() client id: f_00001-7-2 loss: 0.645656  [   96/  265]
train() client id: f_00001-7-3 loss: 0.498782  [  128/  265]
train() client id: f_00001-7-4 loss: 0.489872  [  160/  265]
train() client id: f_00001-7-5 loss: 0.509314  [  192/  265]
train() client id: f_00001-7-6 loss: 0.475472  [  224/  265]
train() client id: f_00001-7-7 loss: 0.614995  [  256/  265]
train() client id: f_00001-8-0 loss: 0.491252  [   32/  265]
train() client id: f_00001-8-1 loss: 0.516360  [   64/  265]
train() client id: f_00001-8-2 loss: 0.475888  [   96/  265]
train() client id: f_00001-8-3 loss: 0.552710  [  128/  265]
train() client id: f_00001-8-4 loss: 0.421342  [  160/  265]
train() client id: f_00001-8-5 loss: 0.558482  [  192/  265]
train() client id: f_00001-8-6 loss: 0.477373  [  224/  265]
train() client id: f_00001-8-7 loss: 0.481841  [  256/  265]
train() client id: f_00001-9-0 loss: 0.532581  [   32/  265]
train() client id: f_00001-9-1 loss: 0.527739  [   64/  265]
train() client id: f_00001-9-2 loss: 0.568658  [   96/  265]
train() client id: f_00001-9-3 loss: 0.521157  [  128/  265]
train() client id: f_00001-9-4 loss: 0.421758  [  160/  265]
train() client id: f_00001-9-5 loss: 0.545298  [  192/  265]
train() client id: f_00001-9-6 loss: 0.465778  [  224/  265]
train() client id: f_00001-9-7 loss: 0.479367  [  256/  265]
train() client id: f_00001-10-0 loss: 0.438221  [   32/  265]
train() client id: f_00001-10-1 loss: 0.562119  [   64/  265]
train() client id: f_00001-10-2 loss: 0.465709  [   96/  265]
train() client id: f_00001-10-3 loss: 0.502836  [  128/  265]
train() client id: f_00001-10-4 loss: 0.519699  [  160/  265]
train() client id: f_00001-10-5 loss: 0.488987  [  192/  265]
train() client id: f_00001-10-6 loss: 0.521612  [  224/  265]
train() client id: f_00001-10-7 loss: 0.506546  [  256/  265]
train() client id: f_00001-11-0 loss: 0.566032  [   32/  265]
train() client id: f_00001-11-1 loss: 0.546817  [   64/  265]
train() client id: f_00001-11-2 loss: 0.467672  [   96/  265]
train() client id: f_00001-11-3 loss: 0.420695  [  128/  265]
train() client id: f_00001-11-4 loss: 0.469605  [  160/  265]
train() client id: f_00001-11-5 loss: 0.480036  [  192/  265]
train() client id: f_00001-11-6 loss: 0.575427  [  224/  265]
train() client id: f_00001-11-7 loss: 0.538230  [  256/  265]
train() client id: f_00002-0-0 loss: 1.301841  [   32/  124]
train() client id: f_00002-0-1 loss: 1.344143  [   64/  124]
train() client id: f_00002-0-2 loss: 1.274428  [   96/  124]
train() client id: f_00002-1-0 loss: 1.184009  [   32/  124]
train() client id: f_00002-1-1 loss: 1.247239  [   64/  124]
train() client id: f_00002-1-2 loss: 1.312510  [   96/  124]
train() client id: f_00002-2-0 loss: 1.245898  [   32/  124]
train() client id: f_00002-2-1 loss: 1.297270  [   64/  124]
train() client id: f_00002-2-2 loss: 1.234170  [   96/  124]
train() client id: f_00002-3-0 loss: 1.213202  [   32/  124]
train() client id: f_00002-3-1 loss: 1.191967  [   64/  124]
train() client id: f_00002-3-2 loss: 1.126192  [   96/  124]
train() client id: f_00002-4-0 loss: 1.114963  [   32/  124]
train() client id: f_00002-4-1 loss: 1.207171  [   64/  124]
train() client id: f_00002-4-2 loss: 1.144742  [   96/  124]
train() client id: f_00002-5-0 loss: 1.146563  [   32/  124]
train() client id: f_00002-5-1 loss: 1.210037  [   64/  124]
train() client id: f_00002-5-2 loss: 1.128555  [   96/  124]
train() client id: f_00002-6-0 loss: 1.191803  [   32/  124]
train() client id: f_00002-6-1 loss: 0.991118  [   64/  124]
train() client id: f_00002-6-2 loss: 1.194675  [   96/  124]
train() client id: f_00002-7-0 loss: 1.116573  [   32/  124]
train() client id: f_00002-7-1 loss: 1.177183  [   64/  124]
train() client id: f_00002-7-2 loss: 0.962412  [   96/  124]
train() client id: f_00002-8-0 loss: 1.235113  [   32/  124]
train() client id: f_00002-8-1 loss: 1.119639  [   64/  124]
train() client id: f_00002-8-2 loss: 0.992226  [   96/  124]
train() client id: f_00002-9-0 loss: 1.158730  [   32/  124]
train() client id: f_00002-9-1 loss: 1.124930  [   64/  124]
train() client id: f_00002-9-2 loss: 1.043333  [   96/  124]
train() client id: f_00002-10-0 loss: 1.183442  [   32/  124]
train() client id: f_00002-10-1 loss: 0.967125  [   64/  124]
train() client id: f_00002-10-2 loss: 1.200817  [   96/  124]
train() client id: f_00002-11-0 loss: 1.092482  [   32/  124]
train() client id: f_00002-11-1 loss: 1.092086  [   64/  124]
train() client id: f_00002-11-2 loss: 1.071724  [   96/  124]
train() client id: f_00003-0-0 loss: 0.905294  [   32/   43]
train() client id: f_00003-1-0 loss: 0.838642  [   32/   43]
train() client id: f_00003-2-0 loss: 0.819999  [   32/   43]
train() client id: f_00003-3-0 loss: 0.703556  [   32/   43]
train() client id: f_00003-4-0 loss: 0.809335  [   32/   43]
train() client id: f_00003-5-0 loss: 0.918564  [   32/   43]
train() client id: f_00003-6-0 loss: 0.837780  [   32/   43]
train() client id: f_00003-7-0 loss: 0.787548  [   32/   43]
train() client id: f_00003-8-0 loss: 0.855421  [   32/   43]
train() client id: f_00003-9-0 loss: 0.796675  [   32/   43]
train() client id: f_00003-10-0 loss: 0.803914  [   32/   43]
train() client id: f_00003-11-0 loss: 0.803416  [   32/   43]
train() client id: f_00004-0-0 loss: 0.862460  [   32/  306]
train() client id: f_00004-0-1 loss: 0.904821  [   64/  306]
train() client id: f_00004-0-2 loss: 0.923722  [   96/  306]
train() client id: f_00004-0-3 loss: 1.110809  [  128/  306]
train() client id: f_00004-0-4 loss: 0.838476  [  160/  306]
train() client id: f_00004-0-5 loss: 0.828013  [  192/  306]
train() client id: f_00004-0-6 loss: 0.849196  [  224/  306]
train() client id: f_00004-0-7 loss: 0.970695  [  256/  306]
train() client id: f_00004-0-8 loss: 0.943895  [  288/  306]
train() client id: f_00004-1-0 loss: 1.082648  [   32/  306]
train() client id: f_00004-1-1 loss: 1.022470  [   64/  306]
train() client id: f_00004-1-2 loss: 0.859896  [   96/  306]
train() client id: f_00004-1-3 loss: 0.899919  [  128/  306]
train() client id: f_00004-1-4 loss: 0.839068  [  160/  306]
train() client id: f_00004-1-5 loss: 0.833447  [  192/  306]
train() client id: f_00004-1-6 loss: 0.848333  [  224/  306]
train() client id: f_00004-1-7 loss: 0.926102  [  256/  306]
train() client id: f_00004-1-8 loss: 0.972353  [  288/  306]
train() client id: f_00004-2-0 loss: 0.922320  [   32/  306]
train() client id: f_00004-2-1 loss: 0.893871  [   64/  306]
train() client id: f_00004-2-2 loss: 0.921689  [   96/  306]
train() client id: f_00004-2-3 loss: 0.902050  [  128/  306]
train() client id: f_00004-2-4 loss: 1.022744  [  160/  306]
train() client id: f_00004-2-5 loss: 0.923039  [  192/  306]
train() client id: f_00004-2-6 loss: 0.931556  [  224/  306]
train() client id: f_00004-2-7 loss: 0.812010  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939737  [  288/  306]
train() client id: f_00004-3-0 loss: 0.966316  [   32/  306]
train() client id: f_00004-3-1 loss: 0.887743  [   64/  306]
train() client id: f_00004-3-2 loss: 0.997291  [   96/  306]
train() client id: f_00004-3-3 loss: 0.942514  [  128/  306]
train() client id: f_00004-3-4 loss: 0.782422  [  160/  306]
train() client id: f_00004-3-5 loss: 0.833422  [  192/  306]
train() client id: f_00004-3-6 loss: 0.900722  [  224/  306]
train() client id: f_00004-3-7 loss: 0.978267  [  256/  306]
train() client id: f_00004-3-8 loss: 0.896009  [  288/  306]
train() client id: f_00004-4-0 loss: 0.963410  [   32/  306]
train() client id: f_00004-4-1 loss: 0.989944  [   64/  306]
train() client id: f_00004-4-2 loss: 0.962037  [   96/  306]
train() client id: f_00004-4-3 loss: 0.872991  [  128/  306]
train() client id: f_00004-4-4 loss: 0.919654  [  160/  306]
train() client id: f_00004-4-5 loss: 0.814271  [  192/  306]
train() client id: f_00004-4-6 loss: 0.897303  [  224/  306]
train() client id: f_00004-4-7 loss: 0.975745  [  256/  306]
train() client id: f_00004-4-8 loss: 0.887263  [  288/  306]
train() client id: f_00004-5-0 loss: 0.895998  [   32/  306]
train() client id: f_00004-5-1 loss: 0.845465  [   64/  306]
train() client id: f_00004-5-2 loss: 1.075222  [   96/  306]
train() client id: f_00004-5-3 loss: 0.885830  [  128/  306]
train() client id: f_00004-5-4 loss: 0.905266  [  160/  306]
train() client id: f_00004-5-5 loss: 0.899843  [  192/  306]
train() client id: f_00004-5-6 loss: 0.988999  [  224/  306]
train() client id: f_00004-5-7 loss: 0.899905  [  256/  306]
train() client id: f_00004-5-8 loss: 0.848643  [  288/  306]
train() client id: f_00004-6-0 loss: 0.979365  [   32/  306]
train() client id: f_00004-6-1 loss: 0.821573  [   64/  306]
train() client id: f_00004-6-2 loss: 0.838457  [   96/  306]
train() client id: f_00004-6-3 loss: 0.981113  [  128/  306]
train() client id: f_00004-6-4 loss: 0.805149  [  160/  306]
train() client id: f_00004-6-5 loss: 0.979872  [  192/  306]
train() client id: f_00004-6-6 loss: 0.886551  [  224/  306]
train() client id: f_00004-6-7 loss: 1.010787  [  256/  306]
train() client id: f_00004-6-8 loss: 0.885927  [  288/  306]
train() client id: f_00004-7-0 loss: 0.888692  [   32/  306]
train() client id: f_00004-7-1 loss: 0.903529  [   64/  306]
train() client id: f_00004-7-2 loss: 0.940100  [   96/  306]
train() client id: f_00004-7-3 loss: 1.024498  [  128/  306]
train() client id: f_00004-7-4 loss: 0.906443  [  160/  306]
train() client id: f_00004-7-5 loss: 0.916199  [  192/  306]
train() client id: f_00004-7-6 loss: 0.925447  [  224/  306]
train() client id: f_00004-7-7 loss: 0.853642  [  256/  306]
train() client id: f_00004-7-8 loss: 0.876978  [  288/  306]
train() client id: f_00004-8-0 loss: 0.907258  [   32/  306]
train() client id: f_00004-8-1 loss: 0.961818  [   64/  306]
train() client id: f_00004-8-2 loss: 0.865402  [   96/  306]
train() client id: f_00004-8-3 loss: 0.880072  [  128/  306]
train() client id: f_00004-8-4 loss: 0.963080  [  160/  306]
train() client id: f_00004-8-5 loss: 0.890882  [  192/  306]
train() client id: f_00004-8-6 loss: 0.958934  [  224/  306]
train() client id: f_00004-8-7 loss: 0.901401  [  256/  306]
train() client id: f_00004-8-8 loss: 0.884717  [  288/  306]
train() client id: f_00004-9-0 loss: 1.002141  [   32/  306]
train() client id: f_00004-9-1 loss: 0.822796  [   64/  306]
train() client id: f_00004-9-2 loss: 1.060508  [   96/  306]
train() client id: f_00004-9-3 loss: 0.873580  [  128/  306]
train() client id: f_00004-9-4 loss: 0.985713  [  160/  306]
train() client id: f_00004-9-5 loss: 0.990772  [  192/  306]
train() client id: f_00004-9-6 loss: 0.763483  [  224/  306]
train() client id: f_00004-9-7 loss: 0.812155  [  256/  306]
train() client id: f_00004-9-8 loss: 0.918847  [  288/  306]
train() client id: f_00004-10-0 loss: 0.965502  [   32/  306]
train() client id: f_00004-10-1 loss: 0.878836  [   64/  306]
train() client id: f_00004-10-2 loss: 0.851861  [   96/  306]
train() client id: f_00004-10-3 loss: 0.829387  [  128/  306]
train() client id: f_00004-10-4 loss: 1.025811  [  160/  306]
train() client id: f_00004-10-5 loss: 0.838377  [  192/  306]
train() client id: f_00004-10-6 loss: 0.962429  [  224/  306]
train() client id: f_00004-10-7 loss: 0.945306  [  256/  306]
train() client id: f_00004-10-8 loss: 0.856062  [  288/  306]
train() client id: f_00004-11-0 loss: 0.932345  [   32/  306]
train() client id: f_00004-11-1 loss: 0.891493  [   64/  306]
train() client id: f_00004-11-2 loss: 0.989128  [   96/  306]
train() client id: f_00004-11-3 loss: 0.824717  [  128/  306]
train() client id: f_00004-11-4 loss: 1.009284  [  160/  306]
train() client id: f_00004-11-5 loss: 0.922451  [  192/  306]
train() client id: f_00004-11-6 loss: 0.919760  [  224/  306]
train() client id: f_00004-11-7 loss: 0.872551  [  256/  306]
train() client id: f_00004-11-8 loss: 0.845080  [  288/  306]
train() client id: f_00005-0-0 loss: 0.896564  [   32/  146]
train() client id: f_00005-0-1 loss: 0.662397  [   64/  146]
train() client id: f_00005-0-2 loss: 0.918515  [   96/  146]
train() client id: f_00005-0-3 loss: 0.767071  [  128/  146]
train() client id: f_00005-1-0 loss: 0.888680  [   32/  146]
train() client id: f_00005-1-1 loss: 0.599517  [   64/  146]
train() client id: f_00005-1-2 loss: 0.943698  [   96/  146]
train() client id: f_00005-1-3 loss: 0.852340  [  128/  146]
train() client id: f_00005-2-0 loss: 0.893595  [   32/  146]
train() client id: f_00005-2-1 loss: 0.645313  [   64/  146]
train() client id: f_00005-2-2 loss: 0.886591  [   96/  146]
train() client id: f_00005-2-3 loss: 0.789780  [  128/  146]
train() client id: f_00005-3-0 loss: 0.792174  [   32/  146]
train() client id: f_00005-3-1 loss: 0.646899  [   64/  146]
train() client id: f_00005-3-2 loss: 0.814936  [   96/  146]
train() client id: f_00005-3-3 loss: 0.857417  [  128/  146]
train() client id: f_00005-4-0 loss: 0.610211  [   32/  146]
train() client id: f_00005-4-1 loss: 0.645175  [   64/  146]
train() client id: f_00005-4-2 loss: 1.002023  [   96/  146]
train() client id: f_00005-4-3 loss: 0.908793  [  128/  146]
train() client id: f_00005-5-0 loss: 0.725445  [   32/  146]
train() client id: f_00005-5-1 loss: 0.759281  [   64/  146]
train() client id: f_00005-5-2 loss: 0.749392  [   96/  146]
train() client id: f_00005-5-3 loss: 0.836271  [  128/  146]
train() client id: f_00005-6-0 loss: 0.670489  [   32/  146]
train() client id: f_00005-6-1 loss: 0.734948  [   64/  146]
train() client id: f_00005-6-2 loss: 0.855615  [   96/  146]
train() client id: f_00005-6-3 loss: 0.922952  [  128/  146]
train() client id: f_00005-7-0 loss: 1.035383  [   32/  146]
train() client id: f_00005-7-1 loss: 0.775070  [   64/  146]
train() client id: f_00005-7-2 loss: 0.661739  [   96/  146]
train() client id: f_00005-7-3 loss: 0.811586  [  128/  146]
train() client id: f_00005-8-0 loss: 0.843843  [   32/  146]
train() client id: f_00005-8-1 loss: 0.693919  [   64/  146]
train() client id: f_00005-8-2 loss: 0.882311  [   96/  146]
train() client id: f_00005-8-3 loss: 0.853733  [  128/  146]
train() client id: f_00005-9-0 loss: 0.741050  [   32/  146]
train() client id: f_00005-9-1 loss: 0.805619  [   64/  146]
train() client id: f_00005-9-2 loss: 0.698138  [   96/  146]
train() client id: f_00005-9-3 loss: 0.824994  [  128/  146]
train() client id: f_00005-10-0 loss: 0.629333  [   32/  146]
train() client id: f_00005-10-1 loss: 0.827120  [   64/  146]
train() client id: f_00005-10-2 loss: 0.952220  [   96/  146]
train() client id: f_00005-10-3 loss: 0.682073  [  128/  146]
train() client id: f_00005-11-0 loss: 0.738833  [   32/  146]
train() client id: f_00005-11-1 loss: 0.837888  [   64/  146]
train() client id: f_00005-11-2 loss: 0.653281  [   96/  146]
train() client id: f_00005-11-3 loss: 1.009995  [  128/  146]
train() client id: f_00006-0-0 loss: 0.693400  [   32/   54]
train() client id: f_00006-1-0 loss: 0.632306  [   32/   54]
train() client id: f_00006-2-0 loss: 0.593512  [   32/   54]
train() client id: f_00006-3-0 loss: 0.692033  [   32/   54]
train() client id: f_00006-4-0 loss: 0.629625  [   32/   54]
train() client id: f_00006-5-0 loss: 0.588201  [   32/   54]
train() client id: f_00006-6-0 loss: 0.634165  [   32/   54]
train() client id: f_00006-7-0 loss: 0.631972  [   32/   54]
train() client id: f_00006-8-0 loss: 0.637030  [   32/   54]
train() client id: f_00006-9-0 loss: 0.696519  [   32/   54]
train() client id: f_00006-10-0 loss: 0.640106  [   32/   54]
train() client id: f_00006-11-0 loss: 0.693607  [   32/   54]
train() client id: f_00007-0-0 loss: 0.644364  [   32/  179]
train() client id: f_00007-0-1 loss: 0.679137  [   64/  179]
train() client id: f_00007-0-2 loss: 0.731992  [   96/  179]
train() client id: f_00007-0-3 loss: 0.832880  [  128/  179]
train() client id: f_00007-0-4 loss: 0.618423  [  160/  179]
train() client id: f_00007-1-0 loss: 0.721886  [   32/  179]
train() client id: f_00007-1-1 loss: 0.720097  [   64/  179]
train() client id: f_00007-1-2 loss: 0.715648  [   96/  179]
train() client id: f_00007-1-3 loss: 0.638342  [  128/  179]
train() client id: f_00007-1-4 loss: 0.558680  [  160/  179]
train() client id: f_00007-2-0 loss: 0.602551  [   32/  179]
train() client id: f_00007-2-1 loss: 0.672252  [   64/  179]
train() client id: f_00007-2-2 loss: 0.568271  [   96/  179]
train() client id: f_00007-2-3 loss: 0.679997  [  128/  179]
train() client id: f_00007-2-4 loss: 0.751880  [  160/  179]
train() client id: f_00007-3-0 loss: 0.794212  [   32/  179]
train() client id: f_00007-3-1 loss: 0.544193  [   64/  179]
train() client id: f_00007-3-2 loss: 0.610704  [   96/  179]
train() client id: f_00007-3-3 loss: 0.718906  [  128/  179]
train() client id: f_00007-3-4 loss: 0.595741  [  160/  179]
train() client id: f_00007-4-0 loss: 0.690235  [   32/  179]
train() client id: f_00007-4-1 loss: 0.605228  [   64/  179]
train() client id: f_00007-4-2 loss: 0.863414  [   96/  179]
train() client id: f_00007-4-3 loss: 0.594414  [  128/  179]
train() client id: f_00007-4-4 loss: 0.521694  [  160/  179]
train() client id: f_00007-5-0 loss: 0.584983  [   32/  179]
train() client id: f_00007-5-1 loss: 0.698570  [   64/  179]
train() client id: f_00007-5-2 loss: 0.626175  [   96/  179]
train() client id: f_00007-5-3 loss: 0.620081  [  128/  179]
train() client id: f_00007-5-4 loss: 0.672451  [  160/  179]
train() client id: f_00007-6-0 loss: 0.672960  [   32/  179]
train() client id: f_00007-6-1 loss: 0.607779  [   64/  179]
train() client id: f_00007-6-2 loss: 0.532111  [   96/  179]
train() client id: f_00007-6-3 loss: 0.616535  [  128/  179]
train() client id: f_00007-6-4 loss: 0.807100  [  160/  179]
train() client id: f_00007-7-0 loss: 0.523751  [   32/  179]
train() client id: f_00007-7-1 loss: 0.592010  [   64/  179]
train() client id: f_00007-7-2 loss: 0.625869  [   96/  179]
train() client id: f_00007-7-3 loss: 0.745893  [  128/  179]
train() client id: f_00007-7-4 loss: 0.719981  [  160/  179]
train() client id: f_00007-8-0 loss: 0.635923  [   32/  179]
train() client id: f_00007-8-1 loss: 0.614383  [   64/  179]
train() client id: f_00007-8-2 loss: 0.658294  [   96/  179]
train() client id: f_00007-8-3 loss: 0.608085  [  128/  179]
train() client id: f_00007-8-4 loss: 0.507433  [  160/  179]
train() client id: f_00007-9-0 loss: 0.840722  [   32/  179]
train() client id: f_00007-9-1 loss: 0.535923  [   64/  179]
train() client id: f_00007-9-2 loss: 0.574629  [   96/  179]
train() client id: f_00007-9-3 loss: 0.588980  [  128/  179]
train() client id: f_00007-9-4 loss: 0.516346  [  160/  179]
train() client id: f_00007-10-0 loss: 0.664332  [   32/  179]
train() client id: f_00007-10-1 loss: 0.593030  [   64/  179]
train() client id: f_00007-10-2 loss: 0.598623  [   96/  179]
train() client id: f_00007-10-3 loss: 0.586986  [  128/  179]
train() client id: f_00007-10-4 loss: 0.499315  [  160/  179]
train() client id: f_00007-11-0 loss: 0.837605  [   32/  179]
train() client id: f_00007-11-1 loss: 0.658039  [   64/  179]
train() client id: f_00007-11-2 loss: 0.571981  [   96/  179]
train() client id: f_00007-11-3 loss: 0.607659  [  128/  179]
train() client id: f_00007-11-4 loss: 0.528267  [  160/  179]
train() client id: f_00008-0-0 loss: 0.780190  [   32/  130]
train() client id: f_00008-0-1 loss: 0.642652  [   64/  130]
train() client id: f_00008-0-2 loss: 0.851930  [   96/  130]
train() client id: f_00008-0-3 loss: 0.762312  [  128/  130]
train() client id: f_00008-1-0 loss: 0.788315  [   32/  130]
train() client id: f_00008-1-1 loss: 0.748642  [   64/  130]
train() client id: f_00008-1-2 loss: 0.754689  [   96/  130]
train() client id: f_00008-1-3 loss: 0.725677  [  128/  130]
train() client id: f_00008-2-0 loss: 0.778136  [   32/  130]
train() client id: f_00008-2-1 loss: 0.794469  [   64/  130]
train() client id: f_00008-2-2 loss: 0.704711  [   96/  130]
train() client id: f_00008-2-3 loss: 0.740495  [  128/  130]
train() client id: f_00008-3-0 loss: 0.756158  [   32/  130]
train() client id: f_00008-3-1 loss: 0.682831  [   64/  130]
train() client id: f_00008-3-2 loss: 0.817021  [   96/  130]
train() client id: f_00008-3-3 loss: 0.754815  [  128/  130]
train() client id: f_00008-4-0 loss: 0.829084  [   32/  130]
train() client id: f_00008-4-1 loss: 0.788841  [   64/  130]
train() client id: f_00008-4-2 loss: 0.655283  [   96/  130]
train() client id: f_00008-4-3 loss: 0.695293  [  128/  130]
train() client id: f_00008-5-0 loss: 0.667956  [   32/  130]
train() client id: f_00008-5-1 loss: 0.741064  [   64/  130]
train() client id: f_00008-5-2 loss: 0.760568  [   96/  130]
train() client id: f_00008-5-3 loss: 0.753583  [  128/  130]
train() client id: f_00008-6-0 loss: 0.700211  [   32/  130]
train() client id: f_00008-6-1 loss: 0.813208  [   64/  130]
train() client id: f_00008-6-2 loss: 0.744788  [   96/  130]
train() client id: f_00008-6-3 loss: 0.733243  [  128/  130]
train() client id: f_00008-7-0 loss: 0.696304  [   32/  130]
train() client id: f_00008-7-1 loss: 0.723933  [   64/  130]
train() client id: f_00008-7-2 loss: 0.718585  [   96/  130]
train() client id: f_00008-7-3 loss: 0.845566  [  128/  130]
train() client id: f_00008-8-0 loss: 0.828694  [   32/  130]
train() client id: f_00008-8-1 loss: 0.770810  [   64/  130]
train() client id: f_00008-8-2 loss: 0.754739  [   96/  130]
train() client id: f_00008-8-3 loss: 0.625003  [  128/  130]
train() client id: f_00008-9-0 loss: 0.760996  [   32/  130]
train() client id: f_00008-9-1 loss: 0.715058  [   64/  130]
train() client id: f_00008-9-2 loss: 0.840310  [   96/  130]
train() client id: f_00008-9-3 loss: 0.661033  [  128/  130]
train() client id: f_00008-10-0 loss: 0.684895  [   32/  130]
train() client id: f_00008-10-1 loss: 0.839779  [   64/  130]
train() client id: f_00008-10-2 loss: 0.730266  [   96/  130]
train() client id: f_00008-10-3 loss: 0.720734  [  128/  130]
train() client id: f_00008-11-0 loss: 0.732452  [   32/  130]
train() client id: f_00008-11-1 loss: 0.774705  [   64/  130]
train() client id: f_00008-11-2 loss: 0.735400  [   96/  130]
train() client id: f_00008-11-3 loss: 0.734671  [  128/  130]
train() client id: f_00009-0-0 loss: 1.241861  [   32/  118]
train() client id: f_00009-0-1 loss: 1.350923  [   64/  118]
train() client id: f_00009-0-2 loss: 1.127849  [   96/  118]
train() client id: f_00009-1-0 loss: 1.207355  [   32/  118]
train() client id: f_00009-1-1 loss: 1.109547  [   64/  118]
train() client id: f_00009-1-2 loss: 1.163656  [   96/  118]
train() client id: f_00009-2-0 loss: 1.038793  [   32/  118]
train() client id: f_00009-2-1 loss: 1.109635  [   64/  118]
train() client id: f_00009-2-2 loss: 1.153043  [   96/  118]
train() client id: f_00009-3-0 loss: 1.080081  [   32/  118]
train() client id: f_00009-3-1 loss: 1.041016  [   64/  118]
train() client id: f_00009-3-2 loss: 1.111178  [   96/  118]
train() client id: f_00009-4-0 loss: 1.086571  [   32/  118]
train() client id: f_00009-4-1 loss: 1.022443  [   64/  118]
train() client id: f_00009-4-2 loss: 0.981258  [   96/  118]
train() client id: f_00009-5-0 loss: 1.084752  [   32/  118]
train() client id: f_00009-5-1 loss: 0.962605  [   64/  118]
train() client id: f_00009-5-2 loss: 1.022729  [   96/  118]
train() client id: f_00009-6-0 loss: 1.026595  [   32/  118]
train() client id: f_00009-6-1 loss: 0.917848  [   64/  118]
train() client id: f_00009-6-2 loss: 0.988272  [   96/  118]
train() client id: f_00009-7-0 loss: 0.974388  [   32/  118]
train() client id: f_00009-7-1 loss: 0.863358  [   64/  118]
train() client id: f_00009-7-2 loss: 0.947510  [   96/  118]
train() client id: f_00009-8-0 loss: 0.943831  [   32/  118]
train() client id: f_00009-8-1 loss: 1.018914  [   64/  118]
train() client id: f_00009-8-2 loss: 0.884976  [   96/  118]
train() client id: f_00009-9-0 loss: 1.004413  [   32/  118]
train() client id: f_00009-9-1 loss: 0.934233  [   64/  118]
train() client id: f_00009-9-2 loss: 0.923005  [   96/  118]
train() client id: f_00009-10-0 loss: 0.959135  [   32/  118]
train() client id: f_00009-10-1 loss: 0.841481  [   64/  118]
train() client id: f_00009-10-2 loss: 0.987126  [   96/  118]
train() client id: f_00009-11-0 loss: 1.023125  [   32/  118]
train() client id: f_00009-11-1 loss: 0.880267  [   64/  118]
train() client id: f_00009-11-2 loss: 0.924850  [   96/  118]
At round 12 accuracy: 0.6312997347480106
At round 12 training accuracy: 0.5727699530516432
At round 12 training loss: 0.8607914824834106
update_location
xs = [ -3.9056584    4.20031788  80.00902392  18.81129433   0.97929623
   3.95640986 -42.44319194 -21.32485185  64.66397685  -7.06087855]
ys = [ 72.5879595   55.55583871   1.32061395 -42.45517586  34.35018685
  17.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [123.62955161 114.47311424 128.07493092 110.25564272 105.73974824
 101.65135093 108.66607145 102.25177535 120.37483074 100.32879877]
dists_bs = [199.40285797 215.35937777 308.53546217 291.28949744 225.28000941
 238.18645603 221.6347049  232.27980029 286.74612406 239.67486674]
uav_gains = [5.88385192e-11 7.13231094e-11 5.38624893e-11 7.83414303e-11
 8.69762693e-11 9.59875921e-11 8.12381170e-11 9.45846506e-11
 6.28982282e-11 9.91823280e-11]
bs_gains = [4.01833579e-11 3.23918296e-11 1.18369507e-11 1.39054348e-11
 2.85542429e-11 2.44301599e-11 2.98887905e-11 2.62096995e-11
 1.45311815e-11 2.40077301e-11]
Round 13
-------------------------------
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.27548856 19.34614452  9.13389385  3.26646163 22.31552022 10.75645448
  4.0605729  13.0924169   9.62823145  8.73319217]
obj_prev = 109.60837667978073
eta_min = 9.621283683779976e-11	eta_max = 0.9207981058157783
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 25.487961404153893	eta = 0.9090909090909091
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 44.16384428532155	eta = 0.5246570894979226
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 35.24177796016038	eta = 0.6574831164866354
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.64173104871985	eta = 0.6887539160877387
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56259776728117	eta = 0.6903778475206297
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56239040785321	eta = 0.6903821129008305
eta = 0.6903821129008305
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.0305736  0.06430162 0.03008831 0.01043385 0.07425017 0.03542655
 0.01310297 0.04343393 0.03154419 0.02863242]
ene_total = [2.87721447 5.53384282 2.84956312 1.30490774 6.3139176  3.36162512
 1.50604088 3.81675253 3.15491369 2.84361242]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 0 obj = 5.882096701797337
eta = 0.6903821129008305
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
eta_min = 0.6903821129008529	eta_max = 0.6903821129008285
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 0.04372443773870311	eta = 0.909090909090909
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 19.912896282792712	eta = 0.0019961681258650003
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.05768598057864	eta = 0.019317568000433452
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.002030029818999	eta = 0.019854591720065342
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.0020149221690455	eta = 0.019854741547231138
eta = 0.019854741547231138
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.92521000e-04 1.90294690e-03 1.84965388e-04 7.47180525e-06
 2.97469966e-03 3.29684180e-04 1.47562323e-05 5.31384189e-04
 2.52376804e-04 1.74465793e-04]
ene_total = [0.17335085 0.23740048 0.17618224 0.15978245 0.26890909 0.21220106
 0.15887471 0.16699434 0.23902414 0.20929556]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 1 obj = 5.882096701797758
eta = 0.6903821129008529
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
Done!
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.80012808e-05 1.77931142e-04 1.72948088e-05 6.98635807e-07
 2.78143184e-04 3.08264423e-05 1.37975120e-06 4.96859874e-05
 2.35979748e-05 1.63130657e-05]
ene_total = [0.00698572 0.00808079 0.00710952 0.00659303 0.00841071 0.00846609
 0.00654893 0.00641599 0.00964409 0.00848678]
At round 13 energy consumption: 0.07674165610159829
At round 13 eta: 0.6903821129008529
At round 13 a_n: 23.72950684565325
At round 13 local rounds: 12.132381383768099
At round 13 global rounds: 76.641266006878
gradient difference: 0.40264683961868286
train() client id: f_00000-0-0 loss: 1.507375  [   32/  126]
train() client id: f_00000-0-1 loss: 1.151610  [   64/  126]
train() client id: f_00000-0-2 loss: 1.227918  [   96/  126]
train() client id: f_00000-1-0 loss: 1.201796  [   32/  126]
train() client id: f_00000-1-1 loss: 1.182894  [   64/  126]
train() client id: f_00000-1-2 loss: 1.152835  [   96/  126]
train() client id: f_00000-2-0 loss: 1.107550  [   32/  126]
train() client id: f_00000-2-1 loss: 1.043198  [   64/  126]
train() client id: f_00000-2-2 loss: 1.141011  [   96/  126]
train() client id: f_00000-3-0 loss: 1.111149  [   32/  126]
train() client id: f_00000-3-1 loss: 1.061796  [   64/  126]
train() client id: f_00000-3-2 loss: 1.050599  [   96/  126]
train() client id: f_00000-4-0 loss: 0.993313  [   32/  126]
train() client id: f_00000-4-1 loss: 0.995516  [   64/  126]
train() client id: f_00000-4-2 loss: 1.009626  [   96/  126]
train() client id: f_00000-5-0 loss: 0.933035  [   32/  126]
train() client id: f_00000-5-1 loss: 0.995622  [   64/  126]
train() client id: f_00000-5-2 loss: 1.053006  [   96/  126]
train() client id: f_00000-6-0 loss: 0.945196  [   32/  126]
train() client id: f_00000-6-1 loss: 0.938132  [   64/  126]
train() client id: f_00000-6-2 loss: 1.032627  [   96/  126]
train() client id: f_00000-7-0 loss: 0.978384  [   32/  126]
train() client id: f_00000-7-1 loss: 0.947255  [   64/  126]
train() client id: f_00000-7-2 loss: 0.932879  [   96/  126]
train() client id: f_00000-8-0 loss: 0.964411  [   32/  126]
train() client id: f_00000-8-1 loss: 0.887344  [   64/  126]
train() client id: f_00000-8-2 loss: 0.920303  [   96/  126]
train() client id: f_00000-9-0 loss: 0.973297  [   32/  126]
train() client id: f_00000-9-1 loss: 0.878159  [   64/  126]
train() client id: f_00000-9-2 loss: 0.919297  [   96/  126]
train() client id: f_00000-10-0 loss: 0.957784  [   32/  126]
train() client id: f_00000-10-1 loss: 1.006271  [   64/  126]
train() client id: f_00000-10-2 loss: 0.903425  [   96/  126]
train() client id: f_00000-11-0 loss: 0.989156  [   32/  126]
train() client id: f_00000-11-1 loss: 0.884343  [   64/  126]
train() client id: f_00000-11-2 loss: 0.948140  [   96/  126]
train() client id: f_00001-0-0 loss: 0.471369  [   32/  265]
train() client id: f_00001-0-1 loss: 0.395140  [   64/  265]
train() client id: f_00001-0-2 loss: 0.480929  [   96/  265]
train() client id: f_00001-0-3 loss: 0.492475  [  128/  265]
train() client id: f_00001-0-4 loss: 0.435754  [  160/  265]
train() client id: f_00001-0-5 loss: 0.491928  [  192/  265]
train() client id: f_00001-0-6 loss: 0.481578  [  224/  265]
train() client id: f_00001-0-7 loss: 0.480734  [  256/  265]
train() client id: f_00001-1-0 loss: 0.415046  [   32/  265]
train() client id: f_00001-1-1 loss: 0.493232  [   64/  265]
train() client id: f_00001-1-2 loss: 0.420645  [   96/  265]
train() client id: f_00001-1-3 loss: 0.470648  [  128/  265]
train() client id: f_00001-1-4 loss: 0.499932  [  160/  265]
train() client id: f_00001-1-5 loss: 0.364901  [  192/  265]
train() client id: f_00001-1-6 loss: 0.476887  [  224/  265]
train() client id: f_00001-1-7 loss: 0.492321  [  256/  265]
train() client id: f_00001-2-0 loss: 0.433371  [   32/  265]
train() client id: f_00001-2-1 loss: 0.398966  [   64/  265]
train() client id: f_00001-2-2 loss: 0.446282  [   96/  265]
train() client id: f_00001-2-3 loss: 0.424052  [  128/  265]
train() client id: f_00001-2-4 loss: 0.440997  [  160/  265]
train() client id: f_00001-2-5 loss: 0.474288  [  192/  265]
train() client id: f_00001-2-6 loss: 0.448527  [  224/  265]
train() client id: f_00001-2-7 loss: 0.498164  [  256/  265]
train() client id: f_00001-3-0 loss: 0.408029  [   32/  265]
train() client id: f_00001-3-1 loss: 0.397388  [   64/  265]
train() client id: f_00001-3-2 loss: 0.381859  [   96/  265]
train() client id: f_00001-3-3 loss: 0.411684  [  128/  265]
train() client id: f_00001-3-4 loss: 0.372407  [  160/  265]
train() client id: f_00001-3-5 loss: 0.523023  [  192/  265]
train() client id: f_00001-3-6 loss: 0.514442  [  224/  265]
train() client id: f_00001-3-7 loss: 0.494715  [  256/  265]
train() client id: f_00001-4-0 loss: 0.526759  [   32/  265]
train() client id: f_00001-4-1 loss: 0.389449  [   64/  265]
train() client id: f_00001-4-2 loss: 0.445977  [   96/  265]
train() client id: f_00001-4-3 loss: 0.540694  [  128/  265]
train() client id: f_00001-4-4 loss: 0.356926  [  160/  265]
train() client id: f_00001-4-5 loss: 0.357674  [  192/  265]
train() client id: f_00001-4-6 loss: 0.475955  [  224/  265]
train() client id: f_00001-4-7 loss: 0.363820  [  256/  265]
train() client id: f_00001-5-0 loss: 0.399269  [   32/  265]
train() client id: f_00001-5-1 loss: 0.451739  [   64/  265]
train() client id: f_00001-5-2 loss: 0.388730  [   96/  265]
train() client id: f_00001-5-3 loss: 0.350214  [  128/  265]
train() client id: f_00001-5-4 loss: 0.565231  [  160/  265]
train() client id: f_00001-5-5 loss: 0.400620  [  192/  265]
train() client id: f_00001-5-6 loss: 0.486730  [  224/  265]
train() client id: f_00001-5-7 loss: 0.388564  [  256/  265]
train() client id: f_00001-6-0 loss: 0.425432  [   32/  265]
train() client id: f_00001-6-1 loss: 0.398732  [   64/  265]
train() client id: f_00001-6-2 loss: 0.398793  [   96/  265]
train() client id: f_00001-6-3 loss: 0.380764  [  128/  265]
train() client id: f_00001-6-4 loss: 0.333859  [  160/  265]
train() client id: f_00001-6-5 loss: 0.523808  [  192/  265]
train() client id: f_00001-6-6 loss: 0.323537  [  224/  265]
train() client id: f_00001-6-7 loss: 0.544747  [  256/  265]
train() client id: f_00001-7-0 loss: 0.408221  [   32/  265]
train() client id: f_00001-7-1 loss: 0.380871  [   64/  265]
train() client id: f_00001-7-2 loss: 0.443387  [   96/  265]
train() client id: f_00001-7-3 loss: 0.403861  [  128/  265]
train() client id: f_00001-7-4 loss: 0.441708  [  160/  265]
train() client id: f_00001-7-5 loss: 0.391694  [  192/  265]
train() client id: f_00001-7-6 loss: 0.495652  [  224/  265]
train() client id: f_00001-7-7 loss: 0.349291  [  256/  265]
train() client id: f_00001-8-0 loss: 0.463337  [   32/  265]
train() client id: f_00001-8-1 loss: 0.412776  [   64/  265]
train() client id: f_00001-8-2 loss: 0.339484  [   96/  265]
train() client id: f_00001-8-3 loss: 0.479911  [  128/  265]
train() client id: f_00001-8-4 loss: 0.433338  [  160/  265]
train() client id: f_00001-8-5 loss: 0.400264  [  192/  265]
train() client id: f_00001-8-6 loss: 0.361649  [  224/  265]
train() client id: f_00001-8-7 loss: 0.473012  [  256/  265]
train() client id: f_00001-9-0 loss: 0.388046  [   32/  265]
train() client id: f_00001-9-1 loss: 0.364218  [   64/  265]
train() client id: f_00001-9-2 loss: 0.409114  [   96/  265]
train() client id: f_00001-9-3 loss: 0.342918  [  128/  265]
train() client id: f_00001-9-4 loss: 0.533892  [  160/  265]
train() client id: f_00001-9-5 loss: 0.396582  [  192/  265]
train() client id: f_00001-9-6 loss: 0.463949  [  224/  265]
train() client id: f_00001-9-7 loss: 0.392464  [  256/  265]
train() client id: f_00001-10-0 loss: 0.389096  [   32/  265]
train() client id: f_00001-10-1 loss: 0.470982  [   64/  265]
train() client id: f_00001-10-2 loss: 0.455956  [   96/  265]
train() client id: f_00001-10-3 loss: 0.387587  [  128/  265]
train() client id: f_00001-10-4 loss: 0.456913  [  160/  265]
train() client id: f_00001-10-5 loss: 0.425958  [  192/  265]
train() client id: f_00001-10-6 loss: 0.415326  [  224/  265]
train() client id: f_00001-10-7 loss: 0.341587  [  256/  265]
train() client id: f_00001-11-0 loss: 0.377277  [   32/  265]
train() client id: f_00001-11-1 loss: 0.485390  [   64/  265]
train() client id: f_00001-11-2 loss: 0.364242  [   96/  265]
train() client id: f_00001-11-3 loss: 0.582661  [  128/  265]
train() client id: f_00001-11-4 loss: 0.373701  [  160/  265]
train() client id: f_00001-11-5 loss: 0.410418  [  192/  265]
train() client id: f_00001-11-6 loss: 0.373417  [  224/  265]
train() client id: f_00001-11-7 loss: 0.317690  [  256/  265]
train() client id: f_00002-0-0 loss: 1.335080  [   32/  124]
train() client id: f_00002-0-1 loss: 1.353309  [   64/  124]
train() client id: f_00002-0-2 loss: 1.116384  [   96/  124]
train() client id: f_00002-1-0 loss: 1.134740  [   32/  124]
train() client id: f_00002-1-1 loss: 1.196861  [   64/  124]
train() client id: f_00002-1-2 loss: 1.203378  [   96/  124]
train() client id: f_00002-2-0 loss: 1.176747  [   32/  124]
train() client id: f_00002-2-1 loss: 1.120529  [   64/  124]
train() client id: f_00002-2-2 loss: 1.130521  [   96/  124]
train() client id: f_00002-3-0 loss: 1.128191  [   32/  124]
train() client id: f_00002-3-1 loss: 1.190656  [   64/  124]
train() client id: f_00002-3-2 loss: 1.032996  [   96/  124]
train() client id: f_00002-4-0 loss: 0.992292  [   32/  124]
train() client id: f_00002-4-1 loss: 1.047880  [   64/  124]
train() client id: f_00002-4-2 loss: 1.162976  [   96/  124]
train() client id: f_00002-5-0 loss: 1.192169  [   32/  124]
train() client id: f_00002-5-1 loss: 0.979977  [   64/  124]
train() client id: f_00002-5-2 loss: 1.007862  [   96/  124]
train() client id: f_00002-6-0 loss: 1.082665  [   32/  124]
train() client id: f_00002-6-1 loss: 1.088101  [   64/  124]
train() client id: f_00002-6-2 loss: 0.998743  [   96/  124]
train() client id: f_00002-7-0 loss: 1.141930  [   32/  124]
train() client id: f_00002-7-1 loss: 0.984103  [   64/  124]
train() client id: f_00002-7-2 loss: 1.038381  [   96/  124]
train() client id: f_00002-8-0 loss: 1.030792  [   32/  124]
train() client id: f_00002-8-1 loss: 0.911269  [   64/  124]
train() client id: f_00002-8-2 loss: 1.135282  [   96/  124]
train() client id: f_00002-9-0 loss: 1.009354  [   32/  124]
train() client id: f_00002-9-1 loss: 0.943218  [   64/  124]
train() client id: f_00002-9-2 loss: 1.078572  [   96/  124]
train() client id: f_00002-10-0 loss: 1.083777  [   32/  124]
train() client id: f_00002-10-1 loss: 0.887083  [   64/  124]
train() client id: f_00002-10-2 loss: 1.051497  [   96/  124]
train() client id: f_00002-11-0 loss: 1.084437  [   32/  124]
train() client id: f_00002-11-1 loss: 1.066697  [   64/  124]
train() client id: f_00002-11-2 loss: 0.932667  [   96/  124]
train() client id: f_00003-0-0 loss: 0.782461  [   32/   43]
train() client id: f_00003-1-0 loss: 0.859634  [   32/   43]
train() client id: f_00003-2-0 loss: 0.960783  [   32/   43]
train() client id: f_00003-3-0 loss: 0.814740  [   32/   43]
train() client id: f_00003-4-0 loss: 0.873051  [   32/   43]
train() client id: f_00003-5-0 loss: 0.879673  [   32/   43]
train() client id: f_00003-6-0 loss: 0.852155  [   32/   43]
train() client id: f_00003-7-0 loss: 0.841638  [   32/   43]
train() client id: f_00003-8-0 loss: 0.989165  [   32/   43]
train() client id: f_00003-9-0 loss: 0.850167  [   32/   43]
train() client id: f_00003-10-0 loss: 0.901700  [   32/   43]
train() client id: f_00003-11-0 loss: 0.949601  [   32/   43]
train() client id: f_00004-0-0 loss: 1.116777  [   32/  306]
train() client id: f_00004-0-1 loss: 1.034952  [   64/  306]
train() client id: f_00004-0-2 loss: 0.973967  [   96/  306]
train() client id: f_00004-0-3 loss: 0.876545  [  128/  306]
train() client id: f_00004-0-4 loss: 0.903974  [  160/  306]
train() client id: f_00004-0-5 loss: 0.987726  [  192/  306]
train() client id: f_00004-0-6 loss: 1.109797  [  224/  306]
train() client id: f_00004-0-7 loss: 0.881736  [  256/  306]
train() client id: f_00004-0-8 loss: 1.011212  [  288/  306]
train() client id: f_00004-1-0 loss: 0.980540  [   32/  306]
train() client id: f_00004-1-1 loss: 0.971234  [   64/  306]
train() client id: f_00004-1-2 loss: 0.992516  [   96/  306]
train() client id: f_00004-1-3 loss: 0.991612  [  128/  306]
train() client id: f_00004-1-4 loss: 1.047062  [  160/  306]
train() client id: f_00004-1-5 loss: 1.115074  [  192/  306]
train() client id: f_00004-1-6 loss: 0.986957  [  224/  306]
train() client id: f_00004-1-7 loss: 0.960375  [  256/  306]
train() client id: f_00004-1-8 loss: 0.882697  [  288/  306]
train() client id: f_00004-2-0 loss: 0.867219  [   32/  306]
train() client id: f_00004-2-1 loss: 0.972469  [   64/  306]
train() client id: f_00004-2-2 loss: 1.011315  [   96/  306]
train() client id: f_00004-2-3 loss: 1.026002  [  128/  306]
train() client id: f_00004-2-4 loss: 0.991594  [  160/  306]
train() client id: f_00004-2-5 loss: 0.937659  [  192/  306]
train() client id: f_00004-2-6 loss: 1.025936  [  224/  306]
train() client id: f_00004-2-7 loss: 0.960288  [  256/  306]
train() client id: f_00004-2-8 loss: 1.066441  [  288/  306]
train() client id: f_00004-3-0 loss: 0.915462  [   32/  306]
train() client id: f_00004-3-1 loss: 1.004046  [   64/  306]
train() client id: f_00004-3-2 loss: 0.993861  [   96/  306]
train() client id: f_00004-3-3 loss: 1.027195  [  128/  306]
train() client id: f_00004-3-4 loss: 0.962369  [  160/  306]
train() client id: f_00004-3-5 loss: 1.083820  [  192/  306]
train() client id: f_00004-3-6 loss: 1.037436  [  224/  306]
train() client id: f_00004-3-7 loss: 0.811860  [  256/  306]
train() client id: f_00004-3-8 loss: 0.970288  [  288/  306]
train() client id: f_00004-4-0 loss: 0.991364  [   32/  306]
train() client id: f_00004-4-1 loss: 0.933750  [   64/  306]
train() client id: f_00004-4-2 loss: 0.964965  [   96/  306]
train() client id: f_00004-4-3 loss: 0.942240  [  128/  306]
train() client id: f_00004-4-4 loss: 0.984233  [  160/  306]
train() client id: f_00004-4-5 loss: 0.899209  [  192/  306]
train() client id: f_00004-4-6 loss: 0.994802  [  224/  306]
train() client id: f_00004-4-7 loss: 1.045248  [  256/  306]
train() client id: f_00004-4-8 loss: 1.109980  [  288/  306]
train() client id: f_00004-5-0 loss: 1.029482  [   32/  306]
train() client id: f_00004-5-1 loss: 0.967160  [   64/  306]
train() client id: f_00004-5-2 loss: 0.948397  [   96/  306]
train() client id: f_00004-5-3 loss: 0.890578  [  128/  306]
train() client id: f_00004-5-4 loss: 0.912319  [  160/  306]
train() client id: f_00004-5-5 loss: 0.826464  [  192/  306]
train() client id: f_00004-5-6 loss: 1.093986  [  224/  306]
train() client id: f_00004-5-7 loss: 1.066074  [  256/  306]
train() client id: f_00004-5-8 loss: 1.044289  [  288/  306]
train() client id: f_00004-6-0 loss: 0.979137  [   32/  306]
train() client id: f_00004-6-1 loss: 0.850965  [   64/  306]
train() client id: f_00004-6-2 loss: 1.024293  [   96/  306]
train() client id: f_00004-6-3 loss: 0.997793  [  128/  306]
train() client id: f_00004-6-4 loss: 0.912407  [  160/  306]
train() client id: f_00004-6-5 loss: 1.092130  [  192/  306]
train() client id: f_00004-6-6 loss: 1.005379  [  224/  306]
train() client id: f_00004-6-7 loss: 0.996048  [  256/  306]
train() client id: f_00004-6-8 loss: 0.931853  [  288/  306]
train() client id: f_00004-7-0 loss: 0.977561  [   32/  306]
train() client id: f_00004-7-1 loss: 0.985982  [   64/  306]
train() client id: f_00004-7-2 loss: 0.905709  [   96/  306]
train() client id: f_00004-7-3 loss: 0.925768  [  128/  306]
train() client id: f_00004-7-4 loss: 1.107131  [  160/  306]
train() client id: f_00004-7-5 loss: 0.982728  [  192/  306]
train() client id: f_00004-7-6 loss: 0.892824  [  224/  306]
train() client id: f_00004-7-7 loss: 0.970698  [  256/  306]
train() client id: f_00004-7-8 loss: 0.983009  [  288/  306]
train() client id: f_00004-8-0 loss: 1.032261  [   32/  306]
train() client id: f_00004-8-1 loss: 0.943699  [   64/  306]
train() client id: f_00004-8-2 loss: 0.890260  [   96/  306]
train() client id: f_00004-8-3 loss: 1.011744  [  128/  306]
train() client id: f_00004-8-4 loss: 0.990309  [  160/  306]
train() client id: f_00004-8-5 loss: 1.007298  [  192/  306]
train() client id: f_00004-8-6 loss: 0.955830  [  224/  306]
train() client id: f_00004-8-7 loss: 0.971607  [  256/  306]
train() client id: f_00004-8-8 loss: 0.926313  [  288/  306]
train() client id: f_00004-9-0 loss: 0.963757  [   32/  306]
train() client id: f_00004-9-1 loss: 0.962393  [   64/  306]
train() client id: f_00004-9-2 loss: 1.026487  [   96/  306]
train() client id: f_00004-9-3 loss: 0.954238  [  128/  306]
train() client id: f_00004-9-4 loss: 0.960127  [  160/  306]
train() client id: f_00004-9-5 loss: 0.957540  [  192/  306]
train() client id: f_00004-9-6 loss: 0.973443  [  224/  306]
train() client id: f_00004-9-7 loss: 0.947072  [  256/  306]
train() client id: f_00004-9-8 loss: 0.992148  [  288/  306]
train() client id: f_00004-10-0 loss: 0.931335  [   32/  306]
train() client id: f_00004-10-1 loss: 0.937334  [   64/  306]
train() client id: f_00004-10-2 loss: 0.976767  [   96/  306]
train() client id: f_00004-10-3 loss: 0.923123  [  128/  306]
train() client id: f_00004-10-4 loss: 0.862502  [  160/  306]
train() client id: f_00004-10-5 loss: 0.937701  [  192/  306]
train() client id: f_00004-10-6 loss: 1.057363  [  224/  306]
train() client id: f_00004-10-7 loss: 0.996025  [  256/  306]
train() client id: f_00004-10-8 loss: 1.005053  [  288/  306]
train() client id: f_00004-11-0 loss: 0.966062  [   32/  306]
train() client id: f_00004-11-1 loss: 0.989616  [   64/  306]
train() client id: f_00004-11-2 loss: 0.844990  [   96/  306]
train() client id: f_00004-11-3 loss: 0.875358  [  128/  306]
train() client id: f_00004-11-4 loss: 1.072367  [  160/  306]
train() client id: f_00004-11-5 loss: 1.027579  [  192/  306]
train() client id: f_00004-11-6 loss: 1.001567  [  224/  306]
train() client id: f_00004-11-7 loss: 0.923191  [  256/  306]
train() client id: f_00004-11-8 loss: 0.983457  [  288/  306]
train() client id: f_00005-0-0 loss: 0.463679  [   32/  146]
train() client id: f_00005-0-1 loss: 0.564170  [   64/  146]
train() client id: f_00005-0-2 loss: 0.622582  [   96/  146]
train() client id: f_00005-0-3 loss: 0.692766  [  128/  146]
train() client id: f_00005-1-0 loss: 0.515145  [   32/  146]
train() client id: f_00005-1-1 loss: 0.528824  [   64/  146]
train() client id: f_00005-1-2 loss: 0.671645  [   96/  146]
train() client id: f_00005-1-3 loss: 0.471417  [  128/  146]
train() client id: f_00005-2-0 loss: 0.439797  [   32/  146]
train() client id: f_00005-2-1 loss: 0.427722  [   64/  146]
train() client id: f_00005-2-2 loss: 0.534914  [   96/  146]
train() client id: f_00005-2-3 loss: 0.710121  [  128/  146]
train() client id: f_00005-3-0 loss: 0.599911  [   32/  146]
train() client id: f_00005-3-1 loss: 0.573623  [   64/  146]
train() client id: f_00005-3-2 loss: 0.503131  [   96/  146]
train() client id: f_00005-3-3 loss: 0.427432  [  128/  146]
train() client id: f_00005-4-0 loss: 0.429962  [   32/  146]
train() client id: f_00005-4-1 loss: 0.599588  [   64/  146]
train() client id: f_00005-4-2 loss: 0.486456  [   96/  146]
train() client id: f_00005-4-3 loss: 0.613261  [  128/  146]
train() client id: f_00005-5-0 loss: 0.307613  [   32/  146]
train() client id: f_00005-5-1 loss: 0.560896  [   64/  146]
train() client id: f_00005-5-2 loss: 0.597188  [   96/  146]
train() client id: f_00005-5-3 loss: 0.643618  [  128/  146]
train() client id: f_00005-6-0 loss: 0.454648  [   32/  146]
train() client id: f_00005-6-1 loss: 0.448507  [   64/  146]
train() client id: f_00005-6-2 loss: 0.384074  [   96/  146]
train() client id: f_00005-6-3 loss: 0.771911  [  128/  146]
train() client id: f_00005-7-0 loss: 0.609996  [   32/  146]
train() client id: f_00005-7-1 loss: 0.522459  [   64/  146]
train() client id: f_00005-7-2 loss: 0.420859  [   96/  146]
train() client id: f_00005-7-3 loss: 0.654807  [  128/  146]
train() client id: f_00005-8-0 loss: 0.459343  [   32/  146]
train() client id: f_00005-8-1 loss: 0.512480  [   64/  146]
train() client id: f_00005-8-2 loss: 0.581263  [   96/  146]
train() client id: f_00005-8-3 loss: 0.662172  [  128/  146]
train() client id: f_00005-9-0 loss: 0.606998  [   32/  146]
train() client id: f_00005-9-1 loss: 0.446795  [   64/  146]
train() client id: f_00005-9-2 loss: 0.511680  [   96/  146]
train() client id: f_00005-9-3 loss: 0.554402  [  128/  146]
train() client id: f_00005-10-0 loss: 0.472954  [   32/  146]
train() client id: f_00005-10-1 loss: 0.397447  [   64/  146]
train() client id: f_00005-10-2 loss: 0.534744  [   96/  146]
train() client id: f_00005-10-3 loss: 0.520651  [  128/  146]
train() client id: f_00005-11-0 loss: 0.410047  [   32/  146]
train() client id: f_00005-11-1 loss: 0.495969  [   64/  146]
train() client id: f_00005-11-2 loss: 0.412467  [   96/  146]
train() client id: f_00005-11-3 loss: 0.622177  [  128/  146]
train() client id: f_00006-0-0 loss: 0.583136  [   32/   54]
train() client id: f_00006-1-0 loss: 0.584173  [   32/   54]
train() client id: f_00006-2-0 loss: 0.543095  [   32/   54]
train() client id: f_00006-3-0 loss: 0.549583  [   32/   54]
train() client id: f_00006-4-0 loss: 0.534499  [   32/   54]
train() client id: f_00006-5-0 loss: 0.584459  [   32/   54]
train() client id: f_00006-6-0 loss: 0.627090  [   32/   54]
train() client id: f_00006-7-0 loss: 0.601502  [   32/   54]
train() client id: f_00006-8-0 loss: 0.640807  [   32/   54]
train() client id: f_00006-9-0 loss: 0.580568  [   32/   54]
train() client id: f_00006-10-0 loss: 0.578555  [   32/   54]
train() client id: f_00006-11-0 loss: 0.633505  [   32/   54]
train() client id: f_00007-0-0 loss: 0.746505  [   32/  179]
train() client id: f_00007-0-1 loss: 0.656827  [   64/  179]
train() client id: f_00007-0-2 loss: 0.795616  [   96/  179]
train() client id: f_00007-0-3 loss: 0.762182  [  128/  179]
train() client id: f_00007-0-4 loss: 0.776383  [  160/  179]
train() client id: f_00007-1-0 loss: 0.790187  [   32/  179]
train() client id: f_00007-1-1 loss: 0.687329  [   64/  179]
train() client id: f_00007-1-2 loss: 0.728820  [   96/  179]
train() client id: f_00007-1-3 loss: 0.705596  [  128/  179]
train() client id: f_00007-1-4 loss: 0.780372  [  160/  179]
train() client id: f_00007-2-0 loss: 0.601822  [   32/  179]
train() client id: f_00007-2-1 loss: 0.780887  [   64/  179]
train() client id: f_00007-2-2 loss: 0.786322  [   96/  179]
train() client id: f_00007-2-3 loss: 0.727101  [  128/  179]
train() client id: f_00007-2-4 loss: 0.744795  [  160/  179]
train() client id: f_00007-3-0 loss: 0.733350  [   32/  179]
train() client id: f_00007-3-1 loss: 0.695452  [   64/  179]
train() client id: f_00007-3-2 loss: 0.681401  [   96/  179]
train() client id: f_00007-3-3 loss: 0.697713  [  128/  179]
train() client id: f_00007-3-4 loss: 0.805173  [  160/  179]
train() client id: f_00007-4-0 loss: 0.694497  [   32/  179]
train() client id: f_00007-4-1 loss: 0.628847  [   64/  179]
train() client id: f_00007-4-2 loss: 0.753023  [   96/  179]
train() client id: f_00007-4-3 loss: 0.628989  [  128/  179]
train() client id: f_00007-4-4 loss: 0.860988  [  160/  179]
train() client id: f_00007-5-0 loss: 0.739204  [   32/  179]
train() client id: f_00007-5-1 loss: 0.785440  [   64/  179]
train() client id: f_00007-5-2 loss: 0.741111  [   96/  179]
train() client id: f_00007-5-3 loss: 0.754608  [  128/  179]
train() client id: f_00007-5-4 loss: 0.632640  [  160/  179]
train() client id: f_00007-6-0 loss: 0.759971  [   32/  179]
train() client id: f_00007-6-1 loss: 0.681232  [   64/  179]
train() client id: f_00007-6-2 loss: 0.788112  [   96/  179]
train() client id: f_00007-6-3 loss: 0.743271  [  128/  179]
train() client id: f_00007-6-4 loss: 0.660676  [  160/  179]
train() client id: f_00007-7-0 loss: 0.619269  [   32/  179]
train() client id: f_00007-7-1 loss: 0.602972  [   64/  179]
train() client id: f_00007-7-2 loss: 0.783290  [   96/  179]
train() client id: f_00007-7-3 loss: 0.852736  [  128/  179]
train() client id: f_00007-7-4 loss: 0.650764  [  160/  179]
train() client id: f_00007-8-0 loss: 0.662849  [   32/  179]
train() client id: f_00007-8-1 loss: 0.600132  [   64/  179]
train() client id: f_00007-8-2 loss: 0.689701  [   96/  179]
train() client id: f_00007-8-3 loss: 0.775281  [  128/  179]
train() client id: f_00007-8-4 loss: 0.683521  [  160/  179]
train() client id: f_00007-9-0 loss: 0.615369  [   32/  179]
train() client id: f_00007-9-1 loss: 0.770174  [   64/  179]
train() client id: f_00007-9-2 loss: 0.705275  [   96/  179]
train() client id: f_00007-9-3 loss: 0.836091  [  128/  179]
train() client id: f_00007-9-4 loss: 0.728877  [  160/  179]
train() client id: f_00007-10-0 loss: 0.755626  [   32/  179]
train() client id: f_00007-10-1 loss: 0.691551  [   64/  179]
train() client id: f_00007-10-2 loss: 0.592925  [   96/  179]
train() client id: f_00007-10-3 loss: 0.863442  [  128/  179]
train() client id: f_00007-10-4 loss: 0.758362  [  160/  179]
train() client id: f_00007-11-0 loss: 0.692900  [   32/  179]
train() client id: f_00007-11-1 loss: 0.748954  [   64/  179]
train() client id: f_00007-11-2 loss: 0.849520  [   96/  179]
train() client id: f_00007-11-3 loss: 0.594288  [  128/  179]
train() client id: f_00007-11-4 loss: 0.686833  [  160/  179]
train() client id: f_00008-0-0 loss: 0.768156  [   32/  130]
train() client id: f_00008-0-1 loss: 0.742535  [   64/  130]
train() client id: f_00008-0-2 loss: 0.846578  [   96/  130]
train() client id: f_00008-0-3 loss: 0.862030  [  128/  130]
train() client id: f_00008-1-0 loss: 0.768150  [   32/  130]
train() client id: f_00008-1-1 loss: 0.859839  [   64/  130]
train() client id: f_00008-1-2 loss: 0.909002  [   96/  130]
train() client id: f_00008-1-3 loss: 0.701826  [  128/  130]
train() client id: f_00008-2-0 loss: 0.711445  [   32/  130]
train() client id: f_00008-2-1 loss: 0.844307  [   64/  130]
train() client id: f_00008-2-2 loss: 0.788248  [   96/  130]
train() client id: f_00008-2-3 loss: 0.889112  [  128/  130]
train() client id: f_00008-3-0 loss: 0.787200  [   32/  130]
train() client id: f_00008-3-1 loss: 0.806739  [   64/  130]
train() client id: f_00008-3-2 loss: 0.834473  [   96/  130]
train() client id: f_00008-3-3 loss: 0.803278  [  128/  130]
train() client id: f_00008-4-0 loss: 0.742813  [   32/  130]
train() client id: f_00008-4-1 loss: 0.926318  [   64/  130]
train() client id: f_00008-4-2 loss: 0.759750  [   96/  130]
train() client id: f_00008-4-3 loss: 0.800343  [  128/  130]
train() client id: f_00008-5-0 loss: 0.709344  [   32/  130]
train() client id: f_00008-5-1 loss: 0.788858  [   64/  130]
train() client id: f_00008-5-2 loss: 1.014193  [   96/  130]
train() client id: f_00008-5-3 loss: 0.677016  [  128/  130]
train() client id: f_00008-6-0 loss: 0.767290  [   32/  130]
train() client id: f_00008-6-1 loss: 0.784039  [   64/  130]
train() client id: f_00008-6-2 loss: 0.828215  [   96/  130]
train() client id: f_00008-6-3 loss: 0.810942  [  128/  130]
train() client id: f_00008-7-0 loss: 0.737864  [   32/  130]
train() client id: f_00008-7-1 loss: 0.755856  [   64/  130]
train() client id: f_00008-7-2 loss: 0.846407  [   96/  130]
train() client id: f_00008-7-3 loss: 0.895048  [  128/  130]
train() client id: f_00008-8-0 loss: 0.831287  [   32/  130]
train() client id: f_00008-8-1 loss: 0.827543  [   64/  130]
train() client id: f_00008-8-2 loss: 0.890591  [   96/  130]
train() client id: f_00008-8-3 loss: 0.676076  [  128/  130]
train() client id: f_00008-9-0 loss: 0.746237  [   32/  130]
train() client id: f_00008-9-1 loss: 0.777149  [   64/  130]
train() client id: f_00008-9-2 loss: 0.824878  [   96/  130]
train() client id: f_00008-9-3 loss: 0.840715  [  128/  130]
train() client id: f_00008-10-0 loss: 0.833610  [   32/  130]
train() client id: f_00008-10-1 loss: 0.691835  [   64/  130]
train() client id: f_00008-10-2 loss: 0.918147  [   96/  130]
train() client id: f_00008-10-3 loss: 0.792108  [  128/  130]
train() client id: f_00008-11-0 loss: 0.739344  [   32/  130]
train() client id: f_00008-11-1 loss: 0.771209  [   64/  130]
train() client id: f_00008-11-2 loss: 0.932134  [   96/  130]
train() client id: f_00008-11-3 loss: 0.793775  [  128/  130]
train() client id: f_00009-0-0 loss: 1.142183  [   32/  118]
train() client id: f_00009-0-1 loss: 1.193224  [   64/  118]
train() client id: f_00009-0-2 loss: 1.224893  [   96/  118]
train() client id: f_00009-1-0 loss: 0.996060  [   32/  118]
train() client id: f_00009-1-1 loss: 1.130262  [   64/  118]
train() client id: f_00009-1-2 loss: 1.186987  [   96/  118]
train() client id: f_00009-2-0 loss: 1.036070  [   32/  118]
train() client id: f_00009-2-1 loss: 1.022276  [   64/  118]
train() client id: f_00009-2-2 loss: 1.110964  [   96/  118]
train() client id: f_00009-3-0 loss: 0.963030  [   32/  118]
train() client id: f_00009-3-1 loss: 1.009317  [   64/  118]
train() client id: f_00009-3-2 loss: 1.095222  [   96/  118]
train() client id: f_00009-4-0 loss: 1.104042  [   32/  118]
train() client id: f_00009-4-1 loss: 0.907921  [   64/  118]
train() client id: f_00009-4-2 loss: 1.023543  [   96/  118]
train() client id: f_00009-5-0 loss: 1.166582  [   32/  118]
train() client id: f_00009-5-1 loss: 0.952986  [   64/  118]
train() client id: f_00009-5-2 loss: 0.867605  [   96/  118]
train() client id: f_00009-6-0 loss: 0.865055  [   32/  118]
train() client id: f_00009-6-1 loss: 0.981920  [   64/  118]
train() client id: f_00009-6-2 loss: 0.939204  [   96/  118]
train() client id: f_00009-7-0 loss: 0.864144  [   32/  118]
train() client id: f_00009-7-1 loss: 0.965990  [   64/  118]
train() client id: f_00009-7-2 loss: 0.981358  [   96/  118]
train() client id: f_00009-8-0 loss: 0.792313  [   32/  118]
train() client id: f_00009-8-1 loss: 1.056119  [   64/  118]
train() client id: f_00009-8-2 loss: 0.917288  [   96/  118]
train() client id: f_00009-9-0 loss: 0.852202  [   32/  118]
train() client id: f_00009-9-1 loss: 0.936105  [   64/  118]
train() client id: f_00009-9-2 loss: 0.979161  [   96/  118]
train() client id: f_00009-10-0 loss: 0.954600  [   32/  118]
train() client id: f_00009-10-1 loss: 0.943675  [   64/  118]
train() client id: f_00009-10-2 loss: 0.834143  [   96/  118]
train() client id: f_00009-11-0 loss: 0.905675  [   32/  118]
train() client id: f_00009-11-1 loss: 0.953238  [   64/  118]
train() client id: f_00009-11-2 loss: 0.876336  [   96/  118]
At round 13 accuracy: 0.6312997347480106
At round 13 training accuracy: 0.5714285714285714
At round 13 training loss: 0.8579368174817308
update_location
xs = [ -3.9056584    4.20031788  85.00902392  18.81129433   0.97929623
   3.95640986 -47.44319194 -26.32485185  69.66397685 -12.06087855]
ys = [ 77.5879595   60.55583871   1.32061395 -47.45517586  39.35018685
  22.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [126.62995549 116.98141849 131.25653572 112.27581445 107.46811725
 102.64569476 110.71470997 103.41022232 123.13301606 100.80414995]
dists_bs = [196.88163795 212.62695027 312.6807024  295.04088374 222.19267429
 234.91685624 218.6809876  229.00251994 290.93810241 236.19832881]
uav_gains = [5.54132201e-11 6.75605000e-11 5.06550612e-11 7.48645336e-11
 8.35211431e-11 9.36797744e-11 7.75317969e-11 9.19578362e-11
 5.94337263e-11 9.80171753e-11]
bs_gains = [4.16408405e-11 3.35708871e-11 1.14027884e-11 1.34160270e-11
 2.96791066e-11 2.53941909e-11 3.10329607e-11 2.72735301e-11
 1.39525118e-11 2.50103050e-11]
Round 14
-------------------------------
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.14357806 19.06538973  9.00414718  3.22065168 21.99169094 10.5993586
  4.00331986 12.90419715  9.4917528   8.60518442]
obj_prev = 108.02927042739734
eta_min = 6.894580279323513e-11	eta_max = 0.920963466545226
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 25.120031493789718	eta = 0.9090909090909091
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 43.56050107516789	eta = 0.5242453990066629
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 34.74720289395531	eta = 0.6572152681404528
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.166400765192755	eta = 0.6885399603277947
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08814089362885	eta = 0.6901684908951391
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08793538155577	eta = 0.6901727775922599
eta = 0.6901727775922599
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.03059863 0.06435425 0.03011293 0.01044239 0.07431095 0.03545555
 0.0131137  0.04346948 0.03157001 0.02865586]
ene_total = [2.84199716 5.44950306 2.81511635 1.29013739 6.21774054 3.30722781
 1.48846279 3.76427446 3.1172366  2.79623923]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 0 obj = 5.8017097996113725
eta = 0.6901727775922599
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
eta_min = 0.6901727775922659	eta_max = 0.6901727775922564
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 0.041597769241550876	eta = 0.9090909090909091
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 19.658200800192578	eta = 0.0019236833645317572
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 2.023882036535733	eta = 0.01868495948542783
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708396260580614	eta = 0.01918783921124654
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708261485414031	eta = 0.01918797042749958
eta = 0.01918797042749958
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.89202933e-04 1.85088870e-03 1.81815891e-04 7.33300812e-06
 2.89081782e-03 3.20149601e-04 1.44834795e-05 5.20846430e-04
 2.47774470e-04 1.69356454e-04]
ene_total = [0.17265704 0.23107452 0.17557065 0.15872099 0.26113616 0.20692952
 0.15784421 0.16500284 0.23783644 0.20405377]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 1 obj = 5.801709799611484
eta = 0.6901727775922659
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
Done!
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.75381754e-05 1.71568221e-04 1.68534331e-05 6.79733555e-07
 2.67964503e-04 2.96762833e-05 1.34254685e-06 4.82798860e-05
 2.29674669e-05 1.56985050e-05]
ene_total = [0.0070693  0.00801158 0.00719819 0.00664985 0.00832878 0.00838782
 0.00660659 0.00644739 0.00974968 0.00840403]
At round 14 energy consumption: 0.0768532209744656
At round 14 eta: 0.6901727775922659
At round 14 a_n: 23.386960998683932
At round 14 local rounds: 12.142311740979952
At round 14 global rounds: 75.48388039288098
gradient difference: 0.4204351007938385
train() client id: f_00000-0-0 loss: 1.508660  [   32/  126]
train() client id: f_00000-0-1 loss: 1.396351  [   64/  126]
train() client id: f_00000-0-2 loss: 1.237830  [   96/  126]
train() client id: f_00000-1-0 loss: 1.127936  [   32/  126]
train() client id: f_00000-1-1 loss: 1.364501  [   64/  126]
train() client id: f_00000-1-2 loss: 1.296818  [   96/  126]
train() client id: f_00000-2-0 loss: 1.162106  [   32/  126]
train() client id: f_00000-2-1 loss: 1.201734  [   64/  126]
train() client id: f_00000-2-2 loss: 1.123223  [   96/  126]
train() client id: f_00000-3-0 loss: 1.095084  [   32/  126]
train() client id: f_00000-3-1 loss: 1.103278  [   64/  126]
train() client id: f_00000-3-2 loss: 1.030812  [   96/  126]
train() client id: f_00000-4-0 loss: 0.988817  [   32/  126]
train() client id: f_00000-4-1 loss: 1.029990  [   64/  126]
train() client id: f_00000-4-2 loss: 0.996383  [   96/  126]
train() client id: f_00000-5-0 loss: 0.990963  [   32/  126]
train() client id: f_00000-5-1 loss: 0.906485  [   64/  126]
train() client id: f_00000-5-2 loss: 0.961132  [   96/  126]
train() client id: f_00000-6-0 loss: 0.949115  [   32/  126]
train() client id: f_00000-6-1 loss: 0.938774  [   64/  126]
train() client id: f_00000-6-2 loss: 0.951940  [   96/  126]
train() client id: f_00000-7-0 loss: 0.930299  [   32/  126]
train() client id: f_00000-7-1 loss: 0.985972  [   64/  126]
train() client id: f_00000-7-2 loss: 0.896762  [   96/  126]
train() client id: f_00000-8-0 loss: 0.962883  [   32/  126]
train() client id: f_00000-8-1 loss: 0.889237  [   64/  126]
train() client id: f_00000-8-2 loss: 0.957072  [   96/  126]
train() client id: f_00000-9-0 loss: 0.893370  [   32/  126]
train() client id: f_00000-9-1 loss: 0.831436  [   64/  126]
train() client id: f_00000-9-2 loss: 0.933802  [   96/  126]
train() client id: f_00000-10-0 loss: 0.831506  [   32/  126]
train() client id: f_00000-10-1 loss: 0.860678  [   64/  126]
train() client id: f_00000-10-2 loss: 0.858597  [   96/  126]
train() client id: f_00000-11-0 loss: 0.807533  [   32/  126]
train() client id: f_00000-11-1 loss: 0.810230  [   64/  126]
train() client id: f_00000-11-2 loss: 0.989549  [   96/  126]
train() client id: f_00001-0-0 loss: 0.476461  [   32/  265]
train() client id: f_00001-0-1 loss: 0.636745  [   64/  265]
train() client id: f_00001-0-2 loss: 0.551866  [   96/  265]
train() client id: f_00001-0-3 loss: 0.545648  [  128/  265]
train() client id: f_00001-0-4 loss: 0.585461  [  160/  265]
train() client id: f_00001-0-5 loss: 0.570068  [  192/  265]
train() client id: f_00001-0-6 loss: 0.564606  [  224/  265]
train() client id: f_00001-0-7 loss: 0.551341  [  256/  265]
train() client id: f_00001-1-0 loss: 0.576789  [   32/  265]
train() client id: f_00001-1-1 loss: 0.575324  [   64/  265]
train() client id: f_00001-1-2 loss: 0.599604  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482747  [  128/  265]
train() client id: f_00001-1-4 loss: 0.547652  [  160/  265]
train() client id: f_00001-1-5 loss: 0.525207  [  192/  265]
train() client id: f_00001-1-6 loss: 0.506533  [  224/  265]
train() client id: f_00001-1-7 loss: 0.562972  [  256/  265]
train() client id: f_00001-2-0 loss: 0.621240  [   32/  265]
train() client id: f_00001-2-1 loss: 0.529179  [   64/  265]
train() client id: f_00001-2-2 loss: 0.531905  [   96/  265]
train() client id: f_00001-2-3 loss: 0.507326  [  128/  265]
train() client id: f_00001-2-4 loss: 0.498374  [  160/  265]
train() client id: f_00001-2-5 loss: 0.560505  [  192/  265]
train() client id: f_00001-2-6 loss: 0.620618  [  224/  265]
train() client id: f_00001-2-7 loss: 0.483916  [  256/  265]
train() client id: f_00001-3-0 loss: 0.517662  [   32/  265]
train() client id: f_00001-3-1 loss: 0.508078  [   64/  265]
train() client id: f_00001-3-2 loss: 0.459403  [   96/  265]
train() client id: f_00001-3-3 loss: 0.584642  [  128/  265]
train() client id: f_00001-3-4 loss: 0.608485  [  160/  265]
train() client id: f_00001-3-5 loss: 0.558274  [  192/  265]
train() client id: f_00001-3-6 loss: 0.471531  [  224/  265]
train() client id: f_00001-3-7 loss: 0.565083  [  256/  265]
train() client id: f_00001-4-0 loss: 0.455344  [   32/  265]
train() client id: f_00001-4-1 loss: 0.501852  [   64/  265]
train() client id: f_00001-4-2 loss: 0.606416  [   96/  265]
train() client id: f_00001-4-3 loss: 0.582908  [  128/  265]
train() client id: f_00001-4-4 loss: 0.577588  [  160/  265]
train() client id: f_00001-4-5 loss: 0.521174  [  192/  265]
train() client id: f_00001-4-6 loss: 0.522837  [  224/  265]
train() client id: f_00001-4-7 loss: 0.513482  [  256/  265]
train() client id: f_00001-5-0 loss: 0.502545  [   32/  265]
train() client id: f_00001-5-1 loss: 0.507408  [   64/  265]
train() client id: f_00001-5-2 loss: 0.655049  [   96/  265]
train() client id: f_00001-5-3 loss: 0.568011  [  128/  265]
train() client id: f_00001-5-4 loss: 0.478700  [  160/  265]
train() client id: f_00001-5-5 loss: 0.436551  [  192/  265]
train() client id: f_00001-5-6 loss: 0.530724  [  224/  265]
train() client id: f_00001-5-7 loss: 0.559031  [  256/  265]
train() client id: f_00001-6-0 loss: 0.508091  [   32/  265]
train() client id: f_00001-6-1 loss: 0.447607  [   64/  265]
train() client id: f_00001-6-2 loss: 0.438823  [   96/  265]
train() client id: f_00001-6-3 loss: 0.502721  [  128/  265]
train() client id: f_00001-6-4 loss: 0.513708  [  160/  265]
train() client id: f_00001-6-5 loss: 0.623430  [  192/  265]
train() client id: f_00001-6-6 loss: 0.522473  [  224/  265]
train() client id: f_00001-6-7 loss: 0.634936  [  256/  265]
train() client id: f_00001-7-0 loss: 0.655260  [   32/  265]
train() client id: f_00001-7-1 loss: 0.493674  [   64/  265]
train() client id: f_00001-7-2 loss: 0.439119  [   96/  265]
train() client id: f_00001-7-3 loss: 0.507957  [  128/  265]
train() client id: f_00001-7-4 loss: 0.578054  [  160/  265]
train() client id: f_00001-7-5 loss: 0.607801  [  192/  265]
train() client id: f_00001-7-6 loss: 0.426064  [  224/  265]
train() client id: f_00001-7-7 loss: 0.446944  [  256/  265]
train() client id: f_00001-8-0 loss: 0.498419  [   32/  265]
train() client id: f_00001-8-1 loss: 0.454517  [   64/  265]
train() client id: f_00001-8-2 loss: 0.431326  [   96/  265]
train() client id: f_00001-8-3 loss: 0.549351  [  128/  265]
train() client id: f_00001-8-4 loss: 0.562973  [  160/  265]
train() client id: f_00001-8-5 loss: 0.621202  [  192/  265]
train() client id: f_00001-8-6 loss: 0.594181  [  224/  265]
train() client id: f_00001-8-7 loss: 0.509130  [  256/  265]
train() client id: f_00001-9-0 loss: 0.509349  [   32/  265]
train() client id: f_00001-9-1 loss: 0.602810  [   64/  265]
train() client id: f_00001-9-2 loss: 0.617817  [   96/  265]
train() client id: f_00001-9-3 loss: 0.478825  [  128/  265]
train() client id: f_00001-9-4 loss: 0.605964  [  160/  265]
train() client id: f_00001-9-5 loss: 0.493355  [  192/  265]
train() client id: f_00001-9-6 loss: 0.429681  [  224/  265]
train() client id: f_00001-9-7 loss: 0.484278  [  256/  265]
train() client id: f_00001-10-0 loss: 0.518478  [   32/  265]
train() client id: f_00001-10-1 loss: 0.649552  [   64/  265]
train() client id: f_00001-10-2 loss: 0.555877  [   96/  265]
train() client id: f_00001-10-3 loss: 0.504595  [  128/  265]
train() client id: f_00001-10-4 loss: 0.434790  [  160/  265]
train() client id: f_00001-10-5 loss: 0.437150  [  192/  265]
train() client id: f_00001-10-6 loss: 0.614264  [  224/  265]
train() client id: f_00001-10-7 loss: 0.501855  [  256/  265]
train() client id: f_00001-11-0 loss: 0.439110  [   32/  265]
train() client id: f_00001-11-1 loss: 0.479324  [   64/  265]
train() client id: f_00001-11-2 loss: 0.734660  [   96/  265]
train() client id: f_00001-11-3 loss: 0.491452  [  128/  265]
train() client id: f_00001-11-4 loss: 0.487866  [  160/  265]
train() client id: f_00001-11-5 loss: 0.472005  [  192/  265]
train() client id: f_00001-11-6 loss: 0.546585  [  224/  265]
train() client id: f_00001-11-7 loss: 0.570608  [  256/  265]
train() client id: f_00002-0-0 loss: 1.353718  [   32/  124]
train() client id: f_00002-0-1 loss: 1.104869  [   64/  124]
train() client id: f_00002-0-2 loss: 1.091361  [   96/  124]
train() client id: f_00002-1-0 loss: 1.168362  [   32/  124]
train() client id: f_00002-1-1 loss: 1.141970  [   64/  124]
train() client id: f_00002-1-2 loss: 1.112846  [   96/  124]
train() client id: f_00002-2-0 loss: 1.167827  [   32/  124]
train() client id: f_00002-2-1 loss: 1.116094  [   64/  124]
train() client id: f_00002-2-2 loss: 1.128103  [   96/  124]
train() client id: f_00002-3-0 loss: 1.126584  [   32/  124]
train() client id: f_00002-3-1 loss: 1.124868  [   64/  124]
train() client id: f_00002-3-2 loss: 1.017483  [   96/  124]
train() client id: f_00002-4-0 loss: 1.032048  [   32/  124]
train() client id: f_00002-4-1 loss: 1.063097  [   64/  124]
train() client id: f_00002-4-2 loss: 1.229157  [   96/  124]
train() client id: f_00002-5-0 loss: 1.118981  [   32/  124]
train() client id: f_00002-5-1 loss: 1.087985  [   64/  124]
train() client id: f_00002-5-2 loss: 1.044500  [   96/  124]
train() client id: f_00002-6-0 loss: 1.077807  [   32/  124]
train() client id: f_00002-6-1 loss: 1.064494  [   64/  124]
train() client id: f_00002-6-2 loss: 1.036343  [   96/  124]
train() client id: f_00002-7-0 loss: 0.995765  [   32/  124]
train() client id: f_00002-7-1 loss: 1.121221  [   64/  124]
train() client id: f_00002-7-2 loss: 0.973581  [   96/  124]
train() client id: f_00002-8-0 loss: 1.095100  [   32/  124]
train() client id: f_00002-8-1 loss: 0.989283  [   64/  124]
train() client id: f_00002-8-2 loss: 1.137139  [   96/  124]
train() client id: f_00002-9-0 loss: 1.013419  [   32/  124]
train() client id: f_00002-9-1 loss: 1.131415  [   64/  124]
train() client id: f_00002-9-2 loss: 1.075548  [   96/  124]
train() client id: f_00002-10-0 loss: 1.099700  [   32/  124]
train() client id: f_00002-10-1 loss: 1.020593  [   64/  124]
train() client id: f_00002-10-2 loss: 1.085173  [   96/  124]
train() client id: f_00002-11-0 loss: 0.995251  [   32/  124]
train() client id: f_00002-11-1 loss: 1.026518  [   64/  124]
train() client id: f_00002-11-2 loss: 1.112921  [   96/  124]
train() client id: f_00003-0-0 loss: 0.796948  [   32/   43]
train() client id: f_00003-1-0 loss: 0.819084  [   32/   43]
train() client id: f_00003-2-0 loss: 0.648289  [   32/   43]
train() client id: f_00003-3-0 loss: 0.769953  [   32/   43]
train() client id: f_00003-4-0 loss: 0.676566  [   32/   43]
train() client id: f_00003-5-0 loss: 0.804035  [   32/   43]
train() client id: f_00003-6-0 loss: 0.862075  [   32/   43]
train() client id: f_00003-7-0 loss: 0.780183  [   32/   43]
train() client id: f_00003-8-0 loss: 0.816284  [   32/   43]
train() client id: f_00003-9-0 loss: 0.856880  [   32/   43]
train() client id: f_00003-10-0 loss: 0.755101  [   32/   43]
train() client id: f_00003-11-0 loss: 0.708355  [   32/   43]
train() client id: f_00004-0-0 loss: 0.700561  [   32/  306]
train() client id: f_00004-0-1 loss: 0.912670  [   64/  306]
train() client id: f_00004-0-2 loss: 0.798342  [   96/  306]
train() client id: f_00004-0-3 loss: 1.019793  [  128/  306]
train() client id: f_00004-0-4 loss: 1.071193  [  160/  306]
train() client id: f_00004-0-5 loss: 0.752339  [  192/  306]
train() client id: f_00004-0-6 loss: 0.919329  [  224/  306]
train() client id: f_00004-0-7 loss: 0.883841  [  256/  306]
train() client id: f_00004-0-8 loss: 0.838668  [  288/  306]
train() client id: f_00004-1-0 loss: 0.891998  [   32/  306]
train() client id: f_00004-1-1 loss: 0.948341  [   64/  306]
train() client id: f_00004-1-2 loss: 0.755012  [   96/  306]
train() client id: f_00004-1-3 loss: 1.025248  [  128/  306]
train() client id: f_00004-1-4 loss: 0.852722  [  160/  306]
train() client id: f_00004-1-5 loss: 0.760085  [  192/  306]
train() client id: f_00004-1-6 loss: 0.948164  [  224/  306]
train() client id: f_00004-1-7 loss: 0.922643  [  256/  306]
train() client id: f_00004-1-8 loss: 0.898968  [  288/  306]
train() client id: f_00004-2-0 loss: 0.871115  [   32/  306]
train() client id: f_00004-2-1 loss: 0.916057  [   64/  306]
train() client id: f_00004-2-2 loss: 0.793616  [   96/  306]
train() client id: f_00004-2-3 loss: 0.893926  [  128/  306]
train() client id: f_00004-2-4 loss: 0.914579  [  160/  306]
train() client id: f_00004-2-5 loss: 0.873921  [  192/  306]
train() client id: f_00004-2-6 loss: 0.910525  [  224/  306]
train() client id: f_00004-2-7 loss: 0.804050  [  256/  306]
train() client id: f_00004-2-8 loss: 0.908502  [  288/  306]
train() client id: f_00004-3-0 loss: 0.800145  [   32/  306]
train() client id: f_00004-3-1 loss: 0.909823  [   64/  306]
train() client id: f_00004-3-2 loss: 0.882587  [   96/  306]
train() client id: f_00004-3-3 loss: 0.701507  [  128/  306]
train() client id: f_00004-3-4 loss: 0.937546  [  160/  306]
train() client id: f_00004-3-5 loss: 1.042823  [  192/  306]
train() client id: f_00004-3-6 loss: 0.948817  [  224/  306]
train() client id: f_00004-3-7 loss: 0.820840  [  256/  306]
train() client id: f_00004-3-8 loss: 0.855667  [  288/  306]
train() client id: f_00004-4-0 loss: 1.034746  [   32/  306]
train() client id: f_00004-4-1 loss: 0.927364  [   64/  306]
train() client id: f_00004-4-2 loss: 0.870028  [   96/  306]
train() client id: f_00004-4-3 loss: 0.851274  [  128/  306]
train() client id: f_00004-4-4 loss: 0.895465  [  160/  306]
train() client id: f_00004-4-5 loss: 0.857126  [  192/  306]
train() client id: f_00004-4-6 loss: 0.872804  [  224/  306]
train() client id: f_00004-4-7 loss: 0.762010  [  256/  306]
train() client id: f_00004-4-8 loss: 0.816760  [  288/  306]
train() client id: f_00004-5-0 loss: 0.847286  [   32/  306]
train() client id: f_00004-5-1 loss: 0.984664  [   64/  306]
train() client id: f_00004-5-2 loss: 0.854304  [   96/  306]
train() client id: f_00004-5-3 loss: 0.983131  [  128/  306]
train() client id: f_00004-5-4 loss: 0.864819  [  160/  306]
train() client id: f_00004-5-5 loss: 0.803508  [  192/  306]
train() client id: f_00004-5-6 loss: 0.887970  [  224/  306]
train() client id: f_00004-5-7 loss: 0.781347  [  256/  306]
train() client id: f_00004-5-8 loss: 0.886327  [  288/  306]
train() client id: f_00004-6-0 loss: 0.910452  [   32/  306]
train() client id: f_00004-6-1 loss: 0.890294  [   64/  306]
train() client id: f_00004-6-2 loss: 0.755662  [   96/  306]
train() client id: f_00004-6-3 loss: 0.957857  [  128/  306]
train() client id: f_00004-6-4 loss: 0.893418  [  160/  306]
train() client id: f_00004-6-5 loss: 0.922825  [  192/  306]
train() client id: f_00004-6-6 loss: 0.880641  [  224/  306]
train() client id: f_00004-6-7 loss: 0.770365  [  256/  306]
train() client id: f_00004-6-8 loss: 0.907384  [  288/  306]
train() client id: f_00004-7-0 loss: 0.830189  [   32/  306]
train() client id: f_00004-7-1 loss: 0.961712  [   64/  306]
train() client id: f_00004-7-2 loss: 0.807917  [   96/  306]
train() client id: f_00004-7-3 loss: 0.959115  [  128/  306]
train() client id: f_00004-7-4 loss: 0.841544  [  160/  306]
train() client id: f_00004-7-5 loss: 0.840578  [  192/  306]
train() client id: f_00004-7-6 loss: 0.920278  [  224/  306]
train() client id: f_00004-7-7 loss: 0.791273  [  256/  306]
train() client id: f_00004-7-8 loss: 0.822423  [  288/  306]
train() client id: f_00004-8-0 loss: 0.919183  [   32/  306]
train() client id: f_00004-8-1 loss: 0.874829  [   64/  306]
train() client id: f_00004-8-2 loss: 0.873737  [   96/  306]
train() client id: f_00004-8-3 loss: 0.870729  [  128/  306]
train() client id: f_00004-8-4 loss: 0.862393  [  160/  306]
train() client id: f_00004-8-5 loss: 0.822007  [  192/  306]
train() client id: f_00004-8-6 loss: 0.892140  [  224/  306]
train() client id: f_00004-8-7 loss: 0.941680  [  256/  306]
train() client id: f_00004-8-8 loss: 0.898305  [  288/  306]
train() client id: f_00004-9-0 loss: 0.829082  [   32/  306]
train() client id: f_00004-9-1 loss: 0.926323  [   64/  306]
train() client id: f_00004-9-2 loss: 0.855264  [   96/  306]
train() client id: f_00004-9-3 loss: 0.877902  [  128/  306]
train() client id: f_00004-9-4 loss: 0.971366  [  160/  306]
train() client id: f_00004-9-5 loss: 0.841912  [  192/  306]
train() client id: f_00004-9-6 loss: 0.938102  [  224/  306]
train() client id: f_00004-9-7 loss: 0.803702  [  256/  306]
train() client id: f_00004-9-8 loss: 0.845650  [  288/  306]
train() client id: f_00004-10-0 loss: 0.920934  [   32/  306]
train() client id: f_00004-10-1 loss: 0.888272  [   64/  306]
train() client id: f_00004-10-2 loss: 0.796711  [   96/  306]
train() client id: f_00004-10-3 loss: 0.936776  [  128/  306]
train() client id: f_00004-10-4 loss: 0.941226  [  160/  306]
train() client id: f_00004-10-5 loss: 0.964729  [  192/  306]
train() client id: f_00004-10-6 loss: 0.810128  [  224/  306]
train() client id: f_00004-10-7 loss: 0.812475  [  256/  306]
train() client id: f_00004-10-8 loss: 0.831138  [  288/  306]
train() client id: f_00004-11-0 loss: 0.862841  [   32/  306]
train() client id: f_00004-11-1 loss: 0.740273  [   64/  306]
train() client id: f_00004-11-2 loss: 0.912639  [   96/  306]
train() client id: f_00004-11-3 loss: 0.990256  [  128/  306]
train() client id: f_00004-11-4 loss: 0.776003  [  160/  306]
train() client id: f_00004-11-5 loss: 0.960001  [  192/  306]
train() client id: f_00004-11-6 loss: 0.802667  [  224/  306]
train() client id: f_00004-11-7 loss: 0.912526  [  256/  306]
train() client id: f_00004-11-8 loss: 0.955946  [  288/  306]
train() client id: f_00005-0-0 loss: 0.501376  [   32/  146]
train() client id: f_00005-0-1 loss: 0.676702  [   64/  146]
train() client id: f_00005-0-2 loss: 0.677794  [   96/  146]
train() client id: f_00005-0-3 loss: 0.617925  [  128/  146]
train() client id: f_00005-1-0 loss: 0.808027  [   32/  146]
train() client id: f_00005-1-1 loss: 0.491020  [   64/  146]
train() client id: f_00005-1-2 loss: 0.632571  [   96/  146]
train() client id: f_00005-1-3 loss: 0.565740  [  128/  146]
train() client id: f_00005-2-0 loss: 0.549480  [   32/  146]
train() client id: f_00005-2-1 loss: 0.615283  [   64/  146]
train() client id: f_00005-2-2 loss: 0.749450  [   96/  146]
train() client id: f_00005-2-3 loss: 0.519529  [  128/  146]
train() client id: f_00005-3-0 loss: 0.581694  [   32/  146]
train() client id: f_00005-3-1 loss: 0.655919  [   64/  146]
train() client id: f_00005-3-2 loss: 0.676968  [   96/  146]
train() client id: f_00005-3-3 loss: 0.524735  [  128/  146]
train() client id: f_00005-4-0 loss: 0.701923  [   32/  146]
train() client id: f_00005-4-1 loss: 0.477882  [   64/  146]
train() client id: f_00005-4-2 loss: 0.644536  [   96/  146]
train() client id: f_00005-4-3 loss: 0.702114  [  128/  146]
train() client id: f_00005-5-0 loss: 0.499460  [   32/  146]
train() client id: f_00005-5-1 loss: 0.699702  [   64/  146]
train() client id: f_00005-5-2 loss: 0.596153  [   96/  146]
train() client id: f_00005-5-3 loss: 0.647950  [  128/  146]
train() client id: f_00005-6-0 loss: 0.687209  [   32/  146]
train() client id: f_00005-6-1 loss: 0.591400  [   64/  146]
train() client id: f_00005-6-2 loss: 0.435510  [   96/  146]
train() client id: f_00005-6-3 loss: 0.649150  [  128/  146]
train() client id: f_00005-7-0 loss: 0.554725  [   32/  146]
train() client id: f_00005-7-1 loss: 0.577509  [   64/  146]
train() client id: f_00005-7-2 loss: 0.673990  [   96/  146]
train() client id: f_00005-7-3 loss: 0.524879  [  128/  146]
train() client id: f_00005-8-0 loss: 0.416663  [   32/  146]
train() client id: f_00005-8-1 loss: 0.794183  [   64/  146]
train() client id: f_00005-8-2 loss: 0.589609  [   96/  146]
train() client id: f_00005-8-3 loss: 0.639425  [  128/  146]
train() client id: f_00005-9-0 loss: 0.558186  [   32/  146]
train() client id: f_00005-9-1 loss: 0.581807  [   64/  146]
train() client id: f_00005-9-2 loss: 0.532651  [   96/  146]
train() client id: f_00005-9-3 loss: 0.634093  [  128/  146]
train() client id: f_00005-10-0 loss: 0.831128  [   32/  146]
train() client id: f_00005-10-1 loss: 0.713085  [   64/  146]
train() client id: f_00005-10-2 loss: 0.432088  [   96/  146]
train() client id: f_00005-10-3 loss: 0.490170  [  128/  146]
train() client id: f_00005-11-0 loss: 0.544080  [   32/  146]
train() client id: f_00005-11-1 loss: 0.521555  [   64/  146]
train() client id: f_00005-11-2 loss: 0.587224  [   96/  146]
train() client id: f_00005-11-3 loss: 0.570941  [  128/  146]
train() client id: f_00006-0-0 loss: 0.622177  [   32/   54]
train() client id: f_00006-1-0 loss: 0.573804  [   32/   54]
train() client id: f_00006-2-0 loss: 0.532404  [   32/   54]
train() client id: f_00006-3-0 loss: 0.630554  [   32/   54]
train() client id: f_00006-4-0 loss: 0.573109  [   32/   54]
train() client id: f_00006-5-0 loss: 0.630130  [   32/   54]
train() client id: f_00006-6-0 loss: 0.621773  [   32/   54]
train() client id: f_00006-7-0 loss: 0.565560  [   32/   54]
train() client id: f_00006-8-0 loss: 0.613843  [   32/   54]
train() client id: f_00006-9-0 loss: 0.560536  [   32/   54]
train() client id: f_00006-10-0 loss: 0.593899  [   32/   54]
train() client id: f_00006-11-0 loss: 0.566787  [   32/   54]
train() client id: f_00007-0-0 loss: 0.587064  [   32/  179]
train() client id: f_00007-0-1 loss: 0.810746  [   64/  179]
train() client id: f_00007-0-2 loss: 0.667113  [   96/  179]
train() client id: f_00007-0-3 loss: 0.673442  [  128/  179]
train() client id: f_00007-0-4 loss: 0.564027  [  160/  179]
train() client id: f_00007-1-0 loss: 0.660708  [   32/  179]
train() client id: f_00007-1-1 loss: 0.734015  [   64/  179]
train() client id: f_00007-1-2 loss: 0.557215  [   96/  179]
train() client id: f_00007-1-3 loss: 0.598014  [  128/  179]
train() client id: f_00007-1-4 loss: 0.679569  [  160/  179]
train() client id: f_00007-2-0 loss: 0.585692  [   32/  179]
train() client id: f_00007-2-1 loss: 0.666957  [   64/  179]
train() client id: f_00007-2-2 loss: 0.593902  [   96/  179]
train() client id: f_00007-2-3 loss: 0.533481  [  128/  179]
train() client id: f_00007-2-4 loss: 0.855546  [  160/  179]
train() client id: f_00007-3-0 loss: 0.699946  [   32/  179]
train() client id: f_00007-3-1 loss: 0.585291  [   64/  179]
train() client id: f_00007-3-2 loss: 0.675496  [   96/  179]
train() client id: f_00007-3-3 loss: 0.666712  [  128/  179]
train() client id: f_00007-3-4 loss: 0.540972  [  160/  179]
train() client id: f_00007-4-0 loss: 0.748755  [   32/  179]
train() client id: f_00007-4-1 loss: 0.578908  [   64/  179]
train() client id: f_00007-4-2 loss: 0.598441  [   96/  179]
train() client id: f_00007-4-3 loss: 0.603173  [  128/  179]
train() client id: f_00007-4-4 loss: 0.597257  [  160/  179]
train() client id: f_00007-5-0 loss: 0.696891  [   32/  179]
train() client id: f_00007-5-1 loss: 0.597480  [   64/  179]
train() client id: f_00007-5-2 loss: 0.686848  [   96/  179]
train() client id: f_00007-5-3 loss: 0.651537  [  128/  179]
train() client id: f_00007-5-4 loss: 0.568812  [  160/  179]
train() client id: f_00007-6-0 loss: 0.655320  [   32/  179]
train() client id: f_00007-6-1 loss: 0.511725  [   64/  179]
train() client id: f_00007-6-2 loss: 0.677321  [   96/  179]
train() client id: f_00007-6-3 loss: 0.602499  [  128/  179]
train() client id: f_00007-6-4 loss: 0.656811  [  160/  179]
train() client id: f_00007-7-0 loss: 0.609963  [   32/  179]
train() client id: f_00007-7-1 loss: 0.742695  [   64/  179]
train() client id: f_00007-7-2 loss: 0.652026  [   96/  179]
train() client id: f_00007-7-3 loss: 0.592897  [  128/  179]
train() client id: f_00007-7-4 loss: 0.569487  [  160/  179]
train() client id: f_00007-8-0 loss: 0.598789  [   32/  179]
train() client id: f_00007-8-1 loss: 0.579240  [   64/  179]
train() client id: f_00007-8-2 loss: 0.572104  [   96/  179]
train() client id: f_00007-8-3 loss: 0.746665  [  128/  179]
train() client id: f_00007-8-4 loss: 0.652524  [  160/  179]
train() client id: f_00007-9-0 loss: 0.746463  [   32/  179]
train() client id: f_00007-9-1 loss: 0.693644  [   64/  179]
train() client id: f_00007-9-2 loss: 0.577935  [   96/  179]
train() client id: f_00007-9-3 loss: 0.564867  [  128/  179]
train() client id: f_00007-9-4 loss: 0.563075  [  160/  179]
train() client id: f_00007-10-0 loss: 0.583860  [   32/  179]
train() client id: f_00007-10-1 loss: 0.603435  [   64/  179]
train() client id: f_00007-10-2 loss: 0.734432  [   96/  179]
train() client id: f_00007-10-3 loss: 0.503691  [  128/  179]
train() client id: f_00007-10-4 loss: 0.640957  [  160/  179]
train() client id: f_00007-11-0 loss: 0.772503  [   32/  179]
train() client id: f_00007-11-1 loss: 0.489323  [   64/  179]
train() client id: f_00007-11-2 loss: 0.487370  [   96/  179]
train() client id: f_00007-11-3 loss: 0.718210  [  128/  179]
train() client id: f_00007-11-4 loss: 0.494469  [  160/  179]
train() client id: f_00008-0-0 loss: 0.745347  [   32/  130]
train() client id: f_00008-0-1 loss: 0.862369  [   64/  130]
train() client id: f_00008-0-2 loss: 1.000298  [   96/  130]
train() client id: f_00008-0-3 loss: 0.768358  [  128/  130]
train() client id: f_00008-1-0 loss: 0.761176  [   32/  130]
train() client id: f_00008-1-1 loss: 0.765953  [   64/  130]
train() client id: f_00008-1-2 loss: 1.017905  [   96/  130]
train() client id: f_00008-1-3 loss: 0.830398  [  128/  130]
train() client id: f_00008-2-0 loss: 0.784067  [   32/  130]
train() client id: f_00008-2-1 loss: 0.845833  [   64/  130]
train() client id: f_00008-2-2 loss: 0.896290  [   96/  130]
train() client id: f_00008-2-3 loss: 0.845826  [  128/  130]
train() client id: f_00008-3-0 loss: 0.827737  [   32/  130]
train() client id: f_00008-3-1 loss: 0.828478  [   64/  130]
train() client id: f_00008-3-2 loss: 0.870038  [   96/  130]
train() client id: f_00008-3-3 loss: 0.846241  [  128/  130]
train() client id: f_00008-4-0 loss: 0.814751  [   32/  130]
train() client id: f_00008-4-1 loss: 0.873021  [   64/  130]
train() client id: f_00008-4-2 loss: 0.869107  [   96/  130]
train() client id: f_00008-4-3 loss: 0.816934  [  128/  130]
train() client id: f_00008-5-0 loss: 0.955811  [   32/  130]
train() client id: f_00008-5-1 loss: 0.749968  [   64/  130]
train() client id: f_00008-5-2 loss: 0.826007  [   96/  130]
train() client id: f_00008-5-3 loss: 0.838138  [  128/  130]
train() client id: f_00008-6-0 loss: 0.799492  [   32/  130]
train() client id: f_00008-6-1 loss: 0.840372  [   64/  130]
train() client id: f_00008-6-2 loss: 0.900826  [   96/  130]
train() client id: f_00008-6-3 loss: 0.831696  [  128/  130]
train() client id: f_00008-7-0 loss: 0.782216  [   32/  130]
train() client id: f_00008-7-1 loss: 0.864947  [   64/  130]
train() client id: f_00008-7-2 loss: 0.939141  [   96/  130]
train() client id: f_00008-7-3 loss: 0.792283  [  128/  130]
train() client id: f_00008-8-0 loss: 0.819662  [   32/  130]
train() client id: f_00008-8-1 loss: 0.748650  [   64/  130]
train() client id: f_00008-8-2 loss: 0.861398  [   96/  130]
train() client id: f_00008-8-3 loss: 0.950169  [  128/  130]
train() client id: f_00008-9-0 loss: 0.825861  [   32/  130]
train() client id: f_00008-9-1 loss: 0.688579  [   64/  130]
train() client id: f_00008-9-2 loss: 0.937728  [   96/  130]
train() client id: f_00008-9-3 loss: 0.922322  [  128/  130]
train() client id: f_00008-10-0 loss: 0.860430  [   32/  130]
train() client id: f_00008-10-1 loss: 0.754601  [   64/  130]
train() client id: f_00008-10-2 loss: 0.888896  [   96/  130]
train() client id: f_00008-10-3 loss: 0.841920  [  128/  130]
train() client id: f_00008-11-0 loss: 0.786258  [   32/  130]
train() client id: f_00008-11-1 loss: 0.713994  [   64/  130]
train() client id: f_00008-11-2 loss: 0.914777  [   96/  130]
train() client id: f_00008-11-3 loss: 0.963586  [  128/  130]
train() client id: f_00009-0-0 loss: 1.290954  [   32/  118]
train() client id: f_00009-0-1 loss: 1.144680  [   64/  118]
train() client id: f_00009-0-2 loss: 1.309036  [   96/  118]
train() client id: f_00009-1-0 loss: 1.242140  [   32/  118]
train() client id: f_00009-1-1 loss: 1.126912  [   64/  118]
train() client id: f_00009-1-2 loss: 1.226020  [   96/  118]
train() client id: f_00009-2-0 loss: 1.286557  [   32/  118]
train() client id: f_00009-2-1 loss: 1.214201  [   64/  118]
train() client id: f_00009-2-2 loss: 0.944940  [   96/  118]
train() client id: f_00009-3-0 loss: 1.034334  [   32/  118]
train() client id: f_00009-3-1 loss: 1.200651  [   64/  118]
train() client id: f_00009-3-2 loss: 1.101996  [   96/  118]
train() client id: f_00009-4-0 loss: 1.093940  [   32/  118]
train() client id: f_00009-4-1 loss: 1.022913  [   64/  118]
train() client id: f_00009-4-2 loss: 1.034957  [   96/  118]
train() client id: f_00009-5-0 loss: 1.110378  [   32/  118]
train() client id: f_00009-5-1 loss: 0.911103  [   64/  118]
train() client id: f_00009-5-2 loss: 1.022980  [   96/  118]
train() client id: f_00009-6-0 loss: 1.016864  [   32/  118]
train() client id: f_00009-6-1 loss: 0.855692  [   64/  118]
train() client id: f_00009-6-2 loss: 0.998694  [   96/  118]
train() client id: f_00009-7-0 loss: 0.963490  [   32/  118]
train() client id: f_00009-7-1 loss: 1.053165  [   64/  118]
train() client id: f_00009-7-2 loss: 0.866828  [   96/  118]
train() client id: f_00009-8-0 loss: 0.968441  [   32/  118]
train() client id: f_00009-8-1 loss: 0.970643  [   64/  118]
train() client id: f_00009-8-2 loss: 0.933403  [   96/  118]
train() client id: f_00009-9-0 loss: 0.860483  [   32/  118]
train() client id: f_00009-9-1 loss: 0.927310  [   64/  118]
train() client id: f_00009-9-2 loss: 1.007611  [   96/  118]
train() client id: f_00009-10-0 loss: 1.029664  [   32/  118]
train() client id: f_00009-10-1 loss: 0.880802  [   64/  118]
train() client id: f_00009-10-2 loss: 0.931837  [   96/  118]
train() client id: f_00009-11-0 loss: 0.969182  [   32/  118]
train() client id: f_00009-11-1 loss: 0.884369  [   64/  118]
train() client id: f_00009-11-2 loss: 0.964599  [   96/  118]
At round 14 accuracy: 0.6312997347480106
At round 14 training accuracy: 0.5727699530516432
At round 14 training loss: 0.8566613591806005
update_location
xs = [ -3.9056584    4.20031788  90.00902392  18.81129433   0.97929623
   3.95640986 -52.44319194 -31.32485185  74.66397685 -17.06087855]
ys = [ 82.5879595   65.55583871   1.32061395 -52.45517586  44.35018685
  27.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [129.75370986 119.64618949 134.54875848 114.47886385 109.39788889
 103.8714598  112.94768224 104.79466875 126.02927998 101.52381707]
dists_bs = [194.45631632 209.97804258 316.85061448 298.82883869 219.17592563
 231.70902195 215.80270215 225.7884024  295.15524593 232.77727406]
uav_gains = [5.21358841e-11 6.38604963e-11 4.76097128e-11 7.13141531e-11
 7.98862224e-11 9.09403593e-11 7.37560435e-11 8.89506057e-11
 5.60762293e-11 9.62893323e-11]
bs_gains = [4.31114205e-11 3.47702020e-11 1.09875619e-11 1.29452686e-11
 3.08371410e-11 2.63908782e-11 3.22058545e-11 2.83745851e-11
 1.34014770e-11 2.60531636e-11]
Round 15
-------------------------------
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.01168963 18.78472696  8.87441189  3.17492049 21.66795492 10.44235678
  3.94614583 12.71610262  9.35523145  8.4772718 ]
obj_prev = 106.45081237176021
eta_min = 4.89584141592709e-11	eta_max = 0.9211440323876784
af = 22.501910530386866	bf = 1.761176745733534	zeta = 24.752101583425553	eta = 0.9090909090909091
af = 22.501910530386866	bf = 1.761176745733534	zeta = 42.96421672758091	eta = 0.5237360818902523
af = 22.501910530386866	bf = 1.761176745733534	zeta = 34.25554170777676	eta = 0.6568838035709252
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.69318907763576	eta = 0.6882751779568552
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.6157469047972	eta = 0.6899094046832094
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.61554299983585	eta = 0.6899137178399916
eta = 0.6899137178399916
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.0306296  0.06441941 0.03014342 0.01045296 0.07438618 0.03549145
 0.01312697 0.04351349 0.03160197 0.02868487]
ene_total = [2.80683503 5.36552507 2.7806876  1.27556609 6.12194985 3.25313134
 1.47109077 3.71222193 3.07937974 2.74915557]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 0 obj = 5.7226848172315465
eta = 0.6899137178399916
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
eta_min = 0.6899137178400166	eta_max = 0.6899137178399891
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 0.03956004889123434	eta = 0.9090909090909091
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 19.410648956797782	eta = 0.0018527809600934662
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9911157923761436	eta = 0.018062074012930685
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405838032428373	eta = 0.018532402852232132
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405717992779243	eta = 0.01853251748973932
eta = 0.01853251748973932
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.85887615e-04 1.80012864e-03 1.78662392e-04 7.19578012e-06
 2.80913409e-03 3.10874884e-04 1.42137870e-05 5.10509081e-04
 2.43131519e-04 1.64389475e-04]
ene_total = [0.1719908  0.2249194  0.17497549 0.15774238 0.25358093 0.2017751
 0.15689688 0.1631543  0.23661227 0.19892425]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 1 obj = 5.722684817232005
eta = 0.6899137178400166
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
Done!
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.70806981e-05 1.65408836e-04 1.64167924e-05 6.61200306e-07
 2.58123551e-04 2.85654321e-05 1.30606552e-06 4.69092656e-05
 2.23406819e-05 1.51052936e-05]
ene_total = [0.00715633 0.00794465 0.00729    0.00671175 0.00824907 0.00831134
 0.00666937 0.00648518 0.00985659 0.00832295]
At round 15 energy consumption: 0.07699723536707746
At round 15 eta: 0.6899137178400166
At round 15 a_n: 23.044415151714617
At round 15 local rounds: 12.154605075633594
At round 15 global rounds: 74.31613869273092
gradient difference: 0.36458367109298706
train() client id: f_00000-0-0 loss: 1.611125  [   32/  126]
train() client id: f_00000-0-1 loss: 1.271903  [   64/  126]
train() client id: f_00000-0-2 loss: 1.132686  [   96/  126]
train() client id: f_00000-1-0 loss: 1.348433  [   32/  126]
train() client id: f_00000-1-1 loss: 1.294382  [   64/  126]
train() client id: f_00000-1-2 loss: 1.201687  [   96/  126]
train() client id: f_00000-2-0 loss: 1.250321  [   32/  126]
train() client id: f_00000-2-1 loss: 1.135779  [   64/  126]
train() client id: f_00000-2-2 loss: 1.139372  [   96/  126]
train() client id: f_00000-3-0 loss: 1.065861  [   32/  126]
train() client id: f_00000-3-1 loss: 1.098207  [   64/  126]
train() client id: f_00000-3-2 loss: 1.020938  [   96/  126]
train() client id: f_00000-4-0 loss: 1.008427  [   32/  126]
train() client id: f_00000-4-1 loss: 1.005077  [   64/  126]
train() client id: f_00000-4-2 loss: 0.952415  [   96/  126]
train() client id: f_00000-5-0 loss: 0.944729  [   32/  126]
train() client id: f_00000-5-1 loss: 0.921113  [   64/  126]
train() client id: f_00000-5-2 loss: 0.913358  [   96/  126]
train() client id: f_00000-6-0 loss: 0.916231  [   32/  126]
train() client id: f_00000-6-1 loss: 0.812218  [   64/  126]
train() client id: f_00000-6-2 loss: 0.927815  [   96/  126]
train() client id: f_00000-7-0 loss: 0.897498  [   32/  126]
train() client id: f_00000-7-1 loss: 0.885168  [   64/  126]
train() client id: f_00000-7-2 loss: 0.826423  [   96/  126]
train() client id: f_00000-8-0 loss: 0.812805  [   32/  126]
train() client id: f_00000-8-1 loss: 0.867679  [   64/  126]
train() client id: f_00000-8-2 loss: 0.832271  [   96/  126]
train() client id: f_00000-9-0 loss: 0.827482  [   32/  126]
train() client id: f_00000-9-1 loss: 0.881068  [   64/  126]
train() client id: f_00000-9-2 loss: 0.800804  [   96/  126]
train() client id: f_00000-10-0 loss: 0.772064  [   32/  126]
train() client id: f_00000-10-1 loss: 0.930618  [   64/  126]
train() client id: f_00000-10-2 loss: 0.790223  [   96/  126]
train() client id: f_00000-11-0 loss: 0.772629  [   32/  126]
train() client id: f_00000-11-1 loss: 0.759342  [   64/  126]
train() client id: f_00000-11-2 loss: 0.945734  [   96/  126]
train() client id: f_00001-0-0 loss: 0.460476  [   32/  265]
train() client id: f_00001-0-1 loss: 0.394696  [   64/  265]
train() client id: f_00001-0-2 loss: 0.593232  [   96/  265]
train() client id: f_00001-0-3 loss: 0.499292  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458856  [  160/  265]
train() client id: f_00001-0-5 loss: 0.484502  [  192/  265]
train() client id: f_00001-0-6 loss: 0.489118  [  224/  265]
train() client id: f_00001-0-7 loss: 0.362134  [  256/  265]
train() client id: f_00001-1-0 loss: 0.443149  [   32/  265]
train() client id: f_00001-1-1 loss: 0.435484  [   64/  265]
train() client id: f_00001-1-2 loss: 0.613466  [   96/  265]
train() client id: f_00001-1-3 loss: 0.543210  [  128/  265]
train() client id: f_00001-1-4 loss: 0.457951  [  160/  265]
train() client id: f_00001-1-5 loss: 0.393021  [  192/  265]
train() client id: f_00001-1-6 loss: 0.392977  [  224/  265]
train() client id: f_00001-1-7 loss: 0.462059  [  256/  265]
train() client id: f_00001-2-0 loss: 0.394450  [   32/  265]
train() client id: f_00001-2-1 loss: 0.542742  [   64/  265]
train() client id: f_00001-2-2 loss: 0.387420  [   96/  265]
train() client id: f_00001-2-3 loss: 0.447300  [  128/  265]
train() client id: f_00001-2-4 loss: 0.378339  [  160/  265]
train() client id: f_00001-2-5 loss: 0.536506  [  192/  265]
train() client id: f_00001-2-6 loss: 0.392882  [  224/  265]
train() client id: f_00001-2-7 loss: 0.524036  [  256/  265]
train() client id: f_00001-3-0 loss: 0.370938  [   32/  265]
train() client id: f_00001-3-1 loss: 0.437263  [   64/  265]
train() client id: f_00001-3-2 loss: 0.538404  [   96/  265]
train() client id: f_00001-3-3 loss: 0.464849  [  128/  265]
train() client id: f_00001-3-4 loss: 0.490388  [  160/  265]
train() client id: f_00001-3-5 loss: 0.470604  [  192/  265]
train() client id: f_00001-3-6 loss: 0.356016  [  224/  265]
train() client id: f_00001-3-7 loss: 0.481001  [  256/  265]
train() client id: f_00001-4-0 loss: 0.382638  [   32/  265]
train() client id: f_00001-4-1 loss: 0.466440  [   64/  265]
train() client id: f_00001-4-2 loss: 0.452794  [   96/  265]
train() client id: f_00001-4-3 loss: 0.342038  [  128/  265]
train() client id: f_00001-4-4 loss: 0.445192  [  160/  265]
train() client id: f_00001-4-5 loss: 0.421887  [  192/  265]
train() client id: f_00001-4-6 loss: 0.570113  [  224/  265]
train() client id: f_00001-4-7 loss: 0.438352  [  256/  265]
train() client id: f_00001-5-0 loss: 0.440999  [   32/  265]
train() client id: f_00001-5-1 loss: 0.564246  [   64/  265]
train() client id: f_00001-5-2 loss: 0.362127  [   96/  265]
train() client id: f_00001-5-3 loss: 0.483641  [  128/  265]
train() client id: f_00001-5-4 loss: 0.337859  [  160/  265]
train() client id: f_00001-5-5 loss: 0.580977  [  192/  265]
train() client id: f_00001-5-6 loss: 0.389578  [  224/  265]
train() client id: f_00001-5-7 loss: 0.372576  [  256/  265]
train() client id: f_00001-6-0 loss: 0.401513  [   32/  265]
train() client id: f_00001-6-1 loss: 0.436628  [   64/  265]
train() client id: f_00001-6-2 loss: 0.356902  [   96/  265]
train() client id: f_00001-6-3 loss: 0.359903  [  128/  265]
train() client id: f_00001-6-4 loss: 0.528974  [  160/  265]
train() client id: f_00001-6-5 loss: 0.468199  [  192/  265]
train() client id: f_00001-6-6 loss: 0.500591  [  224/  265]
train() client id: f_00001-6-7 loss: 0.466965  [  256/  265]
train() client id: f_00001-7-0 loss: 0.416784  [   32/  265]
train() client id: f_00001-7-1 loss: 0.465820  [   64/  265]
train() client id: f_00001-7-2 loss: 0.415365  [   96/  265]
train() client id: f_00001-7-3 loss: 0.461071  [  128/  265]
train() client id: f_00001-7-4 loss: 0.403964  [  160/  265]
train() client id: f_00001-7-5 loss: 0.470189  [  192/  265]
train() client id: f_00001-7-6 loss: 0.469236  [  224/  265]
train() client id: f_00001-7-7 loss: 0.393686  [  256/  265]
train() client id: f_00001-8-0 loss: 0.356489  [   32/  265]
train() client id: f_00001-8-1 loss: 0.399424  [   64/  265]
train() client id: f_00001-8-2 loss: 0.524249  [   96/  265]
train() client id: f_00001-8-3 loss: 0.502690  [  128/  265]
train() client id: f_00001-8-4 loss: 0.410699  [  160/  265]
train() client id: f_00001-8-5 loss: 0.389624  [  192/  265]
train() client id: f_00001-8-6 loss: 0.350039  [  224/  265]
train() client id: f_00001-8-7 loss: 0.539920  [  256/  265]
train() client id: f_00001-9-0 loss: 0.507536  [   32/  265]
train() client id: f_00001-9-1 loss: 0.425753  [   64/  265]
train() client id: f_00001-9-2 loss: 0.455508  [   96/  265]
train() client id: f_00001-9-3 loss: 0.342509  [  128/  265]
train() client id: f_00001-9-4 loss: 0.357748  [  160/  265]
train() client id: f_00001-9-5 loss: 0.349815  [  192/  265]
train() client id: f_00001-9-6 loss: 0.389780  [  224/  265]
train() client id: f_00001-9-7 loss: 0.472575  [  256/  265]
train() client id: f_00001-10-0 loss: 0.346600  [   32/  265]
train() client id: f_00001-10-1 loss: 0.463193  [   64/  265]
train() client id: f_00001-10-2 loss: 0.403152  [   96/  265]
train() client id: f_00001-10-3 loss: 0.642119  [  128/  265]
train() client id: f_00001-10-4 loss: 0.389445  [  160/  265]
train() client id: f_00001-10-5 loss: 0.335612  [  192/  265]
train() client id: f_00001-10-6 loss: 0.389404  [  224/  265]
train() client id: f_00001-10-7 loss: 0.484178  [  256/  265]
train() client id: f_00001-11-0 loss: 0.462120  [   32/  265]
train() client id: f_00001-11-1 loss: 0.396332  [   64/  265]
train() client id: f_00001-11-2 loss: 0.449509  [   96/  265]
train() client id: f_00001-11-3 loss: 0.490089  [  128/  265]
train() client id: f_00001-11-4 loss: 0.443406  [  160/  265]
train() client id: f_00001-11-5 loss: 0.345014  [  192/  265]
train() client id: f_00001-11-6 loss: 0.408202  [  224/  265]
train() client id: f_00001-11-7 loss: 0.460381  [  256/  265]
train() client id: f_00002-0-0 loss: 1.171632  [   32/  124]
train() client id: f_00002-0-1 loss: 1.249870  [   64/  124]
train() client id: f_00002-0-2 loss: 1.177217  [   96/  124]
train() client id: f_00002-1-0 loss: 1.198386  [   32/  124]
train() client id: f_00002-1-1 loss: 1.056752  [   64/  124]
train() client id: f_00002-1-2 loss: 1.189450  [   96/  124]
train() client id: f_00002-2-0 loss: 1.042852  [   32/  124]
train() client id: f_00002-2-1 loss: 1.169120  [   64/  124]
train() client id: f_00002-2-2 loss: 0.974427  [   96/  124]
train() client id: f_00002-3-0 loss: 0.953418  [   32/  124]
train() client id: f_00002-3-1 loss: 0.968230  [   64/  124]
train() client id: f_00002-3-2 loss: 1.177994  [   96/  124]
train() client id: f_00002-4-0 loss: 0.925211  [   32/  124]
train() client id: f_00002-4-1 loss: 1.163721  [   64/  124]
train() client id: f_00002-4-2 loss: 0.965317  [   96/  124]
train() client id: f_00002-5-0 loss: 1.021525  [   32/  124]
train() client id: f_00002-5-1 loss: 1.011868  [   64/  124]
train() client id: f_00002-5-2 loss: 1.015782  [   96/  124]
train() client id: f_00002-6-0 loss: 0.901012  [   32/  124]
train() client id: f_00002-6-1 loss: 0.984545  [   64/  124]
train() client id: f_00002-6-2 loss: 1.029181  [   96/  124]
train() client id: f_00002-7-0 loss: 0.992917  [   32/  124]
train() client id: f_00002-7-1 loss: 0.962573  [   64/  124]
train() client id: f_00002-7-2 loss: 0.946347  [   96/  124]
train() client id: f_00002-8-0 loss: 0.910554  [   32/  124]
train() client id: f_00002-8-1 loss: 1.095180  [   64/  124]
train() client id: f_00002-8-2 loss: 0.983387  [   96/  124]
train() client id: f_00002-9-0 loss: 0.867950  [   32/  124]
train() client id: f_00002-9-1 loss: 0.845983  [   64/  124]
train() client id: f_00002-9-2 loss: 1.132313  [   96/  124]
train() client id: f_00002-10-0 loss: 0.960031  [   32/  124]
train() client id: f_00002-10-1 loss: 0.894833  [   64/  124]
train() client id: f_00002-10-2 loss: 1.016744  [   96/  124]
train() client id: f_00002-11-0 loss: 0.964983  [   32/  124]
train() client id: f_00002-11-1 loss: 0.994719  [   64/  124]
train() client id: f_00002-11-2 loss: 0.912322  [   96/  124]
train() client id: f_00003-0-0 loss: 0.973116  [   32/   43]
train() client id: f_00003-1-0 loss: 0.971387  [   32/   43]
train() client id: f_00003-2-0 loss: 1.039180  [   32/   43]
train() client id: f_00003-3-0 loss: 0.911890  [   32/   43]
train() client id: f_00003-4-0 loss: 0.803639  [   32/   43]
train() client id: f_00003-5-0 loss: 1.120948  [   32/   43]
train() client id: f_00003-6-0 loss: 0.942548  [   32/   43]
train() client id: f_00003-7-0 loss: 0.799804  [   32/   43]
train() client id: f_00003-8-0 loss: 0.926323  [   32/   43]
train() client id: f_00003-9-0 loss: 0.813459  [   32/   43]
train() client id: f_00003-10-0 loss: 0.912886  [   32/   43]
train() client id: f_00003-11-0 loss: 0.931864  [   32/   43]
train() client id: f_00004-0-0 loss: 0.690821  [   32/  306]
train() client id: f_00004-0-1 loss: 0.984179  [   64/  306]
train() client id: f_00004-0-2 loss: 0.821844  [   96/  306]
train() client id: f_00004-0-3 loss: 0.867589  [  128/  306]
train() client id: f_00004-0-4 loss: 0.789135  [  160/  306]
train() client id: f_00004-0-5 loss: 0.767642  [  192/  306]
train() client id: f_00004-0-6 loss: 0.915335  [  224/  306]
train() client id: f_00004-0-7 loss: 0.802192  [  256/  306]
train() client id: f_00004-0-8 loss: 0.793865  [  288/  306]
train() client id: f_00004-1-0 loss: 0.794734  [   32/  306]
train() client id: f_00004-1-1 loss: 0.750982  [   64/  306]
train() client id: f_00004-1-2 loss: 0.914017  [   96/  306]
train() client id: f_00004-1-3 loss: 0.903395  [  128/  306]
train() client id: f_00004-1-4 loss: 0.796672  [  160/  306]
train() client id: f_00004-1-5 loss: 0.807005  [  192/  306]
train() client id: f_00004-1-6 loss: 0.844444  [  224/  306]
train() client id: f_00004-1-7 loss: 0.894802  [  256/  306]
train() client id: f_00004-1-8 loss: 0.761566  [  288/  306]
train() client id: f_00004-2-0 loss: 0.799673  [   32/  306]
train() client id: f_00004-2-1 loss: 0.783699  [   64/  306]
train() client id: f_00004-2-2 loss: 0.735261  [   96/  306]
train() client id: f_00004-2-3 loss: 0.870392  [  128/  306]
train() client id: f_00004-2-4 loss: 0.790218  [  160/  306]
train() client id: f_00004-2-5 loss: 0.876761  [  192/  306]
train() client id: f_00004-2-6 loss: 0.864418  [  224/  306]
train() client id: f_00004-2-7 loss: 0.840114  [  256/  306]
train() client id: f_00004-2-8 loss: 0.865445  [  288/  306]
train() client id: f_00004-3-0 loss: 0.779591  [   32/  306]
train() client id: f_00004-3-1 loss: 0.837649  [   64/  306]
train() client id: f_00004-3-2 loss: 0.681158  [   96/  306]
train() client id: f_00004-3-3 loss: 0.892899  [  128/  306]
train() client id: f_00004-3-4 loss: 0.670548  [  160/  306]
train() client id: f_00004-3-5 loss: 0.780092  [  192/  306]
train() client id: f_00004-3-6 loss: 0.796021  [  224/  306]
train() client id: f_00004-3-7 loss: 1.056562  [  256/  306]
train() client id: f_00004-3-8 loss: 0.828435  [  288/  306]
train() client id: f_00004-4-0 loss: 0.747543  [   32/  306]
train() client id: f_00004-4-1 loss: 0.829383  [   64/  306]
train() client id: f_00004-4-2 loss: 0.994589  [   96/  306]
train() client id: f_00004-4-3 loss: 0.791795  [  128/  306]
train() client id: f_00004-4-4 loss: 0.721451  [  160/  306]
train() client id: f_00004-4-5 loss: 0.976330  [  192/  306]
train() client id: f_00004-4-6 loss: 0.785261  [  224/  306]
train() client id: f_00004-4-7 loss: 0.759574  [  256/  306]
train() client id: f_00004-4-8 loss: 0.815209  [  288/  306]
train() client id: f_00004-5-0 loss: 0.889840  [   32/  306]
train() client id: f_00004-5-1 loss: 0.853446  [   64/  306]
train() client id: f_00004-5-2 loss: 0.873831  [   96/  306]
train() client id: f_00004-5-3 loss: 0.681767  [  128/  306]
train() client id: f_00004-5-4 loss: 0.769840  [  160/  306]
train() client id: f_00004-5-5 loss: 0.938086  [  192/  306]
train() client id: f_00004-5-6 loss: 0.747362  [  224/  306]
train() client id: f_00004-5-7 loss: 0.789393  [  256/  306]
train() client id: f_00004-5-8 loss: 0.821354  [  288/  306]
train() client id: f_00004-6-0 loss: 0.710635  [   32/  306]
train() client id: f_00004-6-1 loss: 0.820063  [   64/  306]
train() client id: f_00004-6-2 loss: 0.897814  [   96/  306]
train() client id: f_00004-6-3 loss: 0.824088  [  128/  306]
train() client id: f_00004-6-4 loss: 0.740572  [  160/  306]
train() client id: f_00004-6-5 loss: 0.949403  [  192/  306]
train() client id: f_00004-6-6 loss: 0.734694  [  224/  306]
train() client id: f_00004-6-7 loss: 0.914495  [  256/  306]
train() client id: f_00004-6-8 loss: 0.824181  [  288/  306]
train() client id: f_00004-7-0 loss: 0.924069  [   32/  306]
train() client id: f_00004-7-1 loss: 0.765396  [   64/  306]
train() client id: f_00004-7-2 loss: 0.824360  [   96/  306]
train() client id: f_00004-7-3 loss: 0.662382  [  128/  306]
train() client id: f_00004-7-4 loss: 0.897447  [  160/  306]
train() client id: f_00004-7-5 loss: 0.805918  [  192/  306]
train() client id: f_00004-7-6 loss: 0.901954  [  224/  306]
train() client id: f_00004-7-7 loss: 0.888111  [  256/  306]
train() client id: f_00004-7-8 loss: 0.749220  [  288/  306]
train() client id: f_00004-8-0 loss: 0.795363  [   32/  306]
train() client id: f_00004-8-1 loss: 0.660950  [   64/  306]
train() client id: f_00004-8-2 loss: 0.771179  [   96/  306]
train() client id: f_00004-8-3 loss: 0.791067  [  128/  306]
train() client id: f_00004-8-4 loss: 0.858452  [  160/  306]
train() client id: f_00004-8-5 loss: 1.012472  [  192/  306]
train() client id: f_00004-8-6 loss: 0.789998  [  224/  306]
train() client id: f_00004-8-7 loss: 0.894393  [  256/  306]
train() client id: f_00004-8-8 loss: 0.777053  [  288/  306]
train() client id: f_00004-9-0 loss: 0.829242  [   32/  306]
train() client id: f_00004-9-1 loss: 0.879115  [   64/  306]
train() client id: f_00004-9-2 loss: 0.810308  [   96/  306]
train() client id: f_00004-9-3 loss: 0.752555  [  128/  306]
train() client id: f_00004-9-4 loss: 0.988109  [  160/  306]
train() client id: f_00004-9-5 loss: 0.812979  [  192/  306]
train() client id: f_00004-9-6 loss: 0.757180  [  224/  306]
train() client id: f_00004-9-7 loss: 0.740711  [  256/  306]
train() client id: f_00004-9-8 loss: 0.889419  [  288/  306]
train() client id: f_00004-10-0 loss: 0.760821  [   32/  306]
train() client id: f_00004-10-1 loss: 0.775988  [   64/  306]
train() client id: f_00004-10-2 loss: 0.829224  [   96/  306]
train() client id: f_00004-10-3 loss: 0.915920  [  128/  306]
train() client id: f_00004-10-4 loss: 0.859825  [  160/  306]
train() client id: f_00004-10-5 loss: 0.818029  [  192/  306]
train() client id: f_00004-10-6 loss: 0.835487  [  224/  306]
train() client id: f_00004-10-7 loss: 0.732954  [  256/  306]
train() client id: f_00004-10-8 loss: 0.839554  [  288/  306]
train() client id: f_00004-11-0 loss: 0.853101  [   32/  306]
train() client id: f_00004-11-1 loss: 0.778163  [   64/  306]
train() client id: f_00004-11-2 loss: 0.721930  [   96/  306]
train() client id: f_00004-11-3 loss: 0.884587  [  128/  306]
train() client id: f_00004-11-4 loss: 0.883021  [  160/  306]
train() client id: f_00004-11-5 loss: 0.757073  [  192/  306]
train() client id: f_00004-11-6 loss: 0.785766  [  224/  306]
train() client id: f_00004-11-7 loss: 0.920303  [  256/  306]
train() client id: f_00004-11-8 loss: 0.856710  [  288/  306]
train() client id: f_00005-0-0 loss: 0.575998  [   32/  146]
train() client id: f_00005-0-1 loss: 0.642086  [   64/  146]
train() client id: f_00005-0-2 loss: 0.630346  [   96/  146]
train() client id: f_00005-0-3 loss: 0.827640  [  128/  146]
train() client id: f_00005-1-0 loss: 0.789513  [   32/  146]
train() client id: f_00005-1-1 loss: 0.667108  [   64/  146]
train() client id: f_00005-1-2 loss: 0.657591  [   96/  146]
train() client id: f_00005-1-3 loss: 0.635007  [  128/  146]
train() client id: f_00005-2-0 loss: 0.636688  [   32/  146]
train() client id: f_00005-2-1 loss: 0.716559  [   64/  146]
train() client id: f_00005-2-2 loss: 0.630298  [   96/  146]
train() client id: f_00005-2-3 loss: 0.723848  [  128/  146]
train() client id: f_00005-3-0 loss: 0.612994  [   32/  146]
train() client id: f_00005-3-1 loss: 0.564712  [   64/  146]
train() client id: f_00005-3-2 loss: 0.768534  [   96/  146]
train() client id: f_00005-3-3 loss: 0.696327  [  128/  146]
train() client id: f_00005-4-0 loss: 0.606769  [   32/  146]
train() client id: f_00005-4-1 loss: 0.880835  [   64/  146]
train() client id: f_00005-4-2 loss: 0.573813  [   96/  146]
train() client id: f_00005-4-3 loss: 0.668049  [  128/  146]
train() client id: f_00005-5-0 loss: 0.587649  [   32/  146]
train() client id: f_00005-5-1 loss: 0.783118  [   64/  146]
train() client id: f_00005-5-2 loss: 0.547457  [   96/  146]
train() client id: f_00005-5-3 loss: 0.774338  [  128/  146]
train() client id: f_00005-6-0 loss: 0.657149  [   32/  146]
train() client id: f_00005-6-1 loss: 0.756189  [   64/  146]
train() client id: f_00005-6-2 loss: 0.768726  [   96/  146]
train() client id: f_00005-6-3 loss: 0.583708  [  128/  146]
train() client id: f_00005-7-0 loss: 0.603131  [   32/  146]
train() client id: f_00005-7-1 loss: 0.761377  [   64/  146]
train() client id: f_00005-7-2 loss: 0.782179  [   96/  146]
train() client id: f_00005-7-3 loss: 0.644820  [  128/  146]
train() client id: f_00005-8-0 loss: 0.555801  [   32/  146]
train() client id: f_00005-8-1 loss: 0.590200  [   64/  146]
train() client id: f_00005-8-2 loss: 0.802608  [   96/  146]
train() client id: f_00005-8-3 loss: 0.720127  [  128/  146]
train() client id: f_00005-9-0 loss: 0.499126  [   32/  146]
train() client id: f_00005-9-1 loss: 0.732989  [   64/  146]
train() client id: f_00005-9-2 loss: 0.746420  [   96/  146]
train() client id: f_00005-9-3 loss: 0.727861  [  128/  146]
train() client id: f_00005-10-0 loss: 0.725171  [   32/  146]
train() client id: f_00005-10-1 loss: 0.535826  [   64/  146]
train() client id: f_00005-10-2 loss: 0.732045  [   96/  146]
train() client id: f_00005-10-3 loss: 0.637916  [  128/  146]
train() client id: f_00005-11-0 loss: 0.538026  [   32/  146]
train() client id: f_00005-11-1 loss: 0.617777  [   64/  146]
train() client id: f_00005-11-2 loss: 0.772935  [   96/  146]
train() client id: f_00005-11-3 loss: 0.743778  [  128/  146]
train() client id: f_00006-0-0 loss: 0.671263  [   32/   54]
train() client id: f_00006-1-0 loss: 0.631272  [   32/   54]
train() client id: f_00006-2-0 loss: 0.608262  [   32/   54]
train() client id: f_00006-3-0 loss: 0.578707  [   32/   54]
train() client id: f_00006-4-0 loss: 0.673065  [   32/   54]
train() client id: f_00006-5-0 loss: 0.583073  [   32/   54]
train() client id: f_00006-6-0 loss: 0.622731  [   32/   54]
train() client id: f_00006-7-0 loss: 0.642491  [   32/   54]
train() client id: f_00006-8-0 loss: 0.669985  [   32/   54]
train() client id: f_00006-9-0 loss: 0.608633  [   32/   54]
train() client id: f_00006-10-0 loss: 0.687761  [   32/   54]
train() client id: f_00006-11-0 loss: 0.644740  [   32/   54]
train() client id: f_00007-0-0 loss: 0.740412  [   32/  179]
train() client id: f_00007-0-1 loss: 0.697705  [   64/  179]
train() client id: f_00007-0-2 loss: 0.631306  [   96/  179]
train() client id: f_00007-0-3 loss: 0.676856  [  128/  179]
train() client id: f_00007-0-4 loss: 0.935761  [  160/  179]
train() client id: f_00007-1-0 loss: 0.772265  [   32/  179]
train() client id: f_00007-1-1 loss: 0.723208  [   64/  179]
train() client id: f_00007-1-2 loss: 0.775975  [   96/  179]
train() client id: f_00007-1-3 loss: 0.771432  [  128/  179]
train() client id: f_00007-1-4 loss: 0.655957  [  160/  179]
train() client id: f_00007-2-0 loss: 0.691584  [   32/  179]
train() client id: f_00007-2-1 loss: 0.769933  [   64/  179]
train() client id: f_00007-2-2 loss: 0.798435  [   96/  179]
train() client id: f_00007-2-3 loss: 0.637735  [  128/  179]
train() client id: f_00007-2-4 loss: 0.687231  [  160/  179]
train() client id: f_00007-3-0 loss: 0.890475  [   32/  179]
train() client id: f_00007-3-1 loss: 0.706865  [   64/  179]
train() client id: f_00007-3-2 loss: 0.656607  [   96/  179]
train() client id: f_00007-3-3 loss: 0.664513  [  128/  179]
train() client id: f_00007-3-4 loss: 0.707693  [  160/  179]
train() client id: f_00007-4-0 loss: 0.776104  [   32/  179]
train() client id: f_00007-4-1 loss: 0.638535  [   64/  179]
train() client id: f_00007-4-2 loss: 0.711224  [   96/  179]
train() client id: f_00007-4-3 loss: 0.578107  [  128/  179]
train() client id: f_00007-4-4 loss: 0.833787  [  160/  179]
train() client id: f_00007-5-0 loss: 0.665095  [   32/  179]
train() client id: f_00007-5-1 loss: 0.744562  [   64/  179]
train() client id: f_00007-5-2 loss: 0.787269  [   96/  179]
train() client id: f_00007-5-3 loss: 0.772187  [  128/  179]
train() client id: f_00007-5-4 loss: 0.600896  [  160/  179]
train() client id: f_00007-6-0 loss: 0.739411  [   32/  179]
train() client id: f_00007-6-1 loss: 0.770530  [   64/  179]
train() client id: f_00007-6-2 loss: 0.682063  [   96/  179]
train() client id: f_00007-6-3 loss: 0.677771  [  128/  179]
train() client id: f_00007-6-4 loss: 0.675644  [  160/  179]
train() client id: f_00007-7-0 loss: 0.664748  [   32/  179]
train() client id: f_00007-7-1 loss: 0.663848  [   64/  179]
train() client id: f_00007-7-2 loss: 0.869632  [   96/  179]
train() client id: f_00007-7-3 loss: 0.680099  [  128/  179]
train() client id: f_00007-7-4 loss: 0.607138  [  160/  179]
train() client id: f_00007-8-0 loss: 0.725921  [   32/  179]
train() client id: f_00007-8-1 loss: 0.672688  [   64/  179]
train() client id: f_00007-8-2 loss: 0.666123  [   96/  179]
train() client id: f_00007-8-3 loss: 0.765194  [  128/  179]
train() client id: f_00007-8-4 loss: 0.804296  [  160/  179]
train() client id: f_00007-9-0 loss: 0.674676  [   32/  179]
train() client id: f_00007-9-1 loss: 0.613039  [   64/  179]
train() client id: f_00007-9-2 loss: 0.757054  [   96/  179]
train() client id: f_00007-9-3 loss: 0.681531  [  128/  179]
train() client id: f_00007-9-4 loss: 0.771359  [  160/  179]
train() client id: f_00007-10-0 loss: 0.674957  [   32/  179]
train() client id: f_00007-10-1 loss: 0.677068  [   64/  179]
train() client id: f_00007-10-2 loss: 0.788957  [   96/  179]
train() client id: f_00007-10-3 loss: 0.596704  [  128/  179]
train() client id: f_00007-10-4 loss: 0.674265  [  160/  179]
train() client id: f_00007-11-0 loss: 0.663210  [   32/  179]
train() client id: f_00007-11-1 loss: 0.978926  [   64/  179]
train() client id: f_00007-11-2 loss: 0.653195  [   96/  179]
train() client id: f_00007-11-3 loss: 0.685354  [  128/  179]
train() client id: f_00007-11-4 loss: 0.668803  [  160/  179]
train() client id: f_00008-0-0 loss: 0.571915  [   32/  130]
train() client id: f_00008-0-1 loss: 0.716290  [   64/  130]
train() client id: f_00008-0-2 loss: 0.679862  [   96/  130]
train() client id: f_00008-0-3 loss: 0.680805  [  128/  130]
train() client id: f_00008-1-0 loss: 0.645186  [   32/  130]
train() client id: f_00008-1-1 loss: 0.683025  [   64/  130]
train() client id: f_00008-1-2 loss: 0.663089  [   96/  130]
train() client id: f_00008-1-3 loss: 0.669654  [  128/  130]
train() client id: f_00008-2-0 loss: 0.767082  [   32/  130]
train() client id: f_00008-2-1 loss: 0.649825  [   64/  130]
train() client id: f_00008-2-2 loss: 0.615222  [   96/  130]
train() client id: f_00008-2-3 loss: 0.628305  [  128/  130]
train() client id: f_00008-3-0 loss: 0.695080  [   32/  130]
train() client id: f_00008-3-1 loss: 0.641514  [   64/  130]
train() client id: f_00008-3-2 loss: 0.698799  [   96/  130]
train() client id: f_00008-3-3 loss: 0.628804  [  128/  130]
train() client id: f_00008-4-0 loss: 0.651880  [   32/  130]
train() client id: f_00008-4-1 loss: 0.602695  [   64/  130]
train() client id: f_00008-4-2 loss: 0.755416  [   96/  130]
train() client id: f_00008-4-3 loss: 0.635453  [  128/  130]
train() client id: f_00008-5-0 loss: 0.782239  [   32/  130]
train() client id: f_00008-5-1 loss: 0.654715  [   64/  130]
train() client id: f_00008-5-2 loss: 0.582066  [   96/  130]
train() client id: f_00008-5-3 loss: 0.598052  [  128/  130]
train() client id: f_00008-6-0 loss: 0.626253  [   32/  130]
train() client id: f_00008-6-1 loss: 0.730682  [   64/  130]
train() client id: f_00008-6-2 loss: 0.717772  [   96/  130]
train() client id: f_00008-6-3 loss: 0.583450  [  128/  130]
train() client id: f_00008-7-0 loss: 0.835774  [   32/  130]
train() client id: f_00008-7-1 loss: 0.570317  [   64/  130]
train() client id: f_00008-7-2 loss: 0.633113  [   96/  130]
train() client id: f_00008-7-3 loss: 0.610473  [  128/  130]
train() client id: f_00008-8-0 loss: 0.650921  [   32/  130]
train() client id: f_00008-8-1 loss: 0.617789  [   64/  130]
train() client id: f_00008-8-2 loss: 0.611912  [   96/  130]
train() client id: f_00008-8-3 loss: 0.770337  [  128/  130]
train() client id: f_00008-9-0 loss: 0.621133  [   32/  130]
train() client id: f_00008-9-1 loss: 0.746872  [   64/  130]
train() client id: f_00008-9-2 loss: 0.642115  [   96/  130]
train() client id: f_00008-9-3 loss: 0.591996  [  128/  130]
train() client id: f_00008-10-0 loss: 0.807025  [   32/  130]
train() client id: f_00008-10-1 loss: 0.541248  [   64/  130]
train() client id: f_00008-10-2 loss: 0.623132  [   96/  130]
train() client id: f_00008-10-3 loss: 0.648403  [  128/  130]
train() client id: f_00008-11-0 loss: 0.656386  [   32/  130]
train() client id: f_00008-11-1 loss: 0.646645  [   64/  130]
train() client id: f_00008-11-2 loss: 0.624841  [   96/  130]
train() client id: f_00008-11-3 loss: 0.691485  [  128/  130]
train() client id: f_00009-0-0 loss: 1.031025  [   32/  118]
train() client id: f_00009-0-1 loss: 1.349228  [   64/  118]
train() client id: f_00009-0-2 loss: 1.198930  [   96/  118]
train() client id: f_00009-1-0 loss: 1.192443  [   32/  118]
train() client id: f_00009-1-1 loss: 1.124643  [   64/  118]
train() client id: f_00009-1-2 loss: 1.032097  [   96/  118]
train() client id: f_00009-2-0 loss: 1.176444  [   32/  118]
train() client id: f_00009-2-1 loss: 1.008352  [   64/  118]
train() client id: f_00009-2-2 loss: 1.039524  [   96/  118]
train() client id: f_00009-3-0 loss: 1.006281  [   32/  118]
train() client id: f_00009-3-1 loss: 0.940724  [   64/  118]
train() client id: f_00009-3-2 loss: 1.183887  [   96/  118]
train() client id: f_00009-4-0 loss: 0.908870  [   32/  118]
train() client id: f_00009-4-1 loss: 1.032829  [   64/  118]
train() client id: f_00009-4-2 loss: 1.069868  [   96/  118]
train() client id: f_00009-5-0 loss: 0.995752  [   32/  118]
train() client id: f_00009-5-1 loss: 1.006086  [   64/  118]
train() client id: f_00009-5-2 loss: 0.892781  [   96/  118]
train() client id: f_00009-6-0 loss: 0.876875  [   32/  118]
train() client id: f_00009-6-1 loss: 0.952513  [   64/  118]
train() client id: f_00009-6-2 loss: 0.996955  [   96/  118]
train() client id: f_00009-7-0 loss: 1.018860  [   32/  118]
train() client id: f_00009-7-1 loss: 0.891330  [   64/  118]
train() client id: f_00009-7-2 loss: 1.001001  [   96/  118]
train() client id: f_00009-8-0 loss: 0.870821  [   32/  118]
train() client id: f_00009-8-1 loss: 0.799554  [   64/  118]
train() client id: f_00009-8-2 loss: 1.035502  [   96/  118]
train() client id: f_00009-9-0 loss: 0.904633  [   32/  118]
train() client id: f_00009-9-1 loss: 0.886890  [   64/  118]
train() client id: f_00009-9-2 loss: 0.913056  [   96/  118]
train() client id: f_00009-10-0 loss: 0.875600  [   32/  118]
train() client id: f_00009-10-1 loss: 0.769981  [   64/  118]
train() client id: f_00009-10-2 loss: 0.987786  [   96/  118]
train() client id: f_00009-11-0 loss: 0.793011  [   32/  118]
train() client id: f_00009-11-1 loss: 0.948206  [   64/  118]
train() client id: f_00009-11-2 loss: 0.981190  [   96/  118]
At round 15 accuracy: 0.6339522546419099
At round 15 training accuracy: 0.5700871898054997
At round 15 training loss: 0.8647534871371884
update_location
xs = [ -3.9056584    4.20031788  95.00902392  18.81129433   0.97929623
   3.95640986 -57.44319194 -36.32485185  79.66397685 -22.06087855]
ys = [ 87.5879595   70.55583871   1.32061395 -57.45517586  49.35018685
  32.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [132.99212314 122.45721312 137.94367926 116.85444804 111.51860815
 105.32056622 115.35428402 106.39629278 129.05432647 102.48265326]
dists_bs = [192.13052478 207.41585464 321.04423704 302.65198924 216.2327178
 228.56555375 213.0029065  222.6401832  299.39649124 229.41418462]
uav_gains = [4.90168661e-11 6.02574305e-11 4.47298682e-11 6.77442080e-11
 7.61419638e-11 8.78443206e-11 6.99686398e-11 8.56406368e-11
 5.28456671e-11 9.40528225e-11]
bs_gains = [4.45886427e-11 3.59862526e-11 1.05904015e-11 1.24925816e-11
 3.20268441e-11 2.74197759e-11 3.34052405e-11 2.95123738e-11
 1.28766623e-11 2.71367191e-11]
Round 16
-------------------------------
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.87981483 18.50415528  8.7446806   3.12925602 21.34431125 10.28544805
  3.88903844 12.52812053  9.21866598  8.34945337]
obj_prev = 104.87294435893298
eta_min = 3.443571724151839e-11	eta_max = 0.9213403601232922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 24.38417167306138	eta = 0.9090909090909091
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 42.374357946194124	eta = 0.5231330896350052
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 33.76652681787571	eta = 0.6564912320788918
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.22189998010197	eta = 0.6879615667412922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14522467977385	eta = 0.6896025463975112
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14502216089842	eta = 0.6896068910058766
eta = 0.6896068910058766
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.03066631 0.06449661 0.03017954 0.01046549 0.07447533 0.03553398
 0.0131427  0.04356563 0.03163984 0.02871924]
ene_total = [2.771699   5.28190218 2.74625142 1.26115748 6.02653747 3.19933593
 1.45388682 3.66054961 3.04133893 2.70236334]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 0 obj = 5.644901027509804
eta = 0.6896068910058766
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
eta_min = 0.6896068910059152	eta_max = 0.6896068910058611
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 0.03760750116688597	eta = 0.909090909090909
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 19.169603857786115	eta = 0.0017834816868453646
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9593134418751286	eta = 0.01744929458133157
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111928987478155	eta = 0.017888637743914634
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111822247051171	eta = 0.01788873765279858
eta = 0.01788873765279858
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.82570215e-04 1.75061300e-03 1.75501009e-04 7.05980817e-06
 2.72955744e-03 3.01849167e-04 1.39465251e-05 5.00344854e-04
 2.38449589e-04 1.59558866e-04]
ene_total = [0.17134348 0.21893109 0.17438925 0.15683431 0.24623749 0.19673596
 0.1560201  0.1614347  0.23535039 0.19390545]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 1 obj = 5.644901027510502
eta = 0.6896068910059152
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
Done!
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.66284604e-05 1.59445500e-04 1.59845985e-05 6.43005986e-07
 2.48607572e-04 2.74923648e-05 1.27024685e-06 4.55713142e-05
 2.17179432e-05 1.45325912e-05]
ene_total = [0.00724659 0.00788007 0.00738478 0.00677843 0.00817159 0.00823669
 0.00673695 0.00652909 0.00996482 0.00824357]
At round 16 energy consumption: 0.0771725767342164
At round 16 eta: 0.6896068910059152
At round 16 a_n: 22.701869304745298
At round 16 local rounds: 12.169171106060379
At round 16 global rounds: 73.13908926108901
gradient difference: 0.4055185317993164
train() client id: f_00000-0-0 loss: 1.189061  [   32/  126]
train() client id: f_00000-0-1 loss: 1.279905  [   64/  126]
train() client id: f_00000-0-2 loss: 1.217621  [   96/  126]
train() client id: f_00000-1-0 loss: 1.113518  [   32/  126]
train() client id: f_00000-1-1 loss: 1.188243  [   64/  126]
train() client id: f_00000-1-2 loss: 1.060393  [   96/  126]
train() client id: f_00000-2-0 loss: 0.980084  [   32/  126]
train() client id: f_00000-2-1 loss: 0.977268  [   64/  126]
train() client id: f_00000-2-2 loss: 0.928646  [   96/  126]
train() client id: f_00000-3-0 loss: 0.969487  [   32/  126]
train() client id: f_00000-3-1 loss: 0.953036  [   64/  126]
train() client id: f_00000-3-2 loss: 0.965081  [   96/  126]
train() client id: f_00000-4-0 loss: 0.870654  [   32/  126]
train() client id: f_00000-4-1 loss: 0.895058  [   64/  126]
train() client id: f_00000-4-2 loss: 0.819951  [   96/  126]
train() client id: f_00000-5-0 loss: 0.865402  [   32/  126]
train() client id: f_00000-5-1 loss: 0.870455  [   64/  126]
train() client id: f_00000-5-2 loss: 0.818288  [   96/  126]
train() client id: f_00000-6-0 loss: 0.785061  [   32/  126]
train() client id: f_00000-6-1 loss: 0.851833  [   64/  126]
train() client id: f_00000-6-2 loss: 0.809810  [   96/  126]
train() client id: f_00000-7-0 loss: 0.818370  [   32/  126]
train() client id: f_00000-7-1 loss: 0.836395  [   64/  126]
train() client id: f_00000-7-2 loss: 0.762488  [   96/  126]
train() client id: f_00000-8-0 loss: 0.757571  [   32/  126]
train() client id: f_00000-8-1 loss: 0.710342  [   64/  126]
train() client id: f_00000-8-2 loss: 0.817505  [   96/  126]
train() client id: f_00000-9-0 loss: 0.786866  [   32/  126]
train() client id: f_00000-9-1 loss: 0.772491  [   64/  126]
train() client id: f_00000-9-2 loss: 0.729743  [   96/  126]
train() client id: f_00000-10-0 loss: 0.769942  [   32/  126]
train() client id: f_00000-10-1 loss: 0.744201  [   64/  126]
train() client id: f_00000-10-2 loss: 0.761642  [   96/  126]
train() client id: f_00000-11-0 loss: 0.816125  [   32/  126]
train() client id: f_00000-11-1 loss: 0.801812  [   64/  126]
train() client id: f_00000-11-2 loss: 0.687834  [   96/  126]
train() client id: f_00001-0-0 loss: 0.467873  [   32/  265]
train() client id: f_00001-0-1 loss: 0.404375  [   64/  265]
train() client id: f_00001-0-2 loss: 0.414126  [   96/  265]
train() client id: f_00001-0-3 loss: 0.419844  [  128/  265]
train() client id: f_00001-0-4 loss: 0.520782  [  160/  265]
train() client id: f_00001-0-5 loss: 0.552742  [  192/  265]
train() client id: f_00001-0-6 loss: 0.564600  [  224/  265]
train() client id: f_00001-0-7 loss: 0.471890  [  256/  265]
train() client id: f_00001-1-0 loss: 0.464355  [   32/  265]
train() client id: f_00001-1-1 loss: 0.427679  [   64/  265]
train() client id: f_00001-1-2 loss: 0.530810  [   96/  265]
train() client id: f_00001-1-3 loss: 0.545457  [  128/  265]
train() client id: f_00001-1-4 loss: 0.491918  [  160/  265]
train() client id: f_00001-1-5 loss: 0.389880  [  192/  265]
train() client id: f_00001-1-6 loss: 0.398183  [  224/  265]
train() client id: f_00001-1-7 loss: 0.508368  [  256/  265]
train() client id: f_00001-2-0 loss: 0.431810  [   32/  265]
train() client id: f_00001-2-1 loss: 0.425660  [   64/  265]
train() client id: f_00001-2-2 loss: 0.578797  [   96/  265]
train() client id: f_00001-2-3 loss: 0.379000  [  128/  265]
train() client id: f_00001-2-4 loss: 0.430451  [  160/  265]
train() client id: f_00001-2-5 loss: 0.484704  [  192/  265]
train() client id: f_00001-2-6 loss: 0.404328  [  224/  265]
train() client id: f_00001-2-7 loss: 0.553004  [  256/  265]
train() client id: f_00001-3-0 loss: 0.409747  [   32/  265]
train() client id: f_00001-3-1 loss: 0.556569  [   64/  265]
train() client id: f_00001-3-2 loss: 0.438383  [   96/  265]
train() client id: f_00001-3-3 loss: 0.590952  [  128/  265]
train() client id: f_00001-3-4 loss: 0.419466  [  160/  265]
train() client id: f_00001-3-5 loss: 0.378440  [  192/  265]
train() client id: f_00001-3-6 loss: 0.430229  [  224/  265]
train() client id: f_00001-3-7 loss: 0.407945  [  256/  265]
train() client id: f_00001-4-0 loss: 0.413300  [   32/  265]
train() client id: f_00001-4-1 loss: 0.498203  [   64/  265]
train() client id: f_00001-4-2 loss: 0.411232  [   96/  265]
train() client id: f_00001-4-3 loss: 0.549124  [  128/  265]
train() client id: f_00001-4-4 loss: 0.454796  [  160/  265]
train() client id: f_00001-4-5 loss: 0.373235  [  192/  265]
train() client id: f_00001-4-6 loss: 0.473463  [  224/  265]
train() client id: f_00001-4-7 loss: 0.440750  [  256/  265]
train() client id: f_00001-5-0 loss: 0.493344  [   32/  265]
train() client id: f_00001-5-1 loss: 0.429491  [   64/  265]
train() client id: f_00001-5-2 loss: 0.544061  [   96/  265]
train() client id: f_00001-5-3 loss: 0.378716  [  128/  265]
train() client id: f_00001-5-4 loss: 0.359797  [  160/  265]
train() client id: f_00001-5-5 loss: 0.426411  [  192/  265]
train() client id: f_00001-5-6 loss: 0.409476  [  224/  265]
train() client id: f_00001-5-7 loss: 0.513863  [  256/  265]
train() client id: f_00001-6-0 loss: 0.356865  [   32/  265]
train() client id: f_00001-6-1 loss: 0.620166  [   64/  265]
train() client id: f_00001-6-2 loss: 0.391500  [   96/  265]
train() client id: f_00001-6-3 loss: 0.481300  [  128/  265]
train() client id: f_00001-6-4 loss: 0.404509  [  160/  265]
train() client id: f_00001-6-5 loss: 0.437914  [  192/  265]
train() client id: f_00001-6-6 loss: 0.468431  [  224/  265]
train() client id: f_00001-6-7 loss: 0.415734  [  256/  265]
train() client id: f_00001-7-0 loss: 0.405054  [   32/  265]
train() client id: f_00001-7-1 loss: 0.359346  [   64/  265]
train() client id: f_00001-7-2 loss: 0.496699  [   96/  265]
train() client id: f_00001-7-3 loss: 0.545886  [  128/  265]
train() client id: f_00001-7-4 loss: 0.465293  [  160/  265]
train() client id: f_00001-7-5 loss: 0.430523  [  192/  265]
train() client id: f_00001-7-6 loss: 0.450655  [  224/  265]
train() client id: f_00001-7-7 loss: 0.414835  [  256/  265]
train() client id: f_00001-8-0 loss: 0.422165  [   32/  265]
train() client id: f_00001-8-1 loss: 0.519052  [   64/  265]
train() client id: f_00001-8-2 loss: 0.632840  [   96/  265]
train() client id: f_00001-8-3 loss: 0.432308  [  128/  265]
train() client id: f_00001-8-4 loss: 0.389969  [  160/  265]
train() client id: f_00001-8-5 loss: 0.390191  [  192/  265]
train() client id: f_00001-8-6 loss: 0.369034  [  224/  265]
train() client id: f_00001-8-7 loss: 0.399454  [  256/  265]
train() client id: f_00001-9-0 loss: 0.347842  [   32/  265]
train() client id: f_00001-9-1 loss: 0.546653  [   64/  265]
train() client id: f_00001-9-2 loss: 0.408264  [   96/  265]
train() client id: f_00001-9-3 loss: 0.412791  [  128/  265]
train() client id: f_00001-9-4 loss: 0.411094  [  160/  265]
train() client id: f_00001-9-5 loss: 0.547570  [  192/  265]
train() client id: f_00001-9-6 loss: 0.476517  [  224/  265]
train() client id: f_00001-9-7 loss: 0.410559  [  256/  265]
train() client id: f_00001-10-0 loss: 0.356681  [   32/  265]
train() client id: f_00001-10-1 loss: 0.488873  [   64/  265]
train() client id: f_00001-10-2 loss: 0.491515  [   96/  265]
train() client id: f_00001-10-3 loss: 0.404297  [  128/  265]
train() client id: f_00001-10-4 loss: 0.467653  [  160/  265]
train() client id: f_00001-10-5 loss: 0.460027  [  192/  265]
train() client id: f_00001-10-6 loss: 0.388584  [  224/  265]
train() client id: f_00001-10-7 loss: 0.447514  [  256/  265]
train() client id: f_00001-11-0 loss: 0.432877  [   32/  265]
train() client id: f_00001-11-1 loss: 0.397446  [   64/  265]
train() client id: f_00001-11-2 loss: 0.424248  [   96/  265]
train() client id: f_00001-11-3 loss: 0.577932  [  128/  265]
train() client id: f_00001-11-4 loss: 0.535571  [  160/  265]
train() client id: f_00001-11-5 loss: 0.443182  [  192/  265]
train() client id: f_00001-11-6 loss: 0.405049  [  224/  265]
train() client id: f_00001-11-7 loss: 0.350472  [  256/  265]
train() client id: f_00002-0-0 loss: 1.084625  [   32/  124]
train() client id: f_00002-0-1 loss: 1.088618  [   64/  124]
train() client id: f_00002-0-2 loss: 1.190857  [   96/  124]
train() client id: f_00002-1-0 loss: 1.082216  [   32/  124]
train() client id: f_00002-1-1 loss: 1.170314  [   64/  124]
train() client id: f_00002-1-2 loss: 1.152576  [   96/  124]
train() client id: f_00002-2-0 loss: 1.116033  [   32/  124]
train() client id: f_00002-2-1 loss: 1.019027  [   64/  124]
train() client id: f_00002-2-2 loss: 1.226542  [   96/  124]
train() client id: f_00002-3-0 loss: 1.139861  [   32/  124]
train() client id: f_00002-3-1 loss: 1.146664  [   64/  124]
train() client id: f_00002-3-2 loss: 1.052463  [   96/  124]
train() client id: f_00002-4-0 loss: 1.286141  [   32/  124]
train() client id: f_00002-4-1 loss: 1.003592  [   64/  124]
train() client id: f_00002-4-2 loss: 0.956384  [   96/  124]
train() client id: f_00002-5-0 loss: 1.134794  [   32/  124]
train() client id: f_00002-5-1 loss: 0.977113  [   64/  124]
train() client id: f_00002-5-2 loss: 1.110720  [   96/  124]
train() client id: f_00002-6-0 loss: 1.016501  [   32/  124]
train() client id: f_00002-6-1 loss: 0.990442  [   64/  124]
train() client id: f_00002-6-2 loss: 0.983084  [   96/  124]
train() client id: f_00002-7-0 loss: 0.894765  [   32/  124]
train() client id: f_00002-7-1 loss: 1.147459  [   64/  124]
train() client id: f_00002-7-2 loss: 0.958572  [   96/  124]
train() client id: f_00002-8-0 loss: 0.964502  [   32/  124]
train() client id: f_00002-8-1 loss: 0.847963  [   64/  124]
train() client id: f_00002-8-2 loss: 1.147570  [   96/  124]
train() client id: f_00002-9-0 loss: 0.980160  [   32/  124]
train() client id: f_00002-9-1 loss: 1.067906  [   64/  124]
train() client id: f_00002-9-2 loss: 0.978758  [   96/  124]
train() client id: f_00002-10-0 loss: 1.077815  [   32/  124]
train() client id: f_00002-10-1 loss: 0.976778  [   64/  124]
train() client id: f_00002-10-2 loss: 0.971301  [   96/  124]
train() client id: f_00002-11-0 loss: 1.025902  [   32/  124]
train() client id: f_00002-11-1 loss: 1.003205  [   64/  124]
train() client id: f_00002-11-2 loss: 1.067508  [   96/  124]
train() client id: f_00003-0-0 loss: 0.808094  [   32/   43]
train() client id: f_00003-1-0 loss: 0.825902  [   32/   43]
train() client id: f_00003-2-0 loss: 0.821164  [   32/   43]
train() client id: f_00003-3-0 loss: 0.865365  [   32/   43]
train() client id: f_00003-4-0 loss: 0.964478  [   32/   43]
train() client id: f_00003-5-0 loss: 0.884487  [   32/   43]
train() client id: f_00003-6-0 loss: 0.792807  [   32/   43]
train() client id: f_00003-7-0 loss: 0.909698  [   32/   43]
train() client id: f_00003-8-0 loss: 0.852895  [   32/   43]
train() client id: f_00003-9-0 loss: 0.796549  [   32/   43]
train() client id: f_00003-10-0 loss: 0.857156  [   32/   43]
train() client id: f_00003-11-0 loss: 0.728795  [   32/   43]
train() client id: f_00004-0-0 loss: 0.815605  [   32/  306]
train() client id: f_00004-0-1 loss: 0.989320  [   64/  306]
train() client id: f_00004-0-2 loss: 0.781940  [   96/  306]
train() client id: f_00004-0-3 loss: 0.868350  [  128/  306]
train() client id: f_00004-0-4 loss: 1.028502  [  160/  306]
train() client id: f_00004-0-5 loss: 0.752236  [  192/  306]
train() client id: f_00004-0-6 loss: 0.992888  [  224/  306]
train() client id: f_00004-0-7 loss: 0.791095  [  256/  306]
train() client id: f_00004-0-8 loss: 0.807127  [  288/  306]
train() client id: f_00004-1-0 loss: 0.828567  [   32/  306]
train() client id: f_00004-1-1 loss: 0.945596  [   64/  306]
train() client id: f_00004-1-2 loss: 0.872291  [   96/  306]
train() client id: f_00004-1-3 loss: 0.980879  [  128/  306]
train() client id: f_00004-1-4 loss: 0.845740  [  160/  306]
train() client id: f_00004-1-5 loss: 0.934712  [  192/  306]
train() client id: f_00004-1-6 loss: 0.760370  [  224/  306]
train() client id: f_00004-1-7 loss: 0.894786  [  256/  306]
train() client id: f_00004-1-8 loss: 0.821213  [  288/  306]
train() client id: f_00004-2-0 loss: 0.966478  [   32/  306]
train() client id: f_00004-2-1 loss: 0.914569  [   64/  306]
train() client id: f_00004-2-2 loss: 0.792173  [   96/  306]
train() client id: f_00004-2-3 loss: 0.839694  [  128/  306]
train() client id: f_00004-2-4 loss: 0.948152  [  160/  306]
train() client id: f_00004-2-5 loss: 0.888442  [  192/  306]
train() client id: f_00004-2-6 loss: 0.838632  [  224/  306]
train() client id: f_00004-2-7 loss: 0.868098  [  256/  306]
train() client id: f_00004-2-8 loss: 0.754185  [  288/  306]
train() client id: f_00004-3-0 loss: 0.819430  [   32/  306]
train() client id: f_00004-3-1 loss: 0.840984  [   64/  306]
train() client id: f_00004-3-2 loss: 0.879823  [   96/  306]
train() client id: f_00004-3-3 loss: 0.889915  [  128/  306]
train() client id: f_00004-3-4 loss: 0.946858  [  160/  306]
train() client id: f_00004-3-5 loss: 0.917400  [  192/  306]
train() client id: f_00004-3-6 loss: 0.722002  [  224/  306]
train() client id: f_00004-3-7 loss: 0.857583  [  256/  306]
train() client id: f_00004-3-8 loss: 0.925121  [  288/  306]
train() client id: f_00004-4-0 loss: 0.847538  [   32/  306]
train() client id: f_00004-4-1 loss: 0.955236  [   64/  306]
train() client id: f_00004-4-2 loss: 0.764874  [   96/  306]
train() client id: f_00004-4-3 loss: 0.903117  [  128/  306]
train() client id: f_00004-4-4 loss: 0.918363  [  160/  306]
train() client id: f_00004-4-5 loss: 0.874972  [  192/  306]
train() client id: f_00004-4-6 loss: 0.810691  [  224/  306]
train() client id: f_00004-4-7 loss: 0.937758  [  256/  306]
train() client id: f_00004-4-8 loss: 0.823759  [  288/  306]
train() client id: f_00004-5-0 loss: 0.825310  [   32/  306]
train() client id: f_00004-5-1 loss: 0.878844  [   64/  306]
train() client id: f_00004-5-2 loss: 0.834450  [   96/  306]
train() client id: f_00004-5-3 loss: 0.915444  [  128/  306]
train() client id: f_00004-5-4 loss: 0.898803  [  160/  306]
train() client id: f_00004-5-5 loss: 0.876675  [  192/  306]
train() client id: f_00004-5-6 loss: 0.871189  [  224/  306]
train() client id: f_00004-5-7 loss: 0.747538  [  256/  306]
train() client id: f_00004-5-8 loss: 0.911989  [  288/  306]
train() client id: f_00004-6-0 loss: 0.805801  [   32/  306]
train() client id: f_00004-6-1 loss: 0.959546  [   64/  306]
train() client id: f_00004-6-2 loss: 0.807298  [   96/  306]
train() client id: f_00004-6-3 loss: 0.870904  [  128/  306]
train() client id: f_00004-6-4 loss: 0.925110  [  160/  306]
train() client id: f_00004-6-5 loss: 0.999905  [  192/  306]
train() client id: f_00004-6-6 loss: 0.721992  [  224/  306]
train() client id: f_00004-6-7 loss: 0.741491  [  256/  306]
train() client id: f_00004-6-8 loss: 0.940843  [  288/  306]
train() client id: f_00004-7-0 loss: 0.947618  [   32/  306]
train() client id: f_00004-7-1 loss: 0.832988  [   64/  306]
train() client id: f_00004-7-2 loss: 0.916506  [   96/  306]
train() client id: f_00004-7-3 loss: 0.935734  [  128/  306]
train() client id: f_00004-7-4 loss: 0.831631  [  160/  306]
train() client id: f_00004-7-5 loss: 0.866887  [  192/  306]
train() client id: f_00004-7-6 loss: 0.751205  [  224/  306]
train() client id: f_00004-7-7 loss: 0.902763  [  256/  306]
train() client id: f_00004-7-8 loss: 0.784609  [  288/  306]
train() client id: f_00004-8-0 loss: 0.843875  [   32/  306]
train() client id: f_00004-8-1 loss: 0.791137  [   64/  306]
train() client id: f_00004-8-2 loss: 0.873716  [   96/  306]
train() client id: f_00004-8-3 loss: 0.858787  [  128/  306]
train() client id: f_00004-8-4 loss: 0.910855  [  160/  306]
train() client id: f_00004-8-5 loss: 0.844766  [  192/  306]
train() client id: f_00004-8-6 loss: 0.830764  [  224/  306]
train() client id: f_00004-8-7 loss: 1.072962  [  256/  306]
train() client id: f_00004-8-8 loss: 0.832308  [  288/  306]
train() client id: f_00004-9-0 loss: 0.802234  [   32/  306]
train() client id: f_00004-9-1 loss: 0.859070  [   64/  306]
train() client id: f_00004-9-2 loss: 0.750066  [   96/  306]
train() client id: f_00004-9-3 loss: 0.943903  [  128/  306]
train() client id: f_00004-9-4 loss: 0.820698  [  160/  306]
train() client id: f_00004-9-5 loss: 0.854227  [  192/  306]
train() client id: f_00004-9-6 loss: 0.908148  [  224/  306]
train() client id: f_00004-9-7 loss: 0.953726  [  256/  306]
train() client id: f_00004-9-8 loss: 0.818915  [  288/  306]
train() client id: f_00004-10-0 loss: 0.904274  [   32/  306]
train() client id: f_00004-10-1 loss: 0.883274  [   64/  306]
train() client id: f_00004-10-2 loss: 0.758045  [   96/  306]
train() client id: f_00004-10-3 loss: 0.938205  [  128/  306]
train() client id: f_00004-10-4 loss: 0.726046  [  160/  306]
train() client id: f_00004-10-5 loss: 0.879785  [  192/  306]
train() client id: f_00004-10-6 loss: 0.857863  [  224/  306]
train() client id: f_00004-10-7 loss: 0.885193  [  256/  306]
train() client id: f_00004-10-8 loss: 0.907876  [  288/  306]
train() client id: f_00004-11-0 loss: 0.856330  [   32/  306]
train() client id: f_00004-11-1 loss: 1.005166  [   64/  306]
train() client id: f_00004-11-2 loss: 0.878393  [   96/  306]
train() client id: f_00004-11-3 loss: 0.846716  [  128/  306]
train() client id: f_00004-11-4 loss: 0.841141  [  160/  306]
train() client id: f_00004-11-5 loss: 0.824842  [  192/  306]
train() client id: f_00004-11-6 loss: 0.824523  [  224/  306]
train() client id: f_00004-11-7 loss: 0.910086  [  256/  306]
train() client id: f_00004-11-8 loss: 0.794184  [  288/  306]
train() client id: f_00005-0-0 loss: 0.702004  [   32/  146]
train() client id: f_00005-0-1 loss: 0.821756  [   64/  146]
train() client id: f_00005-0-2 loss: 0.570876  [   96/  146]
train() client id: f_00005-0-3 loss: 0.532442  [  128/  146]
train() client id: f_00005-1-0 loss: 0.643148  [   32/  146]
train() client id: f_00005-1-1 loss: 0.742958  [   64/  146]
train() client id: f_00005-1-2 loss: 0.604634  [   96/  146]
train() client id: f_00005-1-3 loss: 0.635629  [  128/  146]
train() client id: f_00005-2-0 loss: 0.668809  [   32/  146]
train() client id: f_00005-2-1 loss: 0.432557  [   64/  146]
train() client id: f_00005-2-2 loss: 0.928379  [   96/  146]
train() client id: f_00005-2-3 loss: 0.572482  [  128/  146]
train() client id: f_00005-3-0 loss: 0.804783  [   32/  146]
train() client id: f_00005-3-1 loss: 0.624543  [   64/  146]
train() client id: f_00005-3-2 loss: 0.597004  [   96/  146]
train() client id: f_00005-3-3 loss: 0.391371  [  128/  146]
train() client id: f_00005-4-0 loss: 0.826047  [   32/  146]
train() client id: f_00005-4-1 loss: 0.613686  [   64/  146]
train() client id: f_00005-4-2 loss: 0.570289  [   96/  146]
train() client id: f_00005-4-3 loss: 0.568875  [  128/  146]
train() client id: f_00005-5-0 loss: 0.685599  [   32/  146]
train() client id: f_00005-5-1 loss: 0.483806  [   64/  146]
train() client id: f_00005-5-2 loss: 0.560478  [   96/  146]
train() client id: f_00005-5-3 loss: 0.807448  [  128/  146]
train() client id: f_00005-6-0 loss: 0.661072  [   32/  146]
train() client id: f_00005-6-1 loss: 0.551564  [   64/  146]
train() client id: f_00005-6-2 loss: 0.516727  [   96/  146]
train() client id: f_00005-6-3 loss: 0.796473  [  128/  146]
train() client id: f_00005-7-0 loss: 0.559716  [   32/  146]
train() client id: f_00005-7-1 loss: 0.562275  [   64/  146]
train() client id: f_00005-7-2 loss: 0.635729  [   96/  146]
train() client id: f_00005-7-3 loss: 0.597503  [  128/  146]
train() client id: f_00005-8-0 loss: 0.425273  [   32/  146]
train() client id: f_00005-8-1 loss: 0.627823  [   64/  146]
train() client id: f_00005-8-2 loss: 0.714860  [   96/  146]
train() client id: f_00005-8-3 loss: 0.525863  [  128/  146]
train() client id: f_00005-9-0 loss: 0.649722  [   32/  146]
train() client id: f_00005-9-1 loss: 0.543117  [   64/  146]
train() client id: f_00005-9-2 loss: 0.655132  [   96/  146]
train() client id: f_00005-9-3 loss: 0.364111  [  128/  146]
train() client id: f_00005-10-0 loss: 0.527486  [   32/  146]
train() client id: f_00005-10-1 loss: 0.754607  [   64/  146]
train() client id: f_00005-10-2 loss: 0.584243  [   96/  146]
train() client id: f_00005-10-3 loss: 0.547193  [  128/  146]
train() client id: f_00005-11-0 loss: 0.716103  [   32/  146]
train() client id: f_00005-11-1 loss: 0.531929  [   64/  146]
train() client id: f_00005-11-2 loss: 0.587706  [   96/  146]
train() client id: f_00005-11-3 loss: 0.520948  [  128/  146]
train() client id: f_00006-0-0 loss: 0.561500  [   32/   54]
train() client id: f_00006-1-0 loss: 0.602212  [   32/   54]
train() client id: f_00006-2-0 loss: 0.590116  [   32/   54]
train() client id: f_00006-3-0 loss: 0.567748  [   32/   54]
train() client id: f_00006-4-0 loss: 0.606730  [   32/   54]
train() client id: f_00006-5-0 loss: 0.668092  [   32/   54]
train() client id: f_00006-6-0 loss: 0.554120  [   32/   54]
train() client id: f_00006-7-0 loss: 0.603830  [   32/   54]
train() client id: f_00006-8-0 loss: 0.665663  [   32/   54]
train() client id: f_00006-9-0 loss: 0.660827  [   32/   54]
train() client id: f_00006-10-0 loss: 0.662456  [   32/   54]
train() client id: f_00006-11-0 loss: 0.551876  [   32/   54]
train() client id: f_00007-0-0 loss: 0.708924  [   32/  179]
train() client id: f_00007-0-1 loss: 0.717354  [   64/  179]
train() client id: f_00007-0-2 loss: 0.565585  [   96/  179]
train() client id: f_00007-0-3 loss: 0.600733  [  128/  179]
train() client id: f_00007-0-4 loss: 0.818570  [  160/  179]
train() client id: f_00007-1-0 loss: 0.777571  [   32/  179]
train() client id: f_00007-1-1 loss: 0.604596  [   64/  179]
train() client id: f_00007-1-2 loss: 0.638292  [   96/  179]
train() client id: f_00007-1-3 loss: 0.726271  [  128/  179]
train() client id: f_00007-1-4 loss: 0.682190  [  160/  179]
train() client id: f_00007-2-0 loss: 0.585865  [   32/  179]
train() client id: f_00007-2-1 loss: 0.592799  [   64/  179]
train() client id: f_00007-2-2 loss: 0.755807  [   96/  179]
train() client id: f_00007-2-3 loss: 0.806912  [  128/  179]
train() client id: f_00007-2-4 loss: 0.655908  [  160/  179]
train() client id: f_00007-3-0 loss: 0.633599  [   32/  179]
train() client id: f_00007-3-1 loss: 0.529260  [   64/  179]
train() client id: f_00007-3-2 loss: 0.663514  [   96/  179]
train() client id: f_00007-3-3 loss: 0.706343  [  128/  179]
train() client id: f_00007-3-4 loss: 0.611235  [  160/  179]
train() client id: f_00007-4-0 loss: 0.689218  [   32/  179]
train() client id: f_00007-4-1 loss: 0.628543  [   64/  179]
train() client id: f_00007-4-2 loss: 0.611173  [   96/  179]
train() client id: f_00007-4-3 loss: 0.657868  [  128/  179]
train() client id: f_00007-4-4 loss: 0.698731  [  160/  179]
train() client id: f_00007-5-0 loss: 0.551128  [   32/  179]
train() client id: f_00007-5-1 loss: 0.577045  [   64/  179]
train() client id: f_00007-5-2 loss: 0.656670  [   96/  179]
train() client id: f_00007-5-3 loss: 0.835725  [  128/  179]
train() client id: f_00007-5-4 loss: 0.642429  [  160/  179]
train() client id: f_00007-6-0 loss: 0.691381  [   32/  179]
train() client id: f_00007-6-1 loss: 0.522728  [   64/  179]
train() client id: f_00007-6-2 loss: 0.684868  [   96/  179]
train() client id: f_00007-6-3 loss: 0.657609  [  128/  179]
train() client id: f_00007-6-4 loss: 0.661460  [  160/  179]
train() client id: f_00007-7-0 loss: 0.732078  [   32/  179]
train() client id: f_00007-7-1 loss: 0.737444  [   64/  179]
train() client id: f_00007-7-2 loss: 0.521940  [   96/  179]
train() client id: f_00007-7-3 loss: 0.640810  [  128/  179]
train() client id: f_00007-7-4 loss: 0.588750  [  160/  179]
train() client id: f_00007-8-0 loss: 0.611111  [   32/  179]
train() client id: f_00007-8-1 loss: 0.732386  [   64/  179]
train() client id: f_00007-8-2 loss: 0.523926  [   96/  179]
train() client id: f_00007-8-3 loss: 0.743939  [  128/  179]
train() client id: f_00007-8-4 loss: 0.587580  [  160/  179]
train() client id: f_00007-9-0 loss: 0.570110  [   32/  179]
train() client id: f_00007-9-1 loss: 0.571210  [   64/  179]
train() client id: f_00007-9-2 loss: 0.605836  [   96/  179]
train() client id: f_00007-9-3 loss: 0.683305  [  128/  179]
train() client id: f_00007-9-4 loss: 0.690069  [  160/  179]
train() client id: f_00007-10-0 loss: 0.499779  [   32/  179]
train() client id: f_00007-10-1 loss: 0.675796  [   64/  179]
train() client id: f_00007-10-2 loss: 0.765069  [   96/  179]
train() client id: f_00007-10-3 loss: 0.570687  [  128/  179]
train() client id: f_00007-10-4 loss: 0.604322  [  160/  179]
train() client id: f_00007-11-0 loss: 0.771662  [   32/  179]
train() client id: f_00007-11-1 loss: 0.528253  [   64/  179]
train() client id: f_00007-11-2 loss: 0.487819  [   96/  179]
train() client id: f_00007-11-3 loss: 0.745696  [  128/  179]
train() client id: f_00007-11-4 loss: 0.487848  [  160/  179]
train() client id: f_00008-0-0 loss: 0.779375  [   32/  130]
train() client id: f_00008-0-1 loss: 0.826047  [   64/  130]
train() client id: f_00008-0-2 loss: 0.770215  [   96/  130]
train() client id: f_00008-0-3 loss: 0.631761  [  128/  130]
train() client id: f_00008-1-0 loss: 0.650983  [   32/  130]
train() client id: f_00008-1-1 loss: 0.798463  [   64/  130]
train() client id: f_00008-1-2 loss: 0.804140  [   96/  130]
train() client id: f_00008-1-3 loss: 0.737107  [  128/  130]
train() client id: f_00008-2-0 loss: 0.825441  [   32/  130]
train() client id: f_00008-2-1 loss: 0.746681  [   64/  130]
train() client id: f_00008-2-2 loss: 0.714939  [   96/  130]
train() client id: f_00008-2-3 loss: 0.709873  [  128/  130]
train() client id: f_00008-3-0 loss: 0.837688  [   32/  130]
train() client id: f_00008-3-1 loss: 0.707676  [   64/  130]
train() client id: f_00008-3-2 loss: 0.762340  [   96/  130]
train() client id: f_00008-3-3 loss: 0.691016  [  128/  130]
train() client id: f_00008-4-0 loss: 0.855616  [   32/  130]
train() client id: f_00008-4-1 loss: 0.718240  [   64/  130]
train() client id: f_00008-4-2 loss: 0.678366  [   96/  130]
train() client id: f_00008-4-3 loss: 0.738109  [  128/  130]
train() client id: f_00008-5-0 loss: 0.823624  [   32/  130]
train() client id: f_00008-5-1 loss: 0.691997  [   64/  130]
train() client id: f_00008-5-2 loss: 0.694683  [   96/  130]
train() client id: f_00008-5-3 loss: 0.785569  [  128/  130]
train() client id: f_00008-6-0 loss: 0.807252  [   32/  130]
train() client id: f_00008-6-1 loss: 0.760501  [   64/  130]
train() client id: f_00008-6-2 loss: 0.679984  [   96/  130]
train() client id: f_00008-6-3 loss: 0.709466  [  128/  130]
train() client id: f_00008-7-0 loss: 0.687256  [   32/  130]
train() client id: f_00008-7-1 loss: 0.828503  [   64/  130]
train() client id: f_00008-7-2 loss: 0.763723  [   96/  130]
train() client id: f_00008-7-3 loss: 0.678361  [  128/  130]
train() client id: f_00008-8-0 loss: 0.791899  [   32/  130]
train() client id: f_00008-8-1 loss: 0.751460  [   64/  130]
train() client id: f_00008-8-2 loss: 0.749258  [   96/  130]
train() client id: f_00008-8-3 loss: 0.709580  [  128/  130]
train() client id: f_00008-9-0 loss: 0.794855  [   32/  130]
train() client id: f_00008-9-1 loss: 0.698783  [   64/  130]
train() client id: f_00008-9-2 loss: 0.729681  [   96/  130]
train() client id: f_00008-9-3 loss: 0.741199  [  128/  130]
train() client id: f_00008-10-0 loss: 0.890933  [   32/  130]
train() client id: f_00008-10-1 loss: 0.685906  [   64/  130]
train() client id: f_00008-10-2 loss: 0.774203  [   96/  130]
train() client id: f_00008-10-3 loss: 0.645465  [  128/  130]
train() client id: f_00008-11-0 loss: 0.831365  [   32/  130]
train() client id: f_00008-11-1 loss: 0.702409  [   64/  130]
train() client id: f_00008-11-2 loss: 0.671887  [   96/  130]
train() client id: f_00008-11-3 loss: 0.783320  [  128/  130]
train() client id: f_00009-0-0 loss: 1.064628  [   32/  118]
train() client id: f_00009-0-1 loss: 1.054679  [   64/  118]
train() client id: f_00009-0-2 loss: 1.014566  [   96/  118]
train() client id: f_00009-1-0 loss: 0.908211  [   32/  118]
train() client id: f_00009-1-1 loss: 1.067195  [   64/  118]
train() client id: f_00009-1-2 loss: 0.976490  [   96/  118]
train() client id: f_00009-2-0 loss: 0.982561  [   32/  118]
train() client id: f_00009-2-1 loss: 0.895998  [   64/  118]
train() client id: f_00009-2-2 loss: 0.899514  [   96/  118]
train() client id: f_00009-3-0 loss: 0.959258  [   32/  118]
train() client id: f_00009-3-1 loss: 0.881246  [   64/  118]
train() client id: f_00009-3-2 loss: 0.831965  [   96/  118]
train() client id: f_00009-4-0 loss: 0.935158  [   32/  118]
train() client id: f_00009-4-1 loss: 0.890678  [   64/  118]
train() client id: f_00009-4-2 loss: 0.902560  [   96/  118]
train() client id: f_00009-5-0 loss: 0.824706  [   32/  118]
train() client id: f_00009-5-1 loss: 0.931590  [   64/  118]
train() client id: f_00009-5-2 loss: 0.830468  [   96/  118]
train() client id: f_00009-6-0 loss: 0.786124  [   32/  118]
train() client id: f_00009-6-1 loss: 0.805272  [   64/  118]
train() client id: f_00009-6-2 loss: 0.843716  [   96/  118]
train() client id: f_00009-7-0 loss: 0.693608  [   32/  118]
train() client id: f_00009-7-1 loss: 0.989318  [   64/  118]
train() client id: f_00009-7-2 loss: 0.829557  [   96/  118]
train() client id: f_00009-8-0 loss: 0.861947  [   32/  118]
train() client id: f_00009-8-1 loss: 0.686341  [   64/  118]
train() client id: f_00009-8-2 loss: 0.892482  [   96/  118]
train() client id: f_00009-9-0 loss: 0.840507  [   32/  118]
train() client id: f_00009-9-1 loss: 0.724682  [   64/  118]
train() client id: f_00009-9-2 loss: 0.752971  [   96/  118]
train() client id: f_00009-10-0 loss: 0.790622  [   32/  118]
train() client id: f_00009-10-1 loss: 0.878135  [   64/  118]
train() client id: f_00009-10-2 loss: 0.673781  [   96/  118]
train() client id: f_00009-11-0 loss: 0.753857  [   32/  118]
train() client id: f_00009-11-1 loss: 0.696999  [   64/  118]
train() client id: f_00009-11-2 loss: 0.842222  [   96/  118]
At round 16 accuracy: 0.6339522546419099
At round 16 training accuracy: 0.5801475519785378
At round 16 training loss: 0.8407853231876722
update_location
xs = [ -3.9056584    4.20031788 100.00902392  18.81129433   0.97929623
   3.95640986 -62.44319194 -41.32485185  84.66397685 -27.06087855]
ys = [ 92.5879595   75.55583871   1.32061395 -62.45517586  54.35018685
  37.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [136.33702509 125.40465475 141.43390289 119.39226854 113.81960214
 106.98393887 117.92388546 108.20545105 132.19931524 103.6740228 ]
dists_bs = [189.90792018 204.94363894 325.26065297 306.50901839 213.36609411
 225.48914357 210.28473576 219.56069707 303.66082845 226.11164695]
uav_gains = [4.60616434e-11 5.67774848e-11 4.20154197e-11 6.42006706e-11
 7.23514457e-11 8.44693888e-11 6.62184205e-11 8.21054949e-11
 4.97558800e-11 9.13739608e-11]
bs_gains = [4.60652538e-11 3.72149675e-11 1.02104716e-11 1.20573805e-11
 3.32462714e-11 2.84801519e-11 3.46283970e-11 3.06860651e-11
 1.23767188e-11 2.82611527e-11]
Round 17
-------------------------------
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.74794599 18.22367373  8.61494673  3.08364678 21.02075903 10.12863146
  3.83198593 12.340238    9.082055    8.2217282 ]
obj_prev = 103.29561085136496
eta_min = 2.3980574083554427e-11	eta_max = 0.9215530004474818
af = 21.83294705699746	bf = 1.718217883710131	zeta = 24.01624176269721	eta = 0.9090909090909091
af = 21.83294705699746	bf = 1.718217883710131	zeta = 41.790319927079516	eta = 0.5224402946685754
af = 21.83294705699746	bf = 1.718217883710131	zeta = 33.27990204277066	eta = 0.6560400036315671
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.752345685579005	eta = 0.6876010759391976
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.67639108760098	eta = 0.6892498263649575
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.676189751909728	eta = 0.6892542072766557
eta = 0.6892542072766557
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.03070852 0.06458539 0.03022109 0.01047989 0.07457784 0.03558289
 0.0131608  0.0436256  0.0316834  0.02875878]
ene_total = [2.73656278 5.19862747 2.71178509 1.24687687 5.93149526 3.1458414
 1.43681478 3.6092118  3.0031103  2.655864  ]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 0 obj = 5.56824286220838
eta = 0.6892542072766557
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
eta_min = 0.6892542072766561	eta_max = 0.6892542072766527
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 0.03573652736322809	eta = 0.9090909090909091
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 18.93445726929715	eta = 0.0017158005474531947
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.9284046028512083	eta = 0.016846958413371853
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.882600477836571	eta = 0.017256848986739414
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.8825910021965795	eta = 0.01725693584558884
eta = 0.01725693584558884
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.79246673e-04 1.70228989e-03 1.72328565e-04 6.92481033e-06
 2.65200012e-03 2.93061998e-04 1.36811274e-05 4.90328058e-04
 2.33730561e-04 1.54858866e-04]
ene_total = [0.17070727 0.21310565 0.17380525 0.15598508 0.23910014 0.1918103
 0.15520187 0.15983003 0.23404959 0.18899581]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 1 obj = 5.56824286220839
eta = 0.6892542072766561
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
Done!
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.61811483e-05 1.53670942e-04 1.55566295e-05 6.25123921e-07
 2.39404205e-04 2.64556077e-05 1.23503744e-06 4.42634215e-05
 2.10995764e-05 1.39795860e-05]
ene_total = [0.00733991 0.00781786 0.00748236 0.0068496  0.0080964  0.00816389
 0.00680903 0.00657883 0.01007438 0.00816592]
At round 17 energy consumption: 0.07737817222069016
At round 17 eta: 0.6892542072766561
At round 17 a_n: 22.359323457775982
At round 17 local rounds: 12.185922115151007
At round 17 global rounds: 71.9537447693859
gradient difference: 0.42865175008773804
train() client id: f_00000-0-0 loss: 1.388439  [   32/  126]
train() client id: f_00000-0-1 loss: 1.299510  [   64/  126]
train() client id: f_00000-0-2 loss: 1.174440  [   96/  126]
train() client id: f_00000-1-0 loss: 1.233012  [   32/  126]
train() client id: f_00000-1-1 loss: 1.150848  [   64/  126]
train() client id: f_00000-1-2 loss: 1.173605  [   96/  126]
train() client id: f_00000-2-0 loss: 1.115467  [   32/  126]
train() client id: f_00000-2-1 loss: 1.190797  [   64/  126]
train() client id: f_00000-2-2 loss: 1.081030  [   96/  126]
train() client id: f_00000-3-0 loss: 1.105662  [   32/  126]
train() client id: f_00000-3-1 loss: 1.100152  [   64/  126]
train() client id: f_00000-3-2 loss: 0.981284  [   96/  126]
train() client id: f_00000-4-0 loss: 1.012452  [   32/  126]
train() client id: f_00000-4-1 loss: 0.965092  [   64/  126]
train() client id: f_00000-4-2 loss: 1.001216  [   96/  126]
train() client id: f_00000-5-0 loss: 0.970503  [   32/  126]
train() client id: f_00000-5-1 loss: 0.935403  [   64/  126]
train() client id: f_00000-5-2 loss: 1.054270  [   96/  126]
train() client id: f_00000-6-0 loss: 0.963480  [   32/  126]
train() client id: f_00000-6-1 loss: 0.939842  [   64/  126]
train() client id: f_00000-6-2 loss: 1.005748  [   96/  126]
train() client id: f_00000-7-0 loss: 0.920858  [   32/  126]
train() client id: f_00000-7-1 loss: 0.859604  [   64/  126]
train() client id: f_00000-7-2 loss: 1.021819  [   96/  126]
train() client id: f_00000-8-0 loss: 0.840771  [   32/  126]
train() client id: f_00000-8-1 loss: 0.949058  [   64/  126]
train() client id: f_00000-8-2 loss: 1.050679  [   96/  126]
train() client id: f_00000-9-0 loss: 0.944657  [   32/  126]
train() client id: f_00000-9-1 loss: 0.874604  [   64/  126]
train() client id: f_00000-9-2 loss: 0.919671  [   96/  126]
train() client id: f_00000-10-0 loss: 0.938904  [   32/  126]
train() client id: f_00000-10-1 loss: 0.896911  [   64/  126]
train() client id: f_00000-10-2 loss: 0.920382  [   96/  126]
train() client id: f_00000-11-0 loss: 0.870124  [   32/  126]
train() client id: f_00000-11-1 loss: 0.898566  [   64/  126]
train() client id: f_00000-11-2 loss: 0.946676  [   96/  126]
train() client id: f_00001-0-0 loss: 0.564430  [   32/  265]
train() client id: f_00001-0-1 loss: 0.526591  [   64/  265]
train() client id: f_00001-0-2 loss: 0.580715  [   96/  265]
train() client id: f_00001-0-3 loss: 0.519522  [  128/  265]
train() client id: f_00001-0-4 loss: 0.464879  [  160/  265]
train() client id: f_00001-0-5 loss: 0.558510  [  192/  265]
train() client id: f_00001-0-6 loss: 0.499862  [  224/  265]
train() client id: f_00001-0-7 loss: 0.600008  [  256/  265]
train() client id: f_00001-1-0 loss: 0.559894  [   32/  265]
train() client id: f_00001-1-1 loss: 0.617564  [   64/  265]
train() client id: f_00001-1-2 loss: 0.448015  [   96/  265]
train() client id: f_00001-1-3 loss: 0.501359  [  128/  265]
train() client id: f_00001-1-4 loss: 0.519347  [  160/  265]
train() client id: f_00001-1-5 loss: 0.562380  [  192/  265]
train() client id: f_00001-1-6 loss: 0.509107  [  224/  265]
train() client id: f_00001-1-7 loss: 0.527572  [  256/  265]
train() client id: f_00001-2-0 loss: 0.569610  [   32/  265]
train() client id: f_00001-2-1 loss: 0.543579  [   64/  265]
train() client id: f_00001-2-2 loss: 0.540078  [   96/  265]
train() client id: f_00001-2-3 loss: 0.435080  [  128/  265]
train() client id: f_00001-2-4 loss: 0.516522  [  160/  265]
train() client id: f_00001-2-5 loss: 0.610055  [  192/  265]
train() client id: f_00001-2-6 loss: 0.478133  [  224/  265]
train() client id: f_00001-2-7 loss: 0.498541  [  256/  265]
train() client id: f_00001-3-0 loss: 0.464133  [   32/  265]
train() client id: f_00001-3-1 loss: 0.507726  [   64/  265]
train() client id: f_00001-3-2 loss: 0.568859  [   96/  265]
train() client id: f_00001-3-3 loss: 0.557883  [  128/  265]
train() client id: f_00001-3-4 loss: 0.496089  [  160/  265]
train() client id: f_00001-3-5 loss: 0.525043  [  192/  265]
train() client id: f_00001-3-6 loss: 0.579067  [  224/  265]
train() client id: f_00001-3-7 loss: 0.422496  [  256/  265]
train() client id: f_00001-4-0 loss: 0.559906  [   32/  265]
train() client id: f_00001-4-1 loss: 0.566753  [   64/  265]
train() client id: f_00001-4-2 loss: 0.459496  [   96/  265]
train() client id: f_00001-4-3 loss: 0.483213  [  128/  265]
train() client id: f_00001-4-4 loss: 0.492174  [  160/  265]
train() client id: f_00001-4-5 loss: 0.514170  [  192/  265]
train() client id: f_00001-4-6 loss: 0.524568  [  224/  265]
train() client id: f_00001-4-7 loss: 0.551045  [  256/  265]
train() client id: f_00001-5-0 loss: 0.527915  [   32/  265]
train() client id: f_00001-5-1 loss: 0.653978  [   64/  265]
train() client id: f_00001-5-2 loss: 0.490103  [   96/  265]
train() client id: f_00001-5-3 loss: 0.527083  [  128/  265]
train() client id: f_00001-5-4 loss: 0.426719  [  160/  265]
train() client id: f_00001-5-5 loss: 0.581560  [  192/  265]
train() client id: f_00001-5-6 loss: 0.500849  [  224/  265]
train() client id: f_00001-5-7 loss: 0.429689  [  256/  265]
train() client id: f_00001-6-0 loss: 0.664618  [   32/  265]
train() client id: f_00001-6-1 loss: 0.412010  [   64/  265]
train() client id: f_00001-6-2 loss: 0.478277  [   96/  265]
train() client id: f_00001-6-3 loss: 0.547768  [  128/  265]
train() client id: f_00001-6-4 loss: 0.544063  [  160/  265]
train() client id: f_00001-6-5 loss: 0.504706  [  192/  265]
train() client id: f_00001-6-6 loss: 0.440931  [  224/  265]
train() client id: f_00001-6-7 loss: 0.546396  [  256/  265]
train() client id: f_00001-7-0 loss: 0.573545  [   32/  265]
train() client id: f_00001-7-1 loss: 0.568680  [   64/  265]
train() client id: f_00001-7-2 loss: 0.488892  [   96/  265]
train() client id: f_00001-7-3 loss: 0.457722  [  128/  265]
train() client id: f_00001-7-4 loss: 0.495905  [  160/  265]
train() client id: f_00001-7-5 loss: 0.499213  [  192/  265]
train() client id: f_00001-7-6 loss: 0.537066  [  224/  265]
train() client id: f_00001-7-7 loss: 0.489719  [  256/  265]
train() client id: f_00001-8-0 loss: 0.441285  [   32/  265]
train() client id: f_00001-8-1 loss: 0.633141  [   64/  265]
train() client id: f_00001-8-2 loss: 0.526069  [   96/  265]
train() client id: f_00001-8-3 loss: 0.628537  [  128/  265]
train() client id: f_00001-8-4 loss: 0.486983  [  160/  265]
train() client id: f_00001-8-5 loss: 0.530853  [  192/  265]
train() client id: f_00001-8-6 loss: 0.436434  [  224/  265]
train() client id: f_00001-8-7 loss: 0.454992  [  256/  265]
train() client id: f_00001-9-0 loss: 0.472147  [   32/  265]
train() client id: f_00001-9-1 loss: 0.534804  [   64/  265]
train() client id: f_00001-9-2 loss: 0.424803  [   96/  265]
train() client id: f_00001-9-3 loss: 0.489724  [  128/  265]
train() client id: f_00001-9-4 loss: 0.476971  [  160/  265]
train() client id: f_00001-9-5 loss: 0.562835  [  192/  265]
train() client id: f_00001-9-6 loss: 0.702882  [  224/  265]
train() client id: f_00001-9-7 loss: 0.424324  [  256/  265]
train() client id: f_00001-10-0 loss: 0.425124  [   32/  265]
train() client id: f_00001-10-1 loss: 0.589988  [   64/  265]
train() client id: f_00001-10-2 loss: 0.521970  [   96/  265]
train() client id: f_00001-10-3 loss: 0.563693  [  128/  265]
train() client id: f_00001-10-4 loss: 0.637444  [  160/  265]
train() client id: f_00001-10-5 loss: 0.515371  [  192/  265]
train() client id: f_00001-10-6 loss: 0.422520  [  224/  265]
train() client id: f_00001-10-7 loss: 0.464597  [  256/  265]
train() client id: f_00001-11-0 loss: 0.419318  [   32/  265]
train() client id: f_00001-11-1 loss: 0.521930  [   64/  265]
train() client id: f_00001-11-2 loss: 0.474690  [   96/  265]
train() client id: f_00001-11-3 loss: 0.523932  [  128/  265]
train() client id: f_00001-11-4 loss: 0.599228  [  160/  265]
train() client id: f_00001-11-5 loss: 0.435546  [  192/  265]
train() client id: f_00001-11-6 loss: 0.658553  [  224/  265]
train() client id: f_00001-11-7 loss: 0.513960  [  256/  265]
train() client id: f_00002-0-0 loss: 1.104990  [   32/  124]
train() client id: f_00002-0-1 loss: 1.066345  [   64/  124]
train() client id: f_00002-0-2 loss: 1.094432  [   96/  124]
train() client id: f_00002-1-0 loss: 1.047864  [   32/  124]
train() client id: f_00002-1-1 loss: 1.121233  [   64/  124]
train() client id: f_00002-1-2 loss: 1.013568  [   96/  124]
train() client id: f_00002-2-0 loss: 1.001017  [   32/  124]
train() client id: f_00002-2-1 loss: 0.976425  [   64/  124]
train() client id: f_00002-2-2 loss: 1.053378  [   96/  124]
train() client id: f_00002-3-0 loss: 0.972832  [   32/  124]
train() client id: f_00002-3-1 loss: 1.139885  [   64/  124]
train() client id: f_00002-3-2 loss: 0.866028  [   96/  124]
train() client id: f_00002-4-0 loss: 1.001715  [   32/  124]
train() client id: f_00002-4-1 loss: 0.989438  [   64/  124]
train() client id: f_00002-4-2 loss: 1.033086  [   96/  124]
train() client id: f_00002-5-0 loss: 0.980634  [   32/  124]
train() client id: f_00002-5-1 loss: 1.069676  [   64/  124]
train() client id: f_00002-5-2 loss: 0.912820  [   96/  124]
train() client id: f_00002-6-0 loss: 1.053683  [   32/  124]
train() client id: f_00002-6-1 loss: 1.021572  [   64/  124]
train() client id: f_00002-6-2 loss: 0.767645  [   96/  124]
train() client id: f_00002-7-0 loss: 0.890152  [   32/  124]
train() client id: f_00002-7-1 loss: 0.967036  [   64/  124]
train() client id: f_00002-7-2 loss: 1.001764  [   96/  124]
train() client id: f_00002-8-0 loss: 0.826852  [   32/  124]
train() client id: f_00002-8-1 loss: 0.995294  [   64/  124]
train() client id: f_00002-8-2 loss: 0.949334  [   96/  124]
train() client id: f_00002-9-0 loss: 0.805550  [   32/  124]
train() client id: f_00002-9-1 loss: 0.790320  [   64/  124]
train() client id: f_00002-9-2 loss: 1.062608  [   96/  124]
train() client id: f_00002-10-0 loss: 1.016614  [   32/  124]
train() client id: f_00002-10-1 loss: 0.989374  [   64/  124]
train() client id: f_00002-10-2 loss: 0.838431  [   96/  124]
train() client id: f_00002-11-0 loss: 0.853564  [   32/  124]
train() client id: f_00002-11-1 loss: 0.801132  [   64/  124]
train() client id: f_00002-11-2 loss: 0.962101  [   96/  124]
train() client id: f_00003-0-0 loss: 0.994366  [   32/   43]
train() client id: f_00003-1-0 loss: 0.933927  [   32/   43]
train() client id: f_00003-2-0 loss: 0.969453  [   32/   43]
train() client id: f_00003-3-0 loss: 0.925977  [   32/   43]
train() client id: f_00003-4-0 loss: 0.889841  [   32/   43]
train() client id: f_00003-5-0 loss: 0.987415  [   32/   43]
train() client id: f_00003-6-0 loss: 0.968843  [   32/   43]
train() client id: f_00003-7-0 loss: 0.786239  [   32/   43]
train() client id: f_00003-8-0 loss: 0.926898  [   32/   43]
train() client id: f_00003-9-0 loss: 1.044396  [   32/   43]
train() client id: f_00003-10-0 loss: 1.004325  [   32/   43]
train() client id: f_00003-11-0 loss: 0.812966  [   32/   43]
train() client id: f_00004-0-0 loss: 1.045973  [   32/  306]
train() client id: f_00004-0-1 loss: 0.886157  [   64/  306]
train() client id: f_00004-0-2 loss: 1.027500  [   96/  306]
train() client id: f_00004-0-3 loss: 0.939283  [  128/  306]
train() client id: f_00004-0-4 loss: 0.904110  [  160/  306]
train() client id: f_00004-0-5 loss: 0.929061  [  192/  306]
train() client id: f_00004-0-6 loss: 1.025091  [  224/  306]
train() client id: f_00004-0-7 loss: 1.043365  [  256/  306]
train() client id: f_00004-0-8 loss: 0.923250  [  288/  306]
train() client id: f_00004-1-0 loss: 0.822624  [   32/  306]
train() client id: f_00004-1-1 loss: 0.978114  [   64/  306]
train() client id: f_00004-1-2 loss: 0.963544  [   96/  306]
train() client id: f_00004-1-3 loss: 1.039290  [  128/  306]
train() client id: f_00004-1-4 loss: 0.935061  [  160/  306]
train() client id: f_00004-1-5 loss: 1.040059  [  192/  306]
train() client id: f_00004-1-6 loss: 0.922945  [  224/  306]
train() client id: f_00004-1-7 loss: 1.131818  [  256/  306]
train() client id: f_00004-1-8 loss: 0.904671  [  288/  306]
train() client id: f_00004-2-0 loss: 0.785270  [   32/  306]
train() client id: f_00004-2-1 loss: 0.887179  [   64/  306]
train() client id: f_00004-2-2 loss: 1.042970  [   96/  306]
train() client id: f_00004-2-3 loss: 1.103493  [  128/  306]
train() client id: f_00004-2-4 loss: 1.019035  [  160/  306]
train() client id: f_00004-2-5 loss: 0.899350  [  192/  306]
train() client id: f_00004-2-6 loss: 0.954142  [  224/  306]
train() client id: f_00004-2-7 loss: 0.960688  [  256/  306]
train() client id: f_00004-2-8 loss: 1.040432  [  288/  306]
train() client id: f_00004-3-0 loss: 0.909224  [   32/  306]
train() client id: f_00004-3-1 loss: 0.830225  [   64/  306]
train() client id: f_00004-3-2 loss: 0.858243  [   96/  306]
train() client id: f_00004-3-3 loss: 0.974796  [  128/  306]
train() client id: f_00004-3-4 loss: 0.939220  [  160/  306]
train() client id: f_00004-3-5 loss: 1.091587  [  192/  306]
train() client id: f_00004-3-6 loss: 1.033018  [  224/  306]
train() client id: f_00004-3-7 loss: 0.972578  [  256/  306]
train() client id: f_00004-3-8 loss: 0.952233  [  288/  306]
train() client id: f_00004-4-0 loss: 0.999921  [   32/  306]
train() client id: f_00004-4-1 loss: 0.989522  [   64/  306]
train() client id: f_00004-4-2 loss: 0.871226  [   96/  306]
train() client id: f_00004-4-3 loss: 0.890281  [  128/  306]
train() client id: f_00004-4-4 loss: 1.086370  [  160/  306]
train() client id: f_00004-4-5 loss: 1.014700  [  192/  306]
train() client id: f_00004-4-6 loss: 1.028467  [  224/  306]
train() client id: f_00004-4-7 loss: 0.856858  [  256/  306]
train() client id: f_00004-4-8 loss: 0.968244  [  288/  306]
train() client id: f_00004-5-0 loss: 0.889786  [   32/  306]
train() client id: f_00004-5-1 loss: 1.104685  [   64/  306]
train() client id: f_00004-5-2 loss: 0.962859  [   96/  306]
train() client id: f_00004-5-3 loss: 0.879969  [  128/  306]
train() client id: f_00004-5-4 loss: 0.922653  [  160/  306]
train() client id: f_00004-5-5 loss: 0.869938  [  192/  306]
train() client id: f_00004-5-6 loss: 0.963257  [  224/  306]
train() client id: f_00004-5-7 loss: 0.871578  [  256/  306]
train() client id: f_00004-5-8 loss: 1.052741  [  288/  306]
train() client id: f_00004-6-0 loss: 1.071515  [   32/  306]
train() client id: f_00004-6-1 loss: 0.866480  [   64/  306]
train() client id: f_00004-6-2 loss: 1.048588  [   96/  306]
train() client id: f_00004-6-3 loss: 0.892193  [  128/  306]
train() client id: f_00004-6-4 loss: 0.922429  [  160/  306]
train() client id: f_00004-6-5 loss: 0.897345  [  192/  306]
train() client id: f_00004-6-6 loss: 0.918764  [  224/  306]
train() client id: f_00004-6-7 loss: 0.980866  [  256/  306]
train() client id: f_00004-6-8 loss: 0.968590  [  288/  306]
train() client id: f_00004-7-0 loss: 0.983805  [   32/  306]
train() client id: f_00004-7-1 loss: 0.922459  [   64/  306]
train() client id: f_00004-7-2 loss: 1.072215  [   96/  306]
train() client id: f_00004-7-3 loss: 0.868498  [  128/  306]
train() client id: f_00004-7-4 loss: 0.993567  [  160/  306]
train() client id: f_00004-7-5 loss: 0.937700  [  192/  306]
train() client id: f_00004-7-6 loss: 0.938235  [  224/  306]
train() client id: f_00004-7-7 loss: 0.956117  [  256/  306]
train() client id: f_00004-7-8 loss: 0.959242  [  288/  306]
train() client id: f_00004-8-0 loss: 0.865161  [   32/  306]
train() client id: f_00004-8-1 loss: 0.982373  [   64/  306]
train() client id: f_00004-8-2 loss: 0.971940  [   96/  306]
train() client id: f_00004-8-3 loss: 1.059081  [  128/  306]
train() client id: f_00004-8-4 loss: 0.846172  [  160/  306]
train() client id: f_00004-8-5 loss: 0.952239  [  192/  306]
train() client id: f_00004-8-6 loss: 0.864250  [  224/  306]
train() client id: f_00004-8-7 loss: 0.977629  [  256/  306]
train() client id: f_00004-8-8 loss: 0.980057  [  288/  306]
train() client id: f_00004-9-0 loss: 0.911084  [   32/  306]
train() client id: f_00004-9-1 loss: 0.954053  [   64/  306]
train() client id: f_00004-9-2 loss: 0.956925  [   96/  306]
train() client id: f_00004-9-3 loss: 1.076482  [  128/  306]
train() client id: f_00004-9-4 loss: 0.952026  [  160/  306]
train() client id: f_00004-9-5 loss: 0.908067  [  192/  306]
train() client id: f_00004-9-6 loss: 0.898442  [  224/  306]
train() client id: f_00004-9-7 loss: 0.940291  [  256/  306]
train() client id: f_00004-9-8 loss: 0.882322  [  288/  306]
train() client id: f_00004-10-0 loss: 0.959068  [   32/  306]
train() client id: f_00004-10-1 loss: 1.009586  [   64/  306]
train() client id: f_00004-10-2 loss: 0.874116  [   96/  306]
train() client id: f_00004-10-3 loss: 1.104148  [  128/  306]
train() client id: f_00004-10-4 loss: 0.971904  [  160/  306]
train() client id: f_00004-10-5 loss: 0.892896  [  192/  306]
train() client id: f_00004-10-6 loss: 0.853254  [  224/  306]
train() client id: f_00004-10-7 loss: 1.014132  [  256/  306]
train() client id: f_00004-10-8 loss: 0.896990  [  288/  306]
train() client id: f_00004-11-0 loss: 0.969888  [   32/  306]
train() client id: f_00004-11-1 loss: 0.960423  [   64/  306]
train() client id: f_00004-11-2 loss: 0.961372  [   96/  306]
train() client id: f_00004-11-3 loss: 0.903169  [  128/  306]
train() client id: f_00004-11-4 loss: 0.977258  [  160/  306]
train() client id: f_00004-11-5 loss: 0.909214  [  192/  306]
train() client id: f_00004-11-6 loss: 0.930569  [  224/  306]
train() client id: f_00004-11-7 loss: 0.933724  [  256/  306]
train() client id: f_00004-11-8 loss: 0.958176  [  288/  306]
train() client id: f_00005-0-0 loss: 0.892916  [   32/  146]
train() client id: f_00005-0-1 loss: 0.763517  [   64/  146]
train() client id: f_00005-0-2 loss: 0.665122  [   96/  146]
train() client id: f_00005-0-3 loss: 0.749620  [  128/  146]
train() client id: f_00005-1-0 loss: 0.921003  [   32/  146]
train() client id: f_00005-1-1 loss: 0.542446  [   64/  146]
train() client id: f_00005-1-2 loss: 0.739709  [   96/  146]
train() client id: f_00005-1-3 loss: 0.811492  [  128/  146]
train() client id: f_00005-2-0 loss: 0.859436  [   32/  146]
train() client id: f_00005-2-1 loss: 0.641923  [   64/  146]
train() client id: f_00005-2-2 loss: 0.714869  [   96/  146]
train() client id: f_00005-2-3 loss: 0.910085  [  128/  146]
train() client id: f_00005-3-0 loss: 0.544945  [   32/  146]
train() client id: f_00005-3-1 loss: 1.097101  [   64/  146]
train() client id: f_00005-3-2 loss: 0.671068  [   96/  146]
train() client id: f_00005-3-3 loss: 0.806125  [  128/  146]
train() client id: f_00005-4-0 loss: 0.682039  [   32/  146]
train() client id: f_00005-4-1 loss: 0.625311  [   64/  146]
train() client id: f_00005-4-2 loss: 0.781751  [   96/  146]
train() client id: f_00005-4-3 loss: 0.803823  [  128/  146]
train() client id: f_00005-5-0 loss: 0.508472  [   32/  146]
train() client id: f_00005-5-1 loss: 0.799631  [   64/  146]
train() client id: f_00005-5-2 loss: 0.741573  [   96/  146]
train() client id: f_00005-5-3 loss: 0.731751  [  128/  146]
train() client id: f_00005-6-0 loss: 0.802956  [   32/  146]
train() client id: f_00005-6-1 loss: 0.865999  [   64/  146]
train() client id: f_00005-6-2 loss: 0.576747  [   96/  146]
train() client id: f_00005-6-3 loss: 0.725476  [  128/  146]
train() client id: f_00005-7-0 loss: 0.836610  [   32/  146]
train() client id: f_00005-7-1 loss: 0.574620  [   64/  146]
train() client id: f_00005-7-2 loss: 0.803735  [   96/  146]
train() client id: f_00005-7-3 loss: 0.745689  [  128/  146]
train() client id: f_00005-8-0 loss: 0.962169  [   32/  146]
train() client id: f_00005-8-1 loss: 0.751556  [   64/  146]
train() client id: f_00005-8-2 loss: 0.552859  [   96/  146]
train() client id: f_00005-8-3 loss: 0.795235  [  128/  146]
train() client id: f_00005-9-0 loss: 0.739529  [   32/  146]
train() client id: f_00005-9-1 loss: 1.015719  [   64/  146]
train() client id: f_00005-9-2 loss: 0.574247  [   96/  146]
train() client id: f_00005-9-3 loss: 0.663221  [  128/  146]
train() client id: f_00005-10-0 loss: 0.773238  [   32/  146]
train() client id: f_00005-10-1 loss: 0.777786  [   64/  146]
train() client id: f_00005-10-2 loss: 0.762065  [   96/  146]
train() client id: f_00005-10-3 loss: 0.633856  [  128/  146]
train() client id: f_00005-11-0 loss: 0.637647  [   32/  146]
train() client id: f_00005-11-1 loss: 0.826115  [   64/  146]
train() client id: f_00005-11-2 loss: 0.827935  [   96/  146]
train() client id: f_00005-11-3 loss: 0.689244  [  128/  146]
train() client id: f_00006-0-0 loss: 0.685548  [   32/   54]
train() client id: f_00006-1-0 loss: 0.619154  [   32/   54]
train() client id: f_00006-2-0 loss: 0.580934  [   32/   54]
train() client id: f_00006-3-0 loss: 0.634399  [   32/   54]
train() client id: f_00006-4-0 loss: 0.578232  [   32/   54]
train() client id: f_00006-5-0 loss: 0.593130  [   32/   54]
train() client id: f_00006-6-0 loss: 0.621029  [   32/   54]
train() client id: f_00006-7-0 loss: 0.685897  [   32/   54]
train() client id: f_00006-8-0 loss: 0.634339  [   32/   54]
train() client id: f_00006-9-0 loss: 0.685366  [   32/   54]
train() client id: f_00006-10-0 loss: 0.589383  [   32/   54]
train() client id: f_00006-11-0 loss: 0.647190  [   32/   54]
train() client id: f_00007-0-0 loss: 0.637639  [   32/  179]
train() client id: f_00007-0-1 loss: 0.681708  [   64/  179]
train() client id: f_00007-0-2 loss: 0.720162  [   96/  179]
train() client id: f_00007-0-3 loss: 0.850747  [  128/  179]
train() client id: f_00007-0-4 loss: 0.752206  [  160/  179]
train() client id: f_00007-1-0 loss: 0.783064  [   32/  179]
train() client id: f_00007-1-1 loss: 0.687513  [   64/  179]
train() client id: f_00007-1-2 loss: 0.668153  [   96/  179]
train() client id: f_00007-1-3 loss: 0.795481  [  128/  179]
train() client id: f_00007-1-4 loss: 0.742562  [  160/  179]
train() client id: f_00007-2-0 loss: 0.660656  [   32/  179]
train() client id: f_00007-2-1 loss: 0.606522  [   64/  179]
train() client id: f_00007-2-2 loss: 0.632526  [   96/  179]
train() client id: f_00007-2-3 loss: 0.735034  [  128/  179]
train() client id: f_00007-2-4 loss: 0.869573  [  160/  179]
train() client id: f_00007-3-0 loss: 0.564851  [   32/  179]
train() client id: f_00007-3-1 loss: 0.865933  [   64/  179]
train() client id: f_00007-3-2 loss: 0.612431  [   96/  179]
train() client id: f_00007-3-3 loss: 0.656839  [  128/  179]
train() client id: f_00007-3-4 loss: 0.690644  [  160/  179]
train() client id: f_00007-4-0 loss: 0.676807  [   32/  179]
train() client id: f_00007-4-1 loss: 0.729439  [   64/  179]
train() client id: f_00007-4-2 loss: 0.661912  [   96/  179]
train() client id: f_00007-4-3 loss: 0.644837  [  128/  179]
train() client id: f_00007-4-4 loss: 0.728070  [  160/  179]
train() client id: f_00007-5-0 loss: 0.829188  [   32/  179]
train() client id: f_00007-5-1 loss: 0.650190  [   64/  179]
train() client id: f_00007-5-2 loss: 0.657674  [   96/  179]
train() client id: f_00007-5-3 loss: 0.854906  [  128/  179]
train() client id: f_00007-5-4 loss: 0.573563  [  160/  179]
train() client id: f_00007-6-0 loss: 0.835656  [   32/  179]
train() client id: f_00007-6-1 loss: 0.561611  [   64/  179]
train() client id: f_00007-6-2 loss: 0.747359  [   96/  179]
train() client id: f_00007-6-3 loss: 0.649972  [  128/  179]
train() client id: f_00007-6-4 loss: 0.672184  [  160/  179]
train() client id: f_00007-7-0 loss: 0.663734  [   32/  179]
train() client id: f_00007-7-1 loss: 0.583883  [   64/  179]
train() client id: f_00007-7-2 loss: 0.580124  [   96/  179]
train() client id: f_00007-7-3 loss: 0.785941  [  128/  179]
train() client id: f_00007-7-4 loss: 0.930932  [  160/  179]
train() client id: f_00007-8-0 loss: 0.723108  [   32/  179]
train() client id: f_00007-8-1 loss: 0.654973  [   64/  179]
train() client id: f_00007-8-2 loss: 0.759751  [   96/  179]
train() client id: f_00007-8-3 loss: 0.752010  [  128/  179]
train() client id: f_00007-8-4 loss: 0.668883  [  160/  179]
train() client id: f_00007-9-0 loss: 0.725121  [   32/  179]
train() client id: f_00007-9-1 loss: 0.911177  [   64/  179]
train() client id: f_00007-9-2 loss: 0.567612  [   96/  179]
train() client id: f_00007-9-3 loss: 0.658131  [  128/  179]
train() client id: f_00007-9-4 loss: 0.675107  [  160/  179]
train() client id: f_00007-10-0 loss: 0.656864  [   32/  179]
train() client id: f_00007-10-1 loss: 0.703055  [   64/  179]
train() client id: f_00007-10-2 loss: 0.730601  [   96/  179]
train() client id: f_00007-10-3 loss: 0.661038  [  128/  179]
train() client id: f_00007-10-4 loss: 0.803907  [  160/  179]
train() client id: f_00007-11-0 loss: 0.809105  [   32/  179]
train() client id: f_00007-11-1 loss: 0.741169  [   64/  179]
train() client id: f_00007-11-2 loss: 0.578541  [   96/  179]
train() client id: f_00007-11-3 loss: 0.658480  [  128/  179]
train() client id: f_00007-11-4 loss: 0.656452  [  160/  179]
train() client id: f_00008-0-0 loss: 0.743293  [   32/  130]
train() client id: f_00008-0-1 loss: 0.808983  [   64/  130]
train() client id: f_00008-0-2 loss: 0.677972  [   96/  130]
train() client id: f_00008-0-3 loss: 0.767000  [  128/  130]
train() client id: f_00008-1-0 loss: 0.713647  [   32/  130]
train() client id: f_00008-1-1 loss: 0.753536  [   64/  130]
train() client id: f_00008-1-2 loss: 0.808431  [   96/  130]
train() client id: f_00008-1-3 loss: 0.725297  [  128/  130]
train() client id: f_00008-2-0 loss: 0.728785  [   32/  130]
train() client id: f_00008-2-1 loss: 0.710936  [   64/  130]
train() client id: f_00008-2-2 loss: 0.855228  [   96/  130]
train() client id: f_00008-2-3 loss: 0.711736  [  128/  130]
train() client id: f_00008-3-0 loss: 0.801574  [   32/  130]
train() client id: f_00008-3-1 loss: 0.673675  [   64/  130]
train() client id: f_00008-3-2 loss: 0.774032  [   96/  130]
train() client id: f_00008-3-3 loss: 0.756934  [  128/  130]
train() client id: f_00008-4-0 loss: 0.628834  [   32/  130]
train() client id: f_00008-4-1 loss: 0.783371  [   64/  130]
train() client id: f_00008-4-2 loss: 0.798693  [   96/  130]
train() client id: f_00008-4-3 loss: 0.736321  [  128/  130]
train() client id: f_00008-5-0 loss: 0.837526  [   32/  130]
train() client id: f_00008-5-1 loss: 0.670478  [   64/  130]
train() client id: f_00008-5-2 loss: 0.685517  [   96/  130]
train() client id: f_00008-5-3 loss: 0.807042  [  128/  130]
train() client id: f_00008-6-0 loss: 0.820813  [   32/  130]
train() client id: f_00008-6-1 loss: 0.704179  [   64/  130]
train() client id: f_00008-6-2 loss: 0.774430  [   96/  130]
train() client id: f_00008-6-3 loss: 0.670678  [  128/  130]
train() client id: f_00008-7-0 loss: 0.812890  [   32/  130]
train() client id: f_00008-7-1 loss: 0.784224  [   64/  130]
train() client id: f_00008-7-2 loss: 0.773657  [   96/  130]
train() client id: f_00008-7-3 loss: 0.636719  [  128/  130]
train() client id: f_00008-8-0 loss: 0.714032  [   32/  130]
train() client id: f_00008-8-1 loss: 0.651502  [   64/  130]
train() client id: f_00008-8-2 loss: 0.819588  [   96/  130]
train() client id: f_00008-8-3 loss: 0.807073  [  128/  130]
train() client id: f_00008-9-0 loss: 0.851089  [   32/  130]
train() client id: f_00008-9-1 loss: 0.720968  [   64/  130]
train() client id: f_00008-9-2 loss: 0.816469  [   96/  130]
train() client id: f_00008-9-3 loss: 0.614999  [  128/  130]
train() client id: f_00008-10-0 loss: 0.668569  [   32/  130]
train() client id: f_00008-10-1 loss: 0.755907  [   64/  130]
train() client id: f_00008-10-2 loss: 0.770779  [   96/  130]
train() client id: f_00008-10-3 loss: 0.806592  [  128/  130]
train() client id: f_00008-11-0 loss: 0.739847  [   32/  130]
train() client id: f_00008-11-1 loss: 0.797026  [   64/  130]
train() client id: f_00008-11-2 loss: 0.678012  [   96/  130]
train() client id: f_00008-11-3 loss: 0.748501  [  128/  130]
train() client id: f_00009-0-0 loss: 1.247562  [   32/  118]
train() client id: f_00009-0-1 loss: 1.004382  [   64/  118]
train() client id: f_00009-0-2 loss: 1.056448  [   96/  118]
train() client id: f_00009-1-0 loss: 1.041449  [   32/  118]
train() client id: f_00009-1-1 loss: 0.986921  [   64/  118]
train() client id: f_00009-1-2 loss: 1.090958  [   96/  118]
train() client id: f_00009-2-0 loss: 1.153348  [   32/  118]
train() client id: f_00009-2-1 loss: 1.045578  [   64/  118]
train() client id: f_00009-2-2 loss: 0.916431  [   96/  118]
train() client id: f_00009-3-0 loss: 1.040004  [   32/  118]
train() client id: f_00009-3-1 loss: 1.062169  [   64/  118]
train() client id: f_00009-3-2 loss: 0.783891  [   96/  118]
train() client id: f_00009-4-0 loss: 0.873763  [   32/  118]
train() client id: f_00009-4-1 loss: 0.896567  [   64/  118]
train() client id: f_00009-4-2 loss: 1.040529  [   96/  118]
train() client id: f_00009-5-0 loss: 0.872266  [   32/  118]
train() client id: f_00009-5-1 loss: 0.903671  [   64/  118]
train() client id: f_00009-5-2 loss: 1.033201  [   96/  118]
train() client id: f_00009-6-0 loss: 0.943506  [   32/  118]
train() client id: f_00009-6-1 loss: 0.880866  [   64/  118]
train() client id: f_00009-6-2 loss: 0.814462  [   96/  118]
train() client id: f_00009-7-0 loss: 0.788184  [   32/  118]
train() client id: f_00009-7-1 loss: 0.908611  [   64/  118]
train() client id: f_00009-7-2 loss: 0.915074  [   96/  118]
train() client id: f_00009-8-0 loss: 0.932603  [   32/  118]
train() client id: f_00009-8-1 loss: 0.716595  [   64/  118]
train() client id: f_00009-8-2 loss: 1.041192  [   96/  118]
train() client id: f_00009-9-0 loss: 0.874830  [   32/  118]
train() client id: f_00009-9-1 loss: 0.844428  [   64/  118]
train() client id: f_00009-9-2 loss: 0.946365  [   96/  118]
train() client id: f_00009-10-0 loss: 0.878927  [   32/  118]
train() client id: f_00009-10-1 loss: 0.880475  [   64/  118]
train() client id: f_00009-10-2 loss: 0.793346  [   96/  118]
train() client id: f_00009-11-0 loss: 0.828535  [   32/  118]
train() client id: f_00009-11-1 loss: 0.838990  [   64/  118]
train() client id: f_00009-11-2 loss: 0.846145  [   96/  118]
At round 17 accuracy: 0.6339522546419099
At round 17 training accuracy: 0.5761234071093226
At round 17 training loss: 0.8498964584965768
update_location
xs = [ -3.9056584    4.20031788 105.00902392  18.81129433   0.97929623
   3.95640986 -67.44319194 -46.32485185  89.66397685 -32.06087855]
ys = [ 97.5879595   80.55583871   1.32061395 -67.45517586  59.35018685
  42.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [139.78077123 128.47912601 145.01254816 122.08220814 116.29017026
 108.85175553 120.64607197 110.21192383 135.45589215 105.09001756]
dists_bs = [187.79216635 202.56468974 329.49898727 310.39866319 210.57918222
 222.48257319 207.65139541 216.55287625 307.94729826 222.87235287]
uav_gains = [4.32717486e-11 5.34395670e-11 3.94635132e-11 6.07214029e-11
 6.85691473e-11 8.08920872e-11 6.25451947e-11 7.84191512e-11
 4.68155255e-11 8.83269213e-11]
bs_gains = [4.75332122e-11 3.84517052e-11 9.84697079e-12 1.16390766e-11
 3.44929920e-11 2.95709508e-11 3.58720746e-11 3.18944419e-11
 1.19003619e-11 2.94263727e-11]
Round 18
-------------------------------
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.61607623 17.94328127  8.48520447  3.03808191 20.69729735  9.97190604
  3.77497723 12.15244223  8.94539714  8.09409538]
obj_prev = 101.71875926654823
eta_min = 1.6526197130074664e-11	eta_max = 0.921782499053972
af = 21.498465320302763	bf = 1.69748013632556	zeta = 23.64831185233304	eta = 0.9090909090909091
af = 21.498465320302763	bf = 1.69748013632556	zeta = 41.211530046727255	eta = 0.5216614208675814
af = 21.498465320302763	bf = 1.69748013632556	zeta = 32.795424289084025	eta = 0.6555324648584754
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.284347894451706	eta = 0.6871955711793989
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.2090722305542	eta = 0.6888530732821788
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.208871892116978	eta = 0.6888574952218328
eta = 0.6888574952218328
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.03075603 0.06468531 0.03026784 0.01049611 0.07469322 0.03563794
 0.01318116 0.04369309 0.03173241 0.02880327]
ene_total = [2.70140281 5.11569384 2.67726863 1.23269158 5.83681499 3.09264715
 1.41984069 3.55816331 2.96469034 2.60965854]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 0 obj = 5.492600646974247
eta = 0.6888574952218328
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
eta_min = 0.6888574952218411	eta_max = 0.6888574952218237
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 0.033943701105718926	eta = 0.909090909090909
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 18.704633302137236	eta = 0.001649746862055949
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8983226351773008	eta = 0.016255355925430464
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547436567121627	eta = 0.016637291080325793
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547352592532886	eta = 0.016637366406961725
eta = 0.016637366406961725
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.75913676e-04 1.65510932e-03 1.69142568e-04 6.79053635e-06
 2.57637780e-03 2.84503340e-04 1.34170925e-05 4.80434863e-04
 2.28976553e-04 1.50283948e-04]
ene_total = [0.17007518 0.20743924 0.17321761 0.15518366 0.23216334 0.18699632
 0.15443092 0.1583265  0.23270867 0.18419382]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 1 obj = 5.492600646974393
eta = 0.6888574952218411
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
Done!
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.57385166e-05 1.48078115e-04 1.51327241e-05 6.07530761e-07
 2.30501493e-04 2.54537376e-05 1.20039067e-06 4.29831965e-05
 2.04859075e-05 1.34454948e-05]
ene_total = [0.0074361  0.00775808 0.00758262 0.00692498 0.00802352 0.008093
 0.00688532 0.00663408 0.01018527 0.00809004]
At round 18 energy consumption: 0.07761301768692713
At round 18 eta: 0.6888574952218411
At round 18 a_n: 22.016777610806663
At round 18 local rounds: 12.204774532627562
At round 18 global rounds: 70.7610733753795
gradient difference: 0.41052788496017456
train() client id: f_00000-0-0 loss: 1.267628  [   32/  126]
train() client id: f_00000-0-1 loss: 1.208964  [   64/  126]
train() client id: f_00000-0-2 loss: 0.980212  [   96/  126]
train() client id: f_00000-1-0 loss: 1.042767  [   32/  126]
train() client id: f_00000-1-1 loss: 1.147233  [   64/  126]
train() client id: f_00000-1-2 loss: 1.131636  [   96/  126]
train() client id: f_00000-2-0 loss: 1.053651  [   32/  126]
train() client id: f_00000-2-1 loss: 0.970649  [   64/  126]
train() client id: f_00000-2-2 loss: 1.066710  [   96/  126]
train() client id: f_00000-3-0 loss: 0.991071  [   32/  126]
train() client id: f_00000-3-1 loss: 0.971228  [   64/  126]
train() client id: f_00000-3-2 loss: 0.943553  [   96/  126]
train() client id: f_00000-4-0 loss: 0.933569  [   32/  126]
train() client id: f_00000-4-1 loss: 0.850371  [   64/  126]
train() client id: f_00000-4-2 loss: 0.930628  [   96/  126]
train() client id: f_00000-5-0 loss: 0.785292  [   32/  126]
train() client id: f_00000-5-1 loss: 0.875178  [   64/  126]
train() client id: f_00000-5-2 loss: 1.011271  [   96/  126]
train() client id: f_00000-6-0 loss: 0.855404  [   32/  126]
train() client id: f_00000-6-1 loss: 0.947574  [   64/  126]
train() client id: f_00000-6-2 loss: 0.875895  [   96/  126]
train() client id: f_00000-7-0 loss: 0.851416  [   32/  126]
train() client id: f_00000-7-1 loss: 0.845078  [   64/  126]
train() client id: f_00000-7-2 loss: 0.890828  [   96/  126]
train() client id: f_00000-8-0 loss: 0.810862  [   32/  126]
train() client id: f_00000-8-1 loss: 0.856401  [   64/  126]
train() client id: f_00000-8-2 loss: 0.850933  [   96/  126]
train() client id: f_00000-9-0 loss: 0.846395  [   32/  126]
train() client id: f_00000-9-1 loss: 0.868451  [   64/  126]
train() client id: f_00000-9-2 loss: 0.869401  [   96/  126]
train() client id: f_00000-10-0 loss: 0.886896  [   32/  126]
train() client id: f_00000-10-1 loss: 0.907065  [   64/  126]
train() client id: f_00000-10-2 loss: 0.769009  [   96/  126]
train() client id: f_00000-11-0 loss: 0.841116  [   32/  126]
train() client id: f_00000-11-1 loss: 0.895640  [   64/  126]
train() client id: f_00000-11-2 loss: 0.857391  [   96/  126]
train() client id: f_00001-0-0 loss: 0.442267  [   32/  265]
train() client id: f_00001-0-1 loss: 0.436769  [   64/  265]
train() client id: f_00001-0-2 loss: 0.550621  [   96/  265]
train() client id: f_00001-0-3 loss: 0.517629  [  128/  265]
train() client id: f_00001-0-4 loss: 0.544153  [  160/  265]
train() client id: f_00001-0-5 loss: 0.527092  [  192/  265]
train() client id: f_00001-0-6 loss: 0.487875  [  224/  265]
train() client id: f_00001-0-7 loss: 0.496448  [  256/  265]
train() client id: f_00001-1-0 loss: 0.659860  [   32/  265]
train() client id: f_00001-1-1 loss: 0.593265  [   64/  265]
train() client id: f_00001-1-2 loss: 0.429107  [   96/  265]
train() client id: f_00001-1-3 loss: 0.406050  [  128/  265]
train() client id: f_00001-1-4 loss: 0.417142  [  160/  265]
train() client id: f_00001-1-5 loss: 0.512373  [  192/  265]
train() client id: f_00001-1-6 loss: 0.507704  [  224/  265]
train() client id: f_00001-1-7 loss: 0.389417  [  256/  265]
train() client id: f_00001-2-0 loss: 0.563263  [   32/  265]
train() client id: f_00001-2-1 loss: 0.450147  [   64/  265]
train() client id: f_00001-2-2 loss: 0.559270  [   96/  265]
train() client id: f_00001-2-3 loss: 0.470503  [  128/  265]
train() client id: f_00001-2-4 loss: 0.409234  [  160/  265]
train() client id: f_00001-2-5 loss: 0.392848  [  192/  265]
train() client id: f_00001-2-6 loss: 0.440823  [  224/  265]
train() client id: f_00001-2-7 loss: 0.569640  [  256/  265]
train() client id: f_00001-3-0 loss: 0.408604  [   32/  265]
train() client id: f_00001-3-1 loss: 0.538802  [   64/  265]
train() client id: f_00001-3-2 loss: 0.429915  [   96/  265]
train() client id: f_00001-3-3 loss: 0.406629  [  128/  265]
train() client id: f_00001-3-4 loss: 0.550980  [  160/  265]
train() client id: f_00001-3-5 loss: 0.548425  [  192/  265]
train() client id: f_00001-3-6 loss: 0.402242  [  224/  265]
train() client id: f_00001-3-7 loss: 0.529516  [  256/  265]
train() client id: f_00001-4-0 loss: 0.409221  [   32/  265]
train() client id: f_00001-4-1 loss: 0.693895  [   64/  265]
train() client id: f_00001-4-2 loss: 0.417730  [   96/  265]
train() client id: f_00001-4-3 loss: 0.375081  [  128/  265]
train() client id: f_00001-4-4 loss: 0.465150  [  160/  265]
train() client id: f_00001-4-5 loss: 0.385499  [  192/  265]
train() client id: f_00001-4-6 loss: 0.538592  [  224/  265]
train() client id: f_00001-4-7 loss: 0.457164  [  256/  265]
train() client id: f_00001-5-0 loss: 0.618099  [   32/  265]
train() client id: f_00001-5-1 loss: 0.499245  [   64/  265]
train() client id: f_00001-5-2 loss: 0.376447  [   96/  265]
train() client id: f_00001-5-3 loss: 0.372992  [  128/  265]
train() client id: f_00001-5-4 loss: 0.393078  [  160/  265]
train() client id: f_00001-5-5 loss: 0.570955  [  192/  265]
train() client id: f_00001-5-6 loss: 0.367372  [  224/  265]
train() client id: f_00001-5-7 loss: 0.528195  [  256/  265]
train() client id: f_00001-6-0 loss: 0.478971  [   32/  265]
train() client id: f_00001-6-1 loss: 0.419557  [   64/  265]
train() client id: f_00001-6-2 loss: 0.447184  [   96/  265]
train() client id: f_00001-6-3 loss: 0.531475  [  128/  265]
train() client id: f_00001-6-4 loss: 0.471240  [  160/  265]
train() client id: f_00001-6-5 loss: 0.366803  [  192/  265]
train() client id: f_00001-6-6 loss: 0.453139  [  224/  265]
train() client id: f_00001-6-7 loss: 0.464310  [  256/  265]
train() client id: f_00001-7-0 loss: 0.546453  [   32/  265]
train() client id: f_00001-7-1 loss: 0.529701  [   64/  265]
train() client id: f_00001-7-2 loss: 0.397347  [   96/  265]
train() client id: f_00001-7-3 loss: 0.569448  [  128/  265]
train() client id: f_00001-7-4 loss: 0.421934  [  160/  265]
train() client id: f_00001-7-5 loss: 0.373324  [  192/  265]
train() client id: f_00001-7-6 loss: 0.373509  [  224/  265]
train() client id: f_00001-7-7 loss: 0.451436  [  256/  265]
train() client id: f_00001-8-0 loss: 0.366083  [   32/  265]
train() client id: f_00001-8-1 loss: 0.358074  [   64/  265]
train() client id: f_00001-8-2 loss: 0.515712  [   96/  265]
train() client id: f_00001-8-3 loss: 0.538495  [  128/  265]
train() client id: f_00001-8-4 loss: 0.382492  [  160/  265]
train() client id: f_00001-8-5 loss: 0.502071  [  192/  265]
train() client id: f_00001-8-6 loss: 0.409815  [  224/  265]
train() client id: f_00001-8-7 loss: 0.551453  [  256/  265]
train() client id: f_00001-9-0 loss: 0.476806  [   32/  265]
train() client id: f_00001-9-1 loss: 0.375265  [   64/  265]
train() client id: f_00001-9-2 loss: 0.495574  [   96/  265]
train() client id: f_00001-9-3 loss: 0.435219  [  128/  265]
train() client id: f_00001-9-4 loss: 0.344666  [  160/  265]
train() client id: f_00001-9-5 loss: 0.504385  [  192/  265]
train() client id: f_00001-9-6 loss: 0.507259  [  224/  265]
train() client id: f_00001-9-7 loss: 0.517429  [  256/  265]
train() client id: f_00001-10-0 loss: 0.414891  [   32/  265]
train() client id: f_00001-10-1 loss: 0.383150  [   64/  265]
train() client id: f_00001-10-2 loss: 0.355613  [   96/  265]
train() client id: f_00001-10-3 loss: 0.420415  [  128/  265]
train() client id: f_00001-10-4 loss: 0.636350  [  160/  265]
train() client id: f_00001-10-5 loss: 0.517109  [  192/  265]
train() client id: f_00001-10-6 loss: 0.430444  [  224/  265]
train() client id: f_00001-10-7 loss: 0.412227  [  256/  265]
train() client id: f_00001-11-0 loss: 0.354553  [   32/  265]
train() client id: f_00001-11-1 loss: 0.452742  [   64/  265]
train() client id: f_00001-11-2 loss: 0.391586  [   96/  265]
train() client id: f_00001-11-3 loss: 0.417337  [  128/  265]
train() client id: f_00001-11-4 loss: 0.507268  [  160/  265]
train() client id: f_00001-11-5 loss: 0.532533  [  192/  265]
train() client id: f_00001-11-6 loss: 0.483499  [  224/  265]
train() client id: f_00001-11-7 loss: 0.505780  [  256/  265]
train() client id: f_00002-0-0 loss: 1.136408  [   32/  124]
train() client id: f_00002-0-1 loss: 1.096584  [   64/  124]
train() client id: f_00002-0-2 loss: 1.222945  [   96/  124]
train() client id: f_00002-1-0 loss: 1.099580  [   32/  124]
train() client id: f_00002-1-1 loss: 1.149773  [   64/  124]
train() client id: f_00002-1-2 loss: 1.107820  [   96/  124]
train() client id: f_00002-2-0 loss: 1.093264  [   32/  124]
train() client id: f_00002-2-1 loss: 1.072268  [   64/  124]
train() client id: f_00002-2-2 loss: 1.126823  [   96/  124]
train() client id: f_00002-3-0 loss: 1.016503  [   32/  124]
train() client id: f_00002-3-1 loss: 1.083592  [   64/  124]
train() client id: f_00002-3-2 loss: 0.977193  [   96/  124]
train() client id: f_00002-4-0 loss: 0.868831  [   32/  124]
train() client id: f_00002-4-1 loss: 1.131797  [   64/  124]
train() client id: f_00002-4-2 loss: 1.063398  [   96/  124]
train() client id: f_00002-5-0 loss: 1.057670  [   32/  124]
train() client id: f_00002-5-1 loss: 1.004629  [   64/  124]
train() client id: f_00002-5-2 loss: 0.965364  [   96/  124]
train() client id: f_00002-6-0 loss: 0.962460  [   32/  124]
train() client id: f_00002-6-1 loss: 0.952265  [   64/  124]
train() client id: f_00002-6-2 loss: 1.035301  [   96/  124]
train() client id: f_00002-7-0 loss: 1.120169  [   32/  124]
train() client id: f_00002-7-1 loss: 1.040379  [   64/  124]
train() client id: f_00002-7-2 loss: 0.764846  [   96/  124]
train() client id: f_00002-8-0 loss: 0.952127  [   32/  124]
train() client id: f_00002-8-1 loss: 0.995603  [   64/  124]
train() client id: f_00002-8-2 loss: 0.955203  [   96/  124]
train() client id: f_00002-9-0 loss: 0.998144  [   32/  124]
train() client id: f_00002-9-1 loss: 1.029676  [   64/  124]
train() client id: f_00002-9-2 loss: 0.859085  [   96/  124]
train() client id: f_00002-10-0 loss: 0.927877  [   32/  124]
train() client id: f_00002-10-1 loss: 1.094668  [   64/  124]
train() client id: f_00002-10-2 loss: 0.821499  [   96/  124]
train() client id: f_00002-11-0 loss: 0.853513  [   32/  124]
train() client id: f_00002-11-1 loss: 1.079719  [   64/  124]
train() client id: f_00002-11-2 loss: 0.886081  [   96/  124]
train() client id: f_00003-0-0 loss: 1.154735  [   32/   43]
train() client id: f_00003-1-0 loss: 0.936484  [   32/   43]
train() client id: f_00003-2-0 loss: 0.967769  [   32/   43]
train() client id: f_00003-3-0 loss: 0.927979  [   32/   43]
train() client id: f_00003-4-0 loss: 0.868761  [   32/   43]
train() client id: f_00003-5-0 loss: 1.057558  [   32/   43]
train() client id: f_00003-6-0 loss: 0.905910  [   32/   43]
train() client id: f_00003-7-0 loss: 1.023441  [   32/   43]
train() client id: f_00003-8-0 loss: 0.982903  [   32/   43]
train() client id: f_00003-9-0 loss: 0.956397  [   32/   43]
train() client id: f_00003-10-0 loss: 0.922961  [   32/   43]
train() client id: f_00003-11-0 loss: 0.941633  [   32/   43]
train() client id: f_00004-0-0 loss: 0.889210  [   32/  306]
train() client id: f_00004-0-1 loss: 0.822471  [   64/  306]
train() client id: f_00004-0-2 loss: 0.939827  [   96/  306]
train() client id: f_00004-0-3 loss: 0.870012  [  128/  306]
train() client id: f_00004-0-4 loss: 0.781562  [  160/  306]
train() client id: f_00004-0-5 loss: 0.942277  [  192/  306]
train() client id: f_00004-0-6 loss: 0.940508  [  224/  306]
train() client id: f_00004-0-7 loss: 0.984924  [  256/  306]
train() client id: f_00004-0-8 loss: 0.928394  [  288/  306]
train() client id: f_00004-1-0 loss: 0.900722  [   32/  306]
train() client id: f_00004-1-1 loss: 0.687262  [   64/  306]
train() client id: f_00004-1-2 loss: 0.964947  [   96/  306]
train() client id: f_00004-1-3 loss: 0.873683  [  128/  306]
train() client id: f_00004-1-4 loss: 1.017033  [  160/  306]
train() client id: f_00004-1-5 loss: 0.900856  [  192/  306]
train() client id: f_00004-1-6 loss: 0.791707  [  224/  306]
train() client id: f_00004-1-7 loss: 0.887000  [  256/  306]
train() client id: f_00004-1-8 loss: 0.971464  [  288/  306]
train() client id: f_00004-2-0 loss: 0.861947  [   32/  306]
train() client id: f_00004-2-1 loss: 1.007592  [   64/  306]
train() client id: f_00004-2-2 loss: 0.881258  [   96/  306]
train() client id: f_00004-2-3 loss: 0.959537  [  128/  306]
train() client id: f_00004-2-4 loss: 0.740159  [  160/  306]
train() client id: f_00004-2-5 loss: 0.889140  [  192/  306]
train() client id: f_00004-2-6 loss: 0.839580  [  224/  306]
train() client id: f_00004-2-7 loss: 0.950085  [  256/  306]
train() client id: f_00004-2-8 loss: 0.873370  [  288/  306]
train() client id: f_00004-3-0 loss: 0.943185  [   32/  306]
train() client id: f_00004-3-1 loss: 0.892242  [   64/  306]
train() client id: f_00004-3-2 loss: 0.974573  [   96/  306]
train() client id: f_00004-3-3 loss: 0.865479  [  128/  306]
train() client id: f_00004-3-4 loss: 0.879817  [  160/  306]
train() client id: f_00004-3-5 loss: 0.808395  [  192/  306]
train() client id: f_00004-3-6 loss: 0.893379  [  224/  306]
train() client id: f_00004-3-7 loss: 0.904736  [  256/  306]
train() client id: f_00004-3-8 loss: 0.893755  [  288/  306]
train() client id: f_00004-4-0 loss: 0.885983  [   32/  306]
train() client id: f_00004-4-1 loss: 0.946012  [   64/  306]
train() client id: f_00004-4-2 loss: 0.769891  [   96/  306]
train() client id: f_00004-4-3 loss: 1.044098  [  128/  306]
train() client id: f_00004-4-4 loss: 0.912149  [  160/  306]
train() client id: f_00004-4-5 loss: 0.875844  [  192/  306]
train() client id: f_00004-4-6 loss: 0.862077  [  224/  306]
train() client id: f_00004-4-7 loss: 0.871362  [  256/  306]
train() client id: f_00004-4-8 loss: 0.849696  [  288/  306]
train() client id: f_00004-5-0 loss: 1.005304  [   32/  306]
train() client id: f_00004-5-1 loss: 0.974918  [   64/  306]
train() client id: f_00004-5-2 loss: 0.792068  [   96/  306]
train() client id: f_00004-5-3 loss: 0.832202  [  128/  306]
train() client id: f_00004-5-4 loss: 0.793174  [  160/  306]
train() client id: f_00004-5-5 loss: 0.904289  [  192/  306]
train() client id: f_00004-5-6 loss: 0.835915  [  224/  306]
train() client id: f_00004-5-7 loss: 0.901409  [  256/  306]
train() client id: f_00004-5-8 loss: 0.948669  [  288/  306]
train() client id: f_00004-6-0 loss: 1.011067  [   32/  306]
train() client id: f_00004-6-1 loss: 0.926900  [   64/  306]
train() client id: f_00004-6-2 loss: 0.982909  [   96/  306]
train() client id: f_00004-6-3 loss: 0.805980  [  128/  306]
train() client id: f_00004-6-4 loss: 0.890095  [  160/  306]
train() client id: f_00004-6-5 loss: 0.844080  [  192/  306]
train() client id: f_00004-6-6 loss: 0.907667  [  224/  306]
train() client id: f_00004-6-7 loss: 0.820788  [  256/  306]
train() client id: f_00004-6-8 loss: 0.904159  [  288/  306]
train() client id: f_00004-7-0 loss: 0.991279  [   32/  306]
train() client id: f_00004-7-1 loss: 1.017281  [   64/  306]
train() client id: f_00004-7-2 loss: 0.897088  [   96/  306]
train() client id: f_00004-7-3 loss: 0.884709  [  128/  306]
train() client id: f_00004-7-4 loss: 0.757599  [  160/  306]
train() client id: f_00004-7-5 loss: 0.895702  [  192/  306]
train() client id: f_00004-7-6 loss: 0.924468  [  224/  306]
train() client id: f_00004-7-7 loss: 0.808901  [  256/  306]
train() client id: f_00004-7-8 loss: 0.930642  [  288/  306]
train() client id: f_00004-8-0 loss: 0.972636  [   32/  306]
train() client id: f_00004-8-1 loss: 0.789460  [   64/  306]
train() client id: f_00004-8-2 loss: 0.900284  [   96/  306]
train() client id: f_00004-8-3 loss: 0.919227  [  128/  306]
train() client id: f_00004-8-4 loss: 0.946288  [  160/  306]
train() client id: f_00004-8-5 loss: 0.835453  [  192/  306]
train() client id: f_00004-8-6 loss: 0.901616  [  224/  306]
train() client id: f_00004-8-7 loss: 0.883298  [  256/  306]
train() client id: f_00004-8-8 loss: 0.967637  [  288/  306]
train() client id: f_00004-9-0 loss: 0.964130  [   32/  306]
train() client id: f_00004-9-1 loss: 0.749184  [   64/  306]
train() client id: f_00004-9-2 loss: 1.019597  [   96/  306]
train() client id: f_00004-9-3 loss: 0.848940  [  128/  306]
train() client id: f_00004-9-4 loss: 0.995626  [  160/  306]
train() client id: f_00004-9-5 loss: 0.854888  [  192/  306]
train() client id: f_00004-9-6 loss: 1.016228  [  224/  306]
train() client id: f_00004-9-7 loss: 0.753084  [  256/  306]
train() client id: f_00004-9-8 loss: 0.914070  [  288/  306]
train() client id: f_00004-10-0 loss: 0.868543  [   32/  306]
train() client id: f_00004-10-1 loss: 0.864605  [   64/  306]
train() client id: f_00004-10-2 loss: 0.866108  [   96/  306]
train() client id: f_00004-10-3 loss: 1.017560  [  128/  306]
train() client id: f_00004-10-4 loss: 0.911773  [  160/  306]
train() client id: f_00004-10-5 loss: 0.774406  [  192/  306]
train() client id: f_00004-10-6 loss: 1.064286  [  224/  306]
train() client id: f_00004-10-7 loss: 0.950020  [  256/  306]
train() client id: f_00004-10-8 loss: 0.891766  [  288/  306]
train() client id: f_00004-11-0 loss: 0.978350  [   32/  306]
train() client id: f_00004-11-1 loss: 1.007258  [   64/  306]
train() client id: f_00004-11-2 loss: 0.858377  [   96/  306]
train() client id: f_00004-11-3 loss: 0.932571  [  128/  306]
train() client id: f_00004-11-4 loss: 0.841109  [  160/  306]
train() client id: f_00004-11-5 loss: 0.916407  [  192/  306]
train() client id: f_00004-11-6 loss: 0.897118  [  224/  306]
train() client id: f_00004-11-7 loss: 0.928739  [  256/  306]
train() client id: f_00004-11-8 loss: 0.818006  [  288/  306]
train() client id: f_00005-0-0 loss: 0.819360  [   32/  146]
train() client id: f_00005-0-1 loss: 0.620972  [   64/  146]
train() client id: f_00005-0-2 loss: 0.756913  [   96/  146]
train() client id: f_00005-0-3 loss: 0.737724  [  128/  146]
train() client id: f_00005-1-0 loss: 0.868565  [   32/  146]
train() client id: f_00005-1-1 loss: 0.720588  [   64/  146]
train() client id: f_00005-1-2 loss: 0.643157  [   96/  146]
train() client id: f_00005-1-3 loss: 0.693488  [  128/  146]
train() client id: f_00005-2-0 loss: 0.853518  [   32/  146]
train() client id: f_00005-2-1 loss: 0.485733  [   64/  146]
train() client id: f_00005-2-2 loss: 1.008348  [   96/  146]
train() client id: f_00005-2-3 loss: 0.643305  [  128/  146]
train() client id: f_00005-3-0 loss: 0.820074  [   32/  146]
train() client id: f_00005-3-1 loss: 0.759330  [   64/  146]
train() client id: f_00005-3-2 loss: 0.685720  [   96/  146]
train() client id: f_00005-3-3 loss: 0.502515  [  128/  146]
train() client id: f_00005-4-0 loss: 0.517199  [   32/  146]
train() client id: f_00005-4-1 loss: 0.697582  [   64/  146]
train() client id: f_00005-4-2 loss: 0.708944  [   96/  146]
train() client id: f_00005-4-3 loss: 0.805238  [  128/  146]
train() client id: f_00005-5-0 loss: 0.796590  [   32/  146]
train() client id: f_00005-5-1 loss: 0.823715  [   64/  146]
train() client id: f_00005-5-2 loss: 0.664156  [   96/  146]
train() client id: f_00005-5-3 loss: 0.684477  [  128/  146]
train() client id: f_00005-6-0 loss: 0.676417  [   32/  146]
train() client id: f_00005-6-1 loss: 0.888250  [   64/  146]
train() client id: f_00005-6-2 loss: 0.761044  [   96/  146]
train() client id: f_00005-6-3 loss: 0.620150  [  128/  146]
train() client id: f_00005-7-0 loss: 0.496164  [   32/  146]
train() client id: f_00005-7-1 loss: 0.855439  [   64/  146]
train() client id: f_00005-7-2 loss: 0.814691  [   96/  146]
train() client id: f_00005-7-3 loss: 0.803371  [  128/  146]
train() client id: f_00005-8-0 loss: 0.685465  [   32/  146]
train() client id: f_00005-8-1 loss: 0.695794  [   64/  146]
train() client id: f_00005-8-2 loss: 0.848993  [   96/  146]
train() client id: f_00005-8-3 loss: 0.602360  [  128/  146]
train() client id: f_00005-9-0 loss: 0.670293  [   32/  146]
train() client id: f_00005-9-1 loss: 0.791792  [   64/  146]
train() client id: f_00005-9-2 loss: 0.741093  [   96/  146]
train() client id: f_00005-9-3 loss: 0.580747  [  128/  146]
train() client id: f_00005-10-0 loss: 0.687700  [   32/  146]
train() client id: f_00005-10-1 loss: 0.734726  [   64/  146]
train() client id: f_00005-10-2 loss: 0.781956  [   96/  146]
train() client id: f_00005-10-3 loss: 0.623030  [  128/  146]
train() client id: f_00005-11-0 loss: 0.789445  [   32/  146]
train() client id: f_00005-11-1 loss: 0.788891  [   64/  146]
train() client id: f_00005-11-2 loss: 0.769705  [   96/  146]
train() client id: f_00005-11-3 loss: 0.484460  [  128/  146]
train() client id: f_00006-0-0 loss: 0.572401  [   32/   54]
train() client id: f_00006-1-0 loss: 0.562276  [   32/   54]
train() client id: f_00006-2-0 loss: 0.620245  [   32/   54]
train() client id: f_00006-3-0 loss: 0.544173  [   32/   54]
train() client id: f_00006-4-0 loss: 0.566612  [   32/   54]
train() client id: f_00006-5-0 loss: 0.623146  [   32/   54]
train() client id: f_00006-6-0 loss: 0.565931  [   32/   54]
train() client id: f_00006-7-0 loss: 0.520705  [   32/   54]
train() client id: f_00006-8-0 loss: 0.507115  [   32/   54]
train() client id: f_00006-9-0 loss: 0.624216  [   32/   54]
train() client id: f_00006-10-0 loss: 0.614472  [   32/   54]
train() client id: f_00006-11-0 loss: 0.507490  [   32/   54]
train() client id: f_00007-0-0 loss: 0.585557  [   32/  179]
train() client id: f_00007-0-1 loss: 0.784856  [   64/  179]
train() client id: f_00007-0-2 loss: 0.665418  [   96/  179]
train() client id: f_00007-0-3 loss: 0.686293  [  128/  179]
train() client id: f_00007-0-4 loss: 0.602648  [  160/  179]
train() client id: f_00007-1-0 loss: 0.685481  [   32/  179]
train() client id: f_00007-1-1 loss: 0.647819  [   64/  179]
train() client id: f_00007-1-2 loss: 0.545795  [   96/  179]
train() client id: f_00007-1-3 loss: 0.627150  [  128/  179]
train() client id: f_00007-1-4 loss: 0.776682  [  160/  179]
train() client id: f_00007-2-0 loss: 0.744201  [   32/  179]
train() client id: f_00007-2-1 loss: 0.569198  [   64/  179]
train() client id: f_00007-2-2 loss: 0.598166  [   96/  179]
train() client id: f_00007-2-3 loss: 0.538905  [  128/  179]
train() client id: f_00007-2-4 loss: 0.707560  [  160/  179]
train() client id: f_00007-3-0 loss: 0.543282  [   32/  179]
train() client id: f_00007-3-1 loss: 0.736569  [   64/  179]
train() client id: f_00007-3-2 loss: 0.697759  [   96/  179]
train() client id: f_00007-3-3 loss: 0.567715  [  128/  179]
train() client id: f_00007-3-4 loss: 0.489139  [  160/  179]
train() client id: f_00007-4-0 loss: 0.662948  [   32/  179]
train() client id: f_00007-4-1 loss: 0.515002  [   64/  179]
train() client id: f_00007-4-2 loss: 0.611788  [   96/  179]
train() client id: f_00007-4-3 loss: 0.742483  [  128/  179]
train() client id: f_00007-4-4 loss: 0.547278  [  160/  179]
train() client id: f_00007-5-0 loss: 0.570446  [   32/  179]
train() client id: f_00007-5-1 loss: 0.480401  [   64/  179]
train() client id: f_00007-5-2 loss: 0.614709  [   96/  179]
train() client id: f_00007-5-3 loss: 0.583390  [  128/  179]
train() client id: f_00007-5-4 loss: 0.809476  [  160/  179]
train() client id: f_00007-6-0 loss: 0.476261  [   32/  179]
train() client id: f_00007-6-1 loss: 0.581204  [   64/  179]
train() client id: f_00007-6-2 loss: 0.475359  [   96/  179]
train() client id: f_00007-6-3 loss: 0.801375  [  128/  179]
train() client id: f_00007-6-4 loss: 0.746505  [  160/  179]
train() client id: f_00007-7-0 loss: 0.558025  [   32/  179]
train() client id: f_00007-7-1 loss: 0.653202  [   64/  179]
train() client id: f_00007-7-2 loss: 0.583646  [   96/  179]
train() client id: f_00007-7-3 loss: 0.718939  [  128/  179]
train() client id: f_00007-7-4 loss: 0.574351  [  160/  179]
train() client id: f_00007-8-0 loss: 0.612838  [   32/  179]
train() client id: f_00007-8-1 loss: 0.465593  [   64/  179]
train() client id: f_00007-8-2 loss: 0.673998  [   96/  179]
train() client id: f_00007-8-3 loss: 0.537683  [  128/  179]
train() client id: f_00007-8-4 loss: 0.674003  [  160/  179]
train() client id: f_00007-9-0 loss: 0.773434  [   32/  179]
train() client id: f_00007-9-1 loss: 0.541266  [   64/  179]
train() client id: f_00007-9-2 loss: 0.630060  [   96/  179]
train() client id: f_00007-9-3 loss: 0.559531  [  128/  179]
train() client id: f_00007-9-4 loss: 0.567638  [  160/  179]
train() client id: f_00007-10-0 loss: 0.567299  [   32/  179]
train() client id: f_00007-10-1 loss: 0.713428  [   64/  179]
train() client id: f_00007-10-2 loss: 0.746987  [   96/  179]
train() client id: f_00007-10-3 loss: 0.474782  [  128/  179]
train() client id: f_00007-10-4 loss: 0.559887  [  160/  179]
train() client id: f_00007-11-0 loss: 0.538221  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582540  [   64/  179]
train() client id: f_00007-11-2 loss: 0.688088  [   96/  179]
train() client id: f_00007-11-3 loss: 0.654407  [  128/  179]
train() client id: f_00007-11-4 loss: 0.622681  [  160/  179]
train() client id: f_00008-0-0 loss: 0.735628  [   32/  130]
train() client id: f_00008-0-1 loss: 0.738980  [   64/  130]
train() client id: f_00008-0-2 loss: 0.724861  [   96/  130]
train() client id: f_00008-0-3 loss: 0.833026  [  128/  130]
train() client id: f_00008-1-0 loss: 0.690902  [   32/  130]
train() client id: f_00008-1-1 loss: 0.799432  [   64/  130]
train() client id: f_00008-1-2 loss: 0.727855  [   96/  130]
train() client id: f_00008-1-3 loss: 0.789484  [  128/  130]
train() client id: f_00008-2-0 loss: 0.765406  [   32/  130]
train() client id: f_00008-2-1 loss: 0.783902  [   64/  130]
train() client id: f_00008-2-2 loss: 0.848898  [   96/  130]
train() client id: f_00008-2-3 loss: 0.654869  [  128/  130]
train() client id: f_00008-3-0 loss: 0.753292  [   32/  130]
train() client id: f_00008-3-1 loss: 0.914734  [   64/  130]
train() client id: f_00008-3-2 loss: 0.735403  [   96/  130]
train() client id: f_00008-3-3 loss: 0.646518  [  128/  130]
train() client id: f_00008-4-0 loss: 0.864953  [   32/  130]
train() client id: f_00008-4-1 loss: 0.761117  [   64/  130]
train() client id: f_00008-4-2 loss: 0.667179  [   96/  130]
train() client id: f_00008-4-3 loss: 0.737892  [  128/  130]
train() client id: f_00008-5-0 loss: 0.711734  [   32/  130]
train() client id: f_00008-5-1 loss: 0.748791  [   64/  130]
train() client id: f_00008-5-2 loss: 0.701747  [   96/  130]
train() client id: f_00008-5-3 loss: 0.879922  [  128/  130]
train() client id: f_00008-6-0 loss: 0.644547  [   32/  130]
train() client id: f_00008-6-1 loss: 0.738217  [   64/  130]
train() client id: f_00008-6-2 loss: 0.853515  [   96/  130]
train() client id: f_00008-6-3 loss: 0.813196  [  128/  130]
train() client id: f_00008-7-0 loss: 0.675239  [   32/  130]
train() client id: f_00008-7-1 loss: 0.807271  [   64/  130]
train() client id: f_00008-7-2 loss: 0.802709  [   96/  130]
train() client id: f_00008-7-3 loss: 0.754478  [  128/  130]
train() client id: f_00008-8-0 loss: 0.723614  [   32/  130]
train() client id: f_00008-8-1 loss: 0.714173  [   64/  130]
train() client id: f_00008-8-2 loss: 0.770479  [   96/  130]
train() client id: f_00008-8-3 loss: 0.760078  [  128/  130]
train() client id: f_00008-9-0 loss: 0.736434  [   32/  130]
train() client id: f_00008-9-1 loss: 0.731556  [   64/  130]
train() client id: f_00008-9-2 loss: 0.802893  [   96/  130]
train() client id: f_00008-9-3 loss: 0.760874  [  128/  130]
train() client id: f_00008-10-0 loss: 0.759772  [   32/  130]
train() client id: f_00008-10-1 loss: 0.747881  [   64/  130]
train() client id: f_00008-10-2 loss: 0.886819  [   96/  130]
train() client id: f_00008-10-3 loss: 0.650530  [  128/  130]
train() client id: f_00008-11-0 loss: 0.654145  [   32/  130]
train() client id: f_00008-11-1 loss: 0.841432  [   64/  130]
train() client id: f_00008-11-2 loss: 0.784536  [   96/  130]
train() client id: f_00008-11-3 loss: 0.746290  [  128/  130]
train() client id: f_00009-0-0 loss: 1.133500  [   32/  118]
train() client id: f_00009-0-1 loss: 1.173870  [   64/  118]
train() client id: f_00009-0-2 loss: 1.193012  [   96/  118]
train() client id: f_00009-1-0 loss: 1.066155  [   32/  118]
train() client id: f_00009-1-1 loss: 1.080465  [   64/  118]
train() client id: f_00009-1-2 loss: 1.130258  [   96/  118]
train() client id: f_00009-2-0 loss: 1.027347  [   32/  118]
train() client id: f_00009-2-1 loss: 1.124594  [   64/  118]
train() client id: f_00009-2-2 loss: 1.005973  [   96/  118]
train() client id: f_00009-3-0 loss: 1.220409  [   32/  118]
train() client id: f_00009-3-1 loss: 1.047475  [   64/  118]
train() client id: f_00009-3-2 loss: 0.813928  [   96/  118]
train() client id: f_00009-4-0 loss: 1.013430  [   32/  118]
train() client id: f_00009-4-1 loss: 0.933434  [   64/  118]
train() client id: f_00009-4-2 loss: 0.997330  [   96/  118]
train() client id: f_00009-5-0 loss: 0.953901  [   32/  118]
train() client id: f_00009-5-1 loss: 1.032593  [   64/  118]
train() client id: f_00009-5-2 loss: 0.968989  [   96/  118]
train() client id: f_00009-6-0 loss: 0.903925  [   32/  118]
train() client id: f_00009-6-1 loss: 0.972264  [   64/  118]
train() client id: f_00009-6-2 loss: 0.922495  [   96/  118]
train() client id: f_00009-7-0 loss: 1.061454  [   32/  118]
train() client id: f_00009-7-1 loss: 0.887677  [   64/  118]
train() client id: f_00009-7-2 loss: 0.783879  [   96/  118]
train() client id: f_00009-8-0 loss: 0.885712  [   32/  118]
train() client id: f_00009-8-1 loss: 1.013712  [   64/  118]
train() client id: f_00009-8-2 loss: 0.915804  [   96/  118]
train() client id: f_00009-9-0 loss: 0.869973  [   32/  118]
train() client id: f_00009-9-1 loss: 0.886778  [   64/  118]
train() client id: f_00009-9-2 loss: 0.933026  [   96/  118]
train() client id: f_00009-10-0 loss: 0.873814  [   32/  118]
train() client id: f_00009-10-1 loss: 1.024713  [   64/  118]
train() client id: f_00009-10-2 loss: 0.734580  [   96/  118]
train() client id: f_00009-11-0 loss: 0.969143  [   32/  118]
train() client id: f_00009-11-1 loss: 0.928061  [   64/  118]
train() client id: f_00009-11-2 loss: 0.909532  [   96/  118]
At round 18 accuracy: 0.6339522546419099
At round 18 training accuracy: 0.5767940979208585
At round 18 training loss: 0.8470437823013786
update_location
xs = [ -3.9056584    4.20031788 110.00902392  18.81129433   0.97929623
   3.95640986 -72.44319194 -51.32485185  94.66397685 -37.06087855]
ys = [102.5879595   85.55583871   1.32061395 -72.45517586  64.35018685
  47.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [143.31623635 131.67172896 148.67323015 124.91443993 118.91974423
 110.91368802 123.510755   112.40514523 138.81620397 106.72169684]
dists_bs = [185.78691379 200.28233051 333.75840492 314.31971282 207.87518816
 219.54871187 205.10615284 213.61974799 312.25498919 219.69909981]
uav_gains = [4.06456051e-11 5.02562998e-11 3.70692179e-11 5.73364491e-11
 6.48404832e-11 7.71845050e-11 5.89801623e-11 7.46493522e-11
 4.40289659e-11 8.49892801e-11]
bs_gains = [4.89837182e-11 3.96912434e-11 9.49913137e-12 1.12370821e-11
 3.57640465e-11 3.06907558e-11 3.71324630e-11 3.31358541e-11
 1.14463706e-11 3.06319686e-11]
Round 19
-------------------------------
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.48419946 17.66297686  8.35544877  2.99255128 20.37392527  9.81527082
  3.718002   11.96472075  8.80869105  7.96655398]
obj_prev = 100.14234023128081
eta_min = 1.126497835179761e-11	eta_max = 0.9220293976525566
af = 21.163983583608065	bf = 1.67717061299475	zeta = 23.280381941968873	eta = 0.9090909090909091
af = 21.163983583608065	bf = 1.67717061299475	zeta = 40.637450630966356	eta = 0.5207999826514902
af = 21.163983583608065	bf = 1.67717061299475	zeta = 32.312864825181585	eta = 0.6549708203871438
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.81773875501804	eta = 0.6867468035811661
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.743104385934746	eta = 0.6884140039315868
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.74290487459993	eta = 0.6884184715119078
eta = 0.6884184715119078
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.03080864 0.06479595 0.03031961 0.01051406 0.07482098 0.0356989
 0.0132037  0.04376783 0.03178669 0.02885253]
ene_total = [2.66619823 5.03309398 2.64268475 1.21857109 5.74248846 3.03975216
 1.40293291 3.50736005 2.92607578 2.56374747]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 0 obj = 5.417871154985847
eta = 0.6884184715119078
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
eta_min = 0.6884184715119183	eta_max = 0.6884184715118975
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 0.03222576338096967	eta = 0.909090909090909
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 18.47959117596394	eta = 0.0015853244938805354
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8690048705094464	eta = 0.015674730970695012
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8275633379854408	eta = 0.016030168650925096
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.827555909002531	eta = 0.016030233813281204
eta = 0.016030233813281204
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.72568617e-04 1.60902329e-03 1.65941177e-04 6.65676762e-06
 2.50260959e-03 2.76163578e-04 1.31539832e-05 4.70643491e-04
 2.24189901e-04 1.45828825e-04]
ene_total = [0.16944101 0.20192804 0.17262124 0.15441975 0.22542177 0.18229224
 0.15369677 0.1569107  0.23132642 0.17949795]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 1 obj = 5.4178711549860274
eta = 0.6884184715119183
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
Done!
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.53003832e-05 1.42660197e-04 1.47127771e-05 5.90206361e-07
 2.21887886e-04 2.44853823e-05 1.16626642e-06 4.17284781e-05
 1.98772608e-05 1.29295635e-05]
ene_total = [0.00753504 0.00770079 0.00768545 0.0070043  0.00795301 0.00802405
 0.00696556 0.00669454 0.01029748 0.00801597]
At round 19 energy consumption: 0.07787619467639152
At round 19 eta: 0.6884184715119183
At round 19 a_n: 21.674231763837348
At round 19 local rounds: 12.225650330368824
At round 19 global rounds: 69.56199190949924
gradient difference: 0.4246699810028076
train() client id: f_00000-0-0 loss: 1.253961  [   32/  126]
train() client id: f_00000-0-1 loss: 1.425509  [   64/  126]
train() client id: f_00000-0-2 loss: 1.244054  [   96/  126]
train() client id: f_00000-1-0 loss: 1.283860  [   32/  126]
train() client id: f_00000-1-1 loss: 1.164078  [   64/  126]
train() client id: f_00000-1-2 loss: 1.210098  [   96/  126]
train() client id: f_00000-2-0 loss: 1.186738  [   32/  126]
train() client id: f_00000-2-1 loss: 1.028721  [   64/  126]
train() client id: f_00000-2-2 loss: 1.205453  [   96/  126]
train() client id: f_00000-3-0 loss: 1.085735  [   32/  126]
train() client id: f_00000-3-1 loss: 1.095411  [   64/  126]
train() client id: f_00000-3-2 loss: 0.994381  [   96/  126]
train() client id: f_00000-4-0 loss: 0.953623  [   32/  126]
train() client id: f_00000-4-1 loss: 1.119347  [   64/  126]
train() client id: f_00000-4-2 loss: 1.027239  [   96/  126]
train() client id: f_00000-5-0 loss: 1.007202  [   32/  126]
train() client id: f_00000-5-1 loss: 0.992098  [   64/  126]
train() client id: f_00000-5-2 loss: 0.943322  [   96/  126]
train() client id: f_00000-6-0 loss: 0.929437  [   32/  126]
train() client id: f_00000-6-1 loss: 0.994810  [   64/  126]
train() client id: f_00000-6-2 loss: 0.932630  [   96/  126]
train() client id: f_00000-7-0 loss: 0.979084  [   32/  126]
train() client id: f_00000-7-1 loss: 0.865222  [   64/  126]
train() client id: f_00000-7-2 loss: 0.947705  [   96/  126]
train() client id: f_00000-8-0 loss: 0.859888  [   32/  126]
train() client id: f_00000-8-1 loss: 0.937665  [   64/  126]
train() client id: f_00000-8-2 loss: 0.955703  [   96/  126]
train() client id: f_00000-9-0 loss: 0.862990  [   32/  126]
train() client id: f_00000-9-1 loss: 0.868269  [   64/  126]
train() client id: f_00000-9-2 loss: 0.883216  [   96/  126]
train() client id: f_00000-10-0 loss: 0.931732  [   32/  126]
train() client id: f_00000-10-1 loss: 0.934983  [   64/  126]
train() client id: f_00000-10-2 loss: 0.892901  [   96/  126]
train() client id: f_00000-11-0 loss: 0.833790  [   32/  126]
train() client id: f_00000-11-1 loss: 0.818397  [   64/  126]
train() client id: f_00000-11-2 loss: 0.893718  [   96/  126]
train() client id: f_00001-0-0 loss: 0.580302  [   32/  265]
train() client id: f_00001-0-1 loss: 0.408164  [   64/  265]
train() client id: f_00001-0-2 loss: 0.529302  [   96/  265]
train() client id: f_00001-0-3 loss: 0.493509  [  128/  265]
train() client id: f_00001-0-4 loss: 0.360085  [  160/  265]
train() client id: f_00001-0-5 loss: 0.384929  [  192/  265]
train() client id: f_00001-0-6 loss: 0.327799  [  224/  265]
train() client id: f_00001-0-7 loss: 0.388813  [  256/  265]
train() client id: f_00001-1-0 loss: 0.527220  [   32/  265]
train() client id: f_00001-1-1 loss: 0.439529  [   64/  265]
train() client id: f_00001-1-2 loss: 0.461909  [   96/  265]
train() client id: f_00001-1-3 loss: 0.384891  [  128/  265]
train() client id: f_00001-1-4 loss: 0.394137  [  160/  265]
train() client id: f_00001-1-5 loss: 0.357550  [  192/  265]
train() client id: f_00001-1-6 loss: 0.372934  [  224/  265]
train() client id: f_00001-1-7 loss: 0.496504  [  256/  265]
train() client id: f_00001-2-0 loss: 0.434190  [   32/  265]
train() client id: f_00001-2-1 loss: 0.473258  [   64/  265]
train() client id: f_00001-2-2 loss: 0.381195  [   96/  265]
train() client id: f_00001-2-3 loss: 0.345172  [  128/  265]
train() client id: f_00001-2-4 loss: 0.419912  [  160/  265]
train() client id: f_00001-2-5 loss: 0.465039  [  192/  265]
train() client id: f_00001-2-6 loss: 0.373851  [  224/  265]
train() client id: f_00001-2-7 loss: 0.418114  [  256/  265]
train() client id: f_00001-3-0 loss: 0.318338  [   32/  265]
train() client id: f_00001-3-1 loss: 0.600372  [   64/  265]
train() client id: f_00001-3-2 loss: 0.390677  [   96/  265]
train() client id: f_00001-3-3 loss: 0.347137  [  128/  265]
train() client id: f_00001-3-4 loss: 0.374085  [  160/  265]
train() client id: f_00001-3-5 loss: 0.568195  [  192/  265]
train() client id: f_00001-3-6 loss: 0.371714  [  224/  265]
train() client id: f_00001-3-7 loss: 0.340983  [  256/  265]
train() client id: f_00001-4-0 loss: 0.368058  [   32/  265]
train() client id: f_00001-4-1 loss: 0.455355  [   64/  265]
train() client id: f_00001-4-2 loss: 0.439999  [   96/  265]
train() client id: f_00001-4-3 loss: 0.495077  [  128/  265]
train() client id: f_00001-4-4 loss: 0.429976  [  160/  265]
train() client id: f_00001-4-5 loss: 0.381482  [  192/  265]
train() client id: f_00001-4-6 loss: 0.345638  [  224/  265]
train() client id: f_00001-4-7 loss: 0.345507  [  256/  265]
train() client id: f_00001-5-0 loss: 0.347963  [   32/  265]
train() client id: f_00001-5-1 loss: 0.310387  [   64/  265]
train() client id: f_00001-5-2 loss: 0.491020  [   96/  265]
train() client id: f_00001-5-3 loss: 0.365469  [  128/  265]
train() client id: f_00001-5-4 loss: 0.542251  [  160/  265]
train() client id: f_00001-5-5 loss: 0.339561  [  192/  265]
train() client id: f_00001-5-6 loss: 0.304574  [  224/  265]
train() client id: f_00001-5-7 loss: 0.487446  [  256/  265]
train() client id: f_00001-6-0 loss: 0.342524  [   32/  265]
train() client id: f_00001-6-1 loss: 0.347456  [   64/  265]
train() client id: f_00001-6-2 loss: 0.635071  [   96/  265]
train() client id: f_00001-6-3 loss: 0.307551  [  128/  265]
train() client id: f_00001-6-4 loss: 0.555119  [  160/  265]
train() client id: f_00001-6-5 loss: 0.361033  [  192/  265]
train() client id: f_00001-6-6 loss: 0.362522  [  224/  265]
train() client id: f_00001-6-7 loss: 0.303595  [  256/  265]
train() client id: f_00001-7-0 loss: 0.305131  [   32/  265]
train() client id: f_00001-7-1 loss: 0.372636  [   64/  265]
train() client id: f_00001-7-2 loss: 0.394065  [   96/  265]
train() client id: f_00001-7-3 loss: 0.511566  [  128/  265]
train() client id: f_00001-7-4 loss: 0.442121  [  160/  265]
train() client id: f_00001-7-5 loss: 0.426622  [  192/  265]
train() client id: f_00001-7-6 loss: 0.377524  [  224/  265]
train() client id: f_00001-7-7 loss: 0.308303  [  256/  265]
train() client id: f_00001-8-0 loss: 0.388225  [   32/  265]
train() client id: f_00001-8-1 loss: 0.426719  [   64/  265]
train() client id: f_00001-8-2 loss: 0.369407  [   96/  265]
train() client id: f_00001-8-3 loss: 0.480342  [  128/  265]
train() client id: f_00001-8-4 loss: 0.349058  [  160/  265]
train() client id: f_00001-8-5 loss: 0.403214  [  192/  265]
train() client id: f_00001-8-6 loss: 0.357425  [  224/  265]
train() client id: f_00001-8-7 loss: 0.406087  [  256/  265]
train() client id: f_00001-9-0 loss: 0.495688  [   32/  265]
train() client id: f_00001-9-1 loss: 0.428338  [   64/  265]
train() client id: f_00001-9-2 loss: 0.495505  [   96/  265]
train() client id: f_00001-9-3 loss: 0.308400  [  128/  265]
train() client id: f_00001-9-4 loss: 0.481571  [  160/  265]
train() client id: f_00001-9-5 loss: 0.340462  [  192/  265]
train() client id: f_00001-9-6 loss: 0.296846  [  224/  265]
train() client id: f_00001-9-7 loss: 0.298612  [  256/  265]
train() client id: f_00001-10-0 loss: 0.355291  [   32/  265]
train() client id: f_00001-10-1 loss: 0.312147  [   64/  265]
train() client id: f_00001-10-2 loss: 0.379270  [   96/  265]
train() client id: f_00001-10-3 loss: 0.379528  [  128/  265]
train() client id: f_00001-10-4 loss: 0.413902  [  160/  265]
train() client id: f_00001-10-5 loss: 0.391379  [  192/  265]
train() client id: f_00001-10-6 loss: 0.511964  [  224/  265]
train() client id: f_00001-10-7 loss: 0.369700  [  256/  265]
train() client id: f_00001-11-0 loss: 0.522612  [   32/  265]
train() client id: f_00001-11-1 loss: 0.396374  [   64/  265]
train() client id: f_00001-11-2 loss: 0.303321  [   96/  265]
train() client id: f_00001-11-3 loss: 0.424511  [  128/  265]
train() client id: f_00001-11-4 loss: 0.440735  [  160/  265]
train() client id: f_00001-11-5 loss: 0.312660  [  192/  265]
train() client id: f_00001-11-6 loss: 0.283350  [  224/  265]
train() client id: f_00001-11-7 loss: 0.397876  [  256/  265]
train() client id: f_00002-0-0 loss: 1.300423  [   32/  124]
train() client id: f_00002-0-1 loss: 1.213076  [   64/  124]
train() client id: f_00002-0-2 loss: 1.094899  [   96/  124]
train() client id: f_00002-1-0 loss: 1.067936  [   32/  124]
train() client id: f_00002-1-1 loss: 1.173536  [   64/  124]
train() client id: f_00002-1-2 loss: 1.138883  [   96/  124]
train() client id: f_00002-2-0 loss: 1.154655  [   32/  124]
train() client id: f_00002-2-1 loss: 1.097608  [   64/  124]
train() client id: f_00002-2-2 loss: 1.257902  [   96/  124]
train() client id: f_00002-3-0 loss: 1.056632  [   32/  124]
train() client id: f_00002-3-1 loss: 1.217820  [   64/  124]
train() client id: f_00002-3-2 loss: 1.094683  [   96/  124]
train() client id: f_00002-4-0 loss: 1.182662  [   32/  124]
train() client id: f_00002-4-1 loss: 1.012076  [   64/  124]
train() client id: f_00002-4-2 loss: 1.031213  [   96/  124]
train() client id: f_00002-5-0 loss: 1.088832  [   32/  124]
train() client id: f_00002-5-1 loss: 1.052109  [   64/  124]
train() client id: f_00002-5-2 loss: 1.085534  [   96/  124]
train() client id: f_00002-6-0 loss: 1.122273  [   32/  124]
train() client id: f_00002-6-1 loss: 0.974346  [   64/  124]
train() client id: f_00002-6-2 loss: 1.005331  [   96/  124]
train() client id: f_00002-7-0 loss: 1.116490  [   32/  124]
train() client id: f_00002-7-1 loss: 1.108149  [   64/  124]
train() client id: f_00002-7-2 loss: 1.012718  [   96/  124]
train() client id: f_00002-8-0 loss: 1.148938  [   32/  124]
train() client id: f_00002-8-1 loss: 0.993320  [   64/  124]
train() client id: f_00002-8-2 loss: 1.029676  [   96/  124]
train() client id: f_00002-9-0 loss: 0.970476  [   32/  124]
train() client id: f_00002-9-1 loss: 1.097453  [   64/  124]
train() client id: f_00002-9-2 loss: 1.088780  [   96/  124]
train() client id: f_00002-10-0 loss: 1.174098  [   32/  124]
train() client id: f_00002-10-1 loss: 1.030503  [   64/  124]
train() client id: f_00002-10-2 loss: 0.921808  [   96/  124]
train() client id: f_00002-11-0 loss: 1.084401  [   32/  124]
train() client id: f_00002-11-1 loss: 1.035434  [   64/  124]
train() client id: f_00002-11-2 loss: 1.115484  [   96/  124]
train() client id: f_00003-0-0 loss: 0.645790  [   32/   43]
train() client id: f_00003-1-0 loss: 0.794024  [   32/   43]
train() client id: f_00003-2-0 loss: 0.758237  [   32/   43]
train() client id: f_00003-3-0 loss: 0.974157  [   32/   43]
train() client id: f_00003-4-0 loss: 0.704229  [   32/   43]
train() client id: f_00003-5-0 loss: 0.751561  [   32/   43]
train() client id: f_00003-6-0 loss: 0.629965  [   32/   43]
train() client id: f_00003-7-0 loss: 0.997963  [   32/   43]
train() client id: f_00003-8-0 loss: 0.882023  [   32/   43]
train() client id: f_00003-9-0 loss: 0.647891  [   32/   43]
train() client id: f_00003-10-0 loss: 0.725049  [   32/   43]
train() client id: f_00003-11-0 loss: 0.725561  [   32/   43]
train() client id: f_00004-0-0 loss: 0.833133  [   32/  306]
train() client id: f_00004-0-1 loss: 1.008234  [   64/  306]
train() client id: f_00004-0-2 loss: 1.018115  [   96/  306]
train() client id: f_00004-0-3 loss: 0.940961  [  128/  306]
train() client id: f_00004-0-4 loss: 1.042124  [  160/  306]
train() client id: f_00004-0-5 loss: 1.103111  [  192/  306]
train() client id: f_00004-0-6 loss: 1.017034  [  224/  306]
train() client id: f_00004-0-7 loss: 0.972961  [  256/  306]
train() client id: f_00004-0-8 loss: 0.884868  [  288/  306]
train() client id: f_00004-1-0 loss: 1.115006  [   32/  306]
train() client id: f_00004-1-1 loss: 1.016798  [   64/  306]
train() client id: f_00004-1-2 loss: 1.005028  [   96/  306]
train() client id: f_00004-1-3 loss: 0.875331  [  128/  306]
train() client id: f_00004-1-4 loss: 0.854987  [  160/  306]
train() client id: f_00004-1-5 loss: 1.013437  [  192/  306]
train() client id: f_00004-1-6 loss: 0.999261  [  224/  306]
train() client id: f_00004-1-7 loss: 0.943245  [  256/  306]
train() client id: f_00004-1-8 loss: 1.047916  [  288/  306]
train() client id: f_00004-2-0 loss: 0.974970  [   32/  306]
train() client id: f_00004-2-1 loss: 1.080359  [   64/  306]
train() client id: f_00004-2-2 loss: 0.887961  [   96/  306]
train() client id: f_00004-2-3 loss: 0.997910  [  128/  306]
train() client id: f_00004-2-4 loss: 1.112962  [  160/  306]
train() client id: f_00004-2-5 loss: 0.912671  [  192/  306]
train() client id: f_00004-2-6 loss: 1.050749  [  224/  306]
train() client id: f_00004-2-7 loss: 0.919943  [  256/  306]
train() client id: f_00004-2-8 loss: 0.921864  [  288/  306]
train() client id: f_00004-3-0 loss: 1.004334  [   32/  306]
train() client id: f_00004-3-1 loss: 0.920907  [   64/  306]
train() client id: f_00004-3-2 loss: 0.973863  [   96/  306]
train() client id: f_00004-3-3 loss: 1.038132  [  128/  306]
train() client id: f_00004-3-4 loss: 0.929698  [  160/  306]
train() client id: f_00004-3-5 loss: 0.989084  [  192/  306]
train() client id: f_00004-3-6 loss: 0.895856  [  224/  306]
train() client id: f_00004-3-7 loss: 1.068221  [  256/  306]
train() client id: f_00004-3-8 loss: 0.983952  [  288/  306]
train() client id: f_00004-4-0 loss: 1.060616  [   32/  306]
train() client id: f_00004-4-1 loss: 0.963554  [   64/  306]
train() client id: f_00004-4-2 loss: 0.948225  [   96/  306]
train() client id: f_00004-4-3 loss: 0.913127  [  128/  306]
train() client id: f_00004-4-4 loss: 0.966479  [  160/  306]
train() client id: f_00004-4-5 loss: 0.969980  [  192/  306]
train() client id: f_00004-4-6 loss: 0.985642  [  224/  306]
train() client id: f_00004-4-7 loss: 0.925061  [  256/  306]
train() client id: f_00004-4-8 loss: 1.062997  [  288/  306]
train() client id: f_00004-5-0 loss: 0.930656  [   32/  306]
train() client id: f_00004-5-1 loss: 0.944983  [   64/  306]
train() client id: f_00004-5-2 loss: 0.944588  [   96/  306]
train() client id: f_00004-5-3 loss: 0.949878  [  128/  306]
train() client id: f_00004-5-4 loss: 0.980840  [  160/  306]
train() client id: f_00004-5-5 loss: 1.122244  [  192/  306]
train() client id: f_00004-5-6 loss: 1.013754  [  224/  306]
train() client id: f_00004-5-7 loss: 0.961307  [  256/  306]
train() client id: f_00004-5-8 loss: 0.901025  [  288/  306]
train() client id: f_00004-6-0 loss: 0.913717  [   32/  306]
train() client id: f_00004-6-1 loss: 1.098686  [   64/  306]
train() client id: f_00004-6-2 loss: 0.944795  [   96/  306]
train() client id: f_00004-6-3 loss: 1.074174  [  128/  306]
train() client id: f_00004-6-4 loss: 0.896246  [  160/  306]
train() client id: f_00004-6-5 loss: 0.994210  [  192/  306]
train() client id: f_00004-6-6 loss: 0.964415  [  224/  306]
train() client id: f_00004-6-7 loss: 0.925835  [  256/  306]
train() client id: f_00004-6-8 loss: 0.928486  [  288/  306]
train() client id: f_00004-7-0 loss: 0.876731  [   32/  306]
train() client id: f_00004-7-1 loss: 0.897068  [   64/  306]
train() client id: f_00004-7-2 loss: 0.984494  [   96/  306]
train() client id: f_00004-7-3 loss: 0.900624  [  128/  306]
train() client id: f_00004-7-4 loss: 1.026253  [  160/  306]
train() client id: f_00004-7-5 loss: 0.910734  [  192/  306]
train() client id: f_00004-7-6 loss: 1.055831  [  224/  306]
train() client id: f_00004-7-7 loss: 1.007494  [  256/  306]
train() client id: f_00004-7-8 loss: 0.879848  [  288/  306]
train() client id: f_00004-8-0 loss: 0.952217  [   32/  306]
train() client id: f_00004-8-1 loss: 1.057655  [   64/  306]
train() client id: f_00004-8-2 loss: 0.951706  [   96/  306]
train() client id: f_00004-8-3 loss: 0.991768  [  128/  306]
train() client id: f_00004-8-4 loss: 1.050101  [  160/  306]
train() client id: f_00004-8-5 loss: 0.837209  [  192/  306]
train() client id: f_00004-8-6 loss: 0.978948  [  224/  306]
train() client id: f_00004-8-7 loss: 0.905398  [  256/  306]
train() client id: f_00004-8-8 loss: 0.977348  [  288/  306]
train() client id: f_00004-9-0 loss: 0.944463  [   32/  306]
train() client id: f_00004-9-1 loss: 0.850021  [   64/  306]
train() client id: f_00004-9-2 loss: 1.026304  [   96/  306]
train() client id: f_00004-9-3 loss: 1.032724  [  128/  306]
train() client id: f_00004-9-4 loss: 0.911917  [  160/  306]
train() client id: f_00004-9-5 loss: 0.853755  [  192/  306]
train() client id: f_00004-9-6 loss: 1.066687  [  224/  306]
train() client id: f_00004-9-7 loss: 0.993912  [  256/  306]
train() client id: f_00004-9-8 loss: 0.953201  [  288/  306]
train() client id: f_00004-10-0 loss: 0.960856  [   32/  306]
train() client id: f_00004-10-1 loss: 1.021938  [   64/  306]
train() client id: f_00004-10-2 loss: 0.989768  [   96/  306]
train() client id: f_00004-10-3 loss: 0.839958  [  128/  306]
train() client id: f_00004-10-4 loss: 1.022248  [  160/  306]
train() client id: f_00004-10-5 loss: 0.963271  [  192/  306]
train() client id: f_00004-10-6 loss: 0.870179  [  224/  306]
train() client id: f_00004-10-7 loss: 0.975734  [  256/  306]
train() client id: f_00004-10-8 loss: 0.962799  [  288/  306]
train() client id: f_00004-11-0 loss: 0.986620  [   32/  306]
train() client id: f_00004-11-1 loss: 1.009108  [   64/  306]
train() client id: f_00004-11-2 loss: 0.965621  [   96/  306]
train() client id: f_00004-11-3 loss: 0.970914  [  128/  306]
train() client id: f_00004-11-4 loss: 0.991269  [  160/  306]
train() client id: f_00004-11-5 loss: 0.922267  [  192/  306]
train() client id: f_00004-11-6 loss: 0.901961  [  224/  306]
train() client id: f_00004-11-7 loss: 0.997140  [  256/  306]
train() client id: f_00004-11-8 loss: 0.896547  [  288/  306]
train() client id: f_00005-0-0 loss: 0.800693  [   32/  146]
train() client id: f_00005-0-1 loss: 0.878209  [   64/  146]
train() client id: f_00005-0-2 loss: 0.706300  [   96/  146]
train() client id: f_00005-0-3 loss: 0.965943  [  128/  146]
train() client id: f_00005-1-0 loss: 0.769468  [   32/  146]
train() client id: f_00005-1-1 loss: 0.708792  [   64/  146]
train() client id: f_00005-1-2 loss: 1.097164  [   96/  146]
train() client id: f_00005-1-3 loss: 0.931397  [  128/  146]
train() client id: f_00005-2-0 loss: 0.733899  [   32/  146]
train() client id: f_00005-2-1 loss: 0.783253  [   64/  146]
train() client id: f_00005-2-2 loss: 1.057168  [   96/  146]
train() client id: f_00005-2-3 loss: 0.907101  [  128/  146]
train() client id: f_00005-3-0 loss: 0.794385  [   32/  146]
train() client id: f_00005-3-1 loss: 0.892684  [   64/  146]
train() client id: f_00005-3-2 loss: 0.914541  [   96/  146]
train() client id: f_00005-3-3 loss: 0.951379  [  128/  146]
train() client id: f_00005-4-0 loss: 0.837549  [   32/  146]
train() client id: f_00005-4-1 loss: 0.753678  [   64/  146]
train() client id: f_00005-4-2 loss: 0.858676  [   96/  146]
train() client id: f_00005-4-3 loss: 0.883532  [  128/  146]
train() client id: f_00005-5-0 loss: 0.892886  [   32/  146]
train() client id: f_00005-5-1 loss: 0.879990  [   64/  146]
train() client id: f_00005-5-2 loss: 0.937306  [   96/  146]
train() client id: f_00005-5-3 loss: 0.692782  [  128/  146]
train() client id: f_00005-6-0 loss: 0.887158  [   32/  146]
train() client id: f_00005-6-1 loss: 1.090175  [   64/  146]
train() client id: f_00005-6-2 loss: 0.651983  [   96/  146]
train() client id: f_00005-6-3 loss: 0.786647  [  128/  146]
train() client id: f_00005-7-0 loss: 0.958619  [   32/  146]
train() client id: f_00005-7-1 loss: 0.648205  [   64/  146]
train() client id: f_00005-7-2 loss: 0.910384  [   96/  146]
train() client id: f_00005-7-3 loss: 0.898709  [  128/  146]
train() client id: f_00005-8-0 loss: 1.076808  [   32/  146]
train() client id: f_00005-8-1 loss: 0.721276  [   64/  146]
train() client id: f_00005-8-2 loss: 0.715758  [   96/  146]
train() client id: f_00005-8-3 loss: 0.978983  [  128/  146]
train() client id: f_00005-9-0 loss: 0.832551  [   32/  146]
train() client id: f_00005-9-1 loss: 0.883478  [   64/  146]
train() client id: f_00005-9-2 loss: 0.780811  [   96/  146]
train() client id: f_00005-9-3 loss: 0.975259  [  128/  146]
train() client id: f_00005-10-0 loss: 0.905711  [   32/  146]
train() client id: f_00005-10-1 loss: 0.633075  [   64/  146]
train() client id: f_00005-10-2 loss: 1.078793  [   96/  146]
train() client id: f_00005-10-3 loss: 0.906626  [  128/  146]
train() client id: f_00005-11-0 loss: 1.016064  [   32/  146]
train() client id: f_00005-11-1 loss: 0.770511  [   64/  146]
train() client id: f_00005-11-2 loss: 0.661235  [   96/  146]
train() client id: f_00005-11-3 loss: 1.002959  [  128/  146]
train() client id: f_00006-0-0 loss: 0.667111  [   32/   54]
train() client id: f_00006-1-0 loss: 0.602743  [   32/   54]
train() client id: f_00006-2-0 loss: 0.587180  [   32/   54]
train() client id: f_00006-3-0 loss: 0.641054  [   32/   54]
train() client id: f_00006-4-0 loss: 0.665313  [   32/   54]
train() client id: f_00006-5-0 loss: 0.613552  [   32/   54]
train() client id: f_00006-6-0 loss: 0.611901  [   32/   54]
train() client id: f_00006-7-0 loss: 0.650098  [   32/   54]
train() client id: f_00006-8-0 loss: 0.570951  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586627  [   32/   54]
train() client id: f_00006-10-0 loss: 0.556599  [   32/   54]
train() client id: f_00006-11-0 loss: 0.561513  [   32/   54]
train() client id: f_00007-0-0 loss: 0.543398  [   32/  179]
train() client id: f_00007-0-1 loss: 0.633826  [   64/  179]
train() client id: f_00007-0-2 loss: 0.596670  [   96/  179]
train() client id: f_00007-0-3 loss: 0.573881  [  128/  179]
train() client id: f_00007-0-4 loss: 0.667280  [  160/  179]
train() client id: f_00007-1-0 loss: 0.773612  [   32/  179]
train() client id: f_00007-1-1 loss: 0.549459  [   64/  179]
train() client id: f_00007-1-2 loss: 0.539672  [   96/  179]
train() client id: f_00007-1-3 loss: 0.535682  [  128/  179]
train() client id: f_00007-1-4 loss: 0.500312  [  160/  179]
train() client id: f_00007-2-0 loss: 0.585665  [   32/  179]
train() client id: f_00007-2-1 loss: 0.499887  [   64/  179]
train() client id: f_00007-2-2 loss: 0.542021  [   96/  179]
train() client id: f_00007-2-3 loss: 0.596831  [  128/  179]
train() client id: f_00007-2-4 loss: 0.578410  [  160/  179]
train() client id: f_00007-3-0 loss: 0.531180  [   32/  179]
train() client id: f_00007-3-1 loss: 0.588183  [   64/  179]
train() client id: f_00007-3-2 loss: 0.524133  [   96/  179]
train() client id: f_00007-3-3 loss: 0.552931  [  128/  179]
train() client id: f_00007-3-4 loss: 0.617369  [  160/  179]
train() client id: f_00007-4-0 loss: 0.489078  [   32/  179]
train() client id: f_00007-4-1 loss: 0.434951  [   64/  179]
train() client id: f_00007-4-2 loss: 0.571472  [   96/  179]
train() client id: f_00007-4-3 loss: 0.572538  [  128/  179]
train() client id: f_00007-4-4 loss: 0.507441  [  160/  179]
train() client id: f_00007-5-0 loss: 0.420206  [   32/  179]
train() client id: f_00007-5-1 loss: 0.497771  [   64/  179]
train() client id: f_00007-5-2 loss: 0.487032  [   96/  179]
train() client id: f_00007-5-3 loss: 0.853525  [  128/  179]
train() client id: f_00007-5-4 loss: 0.492959  [  160/  179]
train() client id: f_00007-6-0 loss: 0.549780  [   32/  179]
train() client id: f_00007-6-1 loss: 0.624154  [   64/  179]
train() client id: f_00007-6-2 loss: 0.499523  [   96/  179]
train() client id: f_00007-6-3 loss: 0.473989  [  128/  179]
train() client id: f_00007-6-4 loss: 0.489813  [  160/  179]
train() client id: f_00007-7-0 loss: 0.402591  [   32/  179]
train() client id: f_00007-7-1 loss: 0.571055  [   64/  179]
train() client id: f_00007-7-2 loss: 0.622734  [   96/  179]
train() client id: f_00007-7-3 loss: 0.651679  [  128/  179]
train() client id: f_00007-7-4 loss: 0.378144  [  160/  179]
train() client id: f_00007-8-0 loss: 0.405049  [   32/  179]
train() client id: f_00007-8-1 loss: 0.703985  [   64/  179]
train() client id: f_00007-8-2 loss: 0.494618  [   96/  179]
train() client id: f_00007-8-3 loss: 0.559603  [  128/  179]
train() client id: f_00007-8-4 loss: 0.476339  [  160/  179]
train() client id: f_00007-9-0 loss: 0.475713  [   32/  179]
train() client id: f_00007-9-1 loss: 0.684456  [   64/  179]
train() client id: f_00007-9-2 loss: 0.641911  [   96/  179]
train() client id: f_00007-9-3 loss: 0.391802  [  128/  179]
train() client id: f_00007-9-4 loss: 0.482190  [  160/  179]
train() client id: f_00007-10-0 loss: 0.481801  [   32/  179]
train() client id: f_00007-10-1 loss: 0.549837  [   64/  179]
train() client id: f_00007-10-2 loss: 0.599444  [   96/  179]
train() client id: f_00007-10-3 loss: 0.452178  [  128/  179]
train() client id: f_00007-10-4 loss: 0.400839  [  160/  179]
train() client id: f_00007-11-0 loss: 0.589693  [   32/  179]
train() client id: f_00007-11-1 loss: 0.667868  [   64/  179]
train() client id: f_00007-11-2 loss: 0.376888  [   96/  179]
train() client id: f_00007-11-3 loss: 0.565069  [  128/  179]
train() client id: f_00007-11-4 loss: 0.378832  [  160/  179]
train() client id: f_00008-0-0 loss: 0.799485  [   32/  130]
train() client id: f_00008-0-1 loss: 0.680453  [   64/  130]
train() client id: f_00008-0-2 loss: 0.828583  [   96/  130]
train() client id: f_00008-0-3 loss: 0.807280  [  128/  130]
train() client id: f_00008-1-0 loss: 0.844256  [   32/  130]
train() client id: f_00008-1-1 loss: 0.703680  [   64/  130]
train() client id: f_00008-1-2 loss: 0.690486  [   96/  130]
train() client id: f_00008-1-3 loss: 0.876409  [  128/  130]
train() client id: f_00008-2-0 loss: 0.728470  [   32/  130]
train() client id: f_00008-2-1 loss: 0.765225  [   64/  130]
train() client id: f_00008-2-2 loss: 0.785969  [   96/  130]
train() client id: f_00008-2-3 loss: 0.819894  [  128/  130]
train() client id: f_00008-3-0 loss: 0.839725  [   32/  130]
train() client id: f_00008-3-1 loss: 0.835976  [   64/  130]
train() client id: f_00008-3-2 loss: 0.661902  [   96/  130]
train() client id: f_00008-3-3 loss: 0.768796  [  128/  130]
train() client id: f_00008-4-0 loss: 0.832884  [   32/  130]
train() client id: f_00008-4-1 loss: 0.820139  [   64/  130]
train() client id: f_00008-4-2 loss: 0.681130  [   96/  130]
train() client id: f_00008-4-3 loss: 0.779854  [  128/  130]
train() client id: f_00008-5-0 loss: 0.765230  [   32/  130]
train() client id: f_00008-5-1 loss: 0.710318  [   64/  130]
train() client id: f_00008-5-2 loss: 0.791809  [   96/  130]
train() client id: f_00008-5-3 loss: 0.823669  [  128/  130]
train() client id: f_00008-6-0 loss: 0.744660  [   32/  130]
train() client id: f_00008-6-1 loss: 0.731976  [   64/  130]
train() client id: f_00008-6-2 loss: 0.869835  [   96/  130]
train() client id: f_00008-6-3 loss: 0.766139  [  128/  130]
train() client id: f_00008-7-0 loss: 0.859805  [   32/  130]
train() client id: f_00008-7-1 loss: 0.797719  [   64/  130]
train() client id: f_00008-7-2 loss: 0.696352  [   96/  130]
train() client id: f_00008-7-3 loss: 0.755952  [  128/  130]
train() client id: f_00008-8-0 loss: 0.684566  [   32/  130]
train() client id: f_00008-8-1 loss: 0.819699  [   64/  130]
train() client id: f_00008-8-2 loss: 0.787630  [   96/  130]
train() client id: f_00008-8-3 loss: 0.782802  [  128/  130]
train() client id: f_00008-9-0 loss: 0.940487  [   32/  130]
train() client id: f_00008-9-1 loss: 0.731022  [   64/  130]
train() client id: f_00008-9-2 loss: 0.701215  [   96/  130]
train() client id: f_00008-9-3 loss: 0.729980  [  128/  130]
train() client id: f_00008-10-0 loss: 0.849745  [   32/  130]
train() client id: f_00008-10-1 loss: 0.837006  [   64/  130]
train() client id: f_00008-10-2 loss: 0.682967  [   96/  130]
train() client id: f_00008-10-3 loss: 0.732369  [  128/  130]
train() client id: f_00008-11-0 loss: 0.867389  [   32/  130]
train() client id: f_00008-11-1 loss: 0.816653  [   64/  130]
train() client id: f_00008-11-2 loss: 0.622807  [   96/  130]
train() client id: f_00008-11-3 loss: 0.811871  [  128/  130]
train() client id: f_00009-0-0 loss: 1.093770  [   32/  118]
train() client id: f_00009-0-1 loss: 1.139618  [   64/  118]
train() client id: f_00009-0-2 loss: 1.030833  [   96/  118]
train() client id: f_00009-1-0 loss: 1.162179  [   32/  118]
train() client id: f_00009-1-1 loss: 1.001622  [   64/  118]
train() client id: f_00009-1-2 loss: 1.029643  [   96/  118]
train() client id: f_00009-2-0 loss: 1.146681  [   32/  118]
train() client id: f_00009-2-1 loss: 0.954061  [   64/  118]
train() client id: f_00009-2-2 loss: 1.044521  [   96/  118]
train() client id: f_00009-3-0 loss: 0.854664  [   32/  118]
train() client id: f_00009-3-1 loss: 1.064920  [   64/  118]
train() client id: f_00009-3-2 loss: 0.882696  [   96/  118]
train() client id: f_00009-4-0 loss: 0.933502  [   32/  118]
train() client id: f_00009-4-1 loss: 0.957296  [   64/  118]
train() client id: f_00009-4-2 loss: 0.970483  [   96/  118]
train() client id: f_00009-5-0 loss: 0.815220  [   32/  118]
train() client id: f_00009-5-1 loss: 0.982581  [   64/  118]
train() client id: f_00009-5-2 loss: 0.922160  [   96/  118]
train() client id: f_00009-6-0 loss: 0.888223  [   32/  118]
train() client id: f_00009-6-1 loss: 0.878054  [   64/  118]
train() client id: f_00009-6-2 loss: 0.783880  [   96/  118]
train() client id: f_00009-7-0 loss: 0.910154  [   32/  118]
train() client id: f_00009-7-1 loss: 0.987458  [   64/  118]
train() client id: f_00009-7-2 loss: 0.873027  [   96/  118]
train() client id: f_00009-8-0 loss: 0.926329  [   32/  118]
train() client id: f_00009-8-1 loss: 0.935230  [   64/  118]
train() client id: f_00009-8-2 loss: 0.803031  [   96/  118]
train() client id: f_00009-9-0 loss: 0.794121  [   32/  118]
train() client id: f_00009-9-1 loss: 0.878265  [   64/  118]
train() client id: f_00009-9-2 loss: 1.008584  [   96/  118]
train() client id: f_00009-10-0 loss: 0.706019  [   32/  118]
train() client id: f_00009-10-1 loss: 0.860617  [   64/  118]
train() client id: f_00009-10-2 loss: 0.990199  [   96/  118]
train() client id: f_00009-11-0 loss: 0.892113  [   32/  118]
train() client id: f_00009-11-1 loss: 0.949598  [   64/  118]
train() client id: f_00009-11-2 loss: 0.782715  [   96/  118]
At round 19 accuracy: 0.6339522546419099
At round 19 training accuracy: 0.5747820254862508
At round 19 training loss: 0.8421867244178793
update_location
xs = [ -3.9056584    4.20031788 115.00902392  18.81129433   0.97929623
   3.95640986 -77.44319194 -56.32485185  99.66397685 -42.06087855]
ys = [107.5879595   90.55583871   1.32061395 -77.45517586  69.35018685
  52.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [146.93680001 134.9740812  152.41003774 127.87950994 121.69801739
 113.15912556 126.50825475 114.77441    142.27290063 108.55933567]
dists_bs = [183.89577736 198.09989981 338.03810893 318.27100658 205.25738895
 216.69051292 202.65232753 210.76443071 316.58303499 216.59479044]
uav_gains = [3.81792511e-11 4.72350303e-11 3.48260844e-11 5.40686414e-11
 6.12019359e-11 7.34119429e-11 5.55466623e-11 7.08558845e-11
 4.13970915e-11 8.14379617e-11]
bs_gains = [5.04072687e-11 4.09277782e-11 9.16621896e-12 1.08508127e-11
 3.70559068e-11 3.18377503e-11 3.84051633e-11 3.44081712e-11
 1.10135848e-11 3.18771632e-11]
Round 20
-------------------------------
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.3523103  17.38275937  8.22567535  2.94704545 20.05064182  9.65872482
  3.66105067 11.77706151  8.67193539  7.83910309]
obj_prev = 98.5663077651687
eta_min = 7.590997972408306e-12	eta_max = 0.9222942349256907
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 22.912452031604698	eta = 0.9090909090909091
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 40.06758095330687	eta = 0.519859231611392
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 31.832010204673995	eta = 0.654357098812908
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.352361561329435	eta = 0.6862563825495306
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27833469058294	eta = 0.6879341965062457
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27813585075753	eta = 0.6879387142452571
eta = 0.6879387142452571
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.03086617 0.06491693 0.03037623 0.01053369 0.07496069 0.03576556
 0.01322836 0.04384955 0.03184604 0.02890641]
ene_total = [2.63093078 4.95082043 2.60801881 1.20448716 5.64850744 2.98715498
 1.38606222 3.45675966 2.88726353 2.51813083]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 0 obj = 5.343958006285102
eta = 0.6879387142452571
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
eta_min = 0.6879387142452722	eta_max = 0.6879387142452453
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 0.030579617084646203	eta = 0.9090909090909091
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 18.258827212431594	eta = 0.001522532174257359
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8403927678865823	eta = 0.015105282079029619
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8010043726548246	eta = 0.015435638201229906
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.800997812209442	eta = 0.015435694428206238
eta = 0.015435694428206238
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.69209568e-04 1.56398576e-03 1.62723179e-04 6.52331649e-06
 2.43061804e-03 2.68033519e-04 1.28914250e-05 4.60934337e-04
 2.19373145e-04 1.41488442e-04]
ene_total = [0.16879936 0.19656835 0.17201187 0.15368379 0.21887026 0.17769628
 0.15298971 0.15556983 0.22990166 0.17490671]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 1 obj = 5.343958006285359
eta = 0.6879387142452722
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
Done!
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.48666236e-05 1.37410596e-04 1.42967344e-05 5.73133613e-07
 2.13552247e-04 2.35492206e-05 1.13263077e-06 4.04973394e-05
 1.92739572e-05 1.24310667e-05]
ene_total = [0.00763659 0.00764604 0.00779078 0.00708733 0.00788489 0.00795709
 0.00704948 0.00675989 0.01041102 0.00794376]
At round 20 energy consumption: 0.07816688572627806
At round 20 eta: 0.6879387142452722
At round 20 a_n: 21.33168591686803
At round 20 local rounds: 12.248478257840025
At round 20 global rounds: 68.35736084749131
gradient difference: 0.3847554922103882
train() client id: f_00000-0-0 loss: 1.399715  [   32/  126]
train() client id: f_00000-0-1 loss: 1.144173  [   64/  126]
train() client id: f_00000-0-2 loss: 1.420774  [   96/  126]
train() client id: f_00000-1-0 loss: 1.204373  [   32/  126]
train() client id: f_00000-1-1 loss: 1.231046  [   64/  126]
train() client id: f_00000-1-2 loss: 1.198945  [   96/  126]
train() client id: f_00000-2-0 loss: 1.151223  [   32/  126]
train() client id: f_00000-2-1 loss: 1.114842  [   64/  126]
train() client id: f_00000-2-2 loss: 1.051013  [   96/  126]
train() client id: f_00000-3-0 loss: 1.115333  [   32/  126]
train() client id: f_00000-3-1 loss: 0.976699  [   64/  126]
train() client id: f_00000-3-2 loss: 1.029453  [   96/  126]
train() client id: f_00000-4-0 loss: 0.985315  [   32/  126]
train() client id: f_00000-4-1 loss: 0.923559  [   64/  126]
train() client id: f_00000-4-2 loss: 0.989578  [   96/  126]
train() client id: f_00000-5-0 loss: 0.954607  [   32/  126]
train() client id: f_00000-5-1 loss: 0.922842  [   64/  126]
train() client id: f_00000-5-2 loss: 0.948623  [   96/  126]
train() client id: f_00000-6-0 loss: 1.016317  [   32/  126]
train() client id: f_00000-6-1 loss: 0.879495  [   64/  126]
train() client id: f_00000-6-2 loss: 0.887851  [   96/  126]
train() client id: f_00000-7-0 loss: 0.821964  [   32/  126]
train() client id: f_00000-7-1 loss: 0.919530  [   64/  126]
train() client id: f_00000-7-2 loss: 0.958077  [   96/  126]
train() client id: f_00000-8-0 loss: 0.886979  [   32/  126]
train() client id: f_00000-8-1 loss: 0.904384  [   64/  126]
train() client id: f_00000-8-2 loss: 0.796852  [   96/  126]
train() client id: f_00000-9-0 loss: 0.802597  [   32/  126]
train() client id: f_00000-9-1 loss: 0.889578  [   64/  126]
train() client id: f_00000-9-2 loss: 0.889191  [   96/  126]
train() client id: f_00000-10-0 loss: 0.808709  [   32/  126]
train() client id: f_00000-10-1 loss: 0.810588  [   64/  126]
train() client id: f_00000-10-2 loss: 0.903639  [   96/  126]
train() client id: f_00000-11-0 loss: 0.799281  [   32/  126]
train() client id: f_00000-11-1 loss: 0.896972  [   64/  126]
train() client id: f_00000-11-2 loss: 0.875158  [   96/  126]
train() client id: f_00001-0-0 loss: 0.368042  [   32/  265]
train() client id: f_00001-0-1 loss: 0.387430  [   64/  265]
train() client id: f_00001-0-2 loss: 0.434878  [   96/  265]
train() client id: f_00001-0-3 loss: 0.435474  [  128/  265]
train() client id: f_00001-0-4 loss: 0.580312  [  160/  265]
train() client id: f_00001-0-5 loss: 0.467333  [  192/  265]
train() client id: f_00001-0-6 loss: 0.525824  [  224/  265]
train() client id: f_00001-0-7 loss: 0.464222  [  256/  265]
train() client id: f_00001-1-0 loss: 0.446506  [   32/  265]
train() client id: f_00001-1-1 loss: 0.583394  [   64/  265]
train() client id: f_00001-1-2 loss: 0.491595  [   96/  265]
train() client id: f_00001-1-3 loss: 0.407709  [  128/  265]
train() client id: f_00001-1-4 loss: 0.440825  [  160/  265]
train() client id: f_00001-1-5 loss: 0.480731  [  192/  265]
train() client id: f_00001-1-6 loss: 0.368414  [  224/  265]
train() client id: f_00001-1-7 loss: 0.360404  [  256/  265]
train() client id: f_00001-2-0 loss: 0.429016  [   32/  265]
train() client id: f_00001-2-1 loss: 0.392118  [   64/  265]
train() client id: f_00001-2-2 loss: 0.539420  [   96/  265]
train() client id: f_00001-2-3 loss: 0.580939  [  128/  265]
train() client id: f_00001-2-4 loss: 0.412546  [  160/  265]
train() client id: f_00001-2-5 loss: 0.400819  [  192/  265]
train() client id: f_00001-2-6 loss: 0.344882  [  224/  265]
train() client id: f_00001-2-7 loss: 0.429792  [  256/  265]
train() client id: f_00001-3-0 loss: 0.362492  [   32/  265]
train() client id: f_00001-3-1 loss: 0.564268  [   64/  265]
train() client id: f_00001-3-2 loss: 0.397215  [   96/  265]
train() client id: f_00001-3-3 loss: 0.350298  [  128/  265]
train() client id: f_00001-3-4 loss: 0.472946  [  160/  265]
train() client id: f_00001-3-5 loss: 0.428480  [  192/  265]
train() client id: f_00001-3-6 loss: 0.427003  [  224/  265]
train() client id: f_00001-3-7 loss: 0.486864  [  256/  265]
train() client id: f_00001-4-0 loss: 0.405634  [   32/  265]
train() client id: f_00001-4-1 loss: 0.353957  [   64/  265]
train() client id: f_00001-4-2 loss: 0.422071  [   96/  265]
train() client id: f_00001-4-3 loss: 0.490674  [  128/  265]
train() client id: f_00001-4-4 loss: 0.337036  [  160/  265]
train() client id: f_00001-4-5 loss: 0.397394  [  192/  265]
train() client id: f_00001-4-6 loss: 0.608130  [  224/  265]
train() client id: f_00001-4-7 loss: 0.431785  [  256/  265]
train() client id: f_00001-5-0 loss: 0.437090  [   32/  265]
train() client id: f_00001-5-1 loss: 0.442211  [   64/  265]
train() client id: f_00001-5-2 loss: 0.448255  [   96/  265]
train() client id: f_00001-5-3 loss: 0.448636  [  128/  265]
train() client id: f_00001-5-4 loss: 0.348452  [  160/  265]
train() client id: f_00001-5-5 loss: 0.433200  [  192/  265]
train() client id: f_00001-5-6 loss: 0.379565  [  224/  265]
train() client id: f_00001-5-7 loss: 0.503080  [  256/  265]
train() client id: f_00001-6-0 loss: 0.428845  [   32/  265]
train() client id: f_00001-6-1 loss: 0.403034  [   64/  265]
train() client id: f_00001-6-2 loss: 0.383911  [   96/  265]
train() client id: f_00001-6-3 loss: 0.494205  [  128/  265]
train() client id: f_00001-6-4 loss: 0.425268  [  160/  265]
train() client id: f_00001-6-5 loss: 0.409637  [  192/  265]
train() client id: f_00001-6-6 loss: 0.387051  [  224/  265]
train() client id: f_00001-6-7 loss: 0.484254  [  256/  265]
train() client id: f_00001-7-0 loss: 0.599279  [   32/  265]
train() client id: f_00001-7-1 loss: 0.343958  [   64/  265]
train() client id: f_00001-7-2 loss: 0.415729  [   96/  265]
train() client id: f_00001-7-3 loss: 0.383171  [  128/  265]
train() client id: f_00001-7-4 loss: 0.330546  [  160/  265]
train() client id: f_00001-7-5 loss: 0.473502  [  192/  265]
train() client id: f_00001-7-6 loss: 0.404927  [  224/  265]
train() client id: f_00001-7-7 loss: 0.391461  [  256/  265]
train() client id: f_00001-8-0 loss: 0.383942  [   32/  265]
train() client id: f_00001-8-1 loss: 0.456735  [   64/  265]
train() client id: f_00001-8-2 loss: 0.397368  [   96/  265]
train() client id: f_00001-8-3 loss: 0.380837  [  128/  265]
train() client id: f_00001-8-4 loss: 0.476338  [  160/  265]
train() client id: f_00001-8-5 loss: 0.516308  [  192/  265]
train() client id: f_00001-8-6 loss: 0.334105  [  224/  265]
train() client id: f_00001-8-7 loss: 0.350872  [  256/  265]
train() client id: f_00001-9-0 loss: 0.376619  [   32/  265]
train() client id: f_00001-9-1 loss: 0.404132  [   64/  265]
train() client id: f_00001-9-2 loss: 0.443347  [   96/  265]
train() client id: f_00001-9-3 loss: 0.408604  [  128/  265]
train() client id: f_00001-9-4 loss: 0.408064  [  160/  265]
train() client id: f_00001-9-5 loss: 0.382380  [  192/  265]
train() client id: f_00001-9-6 loss: 0.544909  [  224/  265]
train() client id: f_00001-9-7 loss: 0.386554  [  256/  265]
train() client id: f_00001-10-0 loss: 0.517359  [   32/  265]
train() client id: f_00001-10-1 loss: 0.326967  [   64/  265]
train() client id: f_00001-10-2 loss: 0.496873  [   96/  265]
train() client id: f_00001-10-3 loss: 0.399833  [  128/  265]
train() client id: f_00001-10-4 loss: 0.484051  [  160/  265]
train() client id: f_00001-10-5 loss: 0.327148  [  192/  265]
train() client id: f_00001-10-6 loss: 0.432680  [  224/  265]
train() client id: f_00001-10-7 loss: 0.427816  [  256/  265]
train() client id: f_00001-11-0 loss: 0.558623  [   32/  265]
train() client id: f_00001-11-1 loss: 0.403169  [   64/  265]
train() client id: f_00001-11-2 loss: 0.556966  [   96/  265]
train() client id: f_00001-11-3 loss: 0.327322  [  128/  265]
train() client id: f_00001-11-4 loss: 0.486754  [  160/  265]
train() client id: f_00001-11-5 loss: 0.379098  [  192/  265]
train() client id: f_00001-11-6 loss: 0.369812  [  224/  265]
train() client id: f_00001-11-7 loss: 0.331230  [  256/  265]
train() client id: f_00002-0-0 loss: 1.181790  [   32/  124]
train() client id: f_00002-0-1 loss: 1.066248  [   64/  124]
train() client id: f_00002-0-2 loss: 1.025699  [   96/  124]
train() client id: f_00002-1-0 loss: 1.050587  [   32/  124]
train() client id: f_00002-1-1 loss: 1.173208  [   64/  124]
train() client id: f_00002-1-2 loss: 0.981096  [   96/  124]
train() client id: f_00002-2-0 loss: 1.086205  [   32/  124]
train() client id: f_00002-2-1 loss: 1.048888  [   64/  124]
train() client id: f_00002-2-2 loss: 0.972375  [   96/  124]
train() client id: f_00002-3-0 loss: 0.962056  [   32/  124]
train() client id: f_00002-3-1 loss: 0.941175  [   64/  124]
train() client id: f_00002-3-2 loss: 1.038413  [   96/  124]
train() client id: f_00002-4-0 loss: 1.058324  [   32/  124]
train() client id: f_00002-4-1 loss: 0.973696  [   64/  124]
train() client id: f_00002-4-2 loss: 0.868918  [   96/  124]
train() client id: f_00002-5-0 loss: 0.941558  [   32/  124]
train() client id: f_00002-5-1 loss: 0.944266  [   64/  124]
train() client id: f_00002-5-2 loss: 0.989395  [   96/  124]
train() client id: f_00002-6-0 loss: 0.794318  [   32/  124]
train() client id: f_00002-6-1 loss: 1.058632  [   64/  124]
train() client id: f_00002-6-2 loss: 0.974088  [   96/  124]
train() client id: f_00002-7-0 loss: 0.805844  [   32/  124]
train() client id: f_00002-7-1 loss: 1.025518  [   64/  124]
train() client id: f_00002-7-2 loss: 0.873706  [   96/  124]
train() client id: f_00002-8-0 loss: 0.970296  [   32/  124]
train() client id: f_00002-8-1 loss: 0.837338  [   64/  124]
train() client id: f_00002-8-2 loss: 0.971034  [   96/  124]
train() client id: f_00002-9-0 loss: 0.934700  [   32/  124]
train() client id: f_00002-9-1 loss: 0.944627  [   64/  124]
train() client id: f_00002-9-2 loss: 0.845039  [   96/  124]
train() client id: f_00002-10-0 loss: 0.921848  [   32/  124]
train() client id: f_00002-10-1 loss: 0.834109  [   64/  124]
train() client id: f_00002-10-2 loss: 0.906318  [   96/  124]
train() client id: f_00002-11-0 loss: 0.824832  [   32/  124]
train() client id: f_00002-11-1 loss: 0.902880  [   64/  124]
train() client id: f_00002-11-2 loss: 0.999279  [   96/  124]
train() client id: f_00003-0-0 loss: 0.892967  [   32/   43]
train() client id: f_00003-1-0 loss: 0.801033  [   32/   43]
train() client id: f_00003-2-0 loss: 0.902112  [   32/   43]
train() client id: f_00003-3-0 loss: 0.801275  [   32/   43]
train() client id: f_00003-4-0 loss: 0.790044  [   32/   43]
train() client id: f_00003-5-0 loss: 0.832563  [   32/   43]
train() client id: f_00003-6-0 loss: 0.956486  [   32/   43]
train() client id: f_00003-7-0 loss: 0.768521  [   32/   43]
train() client id: f_00003-8-0 loss: 0.838112  [   32/   43]
train() client id: f_00003-9-0 loss: 0.878894  [   32/   43]
train() client id: f_00003-10-0 loss: 0.812407  [   32/   43]
train() client id: f_00003-11-0 loss: 0.743001  [   32/   43]
train() client id: f_00004-0-0 loss: 0.992150  [   32/  306]
train() client id: f_00004-0-1 loss: 0.962575  [   64/  306]
train() client id: f_00004-0-2 loss: 0.808319  [   96/  306]
train() client id: f_00004-0-3 loss: 0.934827  [  128/  306]
train() client id: f_00004-0-4 loss: 0.950764  [  160/  306]
train() client id: f_00004-0-5 loss: 0.923768  [  192/  306]
train() client id: f_00004-0-6 loss: 0.861775  [  224/  306]
train() client id: f_00004-0-7 loss: 0.902637  [  256/  306]
train() client id: f_00004-0-8 loss: 0.926736  [  288/  306]
train() client id: f_00004-1-0 loss: 1.001482  [   32/  306]
train() client id: f_00004-1-1 loss: 0.939734  [   64/  306]
train() client id: f_00004-1-2 loss: 1.002662  [   96/  306]
train() client id: f_00004-1-3 loss: 0.903679  [  128/  306]
train() client id: f_00004-1-4 loss: 0.880759  [  160/  306]
train() client id: f_00004-1-5 loss: 0.917207  [  192/  306]
train() client id: f_00004-1-6 loss: 0.824170  [  224/  306]
train() client id: f_00004-1-7 loss: 0.900566  [  256/  306]
train() client id: f_00004-1-8 loss: 0.929475  [  288/  306]
train() client id: f_00004-2-0 loss: 1.004836  [   32/  306]
train() client id: f_00004-2-1 loss: 0.790855  [   64/  306]
train() client id: f_00004-2-2 loss: 0.957161  [   96/  306]
train() client id: f_00004-2-3 loss: 0.872760  [  128/  306]
train() client id: f_00004-2-4 loss: 1.087695  [  160/  306]
train() client id: f_00004-2-5 loss: 0.811030  [  192/  306]
train() client id: f_00004-2-6 loss: 0.904899  [  224/  306]
train() client id: f_00004-2-7 loss: 0.995428  [  256/  306]
train() client id: f_00004-2-8 loss: 0.822680  [  288/  306]
train() client id: f_00004-3-0 loss: 0.929769  [   32/  306]
train() client id: f_00004-3-1 loss: 0.870666  [   64/  306]
train() client id: f_00004-3-2 loss: 0.974044  [   96/  306]
train() client id: f_00004-3-3 loss: 0.865645  [  128/  306]
train() client id: f_00004-3-4 loss: 0.891818  [  160/  306]
train() client id: f_00004-3-5 loss: 1.014450  [  192/  306]
train() client id: f_00004-3-6 loss: 0.868305  [  224/  306]
train() client id: f_00004-3-7 loss: 0.951153  [  256/  306]
train() client id: f_00004-3-8 loss: 0.920067  [  288/  306]
train() client id: f_00004-4-0 loss: 0.902043  [   32/  306]
train() client id: f_00004-4-1 loss: 0.910944  [   64/  306]
train() client id: f_00004-4-2 loss: 0.726879  [   96/  306]
train() client id: f_00004-4-3 loss: 1.052591  [  128/  306]
train() client id: f_00004-4-4 loss: 0.833665  [  160/  306]
train() client id: f_00004-4-5 loss: 1.009524  [  192/  306]
train() client id: f_00004-4-6 loss: 0.987824  [  224/  306]
train() client id: f_00004-4-7 loss: 0.948187  [  256/  306]
train() client id: f_00004-4-8 loss: 0.900053  [  288/  306]
train() client id: f_00004-5-0 loss: 1.021730  [   32/  306]
train() client id: f_00004-5-1 loss: 0.912441  [   64/  306]
train() client id: f_00004-5-2 loss: 0.956127  [   96/  306]
train() client id: f_00004-5-3 loss: 0.869105  [  128/  306]
train() client id: f_00004-5-4 loss: 0.873280  [  160/  306]
train() client id: f_00004-5-5 loss: 0.867019  [  192/  306]
train() client id: f_00004-5-6 loss: 1.022425  [  224/  306]
train() client id: f_00004-5-7 loss: 0.850859  [  256/  306]
train() client id: f_00004-5-8 loss: 0.822350  [  288/  306]
train() client id: f_00004-6-0 loss: 0.831577  [   32/  306]
train() client id: f_00004-6-1 loss: 0.957377  [   64/  306]
train() client id: f_00004-6-2 loss: 0.931592  [   96/  306]
train() client id: f_00004-6-3 loss: 0.831499  [  128/  306]
train() client id: f_00004-6-4 loss: 0.846363  [  160/  306]
train() client id: f_00004-6-5 loss: 0.951203  [  192/  306]
train() client id: f_00004-6-6 loss: 1.049193  [  224/  306]
train() client id: f_00004-6-7 loss: 0.951933  [  256/  306]
train() client id: f_00004-6-8 loss: 0.941227  [  288/  306]
train() client id: f_00004-7-0 loss: 0.962745  [   32/  306]
train() client id: f_00004-7-1 loss: 0.750027  [   64/  306]
train() client id: f_00004-7-2 loss: 0.968872  [   96/  306]
train() client id: f_00004-7-3 loss: 0.893995  [  128/  306]
train() client id: f_00004-7-4 loss: 0.814392  [  160/  306]
train() client id: f_00004-7-5 loss: 0.800222  [  192/  306]
train() client id: f_00004-7-6 loss: 1.002440  [  224/  306]
train() client id: f_00004-7-7 loss: 0.967946  [  256/  306]
train() client id: f_00004-7-8 loss: 0.934123  [  288/  306]
train() client id: f_00004-8-0 loss: 1.028389  [   32/  306]
train() client id: f_00004-8-1 loss: 0.940178  [   64/  306]
train() client id: f_00004-8-2 loss: 0.988159  [   96/  306]
train() client id: f_00004-8-3 loss: 0.912104  [  128/  306]
train() client id: f_00004-8-4 loss: 0.872206  [  160/  306]
train() client id: f_00004-8-5 loss: 0.976414  [  192/  306]
train() client id: f_00004-8-6 loss: 0.914427  [  224/  306]
train() client id: f_00004-8-7 loss: 0.865359  [  256/  306]
train() client id: f_00004-8-8 loss: 0.762003  [  288/  306]
train() client id: f_00004-9-0 loss: 0.965582  [   32/  306]
train() client id: f_00004-9-1 loss: 0.836697  [   64/  306]
train() client id: f_00004-9-2 loss: 1.027110  [   96/  306]
train() client id: f_00004-9-3 loss: 0.868858  [  128/  306]
train() client id: f_00004-9-4 loss: 0.951955  [  160/  306]
train() client id: f_00004-9-5 loss: 0.866568  [  192/  306]
train() client id: f_00004-9-6 loss: 0.946954  [  224/  306]
train() client id: f_00004-9-7 loss: 0.980081  [  256/  306]
train() client id: f_00004-9-8 loss: 0.860973  [  288/  306]
train() client id: f_00004-10-0 loss: 1.085946  [   32/  306]
train() client id: f_00004-10-1 loss: 0.703479  [   64/  306]
train() client id: f_00004-10-2 loss: 0.841564  [   96/  306]
train() client id: f_00004-10-3 loss: 0.890064  [  128/  306]
train() client id: f_00004-10-4 loss: 0.906591  [  160/  306]
train() client id: f_00004-10-5 loss: 1.057995  [  192/  306]
train() client id: f_00004-10-6 loss: 0.876220  [  224/  306]
train() client id: f_00004-10-7 loss: 1.037050  [  256/  306]
train() client id: f_00004-10-8 loss: 0.889388  [  288/  306]
train() client id: f_00004-11-0 loss: 0.961089  [   32/  306]
train() client id: f_00004-11-1 loss: 0.818202  [   64/  306]
train() client id: f_00004-11-2 loss: 0.925249  [   96/  306]
train() client id: f_00004-11-3 loss: 0.845987  [  128/  306]
train() client id: f_00004-11-4 loss: 0.981667  [  160/  306]
train() client id: f_00004-11-5 loss: 0.939319  [  192/  306]
train() client id: f_00004-11-6 loss: 0.941951  [  224/  306]
train() client id: f_00004-11-7 loss: 0.831947  [  256/  306]
train() client id: f_00004-11-8 loss: 0.975435  [  288/  306]
train() client id: f_00005-0-0 loss: 0.402404  [   32/  146]
train() client id: f_00005-0-1 loss: 0.832292  [   64/  146]
train() client id: f_00005-0-2 loss: 0.390867  [   96/  146]
train() client id: f_00005-0-3 loss: 0.504804  [  128/  146]
train() client id: f_00005-1-0 loss: 0.598662  [   32/  146]
train() client id: f_00005-1-1 loss: 0.437340  [   64/  146]
train() client id: f_00005-1-2 loss: 0.749697  [   96/  146]
train() client id: f_00005-1-3 loss: 0.505339  [  128/  146]
train() client id: f_00005-2-0 loss: 0.482462  [   32/  146]
train() client id: f_00005-2-1 loss: 0.611378  [   64/  146]
train() client id: f_00005-2-2 loss: 0.635525  [   96/  146]
train() client id: f_00005-2-3 loss: 0.627105  [  128/  146]
train() client id: f_00005-3-0 loss: 0.524397  [   32/  146]
train() client id: f_00005-3-1 loss: 0.507707  [   64/  146]
train() client id: f_00005-3-2 loss: 0.700415  [   96/  146]
train() client id: f_00005-3-3 loss: 0.527002  [  128/  146]
train() client id: f_00005-4-0 loss: 0.549441  [   32/  146]
train() client id: f_00005-4-1 loss: 0.340587  [   64/  146]
train() client id: f_00005-4-2 loss: 0.659355  [   96/  146]
train() client id: f_00005-4-3 loss: 0.597751  [  128/  146]
train() client id: f_00005-5-0 loss: 0.765998  [   32/  146]
train() client id: f_00005-5-1 loss: 0.436422  [   64/  146]
train() client id: f_00005-5-2 loss: 0.370351  [   96/  146]
train() client id: f_00005-5-3 loss: 0.797600  [  128/  146]
train() client id: f_00005-6-0 loss: 0.664502  [   32/  146]
train() client id: f_00005-6-1 loss: 0.445174  [   64/  146]
train() client id: f_00005-6-2 loss: 0.570163  [   96/  146]
train() client id: f_00005-6-3 loss: 0.545440  [  128/  146]
train() client id: f_00005-7-0 loss: 0.651127  [   32/  146]
train() client id: f_00005-7-1 loss: 0.379240  [   64/  146]
train() client id: f_00005-7-2 loss: 0.534066  [   96/  146]
train() client id: f_00005-7-3 loss: 0.496603  [  128/  146]
train() client id: f_00005-8-0 loss: 0.687784  [   32/  146]
train() client id: f_00005-8-1 loss: 0.650234  [   64/  146]
train() client id: f_00005-8-2 loss: 0.532751  [   96/  146]
train() client id: f_00005-8-3 loss: 0.417495  [  128/  146]
train() client id: f_00005-9-0 loss: 0.451336  [   32/  146]
train() client id: f_00005-9-1 loss: 0.510482  [   64/  146]
train() client id: f_00005-9-2 loss: 0.756479  [   96/  146]
train() client id: f_00005-9-3 loss: 0.378749  [  128/  146]
train() client id: f_00005-10-0 loss: 0.386979  [   32/  146]
train() client id: f_00005-10-1 loss: 0.648616  [   64/  146]
train() client id: f_00005-10-2 loss: 0.463334  [   96/  146]
train() client id: f_00005-10-3 loss: 0.784846  [  128/  146]
train() client id: f_00005-11-0 loss: 0.738543  [   32/  146]
train() client id: f_00005-11-1 loss: 0.393966  [   64/  146]
train() client id: f_00005-11-2 loss: 0.611575  [   96/  146]
train() client id: f_00005-11-3 loss: 0.564957  [  128/  146]
train() client id: f_00006-0-0 loss: 0.566540  [   32/   54]
train() client id: f_00006-1-0 loss: 0.665900  [   32/   54]
train() client id: f_00006-2-0 loss: 0.671412  [   32/   54]
train() client id: f_00006-3-0 loss: 0.603317  [   32/   54]
train() client id: f_00006-4-0 loss: 0.622179  [   32/   54]
train() client id: f_00006-5-0 loss: 0.558882  [   32/   54]
train() client id: f_00006-6-0 loss: 0.659088  [   32/   54]
train() client id: f_00006-7-0 loss: 0.625894  [   32/   54]
train() client id: f_00006-8-0 loss: 0.625323  [   32/   54]
train() client id: f_00006-9-0 loss: 0.567283  [   32/   54]
train() client id: f_00006-10-0 loss: 0.626752  [   32/   54]
train() client id: f_00006-11-0 loss: 0.611509  [   32/   54]
train() client id: f_00007-0-0 loss: 0.580333  [   32/  179]
train() client id: f_00007-0-1 loss: 0.752962  [   64/  179]
train() client id: f_00007-0-2 loss: 0.749929  [   96/  179]
train() client id: f_00007-0-3 loss: 0.724894  [  128/  179]
train() client id: f_00007-0-4 loss: 0.686102  [  160/  179]
train() client id: f_00007-1-0 loss: 0.699153  [   32/  179]
train() client id: f_00007-1-1 loss: 0.651414  [   64/  179]
train() client id: f_00007-1-2 loss: 0.768942  [   96/  179]
train() client id: f_00007-1-3 loss: 0.708147  [  128/  179]
train() client id: f_00007-1-4 loss: 0.705606  [  160/  179]
train() client id: f_00007-2-0 loss: 0.856197  [   32/  179]
train() client id: f_00007-2-1 loss: 0.696339  [   64/  179]
train() client id: f_00007-2-2 loss: 0.711641  [   96/  179]
train() client id: f_00007-2-3 loss: 0.629794  [  128/  179]
train() client id: f_00007-2-4 loss: 0.725413  [  160/  179]
train() client id: f_00007-3-0 loss: 0.885392  [   32/  179]
train() client id: f_00007-3-1 loss: 0.612656  [   64/  179]
train() client id: f_00007-3-2 loss: 0.582676  [   96/  179]
train() client id: f_00007-3-3 loss: 0.664540  [  128/  179]
train() client id: f_00007-3-4 loss: 0.663472  [  160/  179]
train() client id: f_00007-4-0 loss: 0.573138  [   32/  179]
train() client id: f_00007-4-1 loss: 0.718988  [   64/  179]
train() client id: f_00007-4-2 loss: 0.901365  [   96/  179]
train() client id: f_00007-4-3 loss: 0.658951  [  128/  179]
train() client id: f_00007-4-4 loss: 0.597242  [  160/  179]
train() client id: f_00007-5-0 loss: 0.828837  [   32/  179]
train() client id: f_00007-5-1 loss: 0.711396  [   64/  179]
train() client id: f_00007-5-2 loss: 0.737910  [   96/  179]
train() client id: f_00007-5-3 loss: 0.592244  [  128/  179]
train() client id: f_00007-5-4 loss: 0.568085  [  160/  179]
train() client id: f_00007-6-0 loss: 0.633693  [   32/  179]
train() client id: f_00007-6-1 loss: 0.646636  [   64/  179]
train() client id: f_00007-6-2 loss: 0.689761  [   96/  179]
train() client id: f_00007-6-3 loss: 0.643365  [  128/  179]
train() client id: f_00007-6-4 loss: 0.954329  [  160/  179]
train() client id: f_00007-7-0 loss: 0.632759  [   32/  179]
train() client id: f_00007-7-1 loss: 0.738671  [   64/  179]
train() client id: f_00007-7-2 loss: 0.593224  [   96/  179]
train() client id: f_00007-7-3 loss: 0.764162  [  128/  179]
train() client id: f_00007-7-4 loss: 0.744704  [  160/  179]
train() client id: f_00007-8-0 loss: 0.837088  [   32/  179]
train() client id: f_00007-8-1 loss: 0.718903  [   64/  179]
train() client id: f_00007-8-2 loss: 0.679078  [   96/  179]
train() client id: f_00007-8-3 loss: 0.559910  [  128/  179]
train() client id: f_00007-8-4 loss: 0.670755  [  160/  179]
train() client id: f_00007-9-0 loss: 0.757318  [   32/  179]
train() client id: f_00007-9-1 loss: 0.663601  [   64/  179]
train() client id: f_00007-9-2 loss: 0.647028  [   96/  179]
train() client id: f_00007-9-3 loss: 0.548507  [  128/  179]
train() client id: f_00007-9-4 loss: 0.752768  [  160/  179]
train() client id: f_00007-10-0 loss: 0.664724  [   32/  179]
train() client id: f_00007-10-1 loss: 0.724988  [   64/  179]
train() client id: f_00007-10-2 loss: 0.742418  [   96/  179]
train() client id: f_00007-10-3 loss: 0.644912  [  128/  179]
train() client id: f_00007-10-4 loss: 0.577996  [  160/  179]
train() client id: f_00007-11-0 loss: 0.521543  [   32/  179]
train() client id: f_00007-11-1 loss: 0.662101  [   64/  179]
train() client id: f_00007-11-2 loss: 0.641237  [   96/  179]
train() client id: f_00007-11-3 loss: 0.807265  [  128/  179]
train() client id: f_00007-11-4 loss: 0.760377  [  160/  179]
train() client id: f_00008-0-0 loss: 0.750584  [   32/  130]
train() client id: f_00008-0-1 loss: 0.689515  [   64/  130]
train() client id: f_00008-0-2 loss: 0.666068  [   96/  130]
train() client id: f_00008-0-3 loss: 0.843776  [  128/  130]
train() client id: f_00008-1-0 loss: 0.716589  [   32/  130]
train() client id: f_00008-1-1 loss: 0.695526  [   64/  130]
train() client id: f_00008-1-2 loss: 0.874400  [   96/  130]
train() client id: f_00008-1-3 loss: 0.625083  [  128/  130]
train() client id: f_00008-2-0 loss: 0.811345  [   32/  130]
train() client id: f_00008-2-1 loss: 0.649007  [   64/  130]
train() client id: f_00008-2-2 loss: 0.733558  [   96/  130]
train() client id: f_00008-2-3 loss: 0.740510  [  128/  130]
train() client id: f_00008-3-0 loss: 0.737309  [   32/  130]
train() client id: f_00008-3-1 loss: 0.808258  [   64/  130]
train() client id: f_00008-3-2 loss: 0.731326  [   96/  130]
train() client id: f_00008-3-3 loss: 0.667520  [  128/  130]
train() client id: f_00008-4-0 loss: 0.704149  [   32/  130]
train() client id: f_00008-4-1 loss: 0.708813  [   64/  130]
train() client id: f_00008-4-2 loss: 0.811062  [   96/  130]
train() client id: f_00008-4-3 loss: 0.721137  [  128/  130]
train() client id: f_00008-5-0 loss: 0.677581  [   32/  130]
train() client id: f_00008-5-1 loss: 0.732970  [   64/  130]
train() client id: f_00008-5-2 loss: 0.768100  [   96/  130]
train() client id: f_00008-5-3 loss: 0.756055  [  128/  130]
train() client id: f_00008-6-0 loss: 0.814313  [   32/  130]
train() client id: f_00008-6-1 loss: 0.695857  [   64/  130]
train() client id: f_00008-6-2 loss: 0.694486  [   96/  130]
train() client id: f_00008-6-3 loss: 0.734057  [  128/  130]
train() client id: f_00008-7-0 loss: 0.696607  [   32/  130]
train() client id: f_00008-7-1 loss: 0.797052  [   64/  130]
train() client id: f_00008-7-2 loss: 0.800022  [   96/  130]
train() client id: f_00008-7-3 loss: 0.616082  [  128/  130]
train() client id: f_00008-8-0 loss: 0.798336  [   32/  130]
train() client id: f_00008-8-1 loss: 0.707874  [   64/  130]
train() client id: f_00008-8-2 loss: 0.741216  [   96/  130]
train() client id: f_00008-8-3 loss: 0.676811  [  128/  130]
train() client id: f_00008-9-0 loss: 0.728501  [   32/  130]
train() client id: f_00008-9-1 loss: 0.789803  [   64/  130]
train() client id: f_00008-9-2 loss: 0.676218  [   96/  130]
train() client id: f_00008-9-3 loss: 0.737428  [  128/  130]
train() client id: f_00008-10-0 loss: 0.615813  [   32/  130]
train() client id: f_00008-10-1 loss: 0.758784  [   64/  130]
train() client id: f_00008-10-2 loss: 0.882227  [   96/  130]
train() client id: f_00008-10-3 loss: 0.677794  [  128/  130]
train() client id: f_00008-11-0 loss: 0.699397  [   32/  130]
train() client id: f_00008-11-1 loss: 0.805462  [   64/  130]
train() client id: f_00008-11-2 loss: 0.675333  [   96/  130]
train() client id: f_00008-11-3 loss: 0.736440  [  128/  130]
train() client id: f_00009-0-0 loss: 1.137414  [   32/  118]
train() client id: f_00009-0-1 loss: 1.214090  [   64/  118]
train() client id: f_00009-0-2 loss: 1.264565  [   96/  118]
train() client id: f_00009-1-0 loss: 1.125928  [   32/  118]
train() client id: f_00009-1-1 loss: 1.189313  [   64/  118]
train() client id: f_00009-1-2 loss: 1.040443  [   96/  118]
train() client id: f_00009-2-0 loss: 1.207483  [   32/  118]
train() client id: f_00009-2-1 loss: 1.162899  [   64/  118]
train() client id: f_00009-2-2 loss: 1.000962  [   96/  118]
train() client id: f_00009-3-0 loss: 1.137758  [   32/  118]
train() client id: f_00009-3-1 loss: 0.970421  [   64/  118]
train() client id: f_00009-3-2 loss: 1.122694  [   96/  118]
train() client id: f_00009-4-0 loss: 0.980881  [   32/  118]
train() client id: f_00009-4-1 loss: 1.113209  [   64/  118]
train() client id: f_00009-4-2 loss: 0.937278  [   96/  118]
train() client id: f_00009-5-0 loss: 1.062416  [   32/  118]
train() client id: f_00009-5-1 loss: 1.080047  [   64/  118]
train() client id: f_00009-5-2 loss: 0.867000  [   96/  118]
train() client id: f_00009-6-0 loss: 0.851380  [   32/  118]
train() client id: f_00009-6-1 loss: 0.990328  [   64/  118]
train() client id: f_00009-6-2 loss: 1.093791  [   96/  118]
train() client id: f_00009-7-0 loss: 0.997042  [   32/  118]
train() client id: f_00009-7-1 loss: 0.901091  [   64/  118]
train() client id: f_00009-7-2 loss: 0.945441  [   96/  118]
train() client id: f_00009-8-0 loss: 1.096162  [   32/  118]
train() client id: f_00009-8-1 loss: 0.846816  [   64/  118]
train() client id: f_00009-8-2 loss: 1.037919  [   96/  118]
train() client id: f_00009-9-0 loss: 0.872109  [   32/  118]
train() client id: f_00009-9-1 loss: 1.001684  [   64/  118]
train() client id: f_00009-9-2 loss: 0.950698  [   96/  118]
train() client id: f_00009-10-0 loss: 0.936583  [   32/  118]
train() client id: f_00009-10-1 loss: 0.947240  [   64/  118]
train() client id: f_00009-10-2 loss: 0.799247  [   96/  118]
train() client id: f_00009-11-0 loss: 0.946005  [   32/  118]
train() client id: f_00009-11-1 loss: 0.879348  [   64/  118]
train() client id: f_00009-11-2 loss: 1.009946  [   96/  118]
At round 20 accuracy: 0.6339522546419099
At round 20 training accuracy: 0.5801475519785378
At round 20 training loss: 0.8339617786484693
update_location
xs = [ -3.9056584    4.20031788 120.00902392  18.81129433   0.97929623
   3.95640986 -82.44319194 -61.32485185 104.66397685 -47.06087855]
ys = [112.5879595   95.55583871   1.32061395 -82.45517586  74.35018685
  57.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [150.63632627 138.37832555 156.21750812 130.96839627 124.61504446
 115.57737324 129.62935794 117.30905212 145.81912777 110.59266769]
dists_bs = [182.12231199 196.02073536 342.3373385  322.25143194 202.72912368
 213.91100929 200.2932794  207.99012902 320.93061214 213.56243122]
uav_gains = [3.58669488e-11 4.43787932e-11 3.27265969e-11 5.09343950e-11
 5.76816207e-11 6.96314375e-11 5.22611087e-11 6.70896685e-11
 3.89180544e-11 7.77458897e-11]
bs_gains = [5.17937375e-11 4.21549371e-11 8.84753174e-12 1.04796913e-11
 3.83644419e-11 3.30096791e-11 3.96851685e-11 3.57087343e-11
 1.06009035e-11 3.31607609e-11]
Round 21
-------------------------------
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.22040415 17.10262761  8.09588073  2.90155574 19.72744601  9.50226703
  3.60411444 11.58945304  8.53512887  7.71174179]
obj_prev = 96.99061941020106
eta_min = 5.053940464590129e-12	eta_max = 0.9225775474279215
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 22.544522121240533	eta = 0.9090909090909091
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 39.50145864601099	eta = 0.5188421089429398
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 31.352662916909924	eta = 0.6536931221610769
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.88807124482688	eta = 0.6857257513318464
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81462162317344	eta = 0.6874150666493414
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81442331258453	eta = 0.6874196389895562
eta = 0.6874196389895562
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.03092845 0.06504793 0.03043752 0.01055495 0.07511195 0.03583773
 0.01325505 0.04393804 0.03191031 0.02896474]
ene_total = [2.59558483 4.86886559 2.57325892 1.19041384 5.55486377 2.93485376
 1.36920185 3.40632189 2.84825067 2.47280818]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 0 obj = 5.270771946473119
eta = 0.6874196389895562
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
eta_min = 0.6874196389895763	eta_max = 0.6874196389895456
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 0.029002321139164264	eta = 0.9090909090909091
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 18.041876240655995	eta = 0.0014613638924501724
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.8124320127004978	eta = 0.014547164420730322
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.775015665989668	eta = 0.01485381058620065
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.7750098831933823	eta = 0.01485385897835976
eta = 0.01485385897835976
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.65835243e-04 1.51995270e-03 1.59487963e-04 6.39002520e-06
 2.36032919e-03 2.60104392e-04 1.26291038e-05 4.51290031e-04
 2.14529013e-04 1.37257984e-04]
ene_total = [0.1681456  0.19135646 0.17138602 0.15296698 0.2125038  0.17320668
 0.15230083 0.15429171 0.22843322 0.17041859]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 1 obj = 5.270771946473455
eta = 0.6874196389895763
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
Done!
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.44371661e-05 1.32322957e-04 1.38845892e-05 5.56298252e-07
 2.05483854e-04 2.26439824e-05 1.09945550e-06 3.92880853e-05
 1.86763136e-05 1.19493076e-05]
ene_total = [0.00774068 0.00759388 0.00789857 0.00717382 0.00781923 0.00789217
 0.00713686 0.00682983 0.01052589 0.00787346]
At round 21 energy consumption: 0.07848438862700059
At round 21 eta: 0.6874196389895763
At round 21 a_n: 20.989140069898713
At round 21 local rounds: 12.273194960737252
At round 21 global rounds: 67.14798076901184
gradient difference: 0.39185601472854614
train() client id: f_00000-0-0 loss: 1.420256  [   32/  126]
train() client id: f_00000-0-1 loss: 1.268739  [   64/  126]
train() client id: f_00000-0-2 loss: 1.109397  [   96/  126]
train() client id: f_00000-1-0 loss: 1.344263  [   32/  126]
train() client id: f_00000-1-1 loss: 1.181752  [   64/  126]
train() client id: f_00000-1-2 loss: 1.196796  [   96/  126]
train() client id: f_00000-2-0 loss: 1.117764  [   32/  126]
train() client id: f_00000-2-1 loss: 1.039685  [   64/  126]
train() client id: f_00000-2-2 loss: 1.090148  [   96/  126]
train() client id: f_00000-3-0 loss: 1.052223  [   32/  126]
train() client id: f_00000-3-1 loss: 1.100780  [   64/  126]
train() client id: f_00000-3-2 loss: 1.040374  [   96/  126]
train() client id: f_00000-4-0 loss: 0.968704  [   32/  126]
train() client id: f_00000-4-1 loss: 0.927214  [   64/  126]
train() client id: f_00000-4-2 loss: 0.992792  [   96/  126]
train() client id: f_00000-5-0 loss: 0.940439  [   32/  126]
train() client id: f_00000-5-1 loss: 0.960007  [   64/  126]
train() client id: f_00000-5-2 loss: 0.864081  [   96/  126]
train() client id: f_00000-6-0 loss: 0.882116  [   32/  126]
train() client id: f_00000-6-1 loss: 0.845135  [   64/  126]
train() client id: f_00000-6-2 loss: 0.958257  [   96/  126]
train() client id: f_00000-7-0 loss: 0.849932  [   32/  126]
train() client id: f_00000-7-1 loss: 0.930903  [   64/  126]
train() client id: f_00000-7-2 loss: 0.772519  [   96/  126]
train() client id: f_00000-8-0 loss: 0.833554  [   32/  126]
train() client id: f_00000-8-1 loss: 0.795739  [   64/  126]
train() client id: f_00000-8-2 loss: 0.902671  [   96/  126]
train() client id: f_00000-9-0 loss: 0.879643  [   32/  126]
train() client id: f_00000-9-1 loss: 0.781284  [   64/  126]
train() client id: f_00000-9-2 loss: 0.811558  [   96/  126]
train() client id: f_00000-10-0 loss: 0.841784  [   32/  126]
train() client id: f_00000-10-1 loss: 0.835403  [   64/  126]
train() client id: f_00000-10-2 loss: 0.823755  [   96/  126]
train() client id: f_00000-11-0 loss: 0.748457  [   32/  126]
train() client id: f_00000-11-1 loss: 0.743444  [   64/  126]
train() client id: f_00000-11-2 loss: 0.901427  [   96/  126]
train() client id: f_00001-0-0 loss: 0.486731  [   32/  265]
train() client id: f_00001-0-1 loss: 0.652252  [   64/  265]
train() client id: f_00001-0-2 loss: 0.531165  [   96/  265]
train() client id: f_00001-0-3 loss: 0.526542  [  128/  265]
train() client id: f_00001-0-4 loss: 0.563117  [  160/  265]
train() client id: f_00001-0-5 loss: 0.599877  [  192/  265]
train() client id: f_00001-0-6 loss: 0.441003  [  224/  265]
train() client id: f_00001-0-7 loss: 0.547072  [  256/  265]
train() client id: f_00001-1-0 loss: 0.460457  [   32/  265]
train() client id: f_00001-1-1 loss: 0.450915  [   64/  265]
train() client id: f_00001-1-2 loss: 0.586434  [   96/  265]
train() client id: f_00001-1-3 loss: 0.617431  [  128/  265]
train() client id: f_00001-1-4 loss: 0.487402  [  160/  265]
train() client id: f_00001-1-5 loss: 0.629717  [  192/  265]
train() client id: f_00001-1-6 loss: 0.514738  [  224/  265]
train() client id: f_00001-1-7 loss: 0.532188  [  256/  265]
train() client id: f_00001-2-0 loss: 0.635174  [   32/  265]
train() client id: f_00001-2-1 loss: 0.479314  [   64/  265]
train() client id: f_00001-2-2 loss: 0.585150  [   96/  265]
train() client id: f_00001-2-3 loss: 0.450104  [  128/  265]
train() client id: f_00001-2-4 loss: 0.443764  [  160/  265]
train() client id: f_00001-2-5 loss: 0.570172  [  192/  265]
train() client id: f_00001-2-6 loss: 0.500848  [  224/  265]
train() client id: f_00001-2-7 loss: 0.542129  [  256/  265]
train() client id: f_00001-3-0 loss: 0.548031  [   32/  265]
train() client id: f_00001-3-1 loss: 0.504888  [   64/  265]
train() client id: f_00001-3-2 loss: 0.594301  [   96/  265]
train() client id: f_00001-3-3 loss: 0.484468  [  128/  265]
train() client id: f_00001-3-4 loss: 0.519625  [  160/  265]
train() client id: f_00001-3-5 loss: 0.490076  [  192/  265]
train() client id: f_00001-3-6 loss: 0.468441  [  224/  265]
train() client id: f_00001-3-7 loss: 0.578922  [  256/  265]
train() client id: f_00001-4-0 loss: 0.558559  [   32/  265]
train() client id: f_00001-4-1 loss: 0.472230  [   64/  265]
train() client id: f_00001-4-2 loss: 0.447959  [   96/  265]
train() client id: f_00001-4-3 loss: 0.465761  [  128/  265]
train() client id: f_00001-4-4 loss: 0.505436  [  160/  265]
train() client id: f_00001-4-5 loss: 0.577870  [  192/  265]
train() client id: f_00001-4-6 loss: 0.560246  [  224/  265]
train() client id: f_00001-4-7 loss: 0.581578  [  256/  265]
train() client id: f_00001-5-0 loss: 0.437147  [   32/  265]
train() client id: f_00001-5-1 loss: 0.553180  [   64/  265]
train() client id: f_00001-5-2 loss: 0.589809  [   96/  265]
train() client id: f_00001-5-3 loss: 0.550393  [  128/  265]
train() client id: f_00001-5-4 loss: 0.549005  [  160/  265]
train() client id: f_00001-5-5 loss: 0.437139  [  192/  265]
train() client id: f_00001-5-6 loss: 0.509193  [  224/  265]
train() client id: f_00001-5-7 loss: 0.522566  [  256/  265]
train() client id: f_00001-6-0 loss: 0.482375  [   32/  265]
train() client id: f_00001-6-1 loss: 0.413463  [   64/  265]
train() client id: f_00001-6-2 loss: 0.423619  [   96/  265]
train() client id: f_00001-6-3 loss: 0.543769  [  128/  265]
train() client id: f_00001-6-4 loss: 0.523889  [  160/  265]
train() client id: f_00001-6-5 loss: 0.575768  [  192/  265]
train() client id: f_00001-6-6 loss: 0.562338  [  224/  265]
train() client id: f_00001-6-7 loss: 0.616315  [  256/  265]
train() client id: f_00001-7-0 loss: 0.561527  [   32/  265]
train() client id: f_00001-7-1 loss: 0.413426  [   64/  265]
train() client id: f_00001-7-2 loss: 0.504669  [   96/  265]
train() client id: f_00001-7-3 loss: 0.458964  [  128/  265]
train() client id: f_00001-7-4 loss: 0.431306  [  160/  265]
train() client id: f_00001-7-5 loss: 0.482986  [  192/  265]
train() client id: f_00001-7-6 loss: 0.685483  [  224/  265]
train() client id: f_00001-7-7 loss: 0.601463  [  256/  265]
train() client id: f_00001-8-0 loss: 0.605520  [   32/  265]
train() client id: f_00001-8-1 loss: 0.540064  [   64/  265]
train() client id: f_00001-8-2 loss: 0.574138  [   96/  265]
train() client id: f_00001-8-3 loss: 0.484357  [  128/  265]
train() client id: f_00001-8-4 loss: 0.492972  [  160/  265]
train() client id: f_00001-8-5 loss: 0.486761  [  192/  265]
train() client id: f_00001-8-6 loss: 0.499345  [  224/  265]
train() client id: f_00001-8-7 loss: 0.436794  [  256/  265]
train() client id: f_00001-9-0 loss: 0.567602  [   32/  265]
train() client id: f_00001-9-1 loss: 0.432752  [   64/  265]
train() client id: f_00001-9-2 loss: 0.486981  [   96/  265]
train() client id: f_00001-9-3 loss: 0.480960  [  128/  265]
train() client id: f_00001-9-4 loss: 0.526338  [  160/  265]
train() client id: f_00001-9-5 loss: 0.438602  [  192/  265]
train() client id: f_00001-9-6 loss: 0.586497  [  224/  265]
train() client id: f_00001-9-7 loss: 0.611609  [  256/  265]
train() client id: f_00001-10-0 loss: 0.493385  [   32/  265]
train() client id: f_00001-10-1 loss: 0.546780  [   64/  265]
train() client id: f_00001-10-2 loss: 0.416481  [   96/  265]
train() client id: f_00001-10-3 loss: 0.550126  [  128/  265]
train() client id: f_00001-10-4 loss: 0.528091  [  160/  265]
train() client id: f_00001-10-5 loss: 0.585046  [  192/  265]
train() client id: f_00001-10-6 loss: 0.609440  [  224/  265]
train() client id: f_00001-10-7 loss: 0.407303  [  256/  265]
train() client id: f_00001-11-0 loss: 0.421202  [   32/  265]
train() client id: f_00001-11-1 loss: 0.526601  [   64/  265]
train() client id: f_00001-11-2 loss: 0.508735  [   96/  265]
train() client id: f_00001-11-3 loss: 0.459939  [  128/  265]
train() client id: f_00001-11-4 loss: 0.543050  [  160/  265]
train() client id: f_00001-11-5 loss: 0.559293  [  192/  265]
train() client id: f_00001-11-6 loss: 0.492045  [  224/  265]
train() client id: f_00001-11-7 loss: 0.575638  [  256/  265]
train() client id: f_00002-0-0 loss: 1.051920  [   32/  124]
train() client id: f_00002-0-1 loss: 1.142760  [   64/  124]
train() client id: f_00002-0-2 loss: 0.905277  [   96/  124]
train() client id: f_00002-1-0 loss: 1.047430  [   32/  124]
train() client id: f_00002-1-1 loss: 0.979368  [   64/  124]
train() client id: f_00002-1-2 loss: 1.013580  [   96/  124]
train() client id: f_00002-2-0 loss: 1.014055  [   32/  124]
train() client id: f_00002-2-1 loss: 0.941589  [   64/  124]
train() client id: f_00002-2-2 loss: 0.988872  [   96/  124]
train() client id: f_00002-3-0 loss: 0.917415  [   32/  124]
train() client id: f_00002-3-1 loss: 0.900187  [   64/  124]
train() client id: f_00002-3-2 loss: 1.080564  [   96/  124]
train() client id: f_00002-4-0 loss: 0.836889  [   32/  124]
train() client id: f_00002-4-1 loss: 1.025763  [   64/  124]
train() client id: f_00002-4-2 loss: 0.882229  [   96/  124]
train() client id: f_00002-5-0 loss: 0.799079  [   32/  124]
train() client id: f_00002-5-1 loss: 0.782481  [   64/  124]
train() client id: f_00002-5-2 loss: 1.059412  [   96/  124]
train() client id: f_00002-6-0 loss: 0.847184  [   32/  124]
train() client id: f_00002-6-1 loss: 0.919167  [   64/  124]
train() client id: f_00002-6-2 loss: 0.941293  [   96/  124]
train() client id: f_00002-7-0 loss: 0.885709  [   32/  124]
train() client id: f_00002-7-1 loss: 0.732795  [   64/  124]
train() client id: f_00002-7-2 loss: 0.774775  [   96/  124]
train() client id: f_00002-8-0 loss: 0.803901  [   32/  124]
train() client id: f_00002-8-1 loss: 0.812195  [   64/  124]
train() client id: f_00002-8-2 loss: 0.869206  [   96/  124]
train() client id: f_00002-9-0 loss: 0.799259  [   32/  124]
train() client id: f_00002-9-1 loss: 0.800203  [   64/  124]
train() client id: f_00002-9-2 loss: 0.822436  [   96/  124]
train() client id: f_00002-10-0 loss: 0.986968  [   32/  124]
train() client id: f_00002-10-1 loss: 0.763863  [   64/  124]
train() client id: f_00002-10-2 loss: 0.845910  [   96/  124]
train() client id: f_00002-11-0 loss: 0.837436  [   32/  124]
train() client id: f_00002-11-1 loss: 0.840026  [   64/  124]
train() client id: f_00002-11-2 loss: 0.867880  [   96/  124]
train() client id: f_00003-0-0 loss: 1.019998  [   32/   43]
train() client id: f_00003-1-0 loss: 0.840856  [   32/   43]
train() client id: f_00003-2-0 loss: 0.938224  [   32/   43]
train() client id: f_00003-3-0 loss: 0.806332  [   32/   43]
train() client id: f_00003-4-0 loss: 0.936596  [   32/   43]
train() client id: f_00003-5-0 loss: 0.845575  [   32/   43]
train() client id: f_00003-6-0 loss: 0.987186  [   32/   43]
train() client id: f_00003-7-0 loss: 0.851269  [   32/   43]
train() client id: f_00003-8-0 loss: 0.831023  [   32/   43]
train() client id: f_00003-9-0 loss: 0.898808  [   32/   43]
train() client id: f_00003-10-0 loss: 0.914027  [   32/   43]
train() client id: f_00003-11-0 loss: 0.948246  [   32/   43]
train() client id: f_00004-0-0 loss: 0.888323  [   32/  306]
train() client id: f_00004-0-1 loss: 0.766498  [   64/  306]
train() client id: f_00004-0-2 loss: 0.939534  [   96/  306]
train() client id: f_00004-0-3 loss: 0.892565  [  128/  306]
train() client id: f_00004-0-4 loss: 0.897738  [  160/  306]
train() client id: f_00004-0-5 loss: 0.994488  [  192/  306]
train() client id: f_00004-0-6 loss: 0.944168  [  224/  306]
train() client id: f_00004-0-7 loss: 0.790850  [  256/  306]
train() client id: f_00004-0-8 loss: 0.768634  [  288/  306]
train() client id: f_00004-1-0 loss: 0.954502  [   32/  306]
train() client id: f_00004-1-1 loss: 0.848756  [   64/  306]
train() client id: f_00004-1-2 loss: 0.929685  [   96/  306]
train() client id: f_00004-1-3 loss: 0.903726  [  128/  306]
train() client id: f_00004-1-4 loss: 0.911358  [  160/  306]
train() client id: f_00004-1-5 loss: 0.656324  [  192/  306]
train() client id: f_00004-1-6 loss: 0.932886  [  224/  306]
train() client id: f_00004-1-7 loss: 0.807743  [  256/  306]
train() client id: f_00004-1-8 loss: 0.877069  [  288/  306]
train() client id: f_00004-2-0 loss: 0.839712  [   32/  306]
train() client id: f_00004-2-1 loss: 0.989477  [   64/  306]
train() client id: f_00004-2-2 loss: 0.908001  [   96/  306]
train() client id: f_00004-2-3 loss: 0.866847  [  128/  306]
train() client id: f_00004-2-4 loss: 0.839975  [  160/  306]
train() client id: f_00004-2-5 loss: 0.820502  [  192/  306]
train() client id: f_00004-2-6 loss: 0.707936  [  224/  306]
train() client id: f_00004-2-7 loss: 0.885872  [  256/  306]
train() client id: f_00004-2-8 loss: 0.884875  [  288/  306]
train() client id: f_00004-3-0 loss: 0.884292  [   32/  306]
train() client id: f_00004-3-1 loss: 0.886631  [   64/  306]
train() client id: f_00004-3-2 loss: 0.835829  [   96/  306]
train() client id: f_00004-3-3 loss: 0.893410  [  128/  306]
train() client id: f_00004-3-4 loss: 0.805863  [  160/  306]
train() client id: f_00004-3-5 loss: 0.927215  [  192/  306]
train() client id: f_00004-3-6 loss: 0.837315  [  224/  306]
train() client id: f_00004-3-7 loss: 0.779290  [  256/  306]
train() client id: f_00004-3-8 loss: 0.869515  [  288/  306]
train() client id: f_00004-4-0 loss: 0.965371  [   32/  306]
train() client id: f_00004-4-1 loss: 0.921347  [   64/  306]
train() client id: f_00004-4-2 loss: 0.901205  [   96/  306]
train() client id: f_00004-4-3 loss: 0.850131  [  128/  306]
train() client id: f_00004-4-4 loss: 0.781192  [  160/  306]
train() client id: f_00004-4-5 loss: 0.778102  [  192/  306]
train() client id: f_00004-4-6 loss: 0.779861  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838051  [  256/  306]
train() client id: f_00004-4-8 loss: 0.840995  [  288/  306]
train() client id: f_00004-5-0 loss: 0.844444  [   32/  306]
train() client id: f_00004-5-1 loss: 0.835528  [   64/  306]
train() client id: f_00004-5-2 loss: 0.947800  [   96/  306]
train() client id: f_00004-5-3 loss: 0.869369  [  128/  306]
train() client id: f_00004-5-4 loss: 0.789559  [  160/  306]
train() client id: f_00004-5-5 loss: 0.908777  [  192/  306]
train() client id: f_00004-5-6 loss: 0.804486  [  224/  306]
train() client id: f_00004-5-7 loss: 0.986555  [  256/  306]
train() client id: f_00004-5-8 loss: 0.807055  [  288/  306]
train() client id: f_00004-6-0 loss: 0.852817  [   32/  306]
train() client id: f_00004-6-1 loss: 0.877221  [   64/  306]
train() client id: f_00004-6-2 loss: 0.862369  [   96/  306]
train() client id: f_00004-6-3 loss: 0.835342  [  128/  306]
train() client id: f_00004-6-4 loss: 0.817067  [  160/  306]
train() client id: f_00004-6-5 loss: 0.946554  [  192/  306]
train() client id: f_00004-6-6 loss: 0.831687  [  224/  306]
train() client id: f_00004-6-7 loss: 0.893731  [  256/  306]
train() client id: f_00004-6-8 loss: 0.764938  [  288/  306]
train() client id: f_00004-7-0 loss: 0.920989  [   32/  306]
train() client id: f_00004-7-1 loss: 0.769132  [   64/  306]
train() client id: f_00004-7-2 loss: 0.849786  [   96/  306]
train() client id: f_00004-7-3 loss: 0.847008  [  128/  306]
train() client id: f_00004-7-4 loss: 0.869756  [  160/  306]
train() client id: f_00004-7-5 loss: 0.815532  [  192/  306]
train() client id: f_00004-7-6 loss: 0.842017  [  224/  306]
train() client id: f_00004-7-7 loss: 0.901249  [  256/  306]
train() client id: f_00004-7-8 loss: 0.883741  [  288/  306]
train() client id: f_00004-8-0 loss: 0.775893  [   32/  306]
train() client id: f_00004-8-1 loss: 0.803425  [   64/  306]
train() client id: f_00004-8-2 loss: 0.897219  [   96/  306]
train() client id: f_00004-8-3 loss: 0.871927  [  128/  306]
train() client id: f_00004-8-4 loss: 0.810545  [  160/  306]
train() client id: f_00004-8-5 loss: 0.814792  [  192/  306]
train() client id: f_00004-8-6 loss: 0.886263  [  224/  306]
train() client id: f_00004-8-7 loss: 0.903088  [  256/  306]
train() client id: f_00004-8-8 loss: 0.832516  [  288/  306]
train() client id: f_00004-9-0 loss: 0.827452  [   32/  306]
train() client id: f_00004-9-1 loss: 0.841132  [   64/  306]
train() client id: f_00004-9-2 loss: 0.847365  [   96/  306]
train() client id: f_00004-9-3 loss: 0.923304  [  128/  306]
train() client id: f_00004-9-4 loss: 0.821388  [  160/  306]
train() client id: f_00004-9-5 loss: 0.750849  [  192/  306]
train() client id: f_00004-9-6 loss: 0.918453  [  224/  306]
train() client id: f_00004-9-7 loss: 0.833071  [  256/  306]
train() client id: f_00004-9-8 loss: 0.917965  [  288/  306]
train() client id: f_00004-10-0 loss: 0.822485  [   32/  306]
train() client id: f_00004-10-1 loss: 0.746071  [   64/  306]
train() client id: f_00004-10-2 loss: 0.906955  [   96/  306]
train() client id: f_00004-10-3 loss: 0.839993  [  128/  306]
train() client id: f_00004-10-4 loss: 0.875464  [  160/  306]
train() client id: f_00004-10-5 loss: 0.909649  [  192/  306]
train() client id: f_00004-10-6 loss: 0.845178  [  224/  306]
train() client id: f_00004-10-7 loss: 0.834802  [  256/  306]
train() client id: f_00004-10-8 loss: 0.800602  [  288/  306]
train() client id: f_00004-11-0 loss: 0.765487  [   32/  306]
train() client id: f_00004-11-1 loss: 0.809932  [   64/  306]
train() client id: f_00004-11-2 loss: 0.883021  [   96/  306]
train() client id: f_00004-11-3 loss: 0.902592  [  128/  306]
train() client id: f_00004-11-4 loss: 0.869226  [  160/  306]
train() client id: f_00004-11-5 loss: 0.771420  [  192/  306]
train() client id: f_00004-11-6 loss: 0.823260  [  224/  306]
train() client id: f_00004-11-7 loss: 0.903218  [  256/  306]
train() client id: f_00004-11-8 loss: 0.878654  [  288/  306]
train() client id: f_00005-0-0 loss: 0.619742  [   32/  146]
train() client id: f_00005-0-1 loss: 0.813732  [   64/  146]
train() client id: f_00005-0-2 loss: 0.571113  [   96/  146]
train() client id: f_00005-0-3 loss: 0.629421  [  128/  146]
train() client id: f_00005-1-0 loss: 0.730700  [   32/  146]
train() client id: f_00005-1-1 loss: 0.539929  [   64/  146]
train() client id: f_00005-1-2 loss: 0.582220  [   96/  146]
train() client id: f_00005-1-3 loss: 0.813015  [  128/  146]
train() client id: f_00005-2-0 loss: 0.662179  [   32/  146]
train() client id: f_00005-2-1 loss: 0.962220  [   64/  146]
train() client id: f_00005-2-2 loss: 0.431878  [   96/  146]
train() client id: f_00005-2-3 loss: 0.551814  [  128/  146]
train() client id: f_00005-3-0 loss: 0.494812  [   32/  146]
train() client id: f_00005-3-1 loss: 0.629234  [   64/  146]
train() client id: f_00005-3-2 loss: 0.848073  [   96/  146]
train() client id: f_00005-3-3 loss: 0.670462  [  128/  146]
train() client id: f_00005-4-0 loss: 0.720942  [   32/  146]
train() client id: f_00005-4-1 loss: 0.528189  [   64/  146]
train() client id: f_00005-4-2 loss: 0.618958  [   96/  146]
train() client id: f_00005-4-3 loss: 0.718943  [  128/  146]
train() client id: f_00005-5-0 loss: 0.853212  [   32/  146]
train() client id: f_00005-5-1 loss: 0.467733  [   64/  146]
train() client id: f_00005-5-2 loss: 0.869209  [   96/  146]
train() client id: f_00005-5-3 loss: 0.584811  [  128/  146]
train() client id: f_00005-6-0 loss: 0.792777  [   32/  146]
train() client id: f_00005-6-1 loss: 0.513719  [   64/  146]
train() client id: f_00005-6-2 loss: 0.835892  [   96/  146]
train() client id: f_00005-6-3 loss: 0.531060  [  128/  146]
train() client id: f_00005-7-0 loss: 0.596195  [   32/  146]
train() client id: f_00005-7-1 loss: 0.596031  [   64/  146]
train() client id: f_00005-7-2 loss: 0.606291  [   96/  146]
train() client id: f_00005-7-3 loss: 0.734305  [  128/  146]
train() client id: f_00005-8-0 loss: 0.811679  [   32/  146]
train() client id: f_00005-8-1 loss: 0.600193  [   64/  146]
train() client id: f_00005-8-2 loss: 0.732811  [   96/  146]
train() client id: f_00005-8-3 loss: 0.551189  [  128/  146]
train() client id: f_00005-9-0 loss: 0.763980  [   32/  146]
train() client id: f_00005-9-1 loss: 0.529524  [   64/  146]
train() client id: f_00005-9-2 loss: 0.721578  [   96/  146]
train() client id: f_00005-9-3 loss: 0.597407  [  128/  146]
train() client id: f_00005-10-0 loss: 0.687719  [   32/  146]
train() client id: f_00005-10-1 loss: 0.631193  [   64/  146]
train() client id: f_00005-10-2 loss: 0.753651  [   96/  146]
train() client id: f_00005-10-3 loss: 0.566639  [  128/  146]
train() client id: f_00005-11-0 loss: 0.625968  [   32/  146]
train() client id: f_00005-11-1 loss: 0.713850  [   64/  146]
train() client id: f_00005-11-2 loss: 0.714019  [   96/  146]
train() client id: f_00005-11-3 loss: 0.511108  [  128/  146]
train() client id: f_00006-0-0 loss: 0.632858  [   32/   54]
train() client id: f_00006-1-0 loss: 0.533921  [   32/   54]
train() client id: f_00006-2-0 loss: 0.554546  [   32/   54]
train() client id: f_00006-3-0 loss: 0.562966  [   32/   54]
train() client id: f_00006-4-0 loss: 0.586660  [   32/   54]
train() client id: f_00006-5-0 loss: 0.570680  [   32/   54]
train() client id: f_00006-6-0 loss: 0.581005  [   32/   54]
train() client id: f_00006-7-0 loss: 0.623187  [   32/   54]
train() client id: f_00006-8-0 loss: 0.518795  [   32/   54]
train() client id: f_00006-9-0 loss: 0.625020  [   32/   54]
train() client id: f_00006-10-0 loss: 0.629343  [   32/   54]
train() client id: f_00006-11-0 loss: 0.639075  [   32/   54]
train() client id: f_00007-0-0 loss: 0.572099  [   32/  179]
train() client id: f_00007-0-1 loss: 0.506218  [   64/  179]
train() client id: f_00007-0-2 loss: 0.516279  [   96/  179]
train() client id: f_00007-0-3 loss: 0.666959  [  128/  179]
train() client id: f_00007-0-4 loss: 0.525827  [  160/  179]
train() client id: f_00007-1-0 loss: 0.516075  [   32/  179]
train() client id: f_00007-1-1 loss: 0.555586  [   64/  179]
train() client id: f_00007-1-2 loss: 0.648506  [   96/  179]
train() client id: f_00007-1-3 loss: 0.497021  [  128/  179]
train() client id: f_00007-1-4 loss: 0.541007  [  160/  179]
train() client id: f_00007-2-0 loss: 0.510350  [   32/  179]
train() client id: f_00007-2-1 loss: 0.628257  [   64/  179]
train() client id: f_00007-2-2 loss: 0.596526  [   96/  179]
train() client id: f_00007-2-3 loss: 0.411870  [  128/  179]
train() client id: f_00007-2-4 loss: 0.530532  [  160/  179]
train() client id: f_00007-3-0 loss: 0.486979  [   32/  179]
train() client id: f_00007-3-1 loss: 0.500312  [   64/  179]
train() client id: f_00007-3-2 loss: 0.412198  [   96/  179]
train() client id: f_00007-3-3 loss: 0.569104  [  128/  179]
train() client id: f_00007-3-4 loss: 0.485355  [  160/  179]
train() client id: f_00007-4-0 loss: 0.491595  [   32/  179]
train() client id: f_00007-4-1 loss: 0.448995  [   64/  179]
train() client id: f_00007-4-2 loss: 0.584715  [   96/  179]
train() client id: f_00007-4-3 loss: 0.465120  [  128/  179]
train() client id: f_00007-4-4 loss: 0.640047  [  160/  179]
train() client id: f_00007-5-0 loss: 0.393034  [   32/  179]
train() client id: f_00007-5-1 loss: 0.368693  [   64/  179]
train() client id: f_00007-5-2 loss: 0.802818  [   96/  179]
train() client id: f_00007-5-3 loss: 0.488610  [  128/  179]
train() client id: f_00007-5-4 loss: 0.480995  [  160/  179]
train() client id: f_00007-6-0 loss: 0.493178  [   32/  179]
train() client id: f_00007-6-1 loss: 0.610824  [   64/  179]
train() client id: f_00007-6-2 loss: 0.505850  [   96/  179]
train() client id: f_00007-6-3 loss: 0.389975  [  128/  179]
train() client id: f_00007-6-4 loss: 0.497199  [  160/  179]
train() client id: f_00007-7-0 loss: 0.663985  [   32/  179]
train() client id: f_00007-7-1 loss: 0.394427  [   64/  179]
train() client id: f_00007-7-2 loss: 0.454858  [   96/  179]
train() client id: f_00007-7-3 loss: 0.403190  [  128/  179]
train() client id: f_00007-7-4 loss: 0.553549  [  160/  179]
train() client id: f_00007-8-0 loss: 0.697034  [   32/  179]
train() client id: f_00007-8-1 loss: 0.578932  [   64/  179]
train() client id: f_00007-8-2 loss: 0.551659  [   96/  179]
train() client id: f_00007-8-3 loss: 0.460500  [  128/  179]
train() client id: f_00007-8-4 loss: 0.366467  [  160/  179]
train() client id: f_00007-9-0 loss: 0.458679  [   32/  179]
train() client id: f_00007-9-1 loss: 0.528999  [   64/  179]
train() client id: f_00007-9-2 loss: 0.605572  [   96/  179]
train() client id: f_00007-9-3 loss: 0.482057  [  128/  179]
train() client id: f_00007-9-4 loss: 0.469603  [  160/  179]
train() client id: f_00007-10-0 loss: 0.574712  [   32/  179]
train() client id: f_00007-10-1 loss: 0.464601  [   64/  179]
train() client id: f_00007-10-2 loss: 0.621157  [   96/  179]
train() client id: f_00007-10-3 loss: 0.361354  [  128/  179]
train() client id: f_00007-10-4 loss: 0.508592  [  160/  179]
train() client id: f_00007-11-0 loss: 0.574956  [   32/  179]
train() client id: f_00007-11-1 loss: 0.372641  [   64/  179]
train() client id: f_00007-11-2 loss: 0.408355  [   96/  179]
train() client id: f_00007-11-3 loss: 0.540478  [  128/  179]
train() client id: f_00007-11-4 loss: 0.573168  [  160/  179]
train() client id: f_00008-0-0 loss: 0.736106  [   32/  130]
train() client id: f_00008-0-1 loss: 0.713632  [   64/  130]
train() client id: f_00008-0-2 loss: 0.869898  [   96/  130]
train() client id: f_00008-0-3 loss: 0.724733  [  128/  130]
train() client id: f_00008-1-0 loss: 0.784420  [   32/  130]
train() client id: f_00008-1-1 loss: 0.723106  [   64/  130]
train() client id: f_00008-1-2 loss: 0.700269  [   96/  130]
train() client id: f_00008-1-3 loss: 0.843443  [  128/  130]
train() client id: f_00008-2-0 loss: 0.875431  [   32/  130]
train() client id: f_00008-2-1 loss: 0.804987  [   64/  130]
train() client id: f_00008-2-2 loss: 0.656049  [   96/  130]
train() client id: f_00008-2-3 loss: 0.720279  [  128/  130]
train() client id: f_00008-3-0 loss: 0.710076  [   32/  130]
train() client id: f_00008-3-1 loss: 0.813443  [   64/  130]
train() client id: f_00008-3-2 loss: 0.781386  [   96/  130]
train() client id: f_00008-3-3 loss: 0.717570  [  128/  130]
train() client id: f_00008-4-0 loss: 0.775743  [   32/  130]
train() client id: f_00008-4-1 loss: 0.739899  [   64/  130]
train() client id: f_00008-4-2 loss: 0.739996  [   96/  130]
train() client id: f_00008-4-3 loss: 0.793511  [  128/  130]
train() client id: f_00008-5-0 loss: 0.874598  [   32/  130]
train() client id: f_00008-5-1 loss: 0.756392  [   64/  130]
train() client id: f_00008-5-2 loss: 0.749610  [   96/  130]
train() client id: f_00008-5-3 loss: 0.674928  [  128/  130]
train() client id: f_00008-6-0 loss: 0.823950  [   32/  130]
train() client id: f_00008-6-1 loss: 0.797405  [   64/  130]
train() client id: f_00008-6-2 loss: 0.807127  [   96/  130]
train() client id: f_00008-6-3 loss: 0.637049  [  128/  130]
train() client id: f_00008-7-0 loss: 0.793163  [   32/  130]
train() client id: f_00008-7-1 loss: 0.817823  [   64/  130]
train() client id: f_00008-7-2 loss: 0.709291  [   96/  130]
train() client id: f_00008-7-3 loss: 0.750474  [  128/  130]
train() client id: f_00008-8-0 loss: 0.678345  [   32/  130]
train() client id: f_00008-8-1 loss: 0.896959  [   64/  130]
train() client id: f_00008-8-2 loss: 0.762717  [   96/  130]
train() client id: f_00008-8-3 loss: 0.684884  [  128/  130]
train() client id: f_00008-9-0 loss: 0.747242  [   32/  130]
train() client id: f_00008-9-1 loss: 0.747276  [   64/  130]
train() client id: f_00008-9-2 loss: 0.792194  [   96/  130]
train() client id: f_00008-9-3 loss: 0.790642  [  128/  130]
train() client id: f_00008-10-0 loss: 0.727650  [   32/  130]
train() client id: f_00008-10-1 loss: 0.757741  [   64/  130]
train() client id: f_00008-10-2 loss: 0.774936  [   96/  130]
train() client id: f_00008-10-3 loss: 0.818261  [  128/  130]
train() client id: f_00008-11-0 loss: 0.789428  [   32/  130]
train() client id: f_00008-11-1 loss: 0.772114  [   64/  130]
train() client id: f_00008-11-2 loss: 0.713028  [   96/  130]
train() client id: f_00008-11-3 loss: 0.765854  [  128/  130]
train() client id: f_00009-0-0 loss: 1.119383  [   32/  118]
train() client id: f_00009-0-1 loss: 1.041352  [   64/  118]
train() client id: f_00009-0-2 loss: 0.928479  [   96/  118]
train() client id: f_00009-1-0 loss: 1.096611  [   32/  118]
train() client id: f_00009-1-1 loss: 1.033130  [   64/  118]
train() client id: f_00009-1-2 loss: 0.929062  [   96/  118]
train() client id: f_00009-2-0 loss: 0.887454  [   32/  118]
train() client id: f_00009-2-1 loss: 1.020947  [   64/  118]
train() client id: f_00009-2-2 loss: 0.936227  [   96/  118]
train() client id: f_00009-3-0 loss: 0.946221  [   32/  118]
train() client id: f_00009-3-1 loss: 0.940192  [   64/  118]
train() client id: f_00009-3-2 loss: 0.955627  [   96/  118]
train() client id: f_00009-4-0 loss: 0.869422  [   32/  118]
train() client id: f_00009-4-1 loss: 0.881850  [   64/  118]
train() client id: f_00009-4-2 loss: 0.834804  [   96/  118]
train() client id: f_00009-5-0 loss: 0.840267  [   32/  118]
train() client id: f_00009-5-1 loss: 0.899937  [   64/  118]
train() client id: f_00009-5-2 loss: 0.791078  [   96/  118]
train() client id: f_00009-6-0 loss: 0.846784  [   32/  118]
train() client id: f_00009-6-1 loss: 0.916612  [   64/  118]
train() client id: f_00009-6-2 loss: 0.842627  [   96/  118]
train() client id: f_00009-7-0 loss: 0.904446  [   32/  118]
train() client id: f_00009-7-1 loss: 0.903514  [   64/  118]
train() client id: f_00009-7-2 loss: 0.734179  [   96/  118]
train() client id: f_00009-8-0 loss: 0.766135  [   32/  118]
train() client id: f_00009-8-1 loss: 0.885404  [   64/  118]
train() client id: f_00009-8-2 loss: 0.830322  [   96/  118]
train() client id: f_00009-9-0 loss: 0.883114  [   32/  118]
train() client id: f_00009-9-1 loss: 0.683830  [   64/  118]
train() client id: f_00009-9-2 loss: 0.802266  [   96/  118]
train() client id: f_00009-10-0 loss: 0.895124  [   32/  118]
train() client id: f_00009-10-1 loss: 0.861721  [   64/  118]
train() client id: f_00009-10-2 loss: 0.701850  [   96/  118]
train() client id: f_00009-11-0 loss: 0.778200  [   32/  118]
train() client id: f_00009-11-1 loss: 0.816607  [   64/  118]
train() client id: f_00009-11-2 loss: 0.860831  [   96/  118]
At round 21 accuracy: 0.636604774535809
At round 21 training accuracy: 0.5707578806170356
At round 21 training loss: 0.8444926002716427
update_location
xs = [ -3.9056584    4.20031788 125.00902392  18.81129433   0.97929623
   3.95640986 -87.44319194 -66.32485185 109.66397685 -52.06087855]
ys = [117.5879595  100.55583871   1.32061395 -87.45517586  79.35018685
  62.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [154.40913958 141.87712771 160.0905996  134.17254778 127.66131432
 118.15782121 132.86535425 119.99859261 149.44851217 112.81111174]
dists_bs = [180.46998676 194.04815659 346.65536714 326.25992268 200.29378287
 211.21330783 198.03239556 205.30012735 325.29693755 210.60513008]
uav_gains = [3.37016863e-11 4.16871884e-11 3.07625343e-11 4.79445886e-11
 5.43001226e-11 6.58910772e-11 4.91340014e-11 6.33925495e-11
 3.65879032e-11 7.39794975e-11]
bs_gains = [5.31324844e-11 4.33658063e-11 8.54239964e-12 1.01231494e-11
 3.96848892e-11 3.42038117e-11 4.09668536e-11 3.70343113e-11
 1.02072830e-11 3.44810940e-11]
Round 22
-------------------------------
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.08847719 16.82258033  7.96606227  2.85607422 19.40433678  9.34589642
  3.54718528 11.40188453  8.39827017  7.58446914]
obj_prev = 95.4152363247567
eta_min = 3.3224549121166505e-12	eta_max = 0.9228798704318716
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 22.17659221087636	eta = 0.909090909090909
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 38.93866072382321	eta = 0.5177512014733847
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 30.874641848632802	eta = 0.6529804773886541
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.424734721470024	eta = 0.6851561641714188
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351835344463147	eta = 0.6868578450692009
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351637432902866	eta = 0.6868624763988199
eta = 0.6868624763988199
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.03099536 0.06518865 0.03050337 0.01057778 0.07527445 0.03591526
 0.01328373 0.04403309 0.03197934 0.0290274 ]
ene_total = [2.56014735 4.7872217  2.53839595 1.17632747 5.46154932 2.88284624
 1.35232746 3.3560089  2.80903437 2.42777865]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 0 obj = 5.19823104157566
eta = 0.6868624763988199
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
eta_min = 0.6868624763988167	eta_max = 0.6868624763987461
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 0.02749108426556509	eta = 0.9090909090909091
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 17.828312615008755	eta = 0.0014018093201842298
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7850725781134285	eta = 0.014000492245133406
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.749550246261795	eta = 0.014284753947636938
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7495451585936168	eta = 0.014284795487626767
eta = 0.014284795487626767
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.62444973e-04 1.47688208e-03 1.56235501e-04 6.25676453e-06
 2.29167254e-03 2.52367846e-04 1.23667633e-05 4.41695438e-04
 2.09660410e-04 1.33132867e-04]
ene_total = [0.16747587 0.18628874 0.17074105 0.15226131 0.20631753 0.16882169
 0.151622   0.15306496 0.22691989 0.16603211]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 1 obj = 5.198231041575606
eta = 0.6868624763988167
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
Done!
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.40119877e-05 1.27391160e-04 1.34763784e-05 5.39688647e-07
 1.97672399e-04 2.17684492e-05 1.06671775e-06 3.80992464e-05
 1.80846414e-05 1.14836185e-05]
ene_total = [0.00784724 0.00754436 0.00800881 0.00726358 0.00775607 0.00782933
 0.00722748 0.00690407 0.01064209 0.0078051 ]
At round 22 energy consumption: 0.0788281303900666
At round 22 eta: 0.6868624763988167
At round 22 a_n: 20.646594222929394
At round 22 local rounds: 12.299746037466248
At round 22 global rounds: 65.93458997020494
gradient difference: 0.4328067898750305
train() client id: f_00000-0-0 loss: 1.309237  [   32/  126]
train() client id: f_00000-0-1 loss: 1.265604  [   64/  126]
train() client id: f_00000-0-2 loss: 1.189850  [   96/  126]
train() client id: f_00000-1-0 loss: 1.318978  [   32/  126]
train() client id: f_00000-1-1 loss: 1.122243  [   64/  126]
train() client id: f_00000-1-2 loss: 1.080700  [   96/  126]
train() client id: f_00000-2-0 loss: 1.050319  [   32/  126]
train() client id: f_00000-2-1 loss: 1.165392  [   64/  126]
train() client id: f_00000-2-2 loss: 0.827081  [   96/  126]
train() client id: f_00000-3-0 loss: 1.051865  [   32/  126]
train() client id: f_00000-3-1 loss: 0.948801  [   64/  126]
train() client id: f_00000-3-2 loss: 0.945752  [   96/  126]
train() client id: f_00000-4-0 loss: 0.862557  [   32/  126]
train() client id: f_00000-4-1 loss: 0.945610  [   64/  126]
train() client id: f_00000-4-2 loss: 0.804314  [   96/  126]
train() client id: f_00000-5-0 loss: 1.010682  [   32/  126]
train() client id: f_00000-5-1 loss: 0.867449  [   64/  126]
train() client id: f_00000-5-2 loss: 0.806804  [   96/  126]
train() client id: f_00000-6-0 loss: 0.915959  [   32/  126]
train() client id: f_00000-6-1 loss: 0.865967  [   64/  126]
train() client id: f_00000-6-2 loss: 0.745165  [   96/  126]
train() client id: f_00000-7-0 loss: 0.733799  [   32/  126]
train() client id: f_00000-7-1 loss: 0.776686  [   64/  126]
train() client id: f_00000-7-2 loss: 0.823956  [   96/  126]
train() client id: f_00000-8-0 loss: 0.827835  [   32/  126]
train() client id: f_00000-8-1 loss: 0.847257  [   64/  126]
train() client id: f_00000-8-2 loss: 0.646593  [   96/  126]
train() client id: f_00000-9-0 loss: 0.912248  [   32/  126]
train() client id: f_00000-9-1 loss: 0.784169  [   64/  126]
train() client id: f_00000-9-2 loss: 0.720113  [   96/  126]
train() client id: f_00000-10-0 loss: 0.703393  [   32/  126]
train() client id: f_00000-10-1 loss: 0.748478  [   64/  126]
train() client id: f_00000-10-2 loss: 0.794106  [   96/  126]
train() client id: f_00000-11-0 loss: 0.751148  [   32/  126]
train() client id: f_00000-11-1 loss: 0.695268  [   64/  126]
train() client id: f_00000-11-2 loss: 0.884803  [   96/  126]
train() client id: f_00001-0-0 loss: 0.444344  [   32/  265]
train() client id: f_00001-0-1 loss: 0.473865  [   64/  265]
train() client id: f_00001-0-2 loss: 0.339310  [   96/  265]
train() client id: f_00001-0-3 loss: 0.380740  [  128/  265]
train() client id: f_00001-0-4 loss: 0.540094  [  160/  265]
train() client id: f_00001-0-5 loss: 0.382236  [  192/  265]
train() client id: f_00001-0-6 loss: 0.483286  [  224/  265]
train() client id: f_00001-0-7 loss: 0.444609  [  256/  265]
train() client id: f_00001-1-0 loss: 0.350497  [   32/  265]
train() client id: f_00001-1-1 loss: 0.467737  [   64/  265]
train() client id: f_00001-1-2 loss: 0.455959  [   96/  265]
train() client id: f_00001-1-3 loss: 0.423472  [  128/  265]
train() client id: f_00001-1-4 loss: 0.526701  [  160/  265]
train() client id: f_00001-1-5 loss: 0.381409  [  192/  265]
train() client id: f_00001-1-6 loss: 0.426361  [  224/  265]
train() client id: f_00001-1-7 loss: 0.324979  [  256/  265]
train() client id: f_00001-2-0 loss: 0.423325  [   32/  265]
train() client id: f_00001-2-1 loss: 0.426046  [   64/  265]
train() client id: f_00001-2-2 loss: 0.381991  [   96/  265]
train() client id: f_00001-2-3 loss: 0.388818  [  128/  265]
train() client id: f_00001-2-4 loss: 0.390396  [  160/  265]
train() client id: f_00001-2-5 loss: 0.443022  [  192/  265]
train() client id: f_00001-2-6 loss: 0.456946  [  224/  265]
train() client id: f_00001-2-7 loss: 0.416019  [  256/  265]
train() client id: f_00001-3-0 loss: 0.516385  [   32/  265]
train() client id: f_00001-3-1 loss: 0.376501  [   64/  265]
train() client id: f_00001-3-2 loss: 0.332460  [   96/  265]
train() client id: f_00001-3-3 loss: 0.316974  [  128/  265]
train() client id: f_00001-3-4 loss: 0.467083  [  160/  265]
train() client id: f_00001-3-5 loss: 0.428113  [  192/  265]
train() client id: f_00001-3-6 loss: 0.312288  [  224/  265]
train() client id: f_00001-3-7 loss: 0.530427  [  256/  265]
train() client id: f_00001-4-0 loss: 0.637654  [   32/  265]
train() client id: f_00001-4-1 loss: 0.313561  [   64/  265]
train() client id: f_00001-4-2 loss: 0.386844  [   96/  265]
train() client id: f_00001-4-3 loss: 0.421888  [  128/  265]
train() client id: f_00001-4-4 loss: 0.348925  [  160/  265]
train() client id: f_00001-4-5 loss: 0.311019  [  192/  265]
train() client id: f_00001-4-6 loss: 0.341073  [  224/  265]
train() client id: f_00001-4-7 loss: 0.468489  [  256/  265]
train() client id: f_00001-5-0 loss: 0.396183  [   32/  265]
train() client id: f_00001-5-1 loss: 0.396953  [   64/  265]
train() client id: f_00001-5-2 loss: 0.310315  [   96/  265]
train() client id: f_00001-5-3 loss: 0.446673  [  128/  265]
train() client id: f_00001-5-4 loss: 0.481263  [  160/  265]
train() client id: f_00001-5-5 loss: 0.409612  [  192/  265]
train() client id: f_00001-5-6 loss: 0.319446  [  224/  265]
train() client id: f_00001-5-7 loss: 0.440935  [  256/  265]
train() client id: f_00001-6-0 loss: 0.332965  [   32/  265]
train() client id: f_00001-6-1 loss: 0.304618  [   64/  265]
train() client id: f_00001-6-2 loss: 0.348895  [   96/  265]
train() client id: f_00001-6-3 loss: 0.426472  [  128/  265]
train() client id: f_00001-6-4 loss: 0.544935  [  160/  265]
train() client id: f_00001-6-5 loss: 0.393759  [  192/  265]
train() client id: f_00001-6-6 loss: 0.431720  [  224/  265]
train() client id: f_00001-6-7 loss: 0.283541  [  256/  265]
train() client id: f_00001-7-0 loss: 0.608072  [   32/  265]
train() client id: f_00001-7-1 loss: 0.351091  [   64/  265]
train() client id: f_00001-7-2 loss: 0.430804  [   96/  265]
train() client id: f_00001-7-3 loss: 0.391386  [  128/  265]
train() client id: f_00001-7-4 loss: 0.330331  [  160/  265]
train() client id: f_00001-7-5 loss: 0.351276  [  192/  265]
train() client id: f_00001-7-6 loss: 0.325412  [  224/  265]
train() client id: f_00001-7-7 loss: 0.368195  [  256/  265]
train() client id: f_00001-8-0 loss: 0.471840  [   32/  265]
train() client id: f_00001-8-1 loss: 0.386280  [   64/  265]
train() client id: f_00001-8-2 loss: 0.482812  [   96/  265]
train() client id: f_00001-8-3 loss: 0.437603  [  128/  265]
train() client id: f_00001-8-4 loss: 0.358123  [  160/  265]
train() client id: f_00001-8-5 loss: 0.339111  [  192/  265]
train() client id: f_00001-8-6 loss: 0.303828  [  224/  265]
train() client id: f_00001-8-7 loss: 0.308395  [  256/  265]
train() client id: f_00001-9-0 loss: 0.391316  [   32/  265]
train() client id: f_00001-9-1 loss: 0.433730  [   64/  265]
train() client id: f_00001-9-2 loss: 0.309826  [   96/  265]
train() client id: f_00001-9-3 loss: 0.383767  [  128/  265]
train() client id: f_00001-9-4 loss: 0.368643  [  160/  265]
train() client id: f_00001-9-5 loss: 0.340236  [  192/  265]
train() client id: f_00001-9-6 loss: 0.447577  [  224/  265]
train() client id: f_00001-9-7 loss: 0.457061  [  256/  265]
train() client id: f_00001-10-0 loss: 0.386570  [   32/  265]
train() client id: f_00001-10-1 loss: 0.355126  [   64/  265]
train() client id: f_00001-10-2 loss: 0.365251  [   96/  265]
train() client id: f_00001-10-3 loss: 0.478526  [  128/  265]
train() client id: f_00001-10-4 loss: 0.355054  [  160/  265]
train() client id: f_00001-10-5 loss: 0.362594  [  192/  265]
train() client id: f_00001-10-6 loss: 0.347999  [  224/  265]
train() client id: f_00001-10-7 loss: 0.381542  [  256/  265]
train() client id: f_00001-11-0 loss: 0.422356  [   32/  265]
train() client id: f_00001-11-1 loss: 0.375389  [   64/  265]
train() client id: f_00001-11-2 loss: 0.399843  [   96/  265]
train() client id: f_00001-11-3 loss: 0.309455  [  128/  265]
train() client id: f_00001-11-4 loss: 0.350867  [  160/  265]
train() client id: f_00001-11-5 loss: 0.364721  [  192/  265]
train() client id: f_00001-11-6 loss: 0.572477  [  224/  265]
train() client id: f_00001-11-7 loss: 0.313545  [  256/  265]
train() client id: f_00002-0-0 loss: 1.194075  [   32/  124]
train() client id: f_00002-0-1 loss: 1.168374  [   64/  124]
train() client id: f_00002-0-2 loss: 1.130063  [   96/  124]
train() client id: f_00002-1-0 loss: 1.180787  [   32/  124]
train() client id: f_00002-1-1 loss: 1.199399  [   64/  124]
train() client id: f_00002-1-2 loss: 1.036692  [   96/  124]
train() client id: f_00002-2-0 loss: 1.151360  [   32/  124]
train() client id: f_00002-2-1 loss: 1.126586  [   64/  124]
train() client id: f_00002-2-2 loss: 1.024103  [   96/  124]
train() client id: f_00002-3-0 loss: 1.204713  [   32/  124]
train() client id: f_00002-3-1 loss: 1.062009  [   64/  124]
train() client id: f_00002-3-2 loss: 0.978523  [   96/  124]
train() client id: f_00002-4-0 loss: 1.055995  [   32/  124]
train() client id: f_00002-4-1 loss: 1.084512  [   64/  124]
train() client id: f_00002-4-2 loss: 1.022801  [   96/  124]
train() client id: f_00002-5-0 loss: 0.895485  [   32/  124]
train() client id: f_00002-5-1 loss: 0.951437  [   64/  124]
train() client id: f_00002-5-2 loss: 1.261478  [   96/  124]
train() client id: f_00002-6-0 loss: 1.022382  [   32/  124]
train() client id: f_00002-6-1 loss: 0.993654  [   64/  124]
train() client id: f_00002-6-2 loss: 1.098217  [   96/  124]
train() client id: f_00002-7-0 loss: 0.927469  [   32/  124]
train() client id: f_00002-7-1 loss: 1.088554  [   64/  124]
train() client id: f_00002-7-2 loss: 0.835990  [   96/  124]
train() client id: f_00002-8-0 loss: 1.026940  [   32/  124]
train() client id: f_00002-8-1 loss: 0.784504  [   64/  124]
train() client id: f_00002-8-2 loss: 1.117327  [   96/  124]
train() client id: f_00002-9-0 loss: 1.019148  [   32/  124]
train() client id: f_00002-9-1 loss: 0.948532  [   64/  124]
train() client id: f_00002-9-2 loss: 0.812633  [   96/  124]
train() client id: f_00002-10-0 loss: 0.951575  [   32/  124]
train() client id: f_00002-10-1 loss: 0.974786  [   64/  124]
train() client id: f_00002-10-2 loss: 0.979379  [   96/  124]
train() client id: f_00002-11-0 loss: 1.039328  [   32/  124]
train() client id: f_00002-11-1 loss: 1.002127  [   64/  124]
train() client id: f_00002-11-2 loss: 0.869725  [   96/  124]
train() client id: f_00003-0-0 loss: 0.683814  [   32/   43]
train() client id: f_00003-1-0 loss: 0.638512  [   32/   43]
train() client id: f_00003-2-0 loss: 0.745983  [   32/   43]
train() client id: f_00003-3-0 loss: 0.576556  [   32/   43]
train() client id: f_00003-4-0 loss: 0.656832  [   32/   43]
train() client id: f_00003-5-0 loss: 0.629802  [   32/   43]
train() client id: f_00003-6-0 loss: 0.731979  [   32/   43]
train() client id: f_00003-7-0 loss: 0.691996  [   32/   43]
train() client id: f_00003-8-0 loss: 0.623133  [   32/   43]
train() client id: f_00003-9-0 loss: 0.660036  [   32/   43]
train() client id: f_00003-10-0 loss: 0.650027  [   32/   43]
train() client id: f_00003-11-0 loss: 0.767610  [   32/   43]
train() client id: f_00004-0-0 loss: 0.701885  [   32/  306]
train() client id: f_00004-0-1 loss: 0.754629  [   64/  306]
train() client id: f_00004-0-2 loss: 0.873604  [   96/  306]
train() client id: f_00004-0-3 loss: 0.845195  [  128/  306]
train() client id: f_00004-0-4 loss: 1.026057  [  160/  306]
train() client id: f_00004-0-5 loss: 0.666843  [  192/  306]
train() client id: f_00004-0-6 loss: 0.826597  [  224/  306]
train() client id: f_00004-0-7 loss: 0.910339  [  256/  306]
train() client id: f_00004-0-8 loss: 0.917113  [  288/  306]
train() client id: f_00004-1-0 loss: 0.884640  [   32/  306]
train() client id: f_00004-1-1 loss: 0.837891  [   64/  306]
train() client id: f_00004-1-2 loss: 0.872487  [   96/  306]
train() client id: f_00004-1-3 loss: 0.907580  [  128/  306]
train() client id: f_00004-1-4 loss: 0.810640  [  160/  306]
train() client id: f_00004-1-5 loss: 0.753303  [  192/  306]
train() client id: f_00004-1-6 loss: 0.847843  [  224/  306]
train() client id: f_00004-1-7 loss: 0.742268  [  256/  306]
train() client id: f_00004-1-8 loss: 0.820186  [  288/  306]
train() client id: f_00004-2-0 loss: 0.800270  [   32/  306]
train() client id: f_00004-2-1 loss: 0.793741  [   64/  306]
train() client id: f_00004-2-2 loss: 0.993333  [   96/  306]
train() client id: f_00004-2-3 loss: 0.712212  [  128/  306]
train() client id: f_00004-2-4 loss: 0.878734  [  160/  306]
train() client id: f_00004-2-5 loss: 0.837345  [  192/  306]
train() client id: f_00004-2-6 loss: 0.776317  [  224/  306]
train() client id: f_00004-2-7 loss: 0.729238  [  256/  306]
train() client id: f_00004-2-8 loss: 0.953314  [  288/  306]
train() client id: f_00004-3-0 loss: 0.946710  [   32/  306]
train() client id: f_00004-3-1 loss: 0.778953  [   64/  306]
train() client id: f_00004-3-2 loss: 0.853202  [   96/  306]
train() client id: f_00004-3-3 loss: 0.811028  [  128/  306]
train() client id: f_00004-3-4 loss: 0.749106  [  160/  306]
train() client id: f_00004-3-5 loss: 0.754407  [  192/  306]
train() client id: f_00004-3-6 loss: 0.896986  [  224/  306]
train() client id: f_00004-3-7 loss: 0.764317  [  256/  306]
train() client id: f_00004-3-8 loss: 0.850786  [  288/  306]
train() client id: f_00004-4-0 loss: 1.022922  [   32/  306]
train() client id: f_00004-4-1 loss: 0.820775  [   64/  306]
train() client id: f_00004-4-2 loss: 0.894064  [   96/  306]
train() client id: f_00004-4-3 loss: 0.844112  [  128/  306]
train() client id: f_00004-4-4 loss: 0.705088  [  160/  306]
train() client id: f_00004-4-5 loss: 0.836558  [  192/  306]
train() client id: f_00004-4-6 loss: 0.736103  [  224/  306]
train() client id: f_00004-4-7 loss: 0.807931  [  256/  306]
train() client id: f_00004-4-8 loss: 0.839084  [  288/  306]
train() client id: f_00004-5-0 loss: 0.811587  [   32/  306]
train() client id: f_00004-5-1 loss: 0.801815  [   64/  306]
train() client id: f_00004-5-2 loss: 0.846457  [   96/  306]
train() client id: f_00004-5-3 loss: 0.916935  [  128/  306]
train() client id: f_00004-5-4 loss: 0.764869  [  160/  306]
train() client id: f_00004-5-5 loss: 0.852651  [  192/  306]
train() client id: f_00004-5-6 loss: 0.811751  [  224/  306]
train() client id: f_00004-5-7 loss: 0.856814  [  256/  306]
train() client id: f_00004-5-8 loss: 0.770373  [  288/  306]
train() client id: f_00004-6-0 loss: 0.622048  [   32/  306]
train() client id: f_00004-6-1 loss: 0.858632  [   64/  306]
train() client id: f_00004-6-2 loss: 0.850952  [   96/  306]
train() client id: f_00004-6-3 loss: 0.856650  [  128/  306]
train() client id: f_00004-6-4 loss: 0.809441  [  160/  306]
train() client id: f_00004-6-5 loss: 0.860965  [  192/  306]
train() client id: f_00004-6-6 loss: 0.920835  [  224/  306]
train() client id: f_00004-6-7 loss: 0.790152  [  256/  306]
train() client id: f_00004-6-8 loss: 0.855706  [  288/  306]
train() client id: f_00004-7-0 loss: 0.817623  [   32/  306]
train() client id: f_00004-7-1 loss: 0.806927  [   64/  306]
train() client id: f_00004-7-2 loss: 0.852866  [   96/  306]
train() client id: f_00004-7-3 loss: 0.814001  [  128/  306]
train() client id: f_00004-7-4 loss: 0.822342  [  160/  306]
train() client id: f_00004-7-5 loss: 0.855260  [  192/  306]
train() client id: f_00004-7-6 loss: 0.780521  [  224/  306]
train() client id: f_00004-7-7 loss: 0.803998  [  256/  306]
train() client id: f_00004-7-8 loss: 0.855999  [  288/  306]
train() client id: f_00004-8-0 loss: 0.881951  [   32/  306]
train() client id: f_00004-8-1 loss: 0.769137  [   64/  306]
train() client id: f_00004-8-2 loss: 0.778608  [   96/  306]
train() client id: f_00004-8-3 loss: 0.961661  [  128/  306]
train() client id: f_00004-8-4 loss: 0.878825  [  160/  306]
train() client id: f_00004-8-5 loss: 0.777324  [  192/  306]
train() client id: f_00004-8-6 loss: 0.769896  [  224/  306]
train() client id: f_00004-8-7 loss: 0.726812  [  256/  306]
train() client id: f_00004-8-8 loss: 0.855963  [  288/  306]
train() client id: f_00004-9-0 loss: 0.831937  [   32/  306]
train() client id: f_00004-9-1 loss: 0.826439  [   64/  306]
train() client id: f_00004-9-2 loss: 0.689761  [   96/  306]
train() client id: f_00004-9-3 loss: 0.922784  [  128/  306]
train() client id: f_00004-9-4 loss: 1.011326  [  160/  306]
train() client id: f_00004-9-5 loss: 0.803654  [  192/  306]
train() client id: f_00004-9-6 loss: 0.819861  [  224/  306]
train() client id: f_00004-9-7 loss: 0.805024  [  256/  306]
train() client id: f_00004-9-8 loss: 0.769430  [  288/  306]
train() client id: f_00004-10-0 loss: 0.820533  [   32/  306]
train() client id: f_00004-10-1 loss: 0.778553  [   64/  306]
train() client id: f_00004-10-2 loss: 0.887177  [   96/  306]
train() client id: f_00004-10-3 loss: 0.793085  [  128/  306]
train() client id: f_00004-10-4 loss: 0.822866  [  160/  306]
train() client id: f_00004-10-5 loss: 0.795348  [  192/  306]
train() client id: f_00004-10-6 loss: 0.963628  [  224/  306]
train() client id: f_00004-10-7 loss: 0.786917  [  256/  306]
train() client id: f_00004-10-8 loss: 0.827680  [  288/  306]
train() client id: f_00004-11-0 loss: 0.933072  [   32/  306]
train() client id: f_00004-11-1 loss: 0.947266  [   64/  306]
train() client id: f_00004-11-2 loss: 0.801834  [   96/  306]
train() client id: f_00004-11-3 loss: 0.866783  [  128/  306]
train() client id: f_00004-11-4 loss: 0.853819  [  160/  306]
train() client id: f_00004-11-5 loss: 0.804066  [  192/  306]
train() client id: f_00004-11-6 loss: 0.761678  [  224/  306]
train() client id: f_00004-11-7 loss: 0.751286  [  256/  306]
train() client id: f_00004-11-8 loss: 0.768791  [  288/  306]
train() client id: f_00005-0-0 loss: 0.925545  [   32/  146]
train() client id: f_00005-0-1 loss: 0.798601  [   64/  146]
train() client id: f_00005-0-2 loss: 0.843851  [   96/  146]
train() client id: f_00005-0-3 loss: 0.698634  [  128/  146]
train() client id: f_00005-1-0 loss: 0.969457  [   32/  146]
train() client id: f_00005-1-1 loss: 0.660423  [   64/  146]
train() client id: f_00005-1-2 loss: 0.717132  [   96/  146]
train() client id: f_00005-1-3 loss: 0.804545  [  128/  146]
train() client id: f_00005-2-0 loss: 0.702088  [   32/  146]
train() client id: f_00005-2-1 loss: 0.739012  [   64/  146]
train() client id: f_00005-2-2 loss: 0.885697  [   96/  146]
train() client id: f_00005-2-3 loss: 0.647042  [  128/  146]
train() client id: f_00005-3-0 loss: 0.723636  [   32/  146]
train() client id: f_00005-3-1 loss: 0.875612  [   64/  146]
train() client id: f_00005-3-2 loss: 0.660069  [   96/  146]
train() client id: f_00005-3-3 loss: 0.816188  [  128/  146]
train() client id: f_00005-4-0 loss: 0.784081  [   32/  146]
train() client id: f_00005-4-1 loss: 1.019967  [   64/  146]
train() client id: f_00005-4-2 loss: 0.718881  [   96/  146]
train() client id: f_00005-4-3 loss: 0.585663  [  128/  146]
train() client id: f_00005-5-0 loss: 0.788899  [   32/  146]
train() client id: f_00005-5-1 loss: 0.842473  [   64/  146]
train() client id: f_00005-5-2 loss: 0.643062  [   96/  146]
train() client id: f_00005-5-3 loss: 0.871311  [  128/  146]
train() client id: f_00005-6-0 loss: 0.770504  [   32/  146]
train() client id: f_00005-6-1 loss: 0.708895  [   64/  146]
train() client id: f_00005-6-2 loss: 0.952995  [   96/  146]
train() client id: f_00005-6-3 loss: 0.704725  [  128/  146]
train() client id: f_00005-7-0 loss: 0.932240  [   32/  146]
train() client id: f_00005-7-1 loss: 0.644697  [   64/  146]
train() client id: f_00005-7-2 loss: 0.850017  [   96/  146]
train() client id: f_00005-7-3 loss: 0.748687  [  128/  146]
train() client id: f_00005-8-0 loss: 0.705119  [   32/  146]
train() client id: f_00005-8-1 loss: 0.902792  [   64/  146]
train() client id: f_00005-8-2 loss: 0.778280  [   96/  146]
train() client id: f_00005-8-3 loss: 0.838269  [  128/  146]
train() client id: f_00005-9-0 loss: 0.807151  [   32/  146]
train() client id: f_00005-9-1 loss: 0.761188  [   64/  146]
train() client id: f_00005-9-2 loss: 0.668240  [   96/  146]
train() client id: f_00005-9-3 loss: 0.676129  [  128/  146]
train() client id: f_00005-10-0 loss: 0.798715  [   32/  146]
train() client id: f_00005-10-1 loss: 0.914365  [   64/  146]
train() client id: f_00005-10-2 loss: 0.617350  [   96/  146]
train() client id: f_00005-10-3 loss: 0.940642  [  128/  146]
train() client id: f_00005-11-0 loss: 0.668681  [   32/  146]
train() client id: f_00005-11-1 loss: 0.852609  [   64/  146]
train() client id: f_00005-11-2 loss: 0.728190  [   96/  146]
train() client id: f_00005-11-3 loss: 0.958290  [  128/  146]
train() client id: f_00006-0-0 loss: 0.492714  [   32/   54]
train() client id: f_00006-1-0 loss: 0.577144  [   32/   54]
train() client id: f_00006-2-0 loss: 0.462137  [   32/   54]
train() client id: f_00006-3-0 loss: 0.566771  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556253  [   32/   54]
train() client id: f_00006-5-0 loss: 0.538972  [   32/   54]
train() client id: f_00006-6-0 loss: 0.504082  [   32/   54]
train() client id: f_00006-7-0 loss: 0.521957  [   32/   54]
train() client id: f_00006-8-0 loss: 0.465040  [   32/   54]
train() client id: f_00006-9-0 loss: 0.571236  [   32/   54]
train() client id: f_00006-10-0 loss: 0.513220  [   32/   54]
train() client id: f_00006-11-0 loss: 0.515814  [   32/   54]
train() client id: f_00007-0-0 loss: 0.606054  [   32/  179]
train() client id: f_00007-0-1 loss: 0.634510  [   64/  179]
train() client id: f_00007-0-2 loss: 0.596334  [   96/  179]
train() client id: f_00007-0-3 loss: 0.445689  [  128/  179]
train() client id: f_00007-0-4 loss: 0.585328  [  160/  179]
train() client id: f_00007-1-0 loss: 0.684405  [   32/  179]
train() client id: f_00007-1-1 loss: 0.668508  [   64/  179]
train() client id: f_00007-1-2 loss: 0.519337  [   96/  179]
train() client id: f_00007-1-3 loss: 0.459770  [  128/  179]
train() client id: f_00007-1-4 loss: 0.546894  [  160/  179]
train() client id: f_00007-2-0 loss: 0.427256  [   32/  179]
train() client id: f_00007-2-1 loss: 0.447896  [   64/  179]
train() client id: f_00007-2-2 loss: 0.548513  [   96/  179]
train() client id: f_00007-2-3 loss: 0.712162  [  128/  179]
train() client id: f_00007-2-4 loss: 0.663234  [  160/  179]
train() client id: f_00007-3-0 loss: 0.567060  [   32/  179]
train() client id: f_00007-3-1 loss: 0.702414  [   64/  179]
train() client id: f_00007-3-2 loss: 0.552868  [   96/  179]
train() client id: f_00007-3-3 loss: 0.589787  [  128/  179]
train() client id: f_00007-3-4 loss: 0.391419  [  160/  179]
train() client id: f_00007-4-0 loss: 0.525069  [   32/  179]
train() client id: f_00007-4-1 loss: 0.410653  [   64/  179]
train() client id: f_00007-4-2 loss: 0.675400  [   96/  179]
train() client id: f_00007-4-3 loss: 0.484207  [  128/  179]
train() client id: f_00007-4-4 loss: 0.600199  [  160/  179]
train() client id: f_00007-5-0 loss: 0.496815  [   32/  179]
train() client id: f_00007-5-1 loss: 0.620309  [   64/  179]
train() client id: f_00007-5-2 loss: 0.602490  [   96/  179]
train() client id: f_00007-5-3 loss: 0.420396  [  128/  179]
train() client id: f_00007-5-4 loss: 0.611641  [  160/  179]
train() client id: f_00007-6-0 loss: 0.415595  [   32/  179]
train() client id: f_00007-6-1 loss: 0.728161  [   64/  179]
train() client id: f_00007-6-2 loss: 0.477642  [   96/  179]
train() client id: f_00007-6-3 loss: 0.667788  [  128/  179]
train() client id: f_00007-6-4 loss: 0.429825  [  160/  179]
train() client id: f_00007-7-0 loss: 0.409911  [   32/  179]
train() client id: f_00007-7-1 loss: 0.559618  [   64/  179]
train() client id: f_00007-7-2 loss: 0.631227  [   96/  179]
train() client id: f_00007-7-3 loss: 0.551178  [  128/  179]
train() client id: f_00007-7-4 loss: 0.563771  [  160/  179]
train() client id: f_00007-8-0 loss: 0.531101  [   32/  179]
train() client id: f_00007-8-1 loss: 0.490473  [   64/  179]
train() client id: f_00007-8-2 loss: 0.644497  [   96/  179]
train() client id: f_00007-8-3 loss: 0.488284  [  128/  179]
train() client id: f_00007-8-4 loss: 0.525772  [  160/  179]
train() client id: f_00007-9-0 loss: 0.585910  [   32/  179]
train() client id: f_00007-9-1 loss: 0.453538  [   64/  179]
train() client id: f_00007-9-2 loss: 0.457199  [   96/  179]
train() client id: f_00007-9-3 loss: 0.497266  [  128/  179]
train() client id: f_00007-9-4 loss: 0.579371  [  160/  179]
train() client id: f_00007-10-0 loss: 0.730139  [   32/  179]
train() client id: f_00007-10-1 loss: 0.405308  [   64/  179]
train() client id: f_00007-10-2 loss: 0.661330  [   96/  179]
train() client id: f_00007-10-3 loss: 0.358807  [  128/  179]
train() client id: f_00007-10-4 loss: 0.465496  [  160/  179]
train() client id: f_00007-11-0 loss: 0.500857  [   32/  179]
train() client id: f_00007-11-1 loss: 0.686142  [   64/  179]
train() client id: f_00007-11-2 loss: 0.479616  [   96/  179]
train() client id: f_00007-11-3 loss: 0.369357  [  128/  179]
train() client id: f_00007-11-4 loss: 0.462404  [  160/  179]
train() client id: f_00008-0-0 loss: 0.716996  [   32/  130]
train() client id: f_00008-0-1 loss: 0.714650  [   64/  130]
train() client id: f_00008-0-2 loss: 0.717946  [   96/  130]
train() client id: f_00008-0-3 loss: 0.827943  [  128/  130]
train() client id: f_00008-1-0 loss: 0.758941  [   32/  130]
train() client id: f_00008-1-1 loss: 0.689825  [   64/  130]
train() client id: f_00008-1-2 loss: 0.814868  [   96/  130]
train() client id: f_00008-1-3 loss: 0.683581  [  128/  130]
train() client id: f_00008-2-0 loss: 0.909985  [   32/  130]
train() client id: f_00008-2-1 loss: 0.705273  [   64/  130]
train() client id: f_00008-2-2 loss: 0.701552  [   96/  130]
train() client id: f_00008-2-3 loss: 0.669087  [  128/  130]
train() client id: f_00008-3-0 loss: 0.616572  [   32/  130]
train() client id: f_00008-3-1 loss: 0.908967  [   64/  130]
train() client id: f_00008-3-2 loss: 0.718586  [   96/  130]
train() client id: f_00008-3-3 loss: 0.698842  [  128/  130]
train() client id: f_00008-4-0 loss: 0.717883  [   32/  130]
train() client id: f_00008-4-1 loss: 0.825712  [   64/  130]
train() client id: f_00008-4-2 loss: 0.698555  [   96/  130]
train() client id: f_00008-4-3 loss: 0.744030  [  128/  130]
train() client id: f_00008-5-0 loss: 0.630248  [   32/  130]
train() client id: f_00008-5-1 loss: 0.718590  [   64/  130]
train() client id: f_00008-5-2 loss: 0.784245  [   96/  130]
train() client id: f_00008-5-3 loss: 0.787052  [  128/  130]
train() client id: f_00008-6-0 loss: 0.715053  [   32/  130]
train() client id: f_00008-6-1 loss: 0.766127  [   64/  130]
train() client id: f_00008-6-2 loss: 0.882988  [   96/  130]
train() client id: f_00008-6-3 loss: 0.623808  [  128/  130]
train() client id: f_00008-7-0 loss: 0.799807  [   32/  130]
train() client id: f_00008-7-1 loss: 0.856324  [   64/  130]
train() client id: f_00008-7-2 loss: 0.700069  [   96/  130]
train() client id: f_00008-7-3 loss: 0.622660  [  128/  130]
train() client id: f_00008-8-0 loss: 0.786150  [   32/  130]
train() client id: f_00008-8-1 loss: 0.802764  [   64/  130]
train() client id: f_00008-8-2 loss: 0.647003  [   96/  130]
train() client id: f_00008-8-3 loss: 0.711086  [  128/  130]
train() client id: f_00008-9-0 loss: 0.716049  [   32/  130]
train() client id: f_00008-9-1 loss: 0.873451  [   64/  130]
train() client id: f_00008-9-2 loss: 0.711605  [   96/  130]
train() client id: f_00008-9-3 loss: 0.684967  [  128/  130]
train() client id: f_00008-10-0 loss: 0.751141  [   32/  130]
train() client id: f_00008-10-1 loss: 0.759074  [   64/  130]
train() client id: f_00008-10-2 loss: 0.709464  [   96/  130]
train() client id: f_00008-10-3 loss: 0.751244  [  128/  130]
train() client id: f_00008-11-0 loss: 0.703467  [   32/  130]
train() client id: f_00008-11-1 loss: 0.789920  [   64/  130]
train() client id: f_00008-11-2 loss: 0.680093  [   96/  130]
train() client id: f_00008-11-3 loss: 0.778150  [  128/  130]
train() client id: f_00009-0-0 loss: 1.027945  [   32/  118]
train() client id: f_00009-0-1 loss: 1.039881  [   64/  118]
train() client id: f_00009-0-2 loss: 0.999154  [   96/  118]
train() client id: f_00009-1-0 loss: 0.955072  [   32/  118]
train() client id: f_00009-1-1 loss: 0.950952  [   64/  118]
train() client id: f_00009-1-2 loss: 0.946716  [   96/  118]
train() client id: f_00009-2-0 loss: 1.099738  [   32/  118]
train() client id: f_00009-2-1 loss: 0.855258  [   64/  118]
train() client id: f_00009-2-2 loss: 0.865917  [   96/  118]
train() client id: f_00009-3-0 loss: 0.861370  [   32/  118]
train() client id: f_00009-3-1 loss: 0.847205  [   64/  118]
train() client id: f_00009-3-2 loss: 0.889714  [   96/  118]
train() client id: f_00009-4-0 loss: 0.848270  [   32/  118]
train() client id: f_00009-4-1 loss: 0.938141  [   64/  118]
train() client id: f_00009-4-2 loss: 0.834121  [   96/  118]
train() client id: f_00009-5-0 loss: 0.885234  [   32/  118]
train() client id: f_00009-5-1 loss: 0.793259  [   64/  118]
train() client id: f_00009-5-2 loss: 0.951649  [   96/  118]
train() client id: f_00009-6-0 loss: 0.935008  [   32/  118]
train() client id: f_00009-6-1 loss: 0.858406  [   64/  118]
train() client id: f_00009-6-2 loss: 0.765231  [   96/  118]
train() client id: f_00009-7-0 loss: 0.980324  [   32/  118]
train() client id: f_00009-7-1 loss: 0.933116  [   64/  118]
train() client id: f_00009-7-2 loss: 0.618046  [   96/  118]
train() client id: f_00009-8-0 loss: 0.733060  [   32/  118]
train() client id: f_00009-8-1 loss: 1.019789  [   64/  118]
train() client id: f_00009-8-2 loss: 0.715513  [   96/  118]
train() client id: f_00009-9-0 loss: 0.773411  [   32/  118]
train() client id: f_00009-9-1 loss: 0.999622  [   64/  118]
train() client id: f_00009-9-2 loss: 0.681326  [   96/  118]
train() client id: f_00009-10-0 loss: 0.874074  [   32/  118]
train() client id: f_00009-10-1 loss: 0.876908  [   64/  118]
train() client id: f_00009-10-2 loss: 0.765767  [   96/  118]
train() client id: f_00009-11-0 loss: 0.800324  [   32/  118]
train() client id: f_00009-11-1 loss: 0.937655  [   64/  118]
train() client id: f_00009-11-2 loss: 0.804682  [   96/  118]
At round 22 accuracy: 0.636604774535809
At round 22 training accuracy: 0.5828303152246814
At round 22 training loss: 0.8325733108454747
update_location
xs = [ -3.9056584    4.20031788 130.00902392  18.81129433   0.97929623
   3.95640986 -92.44319194 -71.32485185 114.66397685 -57.06087855]
ys = [122.5879595  105.55583871   1.32061395 -92.45517586  84.35018685
  67.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [158.24999836 145.46366473 164.02466376 137.48390574 130.82779919
 120.89008322 136.20805511 122.83285695 153.15514213 115.2039744 ]
dists_bs = [178.94215746 192.18544551 350.99150105 330.29545698 197.95479617
 208.60058224 195.87307526 202.69778195 329.6812663  207.72609273]
uav_gains = [3.16755799e-11 3.91571505e-11 2.89252519e-11 4.51054531e-11
 5.10714755e-11 6.22299652e-11 4.61709278e-11 5.97976208e-11
 3.44011166e-11 7.01971242e-11]
bs_gains = [5.44124914e-11 4.45529740e-11 8.25018342e-12 9.78062943e-12
 4.10118354e-11 3.54169077e-11 4.22439780e-11 3.83810539e-11
 9.83173423e-12 3.58359679e-11]
Round 23
-------------------------------
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.95652636 16.54261616  7.83621819  2.81059366 19.08131306  9.18961195
  3.4902559  11.21434588  8.261358    7.45728421]
obj_prev = 93.84012336038582
eta_min = 2.1552750513700054e-12	eta_max = 0.9232017387246043
af = 19.826056636829264	bf = 1.599357197144509	zeta = 21.808662300512193	eta = 0.909090909090909
af = 19.826056636829264	bf = 1.599357197144509	zeta = 38.37880442012573	eta = 0.5165886987983539
af = 19.826056636829264	bf = 1.599357197144509	zeta = 30.397782640654498	eta = 0.6522204882902731
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.962231155820284	eta = 0.6845486637463356
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889857955433257	eta = 0.686263555446129
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889660323483184	eta = 0.6862682501224668
eta = 0.6862682501224668
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.03106678 0.06533886 0.03057366 0.01060215 0.07544789 0.03599801
 0.01331433 0.04413455 0.03205303 0.02909429]
ene_total = [2.52460799 4.70588087 2.50342372 1.16220666 5.36855605 2.83112977
 1.33541705 3.30578544 2.76961187 2.3830409 ]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 0 obj = 5.126260825771548
eta = 0.6862682501224668
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
eta_min = 0.686268250122481	eta_max = 0.6862682501224266
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 0.026043258509324534	eta = 0.9090909090909091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 17.617751045101738	eta = 0.0013438542463973541
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7582687679572628	eta = 0.01346534158224146
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245653152193903	eta = 0.013728496882659091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245608478782077	eta = 0.013728532445265985
eta = 0.013728532445265985
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.59038678e-04 1.43473385e-03 1.52966329e-04 6.12343236e-06
 2.22458099e-03 2.44815942e-04 1.21042015e-05 4.32137624e-04
 2.04770398e-04 1.29108736e-04]
ene_total = [0.16678716 0.18136157 0.17007523 0.15155948 0.20030673 0.16453951
 0.15094588 0.151879   0.22536052 0.16174577]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 1 obj = 5.126260825771778
eta = 0.686268250122481
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
Done!
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.35911095e-05 1.22609325e-04 1.30721794e-05 5.23295595e-07
 1.90107992e-04 2.09214532e-05 1.03439949e-06 3.69295686e-05
 1.74992457e-05 1.10333598e-05]
ene_total = [0.00795626 0.00749753 0.00812153 0.00735642 0.00769545 0.00776863
 0.00732114 0.00698233 0.01075964 0.00773874]
At round 23 energy consumption: 0.07919768178596374
At round 23 eta: 0.686268250122481
At round 23 a_n: 20.30404837596008
At round 23 local rounds: 12.328087095105714
At round 23 global rounds: 64.71786290003097
gradient difference: 0.45529064536094666
train() client id: f_00000-0-0 loss: 1.475387  [   32/  126]
train() client id: f_00000-0-1 loss: 1.144425  [   64/  126]
train() client id: f_00000-0-2 loss: 1.235837  [   96/  126]
train() client id: f_00000-1-0 loss: 0.985383  [   32/  126]
train() client id: f_00000-1-1 loss: 1.450453  [   64/  126]
train() client id: f_00000-1-2 loss: 1.015435  [   96/  126]
train() client id: f_00000-2-0 loss: 1.001085  [   32/  126]
train() client id: f_00000-2-1 loss: 1.007859  [   64/  126]
train() client id: f_00000-2-2 loss: 1.079111  [   96/  126]
train() client id: f_00000-3-0 loss: 1.045215  [   32/  126]
train() client id: f_00000-3-1 loss: 0.941978  [   64/  126]
train() client id: f_00000-3-2 loss: 0.887292  [   96/  126]
train() client id: f_00000-4-0 loss: 0.929414  [   32/  126]
train() client id: f_00000-4-1 loss: 0.884886  [   64/  126]
train() client id: f_00000-4-2 loss: 0.860966  [   96/  126]
train() client id: f_00000-5-0 loss: 0.889627  [   32/  126]
train() client id: f_00000-5-1 loss: 0.783938  [   64/  126]
train() client id: f_00000-5-2 loss: 0.830618  [   96/  126]
train() client id: f_00000-6-0 loss: 0.757959  [   32/  126]
train() client id: f_00000-6-1 loss: 0.822865  [   64/  126]
train() client id: f_00000-6-2 loss: 0.820679  [   96/  126]
train() client id: f_00000-7-0 loss: 0.902504  [   32/  126]
train() client id: f_00000-7-1 loss: 0.770026  [   64/  126]
train() client id: f_00000-7-2 loss: 0.730033  [   96/  126]
train() client id: f_00000-8-0 loss: 0.785016  [   32/  126]
train() client id: f_00000-8-1 loss: 0.709999  [   64/  126]
train() client id: f_00000-8-2 loss: 0.745885  [   96/  126]
train() client id: f_00000-9-0 loss: 0.838173  [   32/  126]
train() client id: f_00000-9-1 loss: 0.688236  [   64/  126]
train() client id: f_00000-9-2 loss: 0.763388  [   96/  126]
train() client id: f_00000-10-0 loss: 0.738348  [   32/  126]
train() client id: f_00000-10-1 loss: 0.760254  [   64/  126]
train() client id: f_00000-10-2 loss: 0.654733  [   96/  126]
train() client id: f_00000-11-0 loss: 0.665354  [   32/  126]
train() client id: f_00000-11-1 loss: 0.805435  [   64/  126]
train() client id: f_00000-11-2 loss: 0.753092  [   96/  126]
train() client id: f_00001-0-0 loss: 0.450992  [   32/  265]
train() client id: f_00001-0-1 loss: 0.583954  [   64/  265]
train() client id: f_00001-0-2 loss: 0.587455  [   96/  265]
train() client id: f_00001-0-3 loss: 0.621502  [  128/  265]
train() client id: f_00001-0-4 loss: 0.459142  [  160/  265]
train() client id: f_00001-0-5 loss: 0.459991  [  192/  265]
train() client id: f_00001-0-6 loss: 0.631501  [  224/  265]
train() client id: f_00001-0-7 loss: 0.596459  [  256/  265]
train() client id: f_00001-1-0 loss: 0.478571  [   32/  265]
train() client id: f_00001-1-1 loss: 0.572905  [   64/  265]
train() client id: f_00001-1-2 loss: 0.503140  [   96/  265]
train() client id: f_00001-1-3 loss: 0.582780  [  128/  265]
train() client id: f_00001-1-4 loss: 0.446097  [  160/  265]
train() client id: f_00001-1-5 loss: 0.696157  [  192/  265]
train() client id: f_00001-1-6 loss: 0.507734  [  224/  265]
train() client id: f_00001-1-7 loss: 0.558201  [  256/  265]
train() client id: f_00001-2-0 loss: 0.554391  [   32/  265]
train() client id: f_00001-2-1 loss: 0.545996  [   64/  265]
train() client id: f_00001-2-2 loss: 0.476943  [   96/  265]
train() client id: f_00001-2-3 loss: 0.568522  [  128/  265]
train() client id: f_00001-2-4 loss: 0.491506  [  160/  265]
train() client id: f_00001-2-5 loss: 0.547890  [  192/  265]
train() client id: f_00001-2-6 loss: 0.607504  [  224/  265]
train() client id: f_00001-2-7 loss: 0.465775  [  256/  265]
train() client id: f_00001-3-0 loss: 0.769295  [   32/  265]
train() client id: f_00001-3-1 loss: 0.558794  [   64/  265]
train() client id: f_00001-3-2 loss: 0.439946  [   96/  265]
train() client id: f_00001-3-3 loss: 0.492531  [  128/  265]
train() client id: f_00001-3-4 loss: 0.510587  [  160/  265]
train() client id: f_00001-3-5 loss: 0.437444  [  192/  265]
train() client id: f_00001-3-6 loss: 0.509570  [  224/  265]
train() client id: f_00001-3-7 loss: 0.529855  [  256/  265]
train() client id: f_00001-4-0 loss: 0.422080  [   32/  265]
train() client id: f_00001-4-1 loss: 0.666330  [   64/  265]
train() client id: f_00001-4-2 loss: 0.575230  [   96/  265]
train() client id: f_00001-4-3 loss: 0.441758  [  128/  265]
train() client id: f_00001-4-4 loss: 0.474598  [  160/  265]
train() client id: f_00001-4-5 loss: 0.474218  [  192/  265]
train() client id: f_00001-4-6 loss: 0.534786  [  224/  265]
train() client id: f_00001-4-7 loss: 0.582418  [  256/  265]
train() client id: f_00001-5-0 loss: 0.510176  [   32/  265]
train() client id: f_00001-5-1 loss: 0.497418  [   64/  265]
train() client id: f_00001-5-2 loss: 0.565591  [   96/  265]
train() client id: f_00001-5-3 loss: 0.641638  [  128/  265]
train() client id: f_00001-5-4 loss: 0.476684  [  160/  265]
train() client id: f_00001-5-5 loss: 0.519760  [  192/  265]
train() client id: f_00001-5-6 loss: 0.476450  [  224/  265]
train() client id: f_00001-5-7 loss: 0.479975  [  256/  265]
train() client id: f_00001-6-0 loss: 0.448436  [   32/  265]
train() client id: f_00001-6-1 loss: 0.521248  [   64/  265]
train() client id: f_00001-6-2 loss: 0.531042  [   96/  265]
train() client id: f_00001-6-3 loss: 0.416799  [  128/  265]
train() client id: f_00001-6-4 loss: 0.618218  [  160/  265]
train() client id: f_00001-6-5 loss: 0.601895  [  192/  265]
train() client id: f_00001-6-6 loss: 0.456893  [  224/  265]
train() client id: f_00001-6-7 loss: 0.495251  [  256/  265]
train() client id: f_00001-7-0 loss: 0.526025  [   32/  265]
train() client id: f_00001-7-1 loss: 0.414590  [   64/  265]
train() client id: f_00001-7-2 loss: 0.609701  [   96/  265]
train() client id: f_00001-7-3 loss: 0.444998  [  128/  265]
train() client id: f_00001-7-4 loss: 0.429982  [  160/  265]
train() client id: f_00001-7-5 loss: 0.740263  [  192/  265]
train() client id: f_00001-7-6 loss: 0.512169  [  224/  265]
train() client id: f_00001-7-7 loss: 0.530477  [  256/  265]
train() client id: f_00001-8-0 loss: 0.575800  [   32/  265]
train() client id: f_00001-8-1 loss: 0.496973  [   64/  265]
train() client id: f_00001-8-2 loss: 0.490572  [   96/  265]
train() client id: f_00001-8-3 loss: 0.513711  [  128/  265]
train() client id: f_00001-8-4 loss: 0.516538  [  160/  265]
train() client id: f_00001-8-5 loss: 0.463307  [  192/  265]
train() client id: f_00001-8-6 loss: 0.555445  [  224/  265]
train() client id: f_00001-8-7 loss: 0.585404  [  256/  265]
train() client id: f_00001-9-0 loss: 0.519405  [   32/  265]
train() client id: f_00001-9-1 loss: 0.642411  [   64/  265]
train() client id: f_00001-9-2 loss: 0.637548  [   96/  265]
train() client id: f_00001-9-3 loss: 0.589206  [  128/  265]
train() client id: f_00001-9-4 loss: 0.411988  [  160/  265]
train() client id: f_00001-9-5 loss: 0.428026  [  192/  265]
train() client id: f_00001-9-6 loss: 0.481471  [  224/  265]
train() client id: f_00001-9-7 loss: 0.496792  [  256/  265]
train() client id: f_00001-10-0 loss: 0.569181  [   32/  265]
train() client id: f_00001-10-1 loss: 0.629133  [   64/  265]
train() client id: f_00001-10-2 loss: 0.606857  [   96/  265]
train() client id: f_00001-10-3 loss: 0.433570  [  128/  265]
train() client id: f_00001-10-4 loss: 0.423980  [  160/  265]
train() client id: f_00001-10-5 loss: 0.548915  [  192/  265]
train() client id: f_00001-10-6 loss: 0.493681  [  224/  265]
train() client id: f_00001-10-7 loss: 0.500918  [  256/  265]
train() client id: f_00001-11-0 loss: 0.474606  [   32/  265]
train() client id: f_00001-11-1 loss: 0.471294  [   64/  265]
train() client id: f_00001-11-2 loss: 0.514089  [   96/  265]
train() client id: f_00001-11-3 loss: 0.432359  [  128/  265]
train() client id: f_00001-11-4 loss: 0.571282  [  160/  265]
train() client id: f_00001-11-5 loss: 0.607735  [  192/  265]
train() client id: f_00001-11-6 loss: 0.719561  [  224/  265]
train() client id: f_00001-11-7 loss: 0.416371  [  256/  265]
train() client id: f_00002-0-0 loss: 1.191845  [   32/  124]
train() client id: f_00002-0-1 loss: 1.114587  [   64/  124]
train() client id: f_00002-0-2 loss: 1.272739  [   96/  124]
train() client id: f_00002-1-0 loss: 1.301571  [   32/  124]
train() client id: f_00002-1-1 loss: 1.209785  [   64/  124]
train() client id: f_00002-1-2 loss: 1.137851  [   96/  124]
train() client id: f_00002-2-0 loss: 0.977228  [   32/  124]
train() client id: f_00002-2-1 loss: 1.191263  [   64/  124]
train() client id: f_00002-2-2 loss: 1.276456  [   96/  124]
train() client id: f_00002-3-0 loss: 1.194766  [   32/  124]
train() client id: f_00002-3-1 loss: 1.139406  [   64/  124]
train() client id: f_00002-3-2 loss: 1.089055  [   96/  124]
train() client id: f_00002-4-0 loss: 1.096667  [   32/  124]
train() client id: f_00002-4-1 loss: 1.200136  [   64/  124]
train() client id: f_00002-4-2 loss: 1.073646  [   96/  124]
train() client id: f_00002-5-0 loss: 1.122871  [   32/  124]
train() client id: f_00002-5-1 loss: 1.058588  [   64/  124]
train() client id: f_00002-5-2 loss: 1.118990  [   96/  124]
train() client id: f_00002-6-0 loss: 1.022307  [   32/  124]
train() client id: f_00002-6-1 loss: 1.068965  [   64/  124]
train() client id: f_00002-6-2 loss: 1.090100  [   96/  124]
train() client id: f_00002-7-0 loss: 1.066449  [   32/  124]
train() client id: f_00002-7-1 loss: 1.319028  [   64/  124]
train() client id: f_00002-7-2 loss: 1.057461  [   96/  124]
train() client id: f_00002-8-0 loss: 1.033216  [   32/  124]
train() client id: f_00002-8-1 loss: 1.129326  [   64/  124]
train() client id: f_00002-8-2 loss: 1.155244  [   96/  124]
train() client id: f_00002-9-0 loss: 0.913843  [   32/  124]
train() client id: f_00002-9-1 loss: 1.108661  [   64/  124]
train() client id: f_00002-9-2 loss: 1.215308  [   96/  124]
train() client id: f_00002-10-0 loss: 0.975013  [   32/  124]
train() client id: f_00002-10-1 loss: 1.117269  [   64/  124]
train() client id: f_00002-10-2 loss: 1.119053  [   96/  124]
train() client id: f_00002-11-0 loss: 1.068472  [   32/  124]
train() client id: f_00002-11-1 loss: 1.006877  [   64/  124]
train() client id: f_00002-11-2 loss: 1.016450  [   96/  124]
train() client id: f_00003-0-0 loss: 0.522891  [   32/   43]
train() client id: f_00003-1-0 loss: 0.613599  [   32/   43]
train() client id: f_00003-2-0 loss: 0.638790  [   32/   43]
train() client id: f_00003-3-0 loss: 0.488180  [   32/   43]
train() client id: f_00003-4-0 loss: 0.668484  [   32/   43]
train() client id: f_00003-5-0 loss: 0.695192  [   32/   43]
train() client id: f_00003-6-0 loss: 0.936609  [   32/   43]
train() client id: f_00003-7-0 loss: 0.702368  [   32/   43]
train() client id: f_00003-8-0 loss: 0.640143  [   32/   43]
train() client id: f_00003-9-0 loss: 0.697129  [   32/   43]
train() client id: f_00003-10-0 loss: 0.676289  [   32/   43]
train() client id: f_00003-11-0 loss: 0.645567  [   32/   43]
train() client id: f_00004-0-0 loss: 0.934630  [   32/  306]
train() client id: f_00004-0-1 loss: 0.997586  [   64/  306]
train() client id: f_00004-0-2 loss: 0.868451  [   96/  306]
train() client id: f_00004-0-3 loss: 0.928138  [  128/  306]
train() client id: f_00004-0-4 loss: 1.002225  [  160/  306]
train() client id: f_00004-0-5 loss: 1.006091  [  192/  306]
train() client id: f_00004-0-6 loss: 0.885037  [  224/  306]
train() client id: f_00004-0-7 loss: 0.942984  [  256/  306]
train() client id: f_00004-0-8 loss: 0.870707  [  288/  306]
train() client id: f_00004-1-0 loss: 1.115580  [   32/  306]
train() client id: f_00004-1-1 loss: 0.955871  [   64/  306]
train() client id: f_00004-1-2 loss: 0.867869  [   96/  306]
train() client id: f_00004-1-3 loss: 1.044205  [  128/  306]
train() client id: f_00004-1-4 loss: 0.793564  [  160/  306]
train() client id: f_00004-1-5 loss: 0.923418  [  192/  306]
train() client id: f_00004-1-6 loss: 0.845003  [  224/  306]
train() client id: f_00004-1-7 loss: 1.003778  [  256/  306]
train() client id: f_00004-1-8 loss: 0.895629  [  288/  306]
train() client id: f_00004-2-0 loss: 1.108549  [   32/  306]
train() client id: f_00004-2-1 loss: 1.035348  [   64/  306]
train() client id: f_00004-2-2 loss: 0.889011  [   96/  306]
train() client id: f_00004-2-3 loss: 0.854200  [  128/  306]
train() client id: f_00004-2-4 loss: 1.003947  [  160/  306]
train() client id: f_00004-2-5 loss: 0.965249  [  192/  306]
train() client id: f_00004-2-6 loss: 0.881281  [  224/  306]
train() client id: f_00004-2-7 loss: 1.003337  [  256/  306]
train() client id: f_00004-2-8 loss: 0.812266  [  288/  306]
train() client id: f_00004-3-0 loss: 0.889510  [   32/  306]
train() client id: f_00004-3-1 loss: 1.008729  [   64/  306]
train() client id: f_00004-3-2 loss: 0.941955  [   96/  306]
train() client id: f_00004-3-3 loss: 0.896080  [  128/  306]
train() client id: f_00004-3-4 loss: 0.906563  [  160/  306]
train() client id: f_00004-3-5 loss: 0.933520  [  192/  306]
train() client id: f_00004-3-6 loss: 0.864078  [  224/  306]
train() client id: f_00004-3-7 loss: 0.885822  [  256/  306]
train() client id: f_00004-3-8 loss: 0.994087  [  288/  306]
train() client id: f_00004-4-0 loss: 0.931424  [   32/  306]
train() client id: f_00004-4-1 loss: 1.012615  [   64/  306]
train() client id: f_00004-4-2 loss: 0.939067  [   96/  306]
train() client id: f_00004-4-3 loss: 0.855679  [  128/  306]
train() client id: f_00004-4-4 loss: 0.945880  [  160/  306]
train() client id: f_00004-4-5 loss: 0.969734  [  192/  306]
train() client id: f_00004-4-6 loss: 0.886163  [  224/  306]
train() client id: f_00004-4-7 loss: 1.065093  [  256/  306]
train() client id: f_00004-4-8 loss: 0.862129  [  288/  306]
train() client id: f_00004-5-0 loss: 1.030065  [   32/  306]
train() client id: f_00004-5-1 loss: 1.036600  [   64/  306]
train() client id: f_00004-5-2 loss: 0.792473  [   96/  306]
train() client id: f_00004-5-3 loss: 0.932587  [  128/  306]
train() client id: f_00004-5-4 loss: 0.913680  [  160/  306]
train() client id: f_00004-5-5 loss: 0.867724  [  192/  306]
train() client id: f_00004-5-6 loss: 0.888205  [  224/  306]
train() client id: f_00004-5-7 loss: 0.836436  [  256/  306]
train() client id: f_00004-5-8 loss: 1.061379  [  288/  306]
train() client id: f_00004-6-0 loss: 0.878819  [   32/  306]
train() client id: f_00004-6-1 loss: 0.815627  [   64/  306]
train() client id: f_00004-6-2 loss: 0.853069  [   96/  306]
train() client id: f_00004-6-3 loss: 0.931257  [  128/  306]
train() client id: f_00004-6-4 loss: 1.030696  [  160/  306]
train() client id: f_00004-6-5 loss: 0.959398  [  192/  306]
train() client id: f_00004-6-6 loss: 0.974105  [  224/  306]
train() client id: f_00004-6-7 loss: 1.015469  [  256/  306]
train() client id: f_00004-6-8 loss: 0.886809  [  288/  306]
train() client id: f_00004-7-0 loss: 0.963175  [   32/  306]
train() client id: f_00004-7-1 loss: 0.969386  [   64/  306]
train() client id: f_00004-7-2 loss: 0.900470  [   96/  306]
train() client id: f_00004-7-3 loss: 0.828982  [  128/  306]
train() client id: f_00004-7-4 loss: 0.868272  [  160/  306]
train() client id: f_00004-7-5 loss: 0.862334  [  192/  306]
train() client id: f_00004-7-6 loss: 0.970155  [  224/  306]
train() client id: f_00004-7-7 loss: 0.983422  [  256/  306]
train() client id: f_00004-7-8 loss: 1.011075  [  288/  306]
train() client id: f_00004-8-0 loss: 0.847194  [   32/  306]
train() client id: f_00004-8-1 loss: 0.966683  [   64/  306]
train() client id: f_00004-8-2 loss: 0.926875  [   96/  306]
train() client id: f_00004-8-3 loss: 0.920064  [  128/  306]
train() client id: f_00004-8-4 loss: 0.896482  [  160/  306]
train() client id: f_00004-8-5 loss: 1.033920  [  192/  306]
train() client id: f_00004-8-6 loss: 0.919800  [  224/  306]
train() client id: f_00004-8-7 loss: 0.906914  [  256/  306]
train() client id: f_00004-8-8 loss: 0.901039  [  288/  306]
train() client id: f_00004-9-0 loss: 0.885084  [   32/  306]
train() client id: f_00004-9-1 loss: 0.907478  [   64/  306]
train() client id: f_00004-9-2 loss: 0.936353  [   96/  306]
train() client id: f_00004-9-3 loss: 1.038552  [  128/  306]
train() client id: f_00004-9-4 loss: 0.826371  [  160/  306]
train() client id: f_00004-9-5 loss: 0.935723  [  192/  306]
train() client id: f_00004-9-6 loss: 0.963133  [  224/  306]
train() client id: f_00004-9-7 loss: 0.867784  [  256/  306]
train() client id: f_00004-9-8 loss: 0.967559  [  288/  306]
train() client id: f_00004-10-0 loss: 1.010560  [   32/  306]
train() client id: f_00004-10-1 loss: 0.927883  [   64/  306]
train() client id: f_00004-10-2 loss: 0.826808  [   96/  306]
train() client id: f_00004-10-3 loss: 0.806892  [  128/  306]
train() client id: f_00004-10-4 loss: 0.957678  [  160/  306]
train() client id: f_00004-10-5 loss: 0.927966  [  192/  306]
train() client id: f_00004-10-6 loss: 0.836933  [  224/  306]
train() client id: f_00004-10-7 loss: 1.019385  [  256/  306]
train() client id: f_00004-10-8 loss: 1.016156  [  288/  306]
train() client id: f_00004-11-0 loss: 1.000182  [   32/  306]
train() client id: f_00004-11-1 loss: 1.002879  [   64/  306]
train() client id: f_00004-11-2 loss: 0.921874  [   96/  306]
train() client id: f_00004-11-3 loss: 0.869030  [  128/  306]
train() client id: f_00004-11-4 loss: 0.917123  [  160/  306]
train() client id: f_00004-11-5 loss: 0.915014  [  192/  306]
train() client id: f_00004-11-6 loss: 0.913192  [  224/  306]
train() client id: f_00004-11-7 loss: 0.947381  [  256/  306]
train() client id: f_00004-11-8 loss: 0.891092  [  288/  306]
train() client id: f_00005-0-0 loss: 0.568060  [   32/  146]
train() client id: f_00005-0-1 loss: 0.406157  [   64/  146]
train() client id: f_00005-0-2 loss: 0.453620  [   96/  146]
train() client id: f_00005-0-3 loss: 0.603171  [  128/  146]
train() client id: f_00005-1-0 loss: 0.594855  [   32/  146]
train() client id: f_00005-1-1 loss: 0.494652  [   64/  146]
train() client id: f_00005-1-2 loss: 0.386531  [   96/  146]
train() client id: f_00005-1-3 loss: 0.561729  [  128/  146]
train() client id: f_00005-2-0 loss: 0.298475  [   32/  146]
train() client id: f_00005-2-1 loss: 0.553466  [   64/  146]
train() client id: f_00005-2-2 loss: 0.492345  [   96/  146]
train() client id: f_00005-2-3 loss: 0.437617  [  128/  146]
train() client id: f_00005-3-0 loss: 0.379916  [   32/  146]
train() client id: f_00005-3-1 loss: 0.567413  [   64/  146]
train() client id: f_00005-3-2 loss: 0.521129  [   96/  146]
train() client id: f_00005-3-3 loss: 0.512365  [  128/  146]
train() client id: f_00005-4-0 loss: 0.499782  [   32/  146]
train() client id: f_00005-4-1 loss: 0.597313  [   64/  146]
train() client id: f_00005-4-2 loss: 0.455763  [   96/  146]
train() client id: f_00005-4-3 loss: 0.544434  [  128/  146]
train() client id: f_00005-5-0 loss: 0.631225  [   32/  146]
train() client id: f_00005-5-1 loss: 0.434806  [   64/  146]
train() client id: f_00005-5-2 loss: 0.327296  [   96/  146]
train() client id: f_00005-5-3 loss: 0.396976  [  128/  146]
train() client id: f_00005-6-0 loss: 0.260880  [   32/  146]
train() client id: f_00005-6-1 loss: 0.493358  [   64/  146]
train() client id: f_00005-6-2 loss: 0.530754  [   96/  146]
train() client id: f_00005-6-3 loss: 0.576062  [  128/  146]
train() client id: f_00005-7-0 loss: 0.441607  [   32/  146]
train() client id: f_00005-7-1 loss: 0.342365  [   64/  146]
train() client id: f_00005-7-2 loss: 0.580502  [   96/  146]
train() client id: f_00005-7-3 loss: 0.419542  [  128/  146]
train() client id: f_00005-8-0 loss: 0.529474  [   32/  146]
train() client id: f_00005-8-1 loss: 0.358975  [   64/  146]
train() client id: f_00005-8-2 loss: 0.551852  [   96/  146]
train() client id: f_00005-8-3 loss: 0.507488  [  128/  146]
train() client id: f_00005-9-0 loss: 0.359553  [   32/  146]
train() client id: f_00005-9-1 loss: 0.542110  [   64/  146]
train() client id: f_00005-9-2 loss: 0.455828  [   96/  146]
train() client id: f_00005-9-3 loss: 0.489412  [  128/  146]
train() client id: f_00005-10-0 loss: 0.475184  [   32/  146]
train() client id: f_00005-10-1 loss: 0.465557  [   64/  146]
train() client id: f_00005-10-2 loss: 0.320657  [   96/  146]
train() client id: f_00005-10-3 loss: 0.610081  [  128/  146]
train() client id: f_00005-11-0 loss: 0.493063  [   32/  146]
train() client id: f_00005-11-1 loss: 0.556287  [   64/  146]
train() client id: f_00005-11-2 loss: 0.363735  [   96/  146]
train() client id: f_00005-11-3 loss: 0.369718  [  128/  146]
train() client id: f_00006-0-0 loss: 0.622245  [   32/   54]
train() client id: f_00006-1-0 loss: 0.556324  [   32/   54]
train() client id: f_00006-2-0 loss: 0.562345  [   32/   54]
train() client id: f_00006-3-0 loss: 0.611153  [   32/   54]
train() client id: f_00006-4-0 loss: 0.513032  [   32/   54]
train() client id: f_00006-5-0 loss: 0.517099  [   32/   54]
train() client id: f_00006-6-0 loss: 0.567346  [   32/   54]
train() client id: f_00006-7-0 loss: 0.575474  [   32/   54]
train() client id: f_00006-8-0 loss: 0.565698  [   32/   54]
train() client id: f_00006-9-0 loss: 0.619252  [   32/   54]
train() client id: f_00006-10-0 loss: 0.500734  [   32/   54]
train() client id: f_00006-11-0 loss: 0.520876  [   32/   54]
train() client id: f_00007-0-0 loss: 0.759721  [   32/  179]
train() client id: f_00007-0-1 loss: 0.556184  [   64/  179]
train() client id: f_00007-0-2 loss: 0.740209  [   96/  179]
train() client id: f_00007-0-3 loss: 0.592314  [  128/  179]
train() client id: f_00007-0-4 loss: 0.601770  [  160/  179]
train() client id: f_00007-1-0 loss: 0.701531  [   32/  179]
train() client id: f_00007-1-1 loss: 0.675027  [   64/  179]
train() client id: f_00007-1-2 loss: 0.527274  [   96/  179]
train() client id: f_00007-1-3 loss: 0.802114  [  128/  179]
train() client id: f_00007-1-4 loss: 0.544275  [  160/  179]
train() client id: f_00007-2-0 loss: 0.670355  [   32/  179]
train() client id: f_00007-2-1 loss: 0.750095  [   64/  179]
train() client id: f_00007-2-2 loss: 0.708096  [   96/  179]
train() client id: f_00007-2-3 loss: 0.580825  [  128/  179]
train() client id: f_00007-2-4 loss: 0.492153  [  160/  179]
train() client id: f_00007-3-0 loss: 0.766666  [   32/  179]
train() client id: f_00007-3-1 loss: 0.610063  [   64/  179]
train() client id: f_00007-3-2 loss: 0.766673  [   96/  179]
train() client id: f_00007-3-3 loss: 0.551869  [  128/  179]
train() client id: f_00007-3-4 loss: 0.457635  [  160/  179]
train() client id: f_00007-4-0 loss: 0.662127  [   32/  179]
train() client id: f_00007-4-1 loss: 0.521883  [   64/  179]
train() client id: f_00007-4-2 loss: 0.640315  [   96/  179]
train() client id: f_00007-4-3 loss: 0.707149  [  128/  179]
train() client id: f_00007-4-4 loss: 0.666385  [  160/  179]
train() client id: f_00007-5-0 loss: 0.666520  [   32/  179]
train() client id: f_00007-5-1 loss: 0.737373  [   64/  179]
train() client id: f_00007-5-2 loss: 0.586866  [   96/  179]
train() client id: f_00007-5-3 loss: 0.510138  [  128/  179]
train() client id: f_00007-5-4 loss: 0.669305  [  160/  179]
train() client id: f_00007-6-0 loss: 0.562785  [   32/  179]
train() client id: f_00007-6-1 loss: 0.662753  [   64/  179]
train() client id: f_00007-6-2 loss: 0.516331  [   96/  179]
train() client id: f_00007-6-3 loss: 0.737062  [  128/  179]
train() client id: f_00007-6-4 loss: 0.674003  [  160/  179]
train() client id: f_00007-7-0 loss: 0.579463  [   32/  179]
train() client id: f_00007-7-1 loss: 0.713348  [   64/  179]
train() client id: f_00007-7-2 loss: 0.652590  [   96/  179]
train() client id: f_00007-7-3 loss: 0.574010  [  128/  179]
train() client id: f_00007-7-4 loss: 0.549338  [  160/  179]
train() client id: f_00007-8-0 loss: 0.468857  [   32/  179]
train() client id: f_00007-8-1 loss: 0.480257  [   64/  179]
train() client id: f_00007-8-2 loss: 0.665528  [   96/  179]
train() client id: f_00007-8-3 loss: 0.675719  [  128/  179]
train() client id: f_00007-8-4 loss: 0.751438  [  160/  179]
train() client id: f_00007-9-0 loss: 0.899440  [   32/  179]
train() client id: f_00007-9-1 loss: 0.615549  [   64/  179]
train() client id: f_00007-9-2 loss: 0.483761  [   96/  179]
train() client id: f_00007-9-3 loss: 0.465529  [  128/  179]
train() client id: f_00007-9-4 loss: 0.581448  [  160/  179]
train() client id: f_00007-10-0 loss: 0.454382  [   32/  179]
train() client id: f_00007-10-1 loss: 0.455101  [   64/  179]
train() client id: f_00007-10-2 loss: 0.779908  [   96/  179]
train() client id: f_00007-10-3 loss: 0.694800  [  128/  179]
train() client id: f_00007-10-4 loss: 0.642863  [  160/  179]
train() client id: f_00007-11-0 loss: 0.455271  [   32/  179]
train() client id: f_00007-11-1 loss: 0.483837  [   64/  179]
train() client id: f_00007-11-2 loss: 0.564439  [   96/  179]
train() client id: f_00007-11-3 loss: 0.673150  [  128/  179]
train() client id: f_00007-11-4 loss: 0.643014  [  160/  179]
train() client id: f_00008-0-0 loss: 0.708003  [   32/  130]
train() client id: f_00008-0-1 loss: 0.808379  [   64/  130]
train() client id: f_00008-0-2 loss: 0.689589  [   96/  130]
train() client id: f_00008-0-3 loss: 0.743454  [  128/  130]
train() client id: f_00008-1-0 loss: 0.738699  [   32/  130]
train() client id: f_00008-1-1 loss: 0.789216  [   64/  130]
train() client id: f_00008-1-2 loss: 0.678476  [   96/  130]
train() client id: f_00008-1-3 loss: 0.736376  [  128/  130]
train() client id: f_00008-2-0 loss: 0.645032  [   32/  130]
train() client id: f_00008-2-1 loss: 0.746849  [   64/  130]
train() client id: f_00008-2-2 loss: 0.762405  [   96/  130]
train() client id: f_00008-2-3 loss: 0.763369  [  128/  130]
train() client id: f_00008-3-0 loss: 0.678828  [   32/  130]
train() client id: f_00008-3-1 loss: 0.673456  [   64/  130]
train() client id: f_00008-3-2 loss: 0.712827  [   96/  130]
train() client id: f_00008-3-3 loss: 0.855272  [  128/  130]
train() client id: f_00008-4-0 loss: 0.719983  [   32/  130]
train() client id: f_00008-4-1 loss: 0.767536  [   64/  130]
train() client id: f_00008-4-2 loss: 0.803150  [   96/  130]
train() client id: f_00008-4-3 loss: 0.638452  [  128/  130]
train() client id: f_00008-5-0 loss: 0.711883  [   32/  130]
train() client id: f_00008-5-1 loss: 0.816706  [   64/  130]
train() client id: f_00008-5-2 loss: 0.656301  [   96/  130]
train() client id: f_00008-5-3 loss: 0.736523  [  128/  130]
train() client id: f_00008-6-0 loss: 0.728263  [   32/  130]
train() client id: f_00008-6-1 loss: 0.694991  [   64/  130]
train() client id: f_00008-6-2 loss: 0.810819  [   96/  130]
train() client id: f_00008-6-3 loss: 0.700444  [  128/  130]
train() client id: f_00008-7-0 loss: 0.693185  [   32/  130]
train() client id: f_00008-7-1 loss: 0.679523  [   64/  130]
train() client id: f_00008-7-2 loss: 0.776739  [   96/  130]
train() client id: f_00008-7-3 loss: 0.770116  [  128/  130]
train() client id: f_00008-8-0 loss: 0.678043  [   32/  130]
train() client id: f_00008-8-1 loss: 0.773376  [   64/  130]
train() client id: f_00008-8-2 loss: 0.785532  [   96/  130]
train() client id: f_00008-8-3 loss: 0.694458  [  128/  130]
train() client id: f_00008-9-0 loss: 0.710889  [   32/  130]
train() client id: f_00008-9-1 loss: 0.674543  [   64/  130]
train() client id: f_00008-9-2 loss: 0.686076  [   96/  130]
train() client id: f_00008-9-3 loss: 0.871604  [  128/  130]
train() client id: f_00008-10-0 loss: 0.663043  [   32/  130]
train() client id: f_00008-10-1 loss: 0.736417  [   64/  130]
train() client id: f_00008-10-2 loss: 0.694837  [   96/  130]
train() client id: f_00008-10-3 loss: 0.811822  [  128/  130]
train() client id: f_00008-11-0 loss: 0.807890  [   32/  130]
train() client id: f_00008-11-1 loss: 0.800084  [   64/  130]
train() client id: f_00008-11-2 loss: 0.648629  [   96/  130]
train() client id: f_00008-11-3 loss: 0.673795  [  128/  130]
train() client id: f_00009-0-0 loss: 1.116539  [   32/  118]
train() client id: f_00009-0-1 loss: 1.244991  [   64/  118]
train() client id: f_00009-0-2 loss: 1.049192  [   96/  118]
train() client id: f_00009-1-0 loss: 1.187884  [   32/  118]
train() client id: f_00009-1-1 loss: 1.030605  [   64/  118]
train() client id: f_00009-1-2 loss: 1.084837  [   96/  118]
train() client id: f_00009-2-0 loss: 1.029999  [   32/  118]
train() client id: f_00009-2-1 loss: 1.019647  [   64/  118]
train() client id: f_00009-2-2 loss: 1.077714  [   96/  118]
train() client id: f_00009-3-0 loss: 1.042162  [   32/  118]
train() client id: f_00009-3-1 loss: 0.920290  [   64/  118]
train() client id: f_00009-3-2 loss: 1.096772  [   96/  118]
train() client id: f_00009-4-0 loss: 0.966932  [   32/  118]
train() client id: f_00009-4-1 loss: 1.031208  [   64/  118]
train() client id: f_00009-4-2 loss: 0.969492  [   96/  118]
train() client id: f_00009-5-0 loss: 1.025283  [   32/  118]
train() client id: f_00009-5-1 loss: 0.905071  [   64/  118]
train() client id: f_00009-5-2 loss: 0.897753  [   96/  118]
train() client id: f_00009-6-0 loss: 1.097234  [   32/  118]
train() client id: f_00009-6-1 loss: 0.911513  [   64/  118]
train() client id: f_00009-6-2 loss: 0.919661  [   96/  118]
train() client id: f_00009-7-0 loss: 1.020890  [   32/  118]
train() client id: f_00009-7-1 loss: 0.874334  [   64/  118]
train() client id: f_00009-7-2 loss: 0.938685  [   96/  118]
train() client id: f_00009-8-0 loss: 1.016264  [   32/  118]
train() client id: f_00009-8-1 loss: 0.959221  [   64/  118]
train() client id: f_00009-8-2 loss: 0.766770  [   96/  118]
train() client id: f_00009-9-0 loss: 0.921597  [   32/  118]
train() client id: f_00009-9-1 loss: 0.831860  [   64/  118]
train() client id: f_00009-9-2 loss: 0.921404  [   96/  118]
train() client id: f_00009-10-0 loss: 0.840994  [   32/  118]
train() client id: f_00009-10-1 loss: 1.017252  [   64/  118]
train() client id: f_00009-10-2 loss: 0.899002  [   96/  118]
train() client id: f_00009-11-0 loss: 0.865361  [   32/  118]
train() client id: f_00009-11-1 loss: 0.871234  [   64/  118]
train() client id: f_00009-11-2 loss: 0.909092  [   96/  118]
At round 23 accuracy: 0.6445623342175066
At round 23 training accuracy: 0.5814889336016097
At round 23 training loss: 0.8403653286277529
update_location
xs = [ -3.9056584    4.20031788 135.00902392  18.81129433   0.97929623
   3.95640986 -97.44319194 -76.32485185 119.66397685 -62.06087855]
ys = [127.5879595  110.55583871   1.32061395 -97.45517586  89.35018685
  72.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [162.15406741 149.13160679 168.01541763 140.89491153 134.10598387
 123.76410517 139.64979842 125.80206384 156.93354431 117.76062374]
dists_bs = [177.54203815 190.43582607 355.34507742 334.35705565 195.71561817
 206.07606464 193.81871306 200.18651135 334.08288959 204.92861778]
uav_gains = [2.97801922e-11 3.67836040e-11 2.72058983e-11 4.24194154e-11
 4.80041803e-11 5.86786623e-11 4.33735009e-11 5.63299134e-11
 3.23510419e-11 6.64482197e-11]
bs_gains = [5.56225278e-11 4.57085931e-11 7.97027370e-12 9.45158637e-12
 4.23392081e-11 3.66451867e-11 4.35097021e-11 3.97444612e-11
 9.47332053e-12 3.72226060e-11]
Round 24
-------------------------------
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.82454946 16.26273367  7.70634767  2.76510758 18.75837368  9.03341251
  3.43331975 11.02682771  8.12439108  7.33018602]
obj_prev = 92.26524913835074
eta_min = 1.3786631317009973e-12	eta_max = 0.9235436873576162
af = 19.491574900134562	bf = 1.580577038913744	zeta = 21.44073239014802	eta = 0.909090909090909
af = 19.491574900134562	bf = 1.580577038913744	zeta = 37.821548020465336	eta = 0.5153563489677266
af = 19.491574900134562	bf = 1.580577038913744	zeta = 29.921938016706985	eta = 0.65141418611493
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.500452198585428	eta = 0.6839040575328834
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428583728430333	eta = 0.6856329912995908
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428386266155645	eta = 0.685637753675084
eta = 0.685637753675084
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.03114263 0.06549838 0.0306483  0.01062804 0.07563209 0.0360859
 0.01334684 0.0442423  0.03213128 0.02916532]
ene_total = [2.48895914 4.62483507 2.46833909 1.14803217 5.27587599 2.77970132
 1.31845091 3.25561899 2.72998043 2.33859317]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 0 obj = 5.054794435766291
eta = 0.685637753675084
freqs = [41745313.04826085 86137162.39129016 41270331.48315956 14019810.25468265
 99776755.16786237 47903390.74277508 17590043.05099849 57717284.54230393
 46575586.46414848 38689551.73454573]
eta_min = 0.6856377536750926	eta_max = 0.6856377536750637
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 0.024656332621380766	eta = 0.909090909090909
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 17.409847423005747	eta = 0.0012874810038828607
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7319792587002336	eta = 0.012941753040645609
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7000222977574324	eta = 0.013185031671165255
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.700018383049954	eta = 0.013185062032920516
eta = 0.013185062032920516
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.55616847e-04 1.39346998e-03 1.49681533e-04 5.98995210e-06
 2.15899085e-03 2.37441151e-04 1.18412674e-05 4.22605781e-04
 1.99862188e-04 1.25181464e-04]
ene_total = [0.16607727 0.17657133 0.16938779 0.15085494 0.1944668  0.16035839
 0.15026585 0.15072403 0.2237539  0.15755809]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 1 obj = 5.0547944357664285
eta = 0.6856377536750926
freqs = [41745313.04826085 86137162.3912901  41270331.48315956 14019810.25468264
 99776755.16786231 47903390.74277506 17590043.05099848 57717284.54230388
 46575586.46414861 38689551.73454572]
Done!
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.31745940e-05 1.17971811e-04 1.26721075e-05 5.07112105e-07
 1.82781160e-04 2.01018772e-05 1.00248715e-06 3.57780002e-05
 1.69204250e-05 1.05979204e-05]
ene_total = [0.00806774 0.00745347 0.00823683 0.00745217 0.00763743 0.00771012
 0.00741769 0.00706435 0.01087853 0.00767444]
At round 24 energy consumption: 0.07959277336451151
At round 24 eta: 0.6856377536750926
At round 24 a_n: 19.96150252899076
At round 24 local rounds: 12.358184869136853
At round 24 global rounds: 63.498409119903215
gradient difference: 0.4539724886417389
train() client id: f_00000-0-0 loss: 1.249255  [   32/  126]
train() client id: f_00000-0-1 loss: 1.139752  [   64/  126]
train() client id: f_00000-0-2 loss: 0.898477  [   96/  126]
train() client id: f_00000-1-0 loss: 1.076601  [   32/  126]
train() client id: f_00000-1-1 loss: 1.080034  [   64/  126]
train() client id: f_00000-1-2 loss: 0.927763  [   96/  126]
train() client id: f_00000-2-0 loss: 0.943208  [   32/  126]
train() client id: f_00000-2-1 loss: 1.072393  [   64/  126]
train() client id: f_00000-2-2 loss: 1.027838  [   96/  126]
train() client id: f_00000-3-0 loss: 1.065444  [   32/  126]
train() client id: f_00000-3-1 loss: 0.975456  [   64/  126]
train() client id: f_00000-3-2 loss: 0.865163  [   96/  126]
train() client id: f_00000-4-0 loss: 0.863685  [   32/  126]
train() client id: f_00000-4-1 loss: 0.852932  [   64/  126]
train() client id: f_00000-4-2 loss: 1.000765  [   96/  126]
train() client id: f_00000-5-0 loss: 0.846789  [   32/  126]
train() client id: f_00000-5-1 loss: 0.858284  [   64/  126]
train() client id: f_00000-5-2 loss: 0.995290  [   96/  126]
train() client id: f_00000-6-0 loss: 0.864160  [   32/  126]
train() client id: f_00000-6-1 loss: 0.842009  [   64/  126]
train() client id: f_00000-6-2 loss: 0.869579  [   96/  126]
train() client id: f_00000-7-0 loss: 0.944332  [   32/  126]
train() client id: f_00000-7-1 loss: 0.751914  [   64/  126]
train() client id: f_00000-7-2 loss: 0.914694  [   96/  126]
train() client id: f_00000-8-0 loss: 0.943618  [   32/  126]
train() client id: f_00000-8-1 loss: 0.828849  [   64/  126]
train() client id: f_00000-8-2 loss: 0.763232  [   96/  126]
train() client id: f_00000-9-0 loss: 0.797313  [   32/  126]
train() client id: f_00000-9-1 loss: 0.877176  [   64/  126]
train() client id: f_00000-9-2 loss: 0.874293  [   96/  126]
train() client id: f_00000-10-0 loss: 0.829312  [   32/  126]
train() client id: f_00000-10-1 loss: 0.911034  [   64/  126]
train() client id: f_00000-10-2 loss: 0.788858  [   96/  126]
train() client id: f_00000-11-0 loss: 0.810731  [   32/  126]
train() client id: f_00000-11-1 loss: 0.893561  [   64/  126]
train() client id: f_00000-11-2 loss: 0.910922  [   96/  126]
train() client id: f_00001-0-0 loss: 0.529236  [   32/  265]
train() client id: f_00001-0-1 loss: 0.456989  [   64/  265]
train() client id: f_00001-0-2 loss: 0.626057  [   96/  265]
train() client id: f_00001-0-3 loss: 0.530701  [  128/  265]
train() client id: f_00001-0-4 loss: 0.577342  [  160/  265]
train() client id: f_00001-0-5 loss: 0.439284  [  192/  265]
train() client id: f_00001-0-6 loss: 0.463373  [  224/  265]
train() client id: f_00001-0-7 loss: 0.568498  [  256/  265]
train() client id: f_00001-1-0 loss: 0.510785  [   32/  265]
train() client id: f_00001-1-1 loss: 0.452802  [   64/  265]
train() client id: f_00001-1-2 loss: 0.502525  [   96/  265]
train() client id: f_00001-1-3 loss: 0.475637  [  128/  265]
train() client id: f_00001-1-4 loss: 0.705228  [  160/  265]
train() client id: f_00001-1-5 loss: 0.429443  [  192/  265]
train() client id: f_00001-1-6 loss: 0.494088  [  224/  265]
train() client id: f_00001-1-7 loss: 0.478591  [  256/  265]
train() client id: f_00001-2-0 loss: 0.536855  [   32/  265]
train() client id: f_00001-2-1 loss: 0.540697  [   64/  265]
train() client id: f_00001-2-2 loss: 0.632022  [   96/  265]
train() client id: f_00001-2-3 loss: 0.501986  [  128/  265]
train() client id: f_00001-2-4 loss: 0.504038  [  160/  265]
train() client id: f_00001-2-5 loss: 0.421113  [  192/  265]
train() client id: f_00001-2-6 loss: 0.414834  [  224/  265]
train() client id: f_00001-2-7 loss: 0.515653  [  256/  265]
train() client id: f_00001-3-0 loss: 0.490085  [   32/  265]
train() client id: f_00001-3-1 loss: 0.546420  [   64/  265]
train() client id: f_00001-3-2 loss: 0.410259  [   96/  265]
train() client id: f_00001-3-3 loss: 0.608204  [  128/  265]
train() client id: f_00001-3-4 loss: 0.471874  [  160/  265]
train() client id: f_00001-3-5 loss: 0.418286  [  192/  265]
train() client id: f_00001-3-6 loss: 0.505692  [  224/  265]
train() client id: f_00001-3-7 loss: 0.583772  [  256/  265]
train() client id: f_00001-4-0 loss: 0.490660  [   32/  265]
train() client id: f_00001-4-1 loss: 0.409833  [   64/  265]
train() client id: f_00001-4-2 loss: 0.546044  [   96/  265]
train() client id: f_00001-4-3 loss: 0.552350  [  128/  265]
train() client id: f_00001-4-4 loss: 0.523797  [  160/  265]
train() client id: f_00001-4-5 loss: 0.526196  [  192/  265]
train() client id: f_00001-4-6 loss: 0.546121  [  224/  265]
train() client id: f_00001-4-7 loss: 0.397693  [  256/  265]
train() client id: f_00001-5-0 loss: 0.567046  [   32/  265]
train() client id: f_00001-5-1 loss: 0.546330  [   64/  265]
train() client id: f_00001-5-2 loss: 0.508222  [   96/  265]
train() client id: f_00001-5-3 loss: 0.419846  [  128/  265]
train() client id: f_00001-5-4 loss: 0.393007  [  160/  265]
train() client id: f_00001-5-5 loss: 0.469416  [  192/  265]
train() client id: f_00001-5-6 loss: 0.402714  [  224/  265]
train() client id: f_00001-5-7 loss: 0.563661  [  256/  265]
train() client id: f_00001-6-0 loss: 0.420027  [   32/  265]
train() client id: f_00001-6-1 loss: 0.455636  [   64/  265]
train() client id: f_00001-6-2 loss: 0.577325  [   96/  265]
train() client id: f_00001-6-3 loss: 0.480973  [  128/  265]
train() client id: f_00001-6-4 loss: 0.476044  [  160/  265]
train() client id: f_00001-6-5 loss: 0.549396  [  192/  265]
train() client id: f_00001-6-6 loss: 0.498944  [  224/  265]
train() client id: f_00001-6-7 loss: 0.506105  [  256/  265]
train() client id: f_00001-7-0 loss: 0.494170  [   32/  265]
train() client id: f_00001-7-1 loss: 0.425215  [   64/  265]
train() client id: f_00001-7-2 loss: 0.465147  [   96/  265]
train() client id: f_00001-7-3 loss: 0.470218  [  128/  265]
train() client id: f_00001-7-4 loss: 0.532884  [  160/  265]
train() client id: f_00001-7-5 loss: 0.512009  [  192/  265]
train() client id: f_00001-7-6 loss: 0.500667  [  224/  265]
train() client id: f_00001-7-7 loss: 0.461025  [  256/  265]
train() client id: f_00001-8-0 loss: 0.645207  [   32/  265]
train() client id: f_00001-8-1 loss: 0.391317  [   64/  265]
train() client id: f_00001-8-2 loss: 0.448153  [   96/  265]
train() client id: f_00001-8-3 loss: 0.498829  [  128/  265]
train() client id: f_00001-8-4 loss: 0.625157  [  160/  265]
train() client id: f_00001-8-5 loss: 0.453714  [  192/  265]
train() client id: f_00001-8-6 loss: 0.411562  [  224/  265]
train() client id: f_00001-8-7 loss: 0.475437  [  256/  265]
train() client id: f_00001-9-0 loss: 0.452074  [   32/  265]
train() client id: f_00001-9-1 loss: 0.467427  [   64/  265]
train() client id: f_00001-9-2 loss: 0.537443  [   96/  265]
train() client id: f_00001-9-3 loss: 0.526339  [  128/  265]
train() client id: f_00001-9-4 loss: 0.400676  [  160/  265]
train() client id: f_00001-9-5 loss: 0.520697  [  192/  265]
train() client id: f_00001-9-6 loss: 0.405675  [  224/  265]
train() client id: f_00001-9-7 loss: 0.557045  [  256/  265]
train() client id: f_00001-10-0 loss: 0.393839  [   32/  265]
train() client id: f_00001-10-1 loss: 0.494923  [   64/  265]
train() client id: f_00001-10-2 loss: 0.470246  [   96/  265]
train() client id: f_00001-10-3 loss: 0.688196  [  128/  265]
train() client id: f_00001-10-4 loss: 0.494428  [  160/  265]
train() client id: f_00001-10-5 loss: 0.452591  [  192/  265]
train() client id: f_00001-10-6 loss: 0.462102  [  224/  265]
train() client id: f_00001-10-7 loss: 0.477868  [  256/  265]
train() client id: f_00001-11-0 loss: 0.549617  [   32/  265]
train() client id: f_00001-11-1 loss: 0.463788  [   64/  265]
train() client id: f_00001-11-2 loss: 0.407246  [   96/  265]
train() client id: f_00001-11-3 loss: 0.543507  [  128/  265]
train() client id: f_00001-11-4 loss: 0.445233  [  160/  265]
train() client id: f_00001-11-5 loss: 0.553230  [  192/  265]
train() client id: f_00001-11-6 loss: 0.464717  [  224/  265]
train() client id: f_00001-11-7 loss: 0.493569  [  256/  265]
train() client id: f_00002-0-0 loss: 1.175989  [   32/  124]
train() client id: f_00002-0-1 loss: 1.238089  [   64/  124]
train() client id: f_00002-0-2 loss: 1.110082  [   96/  124]
train() client id: f_00002-1-0 loss: 1.049360  [   32/  124]
train() client id: f_00002-1-1 loss: 1.099689  [   64/  124]
train() client id: f_00002-1-2 loss: 1.235699  [   96/  124]
train() client id: f_00002-2-0 loss: 1.052845  [   32/  124]
train() client id: f_00002-2-1 loss: 1.118093  [   64/  124]
train() client id: f_00002-2-2 loss: 1.174783  [   96/  124]
train() client id: f_00002-3-0 loss: 1.150884  [   32/  124]
train() client id: f_00002-3-1 loss: 1.082598  [   64/  124]
train() client id: f_00002-3-2 loss: 0.937367  [   96/  124]
train() client id: f_00002-4-0 loss: 1.121737  [   32/  124]
train() client id: f_00002-4-1 loss: 1.134746  [   64/  124]
train() client id: f_00002-4-2 loss: 1.084237  [   96/  124]
train() client id: f_00002-5-0 loss: 1.133674  [   32/  124]
train() client id: f_00002-5-1 loss: 0.983739  [   64/  124]
train() client id: f_00002-5-2 loss: 1.088757  [   96/  124]
train() client id: f_00002-6-0 loss: 0.922497  [   32/  124]
train() client id: f_00002-6-1 loss: 1.166271  [   64/  124]
train() client id: f_00002-6-2 loss: 1.023914  [   96/  124]
train() client id: f_00002-7-0 loss: 0.969355  [   32/  124]
train() client id: f_00002-7-1 loss: 1.014512  [   64/  124]
train() client id: f_00002-7-2 loss: 1.072675  [   96/  124]
train() client id: f_00002-8-0 loss: 0.961521  [   32/  124]
train() client id: f_00002-8-1 loss: 1.070910  [   64/  124]
train() client id: f_00002-8-2 loss: 1.025321  [   96/  124]
train() client id: f_00002-9-0 loss: 0.996235  [   32/  124]
train() client id: f_00002-9-1 loss: 1.045902  [   64/  124]
train() client id: f_00002-9-2 loss: 1.053550  [   96/  124]
train() client id: f_00002-10-0 loss: 1.093400  [   32/  124]
train() client id: f_00002-10-1 loss: 1.068242  [   64/  124]
train() client id: f_00002-10-2 loss: 0.973857  [   96/  124]
train() client id: f_00002-11-0 loss: 1.184397  [   32/  124]
train() client id: f_00002-11-1 loss: 1.014265  [   64/  124]
train() client id: f_00002-11-2 loss: 0.877991  [   96/  124]
train() client id: f_00003-0-0 loss: 0.886604  [   32/   43]
train() client id: f_00003-1-0 loss: 0.711541  [   32/   43]
train() client id: f_00003-2-0 loss: 0.892042  [   32/   43]
train() client id: f_00003-3-0 loss: 0.754275  [   32/   43]
train() client id: f_00003-4-0 loss: 0.867330  [   32/   43]
train() client id: f_00003-5-0 loss: 0.888623  [   32/   43]
train() client id: f_00003-6-0 loss: 0.842164  [   32/   43]
train() client id: f_00003-7-0 loss: 0.870709  [   32/   43]
train() client id: f_00003-8-0 loss: 0.930600  [   32/   43]
train() client id: f_00003-9-0 loss: 0.716651  [   32/   43]
train() client id: f_00003-10-0 loss: 0.846655  [   32/   43]
train() client id: f_00003-11-0 loss: 0.907191  [   32/   43]
train() client id: f_00004-0-0 loss: 0.841727  [   32/  306]
train() client id: f_00004-0-1 loss: 0.985936  [   64/  306]
train() client id: f_00004-0-2 loss: 0.795757  [   96/  306]
train() client id: f_00004-0-3 loss: 0.912055  [  128/  306]
train() client id: f_00004-0-4 loss: 1.058470  [  160/  306]
train() client id: f_00004-0-5 loss: 1.051607  [  192/  306]
train() client id: f_00004-0-6 loss: 0.925716  [  224/  306]
train() client id: f_00004-0-7 loss: 0.952616  [  256/  306]
train() client id: f_00004-0-8 loss: 0.717477  [  288/  306]
train() client id: f_00004-1-0 loss: 0.964617  [   32/  306]
train() client id: f_00004-1-1 loss: 0.895159  [   64/  306]
train() client id: f_00004-1-2 loss: 0.890839  [   96/  306]
train() client id: f_00004-1-3 loss: 0.980036  [  128/  306]
train() client id: f_00004-1-4 loss: 1.074157  [  160/  306]
train() client id: f_00004-1-5 loss: 0.771549  [  192/  306]
train() client id: f_00004-1-6 loss: 0.824133  [  224/  306]
train() client id: f_00004-1-7 loss: 0.867550  [  256/  306]
train() client id: f_00004-1-8 loss: 0.981742  [  288/  306]
train() client id: f_00004-2-0 loss: 0.868318  [   32/  306]
train() client id: f_00004-2-1 loss: 0.869246  [   64/  306]
train() client id: f_00004-2-2 loss: 0.933346  [   96/  306]
train() client id: f_00004-2-3 loss: 0.913406  [  128/  306]
train() client id: f_00004-2-4 loss: 1.092935  [  160/  306]
train() client id: f_00004-2-5 loss: 0.847184  [  192/  306]
train() client id: f_00004-2-6 loss: 0.867388  [  224/  306]
train() client id: f_00004-2-7 loss: 0.922253  [  256/  306]
train() client id: f_00004-2-8 loss: 0.845596  [  288/  306]
train() client id: f_00004-3-0 loss: 0.821602  [   32/  306]
train() client id: f_00004-3-1 loss: 0.940142  [   64/  306]
train() client id: f_00004-3-2 loss: 0.953779  [   96/  306]
train() client id: f_00004-3-3 loss: 0.980802  [  128/  306]
train() client id: f_00004-3-4 loss: 1.043596  [  160/  306]
train() client id: f_00004-3-5 loss: 0.944224  [  192/  306]
train() client id: f_00004-3-6 loss: 0.828112  [  224/  306]
train() client id: f_00004-3-7 loss: 0.798171  [  256/  306]
train() client id: f_00004-3-8 loss: 0.851661  [  288/  306]
train() client id: f_00004-4-0 loss: 0.919892  [   32/  306]
train() client id: f_00004-4-1 loss: 0.783284  [   64/  306]
train() client id: f_00004-4-2 loss: 0.829724  [   96/  306]
train() client id: f_00004-4-3 loss: 0.882300  [  128/  306]
train() client id: f_00004-4-4 loss: 0.890155  [  160/  306]
train() client id: f_00004-4-5 loss: 0.851408  [  192/  306]
train() client id: f_00004-4-6 loss: 0.965795  [  224/  306]
train() client id: f_00004-4-7 loss: 0.980659  [  256/  306]
train() client id: f_00004-4-8 loss: 0.974600  [  288/  306]
train() client id: f_00004-5-0 loss: 0.885934  [   32/  306]
train() client id: f_00004-5-1 loss: 0.958232  [   64/  306]
train() client id: f_00004-5-2 loss: 0.908539  [   96/  306]
train() client id: f_00004-5-3 loss: 0.904909  [  128/  306]
train() client id: f_00004-5-4 loss: 0.845599  [  160/  306]
train() client id: f_00004-5-5 loss: 0.831626  [  192/  306]
train() client id: f_00004-5-6 loss: 0.839397  [  224/  306]
train() client id: f_00004-5-7 loss: 0.855804  [  256/  306]
train() client id: f_00004-5-8 loss: 1.107194  [  288/  306]
train() client id: f_00004-6-0 loss: 0.906965  [   32/  306]
train() client id: f_00004-6-1 loss: 0.917213  [   64/  306]
train() client id: f_00004-6-2 loss: 0.911201  [   96/  306]
train() client id: f_00004-6-3 loss: 0.944135  [  128/  306]
train() client id: f_00004-6-4 loss: 0.756482  [  160/  306]
train() client id: f_00004-6-5 loss: 0.985329  [  192/  306]
train() client id: f_00004-6-6 loss: 0.802641  [  224/  306]
train() client id: f_00004-6-7 loss: 0.872030  [  256/  306]
train() client id: f_00004-6-8 loss: 0.941405  [  288/  306]
train() client id: f_00004-7-0 loss: 0.971650  [   32/  306]
train() client id: f_00004-7-1 loss: 0.897311  [   64/  306]
train() client id: f_00004-7-2 loss: 0.876541  [   96/  306]
train() client id: f_00004-7-3 loss: 0.923576  [  128/  306]
train() client id: f_00004-7-4 loss: 1.104774  [  160/  306]
train() client id: f_00004-7-5 loss: 0.871817  [  192/  306]
train() client id: f_00004-7-6 loss: 0.866831  [  224/  306]
train() client id: f_00004-7-7 loss: 0.902960  [  256/  306]
train() client id: f_00004-7-8 loss: 0.783877  [  288/  306]
train() client id: f_00004-8-0 loss: 0.999395  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825642  [   64/  306]
train() client id: f_00004-8-2 loss: 0.795633  [   96/  306]
train() client id: f_00004-8-3 loss: 0.897802  [  128/  306]
train() client id: f_00004-8-4 loss: 1.029515  [  160/  306]
train() client id: f_00004-8-5 loss: 0.887444  [  192/  306]
train() client id: f_00004-8-6 loss: 0.823336  [  224/  306]
train() client id: f_00004-8-7 loss: 0.927778  [  256/  306]
train() client id: f_00004-8-8 loss: 0.877485  [  288/  306]
train() client id: f_00004-9-0 loss: 1.006670  [   32/  306]
train() client id: f_00004-9-1 loss: 0.868961  [   64/  306]
train() client id: f_00004-9-2 loss: 0.864370  [   96/  306]
train() client id: f_00004-9-3 loss: 0.973657  [  128/  306]
train() client id: f_00004-9-4 loss: 0.889076  [  160/  306]
train() client id: f_00004-9-5 loss: 0.845865  [  192/  306]
train() client id: f_00004-9-6 loss: 0.846285  [  224/  306]
train() client id: f_00004-9-7 loss: 0.878251  [  256/  306]
train() client id: f_00004-9-8 loss: 0.908179  [  288/  306]
train() client id: f_00004-10-0 loss: 0.806504  [   32/  306]
train() client id: f_00004-10-1 loss: 0.993376  [   64/  306]
train() client id: f_00004-10-2 loss: 1.042351  [   96/  306]
train() client id: f_00004-10-3 loss: 0.843409  [  128/  306]
train() client id: f_00004-10-4 loss: 0.885997  [  160/  306]
train() client id: f_00004-10-5 loss: 0.845987  [  192/  306]
train() client id: f_00004-10-6 loss: 0.879967  [  224/  306]
train() client id: f_00004-10-7 loss: 0.866225  [  256/  306]
train() client id: f_00004-10-8 loss: 0.820661  [  288/  306]
train() client id: f_00004-11-0 loss: 0.964437  [   32/  306]
train() client id: f_00004-11-1 loss: 0.867851  [   64/  306]
train() client id: f_00004-11-2 loss: 0.872231  [   96/  306]
train() client id: f_00004-11-3 loss: 0.928327  [  128/  306]
train() client id: f_00004-11-4 loss: 0.823223  [  160/  306]
train() client id: f_00004-11-5 loss: 1.021245  [  192/  306]
train() client id: f_00004-11-6 loss: 0.837659  [  224/  306]
train() client id: f_00004-11-7 loss: 0.924181  [  256/  306]
train() client id: f_00004-11-8 loss: 0.837644  [  288/  306]
train() client id: f_00005-0-0 loss: 0.948414  [   32/  146]
train() client id: f_00005-0-1 loss: 0.631463  [   64/  146]
train() client id: f_00005-0-2 loss: 0.653404  [   96/  146]
train() client id: f_00005-0-3 loss: 0.825337  [  128/  146]
train() client id: f_00005-1-0 loss: 0.664005  [   32/  146]
train() client id: f_00005-1-1 loss: 0.744279  [   64/  146]
train() client id: f_00005-1-2 loss: 0.676250  [   96/  146]
train() client id: f_00005-1-3 loss: 0.658089  [  128/  146]
train() client id: f_00005-2-0 loss: 0.801629  [   32/  146]
train() client id: f_00005-2-1 loss: 0.641872  [   64/  146]
train() client id: f_00005-2-2 loss: 0.662122  [   96/  146]
train() client id: f_00005-2-3 loss: 0.899292  [  128/  146]
train() client id: f_00005-3-0 loss: 0.899398  [   32/  146]
train() client id: f_00005-3-1 loss: 0.608005  [   64/  146]
train() client id: f_00005-3-2 loss: 0.793666  [   96/  146]
train() client id: f_00005-3-3 loss: 0.632248  [  128/  146]
train() client id: f_00005-4-0 loss: 0.797464  [   32/  146]
train() client id: f_00005-4-1 loss: 0.761845  [   64/  146]
train() client id: f_00005-4-2 loss: 0.770460  [   96/  146]
train() client id: f_00005-4-3 loss: 0.719867  [  128/  146]
train() client id: f_00005-5-0 loss: 0.995018  [   32/  146]
train() client id: f_00005-5-1 loss: 0.656432  [   64/  146]
train() client id: f_00005-5-2 loss: 0.966118  [   96/  146]
train() client id: f_00005-5-3 loss: 0.470780  [  128/  146]
train() client id: f_00005-6-0 loss: 0.685335  [   32/  146]
train() client id: f_00005-6-1 loss: 0.932502  [   64/  146]
train() client id: f_00005-6-2 loss: 0.812906  [   96/  146]
train() client id: f_00005-6-3 loss: 0.577406  [  128/  146]
train() client id: f_00005-7-0 loss: 0.843978  [   32/  146]
train() client id: f_00005-7-1 loss: 0.627809  [   64/  146]
train() client id: f_00005-7-2 loss: 0.837216  [   96/  146]
train() client id: f_00005-7-3 loss: 0.512565  [  128/  146]
train() client id: f_00005-8-0 loss: 0.854550  [   32/  146]
train() client id: f_00005-8-1 loss: 0.854654  [   64/  146]
train() client id: f_00005-8-2 loss: 0.695048  [   96/  146]
train() client id: f_00005-8-3 loss: 0.618567  [  128/  146]
train() client id: f_00005-9-0 loss: 0.981447  [   32/  146]
train() client id: f_00005-9-1 loss: 0.728583  [   64/  146]
train() client id: f_00005-9-2 loss: 0.721436  [   96/  146]
train() client id: f_00005-9-3 loss: 0.624532  [  128/  146]
train() client id: f_00005-10-0 loss: 0.709272  [   32/  146]
train() client id: f_00005-10-1 loss: 0.852764  [   64/  146]
train() client id: f_00005-10-2 loss: 0.637403  [   96/  146]
train() client id: f_00005-10-3 loss: 0.668101  [  128/  146]
train() client id: f_00005-11-0 loss: 0.762612  [   32/  146]
train() client id: f_00005-11-1 loss: 0.559384  [   64/  146]
train() client id: f_00005-11-2 loss: 0.889653  [   96/  146]
train() client id: f_00005-11-3 loss: 0.656743  [  128/  146]
train() client id: f_00006-0-0 loss: 0.650324  [   32/   54]
train() client id: f_00006-1-0 loss: 0.652412  [   32/   54]
train() client id: f_00006-2-0 loss: 0.612872  [   32/   54]
train() client id: f_00006-3-0 loss: 0.658072  [   32/   54]
train() client id: f_00006-4-0 loss: 0.646219  [   32/   54]
train() client id: f_00006-5-0 loss: 0.599241  [   32/   54]
train() client id: f_00006-6-0 loss: 0.602862  [   32/   54]
train() client id: f_00006-7-0 loss: 0.558821  [   32/   54]
train() client id: f_00006-8-0 loss: 0.651260  [   32/   54]
train() client id: f_00006-9-0 loss: 0.613169  [   32/   54]
train() client id: f_00006-10-0 loss: 0.558880  [   32/   54]
train() client id: f_00006-11-0 loss: 0.544067  [   32/   54]
train() client id: f_00007-0-0 loss: 0.596265  [   32/  179]
train() client id: f_00007-0-1 loss: 0.515296  [   64/  179]
train() client id: f_00007-0-2 loss: 0.622665  [   96/  179]
train() client id: f_00007-0-3 loss: 0.469735  [  128/  179]
train() client id: f_00007-0-4 loss: 0.613728  [  160/  179]
train() client id: f_00007-1-0 loss: 0.633957  [   32/  179]
train() client id: f_00007-1-1 loss: 0.512901  [   64/  179]
train() client id: f_00007-1-2 loss: 0.664890  [   96/  179]
train() client id: f_00007-1-3 loss: 0.417934  [  128/  179]
train() client id: f_00007-1-4 loss: 0.586363  [  160/  179]
train() client id: f_00007-2-0 loss: 0.716176  [   32/  179]
train() client id: f_00007-2-1 loss: 0.517004  [   64/  179]
train() client id: f_00007-2-2 loss: 0.552924  [   96/  179]
train() client id: f_00007-2-3 loss: 0.380422  [  128/  179]
train() client id: f_00007-2-4 loss: 0.629917  [  160/  179]
train() client id: f_00007-3-0 loss: 0.484256  [   32/  179]
train() client id: f_00007-3-1 loss: 0.405292  [   64/  179]
train() client id: f_00007-3-2 loss: 0.571505  [   96/  179]
train() client id: f_00007-3-3 loss: 0.483923  [  128/  179]
train() client id: f_00007-3-4 loss: 0.582383  [  160/  179]
train() client id: f_00007-4-0 loss: 0.452732  [   32/  179]
train() client id: f_00007-4-1 loss: 0.509580  [   64/  179]
train() client id: f_00007-4-2 loss: 0.615817  [   96/  179]
train() client id: f_00007-4-3 loss: 0.473082  [  128/  179]
train() client id: f_00007-4-4 loss: 0.583764  [  160/  179]
train() client id: f_00007-5-0 loss: 0.462970  [   32/  179]
train() client id: f_00007-5-1 loss: 0.573769  [   64/  179]
train() client id: f_00007-5-2 loss: 0.359907  [   96/  179]
train() client id: f_00007-5-3 loss: 0.517375  [  128/  179]
train() client id: f_00007-5-4 loss: 0.702959  [  160/  179]
train() client id: f_00007-6-0 loss: 0.400891  [   32/  179]
train() client id: f_00007-6-1 loss: 0.597111  [   64/  179]
train() client id: f_00007-6-2 loss: 0.383490  [   96/  179]
train() client id: f_00007-6-3 loss: 0.719960  [  128/  179]
train() client id: f_00007-6-4 loss: 0.409607  [  160/  179]
train() client id: f_00007-7-0 loss: 0.644259  [   32/  179]
train() client id: f_00007-7-1 loss: 0.447059  [   64/  179]
train() client id: f_00007-7-2 loss: 0.564944  [   96/  179]
train() client id: f_00007-7-3 loss: 0.479641  [  128/  179]
train() client id: f_00007-7-4 loss: 0.466569  [  160/  179]
train() client id: f_00007-8-0 loss: 0.501486  [   32/  179]
train() client id: f_00007-8-1 loss: 0.563840  [   64/  179]
train() client id: f_00007-8-2 loss: 0.473970  [   96/  179]
train() client id: f_00007-8-3 loss: 0.487945  [  128/  179]
train() client id: f_00007-8-4 loss: 0.447027  [  160/  179]
train() client id: f_00007-9-0 loss: 0.573588  [   32/  179]
train() client id: f_00007-9-1 loss: 0.458799  [   64/  179]
train() client id: f_00007-9-2 loss: 0.453998  [   96/  179]
train() client id: f_00007-9-3 loss: 0.480928  [  128/  179]
train() client id: f_00007-9-4 loss: 0.453498  [  160/  179]
train() client id: f_00007-10-0 loss: 0.533702  [   32/  179]
train() client id: f_00007-10-1 loss: 0.520017  [   64/  179]
train() client id: f_00007-10-2 loss: 0.562848  [   96/  179]
train() client id: f_00007-10-3 loss: 0.446220  [  128/  179]
train() client id: f_00007-10-4 loss: 0.459800  [  160/  179]
train() client id: f_00007-11-0 loss: 0.471812  [   32/  179]
train() client id: f_00007-11-1 loss: 0.378523  [   64/  179]
train() client id: f_00007-11-2 loss: 0.448239  [   96/  179]
train() client id: f_00007-11-3 loss: 0.757674  [  128/  179]
train() client id: f_00007-11-4 loss: 0.470563  [  160/  179]
train() client id: f_00008-0-0 loss: 0.709450  [   32/  130]
train() client id: f_00008-0-1 loss: 0.749323  [   64/  130]
train() client id: f_00008-0-2 loss: 0.833141  [   96/  130]
train() client id: f_00008-0-3 loss: 0.807937  [  128/  130]
train() client id: f_00008-1-0 loss: 0.779430  [   32/  130]
train() client id: f_00008-1-1 loss: 0.798591  [   64/  130]
train() client id: f_00008-1-2 loss: 0.642505  [   96/  130]
train() client id: f_00008-1-3 loss: 0.867200  [  128/  130]
train() client id: f_00008-2-0 loss: 0.653011  [   32/  130]
train() client id: f_00008-2-1 loss: 0.858818  [   64/  130]
train() client id: f_00008-2-2 loss: 0.669625  [   96/  130]
train() client id: f_00008-2-3 loss: 0.867954  [  128/  130]
train() client id: f_00008-3-0 loss: 0.621923  [   32/  130]
train() client id: f_00008-3-1 loss: 0.806743  [   64/  130]
train() client id: f_00008-3-2 loss: 0.902414  [   96/  130]
train() client id: f_00008-3-3 loss: 0.763524  [  128/  130]
train() client id: f_00008-4-0 loss: 0.703756  [   32/  130]
train() client id: f_00008-4-1 loss: 0.779197  [   64/  130]
train() client id: f_00008-4-2 loss: 0.751411  [   96/  130]
train() client id: f_00008-4-3 loss: 0.826643  [  128/  130]
train() client id: f_00008-5-0 loss: 0.780577  [   32/  130]
train() client id: f_00008-5-1 loss: 0.700065  [   64/  130]
train() client id: f_00008-5-2 loss: 0.858647  [   96/  130]
train() client id: f_00008-5-3 loss: 0.749281  [  128/  130]
train() client id: f_00008-6-0 loss: 0.798064  [   32/  130]
train() client id: f_00008-6-1 loss: 0.766349  [   64/  130]
train() client id: f_00008-6-2 loss: 0.830803  [   96/  130]
train() client id: f_00008-6-3 loss: 0.688298  [  128/  130]
train() client id: f_00008-7-0 loss: 0.725746  [   32/  130]
train() client id: f_00008-7-1 loss: 0.773512  [   64/  130]
train() client id: f_00008-7-2 loss: 0.830810  [   96/  130]
train() client id: f_00008-7-3 loss: 0.749413  [  128/  130]
train() client id: f_00008-8-0 loss: 0.723689  [   32/  130]
train() client id: f_00008-8-1 loss: 0.736839  [   64/  130]
train() client id: f_00008-8-2 loss: 0.752583  [   96/  130]
train() client id: f_00008-8-3 loss: 0.844729  [  128/  130]
train() client id: f_00008-9-0 loss: 0.772211  [   32/  130]
train() client id: f_00008-9-1 loss: 0.718490  [   64/  130]
train() client id: f_00008-9-2 loss: 0.770289  [   96/  130]
train() client id: f_00008-9-3 loss: 0.811938  [  128/  130]
train() client id: f_00008-10-0 loss: 0.786716  [   32/  130]
train() client id: f_00008-10-1 loss: 0.679061  [   64/  130]
train() client id: f_00008-10-2 loss: 0.793238  [   96/  130]
train() client id: f_00008-10-3 loss: 0.812356  [  128/  130]
train() client id: f_00008-11-0 loss: 0.836667  [   32/  130]
train() client id: f_00008-11-1 loss: 0.763073  [   64/  130]
train() client id: f_00008-11-2 loss: 0.728909  [   96/  130]
train() client id: f_00008-11-3 loss: 0.749800  [  128/  130]
train() client id: f_00009-0-0 loss: 1.128513  [   32/  118]
train() client id: f_00009-0-1 loss: 1.103953  [   64/  118]
train() client id: f_00009-0-2 loss: 1.095285  [   96/  118]
train() client id: f_00009-1-0 loss: 1.205820  [   32/  118]
train() client id: f_00009-1-1 loss: 0.989952  [   64/  118]
train() client id: f_00009-1-2 loss: 1.086897  [   96/  118]
train() client id: f_00009-2-0 loss: 0.997681  [   32/  118]
train() client id: f_00009-2-1 loss: 1.067310  [   64/  118]
train() client id: f_00009-2-2 loss: 1.104674  [   96/  118]
train() client id: f_00009-3-0 loss: 1.210411  [   32/  118]
train() client id: f_00009-3-1 loss: 0.910453  [   64/  118]
train() client id: f_00009-3-2 loss: 0.977826  [   96/  118]
train() client id: f_00009-4-0 loss: 1.022578  [   32/  118]
train() client id: f_00009-4-1 loss: 0.963382  [   64/  118]
train() client id: f_00009-4-2 loss: 1.019171  [   96/  118]
train() client id: f_00009-5-0 loss: 0.849348  [   32/  118]
train() client id: f_00009-5-1 loss: 0.936804  [   64/  118]
train() client id: f_00009-5-2 loss: 0.891361  [   96/  118]
train() client id: f_00009-6-0 loss: 0.950498  [   32/  118]
train() client id: f_00009-6-1 loss: 0.937844  [   64/  118]
train() client id: f_00009-6-2 loss: 0.925325  [   96/  118]
train() client id: f_00009-7-0 loss: 0.952790  [   32/  118]
train() client id: f_00009-7-1 loss: 0.759233  [   64/  118]
train() client id: f_00009-7-2 loss: 0.847031  [   96/  118]
train() client id: f_00009-8-0 loss: 0.886301  [   32/  118]
train() client id: f_00009-8-1 loss: 0.842007  [   64/  118]
train() client id: f_00009-8-2 loss: 0.938346  [   96/  118]
train() client id: f_00009-9-0 loss: 0.788135  [   32/  118]
train() client id: f_00009-9-1 loss: 1.090427  [   64/  118]
train() client id: f_00009-9-2 loss: 0.729309  [   96/  118]
train() client id: f_00009-10-0 loss: 0.868894  [   32/  118]
train() client id: f_00009-10-1 loss: 0.774868  [   64/  118]
train() client id: f_00009-10-2 loss: 0.872071  [   96/  118]
train() client id: f_00009-11-0 loss: 0.962588  [   32/  118]
train() client id: f_00009-11-1 loss: 0.886686  [   64/  118]
train() client id: f_00009-11-2 loss: 0.751525  [   96/  118]
At round 24 accuracy: 0.6445623342175066
At round 24 training accuracy: 0.5861837692823608
At round 24 training loss: 0.823649066261299
update_location
xs = [  -3.9056584     4.20031788  140.00902392   18.81129433    0.97929623
    3.95640986 -102.44319194  -81.32485185  124.66397685  -67.06087855]
ys = [ 132.5879595   115.55583871    1.32061395 -102.45517586   94.35018685
   77.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [166.11689008 152.87509454 172.05891665 144.39850365 137.48787866
 126.77024586 143.18344219 128.89688819 160.77865871 120.47063247]
dists_bs = [176.27267203 188.80244235 359.71546295 338.4437803  193.57971242
 203.64303555 191.87268031 197.76978496 338.50113277 202.21609029]
uav_gains = [2.80067787e-11 3.45600080e-11 2.55955797e-11 3.98858645e-11
 4.51021890e-11 5.52599497e-11 4.07401996e-11 5.30073043e-11
 3.04302492e-11 6.27732201e-11]
bs_gains = [5.67513399e-11 4.68244606e-11 7.70208994e-12 9.13548862e-12
 4.36602816e-11 3.78843039e-11 4.47566205e-11 4.11193508e-11
 9.13115535e-12 3.86375962e-11]
Round 25
-------------------------------
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.69254516 15.98293133  7.57645092  2.71961017 18.43551744  8.87729699
  3.37637101 10.83932141  7.98736814  7.20317358]
obj_prev = 90.69058614039777
eta_min = 8.689629583387306e-13	eta_max = 0.9239062523538892
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 21.07280247978385	eta = 0.909090909090909
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 37.266591851058905	eta = 0.5140554102721235
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 29.4469781495679	eta = 0.65056227726107
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 28.03930224522773	eta = 0.683222891778642
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967919357716607	eta = 0.6849666905290988
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967721963314986	eta = 0.684971524980406
eta = 0.684971524980406
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.03122285 0.0656671  0.03072725 0.01065542 0.07582691 0.03617885
 0.01338122 0.04435627 0.03221405 0.02924044]
ene_total = [2.45319609 4.54407619 2.43314225 1.13378689 5.18350126 2.72855748
 1.30141153 3.20547972 2.69013729 2.29443326]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 0 obj = 4.983772760330714
eta = 0.684971524980406
freqs = [41087307.98029104 84478875.62799275 40620710.39935426 13797163.01214605
 97820151.49584162 46948740.71955757 17311339.16633044 56794967.35235866
 45781631.37350148 37912918.15352243]
eta_min = 0.6849715249804152	eta_max = 0.68497152498039
af = 0.02120720489719426	bf = 1.5620059925242489	zeta = 0.023327925386913688	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [3.31926004e-06 2.95119211e-05 3.19280262e-06 1.27733086e-07
 4.56912927e-05 5.02176180e-06 2.52528528e-07 9.01006858e-06
 4.25188234e-06 2.64674311e-06]
ene_total = [1.76365275 1.58149679 1.80108751 1.6294247  1.6082219  1.64868503
 1.62207175 1.53747541 2.37101979 1.64116416]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 1 obj = 4.983772760330463
eta = 0.68497152498039
freqs = [41087307.98029105 84478875.62799287 40620710.39935425 13797163.01214607
 97820151.49584176 46948740.71955761 17311339.16633046 56794967.35235876
 45781631.37350126 37912918.15352247]
Done!
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.27625416e-05 1.13473219e-04 1.22763133e-05 4.91133205e-07
 1.75682838e-04 1.93086541e-05 9.70971182e-07 3.46436777e-05
 1.63484706e-05 1.01767167e-05]
ene_total = [0.00818175 0.0074122  0.00835485 0.00755069 0.00758207 0.00765386
 0.00751697 0.00714989 0.01099878 0.00761225]
At round 25 energy consumption: 0.08001331387073954
At round 25 eta: 0.68497152498039
At round 25 a_n: 19.61895668202144
At round 25 local rounds: 12.390018469521971
At round 25 global rounds: 62.276772538737
gradient difference: 0.5118833184242249
train() client id: f_00000-0-0 loss: 1.308242  [   32/  126]
train() client id: f_00000-0-1 loss: 1.099468  [   64/  126]
train() client id: f_00000-0-2 loss: 1.213192  [   96/  126]
train() client id: f_00000-1-0 loss: 1.116741  [   32/  126]
train() client id: f_00000-1-1 loss: 1.031114  [   64/  126]
train() client id: f_00000-1-2 loss: 1.312547  [   96/  126]
train() client id: f_00000-2-0 loss: 1.040489  [   32/  126]
train() client id: f_00000-2-1 loss: 1.190910  [   64/  126]
train() client id: f_00000-2-2 loss: 0.885870  [   96/  126]
train() client id: f_00000-3-0 loss: 0.958175  [   32/  126]
train() client id: f_00000-3-1 loss: 1.081338  [   64/  126]
train() client id: f_00000-3-2 loss: 0.880900  [   96/  126]
train() client id: f_00000-4-0 loss: 0.937918  [   32/  126]
train() client id: f_00000-4-1 loss: 0.938351  [   64/  126]
train() client id: f_00000-4-2 loss: 0.909793  [   96/  126]
train() client id: f_00000-5-0 loss: 0.860387  [   32/  126]
train() client id: f_00000-5-1 loss: 0.826118  [   64/  126]
train() client id: f_00000-5-2 loss: 0.931773  [   96/  126]
train() client id: f_00000-6-0 loss: 0.831838  [   32/  126]
train() client id: f_00000-6-1 loss: 0.881837  [   64/  126]
train() client id: f_00000-6-2 loss: 0.789566  [   96/  126]
train() client id: f_00000-7-0 loss: 0.795496  [   32/  126]
train() client id: f_00000-7-1 loss: 0.896771  [   64/  126]
train() client id: f_00000-7-2 loss: 0.812534  [   96/  126]
train() client id: f_00000-8-0 loss: 0.880367  [   32/  126]
train() client id: f_00000-8-1 loss: 0.825309  [   64/  126]
train() client id: f_00000-8-2 loss: 0.825809  [   96/  126]
train() client id: f_00000-9-0 loss: 0.857396  [   32/  126]
train() client id: f_00000-9-1 loss: 0.781830  [   64/  126]
train() client id: f_00000-9-2 loss: 0.764050  [   96/  126]
train() client id: f_00000-10-0 loss: 0.818291  [   32/  126]
train() client id: f_00000-10-1 loss: 0.794607  [   64/  126]
train() client id: f_00000-10-2 loss: 0.777895  [   96/  126]
train() client id: f_00000-11-0 loss: 0.803258  [   32/  126]
train() client id: f_00000-11-1 loss: 0.801122  [   64/  126]
train() client id: f_00000-11-2 loss: 0.869145  [   96/  126]
train() client id: f_00001-0-0 loss: 0.428877  [   32/  265]
train() client id: f_00001-0-1 loss: 0.412369  [   64/  265]
train() client id: f_00001-0-2 loss: 0.616256  [   96/  265]
train() client id: f_00001-0-3 loss: 0.502262  [  128/  265]
train() client id: f_00001-0-4 loss: 0.374835  [  160/  265]
train() client id: f_00001-0-5 loss: 0.491522  [  192/  265]
train() client id: f_00001-0-6 loss: 0.422918  [  224/  265]
train() client id: f_00001-0-7 loss: 0.383436  [  256/  265]
train() client id: f_00001-1-0 loss: 0.532627  [   32/  265]
train() client id: f_00001-1-1 loss: 0.410159  [   64/  265]
train() client id: f_00001-1-2 loss: 0.366941  [   96/  265]
train() client id: f_00001-1-3 loss: 0.394007  [  128/  265]
train() client id: f_00001-1-4 loss: 0.519192  [  160/  265]
train() client id: f_00001-1-5 loss: 0.477035  [  192/  265]
train() client id: f_00001-1-6 loss: 0.368049  [  224/  265]
train() client id: f_00001-1-7 loss: 0.416738  [  256/  265]
train() client id: f_00001-2-0 loss: 0.473501  [   32/  265]
train() client id: f_00001-2-1 loss: 0.360345  [   64/  265]
train() client id: f_00001-2-2 loss: 0.605390  [   96/  265]
train() client id: f_00001-2-3 loss: 0.416084  [  128/  265]
train() client id: f_00001-2-4 loss: 0.369380  [  160/  265]
train() client id: f_00001-2-5 loss: 0.496499  [  192/  265]
train() client id: f_00001-2-6 loss: 0.380741  [  224/  265]
train() client id: f_00001-2-7 loss: 0.329077  [  256/  265]
train() client id: f_00001-3-0 loss: 0.520758  [   32/  265]
train() client id: f_00001-3-1 loss: 0.494689  [   64/  265]
train() client id: f_00001-3-2 loss: 0.345847  [   96/  265]
train() client id: f_00001-3-3 loss: 0.467434  [  128/  265]
train() client id: f_00001-3-4 loss: 0.342401  [  160/  265]
train() client id: f_00001-3-5 loss: 0.427064  [  192/  265]
train() client id: f_00001-3-6 loss: 0.439575  [  224/  265]
train() client id: f_00001-3-7 loss: 0.441411  [  256/  265]
train() client id: f_00001-4-0 loss: 0.390941  [   32/  265]
train() client id: f_00001-4-1 loss: 0.348479  [   64/  265]
train() client id: f_00001-4-2 loss: 0.372197  [   96/  265]
train() client id: f_00001-4-3 loss: 0.356377  [  128/  265]
train() client id: f_00001-4-4 loss: 0.611704  [  160/  265]
train() client id: f_00001-4-5 loss: 0.446410  [  192/  265]
train() client id: f_00001-4-6 loss: 0.344915  [  224/  265]
train() client id: f_00001-4-7 loss: 0.457750  [  256/  265]
train() client id: f_00001-5-0 loss: 0.413286  [   32/  265]
train() client id: f_00001-5-1 loss: 0.458289  [   64/  265]
train() client id: f_00001-5-2 loss: 0.512535  [   96/  265]
train() client id: f_00001-5-3 loss: 0.328153  [  128/  265]
train() client id: f_00001-5-4 loss: 0.377535  [  160/  265]
train() client id: f_00001-5-5 loss: 0.325996  [  192/  265]
train() client id: f_00001-5-6 loss: 0.334409  [  224/  265]
train() client id: f_00001-5-7 loss: 0.552438  [  256/  265]
train() client id: f_00001-6-0 loss: 0.344097  [   32/  265]
train() client id: f_00001-6-1 loss: 0.332341  [   64/  265]
train() client id: f_00001-6-2 loss: 0.540835  [   96/  265]
train() client id: f_00001-6-3 loss: 0.408875  [  128/  265]
train() client id: f_00001-6-4 loss: 0.329879  [  160/  265]
train() client id: f_00001-6-5 loss: 0.339221  [  192/  265]
train() client id: f_00001-6-6 loss: 0.470692  [  224/  265]
train() client id: f_00001-6-7 loss: 0.552728  [  256/  265]
train() client id: f_00001-7-0 loss: 0.593324  [   32/  265]
train() client id: f_00001-7-1 loss: 0.327124  [   64/  265]
train() client id: f_00001-7-2 loss: 0.313635  [   96/  265]
train() client id: f_00001-7-3 loss: 0.333168  [  128/  265]
train() client id: f_00001-7-4 loss: 0.450775  [  160/  265]
train() client id: f_00001-7-5 loss: 0.348080  [  192/  265]
train() client id: f_00001-7-6 loss: 0.393026  [  224/  265]
train() client id: f_00001-7-7 loss: 0.635965  [  256/  265]
train() client id: f_00001-8-0 loss: 0.331443  [   32/  265]
train() client id: f_00001-8-1 loss: 0.367429  [   64/  265]
train() client id: f_00001-8-2 loss: 0.526626  [   96/  265]
train() client id: f_00001-8-3 loss: 0.459292  [  128/  265]
train() client id: f_00001-8-4 loss: 0.354759  [  160/  265]
train() client id: f_00001-8-5 loss: 0.442639  [  192/  265]
train() client id: f_00001-8-6 loss: 0.368476  [  224/  265]
train() client id: f_00001-8-7 loss: 0.443865  [  256/  265]
train() client id: f_00001-9-0 loss: 0.501309  [   32/  265]
train() client id: f_00001-9-1 loss: 0.345493  [   64/  265]
train() client id: f_00001-9-2 loss: 0.463349  [   96/  265]
train() client id: f_00001-9-3 loss: 0.492091  [  128/  265]
train() client id: f_00001-9-4 loss: 0.434965  [  160/  265]
train() client id: f_00001-9-5 loss: 0.356752  [  192/  265]
train() client id: f_00001-9-6 loss: 0.466559  [  224/  265]
train() client id: f_00001-9-7 loss: 0.321978  [  256/  265]
train() client id: f_00001-10-0 loss: 0.339662  [   32/  265]
train() client id: f_00001-10-1 loss: 0.369069  [   64/  265]
train() client id: f_00001-10-2 loss: 0.504103  [   96/  265]
train() client id: f_00001-10-3 loss: 0.395681  [  128/  265]
train() client id: f_00001-10-4 loss: 0.463323  [  160/  265]
train() client id: f_00001-10-5 loss: 0.522939  [  192/  265]
train() client id: f_00001-10-6 loss: 0.410025  [  224/  265]
train() client id: f_00001-10-7 loss: 0.381136  [  256/  265]
train() client id: f_00001-11-0 loss: 0.370440  [   32/  265]
train() client id: f_00001-11-1 loss: 0.340503  [   64/  265]
train() client id: f_00001-11-2 loss: 0.436512  [   96/  265]
train() client id: f_00001-11-3 loss: 0.511734  [  128/  265]
train() client id: f_00001-11-4 loss: 0.480897  [  160/  265]
train() client id: f_00001-11-5 loss: 0.440905  [  192/  265]
train() client id: f_00001-11-6 loss: 0.358784  [  224/  265]
train() client id: f_00001-11-7 loss: 0.368964  [  256/  265]
train() client id: f_00002-0-0 loss: 1.370517  [   32/  124]
train() client id: f_00002-0-1 loss: 1.228993  [   64/  124]
train() client id: f_00002-0-2 loss: 1.401119  [   96/  124]
train() client id: f_00002-1-0 loss: 1.251490  [   32/  124]
train() client id: f_00002-1-1 loss: 1.185862  [   64/  124]
train() client id: f_00002-1-2 loss: 1.326484  [   96/  124]
train() client id: f_00002-2-0 loss: 1.307932  [   32/  124]
train() client id: f_00002-2-1 loss: 1.095210  [   64/  124]
train() client id: f_00002-2-2 loss: 1.218002  [   96/  124]
train() client id: f_00002-3-0 loss: 1.204770  [   32/  124]
train() client id: f_00002-3-1 loss: 1.169503  [   64/  124]
train() client id: f_00002-3-2 loss: 1.323793  [   96/  124]
train() client id: f_00002-4-0 loss: 1.115409  [   32/  124]
train() client id: f_00002-4-1 loss: 1.328372  [   64/  124]
train() client id: f_00002-4-2 loss: 1.226302  [   96/  124]
train() client id: f_00002-5-0 loss: 1.098230  [   32/  124]
train() client id: f_00002-5-1 loss: 1.368675  [   64/  124]
train() client id: f_00002-5-2 loss: 1.113165  [   96/  124]
train() client id: f_00002-6-0 loss: 1.242235  [   32/  124]
train() client id: f_00002-6-1 loss: 1.115021  [   64/  124]
train() client id: f_00002-6-2 loss: 1.150118  [   96/  124]
train() client id: f_00002-7-0 loss: 1.142583  [   32/  124]
train() client id: f_00002-7-1 loss: 1.153761  [   64/  124]
train() client id: f_00002-7-2 loss: 1.151760  [   96/  124]
train() client id: f_00002-8-0 loss: 1.083434  [   32/  124]
train() client id: f_00002-8-1 loss: 1.411970  [   64/  124]
train() client id: f_00002-8-2 loss: 0.928093  [   96/  124]
train() client id: f_00002-9-0 loss: 1.247792  [   32/  124]
train() client id: f_00002-9-1 loss: 1.153345  [   64/  124]
train() client id: f_00002-9-2 loss: 1.055819  [   96/  124]
train() client id: f_00002-10-0 loss: 1.193053  [   32/  124]
train() client id: f_00002-10-1 loss: 1.067171  [   64/  124]
train() client id: f_00002-10-2 loss: 1.196004  [   96/  124]
train() client id: f_00002-11-0 loss: 0.926962  [   32/  124]
train() client id: f_00002-11-1 loss: 1.305986  [   64/  124]
train() client id: f_00002-11-2 loss: 1.105280  [   96/  124]
train() client id: f_00003-0-0 loss: 0.626983  [   32/   43]
train() client id: f_00003-1-0 loss: 0.666498  [   32/   43]
train() client id: f_00003-2-0 loss: 0.630724  [   32/   43]
train() client id: f_00003-3-0 loss: 0.745585  [   32/   43]
train() client id: f_00003-4-0 loss: 0.724185  [   32/   43]
train() client id: f_00003-5-0 loss: 0.651116  [   32/   43]
train() client id: f_00003-6-0 loss: 0.568004  [   32/   43]
train() client id: f_00003-7-0 loss: 0.720990  [   32/   43]
train() client id: f_00003-8-0 loss: 0.526197  [   32/   43]
train() client id: f_00003-9-0 loss: 0.609526  [   32/   43]
train() client id: f_00003-10-0 loss: 0.540367  [   32/   43]
train() client id: f_00003-11-0 loss: 0.698203  [   32/   43]
train() client id: f_00004-0-0 loss: 1.014885  [   32/  306]
train() client id: f_00004-0-1 loss: 0.989607  [   64/  306]
train() client id: f_00004-0-2 loss: 0.888761  [   96/  306]
train() client id: f_00004-0-3 loss: 0.944770  [  128/  306]
train() client id: f_00004-0-4 loss: 0.997176  [  160/  306]
train() client id: f_00004-0-5 loss: 0.998384  [  192/  306]
train() client id: f_00004-0-6 loss: 0.864055  [  224/  306]
train() client id: f_00004-0-7 loss: 1.010873  [  256/  306]
train() client id: f_00004-0-8 loss: 0.943175  [  288/  306]
train() client id: f_00004-1-0 loss: 0.931629  [   32/  306]
train() client id: f_00004-1-1 loss: 1.115529  [   64/  306]
train() client id: f_00004-1-2 loss: 1.100478  [   96/  306]
train() client id: f_00004-1-3 loss: 0.840537  [  128/  306]
train() client id: f_00004-1-4 loss: 0.833986  [  160/  306]
train() client id: f_00004-1-5 loss: 0.845843  [  192/  306]
train() client id: f_00004-1-6 loss: 1.020886  [  224/  306]
train() client id: f_00004-1-7 loss: 0.913225  [  256/  306]
train() client id: f_00004-1-8 loss: 0.887768  [  288/  306]
train() client id: f_00004-2-0 loss: 0.883432  [   32/  306]
train() client id: f_00004-2-1 loss: 0.953474  [   64/  306]
train() client id: f_00004-2-2 loss: 0.859761  [   96/  306]
train() client id: f_00004-2-3 loss: 0.956973  [  128/  306]
train() client id: f_00004-2-4 loss: 0.932190  [  160/  306]
train() client id: f_00004-2-5 loss: 1.002291  [  192/  306]
train() client id: f_00004-2-6 loss: 1.119317  [  224/  306]
train() client id: f_00004-2-7 loss: 0.845170  [  256/  306]
train() client id: f_00004-2-8 loss: 0.898221  [  288/  306]
train() client id: f_00004-3-0 loss: 0.873502  [   32/  306]
train() client id: f_00004-3-1 loss: 0.851352  [   64/  306]
train() client id: f_00004-3-2 loss: 0.909038  [   96/  306]
train() client id: f_00004-3-3 loss: 0.973264  [  128/  306]
train() client id: f_00004-3-4 loss: 1.027548  [  160/  306]
train() client id: f_00004-3-5 loss: 0.967825  [  192/  306]
train() client id: f_00004-3-6 loss: 0.999806  [  224/  306]
train() client id: f_00004-3-7 loss: 1.026215  [  256/  306]
train() client id: f_00004-3-8 loss: 0.866658  [  288/  306]
train() client id: f_00004-4-0 loss: 1.006671  [   32/  306]
train() client id: f_00004-4-1 loss: 0.805505  [   64/  306]
train() client id: f_00004-4-2 loss: 0.857595  [   96/  306]
train() client id: f_00004-4-3 loss: 0.950868  [  128/  306]
train() client id: f_00004-4-4 loss: 1.105599  [  160/  306]
train() client id: f_00004-4-5 loss: 0.950165  [  192/  306]
train() client id: f_00004-4-6 loss: 0.979692  [  224/  306]
train() client id: f_00004-4-7 loss: 0.925006  [  256/  306]
train() client id: f_00004-4-8 loss: 0.893932  [  288/  306]
train() client id: f_00004-5-0 loss: 1.067644  [   32/  306]
train() client id: f_00004-5-1 loss: 0.924993  [   64/  306]
train() client id: f_00004-5-2 loss: 0.985893  [   96/  306]
train() client id: f_00004-5-3 loss: 0.937846  [  128/  306]
train() client id: f_00004-5-4 loss: 0.953626  [  160/  306]
train() client id: f_00004-5-5 loss: 0.846807  [  192/  306]
train() client id: f_00004-5-6 loss: 0.866338  [  224/  306]
train() client id: f_00004-5-7 loss: 0.889556  [  256/  306]
train() client id: f_00004-5-8 loss: 0.925393  [  288/  306]
train() client id: f_00004-6-0 loss: 0.958079  [   32/  306]
train() client id: f_00004-6-1 loss: 0.976276  [   64/  306]
train() client id: f_00004-6-2 loss: 0.917529  [   96/  306]
train() client id: f_00004-6-3 loss: 0.909924  [  128/  306]
train() client id: f_00004-6-4 loss: 0.942239  [  160/  306]
train() client id: f_00004-6-5 loss: 0.894244  [  192/  306]
train() client id: f_00004-6-6 loss: 0.992384  [  224/  306]
train() client id: f_00004-6-7 loss: 0.963540  [  256/  306]
train() client id: f_00004-6-8 loss: 0.911847  [  288/  306]
train() client id: f_00004-7-0 loss: 1.015268  [   32/  306]
train() client id: f_00004-7-1 loss: 0.869359  [   64/  306]
train() client id: f_00004-7-2 loss: 0.927786  [   96/  306]
train() client id: f_00004-7-3 loss: 0.924565  [  128/  306]
train() client id: f_00004-7-4 loss: 1.032792  [  160/  306]
train() client id: f_00004-7-5 loss: 0.918971  [  192/  306]
train() client id: f_00004-7-6 loss: 0.852919  [  224/  306]
train() client id: f_00004-7-7 loss: 0.958960  [  256/  306]
train() client id: f_00004-7-8 loss: 0.945883  [  288/  306]
train() client id: f_00004-8-0 loss: 0.895270  [   32/  306]
train() client id: f_00004-8-1 loss: 0.968241  [   64/  306]
train() client id: f_00004-8-2 loss: 0.862041  [   96/  306]
train() client id: f_00004-8-3 loss: 0.895089  [  128/  306]
train() client id: f_00004-8-4 loss: 1.124637  [  160/  306]
train() client id: f_00004-8-5 loss: 0.830154  [  192/  306]
train() client id: f_00004-8-6 loss: 0.935283  [  224/  306]
train() client id: f_00004-8-7 loss: 1.042087  [  256/  306]
train() client id: f_00004-8-8 loss: 0.942756  [  288/  306]
train() client id: f_00004-9-0 loss: 0.981099  [   32/  306]
train() client id: f_00004-9-1 loss: 1.033253  [   64/  306]
train() client id: f_00004-9-2 loss: 0.974537  [   96/  306]
train() client id: f_00004-9-3 loss: 0.880626  [  128/  306]
train() client id: f_00004-9-4 loss: 0.958118  [  160/  306]
train() client id: f_00004-9-5 loss: 0.907682  [  192/  306]
train() client id: f_00004-9-6 loss: 0.815592  [  224/  306]
train() client id: f_00004-9-7 loss: 0.901926  [  256/  306]
train() client id: f_00004-9-8 loss: 0.967020  [  288/  306]
train() client id: f_00004-10-0 loss: 0.937439  [   32/  306]
train() client id: f_00004-10-1 loss: 0.848264  [   64/  306]
train() client id: f_00004-10-2 loss: 0.965626  [   96/  306]
train() client id: f_00004-10-3 loss: 0.925370  [  128/  306]
train() client id: f_00004-10-4 loss: 0.969017  [  160/  306]
train() client id: f_00004-10-5 loss: 0.980799  [  192/  306]
train() client id: f_00004-10-6 loss: 0.961022  [  224/  306]
train() client id: f_00004-10-7 loss: 0.826593  [  256/  306]
train() client id: f_00004-10-8 loss: 0.828539  [  288/  306]
train() client id: f_00004-11-0 loss: 0.975617  [   32/  306]
train() client id: f_00004-11-1 loss: 0.956239  [   64/  306]
train() client id: f_00004-11-2 loss: 0.896757  [   96/  306]
train() client id: f_00004-11-3 loss: 0.859597  [  128/  306]
train() client id: f_00004-11-4 loss: 0.863408  [  160/  306]
train() client id: f_00004-11-5 loss: 0.939314  [  192/  306]
train() client id: f_00004-11-6 loss: 1.010146  [  224/  306]
train() client id: f_00004-11-7 loss: 0.826592  [  256/  306]
train() client id: f_00004-11-8 loss: 1.019401  [  288/  306]
train() client id: f_00005-0-0 loss: 0.685802  [   32/  146]
train() client id: f_00005-0-1 loss: 0.707293  [   64/  146]
train() client id: f_00005-0-2 loss: 0.926463  [   96/  146]
train() client id: f_00005-0-3 loss: 0.654899  [  128/  146]
train() client id: f_00005-1-0 loss: 0.798440  [   32/  146]
train() client id: f_00005-1-1 loss: 0.624164  [   64/  146]
train() client id: f_00005-1-2 loss: 0.711481  [   96/  146]
train() client id: f_00005-1-3 loss: 0.693757  [  128/  146]
train() client id: f_00005-2-0 loss: 0.718797  [   32/  146]
train() client id: f_00005-2-1 loss: 0.687211  [   64/  146]
train() client id: f_00005-2-2 loss: 0.982550  [   96/  146]
train() client id: f_00005-2-3 loss: 0.611301  [  128/  146]
train() client id: f_00005-3-0 loss: 0.880523  [   32/  146]
train() client id: f_00005-3-1 loss: 0.945293  [   64/  146]
train() client id: f_00005-3-2 loss: 0.637674  [   96/  146]
train() client id: f_00005-3-3 loss: 0.611743  [  128/  146]
train() client id: f_00005-4-0 loss: 0.768745  [   32/  146]
train() client id: f_00005-4-1 loss: 1.047134  [   64/  146]
train() client id: f_00005-4-2 loss: 0.676500  [   96/  146]
train() client id: f_00005-4-3 loss: 0.727434  [  128/  146]
train() client id: f_00005-5-0 loss: 0.747519  [   32/  146]
train() client id: f_00005-5-1 loss: 0.709804  [   64/  146]
train() client id: f_00005-5-2 loss: 0.742213  [   96/  146]
train() client id: f_00005-5-3 loss: 0.617666  [  128/  146]
train() client id: f_00005-6-0 loss: 0.789109  [   32/  146]
train() client id: f_00005-6-1 loss: 0.807424  [   64/  146]
train() client id: f_00005-6-2 loss: 0.640555  [   96/  146]
train() client id: f_00005-6-3 loss: 0.628200  [  128/  146]
train() client id: f_00005-7-0 loss: 0.620470  [   32/  146]
train() client id: f_00005-7-1 loss: 0.977589  [   64/  146]
train() client id: f_00005-7-2 loss: 0.716905  [   96/  146]
train() client id: f_00005-7-3 loss: 0.619712  [  128/  146]
train() client id: f_00005-8-0 loss: 0.708459  [   32/  146]
train() client id: f_00005-8-1 loss: 0.946990  [   64/  146]
train() client id: f_00005-8-2 loss: 0.515661  [   96/  146]
train() client id: f_00005-8-3 loss: 0.863732  [  128/  146]
train() client id: f_00005-9-0 loss: 0.853829  [   32/  146]
train() client id: f_00005-9-1 loss: 0.474513  [   64/  146]
train() client id: f_00005-9-2 loss: 0.801986  [   96/  146]
train() client id: f_00005-9-3 loss: 0.911230  [  128/  146]
train() client id: f_00005-10-0 loss: 0.941971  [   32/  146]
train() client id: f_00005-10-1 loss: 0.531782  [   64/  146]
train() client id: f_00005-10-2 loss: 0.799129  [   96/  146]
train() client id: f_00005-10-3 loss: 0.735384  [  128/  146]
train() client id: f_00005-11-0 loss: 0.709405  [   32/  146]
train() client id: f_00005-11-1 loss: 0.710980  [   64/  146]
train() client id: f_00005-11-2 loss: 0.929869  [   96/  146]
train() client id: f_00005-11-3 loss: 0.776733  [  128/  146]
train() client id: f_00006-0-0 loss: 0.569771  [   32/   54]
train() client id: f_00006-1-0 loss: 0.645570  [   32/   54]
train() client id: f_00006-2-0 loss: 0.639928  [   32/   54]
train() client id: f_00006-3-0 loss: 0.540216  [   32/   54]
train() client id: f_00006-4-0 loss: 0.628864  [   32/   54]
train() client id: f_00006-5-0 loss: 0.646919  [   32/   54]
train() client id: f_00006-6-0 loss: 0.613258  [   32/   54]
train() client id: f_00006-7-0 loss: 0.650426  [   32/   54]
train() client id: f_00006-8-0 loss: 0.576727  [   32/   54]
train() client id: f_00006-9-0 loss: 0.599177  [   32/   54]
train() client id: f_00006-10-0 loss: 0.573696  [   32/   54]
train() client id: f_00006-11-0 loss: 0.595118  [   32/   54]
train() client id: f_00007-0-0 loss: 0.649356  [   32/  179]
train() client id: f_00007-0-1 loss: 0.675416  [   64/  179]
train() client id: f_00007-0-2 loss: 0.569133  [   96/  179]
train() client id: f_00007-0-3 loss: 0.609802  [  128/  179]
train() client id: f_00007-0-4 loss: 0.494893  [  160/  179]
train() client id: f_00007-1-0 loss: 0.574767  [   32/  179]
train() client id: f_00007-1-1 loss: 0.683483  [   64/  179]
train() client id: f_00007-1-2 loss: 0.536149  [   96/  179]
train() client id: f_00007-1-3 loss: 0.717838  [  128/  179]
train() client id: f_00007-1-4 loss: 0.514045  [  160/  179]
train() client id: f_00007-2-0 loss: 0.580874  [   32/  179]
train() client id: f_00007-2-1 loss: 0.596556  [   64/  179]
train() client id: f_00007-2-2 loss: 0.575229  [   96/  179]
train() client id: f_00007-2-3 loss: 0.755616  [  128/  179]
train() client id: f_00007-2-4 loss: 0.579862  [  160/  179]
train() client id: f_00007-3-0 loss: 0.626982  [   32/  179]
train() client id: f_00007-3-1 loss: 0.455023  [   64/  179]
train() client id: f_00007-3-2 loss: 0.792794  [   96/  179]
train() client id: f_00007-3-3 loss: 0.466714  [  128/  179]
train() client id: f_00007-3-4 loss: 0.601516  [  160/  179]
train() client id: f_00007-4-0 loss: 0.730066  [   32/  179]
train() client id: f_00007-4-1 loss: 0.647870  [   64/  179]
train() client id: f_00007-4-2 loss: 0.463334  [   96/  179]
train() client id: f_00007-4-3 loss: 0.589901  [  128/  179]
train() client id: f_00007-4-4 loss: 0.477340  [  160/  179]
train() client id: f_00007-5-0 loss: 0.545211  [   32/  179]
train() client id: f_00007-5-1 loss: 0.570108  [   64/  179]
train() client id: f_00007-5-2 loss: 0.776538  [   96/  179]
train() client id: f_00007-5-3 loss: 0.552961  [  128/  179]
train() client id: f_00007-5-4 loss: 0.444304  [  160/  179]
train() client id: f_00007-6-0 loss: 0.502523  [   32/  179]
train() client id: f_00007-6-1 loss: 0.619471  [   64/  179]
train() client id: f_00007-6-2 loss: 0.522426  [   96/  179]
train() client id: f_00007-6-3 loss: 0.651701  [  128/  179]
train() client id: f_00007-6-4 loss: 0.682225  [  160/  179]
train() client id: f_00007-7-0 loss: 0.541031  [   32/  179]
train() client id: f_00007-7-1 loss: 0.477029  [   64/  179]
train() client id: f_00007-7-2 loss: 0.422596  [   96/  179]
train() client id: f_00007-7-3 loss: 0.685913  [  128/  179]
train() client id: f_00007-7-4 loss: 0.786208  [  160/  179]
train() client id: f_00007-8-0 loss: 0.593814  [   32/  179]
train() client id: f_00007-8-1 loss: 0.683875  [   64/  179]
train() client id: f_00007-8-2 loss: 0.568057  [   96/  179]
train() client id: f_00007-8-3 loss: 0.565971  [  128/  179]
train() client id: f_00007-8-4 loss: 0.512560  [  160/  179]
train() client id: f_00007-9-0 loss: 0.637421  [   32/  179]
train() client id: f_00007-9-1 loss: 0.758196  [   64/  179]
train() client id: f_00007-9-2 loss: 0.429730  [   96/  179]
train() client id: f_00007-9-3 loss: 0.577660  [  128/  179]
train() client id: f_00007-9-4 loss: 0.458970  [  160/  179]
train() client id: f_00007-10-0 loss: 0.613947  [   32/  179]
train() client id: f_00007-10-1 loss: 0.498897  [   64/  179]
train() client id: f_00007-10-2 loss: 0.572317  [   96/  179]
train() client id: f_00007-10-3 loss: 0.561754  [  128/  179]
train() client id: f_00007-10-4 loss: 0.687149  [  160/  179]
train() client id: f_00007-11-0 loss: 0.589236  [   32/  179]
train() client id: f_00007-11-1 loss: 0.433702  [   64/  179]
train() client id: f_00007-11-2 loss: 0.513564  [   96/  179]
train() client id: f_00007-11-3 loss: 0.668507  [  128/  179]
train() client id: f_00007-11-4 loss: 0.452193  [  160/  179]
train() client id: f_00008-0-0 loss: 0.849354  [   32/  130]
train() client id: f_00008-0-1 loss: 0.764639  [   64/  130]
train() client id: f_00008-0-2 loss: 0.672963  [   96/  130]
train() client id: f_00008-0-3 loss: 0.689616  [  128/  130]
train() client id: f_00008-1-0 loss: 0.865742  [   32/  130]
train() client id: f_00008-1-1 loss: 0.795429  [   64/  130]
train() client id: f_00008-1-2 loss: 0.784497  [   96/  130]
train() client id: f_00008-1-3 loss: 0.600838  [  128/  130]
train() client id: f_00008-2-0 loss: 0.706548  [   32/  130]
train() client id: f_00008-2-1 loss: 0.613347  [   64/  130]
train() client id: f_00008-2-2 loss: 0.754577  [   96/  130]
train() client id: f_00008-2-3 loss: 0.951682  [  128/  130]
train() client id: f_00008-3-0 loss: 0.703239  [   32/  130]
train() client id: f_00008-3-1 loss: 0.846088  [   64/  130]
train() client id: f_00008-3-2 loss: 0.779158  [   96/  130]
train() client id: f_00008-3-3 loss: 0.723661  [  128/  130]
train() client id: f_00008-4-0 loss: 0.693601  [   32/  130]
train() client id: f_00008-4-1 loss: 0.803241  [   64/  130]
train() client id: f_00008-4-2 loss: 0.841253  [   96/  130]
train() client id: f_00008-4-3 loss: 0.709671  [  128/  130]
train() client id: f_00008-5-0 loss: 0.785066  [   32/  130]
train() client id: f_00008-5-1 loss: 0.827923  [   64/  130]
train() client id: f_00008-5-2 loss: 0.660903  [   96/  130]
train() client id: f_00008-5-3 loss: 0.761634  [  128/  130]
train() client id: f_00008-6-0 loss: 0.736988  [   32/  130]
train() client id: f_00008-6-1 loss: 0.832458  [   64/  130]
train() client id: f_00008-6-2 loss: 0.779409  [   96/  130]
train() client id: f_00008-6-3 loss: 0.660709  [  128/  130]
train() client id: f_00008-7-0 loss: 0.765274  [   32/  130]
train() client id: f_00008-7-1 loss: 0.770080  [   64/  130]
train() client id: f_00008-7-2 loss: 0.822318  [   96/  130]
train() client id: f_00008-7-3 loss: 0.693921  [  128/  130]
train() client id: f_00008-8-0 loss: 0.663848  [   32/  130]
train() client id: f_00008-8-1 loss: 0.746554  [   64/  130]
train() client id: f_00008-8-2 loss: 0.831129  [   96/  130]
train() client id: f_00008-8-3 loss: 0.814399  [  128/  130]
train() client id: f_00008-9-0 loss: 0.795875  [   32/  130]
train() client id: f_00008-9-1 loss: 0.681586  [   64/  130]
train() client id: f_00008-9-2 loss: 0.804015  [   96/  130]
train() client id: f_00008-9-3 loss: 0.770957  [  128/  130]
train() client id: f_00008-10-0 loss: 0.682139  [   32/  130]
train() client id: f_00008-10-1 loss: 0.781959  [   64/  130]
train() client id: f_00008-10-2 loss: 0.745104  [   96/  130]
train() client id: f_00008-10-3 loss: 0.843292  [  128/  130]
train() client id: f_00008-11-0 loss: 0.855098  [   32/  130]
train() client id: f_00008-11-1 loss: 0.682641  [   64/  130]
train() client id: f_00008-11-2 loss: 0.730235  [   96/  130]
train() client id: f_00008-11-3 loss: 0.773120  [  128/  130]
train() client id: f_00009-0-0 loss: 1.016557  [   32/  118]
train() client id: f_00009-0-1 loss: 1.018973  [   64/  118]
train() client id: f_00009-0-2 loss: 1.079080  [   96/  118]
train() client id: f_00009-1-0 loss: 1.124663  [   32/  118]
train() client id: f_00009-1-1 loss: 1.028441  [   64/  118]
train() client id: f_00009-1-2 loss: 1.005672  [   96/  118]
train() client id: f_00009-2-0 loss: 0.994767  [   32/  118]
train() client id: f_00009-2-1 loss: 1.063974  [   64/  118]
train() client id: f_00009-2-2 loss: 0.942476  [   96/  118]
train() client id: f_00009-3-0 loss: 1.067837  [   32/  118]
train() client id: f_00009-3-1 loss: 0.987298  [   64/  118]
train() client id: f_00009-3-2 loss: 0.813003  [   96/  118]
train() client id: f_00009-4-0 loss: 0.988208  [   32/  118]
train() client id: f_00009-4-1 loss: 1.023164  [   64/  118]
train() client id: f_00009-4-2 loss: 0.902833  [   96/  118]
train() client id: f_00009-5-0 loss: 0.956207  [   32/  118]
train() client id: f_00009-5-1 loss: 0.978345  [   64/  118]
train() client id: f_00009-5-2 loss: 0.955065  [   96/  118]
train() client id: f_00009-6-0 loss: 1.053860  [   32/  118]
train() client id: f_00009-6-1 loss: 0.797125  [   64/  118]
train() client id: f_00009-6-2 loss: 0.992756  [   96/  118]
train() client id: f_00009-7-0 loss: 0.912455  [   32/  118]
train() client id: f_00009-7-1 loss: 0.999460  [   64/  118]
train() client id: f_00009-7-2 loss: 0.799950  [   96/  118]
train() client id: f_00009-8-0 loss: 0.885612  [   32/  118]
train() client id: f_00009-8-1 loss: 0.964402  [   64/  118]
train() client id: f_00009-8-2 loss: 0.884616  [   96/  118]
train() client id: f_00009-9-0 loss: 0.952149  [   32/  118]
train() client id: f_00009-9-1 loss: 0.928989  [   64/  118]
train() client id: f_00009-9-2 loss: 0.903172  [   96/  118]
train() client id: f_00009-10-0 loss: 0.847771  [   32/  118]
train() client id: f_00009-10-1 loss: 0.920497  [   64/  118]
train() client id: f_00009-10-2 loss: 0.966540  [   96/  118]
train() client id: f_00009-11-0 loss: 1.057967  [   32/  118]
train() client id: f_00009-11-1 loss: 0.839759  [   64/  118]
train() client id: f_00009-11-2 loss: 0.832877  [   96/  118]
At round 25 accuracy: 0.6472148541114059
At round 25 training accuracy: 0.5808182427900738
At round 25 training loss: 0.8398370968598083
update_location
xs = [  -3.9056584     4.20031788  145.00902392   18.81129433    0.97929623
    3.95640986 -107.44319194  -86.32485185  129.66397685  -72.06087855]
ys = [ 137.5879595   120.55583871    1.32061395 -107.45517586   99.35018685
   82.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [170.13436092 156.68871343 176.15152863 147.98810632 140.96601948
 129.89933311 146.80235024 132.10850201 164.68581258 123.32389093]
dists_bs = [175.13690217 187.28833553 364.10205235 342.55473166 191.55053362
 201.30481225 190.03830501 195.4511099  342.93535346 199.59197368]
uav_gains = [2.63464755e-11 3.24787964e-11 2.40854862e-11 3.75018239e-11
 4.23658097e-11 5.19897705e-11 3.82670974e-11 4.98415206e-11
 2.86308105e-11 5.92039294e-11]
bs_gains = [5.77878620e-11 4.78921177e-11 7.44507933e-12 8.83181915e-12
 4.49676985e-11 3.91293331e-11 4.59768137e-11 4.24998396e-11
 8.80439989e-12 4.00768410e-11]
Round 26
-------------------------------
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.5605131  15.70320748  7.4465293   2.67409633 18.11274305  8.72126419
  3.31940453 10.65181908  7.8502879   7.07624586]
obj_prev = 89.11611082472227
eta_min = 5.392428774203226e-13	eta_max = 0.9242899713750001
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 20.70487256941968	eta = 0.909090909090909
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 36.71367954165409	eta = 0.5126865969778287
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 28.972791112910695	eta = 0.6496651065957375
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.57869875098564	eta = 0.6825054219090905
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.5077842638857	eta = 0.6842649064780151
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.50758684227239	eta = 0.6842698174388544
eta = 0.6842698174388544
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.03130742 0.06584498 0.03081048 0.01068428 0.07603231 0.03627686
 0.01341747 0.04447642 0.03230131 0.02931965]
ene_total = [2.4173172  4.46359599 2.39783696 1.11945579 5.09142412 2.67769452
 1.2842835  3.15534052 2.65007964 2.25055859]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 0 obj = 4.913144626268169
eta = 0.6842698174388544
freqs = [40426116.40412594 82838172.3252855  39968031.32913876 13573739.87970061
 95886067.29898961 46005714.64645749 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
eta_min = 0.6842698174388597	eta_max = 0.6842698174388548
af = 0.020050708162629324	bf = 1.5436207515894453	zeta = 0.022055778978892257	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [3.21329018e-06 2.83767235e-05 3.09102528e-06 1.23629712e-07
 4.39023530e-05 4.82205074e-06 2.44433458e-07 8.71936159e-06
 4.10498900e-06 2.54075489e-06]
ene_total = [1.757659   1.54642537 1.79534967 1.62243185 1.57005655 1.60857787
 1.61536282 1.52964443 2.35549529 1.59984681]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 1 obj = 4.913144626268177
eta = 0.6842698174388548
freqs = [40426116.40412594 82838172.32528551 39968031.32913877 13573739.87970061
 95886067.29898961 46005714.64645751 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
Done!
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.23550879e-05 1.09108388e-04 1.18849799e-05 4.75355749e-07
 1.68804372e-04 1.85407658e-05 9.39845672e-07 3.35259105e-05
 1.57836663e-05 9.76919241e-06]
ene_total = [0.0082984  0.0073738  0.00847581 0.00765187 0.00752941 0.0075999
 0.00761888 0.00723873 0.01112038 0.00755223]
At round 26 energy consumption: 0.08045941191860002
At round 26 eta: 0.6842698174388548
At round 26 a_n: 19.276410835052125
At round 26 local rounds: 12.423580809938418
At round 26 global rounds: 61.05343074483861
gradient difference: 0.45517420768737793
train() client id: f_00000-0-0 loss: 1.265470  [   32/  126]
train() client id: f_00000-0-1 loss: 1.016255  [   64/  126]
train() client id: f_00000-0-2 loss: 1.180894  [   96/  126]
train() client id: f_00000-1-0 loss: 1.064658  [   32/  126]
train() client id: f_00000-1-1 loss: 1.210221  [   64/  126]
train() client id: f_00000-1-2 loss: 0.974988  [   96/  126]
train() client id: f_00000-2-0 loss: 1.087197  [   32/  126]
train() client id: f_00000-2-1 loss: 0.900072  [   64/  126]
train() client id: f_00000-2-2 loss: 1.032820  [   96/  126]
train() client id: f_00000-3-0 loss: 0.914939  [   32/  126]
train() client id: f_00000-3-1 loss: 0.998331  [   64/  126]
train() client id: f_00000-3-2 loss: 0.868930  [   96/  126]
train() client id: f_00000-4-0 loss: 0.926889  [   32/  126]
train() client id: f_00000-4-1 loss: 0.910346  [   64/  126]
train() client id: f_00000-4-2 loss: 0.765866  [   96/  126]
train() client id: f_00000-5-0 loss: 0.947635  [   32/  126]
train() client id: f_00000-5-1 loss: 0.803369  [   64/  126]
train() client id: f_00000-5-2 loss: 0.819503  [   96/  126]
train() client id: f_00000-6-0 loss: 0.806269  [   32/  126]
train() client id: f_00000-6-1 loss: 0.798396  [   64/  126]
train() client id: f_00000-6-2 loss: 0.828568  [   96/  126]
train() client id: f_00000-7-0 loss: 0.845041  [   32/  126]
train() client id: f_00000-7-1 loss: 0.797875  [   64/  126]
train() client id: f_00000-7-2 loss: 0.786175  [   96/  126]
train() client id: f_00000-8-0 loss: 0.768279  [   32/  126]
train() client id: f_00000-8-1 loss: 0.802276  [   64/  126]
train() client id: f_00000-8-2 loss: 0.840250  [   96/  126]
train() client id: f_00000-9-0 loss: 0.897941  [   32/  126]
train() client id: f_00000-9-1 loss: 0.857792  [   64/  126]
train() client id: f_00000-9-2 loss: 0.797904  [   96/  126]
train() client id: f_00000-10-0 loss: 0.816379  [   32/  126]
train() client id: f_00000-10-1 loss: 0.868612  [   64/  126]
train() client id: f_00000-10-2 loss: 0.734962  [   96/  126]
train() client id: f_00000-11-0 loss: 0.860479  [   32/  126]
train() client id: f_00000-11-1 loss: 0.716426  [   64/  126]
train() client id: f_00000-11-2 loss: 0.800628  [   96/  126]
train() client id: f_00001-0-0 loss: 0.441167  [   32/  265]
train() client id: f_00001-0-1 loss: 0.533539  [   64/  265]
train() client id: f_00001-0-2 loss: 0.505537  [   96/  265]
train() client id: f_00001-0-3 loss: 0.556334  [  128/  265]
train() client id: f_00001-0-4 loss: 0.425629  [  160/  265]
train() client id: f_00001-0-5 loss: 0.380489  [  192/  265]
train() client id: f_00001-0-6 loss: 0.520138  [  224/  265]
train() client id: f_00001-0-7 loss: 0.407405  [  256/  265]
train() client id: f_00001-1-0 loss: 0.428932  [   32/  265]
train() client id: f_00001-1-1 loss: 0.459942  [   64/  265]
train() client id: f_00001-1-2 loss: 0.385495  [   96/  265]
train() client id: f_00001-1-3 loss: 0.631179  [  128/  265]
train() client id: f_00001-1-4 loss: 0.520715  [  160/  265]
train() client id: f_00001-1-5 loss: 0.378183  [  192/  265]
train() client id: f_00001-1-6 loss: 0.376038  [  224/  265]
train() client id: f_00001-1-7 loss: 0.522479  [  256/  265]
train() client id: f_00001-2-0 loss: 0.510908  [   32/  265]
train() client id: f_00001-2-1 loss: 0.498829  [   64/  265]
train() client id: f_00001-2-2 loss: 0.520529  [   96/  265]
train() client id: f_00001-2-3 loss: 0.379185  [  128/  265]
train() client id: f_00001-2-4 loss: 0.402896  [  160/  265]
train() client id: f_00001-2-5 loss: 0.529676  [  192/  265]
train() client id: f_00001-2-6 loss: 0.427276  [  224/  265]
train() client id: f_00001-2-7 loss: 0.423817  [  256/  265]
train() client id: f_00001-3-0 loss: 0.492386  [   32/  265]
train() client id: f_00001-3-1 loss: 0.514527  [   64/  265]
train() client id: f_00001-3-2 loss: 0.450376  [   96/  265]
train() client id: f_00001-3-3 loss: 0.378165  [  128/  265]
train() client id: f_00001-3-4 loss: 0.435111  [  160/  265]
train() client id: f_00001-3-5 loss: 0.482723  [  192/  265]
train() client id: f_00001-3-6 loss: 0.518546  [  224/  265]
train() client id: f_00001-3-7 loss: 0.351535  [  256/  265]
train() client id: f_00001-4-0 loss: 0.490227  [   32/  265]
train() client id: f_00001-4-1 loss: 0.434759  [   64/  265]
train() client id: f_00001-4-2 loss: 0.446097  [   96/  265]
train() client id: f_00001-4-3 loss: 0.581027  [  128/  265]
train() client id: f_00001-4-4 loss: 0.497372  [  160/  265]
train() client id: f_00001-4-5 loss: 0.350776  [  192/  265]
train() client id: f_00001-4-6 loss: 0.371926  [  224/  265]
train() client id: f_00001-4-7 loss: 0.376258  [  256/  265]
train() client id: f_00001-5-0 loss: 0.433977  [   32/  265]
train() client id: f_00001-5-1 loss: 0.395240  [   64/  265]
train() client id: f_00001-5-2 loss: 0.443101  [   96/  265]
train() client id: f_00001-5-3 loss: 0.482987  [  128/  265]
train() client id: f_00001-5-4 loss: 0.507023  [  160/  265]
train() client id: f_00001-5-5 loss: 0.574642  [  192/  265]
train() client id: f_00001-5-6 loss: 0.362117  [  224/  265]
train() client id: f_00001-5-7 loss: 0.412173  [  256/  265]
train() client id: f_00001-6-0 loss: 0.479733  [   32/  265]
train() client id: f_00001-6-1 loss: 0.368075  [   64/  265]
train() client id: f_00001-6-2 loss: 0.361773  [   96/  265]
train() client id: f_00001-6-3 loss: 0.558188  [  128/  265]
train() client id: f_00001-6-4 loss: 0.425587  [  160/  265]
train() client id: f_00001-6-5 loss: 0.424726  [  192/  265]
train() client id: f_00001-6-6 loss: 0.546875  [  224/  265]
train() client id: f_00001-6-7 loss: 0.419347  [  256/  265]
train() client id: f_00001-7-0 loss: 0.454138  [   32/  265]
train() client id: f_00001-7-1 loss: 0.429772  [   64/  265]
train() client id: f_00001-7-2 loss: 0.493665  [   96/  265]
train() client id: f_00001-7-3 loss: 0.440203  [  128/  265]
train() client id: f_00001-7-4 loss: 0.439396  [  160/  265]
train() client id: f_00001-7-5 loss: 0.434625  [  192/  265]
train() client id: f_00001-7-6 loss: 0.502775  [  224/  265]
train() client id: f_00001-7-7 loss: 0.379133  [  256/  265]
train() client id: f_00001-8-0 loss: 0.370267  [   32/  265]
train() client id: f_00001-8-1 loss: 0.339700  [   64/  265]
train() client id: f_00001-8-2 loss: 0.384350  [   96/  265]
train() client id: f_00001-8-3 loss: 0.459393  [  128/  265]
train() client id: f_00001-8-4 loss: 0.409118  [  160/  265]
train() client id: f_00001-8-5 loss: 0.528389  [  192/  265]
train() client id: f_00001-8-6 loss: 0.392847  [  224/  265]
train() client id: f_00001-8-7 loss: 0.608955  [  256/  265]
train() client id: f_00001-9-0 loss: 0.445810  [   32/  265]
train() client id: f_00001-9-1 loss: 0.409649  [   64/  265]
train() client id: f_00001-9-2 loss: 0.420912  [   96/  265]
train() client id: f_00001-9-3 loss: 0.400738  [  128/  265]
train() client id: f_00001-9-4 loss: 0.602881  [  160/  265]
train() client id: f_00001-9-5 loss: 0.384939  [  192/  265]
train() client id: f_00001-9-6 loss: 0.412270  [  224/  265]
train() client id: f_00001-9-7 loss: 0.479707  [  256/  265]
train() client id: f_00001-10-0 loss: 0.403074  [   32/  265]
train() client id: f_00001-10-1 loss: 0.783307  [   64/  265]
train() client id: f_00001-10-2 loss: 0.354241  [   96/  265]
train() client id: f_00001-10-3 loss: 0.407398  [  128/  265]
train() client id: f_00001-10-4 loss: 0.463709  [  160/  265]
train() client id: f_00001-10-5 loss: 0.402763  [  192/  265]
train() client id: f_00001-10-6 loss: 0.340757  [  224/  265]
train() client id: f_00001-10-7 loss: 0.403945  [  256/  265]
train() client id: f_00001-11-0 loss: 0.342896  [   32/  265]
train() client id: f_00001-11-1 loss: 0.444103  [   64/  265]
train() client id: f_00001-11-2 loss: 0.404603  [   96/  265]
train() client id: f_00001-11-3 loss: 0.377297  [  128/  265]
train() client id: f_00001-11-4 loss: 0.343705  [  160/  265]
train() client id: f_00001-11-5 loss: 0.447748  [  192/  265]
train() client id: f_00001-11-6 loss: 0.475599  [  224/  265]
train() client id: f_00001-11-7 loss: 0.547403  [  256/  265]
train() client id: f_00002-0-0 loss: 1.163449  [   32/  124]
train() client id: f_00002-0-1 loss: 1.308475  [   64/  124]
train() client id: f_00002-0-2 loss: 0.970954  [   96/  124]
train() client id: f_00002-1-0 loss: 1.062823  [   32/  124]
train() client id: f_00002-1-1 loss: 1.120297  [   64/  124]
train() client id: f_00002-1-2 loss: 1.136896  [   96/  124]
train() client id: f_00002-2-0 loss: 1.171636  [   32/  124]
train() client id: f_00002-2-1 loss: 1.067255  [   64/  124]
train() client id: f_00002-2-2 loss: 1.014558  [   96/  124]
train() client id: f_00002-3-0 loss: 1.126708  [   32/  124]
train() client id: f_00002-3-1 loss: 0.993842  [   64/  124]
train() client id: f_00002-3-2 loss: 1.031401  [   96/  124]
train() client id: f_00002-4-0 loss: 1.006052  [   32/  124]
train() client id: f_00002-4-1 loss: 1.138309  [   64/  124]
train() client id: f_00002-4-2 loss: 0.974293  [   96/  124]
train() client id: f_00002-5-0 loss: 1.072373  [   32/  124]
train() client id: f_00002-5-1 loss: 1.121877  [   64/  124]
train() client id: f_00002-5-2 loss: 0.922234  [   96/  124]
train() client id: f_00002-6-0 loss: 1.101440  [   32/  124]
train() client id: f_00002-6-1 loss: 1.059883  [   64/  124]
train() client id: f_00002-6-2 loss: 0.861146  [   96/  124]
train() client id: f_00002-7-0 loss: 1.070726  [   32/  124]
train() client id: f_00002-7-1 loss: 0.975864  [   64/  124]
train() client id: f_00002-7-2 loss: 1.009901  [   96/  124]
train() client id: f_00002-8-0 loss: 1.023079  [   32/  124]
train() client id: f_00002-8-1 loss: 0.899708  [   64/  124]
train() client id: f_00002-8-2 loss: 1.049419  [   96/  124]
train() client id: f_00002-9-0 loss: 0.944425  [   32/  124]
train() client id: f_00002-9-1 loss: 1.148023  [   64/  124]
train() client id: f_00002-9-2 loss: 0.876913  [   96/  124]
train() client id: f_00002-10-0 loss: 0.802852  [   32/  124]
train() client id: f_00002-10-1 loss: 1.112136  [   64/  124]
train() client id: f_00002-10-2 loss: 0.999277  [   96/  124]
train() client id: f_00002-11-0 loss: 1.095955  [   32/  124]
train() client id: f_00002-11-1 loss: 1.024114  [   64/  124]
train() client id: f_00002-11-2 loss: 0.850155  [   96/  124]
train() client id: f_00003-0-0 loss: 0.729790  [   32/   43]
train() client id: f_00003-1-0 loss: 0.709287  [   32/   43]
train() client id: f_00003-2-0 loss: 0.473863  [   32/   43]
train() client id: f_00003-3-0 loss: 0.633686  [   32/   43]
train() client id: f_00003-4-0 loss: 0.658814  [   32/   43]
train() client id: f_00003-5-0 loss: 0.635075  [   32/   43]
train() client id: f_00003-6-0 loss: 0.754592  [   32/   43]
train() client id: f_00003-7-0 loss: 0.690993  [   32/   43]
train() client id: f_00003-8-0 loss: 0.761150  [   32/   43]
train() client id: f_00003-9-0 loss: 0.576142  [   32/   43]
train() client id: f_00003-10-0 loss: 0.681103  [   32/   43]
train() client id: f_00003-11-0 loss: 0.485703  [   32/   43]
train() client id: f_00004-0-0 loss: 0.836738  [   32/  306]
train() client id: f_00004-0-1 loss: 0.797173  [   64/  306]
train() client id: f_00004-0-2 loss: 0.813701  [   96/  306]
train() client id: f_00004-0-3 loss: 0.917991  [  128/  306]
train() client id: f_00004-0-4 loss: 0.649458  [  160/  306]
train() client id: f_00004-0-5 loss: 0.667428  [  192/  306]
train() client id: f_00004-0-6 loss: 0.678845  [  224/  306]
train() client id: f_00004-0-7 loss: 0.708148  [  256/  306]
train() client id: f_00004-0-8 loss: 0.904351  [  288/  306]
train() client id: f_00004-1-0 loss: 0.727988  [   32/  306]
train() client id: f_00004-1-1 loss: 0.718214  [   64/  306]
train() client id: f_00004-1-2 loss: 0.856157  [   96/  306]
train() client id: f_00004-1-3 loss: 0.747554  [  128/  306]
train() client id: f_00004-1-4 loss: 0.735204  [  160/  306]
train() client id: f_00004-1-5 loss: 0.903899  [  192/  306]
train() client id: f_00004-1-6 loss: 0.660221  [  224/  306]
train() client id: f_00004-1-7 loss: 0.763165  [  256/  306]
train() client id: f_00004-1-8 loss: 0.861519  [  288/  306]
train() client id: f_00004-2-0 loss: 0.735473  [   32/  306]
train() client id: f_00004-2-1 loss: 0.722463  [   64/  306]
train() client id: f_00004-2-2 loss: 0.790806  [   96/  306]
train() client id: f_00004-2-3 loss: 0.783755  [  128/  306]
train() client id: f_00004-2-4 loss: 0.791269  [  160/  306]
train() client id: f_00004-2-5 loss: 0.684668  [  192/  306]
train() client id: f_00004-2-6 loss: 0.876483  [  224/  306]
train() client id: f_00004-2-7 loss: 0.810713  [  256/  306]
train() client id: f_00004-2-8 loss: 0.700901  [  288/  306]
train() client id: f_00004-3-0 loss: 0.930188  [   32/  306]
train() client id: f_00004-3-1 loss: 0.792361  [   64/  306]
train() client id: f_00004-3-2 loss: 0.819798  [   96/  306]
train() client id: f_00004-3-3 loss: 0.863518  [  128/  306]
train() client id: f_00004-3-4 loss: 0.695387  [  160/  306]
train() client id: f_00004-3-5 loss: 0.731953  [  192/  306]
train() client id: f_00004-3-6 loss: 0.718980  [  224/  306]
train() client id: f_00004-3-7 loss: 0.821984  [  256/  306]
train() client id: f_00004-3-8 loss: 0.695275  [  288/  306]
train() client id: f_00004-4-0 loss: 0.729486  [   32/  306]
train() client id: f_00004-4-1 loss: 0.901248  [   64/  306]
train() client id: f_00004-4-2 loss: 0.655681  [   96/  306]
train() client id: f_00004-4-3 loss: 0.820497  [  128/  306]
train() client id: f_00004-4-4 loss: 0.768564  [  160/  306]
train() client id: f_00004-4-5 loss: 0.837803  [  192/  306]
train() client id: f_00004-4-6 loss: 0.741722  [  224/  306]
train() client id: f_00004-4-7 loss: 0.747179  [  256/  306]
train() client id: f_00004-4-8 loss: 0.739734  [  288/  306]
train() client id: f_00004-5-0 loss: 0.817828  [   32/  306]
train() client id: f_00004-5-1 loss: 0.777664  [   64/  306]
train() client id: f_00004-5-2 loss: 0.858757  [   96/  306]
train() client id: f_00004-5-3 loss: 0.760774  [  128/  306]
train() client id: f_00004-5-4 loss: 0.752423  [  160/  306]
train() client id: f_00004-5-5 loss: 0.702796  [  192/  306]
train() client id: f_00004-5-6 loss: 0.774220  [  224/  306]
train() client id: f_00004-5-7 loss: 0.738498  [  256/  306]
train() client id: f_00004-5-8 loss: 0.917777  [  288/  306]
train() client id: f_00004-6-0 loss: 0.742473  [   32/  306]
train() client id: f_00004-6-1 loss: 0.838759  [   64/  306]
train() client id: f_00004-6-2 loss: 0.636559  [   96/  306]
train() client id: f_00004-6-3 loss: 0.806498  [  128/  306]
train() client id: f_00004-6-4 loss: 0.802562  [  160/  306]
train() client id: f_00004-6-5 loss: 0.796946  [  192/  306]
train() client id: f_00004-6-6 loss: 0.946850  [  224/  306]
train() client id: f_00004-6-7 loss: 0.683741  [  256/  306]
train() client id: f_00004-6-8 loss: 0.834186  [  288/  306]
train() client id: f_00004-7-0 loss: 0.704900  [   32/  306]
train() client id: f_00004-7-1 loss: 0.780518  [   64/  306]
train() client id: f_00004-7-2 loss: 0.932901  [   96/  306]
train() client id: f_00004-7-3 loss: 0.865779  [  128/  306]
train() client id: f_00004-7-4 loss: 0.722585  [  160/  306]
train() client id: f_00004-7-5 loss: 0.847975  [  192/  306]
train() client id: f_00004-7-6 loss: 0.717257  [  224/  306]
train() client id: f_00004-7-7 loss: 0.905809  [  256/  306]
train() client id: f_00004-7-8 loss: 0.632018  [  288/  306]
train() client id: f_00004-8-0 loss: 0.899352  [   32/  306]
train() client id: f_00004-8-1 loss: 0.742611  [   64/  306]
train() client id: f_00004-8-2 loss: 0.921908  [   96/  306]
train() client id: f_00004-8-3 loss: 0.841369  [  128/  306]
train() client id: f_00004-8-4 loss: 0.768583  [  160/  306]
train() client id: f_00004-8-5 loss: 0.770394  [  192/  306]
train() client id: f_00004-8-6 loss: 0.547255  [  224/  306]
train() client id: f_00004-8-7 loss: 0.739856  [  256/  306]
train() client id: f_00004-8-8 loss: 0.869686  [  288/  306]
train() client id: f_00004-9-0 loss: 0.755576  [   32/  306]
train() client id: f_00004-9-1 loss: 0.851745  [   64/  306]
train() client id: f_00004-9-2 loss: 0.838253  [   96/  306]
train() client id: f_00004-9-3 loss: 0.820083  [  128/  306]
train() client id: f_00004-9-4 loss: 0.832221  [  160/  306]
train() client id: f_00004-9-5 loss: 0.751605  [  192/  306]
train() client id: f_00004-9-6 loss: 0.780565  [  224/  306]
train() client id: f_00004-9-7 loss: 0.738162  [  256/  306]
train() client id: f_00004-9-8 loss: 0.770486  [  288/  306]
train() client id: f_00004-10-0 loss: 0.836417  [   32/  306]
train() client id: f_00004-10-1 loss: 0.825091  [   64/  306]
train() client id: f_00004-10-2 loss: 0.918540  [   96/  306]
train() client id: f_00004-10-3 loss: 0.875199  [  128/  306]
train() client id: f_00004-10-4 loss: 0.801563  [  160/  306]
train() client id: f_00004-10-5 loss: 0.728565  [  192/  306]
train() client id: f_00004-10-6 loss: 0.697345  [  224/  306]
train() client id: f_00004-10-7 loss: 0.755823  [  256/  306]
train() client id: f_00004-10-8 loss: 0.761201  [  288/  306]
train() client id: f_00004-11-0 loss: 0.838287  [   32/  306]
train() client id: f_00004-11-1 loss: 0.721484  [   64/  306]
train() client id: f_00004-11-2 loss: 0.680692  [   96/  306]
train() client id: f_00004-11-3 loss: 0.770042  [  128/  306]
train() client id: f_00004-11-4 loss: 0.849019  [  160/  306]
train() client id: f_00004-11-5 loss: 0.880569  [  192/  306]
train() client id: f_00004-11-6 loss: 0.803978  [  224/  306]
train() client id: f_00004-11-7 loss: 0.753147  [  256/  306]
train() client id: f_00004-11-8 loss: 0.885352  [  288/  306]
train() client id: f_00005-0-0 loss: 0.584702  [   32/  146]
train() client id: f_00005-0-1 loss: 0.883060  [   64/  146]
train() client id: f_00005-0-2 loss: 0.464908  [   96/  146]
train() client id: f_00005-0-3 loss: 0.563825  [  128/  146]
train() client id: f_00005-1-0 loss: 0.695073  [   32/  146]
train() client id: f_00005-1-1 loss: 0.772850  [   64/  146]
train() client id: f_00005-1-2 loss: 0.580843  [   96/  146]
train() client id: f_00005-1-3 loss: 0.560986  [  128/  146]
train() client id: f_00005-2-0 loss: 0.588125  [   32/  146]
train() client id: f_00005-2-1 loss: 0.694124  [   64/  146]
train() client id: f_00005-2-2 loss: 0.552519  [   96/  146]
train() client id: f_00005-2-3 loss: 0.710919  [  128/  146]
train() client id: f_00005-3-0 loss: 0.849745  [   32/  146]
train() client id: f_00005-3-1 loss: 0.625350  [   64/  146]
train() client id: f_00005-3-2 loss: 0.589700  [   96/  146]
train() client id: f_00005-3-3 loss: 0.419949  [  128/  146]
train() client id: f_00005-4-0 loss: 0.613079  [   32/  146]
train() client id: f_00005-4-1 loss: 1.052137  [   64/  146]
train() client id: f_00005-4-2 loss: 0.556532  [   96/  146]
train() client id: f_00005-4-3 loss: 0.514336  [  128/  146]
train() client id: f_00005-5-0 loss: 0.946081  [   32/  146]
train() client id: f_00005-5-1 loss: 0.626580  [   64/  146]
train() client id: f_00005-5-2 loss: 0.592573  [   96/  146]
train() client id: f_00005-5-3 loss: 0.406669  [  128/  146]
train() client id: f_00005-6-0 loss: 0.576573  [   32/  146]
train() client id: f_00005-6-1 loss: 0.452712  [   64/  146]
train() client id: f_00005-6-2 loss: 0.816396  [   96/  146]
train() client id: f_00005-6-3 loss: 0.563585  [  128/  146]
train() client id: f_00005-7-0 loss: 0.706009  [   32/  146]
train() client id: f_00005-7-1 loss: 0.583162  [   64/  146]
train() client id: f_00005-7-2 loss: 0.596851  [   96/  146]
train() client id: f_00005-7-3 loss: 0.654353  [  128/  146]
train() client id: f_00005-8-0 loss: 0.503089  [   32/  146]
train() client id: f_00005-8-1 loss: 0.472832  [   64/  146]
train() client id: f_00005-8-2 loss: 0.860729  [   96/  146]
train() client id: f_00005-8-3 loss: 0.868178  [  128/  146]
train() client id: f_00005-9-0 loss: 0.844265  [   32/  146]
train() client id: f_00005-9-1 loss: 0.595281  [   64/  146]
train() client id: f_00005-9-2 loss: 0.508638  [   96/  146]
train() client id: f_00005-9-3 loss: 0.621494  [  128/  146]
train() client id: f_00005-10-0 loss: 0.433997  [   32/  146]
train() client id: f_00005-10-1 loss: 0.545911  [   64/  146]
train() client id: f_00005-10-2 loss: 0.735474  [   96/  146]
train() client id: f_00005-10-3 loss: 0.595703  [  128/  146]
train() client id: f_00005-11-0 loss: 0.725705  [   32/  146]
train() client id: f_00005-11-1 loss: 0.504667  [   64/  146]
train() client id: f_00005-11-2 loss: 0.490993  [   96/  146]
train() client id: f_00005-11-3 loss: 0.649024  [  128/  146]
train() client id: f_00006-0-0 loss: 0.528893  [   32/   54]
train() client id: f_00006-1-0 loss: 0.586343  [   32/   54]
train() client id: f_00006-2-0 loss: 0.571316  [   32/   54]
train() client id: f_00006-3-0 loss: 0.548780  [   32/   54]
train() client id: f_00006-4-0 loss: 0.586361  [   32/   54]
train() client id: f_00006-5-0 loss: 0.479519  [   32/   54]
train() client id: f_00006-6-0 loss: 0.576789  [   32/   54]
train() client id: f_00006-7-0 loss: 0.528586  [   32/   54]
train() client id: f_00006-8-0 loss: 0.597254  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586892  [   32/   54]
train() client id: f_00006-10-0 loss: 0.588949  [   32/   54]
train() client id: f_00006-11-0 loss: 0.477421  [   32/   54]
train() client id: f_00007-0-0 loss: 0.717735  [   32/  179]
train() client id: f_00007-0-1 loss: 0.659369  [   64/  179]
train() client id: f_00007-0-2 loss: 0.782166  [   96/  179]
train() client id: f_00007-0-3 loss: 0.704948  [  128/  179]
train() client id: f_00007-0-4 loss: 0.803489  [  160/  179]
train() client id: f_00007-1-0 loss: 0.704553  [   32/  179]
train() client id: f_00007-1-1 loss: 0.725141  [   64/  179]
train() client id: f_00007-1-2 loss: 0.833331  [   96/  179]
train() client id: f_00007-1-3 loss: 0.569856  [  128/  179]
train() client id: f_00007-1-4 loss: 0.811225  [  160/  179]
train() client id: f_00007-2-0 loss: 0.775012  [   32/  179]
train() client id: f_00007-2-1 loss: 0.696208  [   64/  179]
train() client id: f_00007-2-2 loss: 0.621484  [   96/  179]
train() client id: f_00007-2-3 loss: 0.800069  [  128/  179]
train() client id: f_00007-2-4 loss: 0.584669  [  160/  179]
train() client id: f_00007-3-0 loss: 0.626063  [   32/  179]
train() client id: f_00007-3-1 loss: 0.807529  [   64/  179]
train() client id: f_00007-3-2 loss: 0.942392  [   96/  179]
train() client id: f_00007-3-3 loss: 0.661517  [  128/  179]
train() client id: f_00007-3-4 loss: 0.575963  [  160/  179]
train() client id: f_00007-4-0 loss: 0.857261  [   32/  179]
train() client id: f_00007-4-1 loss: 0.901268  [   64/  179]
train() client id: f_00007-4-2 loss: 0.624243  [   96/  179]
train() client id: f_00007-4-3 loss: 0.631631  [  128/  179]
train() client id: f_00007-4-4 loss: 0.600525  [  160/  179]
train() client id: f_00007-5-0 loss: 0.717309  [   32/  179]
train() client id: f_00007-5-1 loss: 0.727035  [   64/  179]
train() client id: f_00007-5-2 loss: 0.645384  [   96/  179]
train() client id: f_00007-5-3 loss: 0.790812  [  128/  179]
train() client id: f_00007-5-4 loss: 0.665504  [  160/  179]
train() client id: f_00007-6-0 loss: 0.789152  [   32/  179]
train() client id: f_00007-6-1 loss: 0.746172  [   64/  179]
train() client id: f_00007-6-2 loss: 0.557124  [   96/  179]
train() client id: f_00007-6-3 loss: 0.590428  [  128/  179]
train() client id: f_00007-6-4 loss: 0.830493  [  160/  179]
train() client id: f_00007-7-0 loss: 0.566385  [   32/  179]
train() client id: f_00007-7-1 loss: 0.709018  [   64/  179]
train() client id: f_00007-7-2 loss: 0.801309  [   96/  179]
train() client id: f_00007-7-3 loss: 0.673885  [  128/  179]
train() client id: f_00007-7-4 loss: 0.731790  [  160/  179]
train() client id: f_00007-8-0 loss: 0.716039  [   32/  179]
train() client id: f_00007-8-1 loss: 0.847562  [   64/  179]
train() client id: f_00007-8-2 loss: 0.640409  [   96/  179]
train() client id: f_00007-8-3 loss: 0.667075  [  128/  179]
train() client id: f_00007-8-4 loss: 0.674409  [  160/  179]
train() client id: f_00007-9-0 loss: 0.648303  [   32/  179]
train() client id: f_00007-9-1 loss: 0.835744  [   64/  179]
train() client id: f_00007-9-2 loss: 0.631864  [   96/  179]
train() client id: f_00007-9-3 loss: 0.817131  [  128/  179]
train() client id: f_00007-9-4 loss: 0.538935  [  160/  179]
train() client id: f_00007-10-0 loss: 0.681330  [   32/  179]
train() client id: f_00007-10-1 loss: 0.791953  [   64/  179]
train() client id: f_00007-10-2 loss: 0.542266  [   96/  179]
train() client id: f_00007-10-3 loss: 0.733056  [  128/  179]
train() client id: f_00007-10-4 loss: 0.774336  [  160/  179]
train() client id: f_00007-11-0 loss: 0.567488  [   32/  179]
train() client id: f_00007-11-1 loss: 0.711399  [   64/  179]
train() client id: f_00007-11-2 loss: 0.655865  [   96/  179]
train() client id: f_00007-11-3 loss: 0.792596  [  128/  179]
train() client id: f_00007-11-4 loss: 0.667026  [  160/  179]
train() client id: f_00008-0-0 loss: 0.811632  [   32/  130]
train() client id: f_00008-0-1 loss: 0.606826  [   64/  130]
train() client id: f_00008-0-2 loss: 0.777083  [   96/  130]
train() client id: f_00008-0-3 loss: 0.731060  [  128/  130]
train() client id: f_00008-1-0 loss: 0.697700  [   32/  130]
train() client id: f_00008-1-1 loss: 0.725164  [   64/  130]
train() client id: f_00008-1-2 loss: 0.822303  [   96/  130]
train() client id: f_00008-1-3 loss: 0.678641  [  128/  130]
train() client id: f_00008-2-0 loss: 0.777211  [   32/  130]
train() client id: f_00008-2-1 loss: 0.690404  [   64/  130]
train() client id: f_00008-2-2 loss: 0.658536  [   96/  130]
train() client id: f_00008-2-3 loss: 0.773713  [  128/  130]
train() client id: f_00008-3-0 loss: 0.834160  [   32/  130]
train() client id: f_00008-3-1 loss: 0.800416  [   64/  130]
train() client id: f_00008-3-2 loss: 0.631618  [   96/  130]
train() client id: f_00008-3-3 loss: 0.658748  [  128/  130]
train() client id: f_00008-4-0 loss: 0.814008  [   32/  130]
train() client id: f_00008-4-1 loss: 0.758601  [   64/  130]
train() client id: f_00008-4-2 loss: 0.708411  [   96/  130]
train() client id: f_00008-4-3 loss: 0.639305  [  128/  130]
train() client id: f_00008-5-0 loss: 0.833142  [   32/  130]
train() client id: f_00008-5-1 loss: 0.646691  [   64/  130]
train() client id: f_00008-5-2 loss: 0.665795  [   96/  130]
train() client id: f_00008-5-3 loss: 0.781065  [  128/  130]
train() client id: f_00008-6-0 loss: 0.682813  [   32/  130]
train() client id: f_00008-6-1 loss: 0.683129  [   64/  130]
train() client id: f_00008-6-2 loss: 0.763805  [   96/  130]
train() client id: f_00008-6-3 loss: 0.789407  [  128/  130]
train() client id: f_00008-7-0 loss: 0.680041  [   32/  130]
train() client id: f_00008-7-1 loss: 0.728790  [   64/  130]
train() client id: f_00008-7-2 loss: 0.782775  [   96/  130]
train() client id: f_00008-7-3 loss: 0.731131  [  128/  130]
train() client id: f_00008-8-0 loss: 0.804491  [   32/  130]
train() client id: f_00008-8-1 loss: 0.650973  [   64/  130]
train() client id: f_00008-8-2 loss: 0.683504  [   96/  130]
train() client id: f_00008-8-3 loss: 0.774413  [  128/  130]
train() client id: f_00008-9-0 loss: 0.667149  [   32/  130]
train() client id: f_00008-9-1 loss: 0.651018  [   64/  130]
train() client id: f_00008-9-2 loss: 0.797749  [   96/  130]
train() client id: f_00008-9-3 loss: 0.761750  [  128/  130]
train() client id: f_00008-10-0 loss: 0.635835  [   32/  130]
train() client id: f_00008-10-1 loss: 0.741166  [   64/  130]
train() client id: f_00008-10-2 loss: 0.720231  [   96/  130]
train() client id: f_00008-10-3 loss: 0.738249  [  128/  130]
train() client id: f_00008-11-0 loss: 0.741766  [   32/  130]
train() client id: f_00008-11-1 loss: 0.745782  [   64/  130]
train() client id: f_00008-11-2 loss: 0.732342  [   96/  130]
train() client id: f_00008-11-3 loss: 0.689010  [  128/  130]
train() client id: f_00009-0-0 loss: 1.150551  [   32/  118]
train() client id: f_00009-0-1 loss: 1.330392  [   64/  118]
train() client id: f_00009-0-2 loss: 1.279757  [   96/  118]
train() client id: f_00009-1-0 loss: 1.211947  [   32/  118]
train() client id: f_00009-1-1 loss: 1.250672  [   64/  118]
train() client id: f_00009-1-2 loss: 1.160024  [   96/  118]
train() client id: f_00009-2-0 loss: 1.277588  [   32/  118]
train() client id: f_00009-2-1 loss: 1.131877  [   64/  118]
train() client id: f_00009-2-2 loss: 1.088509  [   96/  118]
train() client id: f_00009-3-0 loss: 1.077706  [   32/  118]
train() client id: f_00009-3-1 loss: 1.192248  [   64/  118]
train() client id: f_00009-3-2 loss: 1.138255  [   96/  118]
train() client id: f_00009-4-0 loss: 1.067536  [   32/  118]
train() client id: f_00009-4-1 loss: 1.154359  [   64/  118]
train() client id: f_00009-4-2 loss: 1.217192  [   96/  118]
train() client id: f_00009-5-0 loss: 1.078761  [   32/  118]
train() client id: f_00009-5-1 loss: 1.010421  [   64/  118]
train() client id: f_00009-5-2 loss: 1.112571  [   96/  118]
train() client id: f_00009-6-0 loss: 1.080685  [   32/  118]
train() client id: f_00009-6-1 loss: 1.038029  [   64/  118]
train() client id: f_00009-6-2 loss: 1.047892  [   96/  118]
train() client id: f_00009-7-0 loss: 1.006633  [   32/  118]
train() client id: f_00009-7-1 loss: 0.949891  [   64/  118]
train() client id: f_00009-7-2 loss: 0.980737  [   96/  118]
train() client id: f_00009-8-0 loss: 0.980343  [   32/  118]
train() client id: f_00009-8-1 loss: 1.033196  [   64/  118]
train() client id: f_00009-8-2 loss: 0.960294  [   96/  118]
train() client id: f_00009-9-0 loss: 1.052978  [   32/  118]
train() client id: f_00009-9-1 loss: 0.966471  [   64/  118]
train() client id: f_00009-9-2 loss: 1.049133  [   96/  118]
train() client id: f_00009-10-0 loss: 0.882960  [   32/  118]
train() client id: f_00009-10-1 loss: 0.976217  [   64/  118]
train() client id: f_00009-10-2 loss: 1.133564  [   96/  118]
train() client id: f_00009-11-0 loss: 0.951578  [   32/  118]
train() client id: f_00009-11-1 loss: 1.213368  [   64/  118]
train() client id: f_00009-11-2 loss: 0.869560  [   96/  118]
At round 26 accuracy: 0.6525198938992043
At round 26 training accuracy: 0.5814889336016097
At round 26 training loss: 0.8274243583219372
update_location
xs = [  -3.9056584     4.20031788  150.00902392   18.81129433    0.97929623
    3.95640986 -112.44319194  -91.32485185  134.66397685  -77.06087855]
ys = [ 142.5879595   125.55583871    1.32061395 -112.45517586  104.35018685
   87.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [174.20269906 160.56746652 180.28990897 151.65761231 144.53345812
 133.14269882 150.50037194 135.42859676 168.65069414 126.31069179]
dists_bs = [174.13734262 185.89642012 368.50426696 346.68904791 189.63150793
 199.06473556 188.31885006 193.23401585 347.38493983 197.05979992]
uav_gains = [2.47904434e-11 3.05317297e-11 2.26669887e-11 3.52625235e-11
 3.97925071e-11 4.88782381e-11 3.59484763e-11 4.68391522e-11
 2.69445184e-11 5.57642431e-11]
bs_gains = [5.87214444e-11 4.89029672e-11 7.19871577e-12 8.54007614e-12
 4.62535094e-11 4.03747604e-11 4.71619185e-11 4.38793376e-11
 8.49226080e-12 4.15355131e-11]
Round 27
-------------------------------
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.428454   15.42356035  7.31658542  2.62856167 17.79004913  8.56531288
  3.26241587 10.46431357  7.71314909  6.94940181]
obj_prev = 87.54180377366474
eta_min = 3.2918080146347544e-13	eta_max = 0.9246953843511696
af = 18.488129690050464	bf = 1.525402092626451	zeta = 20.336942659055513	eta = 0.909090909090909
af = 18.488129690050464	bf = 1.525402092626451	zeta = 36.16259963393917	eta = 0.5112500173438599
af = 18.488129690050464	bf = 1.525402092626451	zeta = 28.49928344660974	eta = 0.6487226152435002
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.11857262246232	eta = 0.6817515784269833
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.048110971781785	eta = 0.6835275745998821
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.047913433089164	eta = 0.6835325665983885
eta = 0.6835325665983885
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.03139638 0.06603207 0.03089802 0.01071464 0.07624835 0.03637993
 0.01345559 0.04460279 0.03239309 0.02940296]
ene_total = [2.38132415 4.38338617 2.36243079 1.10502586 4.99963694 2.62710832
 1.26705354 3.1051769  2.60980459 2.20696617]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 0 obj = 4.842867033053818
eta = 0.6835325665983885
freqs = [39761858.95611374 81214212.7965591  39312506.23491908 13349480.34302717
 93973501.61782414 45073814.47997239 16750787.69960297 54945517.04012107
 44182442.51361968 36388553.66776452]
eta_min = 0.6835325665983994	eta_max = 0.6835325665983844
af = 0.018943411267469068	bf = 1.525402092626451	zeta = 0.020837752394215977	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [3.10856006e-06 2.72750345e-05 2.99046350e-06 1.19578344e-07
 4.21684466e-05 4.62867660e-06 2.36439257e-07 8.43282168e-06
 3.96002704e-06 2.43818667e-06]
ene_total = [1.75137172 1.512205   1.78937036 1.61521218 1.53279247 1.56936094
 1.60841252 1.52178181 2.33933449 1.55944202]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 1 obj = 4.842867033053756
eta = 0.6835325665983844
freqs = [39761858.95611373 81214212.79655911 39312506.23491906 13349480.34302717
 93973501.61782415 45073814.4799724  16750787.69960297 54945517.04012109
 44182442.51361962 36388553.66776453]
Done!
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.19524010e-05 1.04872399e-04 1.14983202e-05 4.59778254e-07
 1.62137509e-04 1.77972431e-05 9.09107999e-07 3.24241657e-05
 1.52262882e-05 9.37481801e-06]
ene_total = [0.00841788 0.0073383  0.00859999 0.00775562 0.00747952 0.0075483
 0.00772331 0.00733068 0.01124335 0.00749444]
At round 27 energy consumption: 0.08093140167794016
At round 27 eta: 0.6835325665983844
At round 27 a_n: 18.93386498808281
At round 27 local rounds: 12.458880266672946
At round 27 global rounds: 59.82879433933611
gradient difference: 0.5100234746932983
train() client id: f_00000-0-0 loss: 1.151144  [   32/  126]
train() client id: f_00000-0-1 loss: 1.185014  [   64/  126]
train() client id: f_00000-0-2 loss: 0.960655  [   96/  126]
train() client id: f_00000-1-0 loss: 1.049103  [   32/  126]
train() client id: f_00000-1-1 loss: 0.934217  [   64/  126]
train() client id: f_00000-1-2 loss: 0.997289  [   96/  126]
train() client id: f_00000-2-0 loss: 1.007124  [   32/  126]
train() client id: f_00000-2-1 loss: 0.949289  [   64/  126]
train() client id: f_00000-2-2 loss: 0.991630  [   96/  126]
train() client id: f_00000-3-0 loss: 0.958070  [   32/  126]
train() client id: f_00000-3-1 loss: 0.958824  [   64/  126]
train() client id: f_00000-3-2 loss: 0.917796  [   96/  126]
train() client id: f_00000-4-0 loss: 0.765939  [   32/  126]
train() client id: f_00000-4-1 loss: 0.880692  [   64/  126]
train() client id: f_00000-4-2 loss: 0.934352  [   96/  126]
train() client id: f_00000-5-0 loss: 0.855902  [   32/  126]
train() client id: f_00000-5-1 loss: 0.823085  [   64/  126]
train() client id: f_00000-5-2 loss: 0.941406  [   96/  126]
train() client id: f_00000-6-0 loss: 0.844401  [   32/  126]
train() client id: f_00000-6-1 loss: 0.947546  [   64/  126]
train() client id: f_00000-6-2 loss: 0.836556  [   96/  126]
train() client id: f_00000-7-0 loss: 0.831786  [   32/  126]
train() client id: f_00000-7-1 loss: 0.854893  [   64/  126]
train() client id: f_00000-7-2 loss: 0.947681  [   96/  126]
train() client id: f_00000-8-0 loss: 0.894076  [   32/  126]
train() client id: f_00000-8-1 loss: 0.858443  [   64/  126]
train() client id: f_00000-8-2 loss: 0.839115  [   96/  126]
train() client id: f_00000-9-0 loss: 0.888638  [   32/  126]
train() client id: f_00000-9-1 loss: 0.871089  [   64/  126]
train() client id: f_00000-9-2 loss: 0.862370  [   96/  126]
train() client id: f_00000-10-0 loss: 0.923507  [   32/  126]
train() client id: f_00000-10-1 loss: 0.803318  [   64/  126]
train() client id: f_00000-10-2 loss: 0.860503  [   96/  126]
train() client id: f_00000-11-0 loss: 0.923405  [   32/  126]
train() client id: f_00000-11-1 loss: 0.863120  [   64/  126]
train() client id: f_00000-11-2 loss: 0.836031  [   96/  126]
train() client id: f_00001-0-0 loss: 0.415665  [   32/  265]
train() client id: f_00001-0-1 loss: 0.410878  [   64/  265]
train() client id: f_00001-0-2 loss: 0.338496  [   96/  265]
train() client id: f_00001-0-3 loss: 0.442355  [  128/  265]
train() client id: f_00001-0-4 loss: 0.297196  [  160/  265]
train() client id: f_00001-0-5 loss: 0.368118  [  192/  265]
train() client id: f_00001-0-6 loss: 0.406574  [  224/  265]
train() client id: f_00001-0-7 loss: 0.335413  [  256/  265]
train() client id: f_00001-1-0 loss: 0.278663  [   32/  265]
train() client id: f_00001-1-1 loss: 0.393884  [   64/  265]
train() client id: f_00001-1-2 loss: 0.401303  [   96/  265]
train() client id: f_00001-1-3 loss: 0.364749  [  128/  265]
train() client id: f_00001-1-4 loss: 0.470040  [  160/  265]
train() client id: f_00001-1-5 loss: 0.393809  [  192/  265]
train() client id: f_00001-1-6 loss: 0.361070  [  224/  265]
train() client id: f_00001-1-7 loss: 0.277703  [  256/  265]
train() client id: f_00001-2-0 loss: 0.416965  [   32/  265]
train() client id: f_00001-2-1 loss: 0.302995  [   64/  265]
train() client id: f_00001-2-2 loss: 0.401898  [   96/  265]
train() client id: f_00001-2-3 loss: 0.337180  [  128/  265]
train() client id: f_00001-2-4 loss: 0.351600  [  160/  265]
train() client id: f_00001-2-5 loss: 0.380913  [  192/  265]
train() client id: f_00001-2-6 loss: 0.287048  [  224/  265]
train() client id: f_00001-2-7 loss: 0.397416  [  256/  265]
train() client id: f_00001-3-0 loss: 0.319798  [   32/  265]
train() client id: f_00001-3-1 loss: 0.267566  [   64/  265]
train() client id: f_00001-3-2 loss: 0.260515  [   96/  265]
train() client id: f_00001-3-3 loss: 0.332749  [  128/  265]
train() client id: f_00001-3-4 loss: 0.337371  [  160/  265]
train() client id: f_00001-3-5 loss: 0.393997  [  192/  265]
train() client id: f_00001-3-6 loss: 0.473452  [  224/  265]
train() client id: f_00001-3-7 loss: 0.351789  [  256/  265]
train() client id: f_00001-4-0 loss: 0.266559  [   32/  265]
train() client id: f_00001-4-1 loss: 0.514105  [   64/  265]
train() client id: f_00001-4-2 loss: 0.386160  [   96/  265]
train() client id: f_00001-4-3 loss: 0.353130  [  128/  265]
train() client id: f_00001-4-4 loss: 0.268640  [  160/  265]
train() client id: f_00001-4-5 loss: 0.280861  [  192/  265]
train() client id: f_00001-4-6 loss: 0.377544  [  224/  265]
train() client id: f_00001-4-7 loss: 0.236001  [  256/  265]
train() client id: f_00001-5-0 loss: 0.272921  [   32/  265]
train() client id: f_00001-5-1 loss: 0.333412  [   64/  265]
train() client id: f_00001-5-2 loss: 0.248315  [   96/  265]
train() client id: f_00001-5-3 loss: 0.301861  [  128/  265]
train() client id: f_00001-5-4 loss: 0.545373  [  160/  265]
train() client id: f_00001-5-5 loss: 0.342427  [  192/  265]
train() client id: f_00001-5-6 loss: 0.228827  [  224/  265]
train() client id: f_00001-5-7 loss: 0.463110  [  256/  265]
train() client id: f_00001-6-0 loss: 0.379095  [   32/  265]
train() client id: f_00001-6-1 loss: 0.252876  [   64/  265]
train() client id: f_00001-6-2 loss: 0.285594  [   96/  265]
train() client id: f_00001-6-3 loss: 0.449138  [  128/  265]
train() client id: f_00001-6-4 loss: 0.314277  [  160/  265]
train() client id: f_00001-6-5 loss: 0.374602  [  192/  265]
train() client id: f_00001-6-6 loss: 0.334284  [  224/  265]
train() client id: f_00001-6-7 loss: 0.330931  [  256/  265]
train() client id: f_00001-7-0 loss: 0.315908  [   32/  265]
train() client id: f_00001-7-1 loss: 0.245926  [   64/  265]
train() client id: f_00001-7-2 loss: 0.390133  [   96/  265]
train() client id: f_00001-7-3 loss: 0.374755  [  128/  265]
train() client id: f_00001-7-4 loss: 0.283982  [  160/  265]
train() client id: f_00001-7-5 loss: 0.244359  [  192/  265]
train() client id: f_00001-7-6 loss: 0.470277  [  224/  265]
train() client id: f_00001-7-7 loss: 0.383118  [  256/  265]
train() client id: f_00001-8-0 loss: 0.372739  [   32/  265]
train() client id: f_00001-8-1 loss: 0.256730  [   64/  265]
train() client id: f_00001-8-2 loss: 0.240361  [   96/  265]
train() client id: f_00001-8-3 loss: 0.391999  [  128/  265]
train() client id: f_00001-8-4 loss: 0.310613  [  160/  265]
train() client id: f_00001-8-5 loss: 0.274897  [  192/  265]
train() client id: f_00001-8-6 loss: 0.407386  [  224/  265]
train() client id: f_00001-8-7 loss: 0.453194  [  256/  265]
train() client id: f_00001-9-0 loss: 0.250434  [   32/  265]
train() client id: f_00001-9-1 loss: 0.368558  [   64/  265]
train() client id: f_00001-9-2 loss: 0.404297  [   96/  265]
train() client id: f_00001-9-3 loss: 0.376902  [  128/  265]
train() client id: f_00001-9-4 loss: 0.424167  [  160/  265]
train() client id: f_00001-9-5 loss: 0.334802  [  192/  265]
train() client id: f_00001-9-6 loss: 0.253778  [  224/  265]
train() client id: f_00001-9-7 loss: 0.279818  [  256/  265]
train() client id: f_00001-10-0 loss: 0.300587  [   32/  265]
train() client id: f_00001-10-1 loss: 0.252028  [   64/  265]
train() client id: f_00001-10-2 loss: 0.495796  [   96/  265]
train() client id: f_00001-10-3 loss: 0.255380  [  128/  265]
train() client id: f_00001-10-4 loss: 0.288773  [  160/  265]
train() client id: f_00001-10-5 loss: 0.359431  [  192/  265]
train() client id: f_00001-10-6 loss: 0.272863  [  224/  265]
train() client id: f_00001-10-7 loss: 0.409837  [  256/  265]
train() client id: f_00001-11-0 loss: 0.332821  [   32/  265]
train() client id: f_00001-11-1 loss: 0.240729  [   64/  265]
train() client id: f_00001-11-2 loss: 0.239198  [   96/  265]
train() client id: f_00001-11-3 loss: 0.291898  [  128/  265]
train() client id: f_00001-11-4 loss: 0.414831  [  160/  265]
train() client id: f_00001-11-5 loss: 0.250311  [  192/  265]
train() client id: f_00001-11-6 loss: 0.394160  [  224/  265]
train() client id: f_00001-11-7 loss: 0.523944  [  256/  265]
train() client id: f_00002-0-0 loss: 1.226629  [   32/  124]
train() client id: f_00002-0-1 loss: 1.175654  [   64/  124]
train() client id: f_00002-0-2 loss: 1.412270  [   96/  124]
train() client id: f_00002-1-0 loss: 1.226193  [   32/  124]
train() client id: f_00002-1-1 loss: 1.351606  [   64/  124]
train() client id: f_00002-1-2 loss: 1.291377  [   96/  124]
train() client id: f_00002-2-0 loss: 1.213860  [   32/  124]
train() client id: f_00002-2-1 loss: 1.280015  [   64/  124]
train() client id: f_00002-2-2 loss: 1.110400  [   96/  124]
train() client id: f_00002-3-0 loss: 1.010828  [   32/  124]
train() client id: f_00002-3-1 loss: 1.198474  [   64/  124]
train() client id: f_00002-3-2 loss: 1.304680  [   96/  124]
train() client id: f_00002-4-0 loss: 1.120174  [   32/  124]
train() client id: f_00002-4-1 loss: 1.344312  [   64/  124]
train() client id: f_00002-4-2 loss: 1.165845  [   96/  124]
train() client id: f_00002-5-0 loss: 1.232425  [   32/  124]
train() client id: f_00002-5-1 loss: 1.231962  [   64/  124]
train() client id: f_00002-5-2 loss: 1.163390  [   96/  124]
train() client id: f_00002-6-0 loss: 1.117071  [   32/  124]
train() client id: f_00002-6-1 loss: 1.135505  [   64/  124]
train() client id: f_00002-6-2 loss: 1.184237  [   96/  124]
train() client id: f_00002-7-0 loss: 1.026949  [   32/  124]
train() client id: f_00002-7-1 loss: 1.229986  [   64/  124]
train() client id: f_00002-7-2 loss: 1.108962  [   96/  124]
train() client id: f_00002-8-0 loss: 1.191937  [   32/  124]
train() client id: f_00002-8-1 loss: 1.084809  [   64/  124]
train() client id: f_00002-8-2 loss: 1.223462  [   96/  124]
train() client id: f_00002-9-0 loss: 1.152057  [   32/  124]
train() client id: f_00002-9-1 loss: 0.978601  [   64/  124]
train() client id: f_00002-9-2 loss: 1.337769  [   96/  124]
train() client id: f_00002-10-0 loss: 1.306689  [   32/  124]
train() client id: f_00002-10-1 loss: 0.973873  [   64/  124]
train() client id: f_00002-10-2 loss: 0.995983  [   96/  124]
train() client id: f_00002-11-0 loss: 1.154216  [   32/  124]
train() client id: f_00002-11-1 loss: 1.168059  [   64/  124]
train() client id: f_00002-11-2 loss: 1.083959  [   96/  124]
train() client id: f_00003-0-0 loss: 1.030470  [   32/   43]
train() client id: f_00003-1-0 loss: 0.991869  [   32/   43]
train() client id: f_00003-2-0 loss: 1.037877  [   32/   43]
train() client id: f_00003-3-0 loss: 0.938023  [   32/   43]
train() client id: f_00003-4-0 loss: 1.041493  [   32/   43]
train() client id: f_00003-5-0 loss: 1.119966  [   32/   43]
train() client id: f_00003-6-0 loss: 1.118644  [   32/   43]
train() client id: f_00003-7-0 loss: 1.055014  [   32/   43]
train() client id: f_00003-8-0 loss: 1.028539  [   32/   43]
train() client id: f_00003-9-0 loss: 1.036819  [   32/   43]
train() client id: f_00003-10-0 loss: 0.938037  [   32/   43]
train() client id: f_00003-11-0 loss: 0.859609  [   32/   43]
train() client id: f_00004-0-0 loss: 0.868845  [   32/  306]
train() client id: f_00004-0-1 loss: 0.719903  [   64/  306]
train() client id: f_00004-0-2 loss: 0.824243  [   96/  306]
train() client id: f_00004-0-3 loss: 0.784547  [  128/  306]
train() client id: f_00004-0-4 loss: 0.914044  [  160/  306]
train() client id: f_00004-0-5 loss: 0.776411  [  192/  306]
train() client id: f_00004-0-6 loss: 0.813349  [  224/  306]
train() client id: f_00004-0-7 loss: 0.803128  [  256/  306]
train() client id: f_00004-0-8 loss: 0.792267  [  288/  306]
train() client id: f_00004-1-0 loss: 0.767123  [   32/  306]
train() client id: f_00004-1-1 loss: 0.909464  [   64/  306]
train() client id: f_00004-1-2 loss: 0.888481  [   96/  306]
train() client id: f_00004-1-3 loss: 0.839346  [  128/  306]
train() client id: f_00004-1-4 loss: 0.684446  [  160/  306]
train() client id: f_00004-1-5 loss: 0.740838  [  192/  306]
train() client id: f_00004-1-6 loss: 0.838672  [  224/  306]
train() client id: f_00004-1-7 loss: 0.738601  [  256/  306]
train() client id: f_00004-1-8 loss: 0.950391  [  288/  306]
train() client id: f_00004-2-0 loss: 0.772709  [   32/  306]
train() client id: f_00004-2-1 loss: 0.868803  [   64/  306]
train() client id: f_00004-2-2 loss: 0.823441  [   96/  306]
train() client id: f_00004-2-3 loss: 0.674740  [  128/  306]
train() client id: f_00004-2-4 loss: 0.848069  [  160/  306]
train() client id: f_00004-2-5 loss: 0.772035  [  192/  306]
train() client id: f_00004-2-6 loss: 0.921227  [  224/  306]
train() client id: f_00004-2-7 loss: 0.828129  [  256/  306]
train() client id: f_00004-2-8 loss: 0.839486  [  288/  306]
train() client id: f_00004-3-0 loss: 0.824043  [   32/  306]
train() client id: f_00004-3-1 loss: 0.788299  [   64/  306]
train() client id: f_00004-3-2 loss: 0.791210  [   96/  306]
train() client id: f_00004-3-3 loss: 0.878434  [  128/  306]
train() client id: f_00004-3-4 loss: 0.793290  [  160/  306]
train() client id: f_00004-3-5 loss: 0.724600  [  192/  306]
train() client id: f_00004-3-6 loss: 0.884440  [  224/  306]
train() client id: f_00004-3-7 loss: 0.840854  [  256/  306]
train() client id: f_00004-3-8 loss: 0.795862  [  288/  306]
train() client id: f_00004-4-0 loss: 0.837579  [   32/  306]
train() client id: f_00004-4-1 loss: 0.855008  [   64/  306]
train() client id: f_00004-4-2 loss: 0.911280  [   96/  306]
train() client id: f_00004-4-3 loss: 0.843381  [  128/  306]
train() client id: f_00004-4-4 loss: 0.670522  [  160/  306]
train() client id: f_00004-4-5 loss: 0.770311  [  192/  306]
train() client id: f_00004-4-6 loss: 0.776356  [  224/  306]
train() client id: f_00004-4-7 loss: 0.767864  [  256/  306]
train() client id: f_00004-4-8 loss: 0.887575  [  288/  306]
train() client id: f_00004-5-0 loss: 0.882069  [   32/  306]
train() client id: f_00004-5-1 loss: 0.710485  [   64/  306]
train() client id: f_00004-5-2 loss: 0.909591  [   96/  306]
train() client id: f_00004-5-3 loss: 0.752079  [  128/  306]
train() client id: f_00004-5-4 loss: 0.711690  [  160/  306]
train() client id: f_00004-5-5 loss: 0.884908  [  192/  306]
train() client id: f_00004-5-6 loss: 0.845216  [  224/  306]
train() client id: f_00004-5-7 loss: 0.787356  [  256/  306]
train() client id: f_00004-5-8 loss: 0.829476  [  288/  306]
train() client id: f_00004-6-0 loss: 0.805076  [   32/  306]
train() client id: f_00004-6-1 loss: 0.774393  [   64/  306]
train() client id: f_00004-6-2 loss: 0.875871  [   96/  306]
train() client id: f_00004-6-3 loss: 0.853474  [  128/  306]
train() client id: f_00004-6-4 loss: 0.777256  [  160/  306]
train() client id: f_00004-6-5 loss: 0.958598  [  192/  306]
train() client id: f_00004-6-6 loss: 0.740920  [  224/  306]
train() client id: f_00004-6-7 loss: 0.790820  [  256/  306]
train() client id: f_00004-6-8 loss: 0.769527  [  288/  306]
train() client id: f_00004-7-0 loss: 0.921375  [   32/  306]
train() client id: f_00004-7-1 loss: 0.819889  [   64/  306]
train() client id: f_00004-7-2 loss: 0.879895  [   96/  306]
train() client id: f_00004-7-3 loss: 0.869836  [  128/  306]
train() client id: f_00004-7-4 loss: 0.843965  [  160/  306]
train() client id: f_00004-7-5 loss: 0.795090  [  192/  306]
train() client id: f_00004-7-6 loss: 0.716753  [  224/  306]
train() client id: f_00004-7-7 loss: 0.754256  [  256/  306]
train() client id: f_00004-7-8 loss: 0.791509  [  288/  306]
train() client id: f_00004-8-0 loss: 0.755102  [   32/  306]
train() client id: f_00004-8-1 loss: 0.814346  [   64/  306]
train() client id: f_00004-8-2 loss: 0.822020  [   96/  306]
train() client id: f_00004-8-3 loss: 0.798327  [  128/  306]
train() client id: f_00004-8-4 loss: 0.836883  [  160/  306]
train() client id: f_00004-8-5 loss: 0.682867  [  192/  306]
train() client id: f_00004-8-6 loss: 0.838039  [  224/  306]
train() client id: f_00004-8-7 loss: 0.809350  [  256/  306]
train() client id: f_00004-8-8 loss: 0.955317  [  288/  306]
train() client id: f_00004-9-0 loss: 0.850608  [   32/  306]
train() client id: f_00004-9-1 loss: 0.831602  [   64/  306]
train() client id: f_00004-9-2 loss: 0.844934  [   96/  306]
train() client id: f_00004-9-3 loss: 0.864503  [  128/  306]
train() client id: f_00004-9-4 loss: 0.743554  [  160/  306]
train() client id: f_00004-9-5 loss: 0.868839  [  192/  306]
train() client id: f_00004-9-6 loss: 0.728476  [  224/  306]
train() client id: f_00004-9-7 loss: 0.756767  [  256/  306]
train() client id: f_00004-9-8 loss: 0.805508  [  288/  306]
train() client id: f_00004-10-0 loss: 0.711983  [   32/  306]
train() client id: f_00004-10-1 loss: 0.717642  [   64/  306]
train() client id: f_00004-10-2 loss: 0.896397  [   96/  306]
train() client id: f_00004-10-3 loss: 0.780034  [  128/  306]
train() client id: f_00004-10-4 loss: 0.899014  [  160/  306]
train() client id: f_00004-10-5 loss: 0.877553  [  192/  306]
train() client id: f_00004-10-6 loss: 0.827804  [  224/  306]
train() client id: f_00004-10-7 loss: 0.780470  [  256/  306]
train() client id: f_00004-10-8 loss: 0.834006  [  288/  306]
train() client id: f_00004-11-0 loss: 0.855657  [   32/  306]
train() client id: f_00004-11-1 loss: 0.860427  [   64/  306]
train() client id: f_00004-11-2 loss: 0.807922  [   96/  306]
train() client id: f_00004-11-3 loss: 0.847366  [  128/  306]
train() client id: f_00004-11-4 loss: 0.704335  [  160/  306]
train() client id: f_00004-11-5 loss: 0.823129  [  192/  306]
train() client id: f_00004-11-6 loss: 0.732876  [  224/  306]
train() client id: f_00004-11-7 loss: 0.790288  [  256/  306]
train() client id: f_00004-11-8 loss: 0.900411  [  288/  306]
train() client id: f_00005-0-0 loss: 0.643902  [   32/  146]
train() client id: f_00005-0-1 loss: 0.518533  [   64/  146]
train() client id: f_00005-0-2 loss: 0.655051  [   96/  146]
train() client id: f_00005-0-3 loss: 0.563058  [  128/  146]
train() client id: f_00005-1-0 loss: 0.537184  [   32/  146]
train() client id: f_00005-1-1 loss: 0.518578  [   64/  146]
train() client id: f_00005-1-2 loss: 0.711996  [   96/  146]
train() client id: f_00005-1-3 loss: 0.588525  [  128/  146]
train() client id: f_00005-2-0 loss: 0.553134  [   32/  146]
train() client id: f_00005-2-1 loss: 0.684700  [   64/  146]
train() client id: f_00005-2-2 loss: 0.605924  [   96/  146]
train() client id: f_00005-2-3 loss: 0.570909  [  128/  146]
train() client id: f_00005-3-0 loss: 0.502789  [   32/  146]
train() client id: f_00005-3-1 loss: 0.525198  [   64/  146]
train() client id: f_00005-3-2 loss: 0.565961  [   96/  146]
train() client id: f_00005-3-3 loss: 0.644345  [  128/  146]
train() client id: f_00005-4-0 loss: 0.733687  [   32/  146]
train() client id: f_00005-4-1 loss: 0.398186  [   64/  146]
train() client id: f_00005-4-2 loss: 0.788095  [   96/  146]
train() client id: f_00005-4-3 loss: 0.558225  [  128/  146]
train() client id: f_00005-5-0 loss: 0.556690  [   32/  146]
train() client id: f_00005-5-1 loss: 0.522831  [   64/  146]
train() client id: f_00005-5-2 loss: 0.597393  [   96/  146]
train() client id: f_00005-5-3 loss: 0.601940  [  128/  146]
train() client id: f_00005-6-0 loss: 0.615947  [   32/  146]
train() client id: f_00005-6-1 loss: 0.483349  [   64/  146]
train() client id: f_00005-6-2 loss: 0.564005  [   96/  146]
train() client id: f_00005-6-3 loss: 0.735287  [  128/  146]
train() client id: f_00005-7-0 loss: 0.397105  [   32/  146]
train() client id: f_00005-7-1 loss: 0.547594  [   64/  146]
train() client id: f_00005-7-2 loss: 0.603601  [   96/  146]
train() client id: f_00005-7-3 loss: 0.570760  [  128/  146]
train() client id: f_00005-8-0 loss: 0.562886  [   32/  146]
train() client id: f_00005-8-1 loss: 0.550513  [   64/  146]
train() client id: f_00005-8-2 loss: 0.463079  [   96/  146]
train() client id: f_00005-8-3 loss: 0.656024  [  128/  146]
train() client id: f_00005-9-0 loss: 0.401526  [   32/  146]
train() client id: f_00005-9-1 loss: 1.024117  [   64/  146]
train() client id: f_00005-9-2 loss: 0.423609  [   96/  146]
train() client id: f_00005-9-3 loss: 0.560124  [  128/  146]
train() client id: f_00005-10-0 loss: 0.710893  [   32/  146]
train() client id: f_00005-10-1 loss: 0.485797  [   64/  146]
train() client id: f_00005-10-2 loss: 0.500902  [   96/  146]
train() client id: f_00005-10-3 loss: 0.733815  [  128/  146]
train() client id: f_00005-11-0 loss: 0.731162  [   32/  146]
train() client id: f_00005-11-1 loss: 0.548078  [   64/  146]
train() client id: f_00005-11-2 loss: 0.621439  [   96/  146]
train() client id: f_00005-11-3 loss: 0.512857  [  128/  146]
train() client id: f_00006-0-0 loss: 0.559114  [   32/   54]
train() client id: f_00006-1-0 loss: 0.538268  [   32/   54]
train() client id: f_00006-2-0 loss: 0.639916  [   32/   54]
train() client id: f_00006-3-0 loss: 0.581514  [   32/   54]
train() client id: f_00006-4-0 loss: 0.602835  [   32/   54]
train() client id: f_00006-5-0 loss: 0.600564  [   32/   54]
train() client id: f_00006-6-0 loss: 0.643182  [   32/   54]
train() client id: f_00006-7-0 loss: 0.604842  [   32/   54]
train() client id: f_00006-8-0 loss: 0.585326  [   32/   54]
train() client id: f_00006-9-0 loss: 0.589416  [   32/   54]
train() client id: f_00006-10-0 loss: 0.580362  [   32/   54]
train() client id: f_00006-11-0 loss: 0.658714  [   32/   54]
train() client id: f_00007-0-0 loss: 0.733404  [   32/  179]
train() client id: f_00007-0-1 loss: 0.598057  [   64/  179]
train() client id: f_00007-0-2 loss: 0.614428  [   96/  179]
train() client id: f_00007-0-3 loss: 0.638231  [  128/  179]
train() client id: f_00007-0-4 loss: 0.469810  [  160/  179]
train() client id: f_00007-1-0 loss: 0.661675  [   32/  179]
train() client id: f_00007-1-1 loss: 0.565400  [   64/  179]
train() client id: f_00007-1-2 loss: 0.567643  [   96/  179]
train() client id: f_00007-1-3 loss: 0.514695  [  128/  179]
train() client id: f_00007-1-4 loss: 0.776753  [  160/  179]
train() client id: f_00007-2-0 loss: 0.678724  [   32/  179]
train() client id: f_00007-2-1 loss: 0.581050  [   64/  179]
train() client id: f_00007-2-2 loss: 0.480560  [   96/  179]
train() client id: f_00007-2-3 loss: 0.654490  [  128/  179]
train() client id: f_00007-2-4 loss: 0.695590  [  160/  179]
train() client id: f_00007-3-0 loss: 0.499342  [   32/  179]
train() client id: f_00007-3-1 loss: 0.557419  [   64/  179]
train() client id: f_00007-3-2 loss: 0.484793  [   96/  179]
train() client id: f_00007-3-3 loss: 0.681346  [  128/  179]
train() client id: f_00007-3-4 loss: 0.771059  [  160/  179]
train() client id: f_00007-4-0 loss: 0.646229  [   32/  179]
train() client id: f_00007-4-1 loss: 0.629546  [   64/  179]
train() client id: f_00007-4-2 loss: 0.628259  [   96/  179]
train() client id: f_00007-4-3 loss: 0.678127  [  128/  179]
train() client id: f_00007-4-4 loss: 0.458451  [  160/  179]
train() client id: f_00007-5-0 loss: 0.465423  [   32/  179]
train() client id: f_00007-5-1 loss: 0.489129  [   64/  179]
train() client id: f_00007-5-2 loss: 0.733563  [   96/  179]
train() client id: f_00007-5-3 loss: 0.645030  [  128/  179]
train() client id: f_00007-5-4 loss: 0.674778  [  160/  179]
train() client id: f_00007-6-0 loss: 0.465862  [   32/  179]
train() client id: f_00007-6-1 loss: 0.547415  [   64/  179]
train() client id: f_00007-6-2 loss: 0.674458  [   96/  179]
train() client id: f_00007-6-3 loss: 0.497445  [  128/  179]
train() client id: f_00007-6-4 loss: 0.583710  [  160/  179]
train() client id: f_00007-7-0 loss: 0.571797  [   32/  179]
train() client id: f_00007-7-1 loss: 0.568124  [   64/  179]
train() client id: f_00007-7-2 loss: 0.604343  [   96/  179]
train() client id: f_00007-7-3 loss: 0.761199  [  128/  179]
train() client id: f_00007-7-4 loss: 0.468328  [  160/  179]
train() client id: f_00007-8-0 loss: 0.923957  [   32/  179]
train() client id: f_00007-8-1 loss: 0.513057  [   64/  179]
train() client id: f_00007-8-2 loss: 0.430418  [   96/  179]
train() client id: f_00007-8-3 loss: 0.443076  [  128/  179]
train() client id: f_00007-8-4 loss: 0.567378  [  160/  179]
train() client id: f_00007-9-0 loss: 0.780178  [   32/  179]
train() client id: f_00007-9-1 loss: 0.597443  [   64/  179]
train() client id: f_00007-9-2 loss: 0.529070  [   96/  179]
train() client id: f_00007-9-3 loss: 0.424401  [  128/  179]
train() client id: f_00007-9-4 loss: 0.567562  [  160/  179]
train() client id: f_00007-10-0 loss: 0.547265  [   32/  179]
train() client id: f_00007-10-1 loss: 0.579791  [   64/  179]
train() client id: f_00007-10-2 loss: 0.713248  [   96/  179]
train() client id: f_00007-10-3 loss: 0.452099  [  128/  179]
train() client id: f_00007-10-4 loss: 0.503366  [  160/  179]
train() client id: f_00007-11-0 loss: 0.647817  [   32/  179]
train() client id: f_00007-11-1 loss: 0.589008  [   64/  179]
train() client id: f_00007-11-2 loss: 0.511203  [   96/  179]
train() client id: f_00007-11-3 loss: 0.512039  [  128/  179]
train() client id: f_00007-11-4 loss: 0.618503  [  160/  179]
train() client id: f_00008-0-0 loss: 0.675095  [   32/  130]
train() client id: f_00008-0-1 loss: 0.863499  [   64/  130]
train() client id: f_00008-0-2 loss: 0.671863  [   96/  130]
train() client id: f_00008-0-3 loss: 0.827752  [  128/  130]
train() client id: f_00008-1-0 loss: 0.573714  [   32/  130]
train() client id: f_00008-1-1 loss: 0.655237  [   64/  130]
train() client id: f_00008-1-2 loss: 0.837991  [   96/  130]
train() client id: f_00008-1-3 loss: 0.935453  [  128/  130]
train() client id: f_00008-2-0 loss: 0.752661  [   32/  130]
train() client id: f_00008-2-1 loss: 0.685864  [   64/  130]
train() client id: f_00008-2-2 loss: 0.777762  [   96/  130]
train() client id: f_00008-2-3 loss: 0.811769  [  128/  130]
train() client id: f_00008-3-0 loss: 0.742127  [   32/  130]
train() client id: f_00008-3-1 loss: 0.693882  [   64/  130]
train() client id: f_00008-3-2 loss: 0.855941  [   96/  130]
train() client id: f_00008-3-3 loss: 0.739635  [  128/  130]
train() client id: f_00008-4-0 loss: 0.798116  [   32/  130]
train() client id: f_00008-4-1 loss: 0.767012  [   64/  130]
train() client id: f_00008-4-2 loss: 0.717670  [   96/  130]
train() client id: f_00008-4-3 loss: 0.730442  [  128/  130]
train() client id: f_00008-5-0 loss: 0.786153  [   32/  130]
train() client id: f_00008-5-1 loss: 0.771228  [   64/  130]
train() client id: f_00008-5-2 loss: 0.722868  [   96/  130]
train() client id: f_00008-5-3 loss: 0.713125  [  128/  130]
train() client id: f_00008-6-0 loss: 0.719421  [   32/  130]
train() client id: f_00008-6-1 loss: 0.781969  [   64/  130]
train() client id: f_00008-6-2 loss: 0.732135  [   96/  130]
train() client id: f_00008-6-3 loss: 0.773274  [  128/  130]
train() client id: f_00008-7-0 loss: 0.642725  [   32/  130]
train() client id: f_00008-7-1 loss: 0.741880  [   64/  130]
train() client id: f_00008-7-2 loss: 0.751289  [   96/  130]
train() client id: f_00008-7-3 loss: 0.847933  [  128/  130]
train() client id: f_00008-8-0 loss: 0.769993  [   32/  130]
train() client id: f_00008-8-1 loss: 0.712007  [   64/  130]
train() client id: f_00008-8-2 loss: 0.757608  [   96/  130]
train() client id: f_00008-8-3 loss: 0.786415  [  128/  130]
train() client id: f_00008-9-0 loss: 0.635546  [   32/  130]
train() client id: f_00008-9-1 loss: 0.762930  [   64/  130]
train() client id: f_00008-9-2 loss: 0.841287  [   96/  130]
train() client id: f_00008-9-3 loss: 0.776091  [  128/  130]
train() client id: f_00008-10-0 loss: 0.769470  [   32/  130]
train() client id: f_00008-10-1 loss: 0.781590  [   64/  130]
train() client id: f_00008-10-2 loss: 0.807845  [   96/  130]
train() client id: f_00008-10-3 loss: 0.659552  [  128/  130]
train() client id: f_00008-11-0 loss: 0.688615  [   32/  130]
train() client id: f_00008-11-1 loss: 0.728965  [   64/  130]
train() client id: f_00008-11-2 loss: 0.795063  [   96/  130]
train() client id: f_00008-11-3 loss: 0.811941  [  128/  130]
train() client id: f_00009-0-0 loss: 1.258510  [   32/  118]
train() client id: f_00009-0-1 loss: 1.210527  [   64/  118]
train() client id: f_00009-0-2 loss: 1.024677  [   96/  118]
train() client id: f_00009-1-0 loss: 1.103042  [   32/  118]
train() client id: f_00009-1-1 loss: 1.189717  [   64/  118]
train() client id: f_00009-1-2 loss: 1.084243  [   96/  118]
train() client id: f_00009-2-0 loss: 0.911053  [   32/  118]
train() client id: f_00009-2-1 loss: 1.139822  [   64/  118]
train() client id: f_00009-2-2 loss: 1.089719  [   96/  118]
train() client id: f_00009-3-0 loss: 0.950028  [   32/  118]
train() client id: f_00009-3-1 loss: 1.158359  [   64/  118]
train() client id: f_00009-3-2 loss: 0.983636  [   96/  118]
train() client id: f_00009-4-0 loss: 1.039700  [   32/  118]
train() client id: f_00009-4-1 loss: 0.957842  [   64/  118]
train() client id: f_00009-4-2 loss: 1.033426  [   96/  118]
train() client id: f_00009-5-0 loss: 1.034823  [   32/  118]
train() client id: f_00009-5-1 loss: 0.967108  [   64/  118]
train() client id: f_00009-5-2 loss: 1.010459  [   96/  118]
train() client id: f_00009-6-0 loss: 1.000627  [   32/  118]
train() client id: f_00009-6-1 loss: 0.967227  [   64/  118]
train() client id: f_00009-6-2 loss: 0.901190  [   96/  118]
train() client id: f_00009-7-0 loss: 1.063667  [   32/  118]
train() client id: f_00009-7-1 loss: 0.803982  [   64/  118]
train() client id: f_00009-7-2 loss: 0.985433  [   96/  118]
train() client id: f_00009-8-0 loss: 0.881403  [   32/  118]
train() client id: f_00009-8-1 loss: 1.064312  [   64/  118]
train() client id: f_00009-8-2 loss: 0.969353  [   96/  118]
train() client id: f_00009-9-0 loss: 0.873643  [   32/  118]
train() client id: f_00009-9-1 loss: 1.025648  [   64/  118]
train() client id: f_00009-9-2 loss: 0.924939  [   96/  118]
train() client id: f_00009-10-0 loss: 0.978779  [   32/  118]
train() client id: f_00009-10-1 loss: 0.783994  [   64/  118]
train() client id: f_00009-10-2 loss: 1.061541  [   96/  118]
train() client id: f_00009-11-0 loss: 0.955758  [   32/  118]
train() client id: f_00009-11-1 loss: 0.922523  [   64/  118]
train() client id: f_00009-11-2 loss: 0.975253  [   96/  118]
At round 27 accuracy: 0.6525198938992043
At round 27 training accuracy: 0.5808182427900738
At round 27 training loss: 0.8420858935571053
update_location
xs = [  -3.9056584     4.20031788  155.00902392   18.81129433    0.97929623
    3.95640986 -117.44319194  -96.32485185  139.66397685  -82.06087855]
ys = [ 147.5879595   130.55583871    1.32061395 -117.45517586  109.35018685
   92.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [178.31842293 164.50674664 184.47097744 155.40136142 148.18374534
 136.49219669 154.27181815 138.84939085 172.66932676 129.42178968]
dists_bs = [173.27635064 184.62945973 372.92155342 350.84590306 187.82601169
 196.92615481 186.71749037 191.12203797 351.84930892 194.62315774]
uav_gains = [2.33299769e-11 2.87101678e-11 2.13317186e-11 3.31618763e-11
 3.73775877e-11 4.59306330e-11 3.37773315e-11 4.40026087e-11
 2.53630547e-11 5.24710704e-11]
bs_gains = [5.95420887e-11 4.98484075e-11 6.96249872e-12 8.25977351e-12
 4.75092334e-11 4.16144887e-11 4.83032198e-11 4.52505576e-11
 8.19398791e-12 4.30080195e-11]
Round 28
-------------------------------
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.2963697  15.14398805  7.18662324  2.58300247 17.46743421  8.40944175
  3.20540132 10.27679841  7.57595044  6.8226403 ]
obj_prev = 85.96764987436393
eta_min = 1.97492431522131e-13	eta_max = 0.9251230340770827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 19.96901274869134	eta = 0.909090909090909
af = 18.15364795335576	bf = 1.507335053902618	zeta = 35.613187548855024	eta = 0.509745102946827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 28.02638083867362	eta = 0.6477342921247089
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.65886868799346	eta = 0.6809609277055236
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.58884556407932	eta = 0.682754274141212
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.588647822049044	eta = 0.6827593518426902
eta = 0.6827593518426902
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.03148978 0.0662285  0.03098994 0.01074651 0.07647517 0.03648815
 0.01349562 0.04473547 0.03248945 0.02949042]
ene_total = [2.34522213 4.30343838 2.32693545 1.09048608 4.90813228 2.57679446
 1.24971036 3.05496696 2.56930912 2.1636526 ]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 0 obj = 4.772905437718752
eta = 0.6827593518426902
freqs = [39094714.05548728 79606171.90046072 38654407.63725649 13124341.80140644
 92081472.39087442 44152552.9427729  16468811.85493763 54017423.64815775
 43377597.092848   35640012.17367139]
eta_min = 0.6827593518432681	eta_max = 0.6827593518426932
af = 0.017883468190837512	bf = 1.507335053902618	zeta = 0.019671815009921264	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [3.00512114e-06 2.62056364e-05 2.89117971e-06 1.15578985e-07
 4.04875310e-05 4.44139964e-06 2.28546015e-07 8.15034742e-06
 3.81706619e-06 2.33890753e-06]
ene_total = [1.74481114 1.47881446 1.78319283 1.60772537 1.49641265 1.53101993
 1.60118017 1.51381715 2.32252357 1.51993757]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 1 obj = 4.772905437718796
eta = 0.6827593518426932
freqs = [39094714.05548728 79606171.9004607  38654407.6372565  13124341.80140644
 92081472.39087439 44152552.94277289 16468811.85493763 54017423.64815772
 43377597.09284804 35640012.17367138]
Done!
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.15546788e-05 1.00760567e-04 1.11165744e-05 4.44400737e-07
 1.55674395e-04 1.70771639e-05 8.78758515e-07 3.13380533e-05
 1.46766043e-05 8.99309011e-06]
ene_total = [0.00854045 0.00730576 0.0087278  0.00786189 0.00743245 0.00749912
 0.00783021 0.00742555 0.01136769 0.00743895]
At round 28 energy consumption: 0.0814298731583106
At round 28 eta: 0.6827593518426932
At round 28 a_n: 18.59131914111349
At round 28 local rounds: 12.495942598469853
At round 28 global rounds: 58.60320626975522
gradient difference: 0.4718848764896393
train() client id: f_00000-0-0 loss: 1.202701  [   32/  126]
train() client id: f_00000-0-1 loss: 1.136712  [   64/  126]
train() client id: f_00000-0-2 loss: 1.171818  [   96/  126]
train() client id: f_00000-1-0 loss: 1.036802  [   32/  126]
train() client id: f_00000-1-1 loss: 0.958305  [   64/  126]
train() client id: f_00000-1-2 loss: 1.161512  [   96/  126]
train() client id: f_00000-2-0 loss: 0.985334  [   32/  126]
train() client id: f_00000-2-1 loss: 1.022229  [   64/  126]
train() client id: f_00000-2-2 loss: 1.052136  [   96/  126]
train() client id: f_00000-3-0 loss: 0.906927  [   32/  126]
train() client id: f_00000-3-1 loss: 0.959248  [   64/  126]
train() client id: f_00000-3-2 loss: 0.962667  [   96/  126]
train() client id: f_00000-4-0 loss: 0.938003  [   32/  126]
train() client id: f_00000-4-1 loss: 0.987713  [   64/  126]
train() client id: f_00000-4-2 loss: 0.987028  [   96/  126]
train() client id: f_00000-5-0 loss: 0.894314  [   32/  126]
train() client id: f_00000-5-1 loss: 0.895935  [   64/  126]
train() client id: f_00000-5-2 loss: 0.910811  [   96/  126]
train() client id: f_00000-6-0 loss: 0.947435  [   32/  126]
train() client id: f_00000-6-1 loss: 0.846580  [   64/  126]
train() client id: f_00000-6-2 loss: 0.907371  [   96/  126]
train() client id: f_00000-7-0 loss: 0.954924  [   32/  126]
train() client id: f_00000-7-1 loss: 0.832518  [   64/  126]
train() client id: f_00000-7-2 loss: 0.940931  [   96/  126]
train() client id: f_00000-8-0 loss: 0.934388  [   32/  126]
train() client id: f_00000-8-1 loss: 0.895467  [   64/  126]
train() client id: f_00000-8-2 loss: 0.915539  [   96/  126]
train() client id: f_00000-9-0 loss: 0.876789  [   32/  126]
train() client id: f_00000-9-1 loss: 0.877971  [   64/  126]
train() client id: f_00000-9-2 loss: 1.008431  [   96/  126]
train() client id: f_00000-10-0 loss: 0.991121  [   32/  126]
train() client id: f_00000-10-1 loss: 0.992430  [   64/  126]
train() client id: f_00000-10-2 loss: 0.822127  [   96/  126]
train() client id: f_00000-11-0 loss: 0.865954  [   32/  126]
train() client id: f_00000-11-1 loss: 0.973813  [   64/  126]
train() client id: f_00000-11-2 loss: 0.948839  [   96/  126]
train() client id: f_00001-0-0 loss: 0.458667  [   32/  265]
train() client id: f_00001-0-1 loss: 0.280561  [   64/  265]
train() client id: f_00001-0-2 loss: 0.336915  [   96/  265]
train() client id: f_00001-0-3 loss: 0.236835  [  128/  265]
train() client id: f_00001-0-4 loss: 0.297478  [  160/  265]
train() client id: f_00001-0-5 loss: 0.230397  [  192/  265]
train() client id: f_00001-0-6 loss: 0.327412  [  224/  265]
train() client id: f_00001-0-7 loss: 0.301097  [  256/  265]
train() client id: f_00001-1-0 loss: 0.429592  [   32/  265]
train() client id: f_00001-1-1 loss: 0.330832  [   64/  265]
train() client id: f_00001-1-2 loss: 0.368958  [   96/  265]
train() client id: f_00001-1-3 loss: 0.247610  [  128/  265]
train() client id: f_00001-1-4 loss: 0.212704  [  160/  265]
train() client id: f_00001-1-5 loss: 0.220911  [  192/  265]
train() client id: f_00001-1-6 loss: 0.285089  [  224/  265]
train() client id: f_00001-1-7 loss: 0.258655  [  256/  265]
train() client id: f_00001-2-0 loss: 0.184411  [   32/  265]
train() client id: f_00001-2-1 loss: 0.232488  [   64/  265]
train() client id: f_00001-2-2 loss: 0.218581  [   96/  265]
train() client id: f_00001-2-3 loss: 0.230105  [  128/  265]
train() client id: f_00001-2-4 loss: 0.402135  [  160/  265]
train() client id: f_00001-2-5 loss: 0.293984  [  192/  265]
train() client id: f_00001-2-6 loss: 0.338288  [  224/  265]
train() client id: f_00001-2-7 loss: 0.350230  [  256/  265]
train() client id: f_00001-3-0 loss: 0.386110  [   32/  265]
train() client id: f_00001-3-1 loss: 0.256256  [   64/  265]
train() client id: f_00001-3-2 loss: 0.381930  [   96/  265]
train() client id: f_00001-3-3 loss: 0.180009  [  128/  265]
train() client id: f_00001-3-4 loss: 0.252625  [  160/  265]
train() client id: f_00001-3-5 loss: 0.284298  [  192/  265]
train() client id: f_00001-3-6 loss: 0.263111  [  224/  265]
train() client id: f_00001-3-7 loss: 0.183518  [  256/  265]
train() client id: f_00001-4-0 loss: 0.244636  [   32/  265]
train() client id: f_00001-4-1 loss: 0.222013  [   64/  265]
train() client id: f_00001-4-2 loss: 0.189878  [   96/  265]
train() client id: f_00001-4-3 loss: 0.462900  [  128/  265]
train() client id: f_00001-4-4 loss: 0.273240  [  160/  265]
train() client id: f_00001-4-5 loss: 0.245424  [  192/  265]
train() client id: f_00001-4-6 loss: 0.193074  [  224/  265]
train() client id: f_00001-4-7 loss: 0.296539  [  256/  265]
train() client id: f_00001-5-0 loss: 0.186547  [   32/  265]
train() client id: f_00001-5-1 loss: 0.186667  [   64/  265]
train() client id: f_00001-5-2 loss: 0.309422  [   96/  265]
train() client id: f_00001-5-3 loss: 0.197922  [  128/  265]
train() client id: f_00001-5-4 loss: 0.365212  [  160/  265]
train() client id: f_00001-5-5 loss: 0.297365  [  192/  265]
train() client id: f_00001-5-6 loss: 0.260864  [  224/  265]
train() client id: f_00001-5-7 loss: 0.278915  [  256/  265]
train() client id: f_00001-6-0 loss: 0.150965  [   32/  265]
train() client id: f_00001-6-1 loss: 0.286223  [   64/  265]
train() client id: f_00001-6-2 loss: 0.203530  [   96/  265]
train() client id: f_00001-6-3 loss: 0.319826  [  128/  265]
train() client id: f_00001-6-4 loss: 0.317126  [  160/  265]
train() client id: f_00001-6-5 loss: 0.147638  [  192/  265]
train() client id: f_00001-6-6 loss: 0.248155  [  224/  265]
train() client id: f_00001-6-7 loss: 0.314802  [  256/  265]
train() client id: f_00001-7-0 loss: 0.168992  [   32/  265]
train() client id: f_00001-7-1 loss: 0.328335  [   64/  265]
train() client id: f_00001-7-2 loss: 0.399571  [   96/  265]
train() client id: f_00001-7-3 loss: 0.163919  [  128/  265]
train() client id: f_00001-7-4 loss: 0.214930  [  160/  265]
train() client id: f_00001-7-5 loss: 0.335528  [  192/  265]
train() client id: f_00001-7-6 loss: 0.218061  [  224/  265]
train() client id: f_00001-7-7 loss: 0.171846  [  256/  265]
train() client id: f_00001-8-0 loss: 0.140580  [   32/  265]
train() client id: f_00001-8-1 loss: 0.279758  [   64/  265]
train() client id: f_00001-8-2 loss: 0.276814  [   96/  265]
train() client id: f_00001-8-3 loss: 0.323364  [  128/  265]
train() client id: f_00001-8-4 loss: 0.378453  [  160/  265]
train() client id: f_00001-8-5 loss: 0.194021  [  192/  265]
train() client id: f_00001-8-6 loss: 0.214603  [  224/  265]
train() client id: f_00001-8-7 loss: 0.160207  [  256/  265]
train() client id: f_00001-9-0 loss: 0.146200  [   32/  265]
train() client id: f_00001-9-1 loss: 0.223292  [   64/  265]
train() client id: f_00001-9-2 loss: 0.178897  [   96/  265]
train() client id: f_00001-9-3 loss: 0.309718  [  128/  265]
train() client id: f_00001-9-4 loss: 0.204484  [  160/  265]
train() client id: f_00001-9-5 loss: 0.239194  [  192/  265]
train() client id: f_00001-9-6 loss: 0.242803  [  224/  265]
train() client id: f_00001-9-7 loss: 0.394148  [  256/  265]
train() client id: f_00001-10-0 loss: 0.232183  [   32/  265]
train() client id: f_00001-10-1 loss: 0.216464  [   64/  265]
train() client id: f_00001-10-2 loss: 0.203084  [   96/  265]
train() client id: f_00001-10-3 loss: 0.254448  [  128/  265]
train() client id: f_00001-10-4 loss: 0.204357  [  160/  265]
train() client id: f_00001-10-5 loss: 0.136630  [  192/  265]
train() client id: f_00001-10-6 loss: 0.299996  [  224/  265]
train() client id: f_00001-10-7 loss: 0.318512  [  256/  265]
train() client id: f_00001-11-0 loss: 0.160168  [   32/  265]
train() client id: f_00001-11-1 loss: 0.349961  [   64/  265]
train() client id: f_00001-11-2 loss: 0.213029  [   96/  265]
train() client id: f_00001-11-3 loss: 0.151374  [  128/  265]
train() client id: f_00001-11-4 loss: 0.298479  [  160/  265]
train() client id: f_00001-11-5 loss: 0.314143  [  192/  265]
train() client id: f_00001-11-6 loss: 0.198244  [  224/  265]
train() client id: f_00001-11-7 loss: 0.216445  [  256/  265]
train() client id: f_00002-0-0 loss: 1.318152  [   32/  124]
train() client id: f_00002-0-1 loss: 1.103509  [   64/  124]
train() client id: f_00002-0-2 loss: 1.273792  [   96/  124]
train() client id: f_00002-1-0 loss: 1.304406  [   32/  124]
train() client id: f_00002-1-1 loss: 1.183149  [   64/  124]
train() client id: f_00002-1-2 loss: 1.077939  [   96/  124]
train() client id: f_00002-2-0 loss: 1.169831  [   32/  124]
train() client id: f_00002-2-1 loss: 1.133127  [   64/  124]
train() client id: f_00002-2-2 loss: 1.148710  [   96/  124]
train() client id: f_00002-3-0 loss: 1.189793  [   32/  124]
train() client id: f_00002-3-1 loss: 1.115834  [   64/  124]
train() client id: f_00002-3-2 loss: 1.203706  [   96/  124]
train() client id: f_00002-4-0 loss: 1.110531  [   32/  124]
train() client id: f_00002-4-1 loss: 1.156899  [   64/  124]
train() client id: f_00002-4-2 loss: 1.109979  [   96/  124]
train() client id: f_00002-5-0 loss: 1.180661  [   32/  124]
train() client id: f_00002-5-1 loss: 1.009804  [   64/  124]
train() client id: f_00002-5-2 loss: 1.039711  [   96/  124]
train() client id: f_00002-6-0 loss: 1.061931  [   32/  124]
train() client id: f_00002-6-1 loss: 1.011836  [   64/  124]
train() client id: f_00002-6-2 loss: 1.039766  [   96/  124]
train() client id: f_00002-7-0 loss: 1.026628  [   32/  124]
train() client id: f_00002-7-1 loss: 1.001746  [   64/  124]
train() client id: f_00002-7-2 loss: 1.097400  [   96/  124]
train() client id: f_00002-8-0 loss: 0.950266  [   32/  124]
train() client id: f_00002-8-1 loss: 1.149050  [   64/  124]
train() client id: f_00002-8-2 loss: 1.015353  [   96/  124]
train() client id: f_00002-9-0 loss: 1.072088  [   32/  124]
train() client id: f_00002-9-1 loss: 0.966558  [   64/  124]
train() client id: f_00002-9-2 loss: 1.004174  [   96/  124]
train() client id: f_00002-10-0 loss: 1.108207  [   32/  124]
train() client id: f_00002-10-1 loss: 0.949793  [   64/  124]
train() client id: f_00002-10-2 loss: 1.127491  [   96/  124]
train() client id: f_00002-11-0 loss: 1.055948  [   32/  124]
train() client id: f_00002-11-1 loss: 1.040098  [   64/  124]
train() client id: f_00002-11-2 loss: 0.933430  [   96/  124]
train() client id: f_00003-0-0 loss: 0.776190  [   32/   43]
train() client id: f_00003-1-0 loss: 0.756305  [   32/   43]
train() client id: f_00003-2-0 loss: 0.960431  [   32/   43]
train() client id: f_00003-3-0 loss: 0.626650  [   32/   43]
train() client id: f_00003-4-0 loss: 0.752663  [   32/   43]
train() client id: f_00003-5-0 loss: 0.866437  [   32/   43]
train() client id: f_00003-6-0 loss: 0.803674  [   32/   43]
train() client id: f_00003-7-0 loss: 0.873553  [   32/   43]
train() client id: f_00003-8-0 loss: 0.894065  [   32/   43]
train() client id: f_00003-9-0 loss: 0.887834  [   32/   43]
train() client id: f_00003-10-0 loss: 0.940305  [   32/   43]
train() client id: f_00003-11-0 loss: 0.789387  [   32/   43]
train() client id: f_00004-0-0 loss: 0.909812  [   32/  306]
train() client id: f_00004-0-1 loss: 0.708366  [   64/  306]
train() client id: f_00004-0-2 loss: 0.589740  [   96/  306]
train() client id: f_00004-0-3 loss: 0.802337  [  128/  306]
train() client id: f_00004-0-4 loss: 0.719993  [  160/  306]
train() client id: f_00004-0-5 loss: 0.754162  [  192/  306]
train() client id: f_00004-0-6 loss: 0.732512  [  224/  306]
train() client id: f_00004-0-7 loss: 0.786008  [  256/  306]
train() client id: f_00004-0-8 loss: 0.913045  [  288/  306]
train() client id: f_00004-1-0 loss: 0.695448  [   32/  306]
train() client id: f_00004-1-1 loss: 0.690537  [   64/  306]
train() client id: f_00004-1-2 loss: 0.752041  [   96/  306]
train() client id: f_00004-1-3 loss: 0.728581  [  128/  306]
train() client id: f_00004-1-4 loss: 0.744772  [  160/  306]
train() client id: f_00004-1-5 loss: 0.889742  [  192/  306]
train() client id: f_00004-1-6 loss: 0.838859  [  224/  306]
train() client id: f_00004-1-7 loss: 0.721390  [  256/  306]
train() client id: f_00004-1-8 loss: 0.819777  [  288/  306]
train() client id: f_00004-2-0 loss: 0.744696  [   32/  306]
train() client id: f_00004-2-1 loss: 0.704232  [   64/  306]
train() client id: f_00004-2-2 loss: 0.808902  [   96/  306]
train() client id: f_00004-2-3 loss: 0.656942  [  128/  306]
train() client id: f_00004-2-4 loss: 0.791322  [  160/  306]
train() client id: f_00004-2-5 loss: 0.894543  [  192/  306]
train() client id: f_00004-2-6 loss: 0.761517  [  224/  306]
train() client id: f_00004-2-7 loss: 0.841099  [  256/  306]
train() client id: f_00004-2-8 loss: 0.761605  [  288/  306]
train() client id: f_00004-3-0 loss: 0.737574  [   32/  306]
train() client id: f_00004-3-1 loss: 0.807954  [   64/  306]
train() client id: f_00004-3-2 loss: 0.819347  [   96/  306]
train() client id: f_00004-3-3 loss: 0.716546  [  128/  306]
train() client id: f_00004-3-4 loss: 0.735771  [  160/  306]
train() client id: f_00004-3-5 loss: 0.765863  [  192/  306]
train() client id: f_00004-3-6 loss: 0.870883  [  224/  306]
train() client id: f_00004-3-7 loss: 0.678159  [  256/  306]
train() client id: f_00004-3-8 loss: 0.760357  [  288/  306]
train() client id: f_00004-4-0 loss: 0.776987  [   32/  306]
train() client id: f_00004-4-1 loss: 0.726215  [   64/  306]
train() client id: f_00004-4-2 loss: 0.796263  [   96/  306]
train() client id: f_00004-4-3 loss: 0.593817  [  128/  306]
train() client id: f_00004-4-4 loss: 0.930346  [  160/  306]
train() client id: f_00004-4-5 loss: 0.758651  [  192/  306]
train() client id: f_00004-4-6 loss: 0.779731  [  224/  306]
train() client id: f_00004-4-7 loss: 0.802961  [  256/  306]
train() client id: f_00004-4-8 loss: 0.732107  [  288/  306]
train() client id: f_00004-5-0 loss: 0.724697  [   32/  306]
train() client id: f_00004-5-1 loss: 0.710566  [   64/  306]
train() client id: f_00004-5-2 loss: 0.729902  [   96/  306]
train() client id: f_00004-5-3 loss: 0.771506  [  128/  306]
train() client id: f_00004-5-4 loss: 0.803477  [  160/  306]
train() client id: f_00004-5-5 loss: 0.718191  [  192/  306]
train() client id: f_00004-5-6 loss: 0.839133  [  224/  306]
train() client id: f_00004-5-7 loss: 0.783130  [  256/  306]
train() client id: f_00004-5-8 loss: 0.773164  [  288/  306]
train() client id: f_00004-6-0 loss: 0.728795  [   32/  306]
train() client id: f_00004-6-1 loss: 0.825031  [   64/  306]
train() client id: f_00004-6-2 loss: 0.668287  [   96/  306]
train() client id: f_00004-6-3 loss: 0.827874  [  128/  306]
train() client id: f_00004-6-4 loss: 0.810897  [  160/  306]
train() client id: f_00004-6-5 loss: 0.881473  [  192/  306]
train() client id: f_00004-6-6 loss: 0.764145  [  224/  306]
train() client id: f_00004-6-7 loss: 0.695397  [  256/  306]
train() client id: f_00004-6-8 loss: 0.675380  [  288/  306]
train() client id: f_00004-7-0 loss: 0.801302  [   32/  306]
train() client id: f_00004-7-1 loss: 0.793369  [   64/  306]
train() client id: f_00004-7-2 loss: 0.846881  [   96/  306]
train() client id: f_00004-7-3 loss: 0.790944  [  128/  306]
train() client id: f_00004-7-4 loss: 0.715666  [  160/  306]
train() client id: f_00004-7-5 loss: 0.669505  [  192/  306]
train() client id: f_00004-7-6 loss: 0.725030  [  224/  306]
train() client id: f_00004-7-7 loss: 0.839349  [  256/  306]
train() client id: f_00004-7-8 loss: 0.724282  [  288/  306]
train() client id: f_00004-8-0 loss: 0.769299  [   32/  306]
train() client id: f_00004-8-1 loss: 0.803566  [   64/  306]
train() client id: f_00004-8-2 loss: 0.832666  [   96/  306]
train() client id: f_00004-8-3 loss: 0.674473  [  128/  306]
train() client id: f_00004-8-4 loss: 0.753324  [  160/  306]
train() client id: f_00004-8-5 loss: 0.814373  [  192/  306]
train() client id: f_00004-8-6 loss: 0.751325  [  224/  306]
train() client id: f_00004-8-7 loss: 0.721160  [  256/  306]
train() client id: f_00004-8-8 loss: 0.860113  [  288/  306]
train() client id: f_00004-9-0 loss: 0.770916  [   32/  306]
train() client id: f_00004-9-1 loss: 0.747239  [   64/  306]
train() client id: f_00004-9-2 loss: 0.766392  [   96/  306]
train() client id: f_00004-9-3 loss: 0.815597  [  128/  306]
train() client id: f_00004-9-4 loss: 0.682183  [  160/  306]
train() client id: f_00004-9-5 loss: 0.743036  [  192/  306]
train() client id: f_00004-9-6 loss: 0.863520  [  224/  306]
train() client id: f_00004-9-7 loss: 0.874285  [  256/  306]
train() client id: f_00004-9-8 loss: 0.696520  [  288/  306]
train() client id: f_00004-10-0 loss: 0.790243  [   32/  306]
train() client id: f_00004-10-1 loss: 0.732291  [   64/  306]
train() client id: f_00004-10-2 loss: 0.814362  [   96/  306]
train() client id: f_00004-10-3 loss: 0.819813  [  128/  306]
train() client id: f_00004-10-4 loss: 0.836087  [  160/  306]
train() client id: f_00004-10-5 loss: 0.808762  [  192/  306]
train() client id: f_00004-10-6 loss: 0.750214  [  224/  306]
train() client id: f_00004-10-7 loss: 0.794965  [  256/  306]
train() client id: f_00004-10-8 loss: 0.709665  [  288/  306]
train() client id: f_00004-11-0 loss: 0.825280  [   32/  306]
train() client id: f_00004-11-1 loss: 0.736017  [   64/  306]
train() client id: f_00004-11-2 loss: 0.769822  [   96/  306]
train() client id: f_00004-11-3 loss: 0.749570  [  128/  306]
train() client id: f_00004-11-4 loss: 0.727946  [  160/  306]
train() client id: f_00004-11-5 loss: 0.801826  [  192/  306]
train() client id: f_00004-11-6 loss: 0.690077  [  224/  306]
train() client id: f_00004-11-7 loss: 0.831328  [  256/  306]
train() client id: f_00004-11-8 loss: 0.825175  [  288/  306]
train() client id: f_00005-0-0 loss: 0.838536  [   32/  146]
train() client id: f_00005-0-1 loss: 0.660278  [   64/  146]
train() client id: f_00005-0-2 loss: 0.815837  [   96/  146]
train() client id: f_00005-0-3 loss: 0.626168  [  128/  146]
train() client id: f_00005-1-0 loss: 0.758279  [   32/  146]
train() client id: f_00005-1-1 loss: 0.748144  [   64/  146]
train() client id: f_00005-1-2 loss: 0.839135  [   96/  146]
train() client id: f_00005-1-3 loss: 0.587581  [  128/  146]
train() client id: f_00005-2-0 loss: 0.687337  [   32/  146]
train() client id: f_00005-2-1 loss: 0.789030  [   64/  146]
train() client id: f_00005-2-2 loss: 0.800092  [   96/  146]
train() client id: f_00005-2-3 loss: 0.568106  [  128/  146]
train() client id: f_00005-3-0 loss: 0.612304  [   32/  146]
train() client id: f_00005-3-1 loss: 0.642660  [   64/  146]
train() client id: f_00005-3-2 loss: 1.018049  [   96/  146]
train() client id: f_00005-3-3 loss: 0.805666  [  128/  146]
train() client id: f_00005-4-0 loss: 0.714573  [   32/  146]
train() client id: f_00005-4-1 loss: 0.548943  [   64/  146]
train() client id: f_00005-4-2 loss: 0.794902  [   96/  146]
train() client id: f_00005-4-3 loss: 0.816873  [  128/  146]
train() client id: f_00005-5-0 loss: 0.812058  [   32/  146]
train() client id: f_00005-5-1 loss: 0.444196  [   64/  146]
train() client id: f_00005-5-2 loss: 0.787715  [   96/  146]
train() client id: f_00005-5-3 loss: 0.660995  [  128/  146]
train() client id: f_00005-6-0 loss: 0.661852  [   32/  146]
train() client id: f_00005-6-1 loss: 0.818805  [   64/  146]
train() client id: f_00005-6-2 loss: 0.698142  [   96/  146]
train() client id: f_00005-6-3 loss: 0.854359  [  128/  146]
train() client id: f_00005-7-0 loss: 0.856168  [   32/  146]
train() client id: f_00005-7-1 loss: 0.908741  [   64/  146]
train() client id: f_00005-7-2 loss: 0.731727  [   96/  146]
train() client id: f_00005-7-3 loss: 0.626903  [  128/  146]
train() client id: f_00005-8-0 loss: 0.660782  [   32/  146]
train() client id: f_00005-8-1 loss: 0.724506  [   64/  146]
train() client id: f_00005-8-2 loss: 0.670153  [   96/  146]
train() client id: f_00005-8-3 loss: 0.672419  [  128/  146]
train() client id: f_00005-9-0 loss: 0.873149  [   32/  146]
train() client id: f_00005-9-1 loss: 0.871652  [   64/  146]
train() client id: f_00005-9-2 loss: 0.687682  [   96/  146]
train() client id: f_00005-9-3 loss: 0.536325  [  128/  146]
train() client id: f_00005-10-0 loss: 0.691185  [   32/  146]
train() client id: f_00005-10-1 loss: 0.677578  [   64/  146]
train() client id: f_00005-10-2 loss: 0.784300  [   96/  146]
train() client id: f_00005-10-3 loss: 0.860021  [  128/  146]
train() client id: f_00005-11-0 loss: 0.736344  [   32/  146]
train() client id: f_00005-11-1 loss: 0.633599  [   64/  146]
train() client id: f_00005-11-2 loss: 0.781499  [   96/  146]
train() client id: f_00005-11-3 loss: 0.899062  [  128/  146]
train() client id: f_00006-0-0 loss: 0.513243  [   32/   54]
train() client id: f_00006-1-0 loss: 0.545420  [   32/   54]
train() client id: f_00006-2-0 loss: 0.498061  [   32/   54]
train() client id: f_00006-3-0 loss: 0.507713  [   32/   54]
train() client id: f_00006-4-0 loss: 0.460081  [   32/   54]
train() client id: f_00006-5-0 loss: 0.435501  [   32/   54]
train() client id: f_00006-6-0 loss: 0.490913  [   32/   54]
train() client id: f_00006-7-0 loss: 0.543158  [   32/   54]
train() client id: f_00006-8-0 loss: 0.480663  [   32/   54]
train() client id: f_00006-9-0 loss: 0.498276  [   32/   54]
train() client id: f_00006-10-0 loss: 0.550140  [   32/   54]
train() client id: f_00006-11-0 loss: 0.489311  [   32/   54]
train() client id: f_00007-0-0 loss: 0.507203  [   32/  179]
train() client id: f_00007-0-1 loss: 0.596989  [   64/  179]
train() client id: f_00007-0-2 loss: 0.591485  [   96/  179]
train() client id: f_00007-0-3 loss: 0.649721  [  128/  179]
train() client id: f_00007-0-4 loss: 0.500332  [  160/  179]
train() client id: f_00007-1-0 loss: 0.618652  [   32/  179]
train() client id: f_00007-1-1 loss: 0.493398  [   64/  179]
train() client id: f_00007-1-2 loss: 0.752388  [   96/  179]
train() client id: f_00007-1-3 loss: 0.500166  [  128/  179]
train() client id: f_00007-1-4 loss: 0.392568  [  160/  179]
train() client id: f_00007-2-0 loss: 0.452518  [   32/  179]
train() client id: f_00007-2-1 loss: 0.498406  [   64/  179]
train() client id: f_00007-2-2 loss: 0.472688  [   96/  179]
train() client id: f_00007-2-3 loss: 0.573936  [  128/  179]
train() client id: f_00007-2-4 loss: 0.700366  [  160/  179]
train() client id: f_00007-3-0 loss: 0.700743  [   32/  179]
train() client id: f_00007-3-1 loss: 0.445868  [   64/  179]
train() client id: f_00007-3-2 loss: 0.384966  [   96/  179]
train() client id: f_00007-3-3 loss: 0.452435  [  128/  179]
train() client id: f_00007-3-4 loss: 0.604352  [  160/  179]
train() client id: f_00007-4-0 loss: 0.470757  [   32/  179]
train() client id: f_00007-4-1 loss: 0.507678  [   64/  179]
train() client id: f_00007-4-2 loss: 0.483089  [   96/  179]
train() client id: f_00007-4-3 loss: 0.705850  [  128/  179]
train() client id: f_00007-4-4 loss: 0.455720  [  160/  179]
train() client id: f_00007-5-0 loss: 0.643900  [   32/  179]
train() client id: f_00007-5-1 loss: 0.545401  [   64/  179]
train() client id: f_00007-5-2 loss: 0.430953  [   96/  179]
train() client id: f_00007-5-3 loss: 0.538518  [  128/  179]
train() client id: f_00007-5-4 loss: 0.376836  [  160/  179]
train() client id: f_00007-6-0 loss: 0.640511  [   32/  179]
train() client id: f_00007-6-1 loss: 0.601467  [   64/  179]
train() client id: f_00007-6-2 loss: 0.499266  [   96/  179]
train() client id: f_00007-6-3 loss: 0.368282  [  128/  179]
train() client id: f_00007-6-4 loss: 0.482286  [  160/  179]
train() client id: f_00007-7-0 loss: 0.361724  [   32/  179]
train() client id: f_00007-7-1 loss: 0.580559  [   64/  179]
train() client id: f_00007-7-2 loss: 0.373207  [   96/  179]
train() client id: f_00007-7-3 loss: 0.448144  [  128/  179]
train() client id: f_00007-7-4 loss: 0.619013  [  160/  179]
train() client id: f_00007-8-0 loss: 0.602366  [   32/  179]
train() client id: f_00007-8-1 loss: 0.447503  [   64/  179]
train() client id: f_00007-8-2 loss: 0.431251  [   96/  179]
train() client id: f_00007-8-3 loss: 0.613765  [  128/  179]
train() client id: f_00007-8-4 loss: 0.446388  [  160/  179]
train() client id: f_00007-9-0 loss: 0.523321  [   32/  179]
train() client id: f_00007-9-1 loss: 0.458395  [   64/  179]
train() client id: f_00007-9-2 loss: 0.686855  [   96/  179]
train() client id: f_00007-9-3 loss: 0.458600  [  128/  179]
train() client id: f_00007-9-4 loss: 0.423874  [  160/  179]
train() client id: f_00007-10-0 loss: 0.374747  [   32/  179]
train() client id: f_00007-10-1 loss: 0.498121  [   64/  179]
train() client id: f_00007-10-2 loss: 0.600690  [   96/  179]
train() client id: f_00007-10-3 loss: 0.424573  [  128/  179]
train() client id: f_00007-10-4 loss: 0.563210  [  160/  179]
train() client id: f_00007-11-0 loss: 0.359548  [   32/  179]
train() client id: f_00007-11-1 loss: 0.493124  [   64/  179]
train() client id: f_00007-11-2 loss: 0.652715  [   96/  179]
train() client id: f_00007-11-3 loss: 0.441037  [  128/  179]
train() client id: f_00007-11-4 loss: 0.440557  [  160/  179]
train() client id: f_00008-0-0 loss: 0.730910  [   32/  130]
train() client id: f_00008-0-1 loss: 0.708161  [   64/  130]
train() client id: f_00008-0-2 loss: 0.702984  [   96/  130]
train() client id: f_00008-0-3 loss: 0.615754  [  128/  130]
train() client id: f_00008-1-0 loss: 0.863955  [   32/  130]
train() client id: f_00008-1-1 loss: 0.623998  [   64/  130]
train() client id: f_00008-1-2 loss: 0.665435  [   96/  130]
train() client id: f_00008-1-3 loss: 0.636820  [  128/  130]
train() client id: f_00008-2-0 loss: 0.723101  [   32/  130]
train() client id: f_00008-2-1 loss: 0.691215  [   64/  130]
train() client id: f_00008-2-2 loss: 0.599468  [   96/  130]
train() client id: f_00008-2-3 loss: 0.738502  [  128/  130]
train() client id: f_00008-3-0 loss: 0.639556  [   32/  130]
train() client id: f_00008-3-1 loss: 0.719406  [   64/  130]
train() client id: f_00008-3-2 loss: 0.664476  [   96/  130]
train() client id: f_00008-3-3 loss: 0.733605  [  128/  130]
train() client id: f_00008-4-0 loss: 0.733669  [   32/  130]
train() client id: f_00008-4-1 loss: 0.729370  [   64/  130]
train() client id: f_00008-4-2 loss: 0.583988  [   96/  130]
train() client id: f_00008-4-3 loss: 0.697567  [  128/  130]
train() client id: f_00008-5-0 loss: 0.706819  [   32/  130]
train() client id: f_00008-5-1 loss: 0.725627  [   64/  130]
train() client id: f_00008-5-2 loss: 0.726528  [   96/  130]
train() client id: f_00008-5-3 loss: 0.626667  [  128/  130]
train() client id: f_00008-6-0 loss: 0.707993  [   32/  130]
train() client id: f_00008-6-1 loss: 0.678354  [   64/  130]
train() client id: f_00008-6-2 loss: 0.722032  [   96/  130]
train() client id: f_00008-6-3 loss: 0.669243  [  128/  130]
train() client id: f_00008-7-0 loss: 0.848180  [   32/  130]
train() client id: f_00008-7-1 loss: 0.638303  [   64/  130]
train() client id: f_00008-7-2 loss: 0.620451  [   96/  130]
train() client id: f_00008-7-3 loss: 0.679678  [  128/  130]
train() client id: f_00008-8-0 loss: 0.568562  [   32/  130]
train() client id: f_00008-8-1 loss: 0.751754  [   64/  130]
train() client id: f_00008-8-2 loss: 0.705051  [   96/  130]
train() client id: f_00008-8-3 loss: 0.700122  [  128/  130]
train() client id: f_00008-9-0 loss: 0.737618  [   32/  130]
train() client id: f_00008-9-1 loss: 0.624181  [   64/  130]
train() client id: f_00008-9-2 loss: 0.770422  [   96/  130]
train() client id: f_00008-9-3 loss: 0.647287  [  128/  130]
train() client id: f_00008-10-0 loss: 0.680918  [   32/  130]
train() client id: f_00008-10-1 loss: 0.726431  [   64/  130]
train() client id: f_00008-10-2 loss: 0.681277  [   96/  130]
train() client id: f_00008-10-3 loss: 0.692828  [  128/  130]
train() client id: f_00008-11-0 loss: 0.796880  [   32/  130]
train() client id: f_00008-11-1 loss: 0.647045  [   64/  130]
train() client id: f_00008-11-2 loss: 0.632953  [   96/  130]
train() client id: f_00008-11-3 loss: 0.708754  [  128/  130]
train() client id: f_00009-0-0 loss: 1.062864  [   32/  118]
train() client id: f_00009-0-1 loss: 1.019399  [   64/  118]
train() client id: f_00009-0-2 loss: 0.990581  [   96/  118]
train() client id: f_00009-1-0 loss: 0.885645  [   32/  118]
train() client id: f_00009-1-1 loss: 1.129186  [   64/  118]
train() client id: f_00009-1-2 loss: 0.955803  [   96/  118]
train() client id: f_00009-2-0 loss: 0.877104  [   32/  118]
train() client id: f_00009-2-1 loss: 0.967561  [   64/  118]
train() client id: f_00009-2-2 loss: 1.110783  [   96/  118]
train() client id: f_00009-3-0 loss: 0.878488  [   32/  118]
train() client id: f_00009-3-1 loss: 0.864014  [   64/  118]
train() client id: f_00009-3-2 loss: 1.089297  [   96/  118]
train() client id: f_00009-4-0 loss: 0.895452  [   32/  118]
train() client id: f_00009-4-1 loss: 1.033987  [   64/  118]
train() client id: f_00009-4-2 loss: 0.816714  [   96/  118]
train() client id: f_00009-5-0 loss: 0.902804  [   32/  118]
train() client id: f_00009-5-1 loss: 0.836504  [   64/  118]
train() client id: f_00009-5-2 loss: 0.841721  [   96/  118]
train() client id: f_00009-6-0 loss: 0.847621  [   32/  118]
train() client id: f_00009-6-1 loss: 0.885098  [   64/  118]
train() client id: f_00009-6-2 loss: 0.837769  [   96/  118]
train() client id: f_00009-7-0 loss: 0.926182  [   32/  118]
train() client id: f_00009-7-1 loss: 0.932967  [   64/  118]
train() client id: f_00009-7-2 loss: 0.846868  [   96/  118]
train() client id: f_00009-8-0 loss: 0.768154  [   32/  118]
train() client id: f_00009-8-1 loss: 0.911850  [   64/  118]
train() client id: f_00009-8-2 loss: 0.945269  [   96/  118]
train() client id: f_00009-9-0 loss: 0.826767  [   32/  118]
train() client id: f_00009-9-1 loss: 0.774496  [   64/  118]
train() client id: f_00009-9-2 loss: 0.993818  [   96/  118]
train() client id: f_00009-10-0 loss: 0.941478  [   32/  118]
train() client id: f_00009-10-1 loss: 0.770950  [   64/  118]
train() client id: f_00009-10-2 loss: 0.918676  [   96/  118]
train() client id: f_00009-11-0 loss: 0.960873  [   32/  118]
train() client id: f_00009-11-1 loss: 0.903285  [   64/  118]
train() client id: f_00009-11-2 loss: 0.846858  [   96/  118]
At round 28 accuracy: 0.6551724137931034
At round 28 training accuracy: 0.5828303152246814
At round 28 training loss: 0.8296076649907794
update_location
xs = [  -3.9056584     4.20031788  160.00902392   18.81129433    0.97929623
    3.95640986 -122.44319194 -101.32485185  144.66397685  -87.06087855]
ys = [ 152.5879595   135.55583871    1.32061395 -122.45517586  114.35018685
   97.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [182.47832625 168.50230882 188.69189637 159.21411649 151.91090893
 139.94020603 158.11143474 142.36362547 176.73804393 132.64843923]
dists_bs = [172.55600043 183.49004274 377.35338245 355.02450543 186.13734858
 194.89241123 185.23728871 189.11869796 356.32790511 192.28567891]
uav_gains = [2.19565925e-11 2.70052809e-11 2.00716392e-11 3.11928652e-11
 3.51147715e-11 4.31483308e-11 3.17457777e-11 4.13309852e-11
 2.38781215e-11 4.93353388e-11]
bs_gains = [6.02406849e-11 5.07199808e-11 6.73595215e-12 7.99044117e-12
 4.87259393e-11 4.28418567e-11 4.93917622e-11 4.66055429e-11
 7.90887203e-12 4.44879766e-11]
Round 29
-------------------------------
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.16426333 14.86448857  7.05664821  2.53741575 17.14489673  8.25364943
  3.14835783 10.08926783  7.43869069  6.69596016]
obj_prev = 84.39363852739328
eta_min = 1.1633239161667213e-13	eta_max = 0.9255734667757686
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 19.601082838327166	eta = 0.9090909090909091
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 35.06532785864593	eta = 0.5081705292616409
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 27.55402889869821	eta = 0.6466991191078749
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.19954622843339	eta = 0.6801326275385099
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129948192651668	eta = 0.6819441847065052
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129750162898475	eta = 0.6819493529625255
eta = 0.6819493529625255
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.03158773 0.06643451 0.03108634 0.01077994 0.07671306 0.03660165
 0.0135376  0.04487463 0.03259051 0.02958216]
ene_total = [2.30902016 4.22374425 2.291367   1.07582749 4.81690287 2.52674818
 1.23224478 3.00469125 2.52859005 2.12061414]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 0 obj = 4.703234079287297
eta = 0.6819493529625255
freqs = [38424917.91998439 78013241.65854445 37994067.71862566 12898298.99218661
 90209018.95065838 43241454.64083756 16185653.82662158 53086584.80672698
 42569513.0112679  34900040.52352524]
eta_min = 0.6819493529625276	eta_max = 0.6819493529625246
af = 0.016869127524841664	bf = 1.4894091419856037	zeta = 0.018556040277325832	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [2.90303184e-06 2.51673727e-05 2.79324241e-06 1.11631997e-07
 3.88576653e-05 4.25999219e-06 2.20754524e-07 7.87187088e-06
 3.67617412e-06 2.24279318e-06]
ene_total = [1.73801127 1.44623117 1.77687644 1.59993898 1.46089904 1.49353974
 1.59363288 1.50568747 2.3050487  1.48132066]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 1 obj = 4.703234079287284
eta = 0.6819493529625246
freqs = [38424917.91998437 78013241.65854444 37994067.71862565 12898298.99218661
 90209018.95065837 43241454.64083755 16185653.82662157 53086584.80672697
 42569513.01126787 34900040.52352523]
Done!
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.11621458e-05 9.67684474e-05 1.07400059e-05 4.29224582e-07
 1.49407568e-04 1.63796530e-05 8.48800263e-07 3.02673121e-05
 1.41348748e-05 8.62353083e-06]
ene_total = [0.00866643 0.00727623 0.00885973 0.00797066 0.00738824 0.00745242
 0.00793956 0.00752321 0.01149342 0.00738581]
At round 29 energy consumption: 0.08195570742412175
At round 29 eta: 0.6819493529625246
At round 29 a_n: 18.248773294144172
At round 29 local rounds: 12.534813137868813
At round 29 global rounds: 57.376941264307334
gradient difference: 0.5362218022346497
train() client id: f_00000-0-0 loss: 1.003651  [   32/  126]
train() client id: f_00000-0-1 loss: 0.941975  [   64/  126]
train() client id: f_00000-0-2 loss: 1.089942  [   96/  126]
train() client id: f_00000-1-0 loss: 0.934943  [   32/  126]
train() client id: f_00000-1-1 loss: 0.937256  [   64/  126]
train() client id: f_00000-1-2 loss: 0.938584  [   96/  126]
train() client id: f_00000-2-0 loss: 0.816659  [   32/  126]
train() client id: f_00000-2-1 loss: 0.939851  [   64/  126]
train() client id: f_00000-2-2 loss: 0.909672  [   96/  126]
train() client id: f_00000-3-0 loss: 0.885162  [   32/  126]
train() client id: f_00000-3-1 loss: 0.931697  [   64/  126]
train() client id: f_00000-3-2 loss: 0.758026  [   96/  126]
train() client id: f_00000-4-0 loss: 0.757372  [   32/  126]
train() client id: f_00000-4-1 loss: 0.857305  [   64/  126]
train() client id: f_00000-4-2 loss: 0.921779  [   96/  126]
train() client id: f_00000-5-0 loss: 0.800552  [   32/  126]
train() client id: f_00000-5-1 loss: 0.814394  [   64/  126]
train() client id: f_00000-5-2 loss: 0.854874  [   96/  126]
train() client id: f_00000-6-0 loss: 0.907454  [   32/  126]
train() client id: f_00000-6-1 loss: 0.719409  [   64/  126]
train() client id: f_00000-6-2 loss: 0.770784  [   96/  126]
train() client id: f_00000-7-0 loss: 0.868226  [   32/  126]
train() client id: f_00000-7-1 loss: 0.841477  [   64/  126]
train() client id: f_00000-7-2 loss: 0.827805  [   96/  126]
train() client id: f_00000-8-0 loss: 0.871908  [   32/  126]
train() client id: f_00000-8-1 loss: 0.882289  [   64/  126]
train() client id: f_00000-8-2 loss: 0.754099  [   96/  126]
train() client id: f_00000-9-0 loss: 0.800664  [   32/  126]
train() client id: f_00000-9-1 loss: 0.883492  [   64/  126]
train() client id: f_00000-9-2 loss: 0.780149  [   96/  126]
train() client id: f_00000-10-0 loss: 0.895722  [   32/  126]
train() client id: f_00000-10-1 loss: 0.842219  [   64/  126]
train() client id: f_00000-10-2 loss: 0.858228  [   96/  126]
train() client id: f_00000-11-0 loss: 0.839159  [   32/  126]
train() client id: f_00000-11-1 loss: 0.868587  [   64/  126]
train() client id: f_00000-11-2 loss: 0.745076  [   96/  126]
train() client id: f_00001-0-0 loss: 0.472777  [   32/  265]
train() client id: f_00001-0-1 loss: 0.550825  [   64/  265]
train() client id: f_00001-0-2 loss: 0.520041  [   96/  265]
train() client id: f_00001-0-3 loss: 0.432199  [  128/  265]
train() client id: f_00001-0-4 loss: 0.574620  [  160/  265]
train() client id: f_00001-0-5 loss: 0.487727  [  192/  265]
train() client id: f_00001-0-6 loss: 0.417381  [  224/  265]
train() client id: f_00001-0-7 loss: 0.497504  [  256/  265]
train() client id: f_00001-1-0 loss: 0.477058  [   32/  265]
train() client id: f_00001-1-1 loss: 0.415945  [   64/  265]
train() client id: f_00001-1-2 loss: 0.518935  [   96/  265]
train() client id: f_00001-1-3 loss: 0.478049  [  128/  265]
train() client id: f_00001-1-4 loss: 0.545843  [  160/  265]
train() client id: f_00001-1-5 loss: 0.565132  [  192/  265]
train() client id: f_00001-1-6 loss: 0.399951  [  224/  265]
train() client id: f_00001-1-7 loss: 0.424841  [  256/  265]
train() client id: f_00001-2-0 loss: 0.487940  [   32/  265]
train() client id: f_00001-2-1 loss: 0.477178  [   64/  265]
train() client id: f_00001-2-2 loss: 0.479359  [   96/  265]
train() client id: f_00001-2-3 loss: 0.485600  [  128/  265]
train() client id: f_00001-2-4 loss: 0.519109  [  160/  265]
train() client id: f_00001-2-5 loss: 0.545187  [  192/  265]
train() client id: f_00001-2-6 loss: 0.400125  [  224/  265]
train() client id: f_00001-2-7 loss: 0.415884  [  256/  265]
train() client id: f_00001-3-0 loss: 0.485394  [   32/  265]
train() client id: f_00001-3-1 loss: 0.514174  [   64/  265]
train() client id: f_00001-3-2 loss: 0.432635  [   96/  265]
train() client id: f_00001-3-3 loss: 0.441124  [  128/  265]
train() client id: f_00001-3-4 loss: 0.569392  [  160/  265]
train() client id: f_00001-3-5 loss: 0.500253  [  192/  265]
train() client id: f_00001-3-6 loss: 0.406037  [  224/  265]
train() client id: f_00001-3-7 loss: 0.466443  [  256/  265]
train() client id: f_00001-4-0 loss: 0.487590  [   32/  265]
train() client id: f_00001-4-1 loss: 0.394348  [   64/  265]
train() client id: f_00001-4-2 loss: 0.488440  [   96/  265]
train() client id: f_00001-4-3 loss: 0.430777  [  128/  265]
train() client id: f_00001-4-4 loss: 0.400063  [  160/  265]
train() client id: f_00001-4-5 loss: 0.515540  [  192/  265]
train() client id: f_00001-4-6 loss: 0.687961  [  224/  265]
train() client id: f_00001-4-7 loss: 0.431819  [  256/  265]
train() client id: f_00001-5-0 loss: 0.499087  [   32/  265]
train() client id: f_00001-5-1 loss: 0.577435  [   64/  265]
train() client id: f_00001-5-2 loss: 0.484149  [   96/  265]
train() client id: f_00001-5-3 loss: 0.503629  [  128/  265]
train() client id: f_00001-5-4 loss: 0.393857  [  160/  265]
train() client id: f_00001-5-5 loss: 0.459090  [  192/  265]
train() client id: f_00001-5-6 loss: 0.378431  [  224/  265]
train() client id: f_00001-5-7 loss: 0.521132  [  256/  265]
train() client id: f_00001-6-0 loss: 0.383365  [   32/  265]
train() client id: f_00001-6-1 loss: 0.470729  [   64/  265]
train() client id: f_00001-6-2 loss: 0.459947  [   96/  265]
train() client id: f_00001-6-3 loss: 0.540066  [  128/  265]
train() client id: f_00001-6-4 loss: 0.647482  [  160/  265]
train() client id: f_00001-6-5 loss: 0.399278  [  192/  265]
train() client id: f_00001-6-6 loss: 0.381754  [  224/  265]
train() client id: f_00001-6-7 loss: 0.496767  [  256/  265]
train() client id: f_00001-7-0 loss: 0.460426  [   32/  265]
train() client id: f_00001-7-1 loss: 0.375939  [   64/  265]
train() client id: f_00001-7-2 loss: 0.434584  [   96/  265]
train() client id: f_00001-7-3 loss: 0.527270  [  128/  265]
train() client id: f_00001-7-4 loss: 0.447284  [  160/  265]
train() client id: f_00001-7-5 loss: 0.509039  [  192/  265]
train() client id: f_00001-7-6 loss: 0.533829  [  224/  265]
train() client id: f_00001-7-7 loss: 0.438365  [  256/  265]
train() client id: f_00001-8-0 loss: 0.382350  [   32/  265]
train() client id: f_00001-8-1 loss: 0.387936  [   64/  265]
train() client id: f_00001-8-2 loss: 0.435686  [   96/  265]
train() client id: f_00001-8-3 loss: 0.457010  [  128/  265]
train() client id: f_00001-8-4 loss: 0.586922  [  160/  265]
train() client id: f_00001-8-5 loss: 0.458386  [  192/  265]
train() client id: f_00001-8-6 loss: 0.571865  [  224/  265]
train() client id: f_00001-8-7 loss: 0.479673  [  256/  265]
train() client id: f_00001-9-0 loss: 0.432235  [   32/  265]
train() client id: f_00001-9-1 loss: 0.551021  [   64/  265]
train() client id: f_00001-9-2 loss: 0.392514  [   96/  265]
train() client id: f_00001-9-3 loss: 0.454505  [  128/  265]
train() client id: f_00001-9-4 loss: 0.484198  [  160/  265]
train() client id: f_00001-9-5 loss: 0.555682  [  192/  265]
train() client id: f_00001-9-6 loss: 0.456034  [  224/  265]
train() client id: f_00001-9-7 loss: 0.491685  [  256/  265]
train() client id: f_00001-10-0 loss: 0.393272  [   32/  265]
train() client id: f_00001-10-1 loss: 0.390093  [   64/  265]
train() client id: f_00001-10-2 loss: 0.375332  [   96/  265]
train() client id: f_00001-10-3 loss: 0.726641  [  128/  265]
train() client id: f_00001-10-4 loss: 0.433083  [  160/  265]
train() client id: f_00001-10-5 loss: 0.510541  [  192/  265]
train() client id: f_00001-10-6 loss: 0.559828  [  224/  265]
train() client id: f_00001-10-7 loss: 0.439356  [  256/  265]
train() client id: f_00001-11-0 loss: 0.372587  [   32/  265]
train() client id: f_00001-11-1 loss: 0.530959  [   64/  265]
train() client id: f_00001-11-2 loss: 0.381848  [   96/  265]
train() client id: f_00001-11-3 loss: 0.637427  [  128/  265]
train() client id: f_00001-11-4 loss: 0.510477  [  160/  265]
train() client id: f_00001-11-5 loss: 0.451040  [  192/  265]
train() client id: f_00001-11-6 loss: 0.572250  [  224/  265]
train() client id: f_00001-11-7 loss: 0.373151  [  256/  265]
train() client id: f_00002-0-0 loss: 1.245754  [   32/  124]
train() client id: f_00002-0-1 loss: 1.298391  [   64/  124]
train() client id: f_00002-0-2 loss: 1.281603  [   96/  124]
train() client id: f_00002-1-0 loss: 1.230100  [   32/  124]
train() client id: f_00002-1-1 loss: 1.113086  [   64/  124]
train() client id: f_00002-1-2 loss: 1.368592  [   96/  124]
train() client id: f_00002-2-0 loss: 1.091169  [   32/  124]
train() client id: f_00002-2-1 loss: 1.060965  [   64/  124]
train() client id: f_00002-2-2 loss: 1.376892  [   96/  124]
train() client id: f_00002-3-0 loss: 1.149144  [   32/  124]
train() client id: f_00002-3-1 loss: 1.092499  [   64/  124]
train() client id: f_00002-3-2 loss: 1.141855  [   96/  124]
train() client id: f_00002-4-0 loss: 1.163699  [   32/  124]
train() client id: f_00002-4-1 loss: 1.068923  [   64/  124]
train() client id: f_00002-4-2 loss: 1.186022  [   96/  124]
train() client id: f_00002-5-0 loss: 1.122585  [   32/  124]
train() client id: f_00002-5-1 loss: 1.089430  [   64/  124]
train() client id: f_00002-5-2 loss: 1.153714  [   96/  124]
train() client id: f_00002-6-0 loss: 1.044622  [   32/  124]
train() client id: f_00002-6-1 loss: 1.043901  [   64/  124]
train() client id: f_00002-6-2 loss: 1.163106  [   96/  124]
train() client id: f_00002-7-0 loss: 0.880268  [   32/  124]
train() client id: f_00002-7-1 loss: 1.061455  [   64/  124]
train() client id: f_00002-7-2 loss: 1.138837  [   96/  124]
train() client id: f_00002-8-0 loss: 1.071927  [   32/  124]
train() client id: f_00002-8-1 loss: 1.061925  [   64/  124]
train() client id: f_00002-8-2 loss: 1.049789  [   96/  124]
train() client id: f_00002-9-0 loss: 1.171958  [   32/  124]
train() client id: f_00002-9-1 loss: 1.145135  [   64/  124]
train() client id: f_00002-9-2 loss: 1.010481  [   96/  124]
train() client id: f_00002-10-0 loss: 1.152458  [   32/  124]
train() client id: f_00002-10-1 loss: 1.113725  [   64/  124]
train() client id: f_00002-10-2 loss: 0.952720  [   96/  124]
train() client id: f_00002-11-0 loss: 1.134739  [   32/  124]
train() client id: f_00002-11-1 loss: 1.040498  [   64/  124]
train() client id: f_00002-11-2 loss: 1.081103  [   96/  124]
train() client id: f_00003-0-0 loss: 0.984821  [   32/   43]
train() client id: f_00003-1-0 loss: 0.662095  [   32/   43]
train() client id: f_00003-2-0 loss: 0.779489  [   32/   43]
train() client id: f_00003-3-0 loss: 0.786923  [   32/   43]
train() client id: f_00003-4-0 loss: 0.780229  [   32/   43]
train() client id: f_00003-5-0 loss: 0.867531  [   32/   43]
train() client id: f_00003-6-0 loss: 0.870916  [   32/   43]
train() client id: f_00003-7-0 loss: 0.820446  [   32/   43]
train() client id: f_00003-8-0 loss: 0.659037  [   32/   43]
train() client id: f_00003-9-0 loss: 0.780036  [   32/   43]
train() client id: f_00003-10-0 loss: 0.883318  [   32/   43]
train() client id: f_00003-11-0 loss: 0.814435  [   32/   43]
train() client id: f_00004-0-0 loss: 0.787749  [   32/  306]
train() client id: f_00004-0-1 loss: 0.791555  [   64/  306]
train() client id: f_00004-0-2 loss: 0.705411  [   96/  306]
train() client id: f_00004-0-3 loss: 0.626799  [  128/  306]
train() client id: f_00004-0-4 loss: 0.815807  [  160/  306]
train() client id: f_00004-0-5 loss: 0.795183  [  192/  306]
train() client id: f_00004-0-6 loss: 0.811741  [  224/  306]
train() client id: f_00004-0-7 loss: 0.780283  [  256/  306]
train() client id: f_00004-0-8 loss: 0.869804  [  288/  306]
train() client id: f_00004-1-0 loss: 0.775234  [   32/  306]
train() client id: f_00004-1-1 loss: 0.711309  [   64/  306]
train() client id: f_00004-1-2 loss: 0.712336  [   96/  306]
train() client id: f_00004-1-3 loss: 0.788472  [  128/  306]
train() client id: f_00004-1-4 loss: 0.766018  [  160/  306]
train() client id: f_00004-1-5 loss: 0.678834  [  192/  306]
train() client id: f_00004-1-6 loss: 0.907475  [  224/  306]
train() client id: f_00004-1-7 loss: 0.735551  [  256/  306]
train() client id: f_00004-1-8 loss: 0.909583  [  288/  306]
train() client id: f_00004-2-0 loss: 0.707429  [   32/  306]
train() client id: f_00004-2-1 loss: 0.684718  [   64/  306]
train() client id: f_00004-2-2 loss: 0.657851  [   96/  306]
train() client id: f_00004-2-3 loss: 0.902663  [  128/  306]
train() client id: f_00004-2-4 loss: 0.665551  [  160/  306]
train() client id: f_00004-2-5 loss: 0.764904  [  192/  306]
train() client id: f_00004-2-6 loss: 0.940050  [  224/  306]
train() client id: f_00004-2-7 loss: 0.806915  [  256/  306]
train() client id: f_00004-2-8 loss: 0.769866  [  288/  306]
train() client id: f_00004-3-0 loss: 0.802198  [   32/  306]
train() client id: f_00004-3-1 loss: 0.730359  [   64/  306]
train() client id: f_00004-3-2 loss: 0.696565  [   96/  306]
train() client id: f_00004-3-3 loss: 0.728126  [  128/  306]
train() client id: f_00004-3-4 loss: 0.735177  [  160/  306]
train() client id: f_00004-3-5 loss: 0.843854  [  192/  306]
train() client id: f_00004-3-6 loss: 0.818111  [  224/  306]
train() client id: f_00004-3-7 loss: 0.821095  [  256/  306]
train() client id: f_00004-3-8 loss: 0.851994  [  288/  306]
train() client id: f_00004-4-0 loss: 0.728660  [   32/  306]
train() client id: f_00004-4-1 loss: 0.799291  [   64/  306]
train() client id: f_00004-4-2 loss: 0.850124  [   96/  306]
train() client id: f_00004-4-3 loss: 0.726328  [  128/  306]
train() client id: f_00004-4-4 loss: 0.817952  [  160/  306]
train() client id: f_00004-4-5 loss: 0.945362  [  192/  306]
train() client id: f_00004-4-6 loss: 0.693347  [  224/  306]
train() client id: f_00004-4-7 loss: 0.595017  [  256/  306]
train() client id: f_00004-4-8 loss: 0.849835  [  288/  306]
train() client id: f_00004-5-0 loss: 0.824787  [   32/  306]
train() client id: f_00004-5-1 loss: 0.704513  [   64/  306]
train() client id: f_00004-5-2 loss: 0.852348  [   96/  306]
train() client id: f_00004-5-3 loss: 0.802361  [  128/  306]
train() client id: f_00004-5-4 loss: 0.790642  [  160/  306]
train() client id: f_00004-5-5 loss: 0.739686  [  192/  306]
train() client id: f_00004-5-6 loss: 0.788182  [  224/  306]
train() client id: f_00004-5-7 loss: 0.726160  [  256/  306]
train() client id: f_00004-5-8 loss: 0.800831  [  288/  306]
train() client id: f_00004-6-0 loss: 0.807061  [   32/  306]
train() client id: f_00004-6-1 loss: 0.708380  [   64/  306]
train() client id: f_00004-6-2 loss: 0.851641  [   96/  306]
train() client id: f_00004-6-3 loss: 0.802872  [  128/  306]
train() client id: f_00004-6-4 loss: 0.690795  [  160/  306]
train() client id: f_00004-6-5 loss: 0.776751  [  192/  306]
train() client id: f_00004-6-6 loss: 0.799174  [  224/  306]
train() client id: f_00004-6-7 loss: 0.797498  [  256/  306]
train() client id: f_00004-6-8 loss: 0.721068  [  288/  306]
train() client id: f_00004-7-0 loss: 0.874145  [   32/  306]
train() client id: f_00004-7-1 loss: 0.726065  [   64/  306]
train() client id: f_00004-7-2 loss: 0.789195  [   96/  306]
train() client id: f_00004-7-3 loss: 0.709282  [  128/  306]
train() client id: f_00004-7-4 loss: 0.815833  [  160/  306]
train() client id: f_00004-7-5 loss: 0.835644  [  192/  306]
train() client id: f_00004-7-6 loss: 0.732953  [  224/  306]
train() client id: f_00004-7-7 loss: 0.850428  [  256/  306]
train() client id: f_00004-7-8 loss: 0.750211  [  288/  306]
train() client id: f_00004-8-0 loss: 0.764471  [   32/  306]
train() client id: f_00004-8-1 loss: 0.789146  [   64/  306]
train() client id: f_00004-8-2 loss: 0.780464  [   96/  306]
train() client id: f_00004-8-3 loss: 0.842104  [  128/  306]
train() client id: f_00004-8-4 loss: 0.856648  [  160/  306]
train() client id: f_00004-8-5 loss: 0.706112  [  192/  306]
train() client id: f_00004-8-6 loss: 0.787052  [  224/  306]
train() client id: f_00004-8-7 loss: 0.795057  [  256/  306]
train() client id: f_00004-8-8 loss: 0.708107  [  288/  306]
train() client id: f_00004-9-0 loss: 0.854582  [   32/  306]
train() client id: f_00004-9-1 loss: 0.836629  [   64/  306]
train() client id: f_00004-9-2 loss: 0.762537  [   96/  306]
train() client id: f_00004-9-3 loss: 0.738409  [  128/  306]
train() client id: f_00004-9-4 loss: 0.794013  [  160/  306]
train() client id: f_00004-9-5 loss: 0.758862  [  192/  306]
train() client id: f_00004-9-6 loss: 0.715115  [  224/  306]
train() client id: f_00004-9-7 loss: 0.800792  [  256/  306]
train() client id: f_00004-9-8 loss: 0.874150  [  288/  306]
train() client id: f_00004-10-0 loss: 0.755226  [   32/  306]
train() client id: f_00004-10-1 loss: 0.792772  [   64/  306]
train() client id: f_00004-10-2 loss: 0.716518  [   96/  306]
train() client id: f_00004-10-3 loss: 0.812421  [  128/  306]
train() client id: f_00004-10-4 loss: 0.858314  [  160/  306]
train() client id: f_00004-10-5 loss: 0.767396  [  192/  306]
train() client id: f_00004-10-6 loss: 0.789518  [  224/  306]
train() client id: f_00004-10-7 loss: 0.716293  [  256/  306]
train() client id: f_00004-10-8 loss: 0.834987  [  288/  306]
train() client id: f_00004-11-0 loss: 0.718756  [   32/  306]
train() client id: f_00004-11-1 loss: 0.830411  [   64/  306]
train() client id: f_00004-11-2 loss: 0.846595  [   96/  306]
train() client id: f_00004-11-3 loss: 0.783376  [  128/  306]
train() client id: f_00004-11-4 loss: 0.805092  [  160/  306]
train() client id: f_00004-11-5 loss: 0.826503  [  192/  306]
train() client id: f_00004-11-6 loss: 0.794624  [  224/  306]
train() client id: f_00004-11-7 loss: 0.768050  [  256/  306]
train() client id: f_00004-11-8 loss: 0.835476  [  288/  306]
train() client id: f_00005-0-0 loss: 0.442259  [   32/  146]
train() client id: f_00005-0-1 loss: 0.472902  [   64/  146]
train() client id: f_00005-0-2 loss: 0.548626  [   96/  146]
train() client id: f_00005-0-3 loss: 0.563778  [  128/  146]
train() client id: f_00005-1-0 loss: 0.603503  [   32/  146]
train() client id: f_00005-1-1 loss: 0.527587  [   64/  146]
train() client id: f_00005-1-2 loss: 0.431019  [   96/  146]
train() client id: f_00005-1-3 loss: 0.326272  [  128/  146]
train() client id: f_00005-2-0 loss: 0.266816  [   32/  146]
train() client id: f_00005-2-1 loss: 0.398767  [   64/  146]
train() client id: f_00005-2-2 loss: 0.522197  [   96/  146]
train() client id: f_00005-2-3 loss: 0.578619  [  128/  146]
train() client id: f_00005-3-0 loss: 0.475135  [   32/  146]
train() client id: f_00005-3-1 loss: 0.585706  [   64/  146]
train() client id: f_00005-3-2 loss: 0.393914  [   96/  146]
train() client id: f_00005-3-3 loss: 0.562029  [  128/  146]
train() client id: f_00005-4-0 loss: 0.631132  [   32/  146]
train() client id: f_00005-4-1 loss: 0.294346  [   64/  146]
train() client id: f_00005-4-2 loss: 0.374776  [   96/  146]
train() client id: f_00005-4-3 loss: 0.673912  [  128/  146]
train() client id: f_00005-5-0 loss: 0.458579  [   32/  146]
train() client id: f_00005-5-1 loss: 0.645953  [   64/  146]
train() client id: f_00005-5-2 loss: 0.356107  [   96/  146]
train() client id: f_00005-5-3 loss: 0.561192  [  128/  146]
train() client id: f_00005-6-0 loss: 0.349802  [   32/  146]
train() client id: f_00005-6-1 loss: 0.844145  [   64/  146]
train() client id: f_00005-6-2 loss: 0.487694  [   96/  146]
train() client id: f_00005-6-3 loss: 0.429955  [  128/  146]
train() client id: f_00005-7-0 loss: 0.483664  [   32/  146]
train() client id: f_00005-7-1 loss: 0.322672  [   64/  146]
train() client id: f_00005-7-2 loss: 0.489567  [   96/  146]
train() client id: f_00005-7-3 loss: 0.475072  [  128/  146]
train() client id: f_00005-8-0 loss: 0.624358  [   32/  146]
train() client id: f_00005-8-1 loss: 0.320161  [   64/  146]
train() client id: f_00005-8-2 loss: 0.522007  [   96/  146]
train() client id: f_00005-8-3 loss: 0.604737  [  128/  146]
train() client id: f_00005-9-0 loss: 0.649490  [   32/  146]
train() client id: f_00005-9-1 loss: 0.368098  [   64/  146]
train() client id: f_00005-9-2 loss: 0.563762  [   96/  146]
train() client id: f_00005-9-3 loss: 0.470965  [  128/  146]
train() client id: f_00005-10-0 loss: 0.624038  [   32/  146]
train() client id: f_00005-10-1 loss: 0.554503  [   64/  146]
train() client id: f_00005-10-2 loss: 0.622738  [   96/  146]
train() client id: f_00005-10-3 loss: 0.279789  [  128/  146]
train() client id: f_00005-11-0 loss: 0.553283  [   32/  146]
train() client id: f_00005-11-1 loss: 0.369512  [   64/  146]
train() client id: f_00005-11-2 loss: 0.429509  [   96/  146]
train() client id: f_00005-11-3 loss: 0.474209  [  128/  146]
train() client id: f_00006-0-0 loss: 0.570594  [   32/   54]
train() client id: f_00006-1-0 loss: 0.538413  [   32/   54]
train() client id: f_00006-2-0 loss: 0.555894  [   32/   54]
train() client id: f_00006-3-0 loss: 0.596579  [   32/   54]
train() client id: f_00006-4-0 loss: 0.591473  [   32/   54]
train() client id: f_00006-5-0 loss: 0.528829  [   32/   54]
train() client id: f_00006-6-0 loss: 0.515038  [   32/   54]
train() client id: f_00006-7-0 loss: 0.567181  [   32/   54]
train() client id: f_00006-8-0 loss: 0.614190  [   32/   54]
train() client id: f_00006-9-0 loss: 0.601157  [   32/   54]
train() client id: f_00006-10-0 loss: 0.607472  [   32/   54]
train() client id: f_00006-11-0 loss: 0.601619  [   32/   54]
train() client id: f_00007-0-0 loss: 0.522733  [   32/  179]
train() client id: f_00007-0-1 loss: 0.516595  [   64/  179]
train() client id: f_00007-0-2 loss: 0.646434  [   96/  179]
train() client id: f_00007-0-3 loss: 0.839711  [  128/  179]
train() client id: f_00007-0-4 loss: 0.824559  [  160/  179]
train() client id: f_00007-1-0 loss: 0.713596  [   32/  179]
train() client id: f_00007-1-1 loss: 0.650942  [   64/  179]
train() client id: f_00007-1-2 loss: 0.620773  [   96/  179]
train() client id: f_00007-1-3 loss: 0.728533  [  128/  179]
train() client id: f_00007-1-4 loss: 0.587817  [  160/  179]
train() client id: f_00007-2-0 loss: 0.491426  [   32/  179]
train() client id: f_00007-2-1 loss: 0.585123  [   64/  179]
train() client id: f_00007-2-2 loss: 0.493628  [   96/  179]
train() client id: f_00007-2-3 loss: 0.775319  [  128/  179]
train() client id: f_00007-2-4 loss: 0.683523  [  160/  179]
train() client id: f_00007-3-0 loss: 0.692626  [   32/  179]
train() client id: f_00007-3-1 loss: 0.472403  [   64/  179]
train() client id: f_00007-3-2 loss: 0.522443  [   96/  179]
train() client id: f_00007-3-3 loss: 0.849097  [  128/  179]
train() client id: f_00007-3-4 loss: 0.519027  [  160/  179]
train() client id: f_00007-4-0 loss: 0.532640  [   32/  179]
train() client id: f_00007-4-1 loss: 0.730960  [   64/  179]
train() client id: f_00007-4-2 loss: 0.665930  [   96/  179]
train() client id: f_00007-4-3 loss: 0.512752  [  128/  179]
train() client id: f_00007-4-4 loss: 0.698452  [  160/  179]
train() client id: f_00007-5-0 loss: 0.485558  [   32/  179]
train() client id: f_00007-5-1 loss: 0.653313  [   64/  179]
train() client id: f_00007-5-2 loss: 0.620337  [   96/  179]
train() client id: f_00007-5-3 loss: 0.586776  [  128/  179]
train() client id: f_00007-5-4 loss: 0.582179  [  160/  179]
train() client id: f_00007-6-0 loss: 0.601484  [   32/  179]
train() client id: f_00007-6-1 loss: 0.652066  [   64/  179]
train() client id: f_00007-6-2 loss: 0.615231  [   96/  179]
train() client id: f_00007-6-3 loss: 0.552556  [  128/  179]
train() client id: f_00007-6-4 loss: 0.642664  [  160/  179]
train() client id: f_00007-7-0 loss: 0.571474  [   32/  179]
train() client id: f_00007-7-1 loss: 0.727646  [   64/  179]
train() client id: f_00007-7-2 loss: 0.642317  [   96/  179]
train() client id: f_00007-7-3 loss: 0.576084  [  128/  179]
train() client id: f_00007-7-4 loss: 0.516807  [  160/  179]
train() client id: f_00007-8-0 loss: 0.801910  [   32/  179]
train() client id: f_00007-8-1 loss: 0.548022  [   64/  179]
train() client id: f_00007-8-2 loss: 0.702474  [   96/  179]
train() client id: f_00007-8-3 loss: 0.458659  [  128/  179]
train() client id: f_00007-8-4 loss: 0.567526  [  160/  179]
train() client id: f_00007-9-0 loss: 0.636901  [   32/  179]
train() client id: f_00007-9-1 loss: 0.632811  [   64/  179]
train() client id: f_00007-9-2 loss: 0.707399  [   96/  179]
train() client id: f_00007-9-3 loss: 0.497426  [  128/  179]
train() client id: f_00007-9-4 loss: 0.631356  [  160/  179]
train() client id: f_00007-10-0 loss: 0.465580  [   32/  179]
train() client id: f_00007-10-1 loss: 0.587849  [   64/  179]
train() client id: f_00007-10-2 loss: 0.823172  [   96/  179]
train() client id: f_00007-10-3 loss: 0.492191  [  128/  179]
train() client id: f_00007-10-4 loss: 0.559549  [  160/  179]
train() client id: f_00007-11-0 loss: 0.661674  [   32/  179]
train() client id: f_00007-11-1 loss: 0.573750  [   64/  179]
train() client id: f_00007-11-2 loss: 0.853819  [   96/  179]
train() client id: f_00007-11-3 loss: 0.472264  [  128/  179]
train() client id: f_00007-11-4 loss: 0.447848  [  160/  179]
train() client id: f_00008-0-0 loss: 0.726657  [   32/  130]
train() client id: f_00008-0-1 loss: 0.746200  [   64/  130]
train() client id: f_00008-0-2 loss: 0.594121  [   96/  130]
train() client id: f_00008-0-3 loss: 0.779588  [  128/  130]
train() client id: f_00008-1-0 loss: 0.629364  [   32/  130]
train() client id: f_00008-1-1 loss: 0.703966  [   64/  130]
train() client id: f_00008-1-2 loss: 0.811528  [   96/  130]
train() client id: f_00008-1-3 loss: 0.695478  [  128/  130]
train() client id: f_00008-2-0 loss: 0.738920  [   32/  130]
train() client id: f_00008-2-1 loss: 0.596965  [   64/  130]
train() client id: f_00008-2-2 loss: 0.587913  [   96/  130]
train() client id: f_00008-2-3 loss: 0.917815  [  128/  130]
train() client id: f_00008-3-0 loss: 0.701315  [   32/  130]
train() client id: f_00008-3-1 loss: 0.746648  [   64/  130]
train() client id: f_00008-3-2 loss: 0.755494  [   96/  130]
train() client id: f_00008-3-3 loss: 0.679669  [  128/  130]
train() client id: f_00008-4-0 loss: 0.694316  [   32/  130]
train() client id: f_00008-4-1 loss: 0.708443  [   64/  130]
train() client id: f_00008-4-2 loss: 0.635893  [   96/  130]
train() client id: f_00008-4-3 loss: 0.798893  [  128/  130]
train() client id: f_00008-5-0 loss: 0.711647  [   32/  130]
train() client id: f_00008-5-1 loss: 0.744468  [   64/  130]
train() client id: f_00008-5-2 loss: 0.689131  [   96/  130]
train() client id: f_00008-5-3 loss: 0.732659  [  128/  130]
train() client id: f_00008-6-0 loss: 0.692466  [   32/  130]
train() client id: f_00008-6-1 loss: 0.767770  [   64/  130]
train() client id: f_00008-6-2 loss: 0.840648  [   96/  130]
train() client id: f_00008-6-3 loss: 0.574484  [  128/  130]
train() client id: f_00008-7-0 loss: 0.747205  [   32/  130]
train() client id: f_00008-7-1 loss: 0.707548  [   64/  130]
train() client id: f_00008-7-2 loss: 0.689468  [   96/  130]
train() client id: f_00008-7-3 loss: 0.729601  [  128/  130]
train() client id: f_00008-8-0 loss: 0.724634  [   32/  130]
train() client id: f_00008-8-1 loss: 0.679871  [   64/  130]
train() client id: f_00008-8-2 loss: 0.768493  [   96/  130]
train() client id: f_00008-8-3 loss: 0.694211  [  128/  130]
train() client id: f_00008-9-0 loss: 0.589104  [   32/  130]
train() client id: f_00008-9-1 loss: 0.724670  [   64/  130]
train() client id: f_00008-9-2 loss: 0.793381  [   96/  130]
train() client id: f_00008-9-3 loss: 0.765992  [  128/  130]
train() client id: f_00008-10-0 loss: 0.699508  [   32/  130]
train() client id: f_00008-10-1 loss: 0.762043  [   64/  130]
train() client id: f_00008-10-2 loss: 0.682848  [   96/  130]
train() client id: f_00008-10-3 loss: 0.721409  [  128/  130]
train() client id: f_00008-11-0 loss: 0.646087  [   32/  130]
train() client id: f_00008-11-1 loss: 0.665786  [   64/  130]
train() client id: f_00008-11-2 loss: 0.712099  [   96/  130]
train() client id: f_00008-11-3 loss: 0.835825  [  128/  130]
train() client id: f_00009-0-0 loss: 1.015486  [   32/  118]
train() client id: f_00009-0-1 loss: 0.994172  [   64/  118]
train() client id: f_00009-0-2 loss: 1.017068  [   96/  118]
train() client id: f_00009-1-0 loss: 0.890683  [   32/  118]
train() client id: f_00009-1-1 loss: 1.140715  [   64/  118]
train() client id: f_00009-1-2 loss: 1.011748  [   96/  118]
train() client id: f_00009-2-0 loss: 0.938911  [   32/  118]
train() client id: f_00009-2-1 loss: 0.961820  [   64/  118]
train() client id: f_00009-2-2 loss: 0.992856  [   96/  118]
train() client id: f_00009-3-0 loss: 0.785472  [   32/  118]
train() client id: f_00009-3-1 loss: 0.970536  [   64/  118]
train() client id: f_00009-3-2 loss: 0.927518  [   96/  118]
train() client id: f_00009-4-0 loss: 0.963124  [   32/  118]
train() client id: f_00009-4-1 loss: 0.894904  [   64/  118]
train() client id: f_00009-4-2 loss: 0.934640  [   96/  118]
train() client id: f_00009-5-0 loss: 0.872365  [   32/  118]
train() client id: f_00009-5-1 loss: 0.950438  [   64/  118]
train() client id: f_00009-5-2 loss: 0.812514  [   96/  118]
train() client id: f_00009-6-0 loss: 0.849840  [   32/  118]
train() client id: f_00009-6-1 loss: 0.822467  [   64/  118]
train() client id: f_00009-6-2 loss: 0.819998  [   96/  118]
train() client id: f_00009-7-0 loss: 0.765755  [   32/  118]
train() client id: f_00009-7-1 loss: 0.836856  [   64/  118]
train() client id: f_00009-7-2 loss: 0.845659  [   96/  118]
train() client id: f_00009-8-0 loss: 0.884565  [   32/  118]
train() client id: f_00009-8-1 loss: 0.807500  [   64/  118]
train() client id: f_00009-8-2 loss: 0.912435  [   96/  118]
train() client id: f_00009-9-0 loss: 0.878064  [   32/  118]
train() client id: f_00009-9-1 loss: 0.841110  [   64/  118]
train() client id: f_00009-9-2 loss: 0.957185  [   96/  118]
train() client id: f_00009-10-0 loss: 0.802133  [   32/  118]
train() client id: f_00009-10-1 loss: 0.787453  [   64/  118]
train() client id: f_00009-10-2 loss: 0.996318  [   96/  118]
train() client id: f_00009-11-0 loss: 0.854538  [   32/  118]
train() client id: f_00009-11-1 loss: 0.916484  [   64/  118]
train() client id: f_00009-11-2 loss: 0.806595  [   96/  118]
At round 29 accuracy: 0.6551724137931034
At round 29 training accuracy: 0.5814889336016097
At round 29 training loss: 0.833530009561588
update_location
xs = [  -3.9056584     4.20031788  165.00902392   18.81129433    0.97929623
    3.95640986 -127.44319194 -106.32485185  149.66397685  -92.06087855]
ys = [ 157.5879595   140.55583871    1.32061395 -127.45517586  119.35018685
  102.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [186.67945561 172.55024331 192.95005052 163.09103791 155.70942849
 143.47962494 162.01437502 145.96455178 180.85346538 135.9824151 ]
dists_bs = [171.9780593  182.48055834 381.79924762 359.22409609 184.56872542
 192.96681959 183.881171   187.22748312 360.82019861 190.05102236]
uav_gains = [2.06621021e-11 2.54082094e-11 1.88791154e-11 2.93478532e-11
 3.29966588e-11 4.05296333e-11 2.98453693e-11 3.88208176e-11
 2.24815448e-11 4.63629961e-11]
bs_gains = [6.08092382e-11 5.15095318e-11 6.51862344e-12 7.73162525e-12
 4.98943499e-11 4.40496737e-11 5.04184808e-11 4.79357148e-11
 7.63624282e-12 4.59681998e-11]
Round 30
-------------------------------
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.03213939 14.58505974  6.92666737  2.4917993  16.82243501  8.09793447
  3.09128315  9.90171672  7.30136856  6.56936016]
obj_prev = 82.8197638713389
eta_min = 6.720783557496832e-14	eta_max = 0.9260472326332857
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 19.233152927962998	eta = 0.9090909090909091
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 34.518956732515555	eta = 0.5065241286245609
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 27.08219396594083	eta = 0.6456155103960739
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.74057952697607	eta = 0.6792653779081567
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671393607462342	eta = 0.6810960381552402
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671195205593733	eta = 0.6811013020600015
eta = 0.6811013020600015
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.03169041 0.06665046 0.03118739 0.01081498 0.07696242 0.03672063
 0.0135816  0.0450205  0.03269645 0.02967832]
ene_total = [2.2727313  4.14429542 2.25574611 1.06104318 4.72594168 2.47696442
 1.21464967 2.95433273 2.48764402 2.07784665]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 0 obj = 4.633836318316938
eta = 0.6811013020600015
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718008 34168259.23522189]
eta_min = 0.6811013020600248	eta_max = 0.6811013020600005
af = 0.015898726867838657	bf = 1.4716185540757498	zeta = 0.017488599554622525	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [2.80235665e-06 2.41591478e-05 2.69672506e-06 1.07738059e-07
 3.72770088e-05 4.08423848e-06 2.13066208e-07 7.59735426e-06
 3.53741648e-06 2.14972581e-06]
ene_total = [1.73102112 1.4144311  1.77049778 1.59182881 1.42623245 1.45690442
 1.58574596 1.49733708 2.28689603 1.44357775]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 1 obj = 4.633836318316922
eta = 0.6811013020600005
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718006 34168259.23522189]
Done!
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.07750501e-05 9.28918269e-05 1.03688971e-05 4.14252406e-07
 1.43329950e-04 1.57038807e-05 8.19238720e-07 2.92117968e-05
 1.36013522e-05 8.26568717e-06]
ene_total = [0.00879629 0.00724973 0.00899641 0.00808195 0.00734697 0.00740826
 0.00805137 0.00762353 0.01162053 0.00733509]
At round 30 energy consumption: 0.08251011673575459
At round 30 eta: 0.6811013020600005
At round 30 a_n: 17.906227447174857
At round 30 local rounds: 12.575559237281574
At round 30 global rounds: 56.15020557576531
gradient difference: 0.5444285273551941
train() client id: f_00000-0-0 loss: 1.084453  [   32/  126]
train() client id: f_00000-0-1 loss: 1.101486  [   64/  126]
train() client id: f_00000-0-2 loss: 0.855154  [   96/  126]
train() client id: f_00000-1-0 loss: 1.076019  [   32/  126]
train() client id: f_00000-1-1 loss: 1.055525  [   64/  126]
train() client id: f_00000-1-2 loss: 0.977108  [   96/  126]
train() client id: f_00000-2-0 loss: 0.938015  [   32/  126]
train() client id: f_00000-2-1 loss: 1.061938  [   64/  126]
train() client id: f_00000-2-2 loss: 0.966644  [   96/  126]
train() client id: f_00000-3-0 loss: 0.967433  [   32/  126]
train() client id: f_00000-3-1 loss: 0.921255  [   64/  126]
train() client id: f_00000-3-2 loss: 0.952358  [   96/  126]
train() client id: f_00000-4-0 loss: 0.961000  [   32/  126]
train() client id: f_00000-4-1 loss: 0.752436  [   64/  126]
train() client id: f_00000-4-2 loss: 0.912897  [   96/  126]
train() client id: f_00000-5-0 loss: 0.910501  [   32/  126]
train() client id: f_00000-5-1 loss: 0.927509  [   64/  126]
train() client id: f_00000-5-2 loss: 0.893364  [   96/  126]
train() client id: f_00000-6-0 loss: 0.883079  [   32/  126]
train() client id: f_00000-6-1 loss: 0.973152  [   64/  126]
train() client id: f_00000-6-2 loss: 0.954865  [   96/  126]
train() client id: f_00000-7-0 loss: 0.968719  [   32/  126]
train() client id: f_00000-7-1 loss: 0.894125  [   64/  126]
train() client id: f_00000-7-2 loss: 0.866603  [   96/  126]
train() client id: f_00000-8-0 loss: 0.866845  [   32/  126]
train() client id: f_00000-8-1 loss: 0.919480  [   64/  126]
train() client id: f_00000-8-2 loss: 0.963515  [   96/  126]
train() client id: f_00000-9-0 loss: 1.044232  [   32/  126]
train() client id: f_00000-9-1 loss: 0.929756  [   64/  126]
train() client id: f_00000-9-2 loss: 0.871498  [   96/  126]
train() client id: f_00000-10-0 loss: 0.863311  [   32/  126]
train() client id: f_00000-10-1 loss: 0.955490  [   64/  126]
train() client id: f_00000-10-2 loss: 0.962024  [   96/  126]
train() client id: f_00000-11-0 loss: 0.943250  [   32/  126]
train() client id: f_00000-11-1 loss: 0.951242  [   64/  126]
train() client id: f_00000-11-2 loss: 0.839184  [   96/  126]
train() client id: f_00001-0-0 loss: 0.533348  [   32/  265]
train() client id: f_00001-0-1 loss: 0.492550  [   64/  265]
train() client id: f_00001-0-2 loss: 0.582181  [   96/  265]
train() client id: f_00001-0-3 loss: 0.470447  [  128/  265]
train() client id: f_00001-0-4 loss: 0.575452  [  160/  265]
train() client id: f_00001-0-5 loss: 0.427350  [  192/  265]
train() client id: f_00001-0-6 loss: 0.492408  [  224/  265]
train() client id: f_00001-0-7 loss: 0.575400  [  256/  265]
train() client id: f_00001-1-0 loss: 0.421633  [   32/  265]
train() client id: f_00001-1-1 loss: 0.621080  [   64/  265]
train() client id: f_00001-1-2 loss: 0.556531  [   96/  265]
train() client id: f_00001-1-3 loss: 0.535498  [  128/  265]
train() client id: f_00001-1-4 loss: 0.476019  [  160/  265]
train() client id: f_00001-1-5 loss: 0.531777  [  192/  265]
train() client id: f_00001-1-6 loss: 0.496637  [  224/  265]
train() client id: f_00001-1-7 loss: 0.470564  [  256/  265]
train() client id: f_00001-2-0 loss: 0.671746  [   32/  265]
train() client id: f_00001-2-1 loss: 0.458774  [   64/  265]
train() client id: f_00001-2-2 loss: 0.474283  [   96/  265]
train() client id: f_00001-2-3 loss: 0.483712  [  128/  265]
train() client id: f_00001-2-4 loss: 0.488245  [  160/  265]
train() client id: f_00001-2-5 loss: 0.416907  [  192/  265]
train() client id: f_00001-2-6 loss: 0.562263  [  224/  265]
train() client id: f_00001-2-7 loss: 0.489386  [  256/  265]
train() client id: f_00001-3-0 loss: 0.532614  [   32/  265]
train() client id: f_00001-3-1 loss: 0.543167  [   64/  265]
train() client id: f_00001-3-2 loss: 0.459805  [   96/  265]
train() client id: f_00001-3-3 loss: 0.547392  [  128/  265]
train() client id: f_00001-3-4 loss: 0.462732  [  160/  265]
train() client id: f_00001-3-5 loss: 0.485127  [  192/  265]
train() client id: f_00001-3-6 loss: 0.477627  [  224/  265]
train() client id: f_00001-3-7 loss: 0.498865  [  256/  265]
train() client id: f_00001-4-0 loss: 0.443376  [   32/  265]
train() client id: f_00001-4-1 loss: 0.488294  [   64/  265]
train() client id: f_00001-4-2 loss: 0.581845  [   96/  265]
train() client id: f_00001-4-3 loss: 0.442489  [  128/  265]
train() client id: f_00001-4-4 loss: 0.463290  [  160/  265]
train() client id: f_00001-4-5 loss: 0.549595  [  192/  265]
train() client id: f_00001-4-6 loss: 0.549249  [  224/  265]
train() client id: f_00001-4-7 loss: 0.476112  [  256/  265]
train() client id: f_00001-5-0 loss: 0.495552  [   32/  265]
train() client id: f_00001-5-1 loss: 0.644843  [   64/  265]
train() client id: f_00001-5-2 loss: 0.469092  [   96/  265]
train() client id: f_00001-5-3 loss: 0.395150  [  128/  265]
train() client id: f_00001-5-4 loss: 0.514646  [  160/  265]
train() client id: f_00001-5-5 loss: 0.488177  [  192/  265]
train() client id: f_00001-5-6 loss: 0.406855  [  224/  265]
train() client id: f_00001-5-7 loss: 0.529877  [  256/  265]
train() client id: f_00001-6-0 loss: 0.519211  [   32/  265]
train() client id: f_00001-6-1 loss: 0.472130  [   64/  265]
train() client id: f_00001-6-2 loss: 0.491342  [   96/  265]
train() client id: f_00001-6-3 loss: 0.433890  [  128/  265]
train() client id: f_00001-6-4 loss: 0.634124  [  160/  265]
train() client id: f_00001-6-5 loss: 0.416124  [  192/  265]
train() client id: f_00001-6-6 loss: 0.448783  [  224/  265]
train() client id: f_00001-6-7 loss: 0.531120  [  256/  265]
train() client id: f_00001-7-0 loss: 0.471922  [   32/  265]
train() client id: f_00001-7-1 loss: 0.468116  [   64/  265]
train() client id: f_00001-7-2 loss: 0.422749  [   96/  265]
train() client id: f_00001-7-3 loss: 0.565781  [  128/  265]
train() client id: f_00001-7-4 loss: 0.432686  [  160/  265]
train() client id: f_00001-7-5 loss: 0.551520  [  192/  265]
train() client id: f_00001-7-6 loss: 0.458434  [  224/  265]
train() client id: f_00001-7-7 loss: 0.450351  [  256/  265]
train() client id: f_00001-8-0 loss: 0.516391  [   32/  265]
train() client id: f_00001-8-1 loss: 0.442584  [   64/  265]
train() client id: f_00001-8-2 loss: 0.481535  [   96/  265]
train() client id: f_00001-8-3 loss: 0.487606  [  128/  265]
train() client id: f_00001-8-4 loss: 0.440842  [  160/  265]
train() client id: f_00001-8-5 loss: 0.540483  [  192/  265]
train() client id: f_00001-8-6 loss: 0.408619  [  224/  265]
train() client id: f_00001-8-7 loss: 0.557602  [  256/  265]
train() client id: f_00001-9-0 loss: 0.489560  [   32/  265]
train() client id: f_00001-9-1 loss: 0.500906  [   64/  265]
train() client id: f_00001-9-2 loss: 0.507155  [   96/  265]
train() client id: f_00001-9-3 loss: 0.583756  [  128/  265]
train() client id: f_00001-9-4 loss: 0.389959  [  160/  265]
train() client id: f_00001-9-5 loss: 0.476687  [  192/  265]
train() client id: f_00001-9-6 loss: 0.529365  [  224/  265]
train() client id: f_00001-9-7 loss: 0.449316  [  256/  265]
train() client id: f_00001-10-0 loss: 0.492950  [   32/  265]
train() client id: f_00001-10-1 loss: 0.577731  [   64/  265]
train() client id: f_00001-10-2 loss: 0.386527  [   96/  265]
train() client id: f_00001-10-3 loss: 0.550783  [  128/  265]
train() client id: f_00001-10-4 loss: 0.487819  [  160/  265]
train() client id: f_00001-10-5 loss: 0.430845  [  192/  265]
train() client id: f_00001-10-6 loss: 0.439150  [  224/  265]
train() client id: f_00001-10-7 loss: 0.548464  [  256/  265]
train() client id: f_00001-11-0 loss: 0.396767  [   32/  265]
train() client id: f_00001-11-1 loss: 0.501209  [   64/  265]
train() client id: f_00001-11-2 loss: 0.513232  [   96/  265]
train() client id: f_00001-11-3 loss: 0.599263  [  128/  265]
train() client id: f_00001-11-4 loss: 0.375455  [  160/  265]
train() client id: f_00001-11-5 loss: 0.480444  [  192/  265]
train() client id: f_00001-11-6 loss: 0.673647  [  224/  265]
train() client id: f_00001-11-7 loss: 0.392107  [  256/  265]
train() client id: f_00002-0-0 loss: 0.981894  [   32/  124]
train() client id: f_00002-0-1 loss: 1.230577  [   64/  124]
train() client id: f_00002-0-2 loss: 1.124011  [   96/  124]
train() client id: f_00002-1-0 loss: 1.292145  [   32/  124]
train() client id: f_00002-1-1 loss: 1.137068  [   64/  124]
train() client id: f_00002-1-2 loss: 1.008725  [   96/  124]
train() client id: f_00002-2-0 loss: 1.075818  [   32/  124]
train() client id: f_00002-2-1 loss: 1.061987  [   64/  124]
train() client id: f_00002-2-2 loss: 1.131423  [   96/  124]
train() client id: f_00002-3-0 loss: 1.176728  [   32/  124]
train() client id: f_00002-3-1 loss: 0.949179  [   64/  124]
train() client id: f_00002-3-2 loss: 0.970280  [   96/  124]
train() client id: f_00002-4-0 loss: 0.945612  [   32/  124]
train() client id: f_00002-4-1 loss: 0.982114  [   64/  124]
train() client id: f_00002-4-2 loss: 1.079593  [   96/  124]
train() client id: f_00002-5-0 loss: 0.947088  [   32/  124]
train() client id: f_00002-5-1 loss: 1.100507  [   64/  124]
train() client id: f_00002-5-2 loss: 0.943550  [   96/  124]
train() client id: f_00002-6-0 loss: 0.967828  [   32/  124]
train() client id: f_00002-6-1 loss: 1.006011  [   64/  124]
train() client id: f_00002-6-2 loss: 1.026057  [   96/  124]
train() client id: f_00002-7-0 loss: 1.126445  [   32/  124]
train() client id: f_00002-7-1 loss: 0.858241  [   64/  124]
train() client id: f_00002-7-2 loss: 0.952542  [   96/  124]
train() client id: f_00002-8-0 loss: 1.103789  [   32/  124]
train() client id: f_00002-8-1 loss: 0.889877  [   64/  124]
train() client id: f_00002-8-2 loss: 0.913089  [   96/  124]
train() client id: f_00002-9-0 loss: 1.033919  [   32/  124]
train() client id: f_00002-9-1 loss: 0.871168  [   64/  124]
train() client id: f_00002-9-2 loss: 1.009625  [   96/  124]
train() client id: f_00002-10-0 loss: 0.830199  [   32/  124]
train() client id: f_00002-10-1 loss: 0.978989  [   64/  124]
train() client id: f_00002-10-2 loss: 0.872945  [   96/  124]
train() client id: f_00002-11-0 loss: 0.979940  [   32/  124]
train() client id: f_00002-11-1 loss: 1.081926  [   64/  124]
train() client id: f_00002-11-2 loss: 0.896342  [   96/  124]
train() client id: f_00003-0-0 loss: 0.743968  [   32/   43]
train() client id: f_00003-1-0 loss: 0.600885  [   32/   43]
train() client id: f_00003-2-0 loss: 0.683102  [   32/   43]
train() client id: f_00003-3-0 loss: 0.651830  [   32/   43]
train() client id: f_00003-4-0 loss: 0.833629  [   32/   43]
train() client id: f_00003-5-0 loss: 0.742323  [   32/   43]
train() client id: f_00003-6-0 loss: 0.697497  [   32/   43]
train() client id: f_00003-7-0 loss: 0.765863  [   32/   43]
train() client id: f_00003-8-0 loss: 0.611835  [   32/   43]
train() client id: f_00003-9-0 loss: 0.967076  [   32/   43]
train() client id: f_00003-10-0 loss: 0.692897  [   32/   43]
train() client id: f_00003-11-0 loss: 0.677313  [   32/   43]
train() client id: f_00004-0-0 loss: 0.770415  [   32/  306]
train() client id: f_00004-0-1 loss: 0.873761  [   64/  306]
train() client id: f_00004-0-2 loss: 0.891858  [   96/  306]
train() client id: f_00004-0-3 loss: 0.823075  [  128/  306]
train() client id: f_00004-0-4 loss: 0.919381  [  160/  306]
train() client id: f_00004-0-5 loss: 0.785364  [  192/  306]
train() client id: f_00004-0-6 loss: 0.867744  [  224/  306]
train() client id: f_00004-0-7 loss: 0.845472  [  256/  306]
train() client id: f_00004-0-8 loss: 0.866369  [  288/  306]
train() client id: f_00004-1-0 loss: 0.805598  [   32/  306]
train() client id: f_00004-1-1 loss: 0.881597  [   64/  306]
train() client id: f_00004-1-2 loss: 0.747806  [   96/  306]
train() client id: f_00004-1-3 loss: 0.846757  [  128/  306]
train() client id: f_00004-1-4 loss: 0.946957  [  160/  306]
train() client id: f_00004-1-5 loss: 0.810730  [  192/  306]
train() client id: f_00004-1-6 loss: 0.943295  [  224/  306]
train() client id: f_00004-1-7 loss: 0.965791  [  256/  306]
train() client id: f_00004-1-8 loss: 0.618212  [  288/  306]
train() client id: f_00004-2-0 loss: 0.789748  [   32/  306]
train() client id: f_00004-2-1 loss: 0.896991  [   64/  306]
train() client id: f_00004-2-2 loss: 0.935391  [   96/  306]
train() client id: f_00004-2-3 loss: 0.738181  [  128/  306]
train() client id: f_00004-2-4 loss: 0.944855  [  160/  306]
train() client id: f_00004-2-5 loss: 0.917190  [  192/  306]
train() client id: f_00004-2-6 loss: 0.791752  [  224/  306]
train() client id: f_00004-2-7 loss: 0.799427  [  256/  306]
train() client id: f_00004-2-8 loss: 0.784073  [  288/  306]
train() client id: f_00004-3-0 loss: 0.903995  [   32/  306]
train() client id: f_00004-3-1 loss: 0.872703  [   64/  306]
train() client id: f_00004-3-2 loss: 0.813443  [   96/  306]
train() client id: f_00004-3-3 loss: 0.897405  [  128/  306]
train() client id: f_00004-3-4 loss: 0.796141  [  160/  306]
train() client id: f_00004-3-5 loss: 0.845007  [  192/  306]
train() client id: f_00004-3-6 loss: 0.751734  [  224/  306]
train() client id: f_00004-3-7 loss: 0.820073  [  256/  306]
train() client id: f_00004-3-8 loss: 0.758942  [  288/  306]
train() client id: f_00004-4-0 loss: 0.762422  [   32/  306]
train() client id: f_00004-4-1 loss: 0.730384  [   64/  306]
train() client id: f_00004-4-2 loss: 0.752470  [   96/  306]
train() client id: f_00004-4-3 loss: 0.796490  [  128/  306]
train() client id: f_00004-4-4 loss: 1.030959  [  160/  306]
train() client id: f_00004-4-5 loss: 0.841710  [  192/  306]
train() client id: f_00004-4-6 loss: 0.748020  [  224/  306]
train() client id: f_00004-4-7 loss: 0.785081  [  256/  306]
train() client id: f_00004-4-8 loss: 1.052738  [  288/  306]
train() client id: f_00004-5-0 loss: 0.645667  [   32/  306]
train() client id: f_00004-5-1 loss: 0.767108  [   64/  306]
train() client id: f_00004-5-2 loss: 0.816909  [   96/  306]
train() client id: f_00004-5-3 loss: 0.907928  [  128/  306]
train() client id: f_00004-5-4 loss: 0.814766  [  160/  306]
train() client id: f_00004-5-5 loss: 0.862553  [  192/  306]
train() client id: f_00004-5-6 loss: 0.956761  [  224/  306]
train() client id: f_00004-5-7 loss: 0.984483  [  256/  306]
train() client id: f_00004-5-8 loss: 0.772852  [  288/  306]
train() client id: f_00004-6-0 loss: 0.802451  [   32/  306]
train() client id: f_00004-6-1 loss: 0.963019  [   64/  306]
train() client id: f_00004-6-2 loss: 0.795690  [   96/  306]
train() client id: f_00004-6-3 loss: 0.818708  [  128/  306]
train() client id: f_00004-6-4 loss: 0.713124  [  160/  306]
train() client id: f_00004-6-5 loss: 0.905548  [  192/  306]
train() client id: f_00004-6-6 loss: 0.780650  [  224/  306]
train() client id: f_00004-6-7 loss: 0.979958  [  256/  306]
train() client id: f_00004-6-8 loss: 0.769354  [  288/  306]
train() client id: f_00004-7-0 loss: 0.767118  [   32/  306]
train() client id: f_00004-7-1 loss: 0.758003  [   64/  306]
train() client id: f_00004-7-2 loss: 0.851738  [   96/  306]
train() client id: f_00004-7-3 loss: 0.730826  [  128/  306]
train() client id: f_00004-7-4 loss: 0.797861  [  160/  306]
train() client id: f_00004-7-5 loss: 0.879718  [  192/  306]
train() client id: f_00004-7-6 loss: 0.917691  [  224/  306]
train() client id: f_00004-7-7 loss: 0.838405  [  256/  306]
train() client id: f_00004-7-8 loss: 0.945359  [  288/  306]
train() client id: f_00004-8-0 loss: 0.949530  [   32/  306]
train() client id: f_00004-8-1 loss: 0.854309  [   64/  306]
train() client id: f_00004-8-2 loss: 0.841570  [   96/  306]
train() client id: f_00004-8-3 loss: 0.779171  [  128/  306]
train() client id: f_00004-8-4 loss: 0.824714  [  160/  306]
train() client id: f_00004-8-5 loss: 0.865617  [  192/  306]
train() client id: f_00004-8-6 loss: 0.790013  [  224/  306]
train() client id: f_00004-8-7 loss: 0.918957  [  256/  306]
train() client id: f_00004-8-8 loss: 0.782248  [  288/  306]
train() client id: f_00004-9-0 loss: 0.831961  [   32/  306]
train() client id: f_00004-9-1 loss: 0.787397  [   64/  306]
train() client id: f_00004-9-2 loss: 0.831471  [   96/  306]
train() client id: f_00004-9-3 loss: 0.833837  [  128/  306]
train() client id: f_00004-9-4 loss: 0.909948  [  160/  306]
train() client id: f_00004-9-5 loss: 0.702623  [  192/  306]
train() client id: f_00004-9-6 loss: 0.902446  [  224/  306]
train() client id: f_00004-9-7 loss: 0.890685  [  256/  306]
train() client id: f_00004-9-8 loss: 0.767804  [  288/  306]
train() client id: f_00004-10-0 loss: 0.887135  [   32/  306]
train() client id: f_00004-10-1 loss: 0.840012  [   64/  306]
train() client id: f_00004-10-2 loss: 0.878623  [   96/  306]
train() client id: f_00004-10-3 loss: 0.749786  [  128/  306]
train() client id: f_00004-10-4 loss: 0.810723  [  160/  306]
train() client id: f_00004-10-5 loss: 0.851296  [  192/  306]
train() client id: f_00004-10-6 loss: 0.831236  [  224/  306]
train() client id: f_00004-10-7 loss: 0.808164  [  256/  306]
train() client id: f_00004-10-8 loss: 0.827196  [  288/  306]
train() client id: f_00004-11-0 loss: 0.858560  [   32/  306]
train() client id: f_00004-11-1 loss: 0.752332  [   64/  306]
train() client id: f_00004-11-2 loss: 0.793723  [   96/  306]
train() client id: f_00004-11-3 loss: 0.770371  [  128/  306]
train() client id: f_00004-11-4 loss: 0.935320  [  160/  306]
train() client id: f_00004-11-5 loss: 0.904308  [  192/  306]
train() client id: f_00004-11-6 loss: 0.784881  [  224/  306]
train() client id: f_00004-11-7 loss: 0.820423  [  256/  306]
train() client id: f_00004-11-8 loss: 0.898500  [  288/  306]
train() client id: f_00005-0-0 loss: 0.624495  [   32/  146]
train() client id: f_00005-0-1 loss: 0.836704  [   64/  146]
train() client id: f_00005-0-2 loss: 0.579108  [   96/  146]
train() client id: f_00005-0-3 loss: 0.544552  [  128/  146]
train() client id: f_00005-1-0 loss: 0.745952  [   32/  146]
train() client id: f_00005-1-1 loss: 0.510445  [   64/  146]
train() client id: f_00005-1-2 loss: 0.649445  [   96/  146]
train() client id: f_00005-1-3 loss: 0.610948  [  128/  146]
train() client id: f_00005-2-0 loss: 0.559752  [   32/  146]
train() client id: f_00005-2-1 loss: 0.944904  [   64/  146]
train() client id: f_00005-2-2 loss: 0.695286  [   96/  146]
train() client id: f_00005-2-3 loss: 0.488232  [  128/  146]
train() client id: f_00005-3-0 loss: 0.695426  [   32/  146]
train() client id: f_00005-3-1 loss: 0.543665  [   64/  146]
train() client id: f_00005-3-2 loss: 0.592564  [   96/  146]
train() client id: f_00005-3-3 loss: 0.707129  [  128/  146]
train() client id: f_00005-4-0 loss: 0.720138  [   32/  146]
train() client id: f_00005-4-1 loss: 0.674406  [   64/  146]
train() client id: f_00005-4-2 loss: 0.533532  [   96/  146]
train() client id: f_00005-4-3 loss: 0.643764  [  128/  146]
train() client id: f_00005-5-0 loss: 0.715535  [   32/  146]
train() client id: f_00005-5-1 loss: 0.499095  [   64/  146]
train() client id: f_00005-5-2 loss: 0.476619  [   96/  146]
train() client id: f_00005-5-3 loss: 0.779030  [  128/  146]
train() client id: f_00005-6-0 loss: 0.454271  [   32/  146]
train() client id: f_00005-6-1 loss: 0.644175  [   64/  146]
train() client id: f_00005-6-2 loss: 0.594171  [   96/  146]
train() client id: f_00005-6-3 loss: 0.778211  [  128/  146]
train() client id: f_00005-7-0 loss: 0.673074  [   32/  146]
train() client id: f_00005-7-1 loss: 0.428856  [   64/  146]
train() client id: f_00005-7-2 loss: 0.515341  [   96/  146]
train() client id: f_00005-7-3 loss: 0.802944  [  128/  146]
train() client id: f_00005-8-0 loss: 0.399512  [   32/  146]
train() client id: f_00005-8-1 loss: 0.486205  [   64/  146]
train() client id: f_00005-8-2 loss: 0.668833  [   96/  146]
train() client id: f_00005-8-3 loss: 0.844977  [  128/  146]
train() client id: f_00005-9-0 loss: 0.882535  [   32/  146]
train() client id: f_00005-9-1 loss: 0.528248  [   64/  146]
train() client id: f_00005-9-2 loss: 0.557473  [   96/  146]
train() client id: f_00005-9-3 loss: 0.532513  [  128/  146]
train() client id: f_00005-10-0 loss: 0.776912  [   32/  146]
train() client id: f_00005-10-1 loss: 0.562572  [   64/  146]
train() client id: f_00005-10-2 loss: 0.744202  [   96/  146]
train() client id: f_00005-10-3 loss: 0.439282  [  128/  146]
train() client id: f_00005-11-0 loss: 1.158639  [   32/  146]
train() client id: f_00005-11-1 loss: 0.491953  [   64/  146]
train() client id: f_00005-11-2 loss: 0.362707  [   96/  146]
train() client id: f_00005-11-3 loss: 0.704302  [  128/  146]
train() client id: f_00006-0-0 loss: 0.537679  [   32/   54]
train() client id: f_00006-1-0 loss: 0.487965  [   32/   54]
train() client id: f_00006-2-0 loss: 0.429321  [   32/   54]
train() client id: f_00006-3-0 loss: 0.457101  [   32/   54]
train() client id: f_00006-4-0 loss: 0.538139  [   32/   54]
train() client id: f_00006-5-0 loss: 0.501362  [   32/   54]
train() client id: f_00006-6-0 loss: 0.492839  [   32/   54]
train() client id: f_00006-7-0 loss: 0.541972  [   32/   54]
train() client id: f_00006-8-0 loss: 0.439436  [   32/   54]
train() client id: f_00006-9-0 loss: 0.548943  [   32/   54]
train() client id: f_00006-10-0 loss: 0.523032  [   32/   54]
train() client id: f_00006-11-0 loss: 0.519391  [   32/   54]
train() client id: f_00007-0-0 loss: 0.631692  [   32/  179]
train() client id: f_00007-0-1 loss: 0.811218  [   64/  179]
train() client id: f_00007-0-2 loss: 0.841920  [   96/  179]
train() client id: f_00007-0-3 loss: 0.740522  [  128/  179]
train() client id: f_00007-0-4 loss: 0.687756  [  160/  179]
train() client id: f_00007-1-0 loss: 0.599719  [   32/  179]
train() client id: f_00007-1-1 loss: 0.670195  [   64/  179]
train() client id: f_00007-1-2 loss: 0.735113  [   96/  179]
train() client id: f_00007-1-3 loss: 0.708114  [  128/  179]
train() client id: f_00007-1-4 loss: 0.817758  [  160/  179]
train() client id: f_00007-2-0 loss: 0.783034  [   32/  179]
train() client id: f_00007-2-1 loss: 0.745901  [   64/  179]
train() client id: f_00007-2-2 loss: 0.626915  [   96/  179]
train() client id: f_00007-2-3 loss: 0.583365  [  128/  179]
train() client id: f_00007-2-4 loss: 0.878046  [  160/  179]
train() client id: f_00007-3-0 loss: 0.772183  [   32/  179]
train() client id: f_00007-3-1 loss: 0.742453  [   64/  179]
train() client id: f_00007-3-2 loss: 0.673595  [   96/  179]
train() client id: f_00007-3-3 loss: 0.737966  [  128/  179]
train() client id: f_00007-3-4 loss: 0.599079  [  160/  179]
train() client id: f_00007-4-0 loss: 0.667708  [   32/  179]
train() client id: f_00007-4-1 loss: 0.568094  [   64/  179]
train() client id: f_00007-4-2 loss: 0.551422  [   96/  179]
train() client id: f_00007-4-3 loss: 0.707607  [  128/  179]
train() client id: f_00007-4-4 loss: 0.863256  [  160/  179]
train() client id: f_00007-5-0 loss: 0.643263  [   32/  179]
train() client id: f_00007-5-1 loss: 0.733300  [   64/  179]
train() client id: f_00007-5-2 loss: 0.777149  [   96/  179]
train() client id: f_00007-5-3 loss: 0.648194  [  128/  179]
train() client id: f_00007-5-4 loss: 0.728617  [  160/  179]
train() client id: f_00007-6-0 loss: 0.642665  [   32/  179]
train() client id: f_00007-6-1 loss: 0.777127  [   64/  179]
train() client id: f_00007-6-2 loss: 0.574520  [   96/  179]
train() client id: f_00007-6-3 loss: 0.798103  [  128/  179]
train() client id: f_00007-6-4 loss: 0.649433  [  160/  179]
train() client id: f_00007-7-0 loss: 0.668042  [   32/  179]
train() client id: f_00007-7-1 loss: 0.567297  [   64/  179]
train() client id: f_00007-7-2 loss: 0.710189  [   96/  179]
train() client id: f_00007-7-3 loss: 0.607710  [  128/  179]
train() client id: f_00007-7-4 loss: 0.817041  [  160/  179]
train() client id: f_00007-8-0 loss: 0.636391  [   32/  179]
train() client id: f_00007-8-1 loss: 0.652575  [   64/  179]
train() client id: f_00007-8-2 loss: 0.789786  [   96/  179]
train() client id: f_00007-8-3 loss: 0.612489  [  128/  179]
train() client id: f_00007-8-4 loss: 0.744479  [  160/  179]
train() client id: f_00007-9-0 loss: 0.755003  [   32/  179]
train() client id: f_00007-9-1 loss: 0.766377  [   64/  179]
train() client id: f_00007-9-2 loss: 0.731681  [   96/  179]
train() client id: f_00007-9-3 loss: 0.665155  [  128/  179]
train() client id: f_00007-9-4 loss: 0.614313  [  160/  179]
train() client id: f_00007-10-0 loss: 0.626759  [   32/  179]
train() client id: f_00007-10-1 loss: 0.737454  [   64/  179]
train() client id: f_00007-10-2 loss: 0.760506  [   96/  179]
train() client id: f_00007-10-3 loss: 0.674304  [  128/  179]
train() client id: f_00007-10-4 loss: 0.553192  [  160/  179]
train() client id: f_00007-11-0 loss: 0.616077  [   32/  179]
train() client id: f_00007-11-1 loss: 0.661548  [   64/  179]
train() client id: f_00007-11-2 loss: 0.605532  [   96/  179]
train() client id: f_00007-11-3 loss: 0.950845  [  128/  179]
train() client id: f_00007-11-4 loss: 0.685817  [  160/  179]
train() client id: f_00008-0-0 loss: 0.614263  [   32/  130]
train() client id: f_00008-0-1 loss: 0.641448  [   64/  130]
train() client id: f_00008-0-2 loss: 0.731358  [   96/  130]
train() client id: f_00008-0-3 loss: 0.582766  [  128/  130]
train() client id: f_00008-1-0 loss: 0.655614  [   32/  130]
train() client id: f_00008-1-1 loss: 0.681899  [   64/  130]
train() client id: f_00008-1-2 loss: 0.629393  [   96/  130]
train() client id: f_00008-1-3 loss: 0.584802  [  128/  130]
train() client id: f_00008-2-0 loss: 0.665666  [   32/  130]
train() client id: f_00008-2-1 loss: 0.572147  [   64/  130]
train() client id: f_00008-2-2 loss: 0.645017  [   96/  130]
train() client id: f_00008-2-3 loss: 0.640469  [  128/  130]
train() client id: f_00008-3-0 loss: 0.671080  [   32/  130]
train() client id: f_00008-3-1 loss: 0.577454  [   64/  130]
train() client id: f_00008-3-2 loss: 0.536872  [   96/  130]
train() client id: f_00008-3-3 loss: 0.733450  [  128/  130]
train() client id: f_00008-4-0 loss: 0.642150  [   32/  130]
train() client id: f_00008-4-1 loss: 0.675237  [   64/  130]
train() client id: f_00008-4-2 loss: 0.648210  [   96/  130]
train() client id: f_00008-4-3 loss: 0.603682  [  128/  130]
train() client id: f_00008-5-0 loss: 0.690135  [   32/  130]
train() client id: f_00008-5-1 loss: 0.625525  [   64/  130]
train() client id: f_00008-5-2 loss: 0.661686  [   96/  130]
train() client id: f_00008-5-3 loss: 0.585789  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780769  [   32/  130]
train() client id: f_00008-6-1 loss: 0.525588  [   64/  130]
train() client id: f_00008-6-2 loss: 0.643188  [   96/  130]
train() client id: f_00008-6-3 loss: 0.578683  [  128/  130]
train() client id: f_00008-7-0 loss: 0.663435  [   32/  130]
train() client id: f_00008-7-1 loss: 0.613654  [   64/  130]
train() client id: f_00008-7-2 loss: 0.546328  [   96/  130]
train() client id: f_00008-7-3 loss: 0.703454  [  128/  130]
train() client id: f_00008-8-0 loss: 0.683589  [   32/  130]
train() client id: f_00008-8-1 loss: 0.647952  [   64/  130]
train() client id: f_00008-8-2 loss: 0.592603  [   96/  130]
train() client id: f_00008-8-3 loss: 0.614981  [  128/  130]
train() client id: f_00008-9-0 loss: 0.684319  [   32/  130]
train() client id: f_00008-9-1 loss: 0.578587  [   64/  130]
train() client id: f_00008-9-2 loss: 0.628833  [   96/  130]
train() client id: f_00008-9-3 loss: 0.680046  [  128/  130]
train() client id: f_00008-10-0 loss: 0.553305  [   32/  130]
train() client id: f_00008-10-1 loss: 0.690140  [   64/  130]
train() client id: f_00008-10-2 loss: 0.698709  [   96/  130]
train() client id: f_00008-10-3 loss: 0.631786  [  128/  130]
train() client id: f_00008-11-0 loss: 0.600608  [   32/  130]
train() client id: f_00008-11-1 loss: 0.650324  [   64/  130]
train() client id: f_00008-11-2 loss: 0.682061  [   96/  130]
train() client id: f_00008-11-3 loss: 0.649167  [  128/  130]
train() client id: f_00009-0-0 loss: 0.989084  [   32/  118]
train() client id: f_00009-0-1 loss: 1.084844  [   64/  118]
train() client id: f_00009-0-2 loss: 1.101294  [   96/  118]
train() client id: f_00009-1-0 loss: 1.094341  [   32/  118]
train() client id: f_00009-1-1 loss: 0.990054  [   64/  118]
train() client id: f_00009-1-2 loss: 1.037810  [   96/  118]
train() client id: f_00009-2-0 loss: 1.013378  [   32/  118]
train() client id: f_00009-2-1 loss: 0.921863  [   64/  118]
train() client id: f_00009-2-2 loss: 0.947328  [   96/  118]
train() client id: f_00009-3-0 loss: 0.964420  [   32/  118]
train() client id: f_00009-3-1 loss: 0.909398  [   64/  118]
train() client id: f_00009-3-2 loss: 1.010700  [   96/  118]
train() client id: f_00009-4-0 loss: 0.967013  [   32/  118]
train() client id: f_00009-4-1 loss: 0.855304  [   64/  118]
train() client id: f_00009-4-2 loss: 0.910171  [   96/  118]
train() client id: f_00009-5-0 loss: 0.835078  [   32/  118]
train() client id: f_00009-5-1 loss: 0.871712  [   64/  118]
train() client id: f_00009-5-2 loss: 1.047109  [   96/  118]
train() client id: f_00009-6-0 loss: 0.875645  [   32/  118]
train() client id: f_00009-6-1 loss: 0.877900  [   64/  118]
train() client id: f_00009-6-2 loss: 0.832402  [   96/  118]
train() client id: f_00009-7-0 loss: 0.870597  [   32/  118]
train() client id: f_00009-7-1 loss: 0.838076  [   64/  118]
train() client id: f_00009-7-2 loss: 1.045905  [   96/  118]
train() client id: f_00009-8-0 loss: 0.878287  [   32/  118]
train() client id: f_00009-8-1 loss: 1.018248  [   64/  118]
train() client id: f_00009-8-2 loss: 0.767146  [   96/  118]
train() client id: f_00009-9-0 loss: 0.946423  [   32/  118]
train() client id: f_00009-9-1 loss: 0.900729  [   64/  118]
train() client id: f_00009-9-2 loss: 0.914322  [   96/  118]
train() client id: f_00009-10-0 loss: 0.874179  [   32/  118]
train() client id: f_00009-10-1 loss: 0.930951  [   64/  118]
train() client id: f_00009-10-2 loss: 0.819457  [   96/  118]
train() client id: f_00009-11-0 loss: 0.841007  [   32/  118]
train() client id: f_00009-11-1 loss: 0.855659  [   64/  118]
train() client id: f_00009-11-2 loss: 0.858333  [   96/  118]
At round 30 accuracy: 0.6551724137931034
At round 30 training accuracy: 0.5855130784708249
At round 30 training loss: 0.8334453492590492
update_location
xs = [  -3.9056584     4.20031788  170.00902392   18.81129433    0.97929623
    3.95640986 -132.44319194 -111.32485185  154.66397685  -97.06087855]
ys = [ 162.5879595   145.55583871    1.32061395 -132.45517586  124.35018685
  107.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [190.91908951 176.64694974 197.24302835 167.02765761 159.57420841
 147.10385542 165.97617188 149.64591172 185.01247447 139.41601774]
dists_bs = [171.5439666  181.60317332 386.25866427 363.4439475  183.12322702
 191.15264835 182.65190108 185.45182381 365.32568415 187.92285621]
uav_gains = [1.94386836e-11 2.39101861e-11 1.77469892e-11 2.76188258e-11
 3.10151017e-11 3.80704871e-11 2.80673490e-11 3.64667227e-11
 2.11653608e-11 4.35559512e-11]
bs_gains = [6.12410786e-11 5.22093724e-11 6.31008238e-12 7.48288810e-12
 5.10049673e-11 4.52302711e-11 5.13743493e-11 4.92319435e-11
 7.37546672e-12 4.74407113e-11]
Round 31
-------------------------------
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.90000386 14.30569929  6.7966894   2.4461517  16.50004725  7.94229532
  3.0341758   9.71414062  7.16398278  6.44283898]
obj_prev = 81.24602500345188
eta_min = 3.803699202355281e-14	eta_max = 0.9265448863059464
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 18.86522301759883	eta = 0.9090909090909091
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 33.97406433952712	eta = 0.5048027981544223
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 26.610863860443008	eta = 0.6444812477044536
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.281958371636836	eta = 0.6783573681741374
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.213171638427752	eta = 0.6802080670062467
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.212972778010624	eta = 0.680213431961071
eta = 0.680213431961071
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.03179805 0.06687685 0.03129332 0.01085171 0.07722383 0.03684536
 0.01362773 0.04517342 0.03280751 0.02977912]
ene_total = [2.23637289 4.06508358 2.2200981  1.04612844 4.63524192 2.42743785
 1.19692013 2.90387674 2.44646747 2.03534566]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 0 obj = 4.564704952114099
eta = 0.680213431961071
freqs = [37078600.58661079 74869583.15751041 36668274.98538263 12443483.84142069
 86519118.7997421  41447912.13753933 15615771.9937943  51215637.28090309
 40944429.42168844 33444300.55685099]
eta_min = 0.6802134319610765	eta_max = 0.6802134319610728
af = 0.014970687331339992	bf = 1.4539623964515263	zeta = 0.016467756064473992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [2.70316510e-06 2.31799269e-05 2.60170470e-06 1.03898144e-07
 3.57438195e-05 3.91393429e-06 2.05483065e-07 7.32678673e-06
 3.40085688e-06 2.05959391e-06]
ene_total = [1.72390573 1.38338877 1.76415135 1.58337944 1.39239245 1.42109708
 1.57750334 1.48871744 2.26805167 1.40669455]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 1 obj = 4.564704952114124
eta = 0.6802134319610728
freqs = [37078600.58661079 74869583.15751038 36668274.98538262 12443483.84142068
 86519118.79974204 41447912.1375393  15615771.9937943  51215637.28090306
 40944429.42168847 33444300.55685098]
Done!
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.03936590e-05 8.91267263e-05 1.00035442e-05 3.99487948e-07
 1.37434843e-04 1.50490617e-05 7.90081565e-07 2.81714657e-05
 1.30762811e-05 7.91913038e-06]
ene_total = [0.00893058 0.00722633 0.0091386  0.00819582 0.00730867 0.00736669
 0.00816569 0.0077264  0.01174904 0.00728687]
At round 31 energy consumption: 0.08309468916898873
At round 31 eta: 0.6802134319610728
At round 31 a_n: 17.56368160020554
At round 31 local rounds: 12.618272918924527
At round 31 global rounds: 54.923137353497395
gradient difference: 0.4599246382713318
train() client id: f_00000-0-0 loss: 1.264345  [   32/  126]
train() client id: f_00000-0-1 loss: 1.174323  [   64/  126]
train() client id: f_00000-0-2 loss: 1.505342  [   96/  126]
train() client id: f_00000-1-0 loss: 1.389760  [   32/  126]
train() client id: f_00000-1-1 loss: 0.984193  [   64/  126]
train() client id: f_00000-1-2 loss: 1.336507  [   96/  126]
train() client id: f_00000-2-0 loss: 1.109804  [   32/  126]
train() client id: f_00000-2-1 loss: 1.125465  [   64/  126]
train() client id: f_00000-2-2 loss: 1.189769  [   96/  126]
train() client id: f_00000-3-0 loss: 1.170897  [   32/  126]
train() client id: f_00000-3-1 loss: 0.974170  [   64/  126]
train() client id: f_00000-3-2 loss: 1.081907  [   96/  126]
train() client id: f_00000-4-0 loss: 1.127601  [   32/  126]
train() client id: f_00000-4-1 loss: 0.916297  [   64/  126]
train() client id: f_00000-4-2 loss: 1.053642  [   96/  126]
train() client id: f_00000-5-0 loss: 1.001579  [   32/  126]
train() client id: f_00000-5-1 loss: 0.960691  [   64/  126]
train() client id: f_00000-5-2 loss: 1.066571  [   96/  126]
train() client id: f_00000-6-0 loss: 1.128043  [   32/  126]
train() client id: f_00000-6-1 loss: 1.040873  [   64/  126]
train() client id: f_00000-6-2 loss: 0.889087  [   96/  126]
train() client id: f_00000-7-0 loss: 0.947551  [   32/  126]
train() client id: f_00000-7-1 loss: 1.015038  [   64/  126]
train() client id: f_00000-7-2 loss: 1.020950  [   96/  126]
train() client id: f_00000-8-0 loss: 0.912599  [   32/  126]
train() client id: f_00000-8-1 loss: 1.056487  [   64/  126]
train() client id: f_00000-8-2 loss: 0.972364  [   96/  126]
train() client id: f_00000-9-0 loss: 0.949532  [   32/  126]
train() client id: f_00000-9-1 loss: 1.050340  [   64/  126]
train() client id: f_00000-9-2 loss: 0.968942  [   96/  126]
train() client id: f_00000-10-0 loss: 0.995414  [   32/  126]
train() client id: f_00000-10-1 loss: 0.884743  [   64/  126]
train() client id: f_00000-10-2 loss: 0.890655  [   96/  126]
train() client id: f_00000-11-0 loss: 0.929139  [   32/  126]
train() client id: f_00000-11-1 loss: 1.031246  [   64/  126]
train() client id: f_00000-11-2 loss: 0.976433  [   96/  126]
train() client id: f_00001-0-0 loss: 0.508115  [   32/  265]
train() client id: f_00001-0-1 loss: 0.495862  [   64/  265]
train() client id: f_00001-0-2 loss: 0.511273  [   96/  265]
train() client id: f_00001-0-3 loss: 0.416984  [  128/  265]
train() client id: f_00001-0-4 loss: 0.555368  [  160/  265]
train() client id: f_00001-0-5 loss: 0.654390  [  192/  265]
train() client id: f_00001-0-6 loss: 0.472847  [  224/  265]
train() client id: f_00001-0-7 loss: 0.515175  [  256/  265]
train() client id: f_00001-1-0 loss: 0.464518  [   32/  265]
train() client id: f_00001-1-1 loss: 0.433403  [   64/  265]
train() client id: f_00001-1-2 loss: 0.609081  [   96/  265]
train() client id: f_00001-1-3 loss: 0.496887  [  128/  265]
train() client id: f_00001-1-4 loss: 0.535361  [  160/  265]
train() client id: f_00001-1-5 loss: 0.568856  [  192/  265]
train() client id: f_00001-1-6 loss: 0.520339  [  224/  265]
train() client id: f_00001-1-7 loss: 0.474659  [  256/  265]
train() client id: f_00001-2-0 loss: 0.470854  [   32/  265]
train() client id: f_00001-2-1 loss: 0.524460  [   64/  265]
train() client id: f_00001-2-2 loss: 0.524074  [   96/  265]
train() client id: f_00001-2-3 loss: 0.452823  [  128/  265]
train() client id: f_00001-2-4 loss: 0.516935  [  160/  265]
train() client id: f_00001-2-5 loss: 0.491860  [  192/  265]
train() client id: f_00001-2-6 loss: 0.497655  [  224/  265]
train() client id: f_00001-2-7 loss: 0.526105  [  256/  265]
train() client id: f_00001-3-0 loss: 0.481060  [   32/  265]
train() client id: f_00001-3-1 loss: 0.433418  [   64/  265]
train() client id: f_00001-3-2 loss: 0.553899  [   96/  265]
train() client id: f_00001-3-3 loss: 0.432572  [  128/  265]
train() client id: f_00001-3-4 loss: 0.500375  [  160/  265]
train() client id: f_00001-3-5 loss: 0.484563  [  192/  265]
train() client id: f_00001-3-6 loss: 0.413480  [  224/  265]
train() client id: f_00001-3-7 loss: 0.634805  [  256/  265]
train() client id: f_00001-4-0 loss: 0.416418  [   32/  265]
train() client id: f_00001-4-1 loss: 0.538527  [   64/  265]
train() client id: f_00001-4-2 loss: 0.459509  [   96/  265]
train() client id: f_00001-4-3 loss: 0.504113  [  128/  265]
train() client id: f_00001-4-4 loss: 0.461048  [  160/  265]
train() client id: f_00001-4-5 loss: 0.577930  [  192/  265]
train() client id: f_00001-4-6 loss: 0.442107  [  224/  265]
train() client id: f_00001-4-7 loss: 0.584232  [  256/  265]
train() client id: f_00001-5-0 loss: 0.535322  [   32/  265]
train() client id: f_00001-5-1 loss: 0.534252  [   64/  265]
train() client id: f_00001-5-2 loss: 0.470499  [   96/  265]
train() client id: f_00001-5-3 loss: 0.517173  [  128/  265]
train() client id: f_00001-5-4 loss: 0.544586  [  160/  265]
train() client id: f_00001-5-5 loss: 0.428737  [  192/  265]
train() client id: f_00001-5-6 loss: 0.460006  [  224/  265]
train() client id: f_00001-5-7 loss: 0.526018  [  256/  265]
train() client id: f_00001-6-0 loss: 0.629825  [   32/  265]
train() client id: f_00001-6-1 loss: 0.417513  [   64/  265]
train() client id: f_00001-6-2 loss: 0.573977  [   96/  265]
train() client id: f_00001-6-3 loss: 0.555079  [  128/  265]
train() client id: f_00001-6-4 loss: 0.424110  [  160/  265]
train() client id: f_00001-6-5 loss: 0.411757  [  192/  265]
train() client id: f_00001-6-6 loss: 0.543052  [  224/  265]
train() client id: f_00001-6-7 loss: 0.473041  [  256/  265]
train() client id: f_00001-7-0 loss: 0.418397  [   32/  265]
train() client id: f_00001-7-1 loss: 0.471215  [   64/  265]
train() client id: f_00001-7-2 loss: 0.486393  [   96/  265]
train() client id: f_00001-7-3 loss: 0.457613  [  128/  265]
train() client id: f_00001-7-4 loss: 0.540449  [  160/  265]
train() client id: f_00001-7-5 loss: 0.532341  [  192/  265]
train() client id: f_00001-7-6 loss: 0.484223  [  224/  265]
train() client id: f_00001-7-7 loss: 0.512010  [  256/  265]
train() client id: f_00001-8-0 loss: 0.570348  [   32/  265]
train() client id: f_00001-8-1 loss: 0.523266  [   64/  265]
train() client id: f_00001-8-2 loss: 0.473923  [   96/  265]
train() client id: f_00001-8-3 loss: 0.479967  [  128/  265]
train() client id: f_00001-8-4 loss: 0.465684  [  160/  265]
train() client id: f_00001-8-5 loss: 0.546424  [  192/  265]
train() client id: f_00001-8-6 loss: 0.503571  [  224/  265]
train() client id: f_00001-8-7 loss: 0.472093  [  256/  265]
train() client id: f_00001-9-0 loss: 0.563011  [   32/  265]
train() client id: f_00001-9-1 loss: 0.601086  [   64/  265]
train() client id: f_00001-9-2 loss: 0.468072  [   96/  265]
train() client id: f_00001-9-3 loss: 0.580540  [  128/  265]
train() client id: f_00001-9-4 loss: 0.420781  [  160/  265]
train() client id: f_00001-9-5 loss: 0.414788  [  192/  265]
train() client id: f_00001-9-6 loss: 0.402569  [  224/  265]
train() client id: f_00001-9-7 loss: 0.576773  [  256/  265]
train() client id: f_00001-10-0 loss: 0.504641  [   32/  265]
train() client id: f_00001-10-1 loss: 0.445587  [   64/  265]
train() client id: f_00001-10-2 loss: 0.515731  [   96/  265]
train() client id: f_00001-10-3 loss: 0.474423  [  128/  265]
train() client id: f_00001-10-4 loss: 0.653511  [  160/  265]
train() client id: f_00001-10-5 loss: 0.488079  [  192/  265]
train() client id: f_00001-10-6 loss: 0.461671  [  224/  265]
train() client id: f_00001-10-7 loss: 0.494398  [  256/  265]
train() client id: f_00001-11-0 loss: 0.510235  [   32/  265]
train() client id: f_00001-11-1 loss: 0.479745  [   64/  265]
train() client id: f_00001-11-2 loss: 0.529399  [   96/  265]
train() client id: f_00001-11-3 loss: 0.484508  [  128/  265]
train() client id: f_00001-11-4 loss: 0.582637  [  160/  265]
train() client id: f_00001-11-5 loss: 0.466695  [  192/  265]
train() client id: f_00001-11-6 loss: 0.408204  [  224/  265]
train() client id: f_00001-11-7 loss: 0.486137  [  256/  265]
train() client id: f_00002-0-0 loss: 1.268723  [   32/  124]
train() client id: f_00002-0-1 loss: 1.104208  [   64/  124]
train() client id: f_00002-0-2 loss: 1.259879  [   96/  124]
train() client id: f_00002-1-0 loss: 1.248902  [   32/  124]
train() client id: f_00002-1-1 loss: 1.162812  [   64/  124]
train() client id: f_00002-1-2 loss: 1.020698  [   96/  124]
train() client id: f_00002-2-0 loss: 1.182243  [   32/  124]
train() client id: f_00002-2-1 loss: 1.269441  [   64/  124]
train() client id: f_00002-2-2 loss: 1.072405  [   96/  124]
train() client id: f_00002-3-0 loss: 1.064513  [   32/  124]
train() client id: f_00002-3-1 loss: 1.048717  [   64/  124]
train() client id: f_00002-3-2 loss: 1.143790  [   96/  124]
train() client id: f_00002-4-0 loss: 1.154475  [   32/  124]
train() client id: f_00002-4-1 loss: 1.154047  [   64/  124]
train() client id: f_00002-4-2 loss: 1.152573  [   96/  124]
train() client id: f_00002-5-0 loss: 1.254246  [   32/  124]
train() client id: f_00002-5-1 loss: 1.124877  [   64/  124]
train() client id: f_00002-5-2 loss: 0.861907  [   96/  124]
train() client id: f_00002-6-0 loss: 1.033754  [   32/  124]
train() client id: f_00002-6-1 loss: 1.102985  [   64/  124]
train() client id: f_00002-6-2 loss: 1.065342  [   96/  124]
train() client id: f_00002-7-0 loss: 1.000425  [   32/  124]
train() client id: f_00002-7-1 loss: 1.125358  [   64/  124]
train() client id: f_00002-7-2 loss: 1.047756  [   96/  124]
train() client id: f_00002-8-0 loss: 1.234081  [   32/  124]
train() client id: f_00002-8-1 loss: 1.067899  [   64/  124]
train() client id: f_00002-8-2 loss: 0.952184  [   96/  124]
train() client id: f_00002-9-0 loss: 1.313560  [   32/  124]
train() client id: f_00002-9-1 loss: 0.974751  [   64/  124]
train() client id: f_00002-9-2 loss: 1.002125  [   96/  124]
train() client id: f_00002-10-0 loss: 1.070799  [   32/  124]
train() client id: f_00002-10-1 loss: 0.973146  [   64/  124]
train() client id: f_00002-10-2 loss: 1.137594  [   96/  124]
train() client id: f_00002-11-0 loss: 1.109130  [   32/  124]
train() client id: f_00002-11-1 loss: 1.027509  [   64/  124]
train() client id: f_00002-11-2 loss: 1.055865  [   96/  124]
train() client id: f_00003-0-0 loss: 0.661936  [   32/   43]
train() client id: f_00003-1-0 loss: 0.932686  [   32/   43]
train() client id: f_00003-2-0 loss: 0.806220  [   32/   43]
train() client id: f_00003-3-0 loss: 0.699278  [   32/   43]
train() client id: f_00003-4-0 loss: 0.774300  [   32/   43]
train() client id: f_00003-5-0 loss: 0.705970  [   32/   43]
train() client id: f_00003-6-0 loss: 0.773481  [   32/   43]
train() client id: f_00003-7-0 loss: 0.630787  [   32/   43]
train() client id: f_00003-8-0 loss: 0.858435  [   32/   43]
train() client id: f_00003-9-0 loss: 0.922686  [   32/   43]
train() client id: f_00003-10-0 loss: 0.747875  [   32/   43]
train() client id: f_00003-11-0 loss: 0.758832  [   32/   43]
train() client id: f_00004-0-0 loss: 0.606295  [   32/  306]
train() client id: f_00004-0-1 loss: 0.740977  [   64/  306]
train() client id: f_00004-0-2 loss: 0.631165  [   96/  306]
train() client id: f_00004-0-3 loss: 0.755532  [  128/  306]
train() client id: f_00004-0-4 loss: 0.771521  [  160/  306]
train() client id: f_00004-0-5 loss: 0.850210  [  192/  306]
train() client id: f_00004-0-6 loss: 0.816184  [  224/  306]
train() client id: f_00004-0-7 loss: 0.831573  [  256/  306]
train() client id: f_00004-0-8 loss: 0.815645  [  288/  306]
train() client id: f_00004-1-0 loss: 0.784440  [   32/  306]
train() client id: f_00004-1-1 loss: 0.660203  [   64/  306]
train() client id: f_00004-1-2 loss: 0.657361  [   96/  306]
train() client id: f_00004-1-3 loss: 0.804931  [  128/  306]
train() client id: f_00004-1-4 loss: 0.739098  [  160/  306]
train() client id: f_00004-1-5 loss: 0.810006  [  192/  306]
train() client id: f_00004-1-6 loss: 0.728707  [  224/  306]
train() client id: f_00004-1-7 loss: 0.777355  [  256/  306]
train() client id: f_00004-1-8 loss: 0.764832  [  288/  306]
train() client id: f_00004-2-0 loss: 0.861410  [   32/  306]
train() client id: f_00004-2-1 loss: 0.579207  [   64/  306]
train() client id: f_00004-2-2 loss: 0.765453  [   96/  306]
train() client id: f_00004-2-3 loss: 0.800640  [  128/  306]
train() client id: f_00004-2-4 loss: 0.750118  [  160/  306]
train() client id: f_00004-2-5 loss: 0.830671  [  192/  306]
train() client id: f_00004-2-6 loss: 0.738636  [  224/  306]
train() client id: f_00004-2-7 loss: 0.811037  [  256/  306]
train() client id: f_00004-2-8 loss: 0.830023  [  288/  306]
train() client id: f_00004-3-0 loss: 0.769114  [   32/  306]
train() client id: f_00004-3-1 loss: 0.693232  [   64/  306]
train() client id: f_00004-3-2 loss: 0.838684  [   96/  306]
train() client id: f_00004-3-3 loss: 0.853671  [  128/  306]
train() client id: f_00004-3-4 loss: 0.720342  [  160/  306]
train() client id: f_00004-3-5 loss: 0.818385  [  192/  306]
train() client id: f_00004-3-6 loss: 0.854457  [  224/  306]
train() client id: f_00004-3-7 loss: 0.653631  [  256/  306]
train() client id: f_00004-3-8 loss: 0.724906  [  288/  306]
train() client id: f_00004-4-0 loss: 0.732876  [   32/  306]
train() client id: f_00004-4-1 loss: 0.720441  [   64/  306]
train() client id: f_00004-4-2 loss: 0.815201  [   96/  306]
train() client id: f_00004-4-3 loss: 0.750454  [  128/  306]
train() client id: f_00004-4-4 loss: 0.725188  [  160/  306]
train() client id: f_00004-4-5 loss: 0.765667  [  192/  306]
train() client id: f_00004-4-6 loss: 0.834305  [  224/  306]
train() client id: f_00004-4-7 loss: 0.806416  [  256/  306]
train() client id: f_00004-4-8 loss: 0.749041  [  288/  306]
train() client id: f_00004-5-0 loss: 0.656716  [   32/  306]
train() client id: f_00004-5-1 loss: 0.719548  [   64/  306]
train() client id: f_00004-5-2 loss: 0.924200  [   96/  306]
train() client id: f_00004-5-3 loss: 0.830800  [  128/  306]
train() client id: f_00004-5-4 loss: 0.813123  [  160/  306]
train() client id: f_00004-5-5 loss: 0.690563  [  192/  306]
train() client id: f_00004-5-6 loss: 0.749054  [  224/  306]
train() client id: f_00004-5-7 loss: 0.740430  [  256/  306]
train() client id: f_00004-5-8 loss: 0.854882  [  288/  306]
train() client id: f_00004-6-0 loss: 0.828067  [   32/  306]
train() client id: f_00004-6-1 loss: 0.595971  [   64/  306]
train() client id: f_00004-6-2 loss: 0.848181  [   96/  306]
train() client id: f_00004-6-3 loss: 0.833696  [  128/  306]
train() client id: f_00004-6-4 loss: 0.766098  [  160/  306]
train() client id: f_00004-6-5 loss: 0.775636  [  192/  306]
train() client id: f_00004-6-6 loss: 0.805111  [  224/  306]
train() client id: f_00004-6-7 loss: 0.686719  [  256/  306]
train() client id: f_00004-6-8 loss: 0.775305  [  288/  306]
train() client id: f_00004-7-0 loss: 0.775772  [   32/  306]
train() client id: f_00004-7-1 loss: 0.693200  [   64/  306]
train() client id: f_00004-7-2 loss: 0.681511  [   96/  306]
train() client id: f_00004-7-3 loss: 0.837616  [  128/  306]
train() client id: f_00004-7-4 loss: 0.777050  [  160/  306]
train() client id: f_00004-7-5 loss: 0.655370  [  192/  306]
train() client id: f_00004-7-6 loss: 0.801913  [  224/  306]
train() client id: f_00004-7-7 loss: 0.939612  [  256/  306]
train() client id: f_00004-7-8 loss: 0.836143  [  288/  306]
train() client id: f_00004-8-0 loss: 0.784231  [   32/  306]
train() client id: f_00004-8-1 loss: 0.817213  [   64/  306]
train() client id: f_00004-8-2 loss: 0.667165  [   96/  306]
train() client id: f_00004-8-3 loss: 0.852309  [  128/  306]
train() client id: f_00004-8-4 loss: 0.795751  [  160/  306]
train() client id: f_00004-8-5 loss: 0.699354  [  192/  306]
train() client id: f_00004-8-6 loss: 0.720914  [  224/  306]
train() client id: f_00004-8-7 loss: 0.900180  [  256/  306]
train() client id: f_00004-8-8 loss: 0.751290  [  288/  306]
train() client id: f_00004-9-0 loss: 0.791790  [   32/  306]
train() client id: f_00004-9-1 loss: 0.742879  [   64/  306]
train() client id: f_00004-9-2 loss: 0.870669  [   96/  306]
train() client id: f_00004-9-3 loss: 0.829973  [  128/  306]
train() client id: f_00004-9-4 loss: 0.761724  [  160/  306]
train() client id: f_00004-9-5 loss: 0.732089  [  192/  306]
train() client id: f_00004-9-6 loss: 0.776802  [  224/  306]
train() client id: f_00004-9-7 loss: 0.809095  [  256/  306]
train() client id: f_00004-9-8 loss: 0.847270  [  288/  306]
train() client id: f_00004-10-0 loss: 0.807977  [   32/  306]
train() client id: f_00004-10-1 loss: 0.767683  [   64/  306]
train() client id: f_00004-10-2 loss: 0.776294  [   96/  306]
train() client id: f_00004-10-3 loss: 0.870894  [  128/  306]
train() client id: f_00004-10-4 loss: 0.722755  [  160/  306]
train() client id: f_00004-10-5 loss: 0.812531  [  192/  306]
train() client id: f_00004-10-6 loss: 0.810317  [  224/  306]
train() client id: f_00004-10-7 loss: 0.752035  [  256/  306]
train() client id: f_00004-10-8 loss: 0.771789  [  288/  306]
train() client id: f_00004-11-0 loss: 0.738380  [   32/  306]
train() client id: f_00004-11-1 loss: 0.743623  [   64/  306]
train() client id: f_00004-11-2 loss: 0.879190  [   96/  306]
train() client id: f_00004-11-3 loss: 0.867886  [  128/  306]
train() client id: f_00004-11-4 loss: 0.826515  [  160/  306]
train() client id: f_00004-11-5 loss: 0.806034  [  192/  306]
train() client id: f_00004-11-6 loss: 0.788191  [  224/  306]
train() client id: f_00004-11-7 loss: 0.644945  [  256/  306]
train() client id: f_00004-11-8 loss: 0.789994  [  288/  306]
train() client id: f_00005-0-0 loss: 0.825183  [   32/  146]
train() client id: f_00005-0-1 loss: 0.524284  [   64/  146]
train() client id: f_00005-0-2 loss: 0.799190  [   96/  146]
train() client id: f_00005-0-3 loss: 0.421000  [  128/  146]
train() client id: f_00005-1-0 loss: 0.365921  [   32/  146]
train() client id: f_00005-1-1 loss: 0.876205  [   64/  146]
train() client id: f_00005-1-2 loss: 0.588317  [   96/  146]
train() client id: f_00005-1-3 loss: 0.673771  [  128/  146]
train() client id: f_00005-2-0 loss: 0.762889  [   32/  146]
train() client id: f_00005-2-1 loss: 0.441438  [   64/  146]
train() client id: f_00005-2-2 loss: 0.609880  [   96/  146]
train() client id: f_00005-2-3 loss: 0.663827  [  128/  146]
train() client id: f_00005-3-0 loss: 0.731299  [   32/  146]
train() client id: f_00005-3-1 loss: 0.710240  [   64/  146]
train() client id: f_00005-3-2 loss: 0.622901  [   96/  146]
train() client id: f_00005-3-3 loss: 0.312759  [  128/  146]
train() client id: f_00005-4-0 loss: 0.616637  [   32/  146]
train() client id: f_00005-4-1 loss: 0.638315  [   64/  146]
train() client id: f_00005-4-2 loss: 0.477270  [   96/  146]
train() client id: f_00005-4-3 loss: 0.583735  [  128/  146]
train() client id: f_00005-5-0 loss: 0.726870  [   32/  146]
train() client id: f_00005-5-1 loss: 0.446442  [   64/  146]
train() client id: f_00005-5-2 loss: 0.822026  [   96/  146]
train() client id: f_00005-5-3 loss: 0.395488  [  128/  146]
train() client id: f_00005-6-0 loss: 0.493727  [   32/  146]
train() client id: f_00005-6-1 loss: 0.718635  [   64/  146]
train() client id: f_00005-6-2 loss: 0.562175  [   96/  146]
train() client id: f_00005-6-3 loss: 0.699878  [  128/  146]
train() client id: f_00005-7-0 loss: 0.514952  [   32/  146]
train() client id: f_00005-7-1 loss: 0.696615  [   64/  146]
train() client id: f_00005-7-2 loss: 0.595308  [   96/  146]
train() client id: f_00005-7-3 loss: 0.471688  [  128/  146]
train() client id: f_00005-8-0 loss: 0.627993  [   32/  146]
train() client id: f_00005-8-1 loss: 0.467636  [   64/  146]
train() client id: f_00005-8-2 loss: 0.646010  [   96/  146]
train() client id: f_00005-8-3 loss: 0.671888  [  128/  146]
train() client id: f_00005-9-0 loss: 0.513177  [   32/  146]
train() client id: f_00005-9-1 loss: 0.430761  [   64/  146]
train() client id: f_00005-9-2 loss: 0.748607  [   96/  146]
train() client id: f_00005-9-3 loss: 0.661856  [  128/  146]
train() client id: f_00005-10-0 loss: 0.431018  [   32/  146]
train() client id: f_00005-10-1 loss: 0.841152  [   64/  146]
train() client id: f_00005-10-2 loss: 0.360770  [   96/  146]
train() client id: f_00005-10-3 loss: 0.632087  [  128/  146]
train() client id: f_00005-11-0 loss: 0.525186  [   32/  146]
train() client id: f_00005-11-1 loss: 0.508666  [   64/  146]
train() client id: f_00005-11-2 loss: 0.618711  [   96/  146]
train() client id: f_00005-11-3 loss: 0.740987  [  128/  146]
train() client id: f_00006-0-0 loss: 0.448105  [   32/   54]
train() client id: f_00006-1-0 loss: 0.525317  [   32/   54]
train() client id: f_00006-2-0 loss: 0.535200  [   32/   54]
train() client id: f_00006-3-0 loss: 0.535299  [   32/   54]
train() client id: f_00006-4-0 loss: 0.480928  [   32/   54]
train() client id: f_00006-5-0 loss: 0.542938  [   32/   54]
train() client id: f_00006-6-0 loss: 0.544053  [   32/   54]
train() client id: f_00006-7-0 loss: 0.486725  [   32/   54]
train() client id: f_00006-8-0 loss: 0.520153  [   32/   54]
train() client id: f_00006-9-0 loss: 0.512071  [   32/   54]
train() client id: f_00006-10-0 loss: 0.533973  [   32/   54]
train() client id: f_00006-11-0 loss: 0.487331  [   32/   54]
train() client id: f_00007-0-0 loss: 0.423907  [   32/  179]
train() client id: f_00007-0-1 loss: 0.648311  [   64/  179]
train() client id: f_00007-0-2 loss: 0.675338  [   96/  179]
train() client id: f_00007-0-3 loss: 0.568075  [  128/  179]
train() client id: f_00007-0-4 loss: 0.772149  [  160/  179]
train() client id: f_00007-1-0 loss: 0.741090  [   32/  179]
train() client id: f_00007-1-1 loss: 0.521409  [   64/  179]
train() client id: f_00007-1-2 loss: 0.478158  [   96/  179]
train() client id: f_00007-1-3 loss: 0.546083  [  128/  179]
train() client id: f_00007-1-4 loss: 0.623706  [  160/  179]
train() client id: f_00007-2-0 loss: 0.485035  [   32/  179]
train() client id: f_00007-2-1 loss: 0.623634  [   64/  179]
train() client id: f_00007-2-2 loss: 0.619570  [   96/  179]
train() client id: f_00007-2-3 loss: 0.550778  [  128/  179]
train() client id: f_00007-2-4 loss: 0.759047  [  160/  179]
train() client id: f_00007-3-0 loss: 0.703854  [   32/  179]
train() client id: f_00007-3-1 loss: 0.689207  [   64/  179]
train() client id: f_00007-3-2 loss: 0.486302  [   96/  179]
train() client id: f_00007-3-3 loss: 0.603206  [  128/  179]
train() client id: f_00007-3-4 loss: 0.482331  [  160/  179]
train() client id: f_00007-4-0 loss: 0.762201  [   32/  179]
train() client id: f_00007-4-1 loss: 0.571179  [   64/  179]
train() client id: f_00007-4-2 loss: 0.569974  [   96/  179]
train() client id: f_00007-4-3 loss: 0.565281  [  128/  179]
train() client id: f_00007-4-4 loss: 0.546439  [  160/  179]
train() client id: f_00007-5-0 loss: 0.624962  [   32/  179]
train() client id: f_00007-5-1 loss: 0.473759  [   64/  179]
train() client id: f_00007-5-2 loss: 0.785683  [   96/  179]
train() client id: f_00007-5-3 loss: 0.499071  [  128/  179]
train() client id: f_00007-5-4 loss: 0.552560  [  160/  179]
train() client id: f_00007-6-0 loss: 0.455294  [   32/  179]
train() client id: f_00007-6-1 loss: 0.507429  [   64/  179]
train() client id: f_00007-6-2 loss: 0.652666  [   96/  179]
train() client id: f_00007-6-3 loss: 0.497423  [  128/  179]
train() client id: f_00007-6-4 loss: 0.498567  [  160/  179]
train() client id: f_00007-7-0 loss: 0.577268  [   32/  179]
train() client id: f_00007-7-1 loss: 0.767200  [   64/  179]
train() client id: f_00007-7-2 loss: 0.627619  [   96/  179]
train() client id: f_00007-7-3 loss: 0.517265  [  128/  179]
train() client id: f_00007-7-4 loss: 0.501388  [  160/  179]
train() client id: f_00007-8-0 loss: 0.517724  [   32/  179]
train() client id: f_00007-8-1 loss: 0.545677  [   64/  179]
train() client id: f_00007-8-2 loss: 0.516997  [   96/  179]
train() client id: f_00007-8-3 loss: 0.715441  [  128/  179]
train() client id: f_00007-8-4 loss: 0.544501  [  160/  179]
train() client id: f_00007-9-0 loss: 0.462614  [   32/  179]
train() client id: f_00007-9-1 loss: 0.534086  [   64/  179]
train() client id: f_00007-9-2 loss: 0.808276  [   96/  179]
train() client id: f_00007-9-3 loss: 0.561988  [  128/  179]
train() client id: f_00007-9-4 loss: 0.509520  [  160/  179]
train() client id: f_00007-10-0 loss: 0.540875  [   32/  179]
train() client id: f_00007-10-1 loss: 0.515263  [   64/  179]
train() client id: f_00007-10-2 loss: 0.804933  [   96/  179]
train() client id: f_00007-10-3 loss: 0.555982  [  128/  179]
train() client id: f_00007-10-4 loss: 0.596668  [  160/  179]
train() client id: f_00007-11-0 loss: 0.784951  [   32/  179]
train() client id: f_00007-11-1 loss: 0.493661  [   64/  179]
train() client id: f_00007-11-2 loss: 0.608939  [   96/  179]
train() client id: f_00007-11-3 loss: 0.437620  [  128/  179]
train() client id: f_00007-11-4 loss: 0.542331  [  160/  179]
train() client id: f_00008-0-0 loss: 0.622497  [   32/  130]
train() client id: f_00008-0-1 loss: 0.779886  [   64/  130]
train() client id: f_00008-0-2 loss: 0.738647  [   96/  130]
train() client id: f_00008-0-3 loss: 0.840375  [  128/  130]
train() client id: f_00008-1-0 loss: 0.681371  [   32/  130]
train() client id: f_00008-1-1 loss: 0.705757  [   64/  130]
train() client id: f_00008-1-2 loss: 0.806522  [   96/  130]
train() client id: f_00008-1-3 loss: 0.818787  [  128/  130]
train() client id: f_00008-2-0 loss: 0.816825  [   32/  130]
train() client id: f_00008-2-1 loss: 0.714258  [   64/  130]
train() client id: f_00008-2-2 loss: 0.776557  [   96/  130]
train() client id: f_00008-2-3 loss: 0.672022  [  128/  130]
train() client id: f_00008-3-0 loss: 0.687067  [   32/  130]
train() client id: f_00008-3-1 loss: 0.798987  [   64/  130]
train() client id: f_00008-3-2 loss: 0.768112  [   96/  130]
train() client id: f_00008-3-3 loss: 0.739868  [  128/  130]
train() client id: f_00008-4-0 loss: 0.786653  [   32/  130]
train() client id: f_00008-4-1 loss: 0.776658  [   64/  130]
train() client id: f_00008-4-2 loss: 0.724252  [   96/  130]
train() client id: f_00008-4-3 loss: 0.716521  [  128/  130]
train() client id: f_00008-5-0 loss: 0.815825  [   32/  130]
train() client id: f_00008-5-1 loss: 0.792432  [   64/  130]
train() client id: f_00008-5-2 loss: 0.683514  [   96/  130]
train() client id: f_00008-5-3 loss: 0.717850  [  128/  130]
train() client id: f_00008-6-0 loss: 0.667831  [   32/  130]
train() client id: f_00008-6-1 loss: 0.742807  [   64/  130]
train() client id: f_00008-6-2 loss: 0.797442  [   96/  130]
train() client id: f_00008-6-3 loss: 0.786375  [  128/  130]
train() client id: f_00008-7-0 loss: 0.835634  [   32/  130]
train() client id: f_00008-7-1 loss: 0.696786  [   64/  130]
train() client id: f_00008-7-2 loss: 0.759441  [   96/  130]
train() client id: f_00008-7-3 loss: 0.697150  [  128/  130]
train() client id: f_00008-8-0 loss: 0.748496  [   32/  130]
train() client id: f_00008-8-1 loss: 0.744382  [   64/  130]
train() client id: f_00008-8-2 loss: 0.728092  [   96/  130]
train() client id: f_00008-8-3 loss: 0.781685  [  128/  130]
train() client id: f_00008-9-0 loss: 0.697089  [   32/  130]
train() client id: f_00008-9-1 loss: 0.717217  [   64/  130]
train() client id: f_00008-9-2 loss: 0.760684  [   96/  130]
train() client id: f_00008-9-3 loss: 0.811100  [  128/  130]
train() client id: f_00008-10-0 loss: 0.861780  [   32/  130]
train() client id: f_00008-10-1 loss: 0.789994  [   64/  130]
train() client id: f_00008-10-2 loss: 0.594012  [   96/  130]
train() client id: f_00008-10-3 loss: 0.707888  [  128/  130]
train() client id: f_00008-11-0 loss: 0.873870  [   32/  130]
train() client id: f_00008-11-1 loss: 0.646256  [   64/  130]
train() client id: f_00008-11-2 loss: 0.747991  [   96/  130]
train() client id: f_00008-11-3 loss: 0.708994  [  128/  130]
train() client id: f_00009-0-0 loss: 1.112159  [   32/  118]
train() client id: f_00009-0-1 loss: 1.090761  [   64/  118]
train() client id: f_00009-0-2 loss: 1.030117  [   96/  118]
train() client id: f_00009-1-0 loss: 1.199573  [   32/  118]
train() client id: f_00009-1-1 loss: 0.890862  [   64/  118]
train() client id: f_00009-1-2 loss: 0.987877  [   96/  118]
train() client id: f_00009-2-0 loss: 1.100001  [   32/  118]
train() client id: f_00009-2-1 loss: 1.034923  [   64/  118]
train() client id: f_00009-2-2 loss: 0.843027  [   96/  118]
train() client id: f_00009-3-0 loss: 0.984370  [   32/  118]
train() client id: f_00009-3-1 loss: 1.101787  [   64/  118]
train() client id: f_00009-3-2 loss: 0.775962  [   96/  118]
train() client id: f_00009-4-0 loss: 1.019986  [   32/  118]
train() client id: f_00009-4-1 loss: 0.880134  [   64/  118]
train() client id: f_00009-4-2 loss: 0.868063  [   96/  118]
train() client id: f_00009-5-0 loss: 0.863385  [   32/  118]
train() client id: f_00009-5-1 loss: 0.875238  [   64/  118]
train() client id: f_00009-5-2 loss: 1.149150  [   96/  118]
train() client id: f_00009-6-0 loss: 1.037135  [   32/  118]
train() client id: f_00009-6-1 loss: 0.786839  [   64/  118]
train() client id: f_00009-6-2 loss: 0.886728  [   96/  118]
train() client id: f_00009-7-0 loss: 0.997882  [   32/  118]
train() client id: f_00009-7-1 loss: 0.941384  [   64/  118]
train() client id: f_00009-7-2 loss: 0.807215  [   96/  118]
train() client id: f_00009-8-0 loss: 0.843498  [   32/  118]
train() client id: f_00009-8-1 loss: 0.844795  [   64/  118]
train() client id: f_00009-8-2 loss: 0.939778  [   96/  118]
train() client id: f_00009-9-0 loss: 0.849799  [   32/  118]
train() client id: f_00009-9-1 loss: 1.014293  [   64/  118]
train() client id: f_00009-9-2 loss: 0.793663  [   96/  118]
train() client id: f_00009-10-0 loss: 0.905765  [   32/  118]
train() client id: f_00009-10-1 loss: 0.931980  [   64/  118]
train() client id: f_00009-10-2 loss: 0.881312  [   96/  118]
train() client id: f_00009-11-0 loss: 0.913937  [   32/  118]
train() client id: f_00009-11-1 loss: 0.899159  [   64/  118]
train() client id: f_00009-11-2 loss: 0.808215  [   96/  118]
At round 31 accuracy: 0.6551724137931034
At round 31 training accuracy: 0.5861837692823608
At round 31 training loss: 0.8260387431788525
update_location
xs = [  -3.9056584     4.20031788  175.00902392   18.81129433    0.97929623
    3.95640986 -137.44319194 -116.32485185  159.66397685 -102.06087855]
ys = [ 167.5879595   150.55583871    1.32061395 -137.45517586  129.35018685
  112.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [195.19471903 180.78911261 201.56860488 171.01985313 163.50055002
 150.80678296 169.99271029 153.40191463 189.21219695 142.94206794]
dists_bs = [171.2548162  180.85981021 390.7311684  367.68336205 181.80379023
 189.45309836 181.55205558 183.79506923 369.84387958 185.90483768]
uav_gains = [1.82789532e-11 2.25026314e-11 1.66686654e-11 2.59975805e-11
 2.91614959e-11 3.57650883e-11 2.64028374e-11 3.42619275e-11
 1.99218930e-11 4.09129213e-11]
bs_gains = [6.15310409e-11 5.28124466e-11 6.10992006e-12 7.24380826e-12
 5.20482189e-11 4.63755733e-11 5.22505430e-11 5.04846428e-11
 7.12594497e-12 4.88967675e-11]
Round 32
-------------------------------
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.76786425 14.02640477  6.66672465  2.4004724  16.17773155  7.78673034
  2.97703515  9.52653575  7.02653207  6.31639524]
obj_prev = 79.67242616820144
eta_min = 2.1062834512478834e-14	eta_max = 0.9270669874026624
af = 16.815721006576958	bf = 1.436444871137545	zeta = 18.497293107234654	eta = 0.9090909090909091
af = 16.815721006576958	bf = 1.436444871137545	zeta = 33.430696901951336	eta = 0.5030024069164837
af = 16.815721006576958	bf = 1.436444871137545	zeta = 26.140048449463926	eta = 0.643293413900379
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.823688418099213	eta = 0.677406222772134
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755287540536187	eta = 0.6792779513888344
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755088130913464	eta = 0.6792834231754543
eta = 0.6792834231754543
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.03191095 0.06711429 0.03140442 0.01089024 0.07749801 0.03697618
 0.01367612 0.0453338  0.03292399 0.02988485]
ene_total = [2.19996663 3.98610054 2.18445289 1.03108082 4.54479709 2.37816282
 1.17905352 2.85331091 2.40505659 1.99310631]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 0 obj = 4.495842450611667
eta = 0.6792834231754543
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164858 15329090.24416601 50275192.55652302
 40127838.49551824 32727809.18735842]
eta_min = 0.6792834231754692	eta_max = 0.6792834231754555
af = 0.014083508139639458	bf = 1.436444871137545	zeta = 0.015491858953603405	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [2.60553049e-06 2.22287345e-05 2.50826025e-06 1.00113487e-07
 3.42564517e-05 3.74888670e-06 1.98007609e-07 7.06018162e-06
 3.26655702e-06 1.97229209e-06]
ene_total = [1.71674703 1.35307722 1.75794987 1.57458483 1.35935733 1.38609978
 1.56889819 1.47978715 2.24850168 1.37065582]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 1 obj = 4.495842450611684
eta = 0.6792834231754555
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164856 15329090.24416601 50275192.55652302
 40127838.49551825 32727809.18735842]
Done!
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.00182543e-05 8.54693954e-05 9.64425066e-06 3.84935959e-07
 1.31715920e-04 1.44144544e-05 7.61338465e-07 2.71463702e-05
 1.25598987e-05 7.58345522e-06]
ene_total = [0.00906999 0.00720603 0.00928722 0.0083124  0.00727341 0.00732778
 0.00828266 0.00783177 0.01187897 0.0072412 ]
At round 32 energy consumption: 0.08371143671629988
At round 32 eta: 0.6792834231754555
At round 32 a_n: 17.221135753236222
At round 32 local rounds: 12.663073636613152
At round 32 global rounds: 53.69580806749958
gradient difference: 0.404219388961792
train() client id: f_00000-0-0 loss: 1.129428  [   32/  126]
train() client id: f_00000-0-1 loss: 1.252080  [   64/  126]
train() client id: f_00000-0-2 loss: 1.269971  [   96/  126]
train() client id: f_00000-1-0 loss: 1.252685  [   32/  126]
train() client id: f_00000-1-1 loss: 1.120768  [   64/  126]
train() client id: f_00000-1-2 loss: 1.011669  [   96/  126]
train() client id: f_00000-2-0 loss: 1.243370  [   32/  126]
train() client id: f_00000-2-1 loss: 1.065679  [   64/  126]
train() client id: f_00000-2-2 loss: 0.932883  [   96/  126]
train() client id: f_00000-3-0 loss: 0.990479  [   32/  126]
train() client id: f_00000-3-1 loss: 1.096961  [   64/  126]
train() client id: f_00000-3-2 loss: 0.960128  [   96/  126]
train() client id: f_00000-4-0 loss: 0.988213  [   32/  126]
train() client id: f_00000-4-1 loss: 0.971631  [   64/  126]
train() client id: f_00000-4-2 loss: 0.916047  [   96/  126]
train() client id: f_00000-5-0 loss: 0.931226  [   32/  126]
train() client id: f_00000-5-1 loss: 0.865783  [   64/  126]
train() client id: f_00000-5-2 loss: 1.005195  [   96/  126]
train() client id: f_00000-6-0 loss: 0.790388  [   32/  126]
train() client id: f_00000-6-1 loss: 0.874628  [   64/  126]
train() client id: f_00000-6-2 loss: 1.097805  [   96/  126]
train() client id: f_00000-7-0 loss: 0.887871  [   32/  126]
train() client id: f_00000-7-1 loss: 0.861333  [   64/  126]
train() client id: f_00000-7-2 loss: 0.877898  [   96/  126]
train() client id: f_00000-8-0 loss: 0.911946  [   32/  126]
train() client id: f_00000-8-1 loss: 0.875150  [   64/  126]
train() client id: f_00000-8-2 loss: 0.886013  [   96/  126]
train() client id: f_00000-9-0 loss: 0.905990  [   32/  126]
train() client id: f_00000-9-1 loss: 0.840602  [   64/  126]
train() client id: f_00000-9-2 loss: 0.908650  [   96/  126]
train() client id: f_00000-10-0 loss: 0.918204  [   32/  126]
train() client id: f_00000-10-1 loss: 0.848744  [   64/  126]
train() client id: f_00000-10-2 loss: 0.793511  [   96/  126]
train() client id: f_00000-11-0 loss: 0.864072  [   32/  126]
train() client id: f_00000-11-1 loss: 0.842314  [   64/  126]
train() client id: f_00000-11-2 loss: 0.958835  [   96/  126]
train() client id: f_00001-0-0 loss: 0.405108  [   32/  265]
train() client id: f_00001-0-1 loss: 0.628632  [   64/  265]
train() client id: f_00001-0-2 loss: 0.404757  [   96/  265]
train() client id: f_00001-0-3 loss: 0.474413  [  128/  265]
train() client id: f_00001-0-4 loss: 0.604272  [  160/  265]
train() client id: f_00001-0-5 loss: 0.468901  [  192/  265]
train() client id: f_00001-0-6 loss: 0.411056  [  224/  265]
train() client id: f_00001-0-7 loss: 0.504824  [  256/  265]
train() client id: f_00001-1-0 loss: 0.675465  [   32/  265]
train() client id: f_00001-1-1 loss: 0.431813  [   64/  265]
train() client id: f_00001-1-2 loss: 0.393127  [   96/  265]
train() client id: f_00001-1-3 loss: 0.435353  [  128/  265]
train() client id: f_00001-1-4 loss: 0.441869  [  160/  265]
train() client id: f_00001-1-5 loss: 0.468302  [  192/  265]
train() client id: f_00001-1-6 loss: 0.560012  [  224/  265]
train() client id: f_00001-1-7 loss: 0.390126  [  256/  265]
train() client id: f_00001-2-0 loss: 0.485678  [   32/  265]
train() client id: f_00001-2-1 loss: 0.443894  [   64/  265]
train() client id: f_00001-2-2 loss: 0.540676  [   96/  265]
train() client id: f_00001-2-3 loss: 0.615913  [  128/  265]
train() client id: f_00001-2-4 loss: 0.452879  [  160/  265]
train() client id: f_00001-2-5 loss: 0.367721  [  192/  265]
train() client id: f_00001-2-6 loss: 0.523852  [  224/  265]
train() client id: f_00001-2-7 loss: 0.375311  [  256/  265]
train() client id: f_00001-3-0 loss: 0.607865  [   32/  265]
train() client id: f_00001-3-1 loss: 0.369221  [   64/  265]
train() client id: f_00001-3-2 loss: 0.518658  [   96/  265]
train() client id: f_00001-3-3 loss: 0.502272  [  128/  265]
train() client id: f_00001-3-4 loss: 0.450796  [  160/  265]
train() client id: f_00001-3-5 loss: 0.402071  [  192/  265]
train() client id: f_00001-3-6 loss: 0.453755  [  224/  265]
train() client id: f_00001-3-7 loss: 0.418729  [  256/  265]
train() client id: f_00001-4-0 loss: 0.450006  [   32/  265]
train() client id: f_00001-4-1 loss: 0.534674  [   64/  265]
train() client id: f_00001-4-2 loss: 0.473785  [   96/  265]
train() client id: f_00001-4-3 loss: 0.443788  [  128/  265]
train() client id: f_00001-4-4 loss: 0.518873  [  160/  265]
train() client id: f_00001-4-5 loss: 0.511525  [  192/  265]
train() client id: f_00001-4-6 loss: 0.436376  [  224/  265]
train() client id: f_00001-4-7 loss: 0.357688  [  256/  265]
train() client id: f_00001-5-0 loss: 0.502505  [   32/  265]
train() client id: f_00001-5-1 loss: 0.484473  [   64/  265]
train() client id: f_00001-5-2 loss: 0.479924  [   96/  265]
train() client id: f_00001-5-3 loss: 0.446276  [  128/  265]
train() client id: f_00001-5-4 loss: 0.399404  [  160/  265]
train() client id: f_00001-5-5 loss: 0.490139  [  192/  265]
train() client id: f_00001-5-6 loss: 0.457437  [  224/  265]
train() client id: f_00001-5-7 loss: 0.441845  [  256/  265]
train() client id: f_00001-6-0 loss: 0.495313  [   32/  265]
train() client id: f_00001-6-1 loss: 0.452833  [   64/  265]
train() client id: f_00001-6-2 loss: 0.503701  [   96/  265]
train() client id: f_00001-6-3 loss: 0.526845  [  128/  265]
train() client id: f_00001-6-4 loss: 0.417074  [  160/  265]
train() client id: f_00001-6-5 loss: 0.418464  [  192/  265]
train() client id: f_00001-6-6 loss: 0.369913  [  224/  265]
train() client id: f_00001-6-7 loss: 0.493141  [  256/  265]
train() client id: f_00001-7-0 loss: 0.393126  [   32/  265]
train() client id: f_00001-7-1 loss: 0.464313  [   64/  265]
train() client id: f_00001-7-2 loss: 0.362939  [   96/  265]
train() client id: f_00001-7-3 loss: 0.421223  [  128/  265]
train() client id: f_00001-7-4 loss: 0.564582  [  160/  265]
train() client id: f_00001-7-5 loss: 0.552219  [  192/  265]
train() client id: f_00001-7-6 loss: 0.512626  [  224/  265]
train() client id: f_00001-7-7 loss: 0.415905  [  256/  265]
train() client id: f_00001-8-0 loss: 0.491778  [   32/  265]
train() client id: f_00001-8-1 loss: 0.568945  [   64/  265]
train() client id: f_00001-8-2 loss: 0.518644  [   96/  265]
train() client id: f_00001-8-3 loss: 0.496548  [  128/  265]
train() client id: f_00001-8-4 loss: 0.349105  [  160/  265]
train() client id: f_00001-8-5 loss: 0.405086  [  192/  265]
train() client id: f_00001-8-6 loss: 0.479435  [  224/  265]
train() client id: f_00001-8-7 loss: 0.360281  [  256/  265]
train() client id: f_00001-9-0 loss: 0.377142  [   32/  265]
train() client id: f_00001-9-1 loss: 0.485576  [   64/  265]
train() client id: f_00001-9-2 loss: 0.495735  [   96/  265]
train() client id: f_00001-9-3 loss: 0.403077  [  128/  265]
train() client id: f_00001-9-4 loss: 0.587043  [  160/  265]
train() client id: f_00001-9-5 loss: 0.530289  [  192/  265]
train() client id: f_00001-9-6 loss: 0.379521  [  224/  265]
train() client id: f_00001-9-7 loss: 0.347657  [  256/  265]
train() client id: f_00001-10-0 loss: 0.592071  [   32/  265]
train() client id: f_00001-10-1 loss: 0.450396  [   64/  265]
train() client id: f_00001-10-2 loss: 0.518821  [   96/  265]
train() client id: f_00001-10-3 loss: 0.382219  [  128/  265]
train() client id: f_00001-10-4 loss: 0.377643  [  160/  265]
train() client id: f_00001-10-5 loss: 0.511594  [  192/  265]
train() client id: f_00001-10-6 loss: 0.417740  [  224/  265]
train() client id: f_00001-10-7 loss: 0.355618  [  256/  265]
train() client id: f_00001-11-0 loss: 0.471179  [   32/  265]
train() client id: f_00001-11-1 loss: 0.579479  [   64/  265]
train() client id: f_00001-11-2 loss: 0.436192  [   96/  265]
train() client id: f_00001-11-3 loss: 0.419440  [  128/  265]
train() client id: f_00001-11-4 loss: 0.483141  [  160/  265]
train() client id: f_00001-11-5 loss: 0.398892  [  192/  265]
train() client id: f_00001-11-6 loss: 0.444212  [  224/  265]
train() client id: f_00001-11-7 loss: 0.439578  [  256/  265]
train() client id: f_00002-0-0 loss: 1.224247  [   32/  124]
train() client id: f_00002-0-1 loss: 1.260748  [   64/  124]
train() client id: f_00002-0-2 loss: 1.215921  [   96/  124]
train() client id: f_00002-1-0 loss: 1.198019  [   32/  124]
train() client id: f_00002-1-1 loss: 1.137157  [   64/  124]
train() client id: f_00002-1-2 loss: 1.271444  [   96/  124]
train() client id: f_00002-2-0 loss: 1.056906  [   32/  124]
train() client id: f_00002-2-1 loss: 1.184284  [   64/  124]
train() client id: f_00002-2-2 loss: 0.956562  [   96/  124]
train() client id: f_00002-3-0 loss: 1.161407  [   32/  124]
train() client id: f_00002-3-1 loss: 1.024432  [   64/  124]
train() client id: f_00002-3-2 loss: 1.206767  [   96/  124]
train() client id: f_00002-4-0 loss: 1.028990  [   32/  124]
train() client id: f_00002-4-1 loss: 1.254172  [   64/  124]
train() client id: f_00002-4-2 loss: 1.125236  [   96/  124]
train() client id: f_00002-5-0 loss: 0.932323  [   32/  124]
train() client id: f_00002-5-1 loss: 0.960311  [   64/  124]
train() client id: f_00002-5-2 loss: 1.107388  [   96/  124]
train() client id: f_00002-6-0 loss: 0.917221  [   32/  124]
train() client id: f_00002-6-1 loss: 1.076786  [   64/  124]
train() client id: f_00002-6-2 loss: 1.018342  [   96/  124]
train() client id: f_00002-7-0 loss: 1.244731  [   32/  124]
train() client id: f_00002-7-1 loss: 0.931673  [   64/  124]
train() client id: f_00002-7-2 loss: 0.945451  [   96/  124]
train() client id: f_00002-8-0 loss: 0.924163  [   32/  124]
train() client id: f_00002-8-1 loss: 1.113503  [   64/  124]
train() client id: f_00002-8-2 loss: 1.126715  [   96/  124]
train() client id: f_00002-9-0 loss: 1.048366  [   32/  124]
train() client id: f_00002-9-1 loss: 0.906772  [   64/  124]
train() client id: f_00002-9-2 loss: 0.881416  [   96/  124]
train() client id: f_00002-10-0 loss: 0.980402  [   32/  124]
train() client id: f_00002-10-1 loss: 0.995780  [   64/  124]
train() client id: f_00002-10-2 loss: 1.018821  [   96/  124]
train() client id: f_00002-11-0 loss: 1.151945  [   32/  124]
train() client id: f_00002-11-1 loss: 0.826497  [   64/  124]
train() client id: f_00002-11-2 loss: 1.033420  [   96/  124]
train() client id: f_00003-0-0 loss: 0.783723  [   32/   43]
train() client id: f_00003-1-0 loss: 0.618813  [   32/   43]
train() client id: f_00003-2-0 loss: 0.575584  [   32/   43]
train() client id: f_00003-3-0 loss: 0.671014  [   32/   43]
train() client id: f_00003-4-0 loss: 0.617657  [   32/   43]
train() client id: f_00003-5-0 loss: 0.861582  [   32/   43]
train() client id: f_00003-6-0 loss: 0.671505  [   32/   43]
train() client id: f_00003-7-0 loss: 0.713353  [   32/   43]
train() client id: f_00003-8-0 loss: 0.579252  [   32/   43]
train() client id: f_00003-9-0 loss: 0.786831  [   32/   43]
train() client id: f_00003-10-0 loss: 0.509664  [   32/   43]
train() client id: f_00003-11-0 loss: 0.624775  [   32/   43]
train() client id: f_00004-0-0 loss: 0.883279  [   32/  306]
train() client id: f_00004-0-1 loss: 1.039937  [   64/  306]
train() client id: f_00004-0-2 loss: 0.787393  [   96/  306]
train() client id: f_00004-0-3 loss: 0.710077  [  128/  306]
train() client id: f_00004-0-4 loss: 0.804128  [  160/  306]
train() client id: f_00004-0-5 loss: 0.756725  [  192/  306]
train() client id: f_00004-0-6 loss: 0.836369  [  224/  306]
train() client id: f_00004-0-7 loss: 0.958883  [  256/  306]
train() client id: f_00004-0-8 loss: 0.876377  [  288/  306]
train() client id: f_00004-1-0 loss: 0.946740  [   32/  306]
train() client id: f_00004-1-1 loss: 0.927675  [   64/  306]
train() client id: f_00004-1-2 loss: 0.883930  [   96/  306]
train() client id: f_00004-1-3 loss: 0.769546  [  128/  306]
train() client id: f_00004-1-4 loss: 0.769105  [  160/  306]
train() client id: f_00004-1-5 loss: 0.848264  [  192/  306]
train() client id: f_00004-1-6 loss: 0.883894  [  224/  306]
train() client id: f_00004-1-7 loss: 0.774335  [  256/  306]
train() client id: f_00004-1-8 loss: 0.748118  [  288/  306]
train() client id: f_00004-2-0 loss: 0.803069  [   32/  306]
train() client id: f_00004-2-1 loss: 0.812945  [   64/  306]
train() client id: f_00004-2-2 loss: 0.727992  [   96/  306]
train() client id: f_00004-2-3 loss: 0.754124  [  128/  306]
train() client id: f_00004-2-4 loss: 0.845208  [  160/  306]
train() client id: f_00004-2-5 loss: 1.095372  [  192/  306]
train() client id: f_00004-2-6 loss: 0.781314  [  224/  306]
train() client id: f_00004-2-7 loss: 0.775767  [  256/  306]
train() client id: f_00004-2-8 loss: 0.916374  [  288/  306]
train() client id: f_00004-3-0 loss: 0.842141  [   32/  306]
train() client id: f_00004-3-1 loss: 0.916369  [   64/  306]
train() client id: f_00004-3-2 loss: 0.959274  [   96/  306]
train() client id: f_00004-3-3 loss: 0.882795  [  128/  306]
train() client id: f_00004-3-4 loss: 0.808747  [  160/  306]
train() client id: f_00004-3-5 loss: 0.846795  [  192/  306]
train() client id: f_00004-3-6 loss: 0.726911  [  224/  306]
train() client id: f_00004-3-7 loss: 0.812274  [  256/  306]
train() client id: f_00004-3-8 loss: 0.810806  [  288/  306]
train() client id: f_00004-4-0 loss: 0.699269  [   32/  306]
train() client id: f_00004-4-1 loss: 0.811254  [   64/  306]
train() client id: f_00004-4-2 loss: 0.806724  [   96/  306]
train() client id: f_00004-4-3 loss: 0.773601  [  128/  306]
train() client id: f_00004-4-4 loss: 0.871284  [  160/  306]
train() client id: f_00004-4-5 loss: 0.951237  [  192/  306]
train() client id: f_00004-4-6 loss: 0.893773  [  224/  306]
train() client id: f_00004-4-7 loss: 0.889885  [  256/  306]
train() client id: f_00004-4-8 loss: 0.888947  [  288/  306]
train() client id: f_00004-5-0 loss: 0.820877  [   32/  306]
train() client id: f_00004-5-1 loss: 0.782142  [   64/  306]
train() client id: f_00004-5-2 loss: 0.868325  [   96/  306]
train() client id: f_00004-5-3 loss: 0.904123  [  128/  306]
train() client id: f_00004-5-4 loss: 0.895467  [  160/  306]
train() client id: f_00004-5-5 loss: 0.863159  [  192/  306]
train() client id: f_00004-5-6 loss: 0.849570  [  224/  306]
train() client id: f_00004-5-7 loss: 0.788887  [  256/  306]
train() client id: f_00004-5-8 loss: 0.746592  [  288/  306]
train() client id: f_00004-6-0 loss: 0.756498  [   32/  306]
train() client id: f_00004-6-1 loss: 0.749955  [   64/  306]
train() client id: f_00004-6-2 loss: 0.827381  [   96/  306]
train() client id: f_00004-6-3 loss: 0.884566  [  128/  306]
train() client id: f_00004-6-4 loss: 0.875922  [  160/  306]
train() client id: f_00004-6-5 loss: 0.800014  [  192/  306]
train() client id: f_00004-6-6 loss: 0.828881  [  224/  306]
train() client id: f_00004-6-7 loss: 0.944216  [  256/  306]
train() client id: f_00004-6-8 loss: 0.851962  [  288/  306]
train() client id: f_00004-7-0 loss: 0.909820  [   32/  306]
train() client id: f_00004-7-1 loss: 0.899079  [   64/  306]
train() client id: f_00004-7-2 loss: 0.989229  [   96/  306]
train() client id: f_00004-7-3 loss: 0.804687  [  128/  306]
train() client id: f_00004-7-4 loss: 0.920690  [  160/  306]
train() client id: f_00004-7-5 loss: 0.787120  [  192/  306]
train() client id: f_00004-7-6 loss: 0.791504  [  224/  306]
train() client id: f_00004-7-7 loss: 0.731899  [  256/  306]
train() client id: f_00004-7-8 loss: 0.759452  [  288/  306]
train() client id: f_00004-8-0 loss: 0.853727  [   32/  306]
train() client id: f_00004-8-1 loss: 0.830686  [   64/  306]
train() client id: f_00004-8-2 loss: 0.973771  [   96/  306]
train() client id: f_00004-8-3 loss: 0.863970  [  128/  306]
train() client id: f_00004-8-4 loss: 0.775800  [  160/  306]
train() client id: f_00004-8-5 loss: 0.771011  [  192/  306]
train() client id: f_00004-8-6 loss: 0.793315  [  224/  306]
train() client id: f_00004-8-7 loss: 0.860052  [  256/  306]
train() client id: f_00004-8-8 loss: 0.790936  [  288/  306]
train() client id: f_00004-9-0 loss: 0.793701  [   32/  306]
train() client id: f_00004-9-1 loss: 0.846205  [   64/  306]
train() client id: f_00004-9-2 loss: 0.827664  [   96/  306]
train() client id: f_00004-9-3 loss: 0.768655  [  128/  306]
train() client id: f_00004-9-4 loss: 0.802420  [  160/  306]
train() client id: f_00004-9-5 loss: 0.877124  [  192/  306]
train() client id: f_00004-9-6 loss: 0.951533  [  224/  306]
train() client id: f_00004-9-7 loss: 0.785950  [  256/  306]
train() client id: f_00004-9-8 loss: 0.969181  [  288/  306]
train() client id: f_00004-10-0 loss: 0.872202  [   32/  306]
train() client id: f_00004-10-1 loss: 0.789582  [   64/  306]
train() client id: f_00004-10-2 loss: 0.793547  [   96/  306]
train() client id: f_00004-10-3 loss: 0.808626  [  128/  306]
train() client id: f_00004-10-4 loss: 0.888868  [  160/  306]
train() client id: f_00004-10-5 loss: 0.750873  [  192/  306]
train() client id: f_00004-10-6 loss: 0.954723  [  224/  306]
train() client id: f_00004-10-7 loss: 0.919838  [  256/  306]
train() client id: f_00004-10-8 loss: 0.905383  [  288/  306]
train() client id: f_00004-11-0 loss: 0.865378  [   32/  306]
train() client id: f_00004-11-1 loss: 0.854339  [   64/  306]
train() client id: f_00004-11-2 loss: 0.793551  [   96/  306]
train() client id: f_00004-11-3 loss: 0.801405  [  128/  306]
train() client id: f_00004-11-4 loss: 0.790341  [  160/  306]
train() client id: f_00004-11-5 loss: 0.927193  [  192/  306]
train() client id: f_00004-11-6 loss: 0.816937  [  224/  306]
train() client id: f_00004-11-7 loss: 0.869223  [  256/  306]
train() client id: f_00004-11-8 loss: 0.900001  [  288/  306]
train() client id: f_00005-0-0 loss: 0.540859  [   32/  146]
train() client id: f_00005-0-1 loss: 0.516382  [   64/  146]
train() client id: f_00005-0-2 loss: 0.500192  [   96/  146]
train() client id: f_00005-0-3 loss: 0.590036  [  128/  146]
train() client id: f_00005-1-0 loss: 0.651841  [   32/  146]
train() client id: f_00005-1-1 loss: 0.304200  [   64/  146]
train() client id: f_00005-1-2 loss: 0.575110  [   96/  146]
train() client id: f_00005-1-3 loss: 0.564979  [  128/  146]
train() client id: f_00005-2-0 loss: 0.426820  [   32/  146]
train() client id: f_00005-2-1 loss: 0.505329  [   64/  146]
train() client id: f_00005-2-2 loss: 0.441546  [   96/  146]
train() client id: f_00005-2-3 loss: 0.533592  [  128/  146]
train() client id: f_00005-3-0 loss: 0.526318  [   32/  146]
train() client id: f_00005-3-1 loss: 0.469465  [   64/  146]
train() client id: f_00005-3-2 loss: 0.424729  [   96/  146]
train() client id: f_00005-3-3 loss: 0.529581  [  128/  146]
train() client id: f_00005-4-0 loss: 0.574862  [   32/  146]
train() client id: f_00005-4-1 loss: 0.421512  [   64/  146]
train() client id: f_00005-4-2 loss: 0.587579  [   96/  146]
train() client id: f_00005-4-3 loss: 0.624790  [  128/  146]
train() client id: f_00005-5-0 loss: 0.542834  [   32/  146]
train() client id: f_00005-5-1 loss: 0.364340  [   64/  146]
train() client id: f_00005-5-2 loss: 0.487931  [   96/  146]
train() client id: f_00005-5-3 loss: 0.550189  [  128/  146]
train() client id: f_00005-6-0 loss: 0.493248  [   32/  146]
train() client id: f_00005-6-1 loss: 0.299466  [   64/  146]
train() client id: f_00005-6-2 loss: 0.529219  [   96/  146]
train() client id: f_00005-6-3 loss: 0.832240  [  128/  146]
train() client id: f_00005-7-0 loss: 0.258424  [   32/  146]
train() client id: f_00005-7-1 loss: 0.848729  [   64/  146]
train() client id: f_00005-7-2 loss: 0.356410  [   96/  146]
train() client id: f_00005-7-3 loss: 0.753341  [  128/  146]
train() client id: f_00005-8-0 loss: 0.610987  [   32/  146]
train() client id: f_00005-8-1 loss: 0.625689  [   64/  146]
train() client id: f_00005-8-2 loss: 0.596018  [   96/  146]
train() client id: f_00005-8-3 loss: 0.330456  [  128/  146]
train() client id: f_00005-9-0 loss: 0.382351  [   32/  146]
train() client id: f_00005-9-1 loss: 0.513822  [   64/  146]
train() client id: f_00005-9-2 loss: 0.567282  [   96/  146]
train() client id: f_00005-9-3 loss: 0.618077  [  128/  146]
train() client id: f_00005-10-0 loss: 0.686121  [   32/  146]
train() client id: f_00005-10-1 loss: 0.557361  [   64/  146]
train() client id: f_00005-10-2 loss: 0.440210  [   96/  146]
train() client id: f_00005-10-3 loss: 0.567596  [  128/  146]
train() client id: f_00005-11-0 loss: 0.686378  [   32/  146]
train() client id: f_00005-11-1 loss: 0.494678  [   64/  146]
train() client id: f_00005-11-2 loss: 0.498644  [   96/  146]
train() client id: f_00005-11-3 loss: 0.552083  [  128/  146]
train() client id: f_00006-0-0 loss: 0.533534  [   32/   54]
train() client id: f_00006-1-0 loss: 0.458865  [   32/   54]
train() client id: f_00006-2-0 loss: 0.576794  [   32/   54]
train() client id: f_00006-3-0 loss: 0.521736  [   32/   54]
train() client id: f_00006-4-0 loss: 0.585163  [   32/   54]
train() client id: f_00006-5-0 loss: 0.476897  [   32/   54]
train() client id: f_00006-6-0 loss: 0.502015  [   32/   54]
train() client id: f_00006-7-0 loss: 0.503254  [   32/   54]
train() client id: f_00006-8-0 loss: 0.564996  [   32/   54]
train() client id: f_00006-9-0 loss: 0.521885  [   32/   54]
train() client id: f_00006-10-0 loss: 0.558225  [   32/   54]
train() client id: f_00006-11-0 loss: 0.528334  [   32/   54]
train() client id: f_00007-0-0 loss: 0.722823  [   32/  179]
train() client id: f_00007-0-1 loss: 0.813393  [   64/  179]
train() client id: f_00007-0-2 loss: 0.570679  [   96/  179]
train() client id: f_00007-0-3 loss: 0.577985  [  128/  179]
train() client id: f_00007-0-4 loss: 0.543920  [  160/  179]
train() client id: f_00007-1-0 loss: 0.611628  [   32/  179]
train() client id: f_00007-1-1 loss: 0.661039  [   64/  179]
train() client id: f_00007-1-2 loss: 0.478620  [   96/  179]
train() client id: f_00007-1-3 loss: 0.694132  [  128/  179]
train() client id: f_00007-1-4 loss: 0.720430  [  160/  179]
train() client id: f_00007-2-0 loss: 0.689523  [   32/  179]
train() client id: f_00007-2-1 loss: 0.628332  [   64/  179]
train() client id: f_00007-2-2 loss: 0.581968  [   96/  179]
train() client id: f_00007-2-3 loss: 0.620158  [  128/  179]
train() client id: f_00007-2-4 loss: 0.539071  [  160/  179]
train() client id: f_00007-3-0 loss: 0.687252  [   32/  179]
train() client id: f_00007-3-1 loss: 0.548403  [   64/  179]
train() client id: f_00007-3-2 loss: 0.482944  [   96/  179]
train() client id: f_00007-3-3 loss: 0.539872  [  128/  179]
train() client id: f_00007-3-4 loss: 0.637891  [  160/  179]
train() client id: f_00007-4-0 loss: 0.682718  [   32/  179]
train() client id: f_00007-4-1 loss: 0.589795  [   64/  179]
train() client id: f_00007-4-2 loss: 0.685872  [   96/  179]
train() client id: f_00007-4-3 loss: 0.596985  [  128/  179]
train() client id: f_00007-4-4 loss: 0.440368  [  160/  179]
train() client id: f_00007-5-0 loss: 0.709368  [   32/  179]
train() client id: f_00007-5-1 loss: 0.576214  [   64/  179]
train() client id: f_00007-5-2 loss: 0.753401  [   96/  179]
train() client id: f_00007-5-3 loss: 0.485695  [  128/  179]
train() client id: f_00007-5-4 loss: 0.536182  [  160/  179]
train() client id: f_00007-6-0 loss: 0.632293  [   32/  179]
train() client id: f_00007-6-1 loss: 0.658574  [   64/  179]
train() client id: f_00007-6-2 loss: 0.551540  [   96/  179]
train() client id: f_00007-6-3 loss: 0.563307  [  128/  179]
train() client id: f_00007-6-4 loss: 0.668571  [  160/  179]
train() client id: f_00007-7-0 loss: 0.473363  [   32/  179]
train() client id: f_00007-7-1 loss: 0.822085  [   64/  179]
train() client id: f_00007-7-2 loss: 0.679155  [   96/  179]
train() client id: f_00007-7-3 loss: 0.526051  [  128/  179]
train() client id: f_00007-7-4 loss: 0.560156  [  160/  179]
train() client id: f_00007-8-0 loss: 0.586291  [   32/  179]
train() client id: f_00007-8-1 loss: 0.691977  [   64/  179]
train() client id: f_00007-8-2 loss: 0.523589  [   96/  179]
train() client id: f_00007-8-3 loss: 0.674309  [  128/  179]
train() client id: f_00007-8-4 loss: 0.466221  [  160/  179]
train() client id: f_00007-9-0 loss: 0.781152  [   32/  179]
train() client id: f_00007-9-1 loss: 0.441944  [   64/  179]
train() client id: f_00007-9-2 loss: 0.570794  [   96/  179]
train() client id: f_00007-9-3 loss: 0.722955  [  128/  179]
train() client id: f_00007-9-4 loss: 0.434690  [  160/  179]
train() client id: f_00007-10-0 loss: 0.603446  [   32/  179]
train() client id: f_00007-10-1 loss: 0.474063  [   64/  179]
train() client id: f_00007-10-2 loss: 0.651613  [   96/  179]
train() client id: f_00007-10-3 loss: 0.632811  [  128/  179]
train() client id: f_00007-10-4 loss: 0.600952  [  160/  179]
train() client id: f_00007-11-0 loss: 0.571186  [   32/  179]
train() client id: f_00007-11-1 loss: 0.799353  [   64/  179]
train() client id: f_00007-11-2 loss: 0.635181  [   96/  179]
train() client id: f_00007-11-3 loss: 0.529717  [  128/  179]
train() client id: f_00007-11-4 loss: 0.536471  [  160/  179]
train() client id: f_00008-0-0 loss: 0.682329  [   32/  130]
train() client id: f_00008-0-1 loss: 0.869215  [   64/  130]
train() client id: f_00008-0-2 loss: 0.786558  [   96/  130]
train() client id: f_00008-0-3 loss: 0.625914  [  128/  130]
train() client id: f_00008-1-0 loss: 0.687540  [   32/  130]
train() client id: f_00008-1-1 loss: 0.759228  [   64/  130]
train() client id: f_00008-1-2 loss: 0.742824  [   96/  130]
train() client id: f_00008-1-3 loss: 0.785523  [  128/  130]
train() client id: f_00008-2-0 loss: 0.730757  [   32/  130]
train() client id: f_00008-2-1 loss: 0.716447  [   64/  130]
train() client id: f_00008-2-2 loss: 0.765283  [   96/  130]
train() client id: f_00008-2-3 loss: 0.763648  [  128/  130]
train() client id: f_00008-3-0 loss: 0.711070  [   32/  130]
train() client id: f_00008-3-1 loss: 0.731914  [   64/  130]
train() client id: f_00008-3-2 loss: 0.683194  [   96/  130]
train() client id: f_00008-3-3 loss: 0.850325  [  128/  130]
train() client id: f_00008-4-0 loss: 0.779655  [   32/  130]
train() client id: f_00008-4-1 loss: 0.669906  [   64/  130]
train() client id: f_00008-4-2 loss: 0.715860  [   96/  130]
train() client id: f_00008-4-3 loss: 0.811872  [  128/  130]
train() client id: f_00008-5-0 loss: 0.714796  [   32/  130]
train() client id: f_00008-5-1 loss: 0.786243  [   64/  130]
train() client id: f_00008-5-2 loss: 0.697231  [   96/  130]
train() client id: f_00008-5-3 loss: 0.749259  [  128/  130]
train() client id: f_00008-6-0 loss: 0.745081  [   32/  130]
train() client id: f_00008-6-1 loss: 0.754004  [   64/  130]
train() client id: f_00008-6-2 loss: 0.668617  [   96/  130]
train() client id: f_00008-6-3 loss: 0.789370  [  128/  130]
train() client id: f_00008-7-0 loss: 0.779508  [   32/  130]
train() client id: f_00008-7-1 loss: 0.724537  [   64/  130]
train() client id: f_00008-7-2 loss: 0.709907  [   96/  130]
train() client id: f_00008-7-3 loss: 0.758335  [  128/  130]
train() client id: f_00008-8-0 loss: 0.728165  [   32/  130]
train() client id: f_00008-8-1 loss: 0.632427  [   64/  130]
train() client id: f_00008-8-2 loss: 0.723123  [   96/  130]
train() client id: f_00008-8-3 loss: 0.850356  [  128/  130]
train() client id: f_00008-9-0 loss: 0.691816  [   32/  130]
train() client id: f_00008-9-1 loss: 0.790459  [   64/  130]
train() client id: f_00008-9-2 loss: 0.657335  [   96/  130]
train() client id: f_00008-9-3 loss: 0.841586  [  128/  130]
train() client id: f_00008-10-0 loss: 0.659255  [   32/  130]
train() client id: f_00008-10-1 loss: 0.931298  [   64/  130]
train() client id: f_00008-10-2 loss: 0.738610  [   96/  130]
train() client id: f_00008-10-3 loss: 0.644269  [  128/  130]
train() client id: f_00008-11-0 loss: 0.635985  [   32/  130]
train() client id: f_00008-11-1 loss: 0.751725  [   64/  130]
train() client id: f_00008-11-2 loss: 0.846795  [   96/  130]
train() client id: f_00008-11-3 loss: 0.738755  [  128/  130]
train() client id: f_00009-0-0 loss: 1.219278  [   32/  118]
train() client id: f_00009-0-1 loss: 1.242755  [   64/  118]
train() client id: f_00009-0-2 loss: 0.990145  [   96/  118]
train() client id: f_00009-1-0 loss: 0.953727  [   32/  118]
train() client id: f_00009-1-1 loss: 1.261218  [   64/  118]
train() client id: f_00009-1-2 loss: 1.074253  [   96/  118]
train() client id: f_00009-2-0 loss: 1.031007  [   32/  118]
train() client id: f_00009-2-1 loss: 0.998846  [   64/  118]
train() client id: f_00009-2-2 loss: 1.087717  [   96/  118]
train() client id: f_00009-3-0 loss: 1.162846  [   32/  118]
train() client id: f_00009-3-1 loss: 0.899806  [   64/  118]
train() client id: f_00009-3-2 loss: 1.007248  [   96/  118]
train() client id: f_00009-4-0 loss: 1.003874  [   32/  118]
train() client id: f_00009-4-1 loss: 0.910205  [   64/  118]
train() client id: f_00009-4-2 loss: 0.962718  [   96/  118]
train() client id: f_00009-5-0 loss: 0.927526  [   32/  118]
train() client id: f_00009-5-1 loss: 1.059242  [   64/  118]
train() client id: f_00009-5-2 loss: 0.893633  [   96/  118]
train() client id: f_00009-6-0 loss: 0.965294  [   32/  118]
train() client id: f_00009-6-1 loss: 0.874035  [   64/  118]
train() client id: f_00009-6-2 loss: 0.900534  [   96/  118]
train() client id: f_00009-7-0 loss: 0.943845  [   32/  118]
train() client id: f_00009-7-1 loss: 0.964442  [   64/  118]
train() client id: f_00009-7-2 loss: 0.877387  [   96/  118]
train() client id: f_00009-8-0 loss: 0.810852  [   32/  118]
train() client id: f_00009-8-1 loss: 0.966641  [   64/  118]
train() client id: f_00009-8-2 loss: 0.921129  [   96/  118]
train() client id: f_00009-9-0 loss: 0.926805  [   32/  118]
train() client id: f_00009-9-1 loss: 0.976562  [   64/  118]
train() client id: f_00009-9-2 loss: 0.734506  [   96/  118]
train() client id: f_00009-10-0 loss: 0.867981  [   32/  118]
train() client id: f_00009-10-1 loss: 0.898534  [   64/  118]
train() client id: f_00009-10-2 loss: 0.977542  [   96/  118]
train() client id: f_00009-11-0 loss: 0.883515  [   32/  118]
train() client id: f_00009-11-1 loss: 1.004822  [   64/  118]
train() client id: f_00009-11-2 loss: 0.900348  [   96/  118]
At round 32 accuracy: 0.6551724137931034
At round 32 training accuracy: 0.5881958417169685
At round 32 training loss: 0.8211441391123313
update_location
xs = [  -3.9056584     4.20031788  180.00902392   18.81129433    0.97929623
    3.95640986 -142.44319194 -121.32485185  164.66397685 -107.06087855]
ys = [ 172.5879595   155.55583871    1.32061395 -142.45517586  134.35018685
  117.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [199.50402986 184.9736782  205.92472584 175.06382243 167.4841238
 154.58275225 174.06020071 157.22721117 193.44998125 146.55389307]
dists_bs = [171.11134289 180.25212713 395.21631571 371.94167082 180.61317784
 187.87128037 180.58399931 182.26046195 374.37432475 184.00059091]
uav_gains = [1.71760459e-11 2.11772319e-11 1.56382078e-11 2.44758737e-11
 2.74270048e-11 3.36063783e-11 2.48429772e-11 3.21986963e-11
 1.87438290e-11 3.84301675e-11]
bs_gains = [6.16756090e-11 5.33124903e-11 5.91774794e-12 7.01398027e-12
 5.30146221e-11 4.74771870e-11 5.30386105e-11 5.16838882e-11
 6.88711171e-12 5.03269111e-11]
Round 33
-------------------------------
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.63572969 13.74717363  6.53678508  2.35476176 15.85548586  7.6312378
  2.9198615   9.33889895  6.88901514  6.19002745]
obj_prev = 78.09897687730752
eta_min = 1.1396317579661002e-14	eta_max = 0.9276141009437577
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 18.129363196870486	eta = 0.9090909090909091
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 32.88895800241827	eta = 0.5011177085230375
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 25.66977986634642	eta = 0.642048329034932
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.365791295624447	eta = 0.6764089485097873
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.297762088268055	eta = 0.678302767555703
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.29756203224391	eta = 0.678308352418689
eta = 0.678308352418689
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.03202948 0.06736359 0.03152108 0.0109307  0.07778588 0.03711352
 0.01372692 0.0455022  0.03304629 0.02999586]
ene_total = [2.16353862 3.9073382  2.14884473 1.01590035 4.454601   2.32913348
 1.16104971 2.80262517 2.36340733 1.95112344]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 0 obj = 4.427261042788227
eta = 0.678308352418689
freqs = [35725893.75226643 71777219.31597894 35338835.42638909 11985166.67285503
 82896638.99614602 39689662.43089405 15041309.96971984 49331324.0982344
 39308827.86172418 32018442.92741502]
eta_min = 0.6783083524187157	eta_max = 0.6783083524186895
af = 0.013235761297397841	bf = 1.4190753947365375	zeta = 0.014559337427137626	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [2.50952835e-06 2.13046542e-05 2.41647056e-06 9.63855586e-08
 3.28133544e-05 3.58891376e-06 1.90642814e-07 6.79757383e-06
 3.13457667e-06 1.88772088e-06]
ene_total = [1.70964427 1.323468   1.75202375 1.56544914 1.327104   1.35189348
 1.55993376 1.47051205 2.22823206 1.33544537]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 1 obj = 4.427261042788233
eta = 0.6783083524186895
freqs = [35725893.75226644 71777219.31597894 35338835.4263891  11985166.67285503
 82896638.99614602 39689662.43089406 15041309.96971984 49331324.09823441
 39308827.86172419 32018442.92741502]
Done!
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [9.64912650e-06 8.19163105e-05 9.29131967e-06 3.70602088e-07
 1.26167217e-04 1.37993591e-05 7.33020858e-07 2.61366442e-05
 1.20524348e-05 7.25827927e-06]
ene_total = [0.00921539 0.00718889 0.00944338 0.00843188 0.00724122 0.00729159
 0.00840244 0.0079396  0.01201031 0.00719816]
At round 33 energy consumption: 0.08436284522778167
At round 33 eta: 0.6783083524186895
At round 33 a_n: 16.878589906266903
At round 33 local rounds: 12.71011101057834
At round 33 global rounds: 52.468225498458686
gradient difference: 0.4131239056587219
train() client id: f_00000-0-0 loss: 1.093097  [   32/  126]
train() client id: f_00000-0-1 loss: 1.040717  [   64/  126]
train() client id: f_00000-0-2 loss: 1.096343  [   96/  126]
train() client id: f_00000-1-0 loss: 0.916002  [   32/  126]
train() client id: f_00000-1-1 loss: 1.318202  [   64/  126]
train() client id: f_00000-1-2 loss: 0.896417  [   96/  126]
train() client id: f_00000-2-0 loss: 1.190256  [   32/  126]
train() client id: f_00000-2-1 loss: 0.910532  [   64/  126]
train() client id: f_00000-2-2 loss: 0.900507  [   96/  126]
train() client id: f_00000-3-0 loss: 0.812137  [   32/  126]
train() client id: f_00000-3-1 loss: 0.794568  [   64/  126]
train() client id: f_00000-3-2 loss: 0.894618  [   96/  126]
train() client id: f_00000-4-0 loss: 0.791543  [   32/  126]
train() client id: f_00000-4-1 loss: 0.991645  [   64/  126]
train() client id: f_00000-4-2 loss: 0.754676  [   96/  126]
train() client id: f_00000-5-0 loss: 0.960481  [   32/  126]
train() client id: f_00000-5-1 loss: 0.765371  [   64/  126]
train() client id: f_00000-5-2 loss: 0.795949  [   96/  126]
train() client id: f_00000-6-0 loss: 0.874824  [   32/  126]
train() client id: f_00000-6-1 loss: 0.708945  [   64/  126]
train() client id: f_00000-6-2 loss: 0.808118  [   96/  126]
train() client id: f_00000-7-0 loss: 0.745274  [   32/  126]
train() client id: f_00000-7-1 loss: 0.750154  [   64/  126]
train() client id: f_00000-7-2 loss: 0.758746  [   96/  126]
train() client id: f_00000-8-0 loss: 0.765119  [   32/  126]
train() client id: f_00000-8-1 loss: 0.759014  [   64/  126]
train() client id: f_00000-8-2 loss: 0.749604  [   96/  126]
train() client id: f_00000-9-0 loss: 0.703938  [   32/  126]
train() client id: f_00000-9-1 loss: 0.655038  [   64/  126]
train() client id: f_00000-9-2 loss: 0.879723  [   96/  126]
train() client id: f_00000-10-0 loss: 0.779300  [   32/  126]
train() client id: f_00000-10-1 loss: 0.807683  [   64/  126]
train() client id: f_00000-10-2 loss: 0.708639  [   96/  126]
train() client id: f_00000-11-0 loss: 0.632502  [   32/  126]
train() client id: f_00000-11-1 loss: 0.885398  [   64/  126]
train() client id: f_00000-11-2 loss: 0.655424  [   96/  126]
train() client id: f_00001-0-0 loss: 0.427019  [   32/  265]
train() client id: f_00001-0-1 loss: 0.513206  [   64/  265]
train() client id: f_00001-0-2 loss: 0.541754  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444457  [  128/  265]
train() client id: f_00001-0-4 loss: 0.514414  [  160/  265]
train() client id: f_00001-0-5 loss: 0.462107  [  192/  265]
train() client id: f_00001-0-6 loss: 0.501454  [  224/  265]
train() client id: f_00001-0-7 loss: 0.419743  [  256/  265]
train() client id: f_00001-1-0 loss: 0.397750  [   32/  265]
train() client id: f_00001-1-1 loss: 0.431558  [   64/  265]
train() client id: f_00001-1-2 loss: 0.497097  [   96/  265]
train() client id: f_00001-1-3 loss: 0.504961  [  128/  265]
train() client id: f_00001-1-4 loss: 0.457878  [  160/  265]
train() client id: f_00001-1-5 loss: 0.497986  [  192/  265]
train() client id: f_00001-1-6 loss: 0.512401  [  224/  265]
train() client id: f_00001-1-7 loss: 0.525548  [  256/  265]
train() client id: f_00001-2-0 loss: 0.381032  [   32/  265]
train() client id: f_00001-2-1 loss: 0.388597  [   64/  265]
train() client id: f_00001-2-2 loss: 0.383997  [   96/  265]
train() client id: f_00001-2-3 loss: 0.513688  [  128/  265]
train() client id: f_00001-2-4 loss: 0.497570  [  160/  265]
train() client id: f_00001-2-5 loss: 0.647885  [  192/  265]
train() client id: f_00001-2-6 loss: 0.417482  [  224/  265]
train() client id: f_00001-2-7 loss: 0.523405  [  256/  265]
train() client id: f_00001-3-0 loss: 0.453809  [   32/  265]
train() client id: f_00001-3-1 loss: 0.502288  [   64/  265]
train() client id: f_00001-3-2 loss: 0.528678  [   96/  265]
train() client id: f_00001-3-3 loss: 0.433536  [  128/  265]
train() client id: f_00001-3-4 loss: 0.384574  [  160/  265]
train() client id: f_00001-3-5 loss: 0.569842  [  192/  265]
train() client id: f_00001-3-6 loss: 0.437959  [  224/  265]
train() client id: f_00001-3-7 loss: 0.423764  [  256/  265]
train() client id: f_00001-4-0 loss: 0.556666  [   32/  265]
train() client id: f_00001-4-1 loss: 0.375167  [   64/  265]
train() client id: f_00001-4-2 loss: 0.512969  [   96/  265]
train() client id: f_00001-4-3 loss: 0.464093  [  128/  265]
train() client id: f_00001-4-4 loss: 0.418221  [  160/  265]
train() client id: f_00001-4-5 loss: 0.533032  [  192/  265]
train() client id: f_00001-4-6 loss: 0.361092  [  224/  265]
train() client id: f_00001-4-7 loss: 0.477957  [  256/  265]
train() client id: f_00001-5-0 loss: 0.567498  [   32/  265]
train() client id: f_00001-5-1 loss: 0.456646  [   64/  265]
train() client id: f_00001-5-2 loss: 0.361132  [   96/  265]
train() client id: f_00001-5-3 loss: 0.471021  [  128/  265]
train() client id: f_00001-5-4 loss: 0.418329  [  160/  265]
train() client id: f_00001-5-5 loss: 0.441304  [  192/  265]
train() client id: f_00001-5-6 loss: 0.503837  [  224/  265]
train() client id: f_00001-5-7 loss: 0.462828  [  256/  265]
train() client id: f_00001-6-0 loss: 0.578925  [   32/  265]
train() client id: f_00001-6-1 loss: 0.371895  [   64/  265]
train() client id: f_00001-6-2 loss: 0.389033  [   96/  265]
train() client id: f_00001-6-3 loss: 0.521637  [  128/  265]
train() client id: f_00001-6-4 loss: 0.544681  [  160/  265]
train() client id: f_00001-6-5 loss: 0.413609  [  192/  265]
train() client id: f_00001-6-6 loss: 0.430357  [  224/  265]
train() client id: f_00001-6-7 loss: 0.426426  [  256/  265]
train() client id: f_00001-7-0 loss: 0.451219  [   32/  265]
train() client id: f_00001-7-1 loss: 0.468871  [   64/  265]
train() client id: f_00001-7-2 loss: 0.482574  [   96/  265]
train() client id: f_00001-7-3 loss: 0.542950  [  128/  265]
train() client id: f_00001-7-4 loss: 0.539875  [  160/  265]
train() client id: f_00001-7-5 loss: 0.359702  [  192/  265]
train() client id: f_00001-7-6 loss: 0.406298  [  224/  265]
train() client id: f_00001-7-7 loss: 0.431589  [  256/  265]
train() client id: f_00001-8-0 loss: 0.435288  [   32/  265]
train() client id: f_00001-8-1 loss: 0.479649  [   64/  265]
train() client id: f_00001-8-2 loss: 0.376754  [   96/  265]
train() client id: f_00001-8-3 loss: 0.393087  [  128/  265]
train() client id: f_00001-8-4 loss: 0.663624  [  160/  265]
train() client id: f_00001-8-5 loss: 0.363230  [  192/  265]
train() client id: f_00001-8-6 loss: 0.408284  [  224/  265]
train() client id: f_00001-8-7 loss: 0.546197  [  256/  265]
train() client id: f_00001-9-0 loss: 0.458949  [   32/  265]
train() client id: f_00001-9-1 loss: 0.584152  [   64/  265]
train() client id: f_00001-9-2 loss: 0.498453  [   96/  265]
train() client id: f_00001-9-3 loss: 0.380609  [  128/  265]
train() client id: f_00001-9-4 loss: 0.439322  [  160/  265]
train() client id: f_00001-9-5 loss: 0.393978  [  192/  265]
train() client id: f_00001-9-6 loss: 0.471812  [  224/  265]
train() client id: f_00001-9-7 loss: 0.439645  [  256/  265]
train() client id: f_00001-10-0 loss: 0.446684  [   32/  265]
train() client id: f_00001-10-1 loss: 0.368211  [   64/  265]
train() client id: f_00001-10-2 loss: 0.426530  [   96/  265]
train() client id: f_00001-10-3 loss: 0.519828  [  128/  265]
train() client id: f_00001-10-4 loss: 0.400679  [  160/  265]
train() client id: f_00001-10-5 loss: 0.444876  [  192/  265]
train() client id: f_00001-10-6 loss: 0.497783  [  224/  265]
train() client id: f_00001-10-7 loss: 0.559639  [  256/  265]
train() client id: f_00001-11-0 loss: 0.463991  [   32/  265]
train() client id: f_00001-11-1 loss: 0.443808  [   64/  265]
train() client id: f_00001-11-2 loss: 0.446669  [   96/  265]
train() client id: f_00001-11-3 loss: 0.603772  [  128/  265]
train() client id: f_00001-11-4 loss: 0.490498  [  160/  265]
train() client id: f_00001-11-5 loss: 0.358375  [  192/  265]
train() client id: f_00001-11-6 loss: 0.491323  [  224/  265]
train() client id: f_00001-11-7 loss: 0.357451  [  256/  265]
train() client id: f_00002-0-0 loss: 1.114005  [   32/  124]
train() client id: f_00002-0-1 loss: 1.025249  [   64/  124]
train() client id: f_00002-0-2 loss: 1.227746  [   96/  124]
train() client id: f_00002-1-0 loss: 1.145292  [   32/  124]
train() client id: f_00002-1-1 loss: 0.913214  [   64/  124]
train() client id: f_00002-1-2 loss: 1.137649  [   96/  124]
train() client id: f_00002-2-0 loss: 1.009607  [   32/  124]
train() client id: f_00002-2-1 loss: 1.015241  [   64/  124]
train() client id: f_00002-2-2 loss: 1.051980  [   96/  124]
train() client id: f_00002-3-0 loss: 0.843847  [   32/  124]
train() client id: f_00002-3-1 loss: 1.018832  [   64/  124]
train() client id: f_00002-3-2 loss: 0.994764  [   96/  124]
train() client id: f_00002-4-0 loss: 0.960660  [   32/  124]
train() client id: f_00002-4-1 loss: 0.904718  [   64/  124]
train() client id: f_00002-4-2 loss: 0.903619  [   96/  124]
train() client id: f_00002-5-0 loss: 1.006803  [   32/  124]
train() client id: f_00002-5-1 loss: 0.978344  [   64/  124]
train() client id: f_00002-5-2 loss: 0.904510  [   96/  124]
train() client id: f_00002-6-0 loss: 0.872540  [   32/  124]
train() client id: f_00002-6-1 loss: 0.902621  [   64/  124]
train() client id: f_00002-6-2 loss: 0.922861  [   96/  124]
train() client id: f_00002-7-0 loss: 0.992479  [   32/  124]
train() client id: f_00002-7-1 loss: 0.834471  [   64/  124]
train() client id: f_00002-7-2 loss: 0.827735  [   96/  124]
train() client id: f_00002-8-0 loss: 0.730017  [   32/  124]
train() client id: f_00002-8-1 loss: 0.838398  [   64/  124]
train() client id: f_00002-8-2 loss: 0.946405  [   96/  124]
train() client id: f_00002-9-0 loss: 1.034276  [   32/  124]
train() client id: f_00002-9-1 loss: 0.783565  [   64/  124]
train() client id: f_00002-9-2 loss: 0.839827  [   96/  124]
train() client id: f_00002-10-0 loss: 0.969691  [   32/  124]
train() client id: f_00002-10-1 loss: 0.895135  [   64/  124]
train() client id: f_00002-10-2 loss: 0.877404  [   96/  124]
train() client id: f_00002-11-0 loss: 0.830300  [   32/  124]
train() client id: f_00002-11-1 loss: 0.980008  [   64/  124]
train() client id: f_00002-11-2 loss: 0.933757  [   96/  124]
train() client id: f_00003-0-0 loss: 0.728490  [   32/   43]
train() client id: f_00003-1-0 loss: 0.846243  [   32/   43]
train() client id: f_00003-2-0 loss: 0.870335  [   32/   43]
train() client id: f_00003-3-0 loss: 1.001309  [   32/   43]
train() client id: f_00003-4-0 loss: 0.692329  [   32/   43]
train() client id: f_00003-5-0 loss: 0.830738  [   32/   43]
train() client id: f_00003-6-0 loss: 0.804917  [   32/   43]
train() client id: f_00003-7-0 loss: 0.965052  [   32/   43]
train() client id: f_00003-8-0 loss: 0.848305  [   32/   43]
train() client id: f_00003-9-0 loss: 0.938560  [   32/   43]
train() client id: f_00003-10-0 loss: 0.823927  [   32/   43]
train() client id: f_00003-11-0 loss: 0.949491  [   32/   43]
train() client id: f_00004-0-0 loss: 0.912120  [   32/  306]
train() client id: f_00004-0-1 loss: 0.675929  [   64/  306]
train() client id: f_00004-0-2 loss: 0.858005  [   96/  306]
train() client id: f_00004-0-3 loss: 0.820439  [  128/  306]
train() client id: f_00004-0-4 loss: 0.905179  [  160/  306]
train() client id: f_00004-0-5 loss: 0.683732  [  192/  306]
train() client id: f_00004-0-6 loss: 0.766336  [  224/  306]
train() client id: f_00004-0-7 loss: 0.742936  [  256/  306]
train() client id: f_00004-0-8 loss: 0.971524  [  288/  306]
train() client id: f_00004-1-0 loss: 0.825719  [   32/  306]
train() client id: f_00004-1-1 loss: 0.821750  [   64/  306]
train() client id: f_00004-1-2 loss: 0.755446  [   96/  306]
train() client id: f_00004-1-3 loss: 0.805437  [  128/  306]
train() client id: f_00004-1-4 loss: 0.934064  [  160/  306]
train() client id: f_00004-1-5 loss: 0.680019  [  192/  306]
train() client id: f_00004-1-6 loss: 0.828434  [  224/  306]
train() client id: f_00004-1-7 loss: 0.876707  [  256/  306]
train() client id: f_00004-1-8 loss: 0.823524  [  288/  306]
train() client id: f_00004-2-0 loss: 0.773016  [   32/  306]
train() client id: f_00004-2-1 loss: 0.872492  [   64/  306]
train() client id: f_00004-2-2 loss: 0.948913  [   96/  306]
train() client id: f_00004-2-3 loss: 0.696440  [  128/  306]
train() client id: f_00004-2-4 loss: 0.872375  [  160/  306]
train() client id: f_00004-2-5 loss: 0.759041  [  192/  306]
train() client id: f_00004-2-6 loss: 0.839940  [  224/  306]
train() client id: f_00004-2-7 loss: 0.747650  [  256/  306]
train() client id: f_00004-2-8 loss: 0.841984  [  288/  306]
train() client id: f_00004-3-0 loss: 0.851838  [   32/  306]
train() client id: f_00004-3-1 loss: 0.805449  [   64/  306]
train() client id: f_00004-3-2 loss: 0.753670  [   96/  306]
train() client id: f_00004-3-3 loss: 0.897107  [  128/  306]
train() client id: f_00004-3-4 loss: 0.847149  [  160/  306]
train() client id: f_00004-3-5 loss: 0.836296  [  192/  306]
train() client id: f_00004-3-6 loss: 0.735351  [  224/  306]
train() client id: f_00004-3-7 loss: 0.876760  [  256/  306]
train() client id: f_00004-3-8 loss: 0.753472  [  288/  306]
train() client id: f_00004-4-0 loss: 0.823954  [   32/  306]
train() client id: f_00004-4-1 loss: 0.786562  [   64/  306]
train() client id: f_00004-4-2 loss: 0.828450  [   96/  306]
train() client id: f_00004-4-3 loss: 0.910743  [  128/  306]
train() client id: f_00004-4-4 loss: 0.779048  [  160/  306]
train() client id: f_00004-4-5 loss: 0.796420  [  192/  306]
train() client id: f_00004-4-6 loss: 0.686360  [  224/  306]
train() client id: f_00004-4-7 loss: 0.846858  [  256/  306]
train() client id: f_00004-4-8 loss: 0.766509  [  288/  306]
train() client id: f_00004-5-0 loss: 0.830507  [   32/  306]
train() client id: f_00004-5-1 loss: 0.968710  [   64/  306]
train() client id: f_00004-5-2 loss: 0.822943  [   96/  306]
train() client id: f_00004-5-3 loss: 0.735551  [  128/  306]
train() client id: f_00004-5-4 loss: 0.953119  [  160/  306]
train() client id: f_00004-5-5 loss: 0.758952  [  192/  306]
train() client id: f_00004-5-6 loss: 0.669538  [  224/  306]
train() client id: f_00004-5-7 loss: 0.733058  [  256/  306]
train() client id: f_00004-5-8 loss: 0.863509  [  288/  306]
train() client id: f_00004-6-0 loss: 0.901758  [   32/  306]
train() client id: f_00004-6-1 loss: 0.775032  [   64/  306]
train() client id: f_00004-6-2 loss: 0.761502  [   96/  306]
train() client id: f_00004-6-3 loss: 0.748131  [  128/  306]
train() client id: f_00004-6-4 loss: 0.847093  [  160/  306]
train() client id: f_00004-6-5 loss: 0.804067  [  192/  306]
train() client id: f_00004-6-6 loss: 0.863596  [  224/  306]
train() client id: f_00004-6-7 loss: 0.748134  [  256/  306]
train() client id: f_00004-6-8 loss: 0.878262  [  288/  306]
train() client id: f_00004-7-0 loss: 0.709919  [   32/  306]
train() client id: f_00004-7-1 loss: 0.823012  [   64/  306]
train() client id: f_00004-7-2 loss: 0.784051  [   96/  306]
train() client id: f_00004-7-3 loss: 0.944564  [  128/  306]
train() client id: f_00004-7-4 loss: 0.806020  [  160/  306]
train() client id: f_00004-7-5 loss: 0.823105  [  192/  306]
train() client id: f_00004-7-6 loss: 0.800567  [  224/  306]
train() client id: f_00004-7-7 loss: 0.793283  [  256/  306]
train() client id: f_00004-7-8 loss: 0.751339  [  288/  306]
train() client id: f_00004-8-0 loss: 0.831566  [   32/  306]
train() client id: f_00004-8-1 loss: 0.837676  [   64/  306]
train() client id: f_00004-8-2 loss: 0.779588  [   96/  306]
train() client id: f_00004-8-3 loss: 0.665242  [  128/  306]
train() client id: f_00004-8-4 loss: 0.809072  [  160/  306]
train() client id: f_00004-8-5 loss: 0.831351  [  192/  306]
train() client id: f_00004-8-6 loss: 0.918603  [  224/  306]
train() client id: f_00004-8-7 loss: 0.764225  [  256/  306]
train() client id: f_00004-8-8 loss: 0.850159  [  288/  306]
train() client id: f_00004-9-0 loss: 0.797205  [   32/  306]
train() client id: f_00004-9-1 loss: 0.827372  [   64/  306]
train() client id: f_00004-9-2 loss: 0.714309  [   96/  306]
train() client id: f_00004-9-3 loss: 0.864349  [  128/  306]
train() client id: f_00004-9-4 loss: 0.925495  [  160/  306]
train() client id: f_00004-9-5 loss: 0.912229  [  192/  306]
train() client id: f_00004-9-6 loss: 0.748842  [  224/  306]
train() client id: f_00004-9-7 loss: 0.836403  [  256/  306]
train() client id: f_00004-9-8 loss: 0.774213  [  288/  306]
train() client id: f_00004-10-0 loss: 0.799349  [   32/  306]
train() client id: f_00004-10-1 loss: 0.720548  [   64/  306]
train() client id: f_00004-10-2 loss: 0.933410  [   96/  306]
train() client id: f_00004-10-3 loss: 0.836141  [  128/  306]
train() client id: f_00004-10-4 loss: 0.832592  [  160/  306]
train() client id: f_00004-10-5 loss: 0.745918  [  192/  306]
train() client id: f_00004-10-6 loss: 0.789917  [  224/  306]
train() client id: f_00004-10-7 loss: 0.833696  [  256/  306]
train() client id: f_00004-10-8 loss: 0.816698  [  288/  306]
train() client id: f_00004-11-0 loss: 0.867748  [   32/  306]
train() client id: f_00004-11-1 loss: 0.668391  [   64/  306]
train() client id: f_00004-11-2 loss: 0.985934  [   96/  306]
train() client id: f_00004-11-3 loss: 0.816092  [  128/  306]
train() client id: f_00004-11-4 loss: 0.853977  [  160/  306]
train() client id: f_00004-11-5 loss: 0.691367  [  192/  306]
train() client id: f_00004-11-6 loss: 0.833340  [  224/  306]
train() client id: f_00004-11-7 loss: 0.772800  [  256/  306]
train() client id: f_00004-11-8 loss: 0.741065  [  288/  306]
train() client id: f_00005-0-0 loss: 0.668862  [   32/  146]
train() client id: f_00005-0-1 loss: 0.775209  [   64/  146]
train() client id: f_00005-0-2 loss: 0.798144  [   96/  146]
train() client id: f_00005-0-3 loss: 0.279821  [  128/  146]
train() client id: f_00005-1-0 loss: 0.614728  [   32/  146]
train() client id: f_00005-1-1 loss: 0.560283  [   64/  146]
train() client id: f_00005-1-2 loss: 0.722058  [   96/  146]
train() client id: f_00005-1-3 loss: 0.564633  [  128/  146]
train() client id: f_00005-2-0 loss: 0.547723  [   32/  146]
train() client id: f_00005-2-1 loss: 0.518853  [   64/  146]
train() client id: f_00005-2-2 loss: 0.797422  [   96/  146]
train() client id: f_00005-2-3 loss: 0.490279  [  128/  146]
train() client id: f_00005-3-0 loss: 0.650640  [   32/  146]
train() client id: f_00005-3-1 loss: 0.819328  [   64/  146]
train() client id: f_00005-3-2 loss: 0.615777  [   96/  146]
train() client id: f_00005-3-3 loss: 0.517417  [  128/  146]
train() client id: f_00005-4-0 loss: 0.737522  [   32/  146]
train() client id: f_00005-4-1 loss: 0.539362  [   64/  146]
train() client id: f_00005-4-2 loss: 0.587729  [   96/  146]
train() client id: f_00005-4-3 loss: 0.552890  [  128/  146]
train() client id: f_00005-5-0 loss: 0.566115  [   32/  146]
train() client id: f_00005-5-1 loss: 0.665706  [   64/  146]
train() client id: f_00005-5-2 loss: 0.575829  [   96/  146]
train() client id: f_00005-5-3 loss: 0.697585  [  128/  146]
train() client id: f_00005-6-0 loss: 0.542512  [   32/  146]
train() client id: f_00005-6-1 loss: 0.675892  [   64/  146]
train() client id: f_00005-6-2 loss: 0.541801  [   96/  146]
train() client id: f_00005-6-3 loss: 0.533976  [  128/  146]
train() client id: f_00005-7-0 loss: 0.613359  [   32/  146]
train() client id: f_00005-7-1 loss: 0.727328  [   64/  146]
train() client id: f_00005-7-2 loss: 0.469057  [   96/  146]
train() client id: f_00005-7-3 loss: 0.487797  [  128/  146]
train() client id: f_00005-8-0 loss: 0.721991  [   32/  146]
train() client id: f_00005-8-1 loss: 0.493872  [   64/  146]
train() client id: f_00005-8-2 loss: 0.758397  [   96/  146]
train() client id: f_00005-8-3 loss: 0.611900  [  128/  146]
train() client id: f_00005-9-0 loss: 0.635622  [   32/  146]
train() client id: f_00005-9-1 loss: 0.619889  [   64/  146]
train() client id: f_00005-9-2 loss: 0.526629  [   96/  146]
train() client id: f_00005-9-3 loss: 0.819005  [  128/  146]
train() client id: f_00005-10-0 loss: 0.546469  [   32/  146]
train() client id: f_00005-10-1 loss: 0.419099  [   64/  146]
train() client id: f_00005-10-2 loss: 0.918535  [   96/  146]
train() client id: f_00005-10-3 loss: 0.499172  [  128/  146]
train() client id: f_00005-11-0 loss: 0.536702  [   32/  146]
train() client id: f_00005-11-1 loss: 0.446144  [   64/  146]
train() client id: f_00005-11-2 loss: 0.626277  [   96/  146]
train() client id: f_00005-11-3 loss: 0.542854  [  128/  146]
train() client id: f_00006-0-0 loss: 0.549402  [   32/   54]
train() client id: f_00006-1-0 loss: 0.528623  [   32/   54]
train() client id: f_00006-2-0 loss: 0.565623  [   32/   54]
train() client id: f_00006-3-0 loss: 0.559248  [   32/   54]
train() client id: f_00006-4-0 loss: 0.501128  [   32/   54]
train() client id: f_00006-5-0 loss: 0.464954  [   32/   54]
train() client id: f_00006-6-0 loss: 0.551169  [   32/   54]
train() client id: f_00006-7-0 loss: 0.426223  [   32/   54]
train() client id: f_00006-8-0 loss: 0.571028  [   32/   54]
train() client id: f_00006-9-0 loss: 0.495977  [   32/   54]
train() client id: f_00006-10-0 loss: 0.550277  [   32/   54]
train() client id: f_00006-11-0 loss: 0.510098  [   32/   54]
train() client id: f_00007-0-0 loss: 0.609517  [   32/  179]
train() client id: f_00007-0-1 loss: 0.634892  [   64/  179]
train() client id: f_00007-0-2 loss: 0.681336  [   96/  179]
train() client id: f_00007-0-3 loss: 0.523399  [  128/  179]
train() client id: f_00007-0-4 loss: 0.721163  [  160/  179]
train() client id: f_00007-1-0 loss: 0.561624  [   32/  179]
train() client id: f_00007-1-1 loss: 0.569568  [   64/  179]
train() client id: f_00007-1-2 loss: 0.506989  [   96/  179]
train() client id: f_00007-1-3 loss: 0.805188  [  128/  179]
train() client id: f_00007-1-4 loss: 0.718186  [  160/  179]
train() client id: f_00007-2-0 loss: 0.653361  [   32/  179]
train() client id: f_00007-2-1 loss: 0.738847  [   64/  179]
train() client id: f_00007-2-2 loss: 0.656659  [   96/  179]
train() client id: f_00007-2-3 loss: 0.518939  [  128/  179]
train() client id: f_00007-2-4 loss: 0.493841  [  160/  179]
train() client id: f_00007-3-0 loss: 0.639105  [   32/  179]
train() client id: f_00007-3-1 loss: 0.768338  [   64/  179]
train() client id: f_00007-3-2 loss: 0.540926  [   96/  179]
train() client id: f_00007-3-3 loss: 0.653072  [  128/  179]
train() client id: f_00007-3-4 loss: 0.535509  [  160/  179]
train() client id: f_00007-4-0 loss: 0.553923  [   32/  179]
train() client id: f_00007-4-1 loss: 0.596318  [   64/  179]
train() client id: f_00007-4-2 loss: 0.529412  [   96/  179]
train() client id: f_00007-4-3 loss: 0.525440  [  128/  179]
train() client id: f_00007-4-4 loss: 0.665024  [  160/  179]
train() client id: f_00007-5-0 loss: 0.485474  [   32/  179]
train() client id: f_00007-5-1 loss: 0.507622  [   64/  179]
train() client id: f_00007-5-2 loss: 0.606137  [   96/  179]
train() client id: f_00007-5-3 loss: 0.655207  [  128/  179]
train() client id: f_00007-5-4 loss: 0.826272  [  160/  179]
train() client id: f_00007-6-0 loss: 0.590712  [   32/  179]
train() client id: f_00007-6-1 loss: 0.725322  [   64/  179]
train() client id: f_00007-6-2 loss: 0.628922  [   96/  179]
train() client id: f_00007-6-3 loss: 0.551157  [  128/  179]
train() client id: f_00007-6-4 loss: 0.597098  [  160/  179]
train() client id: f_00007-7-0 loss: 0.627545  [   32/  179]
train() client id: f_00007-7-1 loss: 0.452778  [   64/  179]
train() client id: f_00007-7-2 loss: 0.625872  [   96/  179]
train() client id: f_00007-7-3 loss: 0.657866  [  128/  179]
train() client id: f_00007-7-4 loss: 0.550516  [  160/  179]
train() client id: f_00007-8-0 loss: 0.711787  [   32/  179]
train() client id: f_00007-8-1 loss: 0.698624  [   64/  179]
train() client id: f_00007-8-2 loss: 0.483755  [   96/  179]
train() client id: f_00007-8-3 loss: 0.621560  [  128/  179]
train() client id: f_00007-8-4 loss: 0.529502  [  160/  179]
train() client id: f_00007-9-0 loss: 0.637267  [   32/  179]
train() client id: f_00007-9-1 loss: 0.553470  [   64/  179]
train() client id: f_00007-9-2 loss: 0.655857  [   96/  179]
train() client id: f_00007-9-3 loss: 0.548977  [  128/  179]
train() client id: f_00007-9-4 loss: 0.443597  [  160/  179]
train() client id: f_00007-10-0 loss: 0.708082  [   32/  179]
train() client id: f_00007-10-1 loss: 0.595759  [   64/  179]
train() client id: f_00007-10-2 loss: 0.466350  [   96/  179]
train() client id: f_00007-10-3 loss: 0.565978  [  128/  179]
train() client id: f_00007-10-4 loss: 0.602478  [  160/  179]
train() client id: f_00007-11-0 loss: 0.446693  [   32/  179]
train() client id: f_00007-11-1 loss: 0.796869  [   64/  179]
train() client id: f_00007-11-2 loss: 0.590785  [   96/  179]
train() client id: f_00007-11-3 loss: 0.418349  [  128/  179]
train() client id: f_00007-11-4 loss: 0.767609  [  160/  179]
train() client id: f_00008-0-0 loss: 0.705898  [   32/  130]
train() client id: f_00008-0-1 loss: 0.713273  [   64/  130]
train() client id: f_00008-0-2 loss: 0.668317  [   96/  130]
train() client id: f_00008-0-3 loss: 0.799424  [  128/  130]
train() client id: f_00008-1-0 loss: 0.842764  [   32/  130]
train() client id: f_00008-1-1 loss: 0.593738  [   64/  130]
train() client id: f_00008-1-2 loss: 0.709590  [   96/  130]
train() client id: f_00008-1-3 loss: 0.701334  [  128/  130]
train() client id: f_00008-2-0 loss: 0.867093  [   32/  130]
train() client id: f_00008-2-1 loss: 0.765407  [   64/  130]
train() client id: f_00008-2-2 loss: 0.604747  [   96/  130]
train() client id: f_00008-2-3 loss: 0.638370  [  128/  130]
train() client id: f_00008-3-0 loss: 0.699089  [   32/  130]
train() client id: f_00008-3-1 loss: 0.726270  [   64/  130]
train() client id: f_00008-3-2 loss: 0.638392  [   96/  130]
train() client id: f_00008-3-3 loss: 0.800886  [  128/  130]
train() client id: f_00008-4-0 loss: 0.806240  [   32/  130]
train() client id: f_00008-4-1 loss: 0.747144  [   64/  130]
train() client id: f_00008-4-2 loss: 0.728987  [   96/  130]
train() client id: f_00008-4-3 loss: 0.599381  [  128/  130]
train() client id: f_00008-5-0 loss: 0.703264  [   32/  130]
train() client id: f_00008-5-1 loss: 0.746495  [   64/  130]
train() client id: f_00008-5-2 loss: 0.768303  [   96/  130]
train() client id: f_00008-5-3 loss: 0.627975  [  128/  130]
train() client id: f_00008-6-0 loss: 0.703910  [   32/  130]
train() client id: f_00008-6-1 loss: 0.826623  [   64/  130]
train() client id: f_00008-6-2 loss: 0.643543  [   96/  130]
train() client id: f_00008-6-3 loss: 0.681931  [  128/  130]
train() client id: f_00008-7-0 loss: 0.651672  [   32/  130]
train() client id: f_00008-7-1 loss: 0.660477  [   64/  130]
train() client id: f_00008-7-2 loss: 0.753150  [   96/  130]
train() client id: f_00008-7-3 loss: 0.771969  [  128/  130]
train() client id: f_00008-8-0 loss: 0.715996  [   32/  130]
train() client id: f_00008-8-1 loss: 0.720600  [   64/  130]
train() client id: f_00008-8-2 loss: 0.748565  [   96/  130]
train() client id: f_00008-8-3 loss: 0.677405  [  128/  130]
train() client id: f_00008-9-0 loss: 0.738545  [   32/  130]
train() client id: f_00008-9-1 loss: 0.705502  [   64/  130]
train() client id: f_00008-9-2 loss: 0.675967  [   96/  130]
train() client id: f_00008-9-3 loss: 0.756162  [  128/  130]
train() client id: f_00008-10-0 loss: 0.856428  [   32/  130]
train() client id: f_00008-10-1 loss: 0.717938  [   64/  130]
train() client id: f_00008-10-2 loss: 0.737635  [   96/  130]
train() client id: f_00008-10-3 loss: 0.571779  [  128/  130]
train() client id: f_00008-11-0 loss: 0.800639  [   32/  130]
train() client id: f_00008-11-1 loss: 0.721910  [   64/  130]
train() client id: f_00008-11-2 loss: 0.611652  [   96/  130]
train() client id: f_00008-11-3 loss: 0.717339  [  128/  130]
train() client id: f_00009-0-0 loss: 1.089049  [   32/  118]
train() client id: f_00009-0-1 loss: 1.139805  [   64/  118]
train() client id: f_00009-0-2 loss: 1.173561  [   96/  118]
train() client id: f_00009-1-0 loss: 1.097216  [   32/  118]
train() client id: f_00009-1-1 loss: 0.881264  [   64/  118]
train() client id: f_00009-1-2 loss: 1.167111  [   96/  118]
train() client id: f_00009-2-0 loss: 1.045622  [   32/  118]
train() client id: f_00009-2-1 loss: 0.953582  [   64/  118]
train() client id: f_00009-2-2 loss: 0.909854  [   96/  118]
train() client id: f_00009-3-0 loss: 0.879576  [   32/  118]
train() client id: f_00009-3-1 loss: 1.057730  [   64/  118]
train() client id: f_00009-3-2 loss: 0.898115  [   96/  118]
train() client id: f_00009-4-0 loss: 0.876216  [   32/  118]
train() client id: f_00009-4-1 loss: 1.040486  [   64/  118]
train() client id: f_00009-4-2 loss: 0.955175  [   96/  118]
train() client id: f_00009-5-0 loss: 0.910743  [   32/  118]
train() client id: f_00009-5-1 loss: 0.797074  [   64/  118]
train() client id: f_00009-5-2 loss: 0.978510  [   96/  118]
train() client id: f_00009-6-0 loss: 0.867879  [   32/  118]
train() client id: f_00009-6-1 loss: 0.971137  [   64/  118]
train() client id: f_00009-6-2 loss: 0.791753  [   96/  118]
train() client id: f_00009-7-0 loss: 0.818227  [   32/  118]
train() client id: f_00009-7-1 loss: 0.772165  [   64/  118]
train() client id: f_00009-7-2 loss: 1.044809  [   96/  118]
train() client id: f_00009-8-0 loss: 0.928079  [   32/  118]
train() client id: f_00009-8-1 loss: 0.898234  [   64/  118]
train() client id: f_00009-8-2 loss: 0.873116  [   96/  118]
train() client id: f_00009-9-0 loss: 0.867545  [   32/  118]
train() client id: f_00009-9-1 loss: 0.855269  [   64/  118]
train() client id: f_00009-9-2 loss: 0.888027  [   96/  118]
train() client id: f_00009-10-0 loss: 0.874669  [   32/  118]
train() client id: f_00009-10-1 loss: 0.839890  [   64/  118]
train() client id: f_00009-10-2 loss: 0.849593  [   96/  118]
train() client id: f_00009-11-0 loss: 0.810597  [   32/  118]
train() client id: f_00009-11-1 loss: 0.840091  [   64/  118]
train() client id: f_00009-11-2 loss: 0.716010  [   96/  118]
At round 33 accuracy: 0.6551724137931034
At round 33 training accuracy: 0.5848423876592891
At round 33 training loss: 0.8228266030581651
update_location
xs = [  -3.9056584     4.20031788  185.00902392   18.81129433    0.97929623
    3.95640986 -147.44319194 -126.32485185  169.66397685 -112.06087855]
ys = [ 177.5879595   160.55583871    1.32061395 -147.45517586  139.35018685
  122.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [203.84488595 189.197833   210.30949325 179.15605957 171.52094215
 158.42654071 178.17515369 161.11686582 197.72338004 150.24530728]
dists_bs = [171.11391311 179.78149994 399.71368058 376.21823221 179.55395256
 186.4101915  179.74986154 180.85111144 378.91658027 182.21368292]
uav_gains = [1.61237068e-11 1.99260114e-11 1.46504448e-11 2.30455348e-11
 2.58027311e-11 3.15864432e-11 2.33790442e-11 3.02686694e-11
 1.76243014e-11 3.61021161e-11]
bs_gains = [6.16730151e-11 5.37041796e-11 5.73319680e-12 6.79301444e-12
 5.38949620e-11 4.85265100e-11 5.37306513e-11 5.28195595e-11
 6.65843217e-12 5.17210489e-11]
Round 34
-------------------------------
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.50361085 13.46800316  6.40688418  2.3090212  15.53330799  7.47581585
  2.86265615  9.15122779  6.75143071  6.06373403]
obj_prev = 76.52569191747648
eta_min = 6.016067092016877e-15	eta_max = 0.9281867977986356
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 17.761433286506318	eta = 0.9090909090909091
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 32.349008668159414	eta = 0.4991422673511644
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 25.200112188715238	eta = 0.6407414940167677
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.908304317469916	eta = 0.6753618876010812
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.840631284811383	eta = 0.6772789419999331
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.84043047624284	eta = 0.6772846467381501
eta = 0.6772846467381501
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.03215411 0.06762571 0.03164373 0.01097323 0.07808855 0.03725794
 0.01378033 0.04567925 0.03317488 0.03011258]
ene_total = [2.12711913 3.82878864 2.11331163 1.00058971 4.36464773 2.28034368
 1.14291118 2.75181183 2.3215154  1.90939155]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 0 obj = 4.3589825705706815
eta = 0.6772846467381501
freqs = [35048280.46691417 70248512.05463123 34674068.18486581 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
eta_min = 0.677284646738166	eta_max = 0.6772846467381503
af = 0.012426086302136423	bf = 1.4018686060877306	zeta = 0.013668694932350066	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [2.41523466e-06 2.04068274e-05 2.32641206e-06 9.27160332e-08
 3.14130684e-05 3.43384418e-06 1.83392061e-07 6.53901742e-06
 3.00497385e-06 1.80578656e-06]
ene_total = [1.70271381 1.29453122 1.74651965 1.55598755 1.29560796 1.31845798
 1.55062412 1.46086543 2.20722871 1.30104589]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 1 obj = 4.358982570570683
eta = 0.6772846467381503
freqs = [35048280.46691417 70248512.05463123 34674068.18486582 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
Done!
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.28656765e-06 7.84641698e-05 8.94504507e-06 3.56492778e-07
 1.20783123e-04 1.32031172e-05 7.05141742e-07 2.51424959e-05
 1.15541125e-05 6.94324212e-06]
ene_total = [0.00936777 0.00717491 0.00960834 0.00855451 0.00721215 0.00725817
 0.00852528 0.0080499  0.01214308 0.00715781]
At round 34 energy consumption: 0.0850519238371433
At round 34 eta: 0.6772846467381503
At round 34 a_n: 16.536044059297588
At round 34 local rounds: 12.759567346547202
At round 34 global rounds: 51.24033886878731
gradient difference: 0.4920753538608551
train() client id: f_00000-0-0 loss: 1.092755  [   32/  126]
train() client id: f_00000-0-1 loss: 1.252588  [   64/  126]
train() client id: f_00000-0-2 loss: 1.253550  [   96/  126]
train() client id: f_00000-1-0 loss: 1.045141  [   32/  126]
train() client id: f_00000-1-1 loss: 1.348200  [   64/  126]
train() client id: f_00000-1-2 loss: 1.038465  [   96/  126]
train() client id: f_00000-2-0 loss: 1.118726  [   32/  126]
train() client id: f_00000-2-1 loss: 0.997786  [   64/  126]
train() client id: f_00000-2-2 loss: 0.986009  [   96/  126]
train() client id: f_00000-3-0 loss: 0.938926  [   32/  126]
train() client id: f_00000-3-1 loss: 0.975955  [   64/  126]
train() client id: f_00000-3-2 loss: 1.059123  [   96/  126]
train() client id: f_00000-4-0 loss: 1.000489  [   32/  126]
train() client id: f_00000-4-1 loss: 0.810032  [   64/  126]
train() client id: f_00000-4-2 loss: 1.052877  [   96/  126]
train() client id: f_00000-5-0 loss: 0.889132  [   32/  126]
train() client id: f_00000-5-1 loss: 0.970833  [   64/  126]
train() client id: f_00000-5-2 loss: 0.961552  [   96/  126]
train() client id: f_00000-6-0 loss: 0.822080  [   32/  126]
train() client id: f_00000-6-1 loss: 1.026118  [   64/  126]
train() client id: f_00000-6-2 loss: 0.899290  [   96/  126]
train() client id: f_00000-7-0 loss: 0.814977  [   32/  126]
train() client id: f_00000-7-1 loss: 0.862629  [   64/  126]
train() client id: f_00000-7-2 loss: 1.047892  [   96/  126]
train() client id: f_00000-8-0 loss: 0.824093  [   32/  126]
train() client id: f_00000-8-1 loss: 1.009553  [   64/  126]
train() client id: f_00000-8-2 loss: 0.936511  [   96/  126]
train() client id: f_00000-9-0 loss: 1.062350  [   32/  126]
train() client id: f_00000-9-1 loss: 0.903575  [   64/  126]
train() client id: f_00000-9-2 loss: 0.791964  [   96/  126]
train() client id: f_00000-10-0 loss: 0.945911  [   32/  126]
train() client id: f_00000-10-1 loss: 0.973869  [   64/  126]
train() client id: f_00000-10-2 loss: 0.909189  [   96/  126]
train() client id: f_00000-11-0 loss: 0.928714  [   32/  126]
train() client id: f_00000-11-1 loss: 0.921580  [   64/  126]
train() client id: f_00000-11-2 loss: 0.937226  [   96/  126]
train() client id: f_00001-0-0 loss: 0.577252  [   32/  265]
train() client id: f_00001-0-1 loss: 0.545389  [   64/  265]
train() client id: f_00001-0-2 loss: 0.503506  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444589  [  128/  265]
train() client id: f_00001-0-4 loss: 0.506397  [  160/  265]
train() client id: f_00001-0-5 loss: 0.656718  [  192/  265]
train() client id: f_00001-0-6 loss: 0.416342  [  224/  265]
train() client id: f_00001-0-7 loss: 0.424919  [  256/  265]
train() client id: f_00001-1-0 loss: 0.484339  [   32/  265]
train() client id: f_00001-1-1 loss: 0.534845  [   64/  265]
train() client id: f_00001-1-2 loss: 0.487239  [   96/  265]
train() client id: f_00001-1-3 loss: 0.495477  [  128/  265]
train() client id: f_00001-1-4 loss: 0.544170  [  160/  265]
train() client id: f_00001-1-5 loss: 0.400485  [  192/  265]
train() client id: f_00001-1-6 loss: 0.478341  [  224/  265]
train() client id: f_00001-1-7 loss: 0.527359  [  256/  265]
train() client id: f_00001-2-0 loss: 0.415901  [   32/  265]
train() client id: f_00001-2-1 loss: 0.437517  [   64/  265]
train() client id: f_00001-2-2 loss: 0.590703  [   96/  265]
train() client id: f_00001-2-3 loss: 0.497387  [  128/  265]
train() client id: f_00001-2-4 loss: 0.434270  [  160/  265]
train() client id: f_00001-2-5 loss: 0.485021  [  192/  265]
train() client id: f_00001-2-6 loss: 0.548465  [  224/  265]
train() client id: f_00001-2-7 loss: 0.587066  [  256/  265]
train() client id: f_00001-3-0 loss: 0.536502  [   32/  265]
train() client id: f_00001-3-1 loss: 0.393022  [   64/  265]
train() client id: f_00001-3-2 loss: 0.415291  [   96/  265]
train() client id: f_00001-3-3 loss: 0.454580  [  128/  265]
train() client id: f_00001-3-4 loss: 0.550852  [  160/  265]
train() client id: f_00001-3-5 loss: 0.530668  [  192/  265]
train() client id: f_00001-3-6 loss: 0.516009  [  224/  265]
train() client id: f_00001-3-7 loss: 0.482740  [  256/  265]
train() client id: f_00001-4-0 loss: 0.446861  [   32/  265]
train() client id: f_00001-4-1 loss: 0.475028  [   64/  265]
train() client id: f_00001-4-2 loss: 0.390919  [   96/  265]
train() client id: f_00001-4-3 loss: 0.569721  [  128/  265]
train() client id: f_00001-4-4 loss: 0.396113  [  160/  265]
train() client id: f_00001-4-5 loss: 0.487198  [  192/  265]
train() client id: f_00001-4-6 loss: 0.596165  [  224/  265]
train() client id: f_00001-4-7 loss: 0.543097  [  256/  265]
train() client id: f_00001-5-0 loss: 0.480739  [   32/  265]
train() client id: f_00001-5-1 loss: 0.558506  [   64/  265]
train() client id: f_00001-5-2 loss: 0.563741  [   96/  265]
train() client id: f_00001-5-3 loss: 0.461446  [  128/  265]
train() client id: f_00001-5-4 loss: 0.514852  [  160/  265]
train() client id: f_00001-5-5 loss: 0.459617  [  192/  265]
train() client id: f_00001-5-6 loss: 0.459222  [  224/  265]
train() client id: f_00001-5-7 loss: 0.436602  [  256/  265]
train() client id: f_00001-6-0 loss: 0.543451  [   32/  265]
train() client id: f_00001-6-1 loss: 0.496467  [   64/  265]
train() client id: f_00001-6-2 loss: 0.468572  [   96/  265]
train() client id: f_00001-6-3 loss: 0.491344  [  128/  265]
train() client id: f_00001-6-4 loss: 0.405899  [  160/  265]
train() client id: f_00001-6-5 loss: 0.547887  [  192/  265]
train() client id: f_00001-6-6 loss: 0.589030  [  224/  265]
train() client id: f_00001-6-7 loss: 0.388581  [  256/  265]
train() client id: f_00001-7-0 loss: 0.481661  [   32/  265]
train() client id: f_00001-7-1 loss: 0.400256  [   64/  265]
train() client id: f_00001-7-2 loss: 0.521583  [   96/  265]
train() client id: f_00001-7-3 loss: 0.542160  [  128/  265]
train() client id: f_00001-7-4 loss: 0.467471  [  160/  265]
train() client id: f_00001-7-5 loss: 0.484934  [  192/  265]
train() client id: f_00001-7-6 loss: 0.502053  [  224/  265]
train() client id: f_00001-7-7 loss: 0.523711  [  256/  265]
train() client id: f_00001-8-0 loss: 0.473634  [   32/  265]
train() client id: f_00001-8-1 loss: 0.528881  [   64/  265]
train() client id: f_00001-8-2 loss: 0.526995  [   96/  265]
train() client id: f_00001-8-3 loss: 0.483793  [  128/  265]
train() client id: f_00001-8-4 loss: 0.480346  [  160/  265]
train() client id: f_00001-8-5 loss: 0.504816  [  192/  265]
train() client id: f_00001-8-6 loss: 0.397484  [  224/  265]
train() client id: f_00001-8-7 loss: 0.499260  [  256/  265]
train() client id: f_00001-9-0 loss: 0.399930  [   32/  265]
train() client id: f_00001-9-1 loss: 0.384935  [   64/  265]
train() client id: f_00001-9-2 loss: 0.520211  [   96/  265]
train() client id: f_00001-9-3 loss: 0.524932  [  128/  265]
train() client id: f_00001-9-4 loss: 0.405273  [  160/  265]
train() client id: f_00001-9-5 loss: 0.495620  [  192/  265]
train() client id: f_00001-9-6 loss: 0.530561  [  224/  265]
train() client id: f_00001-9-7 loss: 0.592532  [  256/  265]
train() client id: f_00001-10-0 loss: 0.454417  [   32/  265]
train() client id: f_00001-10-1 loss: 0.393679  [   64/  265]
train() client id: f_00001-10-2 loss: 0.502103  [   96/  265]
train() client id: f_00001-10-3 loss: 0.626256  [  128/  265]
train() client id: f_00001-10-4 loss: 0.500871  [  160/  265]
train() client id: f_00001-10-5 loss: 0.402664  [  192/  265]
train() client id: f_00001-10-6 loss: 0.404498  [  224/  265]
train() client id: f_00001-10-7 loss: 0.637042  [  256/  265]
train() client id: f_00001-11-0 loss: 0.487535  [   32/  265]
train() client id: f_00001-11-1 loss: 0.595614  [   64/  265]
train() client id: f_00001-11-2 loss: 0.570898  [   96/  265]
train() client id: f_00001-11-3 loss: 0.416903  [  128/  265]
train() client id: f_00001-11-4 loss: 0.388041  [  160/  265]
train() client id: f_00001-11-5 loss: 0.567946  [  192/  265]
train() client id: f_00001-11-6 loss: 0.411811  [  224/  265]
train() client id: f_00001-11-7 loss: 0.487334  [  256/  265]
train() client id: f_00002-0-0 loss: 1.232280  [   32/  124]
train() client id: f_00002-0-1 loss: 0.980450  [   64/  124]
train() client id: f_00002-0-2 loss: 1.264020  [   96/  124]
train() client id: f_00002-1-0 loss: 1.193282  [   32/  124]
train() client id: f_00002-1-1 loss: 1.073049  [   64/  124]
train() client id: f_00002-1-2 loss: 0.997899  [   96/  124]
train() client id: f_00002-2-0 loss: 1.019057  [   32/  124]
train() client id: f_00002-2-1 loss: 1.154385  [   64/  124]
train() client id: f_00002-2-2 loss: 0.983659  [   96/  124]
train() client id: f_00002-3-0 loss: 1.004762  [   32/  124]
train() client id: f_00002-3-1 loss: 1.006872  [   64/  124]
train() client id: f_00002-3-2 loss: 1.032479  [   96/  124]
train() client id: f_00002-4-0 loss: 0.922917  [   32/  124]
train() client id: f_00002-4-1 loss: 1.051691  [   64/  124]
train() client id: f_00002-4-2 loss: 0.927693  [   96/  124]
train() client id: f_00002-5-0 loss: 0.860669  [   32/  124]
train() client id: f_00002-5-1 loss: 1.075709  [   64/  124]
train() client id: f_00002-5-2 loss: 0.952014  [   96/  124]
train() client id: f_00002-6-0 loss: 1.108045  [   32/  124]
train() client id: f_00002-6-1 loss: 0.857218  [   64/  124]
train() client id: f_00002-6-2 loss: 0.811066  [   96/  124]
train() client id: f_00002-7-0 loss: 0.959647  [   32/  124]
train() client id: f_00002-7-1 loss: 0.895467  [   64/  124]
train() client id: f_00002-7-2 loss: 0.884726  [   96/  124]
train() client id: f_00002-8-0 loss: 0.998659  [   32/  124]
train() client id: f_00002-8-1 loss: 0.894927  [   64/  124]
train() client id: f_00002-8-2 loss: 0.994866  [   96/  124]
train() client id: f_00002-9-0 loss: 0.916551  [   32/  124]
train() client id: f_00002-9-1 loss: 0.949637  [   64/  124]
train() client id: f_00002-9-2 loss: 0.732538  [   96/  124]
train() client id: f_00002-10-0 loss: 0.869841  [   32/  124]
train() client id: f_00002-10-1 loss: 1.133244  [   64/  124]
train() client id: f_00002-10-2 loss: 0.878397  [   96/  124]
train() client id: f_00002-11-0 loss: 0.827478  [   32/  124]
train() client id: f_00002-11-1 loss: 1.066691  [   64/  124]
train() client id: f_00002-11-2 loss: 0.826953  [   96/  124]
train() client id: f_00003-0-0 loss: 0.659155  [   32/   43]
train() client id: f_00003-1-0 loss: 0.519149  [   32/   43]
train() client id: f_00003-2-0 loss: 0.487589  [   32/   43]
train() client id: f_00003-3-0 loss: 0.397303  [   32/   43]
train() client id: f_00003-4-0 loss: 0.576027  [   32/   43]
train() client id: f_00003-5-0 loss: 0.432693  [   32/   43]
train() client id: f_00003-6-0 loss: 0.452046  [   32/   43]
train() client id: f_00003-7-0 loss: 0.631918  [   32/   43]
train() client id: f_00003-8-0 loss: 0.525268  [   32/   43]
train() client id: f_00003-9-0 loss: 0.596493  [   32/   43]
train() client id: f_00003-10-0 loss: 0.685069  [   32/   43]
train() client id: f_00003-11-0 loss: 0.497503  [   32/   43]
train() client id: f_00004-0-0 loss: 0.797900  [   32/  306]
train() client id: f_00004-0-1 loss: 0.901207  [   64/  306]
train() client id: f_00004-0-2 loss: 1.017707  [   96/  306]
train() client id: f_00004-0-3 loss: 0.944366  [  128/  306]
train() client id: f_00004-0-4 loss: 0.997018  [  160/  306]
train() client id: f_00004-0-5 loss: 0.795888  [  192/  306]
train() client id: f_00004-0-6 loss: 0.972233  [  224/  306]
train() client id: f_00004-0-7 loss: 0.988438  [  256/  306]
train() client id: f_00004-0-8 loss: 0.873303  [  288/  306]
train() client id: f_00004-1-0 loss: 0.965319  [   32/  306]
train() client id: f_00004-1-1 loss: 0.865393  [   64/  306]
train() client id: f_00004-1-2 loss: 0.903649  [   96/  306]
train() client id: f_00004-1-3 loss: 0.893880  [  128/  306]
train() client id: f_00004-1-4 loss: 0.996816  [  160/  306]
train() client id: f_00004-1-5 loss: 0.827654  [  192/  306]
train() client id: f_00004-1-6 loss: 0.980232  [  224/  306]
train() client id: f_00004-1-7 loss: 0.835772  [  256/  306]
train() client id: f_00004-1-8 loss: 0.921061  [  288/  306]
train() client id: f_00004-2-0 loss: 0.759702  [   32/  306]
train() client id: f_00004-2-1 loss: 0.887659  [   64/  306]
train() client id: f_00004-2-2 loss: 0.839727  [   96/  306]
train() client id: f_00004-2-3 loss: 0.842903  [  128/  306]
train() client id: f_00004-2-4 loss: 0.774050  [  160/  306]
train() client id: f_00004-2-5 loss: 0.990724  [  192/  306]
train() client id: f_00004-2-6 loss: 0.961340  [  224/  306]
train() client id: f_00004-2-7 loss: 1.112276  [  256/  306]
train() client id: f_00004-2-8 loss: 1.010962  [  288/  306]
train() client id: f_00004-3-0 loss: 0.785587  [   32/  306]
train() client id: f_00004-3-1 loss: 0.894522  [   64/  306]
train() client id: f_00004-3-2 loss: 0.980379  [   96/  306]
train() client id: f_00004-3-3 loss: 1.002035  [  128/  306]
train() client id: f_00004-3-4 loss: 0.849735  [  160/  306]
train() client id: f_00004-3-5 loss: 0.965220  [  192/  306]
train() client id: f_00004-3-6 loss: 0.781718  [  224/  306]
train() client id: f_00004-3-7 loss: 0.994022  [  256/  306]
train() client id: f_00004-3-8 loss: 1.026389  [  288/  306]
train() client id: f_00004-4-0 loss: 0.990645  [   32/  306]
train() client id: f_00004-4-1 loss: 0.827898  [   64/  306]
train() client id: f_00004-4-2 loss: 0.838963  [   96/  306]
train() client id: f_00004-4-3 loss: 0.923137  [  128/  306]
train() client id: f_00004-4-4 loss: 0.961569  [  160/  306]
train() client id: f_00004-4-5 loss: 1.055785  [  192/  306]
train() client id: f_00004-4-6 loss: 0.828856  [  224/  306]
train() client id: f_00004-4-7 loss: 0.804038  [  256/  306]
train() client id: f_00004-4-8 loss: 0.950200  [  288/  306]
train() client id: f_00004-5-0 loss: 1.011827  [   32/  306]
train() client id: f_00004-5-1 loss: 0.901755  [   64/  306]
train() client id: f_00004-5-2 loss: 0.889078  [   96/  306]
train() client id: f_00004-5-3 loss: 0.785323  [  128/  306]
train() client id: f_00004-5-4 loss: 1.109643  [  160/  306]
train() client id: f_00004-5-5 loss: 0.981311  [  192/  306]
train() client id: f_00004-5-6 loss: 0.960805  [  224/  306]
train() client id: f_00004-5-7 loss: 0.765406  [  256/  306]
train() client id: f_00004-5-8 loss: 0.859322  [  288/  306]
train() client id: f_00004-6-0 loss: 0.884461  [   32/  306]
train() client id: f_00004-6-1 loss: 0.799206  [   64/  306]
train() client id: f_00004-6-2 loss: 0.979263  [   96/  306]
train() client id: f_00004-6-3 loss: 0.949988  [  128/  306]
train() client id: f_00004-6-4 loss: 0.927456  [  160/  306]
train() client id: f_00004-6-5 loss: 0.887942  [  192/  306]
train() client id: f_00004-6-6 loss: 0.857524  [  224/  306]
train() client id: f_00004-6-7 loss: 1.005228  [  256/  306]
train() client id: f_00004-6-8 loss: 0.863919  [  288/  306]
train() client id: f_00004-7-0 loss: 0.878772  [   32/  306]
train() client id: f_00004-7-1 loss: 0.865290  [   64/  306]
train() client id: f_00004-7-2 loss: 1.000954  [   96/  306]
train() client id: f_00004-7-3 loss: 0.898629  [  128/  306]
train() client id: f_00004-7-4 loss: 0.934350  [  160/  306]
train() client id: f_00004-7-5 loss: 0.907305  [  192/  306]
train() client id: f_00004-7-6 loss: 0.890706  [  224/  306]
train() client id: f_00004-7-7 loss: 0.820633  [  256/  306]
train() client id: f_00004-7-8 loss: 0.919910  [  288/  306]
train() client id: f_00004-8-0 loss: 0.850105  [   32/  306]
train() client id: f_00004-8-1 loss: 0.889400  [   64/  306]
train() client id: f_00004-8-2 loss: 0.978337  [   96/  306]
train() client id: f_00004-8-3 loss: 0.832517  [  128/  306]
train() client id: f_00004-8-4 loss: 0.858322  [  160/  306]
train() client id: f_00004-8-5 loss: 0.877746  [  192/  306]
train() client id: f_00004-8-6 loss: 0.956393  [  224/  306]
train() client id: f_00004-8-7 loss: 1.052396  [  256/  306]
train() client id: f_00004-8-8 loss: 0.770475  [  288/  306]
train() client id: f_00004-9-0 loss: 0.990774  [   32/  306]
train() client id: f_00004-9-1 loss: 0.957630  [   64/  306]
train() client id: f_00004-9-2 loss: 0.843704  [   96/  306]
train() client id: f_00004-9-3 loss: 1.068127  [  128/  306]
train() client id: f_00004-9-4 loss: 0.747344  [  160/  306]
train() client id: f_00004-9-5 loss: 0.882457  [  192/  306]
train() client id: f_00004-9-6 loss: 0.940822  [  224/  306]
train() client id: f_00004-9-7 loss: 0.949583  [  256/  306]
train() client id: f_00004-9-8 loss: 0.870791  [  288/  306]
train() client id: f_00004-10-0 loss: 0.970731  [   32/  306]
train() client id: f_00004-10-1 loss: 0.931962  [   64/  306]
train() client id: f_00004-10-2 loss: 0.887523  [   96/  306]
train() client id: f_00004-10-3 loss: 0.918257  [  128/  306]
train() client id: f_00004-10-4 loss: 0.869876  [  160/  306]
train() client id: f_00004-10-5 loss: 0.854439  [  192/  306]
train() client id: f_00004-10-6 loss: 0.905899  [  224/  306]
train() client id: f_00004-10-7 loss: 0.966240  [  256/  306]
train() client id: f_00004-10-8 loss: 0.927802  [  288/  306]
train() client id: f_00004-11-0 loss: 0.853592  [   32/  306]
train() client id: f_00004-11-1 loss: 0.941692  [   64/  306]
train() client id: f_00004-11-2 loss: 0.864068  [   96/  306]
train() client id: f_00004-11-3 loss: 0.870570  [  128/  306]
train() client id: f_00004-11-4 loss: 0.930618  [  160/  306]
train() client id: f_00004-11-5 loss: 0.942878  [  192/  306]
train() client id: f_00004-11-6 loss: 0.903331  [  224/  306]
train() client id: f_00004-11-7 loss: 0.924688  [  256/  306]
train() client id: f_00004-11-8 loss: 0.979308  [  288/  306]
train() client id: f_00005-0-0 loss: 0.462398  [   32/  146]
train() client id: f_00005-0-1 loss: 0.910489  [   64/  146]
train() client id: f_00005-0-2 loss: 0.663840  [   96/  146]
train() client id: f_00005-0-3 loss: 0.549606  [  128/  146]
train() client id: f_00005-1-0 loss: 0.575946  [   32/  146]
train() client id: f_00005-1-1 loss: 0.707169  [   64/  146]
train() client id: f_00005-1-2 loss: 0.833899  [   96/  146]
train() client id: f_00005-1-3 loss: 0.593366  [  128/  146]
train() client id: f_00005-2-0 loss: 0.572100  [   32/  146]
train() client id: f_00005-2-1 loss: 0.591833  [   64/  146]
train() client id: f_00005-2-2 loss: 0.953750  [   96/  146]
train() client id: f_00005-2-3 loss: 0.734951  [  128/  146]
train() client id: f_00005-3-0 loss: 0.761818  [   32/  146]
train() client id: f_00005-3-1 loss: 0.723545  [   64/  146]
train() client id: f_00005-3-2 loss: 0.672339  [   96/  146]
train() client id: f_00005-3-3 loss: 0.622978  [  128/  146]
train() client id: f_00005-4-0 loss: 0.651732  [   32/  146]
train() client id: f_00005-4-1 loss: 0.665344  [   64/  146]
train() client id: f_00005-4-2 loss: 0.602322  [   96/  146]
train() client id: f_00005-4-3 loss: 0.779024  [  128/  146]
train() client id: f_00005-5-0 loss: 0.790539  [   32/  146]
train() client id: f_00005-5-1 loss: 0.544696  [   64/  146]
train() client id: f_00005-5-2 loss: 0.760281  [   96/  146]
train() client id: f_00005-5-3 loss: 0.765423  [  128/  146]
train() client id: f_00005-6-0 loss: 0.667694  [   32/  146]
train() client id: f_00005-6-1 loss: 0.836778  [   64/  146]
train() client id: f_00005-6-2 loss: 0.678074  [   96/  146]
train() client id: f_00005-6-3 loss: 0.463312  [  128/  146]
train() client id: f_00005-7-0 loss: 0.642758  [   32/  146]
train() client id: f_00005-7-1 loss: 0.684314  [   64/  146]
train() client id: f_00005-7-2 loss: 0.723206  [   96/  146]
train() client id: f_00005-7-3 loss: 0.515466  [  128/  146]
train() client id: f_00005-8-0 loss: 0.736263  [   32/  146]
train() client id: f_00005-8-1 loss: 0.517478  [   64/  146]
train() client id: f_00005-8-2 loss: 0.669466  [   96/  146]
train() client id: f_00005-8-3 loss: 0.834230  [  128/  146]
train() client id: f_00005-9-0 loss: 0.704305  [   32/  146]
train() client id: f_00005-9-1 loss: 0.388331  [   64/  146]
train() client id: f_00005-9-2 loss: 0.806461  [   96/  146]
train() client id: f_00005-9-3 loss: 0.639728  [  128/  146]
train() client id: f_00005-10-0 loss: 0.462744  [   32/  146]
train() client id: f_00005-10-1 loss: 0.574677  [   64/  146]
train() client id: f_00005-10-2 loss: 0.822382  [   96/  146]
train() client id: f_00005-10-3 loss: 0.731708  [  128/  146]
train() client id: f_00005-11-0 loss: 0.508377  [   32/  146]
train() client id: f_00005-11-1 loss: 1.008489  [   64/  146]
train() client id: f_00005-11-2 loss: 0.449284  [   96/  146]
train() client id: f_00005-11-3 loss: 0.509681  [  128/  146]
train() client id: f_00006-0-0 loss: 0.563418  [   32/   54]
train() client id: f_00006-1-0 loss: 0.520160  [   32/   54]
train() client id: f_00006-2-0 loss: 0.526031  [   32/   54]
train() client id: f_00006-3-0 loss: 0.512117  [   32/   54]
train() client id: f_00006-4-0 loss: 0.562035  [   32/   54]
train() client id: f_00006-5-0 loss: 0.553107  [   32/   54]
train() client id: f_00006-6-0 loss: 0.472861  [   32/   54]
train() client id: f_00006-7-0 loss: 0.567242  [   32/   54]
train() client id: f_00006-8-0 loss: 0.565616  [   32/   54]
train() client id: f_00006-9-0 loss: 0.565549  [   32/   54]
train() client id: f_00006-10-0 loss: 0.557292  [   32/   54]
train() client id: f_00006-11-0 loss: 0.454594  [   32/   54]
train() client id: f_00007-0-0 loss: 0.460902  [   32/  179]
train() client id: f_00007-0-1 loss: 0.365084  [   64/  179]
train() client id: f_00007-0-2 loss: 0.477829  [   96/  179]
train() client id: f_00007-0-3 loss: 0.520736  [  128/  179]
train() client id: f_00007-0-4 loss: 0.526890  [  160/  179]
train() client id: f_00007-1-0 loss: 0.513674  [   32/  179]
train() client id: f_00007-1-1 loss: 0.357083  [   64/  179]
train() client id: f_00007-1-2 loss: 0.448584  [   96/  179]
train() client id: f_00007-1-3 loss: 0.578206  [  128/  179]
train() client id: f_00007-1-4 loss: 0.398993  [  160/  179]
train() client id: f_00007-2-0 loss: 0.359934  [   32/  179]
train() client id: f_00007-2-1 loss: 0.515987  [   64/  179]
train() client id: f_00007-2-2 loss: 0.303353  [   96/  179]
train() client id: f_00007-2-3 loss: 0.355288  [  128/  179]
train() client id: f_00007-2-4 loss: 0.471624  [  160/  179]
train() client id: f_00007-3-0 loss: 0.293362  [   32/  179]
train() client id: f_00007-3-1 loss: 0.593822  [   64/  179]
train() client id: f_00007-3-2 loss: 0.516442  [   96/  179]
train() client id: f_00007-3-3 loss: 0.578621  [  128/  179]
train() client id: f_00007-3-4 loss: 0.319327  [  160/  179]
train() client id: f_00007-4-0 loss: 0.661140  [   32/  179]
train() client id: f_00007-4-1 loss: 0.448938  [   64/  179]
train() client id: f_00007-4-2 loss: 0.296463  [   96/  179]
train() client id: f_00007-4-3 loss: 0.322197  [  128/  179]
train() client id: f_00007-4-4 loss: 0.561450  [  160/  179]
train() client id: f_00007-5-0 loss: 0.390020  [   32/  179]
train() client id: f_00007-5-1 loss: 0.444464  [   64/  179]
train() client id: f_00007-5-2 loss: 0.321400  [   96/  179]
train() client id: f_00007-5-3 loss: 0.283140  [  128/  179]
train() client id: f_00007-5-4 loss: 0.479348  [  160/  179]
train() client id: f_00007-6-0 loss: 0.460045  [   32/  179]
train() client id: f_00007-6-1 loss: 0.618512  [   64/  179]
train() client id: f_00007-6-2 loss: 0.264348  [   96/  179]
train() client id: f_00007-6-3 loss: 0.501998  [  128/  179]
train() client id: f_00007-6-4 loss: 0.387941  [  160/  179]
train() client id: f_00007-7-0 loss: 0.501858  [   32/  179]
train() client id: f_00007-7-1 loss: 0.301378  [   64/  179]
train() client id: f_00007-7-2 loss: 0.353255  [   96/  179]
train() client id: f_00007-7-3 loss: 0.359156  [  128/  179]
train() client id: f_00007-7-4 loss: 0.498328  [  160/  179]
train() client id: f_00007-8-0 loss: 0.268280  [   32/  179]
train() client id: f_00007-8-1 loss: 0.410672  [   64/  179]
train() client id: f_00007-8-2 loss: 0.374632  [   96/  179]
train() client id: f_00007-8-3 loss: 0.376966  [  128/  179]
train() client id: f_00007-8-4 loss: 0.540160  [  160/  179]
train() client id: f_00007-9-0 loss: 0.498721  [   32/  179]
train() client id: f_00007-9-1 loss: 0.438308  [   64/  179]
train() client id: f_00007-9-2 loss: 0.470742  [   96/  179]
train() client id: f_00007-9-3 loss: 0.365699  [  128/  179]
train() client id: f_00007-9-4 loss: 0.428053  [  160/  179]
train() client id: f_00007-10-0 loss: 0.409379  [   32/  179]
train() client id: f_00007-10-1 loss: 0.392055  [   64/  179]
train() client id: f_00007-10-2 loss: 0.335614  [   96/  179]
train() client id: f_00007-10-3 loss: 0.453955  [  128/  179]
train() client id: f_00007-10-4 loss: 0.351406  [  160/  179]
train() client id: f_00007-11-0 loss: 0.261290  [   32/  179]
train() client id: f_00007-11-1 loss: 0.416015  [   64/  179]
train() client id: f_00007-11-2 loss: 0.445409  [   96/  179]
train() client id: f_00007-11-3 loss: 0.295730  [  128/  179]
train() client id: f_00007-11-4 loss: 0.640555  [  160/  179]
train() client id: f_00008-0-0 loss: 0.729598  [   32/  130]
train() client id: f_00008-0-1 loss: 0.804875  [   64/  130]
train() client id: f_00008-0-2 loss: 0.794332  [   96/  130]
train() client id: f_00008-0-3 loss: 0.734332  [  128/  130]
train() client id: f_00008-1-0 loss: 0.861411  [   32/  130]
train() client id: f_00008-1-1 loss: 0.763677  [   64/  130]
train() client id: f_00008-1-2 loss: 0.682333  [   96/  130]
train() client id: f_00008-1-3 loss: 0.735452  [  128/  130]
train() client id: f_00008-2-0 loss: 0.826236  [   32/  130]
train() client id: f_00008-2-1 loss: 0.771898  [   64/  130]
train() client id: f_00008-2-2 loss: 0.705858  [   96/  130]
train() client id: f_00008-2-3 loss: 0.762300  [  128/  130]
train() client id: f_00008-3-0 loss: 0.783172  [   32/  130]
train() client id: f_00008-3-1 loss: 0.671804  [   64/  130]
train() client id: f_00008-3-2 loss: 0.903482  [   96/  130]
train() client id: f_00008-3-3 loss: 0.689066  [  128/  130]
train() client id: f_00008-4-0 loss: 0.720945  [   32/  130]
train() client id: f_00008-4-1 loss: 0.771227  [   64/  130]
train() client id: f_00008-4-2 loss: 0.724059  [   96/  130]
train() client id: f_00008-4-3 loss: 0.826630  [  128/  130]
train() client id: f_00008-5-0 loss: 0.800510  [   32/  130]
train() client id: f_00008-5-1 loss: 0.802892  [   64/  130]
train() client id: f_00008-5-2 loss: 0.659501  [   96/  130]
train() client id: f_00008-5-3 loss: 0.801966  [  128/  130]
train() client id: f_00008-6-0 loss: 0.659556  [   32/  130]
train() client id: f_00008-6-1 loss: 0.710524  [   64/  130]
train() client id: f_00008-6-2 loss: 0.900660  [   96/  130]
train() client id: f_00008-6-3 loss: 0.799729  [  128/  130]
train() client id: f_00008-7-0 loss: 0.715530  [   32/  130]
train() client id: f_00008-7-1 loss: 0.713922  [   64/  130]
train() client id: f_00008-7-2 loss: 0.837331  [   96/  130]
train() client id: f_00008-7-3 loss: 0.794362  [  128/  130]
train() client id: f_00008-8-0 loss: 0.741968  [   32/  130]
train() client id: f_00008-8-1 loss: 0.756158  [   64/  130]
train() client id: f_00008-8-2 loss: 0.774861  [   96/  130]
train() client id: f_00008-8-3 loss: 0.786637  [  128/  130]
train() client id: f_00008-9-0 loss: 0.767724  [   32/  130]
train() client id: f_00008-9-1 loss: 0.748450  [   64/  130]
train() client id: f_00008-9-2 loss: 0.791053  [   96/  130]
train() client id: f_00008-9-3 loss: 0.757300  [  128/  130]
train() client id: f_00008-10-0 loss: 0.730759  [   32/  130]
train() client id: f_00008-10-1 loss: 0.781555  [   64/  130]
train() client id: f_00008-10-2 loss: 0.827338  [   96/  130]
train() client id: f_00008-10-3 loss: 0.729691  [  128/  130]
train() client id: f_00008-11-0 loss: 0.842351  [   32/  130]
train() client id: f_00008-11-1 loss: 0.684143  [   64/  130]
train() client id: f_00008-11-2 loss: 0.798144  [   96/  130]
train() client id: f_00008-11-3 loss: 0.741872  [  128/  130]
train() client id: f_00009-0-0 loss: 1.054463  [   32/  118]
train() client id: f_00009-0-1 loss: 1.176195  [   64/  118]
train() client id: f_00009-0-2 loss: 1.244881  [   96/  118]
train() client id: f_00009-1-0 loss: 0.978182  [   32/  118]
train() client id: f_00009-1-1 loss: 1.191403  [   64/  118]
train() client id: f_00009-1-2 loss: 1.168031  [   96/  118]
train() client id: f_00009-2-0 loss: 1.145971  [   32/  118]
train() client id: f_00009-2-1 loss: 1.130592  [   64/  118]
train() client id: f_00009-2-2 loss: 0.984713  [   96/  118]
train() client id: f_00009-3-0 loss: 0.959358  [   32/  118]
train() client id: f_00009-3-1 loss: 0.968230  [   64/  118]
train() client id: f_00009-3-2 loss: 1.024966  [   96/  118]
train() client id: f_00009-4-0 loss: 1.027290  [   32/  118]
train() client id: f_00009-4-1 loss: 0.969904  [   64/  118]
train() client id: f_00009-4-2 loss: 0.933158  [   96/  118]
train() client id: f_00009-5-0 loss: 0.942792  [   32/  118]
train() client id: f_00009-5-1 loss: 0.988752  [   64/  118]
train() client id: f_00009-5-2 loss: 1.086645  [   96/  118]
train() client id: f_00009-6-0 loss: 1.112731  [   32/  118]
train() client id: f_00009-6-1 loss: 0.887255  [   64/  118]
train() client id: f_00009-6-2 loss: 0.957674  [   96/  118]
train() client id: f_00009-7-0 loss: 1.049780  [   32/  118]
train() client id: f_00009-7-1 loss: 1.091709  [   64/  118]
train() client id: f_00009-7-2 loss: 0.881274  [   96/  118]
train() client id: f_00009-8-0 loss: 1.192151  [   32/  118]
train() client id: f_00009-8-1 loss: 0.838210  [   64/  118]
train() client id: f_00009-8-2 loss: 0.887053  [   96/  118]
train() client id: f_00009-9-0 loss: 0.961446  [   32/  118]
train() client id: f_00009-9-1 loss: 0.881755  [   64/  118]
train() client id: f_00009-9-2 loss: 0.944906  [   96/  118]
train() client id: f_00009-10-0 loss: 0.892806  [   32/  118]
train() client id: f_00009-10-1 loss: 0.857273  [   64/  118]
train() client id: f_00009-10-2 loss: 0.976585  [   96/  118]
train() client id: f_00009-11-0 loss: 1.016030  [   32/  118]
train() client id: f_00009-11-1 loss: 0.895796  [   64/  118]
train() client id: f_00009-11-2 loss: 0.862628  [   96/  118]
At round 34 accuracy: 0.6525198938992043
At round 34 training accuracy: 0.5875251509054326
At round 34 training loss: 0.8245793120123958
update_location
xs = [  -3.9056584     4.20031788  190.00902392   18.81129433    0.97929623
    3.95640986 -152.44319194 -131.32485185  174.66397685 -117.06087855]
ys = [ 182.5879595   165.55583871    1.32061395 -152.45517586  144.35018685
  127.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [208.21531433 193.45898377 214.72115217 183.29333169 175.60733317
 162.33333086 182.33435582 165.066329   202.03013335 154.01058776]
dists_bs = [171.26252029 179.44900699 404.22285522 380.51243082 178.62845167
 185.07269113 179.05151394 179.56996694 383.47022645 180.54759768]
uav_gains = [1.51163930e-11 1.87414015e-11 1.37010772e-11 2.16985605e-11
 2.42798461e-11 2.96968280e-11 2.20025346e-11 2.84631267e-11
 1.65569782e-11 3.39218710e-11]
bs_gains = [6.15232911e-11 5.39832614e-11 5.55591589e-12 6.58053658e-12
 5.46804790e-11 4.95148575e-11 5.43194911e-11 5.38815050e-11
 6.43940092e-12 5.30685578e-11]
Round 35
-------------------------------
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.3715199  13.18889052  6.27703667  2.2632532  15.21119563  7.32046252
  2.80542146  8.96352054  6.61377748  5.93751329]
obj_prev = 74.95259119677627
eta_min = 3.0936410554122904e-15	eta_max = 0.9287856551033156
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 17.39350337614215	eta = 0.9090909090909091
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 31.811065698035446	eta = 0.4970684084145405
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 24.73112036289277	eta = 0.6393675484357765
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.451279643601314	eta = 0.6742606815832007
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38394553843602	eta = 0.6762022161958229
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38374385979249	eta = 0.6762080482621735
eta = 0.6762080482621735
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.03228538 0.0679018  0.03177292 0.01101803 0.07840736 0.03741005
 0.01383659 0.04586574 0.03331032 0.03023552]
ene_total = [2.0907422  3.75044408 2.07789445 0.9851544  4.27493166 2.23178708
 1.1246433  2.70086556 2.2793763  1.86790483]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 0 obj = 4.291038018675944
eta = 0.6762080482621735
freqs = [34370497.98105174 68730577.98040815 34010002.78410535 11523739.23427672
 79334916.60468721 37963436.80656631 14462760.16316765 47433377.96218494
 37664422.59882781 30619785.51757285]
eta_min = 0.6762080482621772	eta_max = 0.6762080482621733
af = 0.011653184886415587	bf = 1.3848442141784587	zeta = 0.012818503375057147	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [2.32272370e-06 1.95344516e-05 2.23815612e-06 8.91067609e-08
 3.00542243e-05 3.28351691e-06 1.76259075e-07 6.28458343e-06
 2.87780483e-06 1.72640090e-06]
ene_total = [1.69608812 1.26623554 1.74159793 1.54622714 1.26484323 1.28577181
 1.54099513 1.45082842 2.18547748 1.2674389 ]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 1 obj = 4.291038018675939
eta = 0.6762080482621733
freqs = [34370497.98105174 68730577.98040818 34010002.78410535 11523739.23427672
 79334916.60468724 37963436.80656631 14462760.16316766 47433377.96218494
 37664422.59882782 30619785.51757286]
Done!
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [8.93086335e-06 7.51098884e-05 8.60570135e-06 3.42615140e-07
 1.15558372e-04 1.26251094e-05 6.77715438e-07 2.41641982e-05
 1.10651481e-05 6.63800456e-06]
ene_total = [0.00952832 0.00716413 0.00978357 0.00868066 0.00718624 0.00722757
 0.00865153 0.00816272 0.01227729 0.00712022]
At round 35 energy consumption: 0.0857822507966446
At round 35 eta: 0.6762080482621733
At round 35 a_n: 16.193498212328272
At round 35 local rounds: 12.811659702043181
At round 35 global rounds: 50.012046702878195
gradient difference: 0.48784446716308594
train() client id: f_00000-0-0 loss: 1.169176  [   32/  126]
train() client id: f_00000-0-1 loss: 1.273552  [   64/  126]
train() client id: f_00000-0-2 loss: 1.153098  [   96/  126]
train() client id: f_00000-1-0 loss: 1.204773  [   32/  126]
train() client id: f_00000-1-1 loss: 1.065880  [   64/  126]
train() client id: f_00000-1-2 loss: 0.948720  [   96/  126]
train() client id: f_00000-2-0 loss: 1.238106  [   32/  126]
train() client id: f_00000-2-1 loss: 0.923435  [   64/  126]
train() client id: f_00000-2-2 loss: 0.768884  [   96/  126]
train() client id: f_00000-3-0 loss: 0.885012  [   32/  126]
train() client id: f_00000-3-1 loss: 0.931280  [   64/  126]
train() client id: f_00000-3-2 loss: 0.974382  [   96/  126]
train() client id: f_00000-4-0 loss: 0.935818  [   32/  126]
train() client id: f_00000-4-1 loss: 1.016175  [   64/  126]
train() client id: f_00000-4-2 loss: 0.844205  [   96/  126]
train() client id: f_00000-5-0 loss: 0.882170  [   32/  126]
train() client id: f_00000-5-1 loss: 0.873390  [   64/  126]
train() client id: f_00000-5-2 loss: 0.857645  [   96/  126]
train() client id: f_00000-6-0 loss: 0.881091  [   32/  126]
train() client id: f_00000-6-1 loss: 0.801111  [   64/  126]
train() client id: f_00000-6-2 loss: 0.845222  [   96/  126]
train() client id: f_00000-7-0 loss: 0.773626  [   32/  126]
train() client id: f_00000-7-1 loss: 0.939433  [   64/  126]
train() client id: f_00000-7-2 loss: 0.771432  [   96/  126]
train() client id: f_00000-8-0 loss: 0.833493  [   32/  126]
train() client id: f_00000-8-1 loss: 0.884684  [   64/  126]
train() client id: f_00000-8-2 loss: 0.812542  [   96/  126]
train() client id: f_00000-9-0 loss: 0.875528  [   32/  126]
train() client id: f_00000-9-1 loss: 0.903197  [   64/  126]
train() client id: f_00000-9-2 loss: 0.778799  [   96/  126]
train() client id: f_00000-10-0 loss: 0.853056  [   32/  126]
train() client id: f_00000-10-1 loss: 0.775912  [   64/  126]
train() client id: f_00000-10-2 loss: 0.883619  [   96/  126]
train() client id: f_00000-11-0 loss: 0.882285  [   32/  126]
train() client id: f_00000-11-1 loss: 0.846450  [   64/  126]
train() client id: f_00000-11-2 loss: 0.852300  [   96/  126]
train() client id: f_00001-0-0 loss: 0.371536  [   32/  265]
train() client id: f_00001-0-1 loss: 0.490219  [   64/  265]
train() client id: f_00001-0-2 loss: 0.447657  [   96/  265]
train() client id: f_00001-0-3 loss: 0.492466  [  128/  265]
train() client id: f_00001-0-4 loss: 0.517492  [  160/  265]
train() client id: f_00001-0-5 loss: 0.461037  [  192/  265]
train() client id: f_00001-0-6 loss: 0.467520  [  224/  265]
train() client id: f_00001-0-7 loss: 0.352226  [  256/  265]
train() client id: f_00001-1-0 loss: 0.557755  [   32/  265]
train() client id: f_00001-1-1 loss: 0.364939  [   64/  265]
train() client id: f_00001-1-2 loss: 0.331684  [   96/  265]
train() client id: f_00001-1-3 loss: 0.546801  [  128/  265]
train() client id: f_00001-1-4 loss: 0.449469  [  160/  265]
train() client id: f_00001-1-5 loss: 0.420508  [  192/  265]
train() client id: f_00001-1-6 loss: 0.431128  [  224/  265]
train() client id: f_00001-1-7 loss: 0.396301  [  256/  265]
train() client id: f_00001-2-0 loss: 0.395658  [   32/  265]
train() client id: f_00001-2-1 loss: 0.398173  [   64/  265]
train() client id: f_00001-2-2 loss: 0.377819  [   96/  265]
train() client id: f_00001-2-3 loss: 0.447751  [  128/  265]
train() client id: f_00001-2-4 loss: 0.516272  [  160/  265]
train() client id: f_00001-2-5 loss: 0.459618  [  192/  265]
train() client id: f_00001-2-6 loss: 0.421075  [  224/  265]
train() client id: f_00001-2-7 loss: 0.438265  [  256/  265]
train() client id: f_00001-3-0 loss: 0.465957  [   32/  265]
train() client id: f_00001-3-1 loss: 0.433721  [   64/  265]
train() client id: f_00001-3-2 loss: 0.429366  [   96/  265]
train() client id: f_00001-3-3 loss: 0.338233  [  128/  265]
train() client id: f_00001-3-4 loss: 0.416331  [  160/  265]
train() client id: f_00001-3-5 loss: 0.382451  [  192/  265]
train() client id: f_00001-3-6 loss: 0.519491  [  224/  265]
train() client id: f_00001-3-7 loss: 0.432325  [  256/  265]
train() client id: f_00001-4-0 loss: 0.427256  [   32/  265]
train() client id: f_00001-4-1 loss: 0.485370  [   64/  265]
train() client id: f_00001-4-2 loss: 0.435904  [   96/  265]
train() client id: f_00001-4-3 loss: 0.411506  [  128/  265]
train() client id: f_00001-4-4 loss: 0.326877  [  160/  265]
train() client id: f_00001-4-5 loss: 0.337937  [  192/  265]
train() client id: f_00001-4-6 loss: 0.523350  [  224/  265]
train() client id: f_00001-4-7 loss: 0.399026  [  256/  265]
train() client id: f_00001-5-0 loss: 0.380201  [   32/  265]
train() client id: f_00001-5-1 loss: 0.367345  [   64/  265]
train() client id: f_00001-5-2 loss: 0.426262  [   96/  265]
train() client id: f_00001-5-3 loss: 0.390124  [  128/  265]
train() client id: f_00001-5-4 loss: 0.578748  [  160/  265]
train() client id: f_00001-5-5 loss: 0.480014  [  192/  265]
train() client id: f_00001-5-6 loss: 0.325521  [  224/  265]
train() client id: f_00001-5-7 loss: 0.385929  [  256/  265]
train() client id: f_00001-6-0 loss: 0.382761  [   32/  265]
train() client id: f_00001-6-1 loss: 0.412239  [   64/  265]
train() client id: f_00001-6-2 loss: 0.441708  [   96/  265]
train() client id: f_00001-6-3 loss: 0.342661  [  128/  265]
train() client id: f_00001-6-4 loss: 0.507171  [  160/  265]
train() client id: f_00001-6-5 loss: 0.433187  [  192/  265]
train() client id: f_00001-6-6 loss: 0.432717  [  224/  265]
train() client id: f_00001-6-7 loss: 0.369448  [  256/  265]
train() client id: f_00001-7-0 loss: 0.373135  [   32/  265]
train() client id: f_00001-7-1 loss: 0.516687  [   64/  265]
train() client id: f_00001-7-2 loss: 0.410678  [   96/  265]
train() client id: f_00001-7-3 loss: 0.318895  [  128/  265]
train() client id: f_00001-7-4 loss: 0.353502  [  160/  265]
train() client id: f_00001-7-5 loss: 0.456442  [  192/  265]
train() client id: f_00001-7-6 loss: 0.418513  [  224/  265]
train() client id: f_00001-7-7 loss: 0.369863  [  256/  265]
train() client id: f_00001-8-0 loss: 0.365127  [   32/  265]
train() client id: f_00001-8-1 loss: 0.434218  [   64/  265]
train() client id: f_00001-8-2 loss: 0.421017  [   96/  265]
train() client id: f_00001-8-3 loss: 0.416029  [  128/  265]
train() client id: f_00001-8-4 loss: 0.434844  [  160/  265]
train() client id: f_00001-8-5 loss: 0.404715  [  192/  265]
train() client id: f_00001-8-6 loss: 0.303777  [  224/  265]
train() client id: f_00001-8-7 loss: 0.489935  [  256/  265]
train() client id: f_00001-9-0 loss: 0.512689  [   32/  265]
train() client id: f_00001-9-1 loss: 0.375535  [   64/  265]
train() client id: f_00001-9-2 loss: 0.504626  [   96/  265]
train() client id: f_00001-9-3 loss: 0.420393  [  128/  265]
train() client id: f_00001-9-4 loss: 0.386511  [  160/  265]
train() client id: f_00001-9-5 loss: 0.356436  [  192/  265]
train() client id: f_00001-9-6 loss: 0.387217  [  224/  265]
train() client id: f_00001-9-7 loss: 0.313272  [  256/  265]
train() client id: f_00001-10-0 loss: 0.352273  [   32/  265]
train() client id: f_00001-10-1 loss: 0.385615  [   64/  265]
train() client id: f_00001-10-2 loss: 0.311609  [   96/  265]
train() client id: f_00001-10-3 loss: 0.558023  [  128/  265]
train() client id: f_00001-10-4 loss: 0.449114  [  160/  265]
train() client id: f_00001-10-5 loss: 0.417931  [  192/  265]
train() client id: f_00001-10-6 loss: 0.315615  [  224/  265]
train() client id: f_00001-10-7 loss: 0.375042  [  256/  265]
train() client id: f_00001-11-0 loss: 0.312379  [   32/  265]
train() client id: f_00001-11-1 loss: 0.372484  [   64/  265]
train() client id: f_00001-11-2 loss: 0.369403  [   96/  265]
train() client id: f_00001-11-3 loss: 0.495178  [  128/  265]
train() client id: f_00001-11-4 loss: 0.302633  [  160/  265]
train() client id: f_00001-11-5 loss: 0.618674  [  192/  265]
train() client id: f_00001-11-6 loss: 0.485378  [  224/  265]
train() client id: f_00001-11-7 loss: 0.305915  [  256/  265]
train() client id: f_00002-0-0 loss: 1.208698  [   32/  124]
train() client id: f_00002-0-1 loss: 1.153311  [   64/  124]
train() client id: f_00002-0-2 loss: 1.112275  [   96/  124]
train() client id: f_00002-1-0 loss: 1.229130  [   32/  124]
train() client id: f_00002-1-1 loss: 1.248620  [   64/  124]
train() client id: f_00002-1-2 loss: 1.055530  [   96/  124]
train() client id: f_00002-2-0 loss: 1.017792  [   32/  124]
train() client id: f_00002-2-1 loss: 1.133859  [   64/  124]
train() client id: f_00002-2-2 loss: 1.184213  [   96/  124]
train() client id: f_00002-3-0 loss: 1.112361  [   32/  124]
train() client id: f_00002-3-1 loss: 1.209563  [   64/  124]
train() client id: f_00002-3-2 loss: 1.061744  [   96/  124]
train() client id: f_00002-4-0 loss: 1.203890  [   32/  124]
train() client id: f_00002-4-1 loss: 1.172765  [   64/  124]
train() client id: f_00002-4-2 loss: 0.896333  [   96/  124]
train() client id: f_00002-5-0 loss: 1.112385  [   32/  124]
train() client id: f_00002-5-1 loss: 1.022253  [   64/  124]
train() client id: f_00002-5-2 loss: 1.154025  [   96/  124]
train() client id: f_00002-6-0 loss: 1.031749  [   32/  124]
train() client id: f_00002-6-1 loss: 1.275397  [   64/  124]
train() client id: f_00002-6-2 loss: 0.985890  [   96/  124]
train() client id: f_00002-7-0 loss: 1.282714  [   32/  124]
train() client id: f_00002-7-1 loss: 0.977427  [   64/  124]
train() client id: f_00002-7-2 loss: 0.882088  [   96/  124]
train() client id: f_00002-8-0 loss: 1.140211  [   32/  124]
train() client id: f_00002-8-1 loss: 1.057798  [   64/  124]
train() client id: f_00002-8-2 loss: 0.998533  [   96/  124]
train() client id: f_00002-9-0 loss: 1.120944  [   32/  124]
train() client id: f_00002-9-1 loss: 1.124519  [   64/  124]
train() client id: f_00002-9-2 loss: 0.944974  [   96/  124]
train() client id: f_00002-10-0 loss: 1.067659  [   32/  124]
train() client id: f_00002-10-1 loss: 0.967136  [   64/  124]
train() client id: f_00002-10-2 loss: 1.055777  [   96/  124]
train() client id: f_00002-11-0 loss: 1.013734  [   32/  124]
train() client id: f_00002-11-1 loss: 1.071733  [   64/  124]
train() client id: f_00002-11-2 loss: 1.114097  [   96/  124]
train() client id: f_00003-0-0 loss: 0.761256  [   32/   43]
train() client id: f_00003-1-0 loss: 0.818742  [   32/   43]
train() client id: f_00003-2-0 loss: 0.734569  [   32/   43]
train() client id: f_00003-3-0 loss: 0.759340  [   32/   43]
train() client id: f_00003-4-0 loss: 0.828086  [   32/   43]
train() client id: f_00003-5-0 loss: 0.842103  [   32/   43]
train() client id: f_00003-6-0 loss: 0.632052  [   32/   43]
train() client id: f_00003-7-0 loss: 0.741508  [   32/   43]
train() client id: f_00003-8-0 loss: 0.800089  [   32/   43]
train() client id: f_00003-9-0 loss: 0.829638  [   32/   43]
train() client id: f_00003-10-0 loss: 0.865287  [   32/   43]
train() client id: f_00003-11-0 loss: 0.743480  [   32/   43]
train() client id: f_00004-0-0 loss: 0.650985  [   32/  306]
train() client id: f_00004-0-1 loss: 0.722905  [   64/  306]
train() client id: f_00004-0-2 loss: 1.036432  [   96/  306]
train() client id: f_00004-0-3 loss: 0.694842  [  128/  306]
train() client id: f_00004-0-4 loss: 0.836993  [  160/  306]
train() client id: f_00004-0-5 loss: 0.759131  [  192/  306]
train() client id: f_00004-0-6 loss: 0.677359  [  224/  306]
train() client id: f_00004-0-7 loss: 0.869000  [  256/  306]
train() client id: f_00004-0-8 loss: 0.956724  [  288/  306]
train() client id: f_00004-1-0 loss: 0.800067  [   32/  306]
train() client id: f_00004-1-1 loss: 0.766049  [   64/  306]
train() client id: f_00004-1-2 loss: 0.790145  [   96/  306]
train() client id: f_00004-1-3 loss: 0.869201  [  128/  306]
train() client id: f_00004-1-4 loss: 0.792342  [  160/  306]
train() client id: f_00004-1-5 loss: 0.724158  [  192/  306]
train() client id: f_00004-1-6 loss: 0.751795  [  224/  306]
train() client id: f_00004-1-7 loss: 0.840127  [  256/  306]
train() client id: f_00004-1-8 loss: 0.826417  [  288/  306]
train() client id: f_00004-2-0 loss: 0.665946  [   32/  306]
train() client id: f_00004-2-1 loss: 0.949636  [   64/  306]
train() client id: f_00004-2-2 loss: 0.754759  [   96/  306]
train() client id: f_00004-2-3 loss: 0.802047  [  128/  306]
train() client id: f_00004-2-4 loss: 0.726137  [  160/  306]
train() client id: f_00004-2-5 loss: 0.763237  [  192/  306]
train() client id: f_00004-2-6 loss: 0.839073  [  224/  306]
train() client id: f_00004-2-7 loss: 0.774014  [  256/  306]
train() client id: f_00004-2-8 loss: 0.722172  [  288/  306]
train() client id: f_00004-3-0 loss: 0.890731  [   32/  306]
train() client id: f_00004-3-1 loss: 0.721718  [   64/  306]
train() client id: f_00004-3-2 loss: 0.762643  [   96/  306]
train() client id: f_00004-3-3 loss: 0.849885  [  128/  306]
train() client id: f_00004-3-4 loss: 0.833169  [  160/  306]
train() client id: f_00004-3-5 loss: 0.824978  [  192/  306]
train() client id: f_00004-3-6 loss: 0.709559  [  224/  306]
train() client id: f_00004-3-7 loss: 0.821917  [  256/  306]
train() client id: f_00004-3-8 loss: 0.672266  [  288/  306]
train() client id: f_00004-4-0 loss: 0.845882  [   32/  306]
train() client id: f_00004-4-1 loss: 0.681753  [   64/  306]
train() client id: f_00004-4-2 loss: 0.779786  [   96/  306]
train() client id: f_00004-4-3 loss: 0.794987  [  128/  306]
train() client id: f_00004-4-4 loss: 0.806729  [  160/  306]
train() client id: f_00004-4-5 loss: 0.813763  [  192/  306]
train() client id: f_00004-4-6 loss: 0.686609  [  224/  306]
train() client id: f_00004-4-7 loss: 0.844433  [  256/  306]
train() client id: f_00004-4-8 loss: 0.906573  [  288/  306]
train() client id: f_00004-5-0 loss: 0.874769  [   32/  306]
train() client id: f_00004-5-1 loss: 0.837089  [   64/  306]
train() client id: f_00004-5-2 loss: 0.910971  [   96/  306]
train() client id: f_00004-5-3 loss: 0.788841  [  128/  306]
train() client id: f_00004-5-4 loss: 0.777763  [  160/  306]
train() client id: f_00004-5-5 loss: 0.722534  [  192/  306]
train() client id: f_00004-5-6 loss: 0.770124  [  224/  306]
train() client id: f_00004-5-7 loss: 0.843607  [  256/  306]
train() client id: f_00004-5-8 loss: 0.641762  [  288/  306]
train() client id: f_00004-6-0 loss: 0.671979  [   32/  306]
train() client id: f_00004-6-1 loss: 0.757051  [   64/  306]
train() client id: f_00004-6-2 loss: 0.835530  [   96/  306]
train() client id: f_00004-6-3 loss: 0.761851  [  128/  306]
train() client id: f_00004-6-4 loss: 0.697893  [  160/  306]
train() client id: f_00004-6-5 loss: 0.837313  [  192/  306]
train() client id: f_00004-6-6 loss: 0.840654  [  224/  306]
train() client id: f_00004-6-7 loss: 0.705325  [  256/  306]
train() client id: f_00004-6-8 loss: 0.965836  [  288/  306]
train() client id: f_00004-7-0 loss: 0.758730  [   32/  306]
train() client id: f_00004-7-1 loss: 0.752712  [   64/  306]
train() client id: f_00004-7-2 loss: 0.758229  [   96/  306]
train() client id: f_00004-7-3 loss: 0.712455  [  128/  306]
train() client id: f_00004-7-4 loss: 0.789016  [  160/  306]
train() client id: f_00004-7-5 loss: 0.940734  [  192/  306]
train() client id: f_00004-7-6 loss: 0.861115  [  224/  306]
train() client id: f_00004-7-7 loss: 0.702422  [  256/  306]
train() client id: f_00004-7-8 loss: 0.778596  [  288/  306]
train() client id: f_00004-8-0 loss: 0.778633  [   32/  306]
train() client id: f_00004-8-1 loss: 0.948282  [   64/  306]
train() client id: f_00004-8-2 loss: 0.738611  [   96/  306]
train() client id: f_00004-8-3 loss: 0.622839  [  128/  306]
train() client id: f_00004-8-4 loss: 0.742475  [  160/  306]
train() client id: f_00004-8-5 loss: 0.725144  [  192/  306]
train() client id: f_00004-8-6 loss: 0.922120  [  224/  306]
train() client id: f_00004-8-7 loss: 0.747814  [  256/  306]
train() client id: f_00004-8-8 loss: 0.841176  [  288/  306]
train() client id: f_00004-9-0 loss: 0.726310  [   32/  306]
train() client id: f_00004-9-1 loss: 0.831536  [   64/  306]
train() client id: f_00004-9-2 loss: 0.818647  [   96/  306]
train() client id: f_00004-9-3 loss: 0.798240  [  128/  306]
train() client id: f_00004-9-4 loss: 0.772234  [  160/  306]
train() client id: f_00004-9-5 loss: 0.755831  [  192/  306]
train() client id: f_00004-9-6 loss: 0.780028  [  224/  306]
train() client id: f_00004-9-7 loss: 0.734840  [  256/  306]
train() client id: f_00004-9-8 loss: 0.872462  [  288/  306]
train() client id: f_00004-10-0 loss: 0.763565  [   32/  306]
train() client id: f_00004-10-1 loss: 0.756837  [   64/  306]
train() client id: f_00004-10-2 loss: 0.672466  [   96/  306]
train() client id: f_00004-10-3 loss: 0.969383  [  128/  306]
train() client id: f_00004-10-4 loss: 0.807825  [  160/  306]
train() client id: f_00004-10-5 loss: 0.707474  [  192/  306]
train() client id: f_00004-10-6 loss: 0.791922  [  224/  306]
train() client id: f_00004-10-7 loss: 0.817765  [  256/  306]
train() client id: f_00004-10-8 loss: 0.772004  [  288/  306]
train() client id: f_00004-11-0 loss: 0.703702  [   32/  306]
train() client id: f_00004-11-1 loss: 0.752115  [   64/  306]
train() client id: f_00004-11-2 loss: 0.626594  [   96/  306]
train() client id: f_00004-11-3 loss: 0.840737  [  128/  306]
train() client id: f_00004-11-4 loss: 0.744844  [  160/  306]
train() client id: f_00004-11-5 loss: 0.719798  [  192/  306]
train() client id: f_00004-11-6 loss: 0.876769  [  224/  306]
train() client id: f_00004-11-7 loss: 0.956084  [  256/  306]
train() client id: f_00004-11-8 loss: 0.840137  [  288/  306]
train() client id: f_00005-0-0 loss: 0.606977  [   32/  146]
train() client id: f_00005-0-1 loss: 0.986034  [   64/  146]
train() client id: f_00005-0-2 loss: 0.758466  [   96/  146]
train() client id: f_00005-0-3 loss: 0.863173  [  128/  146]
train() client id: f_00005-1-0 loss: 0.734621  [   32/  146]
train() client id: f_00005-1-1 loss: 0.786456  [   64/  146]
train() client id: f_00005-1-2 loss: 0.765415  [   96/  146]
train() client id: f_00005-1-3 loss: 0.761197  [  128/  146]
train() client id: f_00005-2-0 loss: 0.633376  [   32/  146]
train() client id: f_00005-2-1 loss: 0.769040  [   64/  146]
train() client id: f_00005-2-2 loss: 0.905148  [   96/  146]
train() client id: f_00005-2-3 loss: 0.685978  [  128/  146]
train() client id: f_00005-3-0 loss: 0.463153  [   32/  146]
train() client id: f_00005-3-1 loss: 0.699723  [   64/  146]
train() client id: f_00005-3-2 loss: 0.998479  [   96/  146]
train() client id: f_00005-3-3 loss: 0.744416  [  128/  146]
train() client id: f_00005-4-0 loss: 0.875580  [   32/  146]
train() client id: f_00005-4-1 loss: 0.570228  [   64/  146]
train() client id: f_00005-4-2 loss: 0.926509  [   96/  146]
train() client id: f_00005-4-3 loss: 0.780334  [  128/  146]
train() client id: f_00005-5-0 loss: 0.486234  [   32/  146]
train() client id: f_00005-5-1 loss: 0.810218  [   64/  146]
train() client id: f_00005-5-2 loss: 0.910815  [   96/  146]
train() client id: f_00005-5-3 loss: 0.889475  [  128/  146]
train() client id: f_00005-6-0 loss: 0.738364  [   32/  146]
train() client id: f_00005-6-1 loss: 0.864966  [   64/  146]
train() client id: f_00005-6-2 loss: 1.021495  [   96/  146]
train() client id: f_00005-6-3 loss: 0.654625  [  128/  146]
train() client id: f_00005-7-0 loss: 0.444968  [   32/  146]
train() client id: f_00005-7-1 loss: 0.748470  [   64/  146]
train() client id: f_00005-7-2 loss: 1.022380  [   96/  146]
train() client id: f_00005-7-3 loss: 0.727772  [  128/  146]
train() client id: f_00005-8-0 loss: 0.749854  [   32/  146]
train() client id: f_00005-8-1 loss: 0.726778  [   64/  146]
train() client id: f_00005-8-2 loss: 0.744721  [   96/  146]
train() client id: f_00005-8-3 loss: 0.687102  [  128/  146]
train() client id: f_00005-9-0 loss: 0.665466  [   32/  146]
train() client id: f_00005-9-1 loss: 1.011429  [   64/  146]
train() client id: f_00005-9-2 loss: 0.584763  [   96/  146]
train() client id: f_00005-9-3 loss: 0.790024  [  128/  146]
train() client id: f_00005-10-0 loss: 0.454435  [   32/  146]
train() client id: f_00005-10-1 loss: 1.186314  [   64/  146]
train() client id: f_00005-10-2 loss: 0.809108  [   96/  146]
train() client id: f_00005-10-3 loss: 0.695812  [  128/  146]
train() client id: f_00005-11-0 loss: 1.005578  [   32/  146]
train() client id: f_00005-11-1 loss: 0.819334  [   64/  146]
train() client id: f_00005-11-2 loss: 0.816905  [   96/  146]
train() client id: f_00005-11-3 loss: 0.560879  [  128/  146]
train() client id: f_00006-0-0 loss: 0.596016  [   32/   54]
train() client id: f_00006-1-0 loss: 0.619051  [   32/   54]
train() client id: f_00006-2-0 loss: 0.567164  [   32/   54]
train() client id: f_00006-3-0 loss: 0.575219  [   32/   54]
train() client id: f_00006-4-0 loss: 0.583085  [   32/   54]
train() client id: f_00006-5-0 loss: 0.566275  [   32/   54]
train() client id: f_00006-6-0 loss: 0.575497  [   32/   54]
train() client id: f_00006-7-0 loss: 0.580546  [   32/   54]
train() client id: f_00006-8-0 loss: 0.572588  [   32/   54]
train() client id: f_00006-9-0 loss: 0.573789  [   32/   54]
train() client id: f_00006-10-0 loss: 0.571236  [   32/   54]
train() client id: f_00006-11-0 loss: 0.628257  [   32/   54]
train() client id: f_00007-0-0 loss: 0.474253  [   32/  179]
train() client id: f_00007-0-1 loss: 0.424389  [   64/  179]
train() client id: f_00007-0-2 loss: 0.314820  [   96/  179]
train() client id: f_00007-0-3 loss: 0.453985  [  128/  179]
train() client id: f_00007-0-4 loss: 0.455464  [  160/  179]
train() client id: f_00007-1-0 loss: 0.391865  [   32/  179]
train() client id: f_00007-1-1 loss: 0.526570  [   64/  179]
train() client id: f_00007-1-2 loss: 0.338658  [   96/  179]
train() client id: f_00007-1-3 loss: 0.246163  [  128/  179]
train() client id: f_00007-1-4 loss: 0.402445  [  160/  179]
train() client id: f_00007-2-0 loss: 0.461613  [   32/  179]
train() client id: f_00007-2-1 loss: 0.365718  [   64/  179]
train() client id: f_00007-2-2 loss: 0.297796  [   96/  179]
train() client id: f_00007-2-3 loss: 0.471701  [  128/  179]
train() client id: f_00007-2-4 loss: 0.393700  [  160/  179]
train() client id: f_00007-3-0 loss: 0.385705  [   32/  179]
train() client id: f_00007-3-1 loss: 0.308549  [   64/  179]
train() client id: f_00007-3-2 loss: 0.202632  [   96/  179]
train() client id: f_00007-3-3 loss: 0.582733  [  128/  179]
train() client id: f_00007-3-4 loss: 0.428964  [  160/  179]
train() client id: f_00007-4-0 loss: 0.231958  [   32/  179]
train() client id: f_00007-4-1 loss: 0.499796  [   64/  179]
train() client id: f_00007-4-2 loss: 0.361135  [   96/  179]
train() client id: f_00007-4-3 loss: 0.248629  [  128/  179]
train() client id: f_00007-4-4 loss: 0.533426  [  160/  179]
train() client id: f_00007-5-0 loss: 0.334258  [   32/  179]
train() client id: f_00007-5-1 loss: 0.217894  [   64/  179]
train() client id: f_00007-5-2 loss: 0.269237  [   96/  179]
train() client id: f_00007-5-3 loss: 0.361965  [  128/  179]
train() client id: f_00007-5-4 loss: 0.528899  [  160/  179]
train() client id: f_00007-6-0 loss: 0.305713  [   32/  179]
train() client id: f_00007-6-1 loss: 0.337984  [   64/  179]
train() client id: f_00007-6-2 loss: 0.266293  [   96/  179]
train() client id: f_00007-6-3 loss: 0.485833  [  128/  179]
train() client id: f_00007-6-4 loss: 0.308898  [  160/  179]
train() client id: f_00007-7-0 loss: 0.258143  [   32/  179]
train() client id: f_00007-7-1 loss: 0.263348  [   64/  179]
train() client id: f_00007-7-2 loss: 0.266682  [   96/  179]
train() client id: f_00007-7-3 loss: 0.279640  [  128/  179]
train() client id: f_00007-7-4 loss: 0.514715  [  160/  179]
train() client id: f_00007-8-0 loss: 0.589173  [   32/  179]
train() client id: f_00007-8-1 loss: 0.430460  [   64/  179]
train() client id: f_00007-8-2 loss: 0.162712  [   96/  179]
train() client id: f_00007-8-3 loss: 0.176969  [  128/  179]
train() client id: f_00007-8-4 loss: 0.350986  [  160/  179]
train() client id: f_00007-9-0 loss: 0.376388  [   32/  179]
train() client id: f_00007-9-1 loss: 0.317958  [   64/  179]
train() client id: f_00007-9-2 loss: 0.319319  [   96/  179]
train() client id: f_00007-9-3 loss: 0.353695  [  128/  179]
train() client id: f_00007-9-4 loss: 0.170745  [  160/  179]
train() client id: f_00007-10-0 loss: 0.403264  [   32/  179]
train() client id: f_00007-10-1 loss: 0.196870  [   64/  179]
train() client id: f_00007-10-2 loss: 0.599250  [   96/  179]
train() client id: f_00007-10-3 loss: 0.184519  [  128/  179]
train() client id: f_00007-10-4 loss: 0.296167  [  160/  179]
train() client id: f_00007-11-0 loss: 0.271108  [   32/  179]
train() client id: f_00007-11-1 loss: 0.187655  [   64/  179]
train() client id: f_00007-11-2 loss: 0.667825  [   96/  179]
train() client id: f_00007-11-3 loss: 0.363701  [  128/  179]
train() client id: f_00007-11-4 loss: 0.161469  [  160/  179]
train() client id: f_00008-0-0 loss: 0.701282  [   32/  130]
train() client id: f_00008-0-1 loss: 0.678422  [   64/  130]
train() client id: f_00008-0-2 loss: 0.741213  [   96/  130]
train() client id: f_00008-0-3 loss: 0.815317  [  128/  130]
train() client id: f_00008-1-0 loss: 0.719777  [   32/  130]
train() client id: f_00008-1-1 loss: 0.786559  [   64/  130]
train() client id: f_00008-1-2 loss: 0.706446  [   96/  130]
train() client id: f_00008-1-3 loss: 0.806302  [  128/  130]
train() client id: f_00008-2-0 loss: 0.864322  [   32/  130]
train() client id: f_00008-2-1 loss: 0.851409  [   64/  130]
train() client id: f_00008-2-2 loss: 0.736737  [   96/  130]
train() client id: f_00008-2-3 loss: 0.564336  [  128/  130]
train() client id: f_00008-3-0 loss: 0.674602  [   32/  130]
train() client id: f_00008-3-1 loss: 0.728025  [   64/  130]
train() client id: f_00008-3-2 loss: 0.846110  [   96/  130]
train() client id: f_00008-3-3 loss: 0.735750  [  128/  130]
train() client id: f_00008-4-0 loss: 0.693726  [   32/  130]
train() client id: f_00008-4-1 loss: 0.815878  [   64/  130]
train() client id: f_00008-4-2 loss: 0.742441  [   96/  130]
train() client id: f_00008-4-3 loss: 0.713248  [  128/  130]
train() client id: f_00008-5-0 loss: 0.767669  [   32/  130]
train() client id: f_00008-5-1 loss: 0.731914  [   64/  130]
train() client id: f_00008-5-2 loss: 0.713564  [   96/  130]
train() client id: f_00008-5-3 loss: 0.807836  [  128/  130]
train() client id: f_00008-6-0 loss: 0.906211  [   32/  130]
train() client id: f_00008-6-1 loss: 0.618943  [   64/  130]
train() client id: f_00008-6-2 loss: 0.844966  [   96/  130]
train() client id: f_00008-6-3 loss: 0.615588  [  128/  130]
train() client id: f_00008-7-0 loss: 0.679035  [   32/  130]
train() client id: f_00008-7-1 loss: 0.783273  [   64/  130]
train() client id: f_00008-7-2 loss: 0.863944  [   96/  130]
train() client id: f_00008-7-3 loss: 0.698399  [  128/  130]
train() client id: f_00008-8-0 loss: 0.802193  [   32/  130]
train() client id: f_00008-8-1 loss: 0.699105  [   64/  130]
train() client id: f_00008-8-2 loss: 0.705380  [   96/  130]
train() client id: f_00008-8-3 loss: 0.819970  [  128/  130]
train() client id: f_00008-9-0 loss: 0.681150  [   32/  130]
train() client id: f_00008-9-1 loss: 0.914121  [   64/  130]
train() client id: f_00008-9-2 loss: 0.732962  [   96/  130]
train() client id: f_00008-9-3 loss: 0.691355  [  128/  130]
train() client id: f_00008-10-0 loss: 0.756165  [   32/  130]
train() client id: f_00008-10-1 loss: 0.747885  [   64/  130]
train() client id: f_00008-10-2 loss: 0.784678  [   96/  130]
train() client id: f_00008-10-3 loss: 0.721316  [  128/  130]
train() client id: f_00008-11-0 loss: 0.720306  [   32/  130]
train() client id: f_00008-11-1 loss: 0.736428  [   64/  130]
train() client id: f_00008-11-2 loss: 0.823466  [   96/  130]
train() client id: f_00008-11-3 loss: 0.748083  [  128/  130]
train() client id: f_00009-0-0 loss: 1.141393  [   32/  118]
train() client id: f_00009-0-1 loss: 1.079728  [   64/  118]
train() client id: f_00009-0-2 loss: 1.121185  [   96/  118]
train() client id: f_00009-1-0 loss: 1.203353  [   32/  118]
train() client id: f_00009-1-1 loss: 0.916377  [   64/  118]
train() client id: f_00009-1-2 loss: 0.949998  [   96/  118]
train() client id: f_00009-2-0 loss: 0.901563  [   32/  118]
train() client id: f_00009-2-1 loss: 1.102127  [   64/  118]
train() client id: f_00009-2-2 loss: 0.900356  [   96/  118]
train() client id: f_00009-3-0 loss: 0.962448  [   32/  118]
train() client id: f_00009-3-1 loss: 0.952562  [   64/  118]
train() client id: f_00009-3-2 loss: 1.043973  [   96/  118]
train() client id: f_00009-4-0 loss: 0.946619  [   32/  118]
train() client id: f_00009-4-1 loss: 0.983439  [   64/  118]
train() client id: f_00009-4-2 loss: 0.975781  [   96/  118]
train() client id: f_00009-5-0 loss: 0.904813  [   32/  118]
train() client id: f_00009-5-1 loss: 0.987475  [   64/  118]
train() client id: f_00009-5-2 loss: 0.972608  [   96/  118]
train() client id: f_00009-6-0 loss: 0.954591  [   32/  118]
train() client id: f_00009-6-1 loss: 0.898506  [   64/  118]
train() client id: f_00009-6-2 loss: 0.958614  [   96/  118]
train() client id: f_00009-7-0 loss: 0.934791  [   32/  118]
train() client id: f_00009-7-1 loss: 0.869139  [   64/  118]
train() client id: f_00009-7-2 loss: 0.872249  [   96/  118]
train() client id: f_00009-8-0 loss: 0.959947  [   32/  118]
train() client id: f_00009-8-1 loss: 0.985914  [   64/  118]
train() client id: f_00009-8-2 loss: 0.819307  [   96/  118]
train() client id: f_00009-9-0 loss: 1.048034  [   32/  118]
train() client id: f_00009-9-1 loss: 0.852729  [   64/  118]
train() client id: f_00009-9-2 loss: 0.833604  [   96/  118]
train() client id: f_00009-10-0 loss: 0.794053  [   32/  118]
train() client id: f_00009-10-1 loss: 1.011816  [   64/  118]
train() client id: f_00009-10-2 loss: 0.980855  [   96/  118]
train() client id: f_00009-11-0 loss: 1.066340  [   32/  118]
train() client id: f_00009-11-1 loss: 0.755325  [   64/  118]
train() client id: f_00009-11-2 loss: 0.958580  [   96/  118]
At round 35 accuracy: 0.6525198938992043
At round 35 training accuracy: 0.5835010060362174
At round 35 training loss: 0.8363832838877473
update_location
xs = [  -3.9056584     4.20031788  195.00902392   18.81129433    0.97929623
    3.95640986 -157.44319194 -136.32485185  179.66397685 -122.06087855]
ys = [ 187.5879595   170.55583871    1.32061395 -157.45517586  149.35018685
  132.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [212.61349138 197.75473898 219.15807863 187.47265721 179.7399158
 166.29868255 186.53484723 169.07140943 206.36815295 157.84444852]
dists_bs = [171.55678491 179.25541692 408.74344878 384.8236762  177.83876297
 183.86147641 178.49055035 178.41979023 388.03486227 179.00570888]
uav_gains = [1.41493817e-11 1.76163185e-11 1.27867778e-11 2.04271955e-11
 2.28496909e-11 2.79287797e-11 2.07052398e-11 2.67731890e-11
 1.55361629e-11 3.18816260e-11]
bs_gains = [6.12282672e-11 5.41466606e-11 5.38557194e-12 6.37618765e-12
 5.53630598e-11 5.04336040e-11 5.47988501e-11 5.48597238e-11
 6.22954026e-12 5.43584189e-11]
Round 36
-------------------------------
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.2394703  12.90983275  6.14725815  2.21746141 14.88914632  7.16517573
  2.74816098  8.77577622  6.47605413  5.8113634 ]
obj_prev = 73.37969937876653
eta_min = 1.5469793420595954e-15	eta_max = 0.9294112566599255
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 17.025573465777978	eta = 0.9090909090909091
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 31.275397678211032	eta = 0.4948872023642162
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 24.26289815788516	eta = 0.6379202500492736
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.99478274194065	eta = 0.6731002520657826
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927768157082628	eta = 0.6750676277672019
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927565477060664	eta = 0.6750735953751348
eta = 0.6750735953751348
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.03242394 0.0681932  0.03190927 0.01106531 0.07874384 0.03757059
 0.01389597 0.04606258 0.03345327 0.03036527]
ene_total = [2.05444491 3.67229692 2.04263567 0.969603   4.18544742 2.18345709
 1.10625446 2.64978349 2.23698535 1.82665715]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 0 obj = 4.223466627519707
eta = 0.6750735953751348
freqs = [33693065.79010019 67222802.04491435 33347173.22625211 11292050.13707799
 77574909.50601687 37111387.37372465 14172192.11427163 46479487.54198894
 36839485.23777357 29929879.4291095 ]
eta_min = 0.6750735953751431	eta_max = 0.675073595375134
af = 0.01091581578850546	bf = 1.3680266359327833	zeta = 0.012007397367356006	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [2.23206564e-06 1.86867792e-05 2.15176626e-06 8.55597305e-08
 2.87355384e-05 3.13778074e-06 1.69247857e-07 6.03435770e-06
 2.75312427e-06 1.64948093e-06]
ene_total = [1.68991368 1.23854826 1.73742856 1.53620773 1.23478239 1.25381221
 1.53108527 1.44039041 2.16296411 1.23460465]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 1 obj = 4.223466627519696
eta = 0.675073595375134
freqs = [33693065.7901002  67222802.04491436 33347173.22625211 11292050.13707799
 77574909.50601688 37111387.37372466 14172192.11427163 46479487.54198895
 36839485.23777357 29929879.42910951]
Done!
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [8.58228348e-06 7.18505913e-05 8.27353268e-06 3.28976822e-07
 1.10488030e-04 1.20647544e-05 6.50757335e-07 2.32020813e-05
 1.05857518e-05 6.34224759e-06]
ene_total = [0.00969842 0.00715654 0.0099707  0.00881077 0.00716353 0.00719985
 0.00878163 0.00827816 0.01241296 0.00708545]
At round 36 energy consumption: 0.08655801199959033
At round 36 eta: 0.675073595375134
At round 36 a_n: 15.850952365358953
At round 36 local rounds: 12.866641222936334
At round 36 global rounds: 48.78320795030245
gradient difference: 0.4420577883720398
train() client id: f_00000-0-0 loss: 1.333291  [   32/  126]
train() client id: f_00000-0-1 loss: 1.174116  [   64/  126]
train() client id: f_00000-0-2 loss: 1.138231  [   96/  126]
train() client id: f_00000-1-0 loss: 1.323943  [   32/  126]
train() client id: f_00000-1-1 loss: 1.055435  [   64/  126]
train() client id: f_00000-1-2 loss: 0.933814  [   96/  126]
train() client id: f_00000-2-0 loss: 1.142465  [   32/  126]
train() client id: f_00000-2-1 loss: 0.971685  [   64/  126]
train() client id: f_00000-2-2 loss: 0.895794  [   96/  126]
train() client id: f_00000-3-0 loss: 1.012542  [   32/  126]
train() client id: f_00000-3-1 loss: 0.904636  [   64/  126]
train() client id: f_00000-3-2 loss: 1.032153  [   96/  126]
train() client id: f_00000-4-0 loss: 0.856900  [   32/  126]
train() client id: f_00000-4-1 loss: 0.849801  [   64/  126]
train() client id: f_00000-4-2 loss: 0.950021  [   96/  126]
train() client id: f_00000-5-0 loss: 0.804354  [   32/  126]
train() client id: f_00000-5-1 loss: 0.959637  [   64/  126]
train() client id: f_00000-5-2 loss: 0.970669  [   96/  126]
train() client id: f_00000-6-0 loss: 0.834434  [   32/  126]
train() client id: f_00000-6-1 loss: 0.949545  [   64/  126]
train() client id: f_00000-6-2 loss: 0.777919  [   96/  126]
train() client id: f_00000-7-0 loss: 0.851427  [   32/  126]
train() client id: f_00000-7-1 loss: 0.960900  [   64/  126]
train() client id: f_00000-7-2 loss: 0.823380  [   96/  126]
train() client id: f_00000-8-0 loss: 0.872774  [   32/  126]
train() client id: f_00000-8-1 loss: 0.839194  [   64/  126]
train() client id: f_00000-8-2 loss: 0.814056  [   96/  126]
train() client id: f_00000-9-0 loss: 0.776359  [   32/  126]
train() client id: f_00000-9-1 loss: 0.830801  [   64/  126]
train() client id: f_00000-9-2 loss: 0.779416  [   96/  126]
train() client id: f_00000-10-0 loss: 0.961193  [   32/  126]
train() client id: f_00000-10-1 loss: 0.796990  [   64/  126]
train() client id: f_00000-10-2 loss: 0.831736  [   96/  126]
train() client id: f_00000-11-0 loss: 0.729691  [   32/  126]
train() client id: f_00000-11-1 loss: 0.842294  [   64/  126]
train() client id: f_00000-11-2 loss: 0.988754  [   96/  126]
train() client id: f_00001-0-0 loss: 0.471622  [   32/  265]
train() client id: f_00001-0-1 loss: 0.533927  [   64/  265]
train() client id: f_00001-0-2 loss: 0.628408  [   96/  265]
train() client id: f_00001-0-3 loss: 0.531912  [  128/  265]
train() client id: f_00001-0-4 loss: 0.471060  [  160/  265]
train() client id: f_00001-0-5 loss: 0.454995  [  192/  265]
train() client id: f_00001-0-6 loss: 0.446779  [  224/  265]
train() client id: f_00001-0-7 loss: 0.450975  [  256/  265]
train() client id: f_00001-1-0 loss: 0.448727  [   32/  265]
train() client id: f_00001-1-1 loss: 0.409442  [   64/  265]
train() client id: f_00001-1-2 loss: 0.431537  [   96/  265]
train() client id: f_00001-1-3 loss: 0.478973  [  128/  265]
train() client id: f_00001-1-4 loss: 0.420135  [  160/  265]
train() client id: f_00001-1-5 loss: 0.522157  [  192/  265]
train() client id: f_00001-1-6 loss: 0.702018  [  224/  265]
train() client id: f_00001-1-7 loss: 0.600831  [  256/  265]
train() client id: f_00001-2-0 loss: 0.465720  [   32/  265]
train() client id: f_00001-2-1 loss: 0.575346  [   64/  265]
train() client id: f_00001-2-2 loss: 0.409878  [   96/  265]
train() client id: f_00001-2-3 loss: 0.568277  [  128/  265]
train() client id: f_00001-2-4 loss: 0.519297  [  160/  265]
train() client id: f_00001-2-5 loss: 0.551352  [  192/  265]
train() client id: f_00001-2-6 loss: 0.448398  [  224/  265]
train() client id: f_00001-2-7 loss: 0.421807  [  256/  265]
train() client id: f_00001-3-0 loss: 0.671964  [   32/  265]
train() client id: f_00001-3-1 loss: 0.601318  [   64/  265]
train() client id: f_00001-3-2 loss: 0.523355  [   96/  265]
train() client id: f_00001-3-3 loss: 0.416753  [  128/  265]
train() client id: f_00001-3-4 loss: 0.449354  [  160/  265]
train() client id: f_00001-3-5 loss: 0.392893  [  192/  265]
train() client id: f_00001-3-6 loss: 0.477987  [  224/  265]
train() client id: f_00001-3-7 loss: 0.406325  [  256/  265]
train() client id: f_00001-4-0 loss: 0.477743  [   32/  265]
train() client id: f_00001-4-1 loss: 0.533545  [   64/  265]
train() client id: f_00001-4-2 loss: 0.490242  [   96/  265]
train() client id: f_00001-4-3 loss: 0.479142  [  128/  265]
train() client id: f_00001-4-4 loss: 0.516033  [  160/  265]
train() client id: f_00001-4-5 loss: 0.452565  [  192/  265]
train() client id: f_00001-4-6 loss: 0.417255  [  224/  265]
train() client id: f_00001-4-7 loss: 0.545822  [  256/  265]
train() client id: f_00001-5-0 loss: 0.447090  [   32/  265]
train() client id: f_00001-5-1 loss: 0.415123  [   64/  265]
train() client id: f_00001-5-2 loss: 0.512754  [   96/  265]
train() client id: f_00001-5-3 loss: 0.463218  [  128/  265]
train() client id: f_00001-5-4 loss: 0.593955  [  160/  265]
train() client id: f_00001-5-5 loss: 0.526858  [  192/  265]
train() client id: f_00001-5-6 loss: 0.457556  [  224/  265]
train() client id: f_00001-5-7 loss: 0.493891  [  256/  265]
train() client id: f_00001-6-0 loss: 0.538772  [   32/  265]
train() client id: f_00001-6-1 loss: 0.541211  [   64/  265]
train() client id: f_00001-6-2 loss: 0.516141  [   96/  265]
train() client id: f_00001-6-3 loss: 0.506043  [  128/  265]
train() client id: f_00001-6-4 loss: 0.390221  [  160/  265]
train() client id: f_00001-6-5 loss: 0.499040  [  192/  265]
train() client id: f_00001-6-6 loss: 0.460568  [  224/  265]
train() client id: f_00001-6-7 loss: 0.463296  [  256/  265]
train() client id: f_00001-7-0 loss: 0.392681  [   32/  265]
train() client id: f_00001-7-1 loss: 0.477878  [   64/  265]
train() client id: f_00001-7-2 loss: 0.472191  [   96/  265]
train() client id: f_00001-7-3 loss: 0.507514  [  128/  265]
train() client id: f_00001-7-4 loss: 0.460509  [  160/  265]
train() client id: f_00001-7-5 loss: 0.523526  [  192/  265]
train() client id: f_00001-7-6 loss: 0.554337  [  224/  265]
train() client id: f_00001-7-7 loss: 0.463816  [  256/  265]
train() client id: f_00001-8-0 loss: 0.717586  [   32/  265]
train() client id: f_00001-8-1 loss: 0.439052  [   64/  265]
train() client id: f_00001-8-2 loss: 0.413408  [   96/  265]
train() client id: f_00001-8-3 loss: 0.406206  [  128/  265]
train() client id: f_00001-8-4 loss: 0.437850  [  160/  265]
train() client id: f_00001-8-5 loss: 0.543038  [  192/  265]
train() client id: f_00001-8-6 loss: 0.510520  [  224/  265]
train() client id: f_00001-8-7 loss: 0.444701  [  256/  265]
train() client id: f_00001-9-0 loss: 0.480213  [   32/  265]
train() client id: f_00001-9-1 loss: 0.510664  [   64/  265]
train() client id: f_00001-9-2 loss: 0.473088  [   96/  265]
train() client id: f_00001-9-3 loss: 0.491279  [  128/  265]
train() client id: f_00001-9-4 loss: 0.486443  [  160/  265]
train() client id: f_00001-9-5 loss: 0.501482  [  192/  265]
train() client id: f_00001-9-6 loss: 0.426023  [  224/  265]
train() client id: f_00001-9-7 loss: 0.549686  [  256/  265]
train() client id: f_00001-10-0 loss: 0.389779  [   32/  265]
train() client id: f_00001-10-1 loss: 0.588487  [   64/  265]
train() client id: f_00001-10-2 loss: 0.391759  [   96/  265]
train() client id: f_00001-10-3 loss: 0.445171  [  128/  265]
train() client id: f_00001-10-4 loss: 0.569594  [  160/  265]
train() client id: f_00001-10-5 loss: 0.677771  [  192/  265]
train() client id: f_00001-10-6 loss: 0.401092  [  224/  265]
train() client id: f_00001-10-7 loss: 0.451264  [  256/  265]
train() client id: f_00001-11-0 loss: 0.500410  [   32/  265]
train() client id: f_00001-11-1 loss: 0.402272  [   64/  265]
train() client id: f_00001-11-2 loss: 0.524210  [   96/  265]
train() client id: f_00001-11-3 loss: 0.647610  [  128/  265]
train() client id: f_00001-11-4 loss: 0.392547  [  160/  265]
train() client id: f_00001-11-5 loss: 0.487117  [  192/  265]
train() client id: f_00001-11-6 loss: 0.513143  [  224/  265]
train() client id: f_00001-11-7 loss: 0.468465  [  256/  265]
train() client id: f_00002-0-0 loss: 1.152997  [   32/  124]
train() client id: f_00002-0-1 loss: 1.034253  [   64/  124]
train() client id: f_00002-0-2 loss: 1.072434  [   96/  124]
train() client id: f_00002-1-0 loss: 1.218574  [   32/  124]
train() client id: f_00002-1-1 loss: 0.949950  [   64/  124]
train() client id: f_00002-1-2 loss: 0.983043  [   96/  124]
train() client id: f_00002-2-0 loss: 1.022830  [   32/  124]
train() client id: f_00002-2-1 loss: 1.071677  [   64/  124]
train() client id: f_00002-2-2 loss: 0.963354  [   96/  124]
train() client id: f_00002-3-0 loss: 0.885289  [   32/  124]
train() client id: f_00002-3-1 loss: 1.088173  [   64/  124]
train() client id: f_00002-3-2 loss: 1.109851  [   96/  124]
train() client id: f_00002-4-0 loss: 1.127576  [   32/  124]
train() client id: f_00002-4-1 loss: 0.887661  [   64/  124]
train() client id: f_00002-4-2 loss: 0.906903  [   96/  124]
train() client id: f_00002-5-0 loss: 1.004616  [   32/  124]
train() client id: f_00002-5-1 loss: 0.946581  [   64/  124]
train() client id: f_00002-5-2 loss: 0.904077  [   96/  124]
train() client id: f_00002-6-0 loss: 0.958382  [   32/  124]
train() client id: f_00002-6-1 loss: 0.925679  [   64/  124]
train() client id: f_00002-6-2 loss: 1.072311  [   96/  124]
train() client id: f_00002-7-0 loss: 1.121946  [   32/  124]
train() client id: f_00002-7-1 loss: 0.909844  [   64/  124]
train() client id: f_00002-7-2 loss: 0.803826  [   96/  124]
train() client id: f_00002-8-0 loss: 0.766228  [   32/  124]
train() client id: f_00002-8-1 loss: 0.894309  [   64/  124]
train() client id: f_00002-8-2 loss: 1.130616  [   96/  124]
train() client id: f_00002-9-0 loss: 0.944129  [   32/  124]
train() client id: f_00002-9-1 loss: 0.804093  [   64/  124]
train() client id: f_00002-9-2 loss: 0.968501  [   96/  124]
train() client id: f_00002-10-0 loss: 0.865011  [   32/  124]
train() client id: f_00002-10-1 loss: 1.059477  [   64/  124]
train() client id: f_00002-10-2 loss: 0.672764  [   96/  124]
train() client id: f_00002-11-0 loss: 0.920227  [   32/  124]
train() client id: f_00002-11-1 loss: 0.688307  [   64/  124]
train() client id: f_00002-11-2 loss: 1.142278  [   96/  124]
train() client id: f_00003-0-0 loss: 0.742386  [   32/   43]
train() client id: f_00003-1-0 loss: 0.612895  [   32/   43]
train() client id: f_00003-2-0 loss: 0.796261  [   32/   43]
train() client id: f_00003-3-0 loss: 0.636995  [   32/   43]
train() client id: f_00003-4-0 loss: 0.732092  [   32/   43]
train() client id: f_00003-5-0 loss: 0.842215  [   32/   43]
train() client id: f_00003-6-0 loss: 0.706517  [   32/   43]
train() client id: f_00003-7-0 loss: 0.770432  [   32/   43]
train() client id: f_00003-8-0 loss: 0.650783  [   32/   43]
train() client id: f_00003-9-0 loss: 0.749629  [   32/   43]
train() client id: f_00003-10-0 loss: 0.842868  [   32/   43]
train() client id: f_00003-11-0 loss: 0.773545  [   32/   43]
train() client id: f_00004-0-0 loss: 0.803009  [   32/  306]
train() client id: f_00004-0-1 loss: 0.898231  [   64/  306]
train() client id: f_00004-0-2 loss: 0.787602  [   96/  306]
train() client id: f_00004-0-3 loss: 0.792566  [  128/  306]
train() client id: f_00004-0-4 loss: 0.880843  [  160/  306]
train() client id: f_00004-0-5 loss: 0.813059  [  192/  306]
train() client id: f_00004-0-6 loss: 0.756011  [  224/  306]
train() client id: f_00004-0-7 loss: 0.667630  [  256/  306]
train() client id: f_00004-0-8 loss: 0.738194  [  288/  306]
train() client id: f_00004-1-0 loss: 0.856985  [   32/  306]
train() client id: f_00004-1-1 loss: 0.732882  [   64/  306]
train() client id: f_00004-1-2 loss: 0.669497  [   96/  306]
train() client id: f_00004-1-3 loss: 0.754748  [  128/  306]
train() client id: f_00004-1-4 loss: 0.900562  [  160/  306]
train() client id: f_00004-1-5 loss: 0.699590  [  192/  306]
train() client id: f_00004-1-6 loss: 0.909908  [  224/  306]
train() client id: f_00004-1-7 loss: 0.742324  [  256/  306]
train() client id: f_00004-1-8 loss: 0.820518  [  288/  306]
train() client id: f_00004-2-0 loss: 0.917399  [   32/  306]
train() client id: f_00004-2-1 loss: 0.849247  [   64/  306]
train() client id: f_00004-2-2 loss: 0.784965  [   96/  306]
train() client id: f_00004-2-3 loss: 0.807446  [  128/  306]
train() client id: f_00004-2-4 loss: 0.867332  [  160/  306]
train() client id: f_00004-2-5 loss: 0.760019  [  192/  306]
train() client id: f_00004-2-6 loss: 0.699790  [  224/  306]
train() client id: f_00004-2-7 loss: 0.686502  [  256/  306]
train() client id: f_00004-2-8 loss: 0.745222  [  288/  306]
train() client id: f_00004-3-0 loss: 0.809742  [   32/  306]
train() client id: f_00004-3-1 loss: 0.859182  [   64/  306]
train() client id: f_00004-3-2 loss: 0.791602  [   96/  306]
train() client id: f_00004-3-3 loss: 0.804673  [  128/  306]
train() client id: f_00004-3-4 loss: 0.863668  [  160/  306]
train() client id: f_00004-3-5 loss: 0.656890  [  192/  306]
train() client id: f_00004-3-6 loss: 0.772462  [  224/  306]
train() client id: f_00004-3-7 loss: 0.744794  [  256/  306]
train() client id: f_00004-3-8 loss: 0.871161  [  288/  306]
train() client id: f_00004-4-0 loss: 0.759349  [   32/  306]
train() client id: f_00004-4-1 loss: 0.860125  [   64/  306]
train() client id: f_00004-4-2 loss: 0.771572  [   96/  306]
train() client id: f_00004-4-3 loss: 0.804299  [  128/  306]
train() client id: f_00004-4-4 loss: 0.731362  [  160/  306]
train() client id: f_00004-4-5 loss: 0.767402  [  192/  306]
train() client id: f_00004-4-6 loss: 0.823995  [  224/  306]
train() client id: f_00004-4-7 loss: 0.741432  [  256/  306]
train() client id: f_00004-4-8 loss: 0.865086  [  288/  306]
train() client id: f_00004-5-0 loss: 0.848126  [   32/  306]
train() client id: f_00004-5-1 loss: 0.773611  [   64/  306]
train() client id: f_00004-5-2 loss: 0.698635  [   96/  306]
train() client id: f_00004-5-3 loss: 0.794461  [  128/  306]
train() client id: f_00004-5-4 loss: 0.761546  [  160/  306]
train() client id: f_00004-5-5 loss: 0.872244  [  192/  306]
train() client id: f_00004-5-6 loss: 0.850930  [  224/  306]
train() client id: f_00004-5-7 loss: 0.782267  [  256/  306]
train() client id: f_00004-5-8 loss: 0.838439  [  288/  306]
train() client id: f_00004-6-0 loss: 0.774623  [   32/  306]
train() client id: f_00004-6-1 loss: 0.863519  [   64/  306]
train() client id: f_00004-6-2 loss: 0.744204  [   96/  306]
train() client id: f_00004-6-3 loss: 0.904778  [  128/  306]
train() client id: f_00004-6-4 loss: 0.748225  [  160/  306]
train() client id: f_00004-6-5 loss: 0.778280  [  192/  306]
train() client id: f_00004-6-6 loss: 0.770172  [  224/  306]
train() client id: f_00004-6-7 loss: 0.809390  [  256/  306]
train() client id: f_00004-6-8 loss: 0.803146  [  288/  306]
train() client id: f_00004-7-0 loss: 0.889931  [   32/  306]
train() client id: f_00004-7-1 loss: 0.743474  [   64/  306]
train() client id: f_00004-7-2 loss: 0.755318  [   96/  306]
train() client id: f_00004-7-3 loss: 0.821645  [  128/  306]
train() client id: f_00004-7-4 loss: 0.865010  [  160/  306]
train() client id: f_00004-7-5 loss: 0.755348  [  192/  306]
train() client id: f_00004-7-6 loss: 0.904127  [  224/  306]
train() client id: f_00004-7-7 loss: 0.798935  [  256/  306]
train() client id: f_00004-7-8 loss: 0.684366  [  288/  306]
train() client id: f_00004-8-0 loss: 0.737363  [   32/  306]
train() client id: f_00004-8-1 loss: 0.800622  [   64/  306]
train() client id: f_00004-8-2 loss: 0.946908  [   96/  306]
train() client id: f_00004-8-3 loss: 0.699947  [  128/  306]
train() client id: f_00004-8-4 loss: 0.833182  [  160/  306]
train() client id: f_00004-8-5 loss: 0.824888  [  192/  306]
train() client id: f_00004-8-6 loss: 0.938479  [  224/  306]
train() client id: f_00004-8-7 loss: 0.809851  [  256/  306]
train() client id: f_00004-8-8 loss: 0.727520  [  288/  306]
train() client id: f_00004-9-0 loss: 0.858836  [   32/  306]
train() client id: f_00004-9-1 loss: 0.732948  [   64/  306]
train() client id: f_00004-9-2 loss: 0.803571  [   96/  306]
train() client id: f_00004-9-3 loss: 1.035444  [  128/  306]
train() client id: f_00004-9-4 loss: 0.726140  [  160/  306]
train() client id: f_00004-9-5 loss: 0.804429  [  192/  306]
train() client id: f_00004-9-6 loss: 0.772660  [  224/  306]
train() client id: f_00004-9-7 loss: 0.742766  [  256/  306]
train() client id: f_00004-9-8 loss: 0.716993  [  288/  306]
train() client id: f_00004-10-0 loss: 0.787851  [   32/  306]
train() client id: f_00004-10-1 loss: 0.784474  [   64/  306]
train() client id: f_00004-10-2 loss: 0.903059  [   96/  306]
train() client id: f_00004-10-3 loss: 0.786181  [  128/  306]
train() client id: f_00004-10-4 loss: 0.862015  [  160/  306]
train() client id: f_00004-10-5 loss: 0.846703  [  192/  306]
train() client id: f_00004-10-6 loss: 0.773914  [  224/  306]
train() client id: f_00004-10-7 loss: 0.742132  [  256/  306]
train() client id: f_00004-10-8 loss: 0.855126  [  288/  306]
train() client id: f_00004-11-0 loss: 0.775333  [   32/  306]
train() client id: f_00004-11-1 loss: 0.857259  [   64/  306]
train() client id: f_00004-11-2 loss: 0.764389  [   96/  306]
train() client id: f_00004-11-3 loss: 0.716505  [  128/  306]
train() client id: f_00004-11-4 loss: 0.765009  [  160/  306]
train() client id: f_00004-11-5 loss: 0.839590  [  192/  306]
train() client id: f_00004-11-6 loss: 0.860539  [  224/  306]
train() client id: f_00004-11-7 loss: 0.883091  [  256/  306]
train() client id: f_00004-11-8 loss: 0.803530  [  288/  306]
train() client id: f_00005-0-0 loss: 0.710195  [   32/  146]
train() client id: f_00005-0-1 loss: 0.931793  [   64/  146]
train() client id: f_00005-0-2 loss: 1.009569  [   96/  146]
train() client id: f_00005-0-3 loss: 0.736022  [  128/  146]
train() client id: f_00005-1-0 loss: 0.722178  [   32/  146]
train() client id: f_00005-1-1 loss: 0.739380  [   64/  146]
train() client id: f_00005-1-2 loss: 0.937432  [   96/  146]
train() client id: f_00005-1-3 loss: 0.740688  [  128/  146]
train() client id: f_00005-2-0 loss: 0.773103  [   32/  146]
train() client id: f_00005-2-1 loss: 0.971419  [   64/  146]
train() client id: f_00005-2-2 loss: 0.822706  [   96/  146]
train() client id: f_00005-2-3 loss: 0.711888  [  128/  146]
train() client id: f_00005-3-0 loss: 0.929132  [   32/  146]
train() client id: f_00005-3-1 loss: 0.624844  [   64/  146]
train() client id: f_00005-3-2 loss: 0.773077  [   96/  146]
train() client id: f_00005-3-3 loss: 0.884355  [  128/  146]
train() client id: f_00005-4-0 loss: 0.741324  [   32/  146]
train() client id: f_00005-4-1 loss: 1.006970  [   64/  146]
train() client id: f_00005-4-2 loss: 0.923159  [   96/  146]
train() client id: f_00005-4-3 loss: 0.757996  [  128/  146]
train() client id: f_00005-5-0 loss: 0.788430  [   32/  146]
train() client id: f_00005-5-1 loss: 0.786146  [   64/  146]
train() client id: f_00005-5-2 loss: 0.987639  [   96/  146]
train() client id: f_00005-5-3 loss: 0.758251  [  128/  146]
train() client id: f_00005-6-0 loss: 0.832487  [   32/  146]
train() client id: f_00005-6-1 loss: 0.728016  [   64/  146]
train() client id: f_00005-6-2 loss: 0.825457  [   96/  146]
train() client id: f_00005-6-3 loss: 0.826476  [  128/  146]
train() client id: f_00005-7-0 loss: 0.696256  [   32/  146]
train() client id: f_00005-7-1 loss: 0.953034  [   64/  146]
train() client id: f_00005-7-2 loss: 0.711799  [   96/  146]
train() client id: f_00005-7-3 loss: 0.842154  [  128/  146]
train() client id: f_00005-8-0 loss: 0.753358  [   32/  146]
train() client id: f_00005-8-1 loss: 0.786564  [   64/  146]
train() client id: f_00005-8-2 loss: 0.807489  [   96/  146]
train() client id: f_00005-8-3 loss: 0.943150  [  128/  146]
train() client id: f_00005-9-0 loss: 0.733549  [   32/  146]
train() client id: f_00005-9-1 loss: 0.780413  [   64/  146]
train() client id: f_00005-9-2 loss: 0.982297  [   96/  146]
train() client id: f_00005-9-3 loss: 0.816706  [  128/  146]
train() client id: f_00005-10-0 loss: 0.478495  [   32/  146]
train() client id: f_00005-10-1 loss: 0.939309  [   64/  146]
train() client id: f_00005-10-2 loss: 1.005450  [   96/  146]
train() client id: f_00005-10-3 loss: 0.923642  [  128/  146]
train() client id: f_00005-11-0 loss: 0.911823  [   32/  146]
train() client id: f_00005-11-1 loss: 0.804514  [   64/  146]
train() client id: f_00005-11-2 loss: 0.818103  [   96/  146]
train() client id: f_00005-11-3 loss: 0.839534  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507760  [   32/   54]
train() client id: f_00006-1-0 loss: 0.453694  [   32/   54]
train() client id: f_00006-2-0 loss: 0.522243  [   32/   54]
train() client id: f_00006-3-0 loss: 0.462007  [   32/   54]
train() client id: f_00006-4-0 loss: 0.512241  [   32/   54]
train() client id: f_00006-5-0 loss: 0.502172  [   32/   54]
train() client id: f_00006-6-0 loss: 0.483836  [   32/   54]
train() client id: f_00006-7-0 loss: 0.475209  [   32/   54]
train() client id: f_00006-8-0 loss: 0.413500  [   32/   54]
train() client id: f_00006-9-0 loss: 0.400164  [   32/   54]
train() client id: f_00006-10-0 loss: 0.409489  [   32/   54]
train() client id: f_00006-11-0 loss: 0.505533  [   32/   54]
train() client id: f_00007-0-0 loss: 0.630566  [   32/  179]
train() client id: f_00007-0-1 loss: 1.008963  [   64/  179]
train() client id: f_00007-0-2 loss: 0.669867  [   96/  179]
train() client id: f_00007-0-3 loss: 0.705790  [  128/  179]
train() client id: f_00007-0-4 loss: 0.682164  [  160/  179]
train() client id: f_00007-1-0 loss: 0.791585  [   32/  179]
train() client id: f_00007-1-1 loss: 0.679158  [   64/  179]
train() client id: f_00007-1-2 loss: 0.764038  [   96/  179]
train() client id: f_00007-1-3 loss: 0.629612  [  128/  179]
train() client id: f_00007-1-4 loss: 0.702695  [  160/  179]
train() client id: f_00007-2-0 loss: 0.620553  [   32/  179]
train() client id: f_00007-2-1 loss: 0.649401  [   64/  179]
train() client id: f_00007-2-2 loss: 0.567786  [   96/  179]
train() client id: f_00007-2-3 loss: 0.794841  [  128/  179]
train() client id: f_00007-2-4 loss: 0.626708  [  160/  179]
train() client id: f_00007-3-0 loss: 0.906585  [   32/  179]
train() client id: f_00007-3-1 loss: 0.649687  [   64/  179]
train() client id: f_00007-3-2 loss: 0.676096  [   96/  179]
train() client id: f_00007-3-3 loss: 0.625178  [  128/  179]
train() client id: f_00007-3-4 loss: 0.742080  [  160/  179]
train() client id: f_00007-4-0 loss: 0.859777  [   32/  179]
train() client id: f_00007-4-1 loss: 0.645533  [   64/  179]
train() client id: f_00007-4-2 loss: 0.575737  [   96/  179]
train() client id: f_00007-4-3 loss: 0.683952  [  128/  179]
train() client id: f_00007-4-4 loss: 0.674536  [  160/  179]
train() client id: f_00007-5-0 loss: 0.655328  [   32/  179]
train() client id: f_00007-5-1 loss: 0.532468  [   64/  179]
train() client id: f_00007-5-2 loss: 0.721836  [   96/  179]
train() client id: f_00007-5-3 loss: 0.738417  [  128/  179]
train() client id: f_00007-5-4 loss: 0.772229  [  160/  179]
train() client id: f_00007-6-0 loss: 0.665048  [   32/  179]
train() client id: f_00007-6-1 loss: 0.671091  [   64/  179]
train() client id: f_00007-6-2 loss: 0.696771  [   96/  179]
train() client id: f_00007-6-3 loss: 0.658351  [  128/  179]
train() client id: f_00007-6-4 loss: 0.799788  [  160/  179]
train() client id: f_00007-7-0 loss: 0.550571  [   32/  179]
train() client id: f_00007-7-1 loss: 0.781265  [   64/  179]
train() client id: f_00007-7-2 loss: 0.695263  [   96/  179]
train() client id: f_00007-7-3 loss: 0.664450  [  128/  179]
train() client id: f_00007-7-4 loss: 0.713164  [  160/  179]
train() client id: f_00007-8-0 loss: 0.723360  [   32/  179]
train() client id: f_00007-8-1 loss: 0.786550  [   64/  179]
train() client id: f_00007-8-2 loss: 0.655698  [   96/  179]
train() client id: f_00007-8-3 loss: 0.646517  [  128/  179]
train() client id: f_00007-8-4 loss: 0.621540  [  160/  179]
train() client id: f_00007-9-0 loss: 0.542627  [   32/  179]
train() client id: f_00007-9-1 loss: 0.706529  [   64/  179]
train() client id: f_00007-9-2 loss: 0.542231  [   96/  179]
train() client id: f_00007-9-3 loss: 0.723569  [  128/  179]
train() client id: f_00007-9-4 loss: 0.704077  [  160/  179]
train() client id: f_00007-10-0 loss: 0.652150  [   32/  179]
train() client id: f_00007-10-1 loss: 0.634144  [   64/  179]
train() client id: f_00007-10-2 loss: 0.510293  [   96/  179]
train() client id: f_00007-10-3 loss: 0.743363  [  128/  179]
train() client id: f_00007-10-4 loss: 0.963335  [  160/  179]
train() client id: f_00007-11-0 loss: 0.642740  [   32/  179]
train() client id: f_00007-11-1 loss: 0.654854  [   64/  179]
train() client id: f_00007-11-2 loss: 0.630020  [   96/  179]
train() client id: f_00007-11-3 loss: 0.633609  [  128/  179]
train() client id: f_00007-11-4 loss: 0.959496  [  160/  179]
train() client id: f_00008-0-0 loss: 0.745597  [   32/  130]
train() client id: f_00008-0-1 loss: 0.729891  [   64/  130]
train() client id: f_00008-0-2 loss: 0.800896  [   96/  130]
train() client id: f_00008-0-3 loss: 0.739785  [  128/  130]
train() client id: f_00008-1-0 loss: 0.733238  [   32/  130]
train() client id: f_00008-1-1 loss: 0.764638  [   64/  130]
train() client id: f_00008-1-2 loss: 0.756950  [   96/  130]
train() client id: f_00008-1-3 loss: 0.758038  [  128/  130]
train() client id: f_00008-2-0 loss: 0.768021  [   32/  130]
train() client id: f_00008-2-1 loss: 0.662709  [   64/  130]
train() client id: f_00008-2-2 loss: 0.781814  [   96/  130]
train() client id: f_00008-2-3 loss: 0.776809  [  128/  130]
train() client id: f_00008-3-0 loss: 0.803090  [   32/  130]
train() client id: f_00008-3-1 loss: 0.750607  [   64/  130]
train() client id: f_00008-3-2 loss: 0.593200  [   96/  130]
train() client id: f_00008-3-3 loss: 0.867024  [  128/  130]
train() client id: f_00008-4-0 loss: 0.768766  [   32/  130]
train() client id: f_00008-4-1 loss: 0.780993  [   64/  130]
train() client id: f_00008-4-2 loss: 0.727247  [   96/  130]
train() client id: f_00008-4-3 loss: 0.693665  [  128/  130]
train() client id: f_00008-5-0 loss: 0.694481  [   32/  130]
train() client id: f_00008-5-1 loss: 0.803522  [   64/  130]
train() client id: f_00008-5-2 loss: 0.681210  [   96/  130]
train() client id: f_00008-5-3 loss: 0.827430  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780816  [   32/  130]
train() client id: f_00008-6-1 loss: 0.759408  [   64/  130]
train() client id: f_00008-6-2 loss: 0.702717  [   96/  130]
train() client id: f_00008-6-3 loss: 0.771435  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708319  [   32/  130]
train() client id: f_00008-7-1 loss: 0.795724  [   64/  130]
train() client id: f_00008-7-2 loss: 0.722448  [   96/  130]
train() client id: f_00008-7-3 loss: 0.756990  [  128/  130]
train() client id: f_00008-8-0 loss: 0.781679  [   32/  130]
train() client id: f_00008-8-1 loss: 0.765306  [   64/  130]
train() client id: f_00008-8-2 loss: 0.647243  [   96/  130]
train() client id: f_00008-8-3 loss: 0.815178  [  128/  130]
train() client id: f_00008-9-0 loss: 0.743808  [   32/  130]
train() client id: f_00008-9-1 loss: 0.705537  [   64/  130]
train() client id: f_00008-9-2 loss: 0.820440  [   96/  130]
train() client id: f_00008-9-3 loss: 0.738806  [  128/  130]
train() client id: f_00008-10-0 loss: 0.764494  [   32/  130]
train() client id: f_00008-10-1 loss: 0.640727  [   64/  130]
train() client id: f_00008-10-2 loss: 0.810322  [   96/  130]
train() client id: f_00008-10-3 loss: 0.749119  [  128/  130]
train() client id: f_00008-11-0 loss: 0.694185  [   32/  130]
train() client id: f_00008-11-1 loss: 0.783683  [   64/  130]
train() client id: f_00008-11-2 loss: 0.774541  [   96/  130]
train() client id: f_00008-11-3 loss: 0.767354  [  128/  130]
train() client id: f_00009-0-0 loss: 1.204629  [   32/  118]
train() client id: f_00009-0-1 loss: 1.001731  [   64/  118]
train() client id: f_00009-0-2 loss: 1.003167  [   96/  118]
train() client id: f_00009-1-0 loss: 1.069888  [   32/  118]
train() client id: f_00009-1-1 loss: 0.993106  [   64/  118]
train() client id: f_00009-1-2 loss: 0.829903  [   96/  118]
train() client id: f_00009-2-0 loss: 0.972996  [   32/  118]
train() client id: f_00009-2-1 loss: 0.892321  [   64/  118]
train() client id: f_00009-2-2 loss: 1.000783  [   96/  118]
train() client id: f_00009-3-0 loss: 1.009530  [   32/  118]
train() client id: f_00009-3-1 loss: 0.894495  [   64/  118]
train() client id: f_00009-3-2 loss: 0.954824  [   96/  118]
train() client id: f_00009-4-0 loss: 0.986872  [   32/  118]
train() client id: f_00009-4-1 loss: 0.849982  [   64/  118]
train() client id: f_00009-4-2 loss: 1.022749  [   96/  118]
train() client id: f_00009-5-0 loss: 1.001046  [   32/  118]
train() client id: f_00009-5-1 loss: 0.985537  [   64/  118]
train() client id: f_00009-5-2 loss: 0.779851  [   96/  118]
train() client id: f_00009-6-0 loss: 0.850200  [   32/  118]
train() client id: f_00009-6-1 loss: 1.000458  [   64/  118]
train() client id: f_00009-6-2 loss: 0.824033  [   96/  118]
train() client id: f_00009-7-0 loss: 0.865218  [   32/  118]
train() client id: f_00009-7-1 loss: 0.882171  [   64/  118]
train() client id: f_00009-7-2 loss: 0.800264  [   96/  118]
train() client id: f_00009-8-0 loss: 0.854884  [   32/  118]
train() client id: f_00009-8-1 loss: 0.892845  [   64/  118]
train() client id: f_00009-8-2 loss: 1.023850  [   96/  118]
train() client id: f_00009-9-0 loss: 0.836384  [   32/  118]
train() client id: f_00009-9-1 loss: 0.871387  [   64/  118]
train() client id: f_00009-9-2 loss: 0.935786  [   96/  118]
train() client id: f_00009-10-0 loss: 0.738078  [   32/  118]
train() client id: f_00009-10-1 loss: 0.777163  [   64/  118]
train() client id: f_00009-10-2 loss: 1.019671  [   96/  118]
train() client id: f_00009-11-0 loss: 0.733698  [   32/  118]
train() client id: f_00009-11-1 loss: 0.906992  [   64/  118]
train() client id: f_00009-11-2 loss: 0.747752  [   96/  118]
At round 36 accuracy: 0.6525198938992043
At round 36 training accuracy: 0.5935613682092555
At round 36 training loss: 0.8137030992357959
update_location
xs = [  -3.9056584     4.20031788  200.00902392   18.81129433    0.97929623
    3.95640986 -162.44319194 -141.32485185  184.66397685 -127.06087855]
ys = [ 192.5879595   175.55583871    1.32061395 -162.45517586  154.35018685
  137.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [217.03773016 202.08289184 223.6187686  191.69128556 183.91557628
 170.31850552 190.7739006  173.12824728 210.73550797 161.74201283]
dists_bs = [171.99595939 179.20117992 413.27508654 389.1514018  177.18670233
 182.77905793 178.06826916 177.40312868 392.61010444 177.59125147]
uav_gains = [1.32188752e-11 1.65442503e-11 1.19052653e-11 1.92240094e-11
 2.15038573e-11 2.62734338e-11 1.94793158e-11 2.51899720e-11
 1.45569033e-11 2.99729913e-11]
bs_gains = [6.07915198e-11 5.41925596e-11 5.22184833e-12 6.17962338e-12
 5.59354239e-11 5.12743388e-11 5.51634959e-11 5.57445631e-11
 6.02839862e-12 5.55793797e-11]
Round 37
-------------------------------
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.10747648 12.63082676  6.01756463  2.17165072 14.56715747  7.00995328
  2.69087946  8.58799468  6.33825934  5.68528245]
obj_prev = 71.80704525657299
eta_min = 7.508228350489047e-16	eta_max = 0.9300641933191646
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 16.65764355541381	eta = 0.909090909090909
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 30.742318165825036	eta = 0.4925884977645458
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 23.795554951531557	eta = 0.636392483972255
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.538890009475697	eta = 0.6718748046925549
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.472173027112785	eta = 0.6738695143025546
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.47196919842037	eta = 0.6738756265369006
eta = 0.6738756265369006
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.0325705  0.06850145 0.03205351 0.01111533 0.07909978 0.03774042
 0.01395879 0.04627079 0.03360448 0.03050253]
ene_total = [2.01826629 3.59433964 2.00757769 0.95394728 4.09618977 2.13534693
 1.08775634 2.59856536 2.19433777 1.78564212]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 0 obj = 4.156314504851677
eta = 0.6738756265369006
freqs = [33016494.15795401 65724604.40972588 32686070.96869287 11059841.67344831
 75827849.2653193  36266244.90193002 13880926.45922257 45522544.15430705
 36013045.58118701 29245868.53629994]
eta_min = 0.673875626536905	eta_max = 0.6738756265369027
af = 0.01021278956996407	bf = 1.3514443765451492	zeta = 0.011234068526960479	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [2.14332394e-06 1.78631149e-05 2.06729513e-06 8.20770299e-08
 2.74558099e-05 2.99649386e-06 1.62362610e-07 5.78843872e-06
 2.63098523e-06 1.57494870e-06]
ene_total = [1.68434772 1.21143542 1.73418576 1.52598255 1.2053965  1.2225551
 1.52094637 1.42954967 2.1396742  1.20252205]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 1 obj = 4.156314504851704
eta = 0.6738756265369027
freqs = [33016494.15795401 65724604.40972584 32686070.96869286 11059841.67344831
 75827849.26531927 36266244.90193    13880926.45922257 45522544.15430704
 36013045.58118702 29245868.53629993]
Done!
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.24107198e-06 6.86836056e-05 7.94874151e-06 3.15585852e-07
 1.05567480e-04 1.15215069e-05 6.24283587e-07 2.22565238e-05
 1.01161277e-05 6.05567145e-06]
ene_total = [0.00987962 0.00715216 0.01017155 0.00894543 0.00714405 0.00717505
 0.00891613 0.00839638 0.01255009 0.00705357]
At round 37 energy consumption: 0.08738402800819062
At round 37 eta: 0.6738756265369027
At round 37 a_n: 15.508406518389636
At round 37 local rounds: 12.924801449949687
At round 37 global rounds: 47.55365676507614
gradient difference: 0.36475232243537903
train() client id: f_00000-0-0 loss: 1.254369  [   32/  126]
train() client id: f_00000-0-1 loss: 0.909397  [   64/  126]
train() client id: f_00000-0-2 loss: 1.304942  [   96/  126]
train() client id: f_00000-1-0 loss: 1.012091  [   32/  126]
train() client id: f_00000-1-1 loss: 0.923716  [   64/  126]
train() client id: f_00000-1-2 loss: 1.137886  [   96/  126]
train() client id: f_00000-2-0 loss: 0.982120  [   32/  126]
train() client id: f_00000-2-1 loss: 0.955449  [   64/  126]
train() client id: f_00000-2-2 loss: 0.890220  [   96/  126]
train() client id: f_00000-3-0 loss: 0.824296  [   32/  126]
train() client id: f_00000-3-1 loss: 0.886298  [   64/  126]
train() client id: f_00000-3-2 loss: 0.961215  [   96/  126]
train() client id: f_00000-4-0 loss: 0.737693  [   32/  126]
train() client id: f_00000-4-1 loss: 0.719409  [   64/  126]
train() client id: f_00000-4-2 loss: 0.959838  [   96/  126]
train() client id: f_00000-5-0 loss: 0.745089  [   32/  126]
train() client id: f_00000-5-1 loss: 0.671253  [   64/  126]
train() client id: f_00000-5-2 loss: 0.835334  [   96/  126]
train() client id: f_00000-6-0 loss: 0.644401  [   32/  126]
train() client id: f_00000-6-1 loss: 0.760356  [   64/  126]
train() client id: f_00000-6-2 loss: 0.715848  [   96/  126]
train() client id: f_00000-7-0 loss: 0.713403  [   32/  126]
train() client id: f_00000-7-1 loss: 0.742633  [   64/  126]
train() client id: f_00000-7-2 loss: 0.769870  [   96/  126]
train() client id: f_00000-8-0 loss: 0.801189  [   32/  126]
train() client id: f_00000-8-1 loss: 0.519627  [   64/  126]
train() client id: f_00000-8-2 loss: 0.752502  [   96/  126]
train() client id: f_00000-9-0 loss: 0.665652  [   32/  126]
train() client id: f_00000-9-1 loss: 0.581320  [   64/  126]
train() client id: f_00000-9-2 loss: 0.682676  [   96/  126]
train() client id: f_00000-10-0 loss: 0.505742  [   32/  126]
train() client id: f_00000-10-1 loss: 0.650858  [   64/  126]
train() client id: f_00000-10-2 loss: 0.759302  [   96/  126]
train() client id: f_00000-11-0 loss: 0.735141  [   32/  126]
train() client id: f_00000-11-1 loss: 0.632631  [   64/  126]
train() client id: f_00000-11-2 loss: 0.598262  [   96/  126]
train() client id: f_00001-0-0 loss: 0.552778  [   32/  265]
train() client id: f_00001-0-1 loss: 0.454779  [   64/  265]
train() client id: f_00001-0-2 loss: 0.524918  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444814  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458471  [  160/  265]
train() client id: f_00001-0-5 loss: 0.413008  [  192/  265]
train() client id: f_00001-0-6 loss: 0.414489  [  224/  265]
train() client id: f_00001-0-7 loss: 0.381680  [  256/  265]
train() client id: f_00001-1-0 loss: 0.558286  [   32/  265]
train() client id: f_00001-1-1 loss: 0.381029  [   64/  265]
train() client id: f_00001-1-2 loss: 0.451675  [   96/  265]
train() client id: f_00001-1-3 loss: 0.371894  [  128/  265]
train() client id: f_00001-1-4 loss: 0.367803  [  160/  265]
train() client id: f_00001-1-5 loss: 0.542915  [  192/  265]
train() client id: f_00001-1-6 loss: 0.474754  [  224/  265]
train() client id: f_00001-1-7 loss: 0.494215  [  256/  265]
train() client id: f_00001-2-0 loss: 0.456942  [   32/  265]
train() client id: f_00001-2-1 loss: 0.353507  [   64/  265]
train() client id: f_00001-2-2 loss: 0.361298  [   96/  265]
train() client id: f_00001-2-3 loss: 0.473839  [  128/  265]
train() client id: f_00001-2-4 loss: 0.423492  [  160/  265]
train() client id: f_00001-2-5 loss: 0.544029  [  192/  265]
train() client id: f_00001-2-6 loss: 0.472638  [  224/  265]
train() client id: f_00001-2-7 loss: 0.488339  [  256/  265]
train() client id: f_00001-3-0 loss: 0.467236  [   32/  265]
train() client id: f_00001-3-1 loss: 0.354637  [   64/  265]
train() client id: f_00001-3-2 loss: 0.441825  [   96/  265]
train() client id: f_00001-3-3 loss: 0.580590  [  128/  265]
train() client id: f_00001-3-4 loss: 0.378054  [  160/  265]
train() client id: f_00001-3-5 loss: 0.406918  [  192/  265]
train() client id: f_00001-3-6 loss: 0.489894  [  224/  265]
train() client id: f_00001-3-7 loss: 0.421865  [  256/  265]
train() client id: f_00001-4-0 loss: 0.442337  [   32/  265]
train() client id: f_00001-4-1 loss: 0.358869  [   64/  265]
train() client id: f_00001-4-2 loss: 0.430753  [   96/  265]
train() client id: f_00001-4-3 loss: 0.394742  [  128/  265]
train() client id: f_00001-4-4 loss: 0.581214  [  160/  265]
train() client id: f_00001-4-5 loss: 0.445953  [  192/  265]
train() client id: f_00001-4-6 loss: 0.329654  [  224/  265]
train() client id: f_00001-4-7 loss: 0.498808  [  256/  265]
train() client id: f_00001-5-0 loss: 0.358768  [   32/  265]
train() client id: f_00001-5-1 loss: 0.488126  [   64/  265]
train() client id: f_00001-5-2 loss: 0.355701  [   96/  265]
train() client id: f_00001-5-3 loss: 0.465307  [  128/  265]
train() client id: f_00001-5-4 loss: 0.400519  [  160/  265]
train() client id: f_00001-5-5 loss: 0.565592  [  192/  265]
train() client id: f_00001-5-6 loss: 0.446477  [  224/  265]
train() client id: f_00001-5-7 loss: 0.325536  [  256/  265]
train() client id: f_00001-6-0 loss: 0.335240  [   32/  265]
train() client id: f_00001-6-1 loss: 0.391495  [   64/  265]
train() client id: f_00001-6-2 loss: 0.464907  [   96/  265]
train() client id: f_00001-6-3 loss: 0.599730  [  128/  265]
train() client id: f_00001-6-4 loss: 0.599551  [  160/  265]
train() client id: f_00001-6-5 loss: 0.324289  [  192/  265]
train() client id: f_00001-6-6 loss: 0.338360  [  224/  265]
train() client id: f_00001-6-7 loss: 0.404021  [  256/  265]
train() client id: f_00001-7-0 loss: 0.411640  [   32/  265]
train() client id: f_00001-7-1 loss: 0.528007  [   64/  265]
train() client id: f_00001-7-2 loss: 0.454307  [   96/  265]
train() client id: f_00001-7-3 loss: 0.375601  [  128/  265]
train() client id: f_00001-7-4 loss: 0.427189  [  160/  265]
train() client id: f_00001-7-5 loss: 0.351184  [  192/  265]
train() client id: f_00001-7-6 loss: 0.430194  [  224/  265]
train() client id: f_00001-7-7 loss: 0.404868  [  256/  265]
train() client id: f_00001-8-0 loss: 0.407234  [   32/  265]
train() client id: f_00001-8-1 loss: 0.471306  [   64/  265]
train() client id: f_00001-8-2 loss: 0.370717  [   96/  265]
train() client id: f_00001-8-3 loss: 0.378250  [  128/  265]
train() client id: f_00001-8-4 loss: 0.520503  [  160/  265]
train() client id: f_00001-8-5 loss: 0.483750  [  192/  265]
train() client id: f_00001-8-6 loss: 0.320138  [  224/  265]
train() client id: f_00001-8-7 loss: 0.377844  [  256/  265]
train() client id: f_00001-9-0 loss: 0.341656  [   32/  265]
train() client id: f_00001-9-1 loss: 0.513381  [   64/  265]
train() client id: f_00001-9-2 loss: 0.333360  [   96/  265]
train() client id: f_00001-9-3 loss: 0.422709  [  128/  265]
train() client id: f_00001-9-4 loss: 0.465734  [  160/  265]
train() client id: f_00001-9-5 loss: 0.310958  [  192/  265]
train() client id: f_00001-9-6 loss: 0.484606  [  224/  265]
train() client id: f_00001-9-7 loss: 0.466974  [  256/  265]
train() client id: f_00001-10-0 loss: 0.327531  [   32/  265]
train() client id: f_00001-10-1 loss: 0.411076  [   64/  265]
train() client id: f_00001-10-2 loss: 0.368435  [   96/  265]
train() client id: f_00001-10-3 loss: 0.537894  [  128/  265]
train() client id: f_00001-10-4 loss: 0.347375  [  160/  265]
train() client id: f_00001-10-5 loss: 0.411627  [  192/  265]
train() client id: f_00001-10-6 loss: 0.543174  [  224/  265]
train() client id: f_00001-10-7 loss: 0.468431  [  256/  265]
train() client id: f_00001-11-0 loss: 0.400426  [   32/  265]
train() client id: f_00001-11-1 loss: 0.401872  [   64/  265]
train() client id: f_00001-11-2 loss: 0.327332  [   96/  265]
train() client id: f_00001-11-3 loss: 0.343883  [  128/  265]
train() client id: f_00001-11-4 loss: 0.534313  [  160/  265]
train() client id: f_00001-11-5 loss: 0.458106  [  192/  265]
train() client id: f_00001-11-6 loss: 0.498559  [  224/  265]
train() client id: f_00001-11-7 loss: 0.439228  [  256/  265]
train() client id: f_00002-0-0 loss: 0.843590  [   32/  124]
train() client id: f_00002-0-1 loss: 1.024010  [   64/  124]
train() client id: f_00002-0-2 loss: 0.922183  [   96/  124]
train() client id: f_00002-1-0 loss: 0.951562  [   32/  124]
train() client id: f_00002-1-1 loss: 0.921359  [   64/  124]
train() client id: f_00002-1-2 loss: 0.932672  [   96/  124]
train() client id: f_00002-2-0 loss: 1.100574  [   32/  124]
train() client id: f_00002-2-1 loss: 0.847521  [   64/  124]
train() client id: f_00002-2-2 loss: 0.783868  [   96/  124]
train() client id: f_00002-3-0 loss: 0.813366  [   32/  124]
train() client id: f_00002-3-1 loss: 0.997170  [   64/  124]
train() client id: f_00002-3-2 loss: 0.766876  [   96/  124]
train() client id: f_00002-4-0 loss: 0.894630  [   32/  124]
train() client id: f_00002-4-1 loss: 0.723557  [   64/  124]
train() client id: f_00002-4-2 loss: 0.824668  [   96/  124]
train() client id: f_00002-5-0 loss: 0.592402  [   32/  124]
train() client id: f_00002-5-1 loss: 0.984603  [   64/  124]
train() client id: f_00002-5-2 loss: 0.709624  [   96/  124]
train() client id: f_00002-6-0 loss: 0.863955  [   32/  124]
train() client id: f_00002-6-1 loss: 0.791804  [   64/  124]
train() client id: f_00002-6-2 loss: 0.666245  [   96/  124]
train() client id: f_00002-7-0 loss: 0.757788  [   32/  124]
train() client id: f_00002-7-1 loss: 0.965315  [   64/  124]
train() client id: f_00002-7-2 loss: 0.617280  [   96/  124]
train() client id: f_00002-8-0 loss: 0.890811  [   32/  124]
train() client id: f_00002-8-1 loss: 0.732642  [   64/  124]
train() client id: f_00002-8-2 loss: 0.643473  [   96/  124]
train() client id: f_00002-9-0 loss: 0.808574  [   32/  124]
train() client id: f_00002-9-1 loss: 0.605641  [   64/  124]
train() client id: f_00002-9-2 loss: 0.742384  [   96/  124]
train() client id: f_00002-10-0 loss: 0.710846  [   32/  124]
train() client id: f_00002-10-1 loss: 0.650945  [   64/  124]
train() client id: f_00002-10-2 loss: 0.710635  [   96/  124]
train() client id: f_00002-11-0 loss: 0.782537  [   32/  124]
train() client id: f_00002-11-1 loss: 0.599661  [   64/  124]
train() client id: f_00002-11-2 loss: 0.755624  [   96/  124]
train() client id: f_00003-0-0 loss: 0.697978  [   32/   43]
train() client id: f_00003-1-0 loss: 0.559903  [   32/   43]
train() client id: f_00003-2-0 loss: 0.665445  [   32/   43]
train() client id: f_00003-3-0 loss: 0.804918  [   32/   43]
train() client id: f_00003-4-0 loss: 0.742987  [   32/   43]
train() client id: f_00003-5-0 loss: 0.736593  [   32/   43]
train() client id: f_00003-6-0 loss: 0.599257  [   32/   43]
train() client id: f_00003-7-0 loss: 0.519901  [   32/   43]
train() client id: f_00003-8-0 loss: 0.709834  [   32/   43]
train() client id: f_00003-9-0 loss: 0.609882  [   32/   43]
train() client id: f_00003-10-0 loss: 0.603430  [   32/   43]
train() client id: f_00003-11-0 loss: 0.592977  [   32/   43]
train() client id: f_00004-0-0 loss: 0.814249  [   32/  306]
train() client id: f_00004-0-1 loss: 0.981774  [   64/  306]
train() client id: f_00004-0-2 loss: 0.921587  [   96/  306]
train() client id: f_00004-0-3 loss: 0.899007  [  128/  306]
train() client id: f_00004-0-4 loss: 0.833766  [  160/  306]
train() client id: f_00004-0-5 loss: 0.716635  [  192/  306]
train() client id: f_00004-0-6 loss: 0.858687  [  224/  306]
train() client id: f_00004-0-7 loss: 0.953087  [  256/  306]
train() client id: f_00004-0-8 loss: 0.854224  [  288/  306]
train() client id: f_00004-1-0 loss: 0.829008  [   32/  306]
train() client id: f_00004-1-1 loss: 0.867306  [   64/  306]
train() client id: f_00004-1-2 loss: 0.827829  [   96/  306]
train() client id: f_00004-1-3 loss: 0.748517  [  128/  306]
train() client id: f_00004-1-4 loss: 1.007113  [  160/  306]
train() client id: f_00004-1-5 loss: 0.899182  [  192/  306]
train() client id: f_00004-1-6 loss: 0.816984  [  224/  306]
train() client id: f_00004-1-7 loss: 0.865120  [  256/  306]
train() client id: f_00004-1-8 loss: 0.932538  [  288/  306]
train() client id: f_00004-2-0 loss: 0.788335  [   32/  306]
train() client id: f_00004-2-1 loss: 0.968069  [   64/  306]
train() client id: f_00004-2-2 loss: 0.833610  [   96/  306]
train() client id: f_00004-2-3 loss: 0.839863  [  128/  306]
train() client id: f_00004-2-4 loss: 0.967678  [  160/  306]
train() client id: f_00004-2-5 loss: 0.871666  [  192/  306]
train() client id: f_00004-2-6 loss: 0.879719  [  224/  306]
train() client id: f_00004-2-7 loss: 0.811559  [  256/  306]
train() client id: f_00004-2-8 loss: 0.878871  [  288/  306]
train() client id: f_00004-3-0 loss: 0.850429  [   32/  306]
train() client id: f_00004-3-1 loss: 0.738650  [   64/  306]
train() client id: f_00004-3-2 loss: 0.902675  [   96/  306]
train() client id: f_00004-3-3 loss: 0.890768  [  128/  306]
train() client id: f_00004-3-4 loss: 0.884016  [  160/  306]
train() client id: f_00004-3-5 loss: 0.894033  [  192/  306]
train() client id: f_00004-3-6 loss: 0.878078  [  224/  306]
train() client id: f_00004-3-7 loss: 0.882878  [  256/  306]
train() client id: f_00004-3-8 loss: 0.967116  [  288/  306]
train() client id: f_00004-4-0 loss: 0.939930  [   32/  306]
train() client id: f_00004-4-1 loss: 0.764958  [   64/  306]
train() client id: f_00004-4-2 loss: 0.853479  [   96/  306]
train() client id: f_00004-4-3 loss: 1.008395  [  128/  306]
train() client id: f_00004-4-4 loss: 0.811086  [  160/  306]
train() client id: f_00004-4-5 loss: 0.948416  [  192/  306]
train() client id: f_00004-4-6 loss: 0.874685  [  224/  306]
train() client id: f_00004-4-7 loss: 0.860157  [  256/  306]
train() client id: f_00004-4-8 loss: 0.805706  [  288/  306]
train() client id: f_00004-5-0 loss: 0.787059  [   32/  306]
train() client id: f_00004-5-1 loss: 0.922959  [   64/  306]
train() client id: f_00004-5-2 loss: 0.963593  [   96/  306]
train() client id: f_00004-5-3 loss: 0.807372  [  128/  306]
train() client id: f_00004-5-4 loss: 0.805465  [  160/  306]
train() client id: f_00004-5-5 loss: 0.949790  [  192/  306]
train() client id: f_00004-5-6 loss: 0.827582  [  224/  306]
train() client id: f_00004-5-7 loss: 0.967945  [  256/  306]
train() client id: f_00004-5-8 loss: 0.806660  [  288/  306]
train() client id: f_00004-6-0 loss: 0.780083  [   32/  306]
train() client id: f_00004-6-1 loss: 0.782104  [   64/  306]
train() client id: f_00004-6-2 loss: 0.743684  [   96/  306]
train() client id: f_00004-6-3 loss: 0.932465  [  128/  306]
train() client id: f_00004-6-4 loss: 0.828631  [  160/  306]
train() client id: f_00004-6-5 loss: 0.818547  [  192/  306]
train() client id: f_00004-6-6 loss: 0.988212  [  224/  306]
train() client id: f_00004-6-7 loss: 0.915932  [  256/  306]
train() client id: f_00004-6-8 loss: 1.013138  [  288/  306]
train() client id: f_00004-7-0 loss: 0.890169  [   32/  306]
train() client id: f_00004-7-1 loss: 0.906529  [   64/  306]
train() client id: f_00004-7-2 loss: 0.927844  [   96/  306]
train() client id: f_00004-7-3 loss: 0.988245  [  128/  306]
train() client id: f_00004-7-4 loss: 0.731168  [  160/  306]
train() client id: f_00004-7-5 loss: 0.816118  [  192/  306]
train() client id: f_00004-7-6 loss: 0.899372  [  224/  306]
train() client id: f_00004-7-7 loss: 0.853178  [  256/  306]
train() client id: f_00004-7-8 loss: 0.838576  [  288/  306]
train() client id: f_00004-8-0 loss: 0.745137  [   32/  306]
train() client id: f_00004-8-1 loss: 0.803739  [   64/  306]
train() client id: f_00004-8-2 loss: 0.899406  [   96/  306]
train() client id: f_00004-8-3 loss: 1.016730  [  128/  306]
train() client id: f_00004-8-4 loss: 0.858553  [  160/  306]
train() client id: f_00004-8-5 loss: 0.784884  [  192/  306]
train() client id: f_00004-8-6 loss: 0.855695  [  224/  306]
train() client id: f_00004-8-7 loss: 0.921287  [  256/  306]
train() client id: f_00004-8-8 loss: 0.948844  [  288/  306]
train() client id: f_00004-9-0 loss: 1.089738  [   32/  306]
train() client id: f_00004-9-1 loss: 0.842912  [   64/  306]
train() client id: f_00004-9-2 loss: 0.876831  [   96/  306]
train() client id: f_00004-9-3 loss: 0.778141  [  128/  306]
train() client id: f_00004-9-4 loss: 0.969841  [  160/  306]
train() client id: f_00004-9-5 loss: 0.740571  [  192/  306]
train() client id: f_00004-9-6 loss: 0.796171  [  224/  306]
train() client id: f_00004-9-7 loss: 0.787841  [  256/  306]
train() client id: f_00004-9-8 loss: 0.952569  [  288/  306]
train() client id: f_00004-10-0 loss: 0.933384  [   32/  306]
train() client id: f_00004-10-1 loss: 0.734904  [   64/  306]
train() client id: f_00004-10-2 loss: 0.889767  [   96/  306]
train() client id: f_00004-10-3 loss: 0.842057  [  128/  306]
train() client id: f_00004-10-4 loss: 0.958781  [  160/  306]
train() client id: f_00004-10-5 loss: 0.897095  [  192/  306]
train() client id: f_00004-10-6 loss: 0.841658  [  224/  306]
train() client id: f_00004-10-7 loss: 0.817763  [  256/  306]
train() client id: f_00004-10-8 loss: 0.871303  [  288/  306]
train() client id: f_00004-11-0 loss: 0.942643  [   32/  306]
train() client id: f_00004-11-1 loss: 0.855321  [   64/  306]
train() client id: f_00004-11-2 loss: 0.822638  [   96/  306]
train() client id: f_00004-11-3 loss: 0.780612  [  128/  306]
train() client id: f_00004-11-4 loss: 0.933138  [  160/  306]
train() client id: f_00004-11-5 loss: 0.923062  [  192/  306]
train() client id: f_00004-11-6 loss: 0.784287  [  224/  306]
train() client id: f_00004-11-7 loss: 0.856240  [  256/  306]
train() client id: f_00004-11-8 loss: 0.853683  [  288/  306]
train() client id: f_00005-0-0 loss: 0.772527  [   32/  146]
train() client id: f_00005-0-1 loss: 0.632417  [   64/  146]
train() client id: f_00005-0-2 loss: 0.578134  [   96/  146]
train() client id: f_00005-0-3 loss: 0.622628  [  128/  146]
train() client id: f_00005-1-0 loss: 0.880031  [   32/  146]
train() client id: f_00005-1-1 loss: 0.527792  [   64/  146]
train() client id: f_00005-1-2 loss: 0.517592  [   96/  146]
train() client id: f_00005-1-3 loss: 0.607413  [  128/  146]
train() client id: f_00005-2-0 loss: 0.624250  [   32/  146]
train() client id: f_00005-2-1 loss: 0.631887  [   64/  146]
train() client id: f_00005-2-2 loss: 0.709881  [   96/  146]
train() client id: f_00005-2-3 loss: 0.694135  [  128/  146]
train() client id: f_00005-3-0 loss: 0.473455  [   32/  146]
train() client id: f_00005-3-1 loss: 0.776560  [   64/  146]
train() client id: f_00005-3-2 loss: 0.463877  [   96/  146]
train() client id: f_00005-3-3 loss: 0.852865  [  128/  146]
train() client id: f_00005-4-0 loss: 0.332127  [   32/  146]
train() client id: f_00005-4-1 loss: 0.701115  [   64/  146]
train() client id: f_00005-4-2 loss: 0.962270  [   96/  146]
train() client id: f_00005-4-3 loss: 0.448701  [  128/  146]
train() client id: f_00005-5-0 loss: 0.483915  [   32/  146]
train() client id: f_00005-5-1 loss: 0.536243  [   64/  146]
train() client id: f_00005-5-2 loss: 0.606925  [   96/  146]
train() client id: f_00005-5-3 loss: 0.677488  [  128/  146]
train() client id: f_00005-6-0 loss: 0.693471  [   32/  146]
train() client id: f_00005-6-1 loss: 0.555312  [   64/  146]
train() client id: f_00005-6-2 loss: 0.716367  [   96/  146]
train() client id: f_00005-6-3 loss: 0.582094  [  128/  146]
train() client id: f_00005-7-0 loss: 0.630963  [   32/  146]
train() client id: f_00005-7-1 loss: 0.563108  [   64/  146]
train() client id: f_00005-7-2 loss: 0.746897  [   96/  146]
train() client id: f_00005-7-3 loss: 0.702416  [  128/  146]
train() client id: f_00005-8-0 loss: 0.619593  [   32/  146]
train() client id: f_00005-8-1 loss: 0.678007  [   64/  146]
train() client id: f_00005-8-2 loss: 0.445200  [   96/  146]
train() client id: f_00005-8-3 loss: 0.774173  [  128/  146]
train() client id: f_00005-9-0 loss: 0.336319  [   32/  146]
train() client id: f_00005-9-1 loss: 0.550303  [   64/  146]
train() client id: f_00005-9-2 loss: 0.696933  [   96/  146]
train() client id: f_00005-9-3 loss: 0.747271  [  128/  146]
train() client id: f_00005-10-0 loss: 0.521871  [   32/  146]
train() client id: f_00005-10-1 loss: 0.688645  [   64/  146]
train() client id: f_00005-10-2 loss: 0.423581  [   96/  146]
train() client id: f_00005-10-3 loss: 0.793762  [  128/  146]
train() client id: f_00005-11-0 loss: 0.813641  [   32/  146]
train() client id: f_00005-11-1 loss: 0.366323  [   64/  146]
train() client id: f_00005-11-2 loss: 0.402998  [   96/  146]
train() client id: f_00005-11-3 loss: 0.783011  [  128/  146]
train() client id: f_00006-0-0 loss: 0.529184  [   32/   54]
train() client id: f_00006-1-0 loss: 0.528513  [   32/   54]
train() client id: f_00006-2-0 loss: 0.472686  [   32/   54]
train() client id: f_00006-3-0 loss: 0.408546  [   32/   54]
train() client id: f_00006-4-0 loss: 0.513605  [   32/   54]
train() client id: f_00006-5-0 loss: 0.492588  [   32/   54]
train() client id: f_00006-6-0 loss: 0.467322  [   32/   54]
train() client id: f_00006-7-0 loss: 0.440392  [   32/   54]
train() client id: f_00006-8-0 loss: 0.484413  [   32/   54]
train() client id: f_00006-9-0 loss: 0.463119  [   32/   54]
train() client id: f_00006-10-0 loss: 0.421385  [   32/   54]
train() client id: f_00006-11-0 loss: 0.449194  [   32/   54]
train() client id: f_00007-0-0 loss: 0.299835  [   32/  179]
train() client id: f_00007-0-1 loss: 0.509496  [   64/  179]
train() client id: f_00007-0-2 loss: 0.494022  [   96/  179]
train() client id: f_00007-0-3 loss: 0.283903  [  128/  179]
train() client id: f_00007-0-4 loss: 0.233160  [  160/  179]
train() client id: f_00007-1-0 loss: 0.336027  [   32/  179]
train() client id: f_00007-1-1 loss: 0.493004  [   64/  179]
train() client id: f_00007-1-2 loss: 0.290086  [   96/  179]
train() client id: f_00007-1-3 loss: 0.355340  [  128/  179]
train() client id: f_00007-1-4 loss: 0.198120  [  160/  179]
train() client id: f_00007-2-0 loss: 0.232392  [   32/  179]
train() client id: f_00007-2-1 loss: 0.349297  [   64/  179]
train() client id: f_00007-2-2 loss: 0.303033  [   96/  179]
train() client id: f_00007-2-3 loss: 0.328147  [  128/  179]
train() client id: f_00007-2-4 loss: 0.387965  [  160/  179]
train() client id: f_00007-3-0 loss: 0.341556  [   32/  179]
train() client id: f_00007-3-1 loss: 0.420424  [   64/  179]
train() client id: f_00007-3-2 loss: 0.304067  [   96/  179]
train() client id: f_00007-3-3 loss: 0.168738  [  128/  179]
train() client id: f_00007-3-4 loss: 0.337427  [  160/  179]
train() client id: f_00007-4-0 loss: 0.337785  [   32/  179]
train() client id: f_00007-4-1 loss: 0.295765  [   64/  179]
train() client id: f_00007-4-2 loss: 0.416288  [   96/  179]
train() client id: f_00007-4-3 loss: 0.185569  [  128/  179]
train() client id: f_00007-4-4 loss: 0.366656  [  160/  179]
train() client id: f_00007-5-0 loss: 0.258737  [   32/  179]
train() client id: f_00007-5-1 loss: 0.188916  [   64/  179]
train() client id: f_00007-5-2 loss: 0.450743  [   96/  179]
train() client id: f_00007-5-3 loss: 0.349682  [  128/  179]
train() client id: f_00007-5-4 loss: 0.309938  [  160/  179]
train() client id: f_00007-6-0 loss: 0.340975  [   32/  179]
train() client id: f_00007-6-1 loss: 0.168310  [   64/  179]
train() client id: f_00007-6-2 loss: 0.256362  [   96/  179]
train() client id: f_00007-6-3 loss: 0.333650  [  128/  179]
train() client id: f_00007-6-4 loss: 0.186067  [  160/  179]
train() client id: f_00007-7-0 loss: 0.256926  [   32/  179]
train() client id: f_00007-7-1 loss: 0.238405  [   64/  179]
train() client id: f_00007-7-2 loss: 0.572063  [   96/  179]
train() client id: f_00007-7-3 loss: 0.167118  [  128/  179]
train() client id: f_00007-7-4 loss: 0.163755  [  160/  179]
train() client id: f_00007-8-0 loss: 0.181153  [   32/  179]
train() client id: f_00007-8-1 loss: 0.128875  [   64/  179]
train() client id: f_00007-8-2 loss: 0.215973  [   96/  179]
train() client id: f_00007-8-3 loss: 0.366497  [  128/  179]
train() client id: f_00007-8-4 loss: 0.491098  [  160/  179]
train() client id: f_00007-9-0 loss: 0.174947  [   32/  179]
train() client id: f_00007-9-1 loss: 0.193251  [   64/  179]
train() client id: f_00007-9-2 loss: 0.115667  [   96/  179]
train() client id: f_00007-9-3 loss: 0.402030  [  128/  179]
train() client id: f_00007-9-4 loss: 0.232885  [  160/  179]
train() client id: f_00007-10-0 loss: 0.250440  [   32/  179]
train() client id: f_00007-10-1 loss: 0.342147  [   64/  179]
train() client id: f_00007-10-2 loss: 0.200805  [   96/  179]
train() client id: f_00007-10-3 loss: 0.205247  [  128/  179]
train() client id: f_00007-10-4 loss: 0.381690  [  160/  179]
train() client id: f_00007-11-0 loss: 0.301395  [   32/  179]
train() client id: f_00007-11-1 loss: 0.514469  [   64/  179]
train() client id: f_00007-11-2 loss: 0.328048  [   96/  179]
train() client id: f_00007-11-3 loss: 0.151518  [  128/  179]
train() client id: f_00007-11-4 loss: 0.147239  [  160/  179]
train() client id: f_00008-0-0 loss: 0.728043  [   32/  130]
train() client id: f_00008-0-1 loss: 0.770647  [   64/  130]
train() client id: f_00008-0-2 loss: 0.625722  [   96/  130]
train() client id: f_00008-0-3 loss: 0.666749  [  128/  130]
train() client id: f_00008-1-0 loss: 0.758834  [   32/  130]
train() client id: f_00008-1-1 loss: 0.761480  [   64/  130]
train() client id: f_00008-1-2 loss: 0.586356  [   96/  130]
train() client id: f_00008-1-3 loss: 0.669668  [  128/  130]
train() client id: f_00008-2-0 loss: 0.840020  [   32/  130]
train() client id: f_00008-2-1 loss: 0.751312  [   64/  130]
train() client id: f_00008-2-2 loss: 0.617161  [   96/  130]
train() client id: f_00008-2-3 loss: 0.606760  [  128/  130]
train() client id: f_00008-3-0 loss: 0.647510  [   32/  130]
train() client id: f_00008-3-1 loss: 0.794905  [   64/  130]
train() client id: f_00008-3-2 loss: 0.635621  [   96/  130]
train() client id: f_00008-3-3 loss: 0.709961  [  128/  130]
train() client id: f_00008-4-0 loss: 0.580571  [   32/  130]
train() client id: f_00008-4-1 loss: 0.798934  [   64/  130]
train() client id: f_00008-4-2 loss: 0.699640  [   96/  130]
train() client id: f_00008-4-3 loss: 0.727792  [  128/  130]
train() client id: f_00008-5-0 loss: 0.635381  [   32/  130]
train() client id: f_00008-5-1 loss: 0.683169  [   64/  130]
train() client id: f_00008-5-2 loss: 0.620988  [   96/  130]
train() client id: f_00008-5-3 loss: 0.856898  [  128/  130]
train() client id: f_00008-6-0 loss: 0.725768  [   32/  130]
train() client id: f_00008-6-1 loss: 0.657172  [   64/  130]
train() client id: f_00008-6-2 loss: 0.803822  [   96/  130]
train() client id: f_00008-6-3 loss: 0.589640  [  128/  130]
train() client id: f_00008-7-0 loss: 0.709657  [   32/  130]
train() client id: f_00008-7-1 loss: 0.746613  [   64/  130]
train() client id: f_00008-7-2 loss: 0.767361  [   96/  130]
train() client id: f_00008-7-3 loss: 0.580085  [  128/  130]
train() client id: f_00008-8-0 loss: 0.676704  [   32/  130]
train() client id: f_00008-8-1 loss: 0.588110  [   64/  130]
train() client id: f_00008-8-2 loss: 0.737388  [   96/  130]
train() client id: f_00008-8-3 loss: 0.785257  [  128/  130]
train() client id: f_00008-9-0 loss: 0.645580  [   32/  130]
train() client id: f_00008-9-1 loss: 0.845112  [   64/  130]
train() client id: f_00008-9-2 loss: 0.633044  [   96/  130]
train() client id: f_00008-9-3 loss: 0.681163  [  128/  130]
train() client id: f_00008-10-0 loss: 0.624675  [   32/  130]
train() client id: f_00008-10-1 loss: 0.769339  [   64/  130]
train() client id: f_00008-10-2 loss: 0.663379  [   96/  130]
train() client id: f_00008-10-3 loss: 0.734494  [  128/  130]
train() client id: f_00008-11-0 loss: 0.714915  [   32/  130]
train() client id: f_00008-11-1 loss: 0.692870  [   64/  130]
train() client id: f_00008-11-2 loss: 0.630115  [   96/  130]
train() client id: f_00008-11-3 loss: 0.751109  [  128/  130]
train() client id: f_00009-0-0 loss: 1.209448  [   32/  118]
train() client id: f_00009-0-1 loss: 1.252886  [   64/  118]
train() client id: f_00009-0-2 loss: 1.017612  [   96/  118]
train() client id: f_00009-1-0 loss: 1.182323  [   32/  118]
train() client id: f_00009-1-1 loss: 1.063436  [   64/  118]
train() client id: f_00009-1-2 loss: 1.135497  [   96/  118]
train() client id: f_00009-2-0 loss: 1.079818  [   32/  118]
train() client id: f_00009-2-1 loss: 1.115012  [   64/  118]
train() client id: f_00009-2-2 loss: 1.127239  [   96/  118]
train() client id: f_00009-3-0 loss: 1.038527  [   32/  118]
train() client id: f_00009-3-1 loss: 1.069708  [   64/  118]
train() client id: f_00009-3-2 loss: 1.112382  [   96/  118]
train() client id: f_00009-4-0 loss: 0.940042  [   32/  118]
train() client id: f_00009-4-1 loss: 1.116726  [   64/  118]
train() client id: f_00009-4-2 loss: 1.001491  [   96/  118]
train() client id: f_00009-5-0 loss: 1.009583  [   32/  118]
train() client id: f_00009-5-1 loss: 0.995280  [   64/  118]
train() client id: f_00009-5-2 loss: 1.042945  [   96/  118]
train() client id: f_00009-6-0 loss: 1.072541  [   32/  118]
train() client id: f_00009-6-1 loss: 0.935304  [   64/  118]
train() client id: f_00009-6-2 loss: 1.018597  [   96/  118]
train() client id: f_00009-7-0 loss: 0.940561  [   32/  118]
train() client id: f_00009-7-1 loss: 0.971641  [   64/  118]
train() client id: f_00009-7-2 loss: 1.089578  [   96/  118]
train() client id: f_00009-8-0 loss: 0.915706  [   32/  118]
train() client id: f_00009-8-1 loss: 1.001450  [   64/  118]
train() client id: f_00009-8-2 loss: 0.966205  [   96/  118]
train() client id: f_00009-9-0 loss: 0.930298  [   32/  118]
train() client id: f_00009-9-1 loss: 1.007903  [   64/  118]
train() client id: f_00009-9-2 loss: 0.878715  [   96/  118]
train() client id: f_00009-10-0 loss: 0.943209  [   32/  118]
train() client id: f_00009-10-1 loss: 0.953772  [   64/  118]
train() client id: f_00009-10-2 loss: 0.976073  [   96/  118]
train() client id: f_00009-11-0 loss: 0.914776  [   32/  118]
train() client id: f_00009-11-1 loss: 1.010713  [   64/  118]
train() client id: f_00009-11-2 loss: 0.924298  [   96/  118]
At round 37 accuracy: 0.6525198938992043
At round 37 training accuracy: 0.5942320590207915
At round 37 training loss: 0.8138776789821822
update_location
xs = [  -3.9056584     4.20031788  205.00902392   18.81129433    0.97929623
    3.95640986 -167.44319194 -146.32485185  189.66397685 -132.06087855]
ys = [ 197.5879595   180.55583871    1.32061395 -167.45517586  159.35018685
  142.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [221.4864689  206.44140467 228.10182794 195.94667825 188.13144625
 174.389033   195.04900171 177.23328842 215.13041181 165.69878545]
dists_bs = [172.57893742 179.28642244 417.81740916 393.49506386 176.67379362
 181.82773585 177.7856586  176.5222892  397.19558643 176.30729249]
uav_gains = [1.23220895e-11 1.55193540e-11 1.10553362e-11 1.80819760e-11
 2.02342599e-11 2.47219556e-11 1.83173556e-11 2.37047028e-11
 1.36151003e-11 2.81872466e-11]
bs_gains = [6.02182696e-11 5.41204454e-11 5.06444427e-12 5.99051385e-12
 5.63913003e-11 5.20290288e-11 5.54093755e-11 5.65269226e-11
 5.83554908e-12 5.67201424e-11]
Round 38
-------------------------------
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.97555343 12.35186937  5.88797184  2.12582729 14.24522634  6.85479284
  2.63358293  8.40017663  6.2003918   5.55926836]
obj_prev = 70.23466082817666
eta_min = 3.529677745883019e-16	eta_max = 0.9307450633473059
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 16.289713645049638	eta = 0.9090909090909091
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 30.212175618890367	eta = 0.49016101234197246
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 23.329211198156166	eta = 0.6347763094355793
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.083685449611664	eta = 0.6705778625672812
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017241376491434	eta = 0.6726015459057763
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017036233942203	eta = 0.6726078128344527
eta = 0.6726078128344527
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.03272589 0.06882827 0.03220643 0.01116836 0.07947717 0.03792048
 0.01402538 0.04649155 0.03376481 0.03064806]
ene_total = [1.98224589 3.51656478 1.97276077 0.93820235 4.0071535  2.08744961
 1.06916394 2.54721356 2.15142878 1.74485306]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 0 obj = 4.0896326721833045
eta = 0.6726078128344527
freqs = [32341261.25530905 64235440.52933019 32027116.84044752 10827227.71361229
 74093064.8966756  35427679.80345908 13589106.57226709 44562788.60807764
 35185358.32378028 28567479.83964483]
eta_min = 0.6726078128344555	eta_max = 0.6726078128344546
af = 0.00954296352207343	bf = 1.3351291140167194	zeta = 0.010497259874280773	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [2.05655253e-06 1.70628135e-05 1.98478162e-06 7.86607991e-08
 2.62139160e-05 2.85952327e-06 1.55607644e-07 5.54693540e-06
 2.51143918e-06 1.50273092e-06]
ene_total = [1.67955348 1.18486183 1.73204102 1.5156187  1.17665519 1.19197501
 1.51064412 1.41831402 2.11559328 1.17116856]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 1 obj = 4.0896326721833285
eta = 0.6726078128344546
freqs = [32341261.25530905 64235440.52933017 32027116.84044754 10827227.71361229
 74093064.89667559 35427679.80345907 13589106.57226709 44562788.60807764
 35185358.32378031 28567479.83964482]
Done!
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [7.90743628e-06 6.56064499e-05 7.63147743e-06 3.02450459e-07
 1.00792402e-04 1.09948555e-05 5.98310768e-07 2.13279445e-05
 9.65647363e-06 5.77799440e-06]
ene_total = [0.01007364 0.00715099 0.01038806 0.00908533 0.00712783 0.00715322
 0.00905573 0.00851762 0.0126887  0.00702464]
At round 38 energy consumption: 0.08826576524222389
At round 38 eta: 0.6726078128344546
At round 38 a_n: 15.165860671420319
At round 38 local rounds: 12.986465296587173
At round 38 global rounds: 46.323221096756726
gradient difference: 0.4546438157558441
train() client id: f_00000-0-0 loss: 1.275231  [   32/  126]
train() client id: f_00000-0-1 loss: 1.264127  [   64/  126]
train() client id: f_00000-0-2 loss: 1.249964  [   96/  126]
train() client id: f_00000-1-0 loss: 1.505874  [   32/  126]
train() client id: f_00000-1-1 loss: 0.979999  [   64/  126]
train() client id: f_00000-1-2 loss: 1.128183  [   96/  126]
train() client id: f_00000-2-0 loss: 1.058437  [   32/  126]
train() client id: f_00000-2-1 loss: 1.156427  [   64/  126]
train() client id: f_00000-2-2 loss: 1.094871  [   96/  126]
train() client id: f_00000-3-0 loss: 1.244908  [   32/  126]
train() client id: f_00000-3-1 loss: 0.929031  [   64/  126]
train() client id: f_00000-3-2 loss: 0.951873  [   96/  126]
train() client id: f_00000-4-0 loss: 1.140525  [   32/  126]
train() client id: f_00000-4-1 loss: 0.922737  [   64/  126]
train() client id: f_00000-4-2 loss: 0.985556  [   96/  126]
train() client id: f_00000-5-0 loss: 0.907238  [   32/  126]
train() client id: f_00000-5-1 loss: 1.051056  [   64/  126]
train() client id: f_00000-5-2 loss: 0.892112  [   96/  126]
train() client id: f_00000-6-0 loss: 0.914658  [   32/  126]
train() client id: f_00000-6-1 loss: 0.900621  [   64/  126]
train() client id: f_00000-6-2 loss: 0.953997  [   96/  126]
train() client id: f_00000-7-0 loss: 0.931476  [   32/  126]
train() client id: f_00000-7-1 loss: 0.999433  [   64/  126]
train() client id: f_00000-7-2 loss: 0.876615  [   96/  126]
train() client id: f_00000-8-0 loss: 0.862152  [   32/  126]
train() client id: f_00000-8-1 loss: 0.965381  [   64/  126]
train() client id: f_00000-8-2 loss: 0.945596  [   96/  126]
train() client id: f_00000-9-0 loss: 0.945929  [   32/  126]
train() client id: f_00000-9-1 loss: 0.921483  [   64/  126]
train() client id: f_00000-9-2 loss: 0.895596  [   96/  126]
train() client id: f_00000-10-0 loss: 0.945885  [   32/  126]
train() client id: f_00000-10-1 loss: 0.804728  [   64/  126]
train() client id: f_00000-10-2 loss: 0.918283  [   96/  126]
train() client id: f_00000-11-0 loss: 0.902794  [   32/  126]
train() client id: f_00000-11-1 loss: 0.847704  [   64/  126]
train() client id: f_00000-11-2 loss: 0.778115  [   96/  126]
train() client id: f_00001-0-0 loss: 0.435630  [   32/  265]
train() client id: f_00001-0-1 loss: 0.354636  [   64/  265]
train() client id: f_00001-0-2 loss: 0.449808  [   96/  265]
train() client id: f_00001-0-3 loss: 0.439307  [  128/  265]
train() client id: f_00001-0-4 loss: 0.456711  [  160/  265]
train() client id: f_00001-0-5 loss: 0.470235  [  192/  265]
train() client id: f_00001-0-6 loss: 0.416848  [  224/  265]
train() client id: f_00001-0-7 loss: 0.439185  [  256/  265]
train() client id: f_00001-1-0 loss: 0.377851  [   32/  265]
train() client id: f_00001-1-1 loss: 0.500745  [   64/  265]
train() client id: f_00001-1-2 loss: 0.504791  [   96/  265]
train() client id: f_00001-1-3 loss: 0.417568  [  128/  265]
train() client id: f_00001-1-4 loss: 0.334990  [  160/  265]
train() client id: f_00001-1-5 loss: 0.335497  [  192/  265]
train() client id: f_00001-1-6 loss: 0.440820  [  224/  265]
train() client id: f_00001-1-7 loss: 0.432004  [  256/  265]
train() client id: f_00001-2-0 loss: 0.472781  [   32/  265]
train() client id: f_00001-2-1 loss: 0.476010  [   64/  265]
train() client id: f_00001-2-2 loss: 0.380076  [   96/  265]
train() client id: f_00001-2-3 loss: 0.327150  [  128/  265]
train() client id: f_00001-2-4 loss: 0.458787  [  160/  265]
train() client id: f_00001-2-5 loss: 0.399152  [  192/  265]
train() client id: f_00001-2-6 loss: 0.324775  [  224/  265]
train() client id: f_00001-2-7 loss: 0.467574  [  256/  265]
train() client id: f_00001-3-0 loss: 0.366212  [   32/  265]
train() client id: f_00001-3-1 loss: 0.364769  [   64/  265]
train() client id: f_00001-3-2 loss: 0.426820  [   96/  265]
train() client id: f_00001-3-3 loss: 0.515652  [  128/  265]
train() client id: f_00001-3-4 loss: 0.396882  [  160/  265]
train() client id: f_00001-3-5 loss: 0.476127  [  192/  265]
train() client id: f_00001-3-6 loss: 0.338207  [  224/  265]
train() client id: f_00001-3-7 loss: 0.350599  [  256/  265]
train() client id: f_00001-4-0 loss: 0.302380  [   32/  265]
train() client id: f_00001-4-1 loss: 0.337113  [   64/  265]
train() client id: f_00001-4-2 loss: 0.313605  [   96/  265]
train() client id: f_00001-4-3 loss: 0.483477  [  128/  265]
train() client id: f_00001-4-4 loss: 0.388036  [  160/  265]
train() client id: f_00001-4-5 loss: 0.454255  [  192/  265]
train() client id: f_00001-4-6 loss: 0.380179  [  224/  265]
train() client id: f_00001-4-7 loss: 0.441765  [  256/  265]
train() client id: f_00001-5-0 loss: 0.390104  [   32/  265]
train() client id: f_00001-5-1 loss: 0.307832  [   64/  265]
train() client id: f_00001-5-2 loss: 0.297819  [   96/  265]
train() client id: f_00001-5-3 loss: 0.462992  [  128/  265]
train() client id: f_00001-5-4 loss: 0.449106  [  160/  265]
train() client id: f_00001-5-5 loss: 0.302487  [  192/  265]
train() client id: f_00001-5-6 loss: 0.547550  [  224/  265]
train() client id: f_00001-5-7 loss: 0.392722  [  256/  265]
train() client id: f_00001-6-0 loss: 0.437688  [   32/  265]
train() client id: f_00001-6-1 loss: 0.373854  [   64/  265]
train() client id: f_00001-6-2 loss: 0.412777  [   96/  265]
train() client id: f_00001-6-3 loss: 0.434511  [  128/  265]
train() client id: f_00001-6-4 loss: 0.316153  [  160/  265]
train() client id: f_00001-6-5 loss: 0.351430  [  192/  265]
train() client id: f_00001-6-6 loss: 0.339189  [  224/  265]
train() client id: f_00001-6-7 loss: 0.388957  [  256/  265]
train() client id: f_00001-7-0 loss: 0.531939  [   32/  265]
train() client id: f_00001-7-1 loss: 0.307922  [   64/  265]
train() client id: f_00001-7-2 loss: 0.372591  [   96/  265]
train() client id: f_00001-7-3 loss: 0.363273  [  128/  265]
train() client id: f_00001-7-4 loss: 0.370503  [  160/  265]
train() client id: f_00001-7-5 loss: 0.282258  [  192/  265]
train() client id: f_00001-7-6 loss: 0.309505  [  224/  265]
train() client id: f_00001-7-7 loss: 0.564966  [  256/  265]
train() client id: f_00001-8-0 loss: 0.309765  [   32/  265]
train() client id: f_00001-8-1 loss: 0.374524  [   64/  265]
train() client id: f_00001-8-2 loss: 0.443439  [   96/  265]
train() client id: f_00001-8-3 loss: 0.386493  [  128/  265]
train() client id: f_00001-8-4 loss: 0.270821  [  160/  265]
train() client id: f_00001-8-5 loss: 0.413374  [  192/  265]
train() client id: f_00001-8-6 loss: 0.387918  [  224/  265]
train() client id: f_00001-8-7 loss: 0.489292  [  256/  265]
train() client id: f_00001-9-0 loss: 0.399351  [   32/  265]
train() client id: f_00001-9-1 loss: 0.480736  [   64/  265]
train() client id: f_00001-9-2 loss: 0.350106  [   96/  265]
train() client id: f_00001-9-3 loss: 0.531435  [  128/  265]
train() client id: f_00001-9-4 loss: 0.360280  [  160/  265]
train() client id: f_00001-9-5 loss: 0.291453  [  192/  265]
train() client id: f_00001-9-6 loss: 0.272168  [  224/  265]
train() client id: f_00001-9-7 loss: 0.317841  [  256/  265]
train() client id: f_00001-10-0 loss: 0.359283  [   32/  265]
train() client id: f_00001-10-1 loss: 0.575519  [   64/  265]
train() client id: f_00001-10-2 loss: 0.379651  [   96/  265]
train() client id: f_00001-10-3 loss: 0.285909  [  128/  265]
train() client id: f_00001-10-4 loss: 0.272180  [  160/  265]
train() client id: f_00001-10-5 loss: 0.467461  [  192/  265]
train() client id: f_00001-10-6 loss: 0.421981  [  224/  265]
train() client id: f_00001-10-7 loss: 0.283764  [  256/  265]
train() client id: f_00001-11-0 loss: 0.369500  [   32/  265]
train() client id: f_00001-11-1 loss: 0.516976  [   64/  265]
train() client id: f_00001-11-2 loss: 0.410070  [   96/  265]
train() client id: f_00001-11-3 loss: 0.365123  [  128/  265]
train() client id: f_00001-11-4 loss: 0.271308  [  160/  265]
train() client id: f_00001-11-5 loss: 0.388052  [  192/  265]
train() client id: f_00001-11-6 loss: 0.281146  [  224/  265]
train() client id: f_00001-11-7 loss: 0.339863  [  256/  265]
train() client id: f_00002-0-0 loss: 0.981017  [   32/  124]
train() client id: f_00002-0-1 loss: 1.165848  [   64/  124]
train() client id: f_00002-0-2 loss: 0.953108  [   96/  124]
train() client id: f_00002-1-0 loss: 1.032579  [   32/  124]
train() client id: f_00002-1-1 loss: 1.024394  [   64/  124]
train() client id: f_00002-1-2 loss: 0.944474  [   96/  124]
train() client id: f_00002-2-0 loss: 0.897067  [   32/  124]
train() client id: f_00002-2-1 loss: 1.060638  [   64/  124]
train() client id: f_00002-2-2 loss: 1.169373  [   96/  124]
train() client id: f_00002-3-0 loss: 1.030892  [   32/  124]
train() client id: f_00002-3-1 loss: 0.913994  [   64/  124]
train() client id: f_00002-3-2 loss: 0.817503  [   96/  124]
train() client id: f_00002-4-0 loss: 0.846661  [   32/  124]
train() client id: f_00002-4-1 loss: 0.899023  [   64/  124]
train() client id: f_00002-4-2 loss: 0.979246  [   96/  124]
train() client id: f_00002-5-0 loss: 0.960907  [   32/  124]
train() client id: f_00002-5-1 loss: 0.871014  [   64/  124]
train() client id: f_00002-5-2 loss: 1.089657  [   96/  124]
train() client id: f_00002-6-0 loss: 0.869341  [   32/  124]
train() client id: f_00002-6-1 loss: 0.816048  [   64/  124]
train() client id: f_00002-6-2 loss: 0.999452  [   96/  124]
train() client id: f_00002-7-0 loss: 0.776801  [   32/  124]
train() client id: f_00002-7-1 loss: 0.919585  [   64/  124]
train() client id: f_00002-7-2 loss: 0.962197  [   96/  124]
train() client id: f_00002-8-0 loss: 0.838529  [   32/  124]
train() client id: f_00002-8-1 loss: 1.019406  [   64/  124]
train() client id: f_00002-8-2 loss: 0.878887  [   96/  124]
train() client id: f_00002-9-0 loss: 0.929444  [   32/  124]
train() client id: f_00002-9-1 loss: 0.704029  [   64/  124]
train() client id: f_00002-9-2 loss: 1.147509  [   96/  124]
train() client id: f_00002-10-0 loss: 0.876516  [   32/  124]
train() client id: f_00002-10-1 loss: 0.935511  [   64/  124]
train() client id: f_00002-10-2 loss: 0.796063  [   96/  124]
train() client id: f_00002-11-0 loss: 0.866166  [   32/  124]
train() client id: f_00002-11-1 loss: 1.025238  [   64/  124]
train() client id: f_00002-11-2 loss: 0.837404  [   96/  124]
train() client id: f_00003-0-0 loss: 0.769462  [   32/   43]
train() client id: f_00003-1-0 loss: 0.694635  [   32/   43]
train() client id: f_00003-2-0 loss: 0.798136  [   32/   43]
train() client id: f_00003-3-0 loss: 0.636680  [   32/   43]
train() client id: f_00003-4-0 loss: 0.924165  [   32/   43]
train() client id: f_00003-5-0 loss: 0.597793  [   32/   43]
train() client id: f_00003-6-0 loss: 0.722645  [   32/   43]
train() client id: f_00003-7-0 loss: 0.819116  [   32/   43]
train() client id: f_00003-8-0 loss: 0.782438  [   32/   43]
train() client id: f_00003-9-0 loss: 0.695948  [   32/   43]
train() client id: f_00003-10-0 loss: 0.750370  [   32/   43]
train() client id: f_00003-11-0 loss: 0.691361  [   32/   43]
train() client id: f_00004-0-0 loss: 0.837125  [   32/  306]
train() client id: f_00004-0-1 loss: 0.973506  [   64/  306]
train() client id: f_00004-0-2 loss: 0.871898  [   96/  306]
train() client id: f_00004-0-3 loss: 0.911634  [  128/  306]
train() client id: f_00004-0-4 loss: 0.825869  [  160/  306]
train() client id: f_00004-0-5 loss: 0.891178  [  192/  306]
train() client id: f_00004-0-6 loss: 0.904811  [  224/  306]
train() client id: f_00004-0-7 loss: 0.911147  [  256/  306]
train() client id: f_00004-0-8 loss: 0.908652  [  288/  306]
train() client id: f_00004-1-0 loss: 0.730404  [   32/  306]
train() client id: f_00004-1-1 loss: 0.840577  [   64/  306]
train() client id: f_00004-1-2 loss: 1.006124  [   96/  306]
train() client id: f_00004-1-3 loss: 0.850779  [  128/  306]
train() client id: f_00004-1-4 loss: 0.795033  [  160/  306]
train() client id: f_00004-1-5 loss: 0.814836  [  192/  306]
train() client id: f_00004-1-6 loss: 0.923353  [  224/  306]
train() client id: f_00004-1-7 loss: 0.980470  [  256/  306]
train() client id: f_00004-1-8 loss: 1.001426  [  288/  306]
train() client id: f_00004-2-0 loss: 0.756408  [   32/  306]
train() client id: f_00004-2-1 loss: 0.903982  [   64/  306]
train() client id: f_00004-2-2 loss: 0.935252  [   96/  306]
train() client id: f_00004-2-3 loss: 0.975698  [  128/  306]
train() client id: f_00004-2-4 loss: 0.824488  [  160/  306]
train() client id: f_00004-2-5 loss: 0.899036  [  192/  306]
train() client id: f_00004-2-6 loss: 0.823257  [  224/  306]
train() client id: f_00004-2-7 loss: 0.851040  [  256/  306]
train() client id: f_00004-2-8 loss: 1.020289  [  288/  306]
train() client id: f_00004-3-0 loss: 0.885932  [   32/  306]
train() client id: f_00004-3-1 loss: 0.922118  [   64/  306]
train() client id: f_00004-3-2 loss: 0.811214  [   96/  306]
train() client id: f_00004-3-3 loss: 0.824817  [  128/  306]
train() client id: f_00004-3-4 loss: 0.777912  [  160/  306]
train() client id: f_00004-3-5 loss: 0.970163  [  192/  306]
train() client id: f_00004-3-6 loss: 0.856416  [  224/  306]
train() client id: f_00004-3-7 loss: 0.872137  [  256/  306]
train() client id: f_00004-3-8 loss: 1.022512  [  288/  306]
train() client id: f_00004-4-0 loss: 0.892051  [   32/  306]
train() client id: f_00004-4-1 loss: 0.974180  [   64/  306]
train() client id: f_00004-4-2 loss: 0.837972  [   96/  306]
train() client id: f_00004-4-3 loss: 0.810806  [  128/  306]
train() client id: f_00004-4-4 loss: 0.916261  [  160/  306]
train() client id: f_00004-4-5 loss: 0.788648  [  192/  306]
train() client id: f_00004-4-6 loss: 0.976605  [  224/  306]
train() client id: f_00004-4-7 loss: 0.762010  [  256/  306]
train() client id: f_00004-4-8 loss: 0.974223  [  288/  306]
train() client id: f_00004-5-0 loss: 0.766921  [   32/  306]
train() client id: f_00004-5-1 loss: 1.009419  [   64/  306]
train() client id: f_00004-5-2 loss: 0.945687  [   96/  306]
train() client id: f_00004-5-3 loss: 0.845611  [  128/  306]
train() client id: f_00004-5-4 loss: 0.828404  [  160/  306]
train() client id: f_00004-5-5 loss: 0.985050  [  192/  306]
train() client id: f_00004-5-6 loss: 0.773560  [  224/  306]
train() client id: f_00004-5-7 loss: 0.950366  [  256/  306]
train() client id: f_00004-5-8 loss: 0.879018  [  288/  306]
train() client id: f_00004-6-0 loss: 0.894504  [   32/  306]
train() client id: f_00004-6-1 loss: 0.876227  [   64/  306]
train() client id: f_00004-6-2 loss: 0.956670  [   96/  306]
train() client id: f_00004-6-3 loss: 0.936180  [  128/  306]
train() client id: f_00004-6-4 loss: 0.729869  [  160/  306]
train() client id: f_00004-6-5 loss: 0.921996  [  192/  306]
train() client id: f_00004-6-6 loss: 0.709685  [  224/  306]
train() client id: f_00004-6-7 loss: 0.981212  [  256/  306]
train() client id: f_00004-6-8 loss: 0.909601  [  288/  306]
train() client id: f_00004-7-0 loss: 0.763216  [   32/  306]
train() client id: f_00004-7-1 loss: 0.841469  [   64/  306]
train() client id: f_00004-7-2 loss: 0.934954  [   96/  306]
train() client id: f_00004-7-3 loss: 0.865877  [  128/  306]
train() client id: f_00004-7-4 loss: 0.881551  [  160/  306]
train() client id: f_00004-7-5 loss: 0.952199  [  192/  306]
train() client id: f_00004-7-6 loss: 0.921541  [  224/  306]
train() client id: f_00004-7-7 loss: 0.915729  [  256/  306]
train() client id: f_00004-7-8 loss: 0.865986  [  288/  306]
train() client id: f_00004-8-0 loss: 0.723836  [   32/  306]
train() client id: f_00004-8-1 loss: 0.899058  [   64/  306]
train() client id: f_00004-8-2 loss: 0.973540  [   96/  306]
train() client id: f_00004-8-3 loss: 0.847002  [  128/  306]
train() client id: f_00004-8-4 loss: 0.963759  [  160/  306]
train() client id: f_00004-8-5 loss: 0.838539  [  192/  306]
train() client id: f_00004-8-6 loss: 0.865745  [  224/  306]
train() client id: f_00004-8-7 loss: 0.933958  [  256/  306]
train() client id: f_00004-8-8 loss: 0.828942  [  288/  306]
train() client id: f_00004-9-0 loss: 0.809018  [   32/  306]
train() client id: f_00004-9-1 loss: 0.922568  [   64/  306]
train() client id: f_00004-9-2 loss: 0.799463  [   96/  306]
train() client id: f_00004-9-3 loss: 0.861158  [  128/  306]
train() client id: f_00004-9-4 loss: 0.879131  [  160/  306]
train() client id: f_00004-9-5 loss: 0.882606  [  192/  306]
train() client id: f_00004-9-6 loss: 0.958324  [  224/  306]
train() client id: f_00004-9-7 loss: 0.848561  [  256/  306]
train() client id: f_00004-9-8 loss: 0.917297  [  288/  306]
train() client id: f_00004-10-0 loss: 0.958629  [   32/  306]
train() client id: f_00004-10-1 loss: 0.842274  [   64/  306]
train() client id: f_00004-10-2 loss: 0.866299  [   96/  306]
train() client id: f_00004-10-3 loss: 0.991079  [  128/  306]
train() client id: f_00004-10-4 loss: 0.792811  [  160/  306]
train() client id: f_00004-10-5 loss: 0.802982  [  192/  306]
train() client id: f_00004-10-6 loss: 0.840369  [  224/  306]
train() client id: f_00004-10-7 loss: 0.844663  [  256/  306]
train() client id: f_00004-10-8 loss: 0.887373  [  288/  306]
train() client id: f_00004-11-0 loss: 0.719457  [   32/  306]
train() client id: f_00004-11-1 loss: 0.929386  [   64/  306]
train() client id: f_00004-11-2 loss: 0.856842  [   96/  306]
train() client id: f_00004-11-3 loss: 0.857170  [  128/  306]
train() client id: f_00004-11-4 loss: 1.003072  [  160/  306]
train() client id: f_00004-11-5 loss: 0.963118  [  192/  306]
train() client id: f_00004-11-6 loss: 0.828074  [  224/  306]
train() client id: f_00004-11-7 loss: 0.914701  [  256/  306]
train() client id: f_00004-11-8 loss: 0.713534  [  288/  306]
train() client id: f_00005-0-0 loss: 0.435233  [   32/  146]
train() client id: f_00005-0-1 loss: 0.356350  [   64/  146]
train() client id: f_00005-0-2 loss: 0.655950  [   96/  146]
train() client id: f_00005-0-3 loss: 0.571761  [  128/  146]
train() client id: f_00005-1-0 loss: 0.345442  [   32/  146]
train() client id: f_00005-1-1 loss: 0.442570  [   64/  146]
train() client id: f_00005-1-2 loss: 0.524931  [   96/  146]
train() client id: f_00005-1-3 loss: 0.612162  [  128/  146]
train() client id: f_00005-2-0 loss: 0.392867  [   32/  146]
train() client id: f_00005-2-1 loss: 0.371599  [   64/  146]
train() client id: f_00005-2-2 loss: 0.719687  [   96/  146]
train() client id: f_00005-2-3 loss: 0.349062  [  128/  146]
train() client id: f_00005-3-0 loss: 0.473789  [   32/  146]
train() client id: f_00005-3-1 loss: 0.600766  [   64/  146]
train() client id: f_00005-3-2 loss: 0.395382  [   96/  146]
train() client id: f_00005-3-3 loss: 0.574003  [  128/  146]
train() client id: f_00005-4-0 loss: 0.422063  [   32/  146]
train() client id: f_00005-4-1 loss: 0.450633  [   64/  146]
train() client id: f_00005-4-2 loss: 0.611603  [   96/  146]
train() client id: f_00005-4-3 loss: 0.501893  [  128/  146]
train() client id: f_00005-5-0 loss: 0.606236  [   32/  146]
train() client id: f_00005-5-1 loss: 0.528171  [   64/  146]
train() client id: f_00005-5-2 loss: 0.594145  [   96/  146]
train() client id: f_00005-5-3 loss: 0.426649  [  128/  146]
train() client id: f_00005-6-0 loss: 0.345128  [   32/  146]
train() client id: f_00005-6-1 loss: 0.787076  [   64/  146]
train() client id: f_00005-6-2 loss: 0.197263  [   96/  146]
train() client id: f_00005-6-3 loss: 0.492211  [  128/  146]
train() client id: f_00005-7-0 loss: 0.956334  [   32/  146]
train() client id: f_00005-7-1 loss: 0.352229  [   64/  146]
train() client id: f_00005-7-2 loss: 0.249132  [   96/  146]
train() client id: f_00005-7-3 loss: 0.524034  [  128/  146]
train() client id: f_00005-8-0 loss: 0.408444  [   32/  146]
train() client id: f_00005-8-1 loss: 0.424228  [   64/  146]
train() client id: f_00005-8-2 loss: 0.649560  [   96/  146]
train() client id: f_00005-8-3 loss: 0.444648  [  128/  146]
train() client id: f_00005-9-0 loss: 0.350923  [   32/  146]
train() client id: f_00005-9-1 loss: 0.466632  [   64/  146]
train() client id: f_00005-9-2 loss: 0.520669  [   96/  146]
train() client id: f_00005-9-3 loss: 0.618791  [  128/  146]
train() client id: f_00005-10-0 loss: 0.529138  [   32/  146]
train() client id: f_00005-10-1 loss: 0.313081  [   64/  146]
train() client id: f_00005-10-2 loss: 0.248276  [   96/  146]
train() client id: f_00005-10-3 loss: 0.795350  [  128/  146]
train() client id: f_00005-11-0 loss: 0.532468  [   32/  146]
train() client id: f_00005-11-1 loss: 0.529424  [   64/  146]
train() client id: f_00005-11-2 loss: 0.375059  [   96/  146]
train() client id: f_00005-11-3 loss: 0.433874  [  128/  146]
train() client id: f_00006-0-0 loss: 0.477096  [   32/   54]
train() client id: f_00006-1-0 loss: 0.536097  [   32/   54]
train() client id: f_00006-2-0 loss: 0.472479  [   32/   54]
train() client id: f_00006-3-0 loss: 0.516247  [   32/   54]
train() client id: f_00006-4-0 loss: 0.576169  [   32/   54]
train() client id: f_00006-5-0 loss: 0.514000  [   32/   54]
train() client id: f_00006-6-0 loss: 0.529336  [   32/   54]
train() client id: f_00006-7-0 loss: 0.514622  [   32/   54]
train() client id: f_00006-8-0 loss: 0.536507  [   32/   54]
train() client id: f_00006-9-0 loss: 0.454173  [   32/   54]
train() client id: f_00006-10-0 loss: 0.577090  [   32/   54]
train() client id: f_00006-11-0 loss: 0.468406  [   32/   54]
train() client id: f_00007-0-0 loss: 0.788153  [   32/  179]
train() client id: f_00007-0-1 loss: 0.735063  [   64/  179]
train() client id: f_00007-0-2 loss: 0.695611  [   96/  179]
train() client id: f_00007-0-3 loss: 0.745494  [  128/  179]
train() client id: f_00007-0-4 loss: 0.631901  [  160/  179]
train() client id: f_00007-1-0 loss: 0.567550  [   32/  179]
train() client id: f_00007-1-1 loss: 0.826278  [   64/  179]
train() client id: f_00007-1-2 loss: 0.759849  [   96/  179]
train() client id: f_00007-1-3 loss: 0.850463  [  128/  179]
train() client id: f_00007-1-4 loss: 0.653726  [  160/  179]
train() client id: f_00007-2-0 loss: 0.684613  [   32/  179]
train() client id: f_00007-2-1 loss: 0.683770  [   64/  179]
train() client id: f_00007-2-2 loss: 0.855235  [   96/  179]
train() client id: f_00007-2-3 loss: 0.656183  [  128/  179]
train() client id: f_00007-2-4 loss: 0.769333  [  160/  179]
train() client id: f_00007-3-0 loss: 0.810996  [   32/  179]
train() client id: f_00007-3-1 loss: 0.631500  [   64/  179]
train() client id: f_00007-3-2 loss: 0.693351  [   96/  179]
train() client id: f_00007-3-3 loss: 0.789648  [  128/  179]
train() client id: f_00007-3-4 loss: 0.694956  [  160/  179]
train() client id: f_00007-4-0 loss: 0.628530  [   32/  179]
train() client id: f_00007-4-1 loss: 0.626978  [   64/  179]
train() client id: f_00007-4-2 loss: 0.694246  [   96/  179]
train() client id: f_00007-4-3 loss: 0.647846  [  128/  179]
train() client id: f_00007-4-4 loss: 0.822227  [  160/  179]
train() client id: f_00007-5-0 loss: 0.659008  [   32/  179]
train() client id: f_00007-5-1 loss: 0.593778  [   64/  179]
train() client id: f_00007-5-2 loss: 0.636279  [   96/  179]
train() client id: f_00007-5-3 loss: 0.859682  [  128/  179]
train() client id: f_00007-5-4 loss: 0.630821  [  160/  179]
train() client id: f_00007-6-0 loss: 0.675312  [   32/  179]
train() client id: f_00007-6-1 loss: 0.703895  [   64/  179]
train() client id: f_00007-6-2 loss: 0.594657  [   96/  179]
train() client id: f_00007-6-3 loss: 0.903415  [  128/  179]
train() client id: f_00007-6-4 loss: 0.563699  [  160/  179]
train() client id: f_00007-7-0 loss: 0.728316  [   32/  179]
train() client id: f_00007-7-1 loss: 0.750322  [   64/  179]
train() client id: f_00007-7-2 loss: 0.834682  [   96/  179]
train() client id: f_00007-7-3 loss: 0.655337  [  128/  179]
train() client id: f_00007-7-4 loss: 0.608549  [  160/  179]
train() client id: f_00007-8-0 loss: 0.684008  [   32/  179]
train() client id: f_00007-8-1 loss: 0.568019  [   64/  179]
train() client id: f_00007-8-2 loss: 0.720471  [   96/  179]
train() client id: f_00007-8-3 loss: 0.668960  [  128/  179]
train() client id: f_00007-8-4 loss: 0.826468  [  160/  179]
train() client id: f_00007-9-0 loss: 0.854969  [   32/  179]
train() client id: f_00007-9-1 loss: 0.666257  [   64/  179]
train() client id: f_00007-9-2 loss: 0.578728  [   96/  179]
train() client id: f_00007-9-3 loss: 0.662120  [  128/  179]
train() client id: f_00007-9-4 loss: 0.802911  [  160/  179]
train() client id: f_00007-10-0 loss: 0.535393  [   32/  179]
train() client id: f_00007-10-1 loss: 0.793583  [   64/  179]
train() client id: f_00007-10-2 loss: 0.656953  [   96/  179]
train() client id: f_00007-10-3 loss: 0.755780  [  128/  179]
train() client id: f_00007-10-4 loss: 0.806852  [  160/  179]
train() client id: f_00007-11-0 loss: 0.717266  [   32/  179]
train() client id: f_00007-11-1 loss: 0.622901  [   64/  179]
train() client id: f_00007-11-2 loss: 0.907696  [   96/  179]
train() client id: f_00007-11-3 loss: 0.551880  [  128/  179]
train() client id: f_00007-11-4 loss: 0.771965  [  160/  179]
train() client id: f_00008-0-0 loss: 0.656806  [   32/  130]
train() client id: f_00008-0-1 loss: 0.749498  [   64/  130]
train() client id: f_00008-0-2 loss: 0.660292  [   96/  130]
train() client id: f_00008-0-3 loss: 0.609475  [  128/  130]
train() client id: f_00008-1-0 loss: 0.787525  [   32/  130]
train() client id: f_00008-1-1 loss: 0.524745  [   64/  130]
train() client id: f_00008-1-2 loss: 0.698157  [   96/  130]
train() client id: f_00008-1-3 loss: 0.640782  [  128/  130]
train() client id: f_00008-2-0 loss: 0.644642  [   32/  130]
train() client id: f_00008-2-1 loss: 0.678016  [   64/  130]
train() client id: f_00008-2-2 loss: 0.605968  [   96/  130]
train() client id: f_00008-2-3 loss: 0.747341  [  128/  130]
train() client id: f_00008-3-0 loss: 0.728356  [   32/  130]
train() client id: f_00008-3-1 loss: 0.634844  [   64/  130]
train() client id: f_00008-3-2 loss: 0.557671  [   96/  130]
train() client id: f_00008-3-3 loss: 0.764541  [  128/  130]
train() client id: f_00008-4-0 loss: 0.628507  [   32/  130]
train() client id: f_00008-4-1 loss: 0.731055  [   64/  130]
train() client id: f_00008-4-2 loss: 0.609747  [   96/  130]
train() client id: f_00008-4-3 loss: 0.639369  [  128/  130]
train() client id: f_00008-5-0 loss: 0.717558  [   32/  130]
train() client id: f_00008-5-1 loss: 0.604807  [   64/  130]
train() client id: f_00008-5-2 loss: 0.651124  [   96/  130]
train() client id: f_00008-5-3 loss: 0.705078  [  128/  130]
train() client id: f_00008-6-0 loss: 0.701611  [   32/  130]
train() client id: f_00008-6-1 loss: 0.624406  [   64/  130]
train() client id: f_00008-6-2 loss: 0.652248  [   96/  130]
train() client id: f_00008-6-3 loss: 0.701435  [  128/  130]
train() client id: f_00008-7-0 loss: 0.839960  [   32/  130]
train() client id: f_00008-7-1 loss: 0.662147  [   64/  130]
train() client id: f_00008-7-2 loss: 0.617465  [   96/  130]
train() client id: f_00008-7-3 loss: 0.536893  [  128/  130]
train() client id: f_00008-8-0 loss: 0.615215  [   32/  130]
train() client id: f_00008-8-1 loss: 0.650811  [   64/  130]
train() client id: f_00008-8-2 loss: 0.733940  [   96/  130]
train() client id: f_00008-8-3 loss: 0.681685  [  128/  130]
train() client id: f_00008-9-0 loss: 0.596017  [   32/  130]
train() client id: f_00008-9-1 loss: 0.653392  [   64/  130]
train() client id: f_00008-9-2 loss: 0.685466  [   96/  130]
train() client id: f_00008-9-3 loss: 0.723054  [  128/  130]
train() client id: f_00008-10-0 loss: 0.731175  [   32/  130]
train() client id: f_00008-10-1 loss: 0.551612  [   64/  130]
train() client id: f_00008-10-2 loss: 0.703372  [   96/  130]
train() client id: f_00008-10-3 loss: 0.668693  [  128/  130]
train() client id: f_00008-11-0 loss: 0.744316  [   32/  130]
train() client id: f_00008-11-1 loss: 0.571575  [   64/  130]
train() client id: f_00008-11-2 loss: 0.740832  [   96/  130]
train() client id: f_00008-11-3 loss: 0.636605  [  128/  130]
train() client id: f_00009-0-0 loss: 1.315846  [   32/  118]
train() client id: f_00009-0-1 loss: 1.057100  [   64/  118]
train() client id: f_00009-0-2 loss: 0.998796  [   96/  118]
train() client id: f_00009-1-0 loss: 1.075973  [   32/  118]
train() client id: f_00009-1-1 loss: 1.014579  [   64/  118]
train() client id: f_00009-1-2 loss: 1.065679  [   96/  118]
train() client id: f_00009-2-0 loss: 1.207330  [   32/  118]
train() client id: f_00009-2-1 loss: 0.951159  [   64/  118]
train() client id: f_00009-2-2 loss: 0.949355  [   96/  118]
train() client id: f_00009-3-0 loss: 0.893464  [   32/  118]
train() client id: f_00009-3-1 loss: 0.995403  [   64/  118]
train() client id: f_00009-3-2 loss: 0.922222  [   96/  118]
train() client id: f_00009-4-0 loss: 1.074316  [   32/  118]
train() client id: f_00009-4-1 loss: 0.908046  [   64/  118]
train() client id: f_00009-4-2 loss: 0.883424  [   96/  118]
train() client id: f_00009-5-0 loss: 0.838874  [   32/  118]
train() client id: f_00009-5-1 loss: 0.904313  [   64/  118]
train() client id: f_00009-5-2 loss: 1.043444  [   96/  118]
train() client id: f_00009-6-0 loss: 0.782813  [   32/  118]
train() client id: f_00009-6-1 loss: 1.151319  [   64/  118]
train() client id: f_00009-6-2 loss: 0.835044  [   96/  118]
train() client id: f_00009-7-0 loss: 1.030528  [   32/  118]
train() client id: f_00009-7-1 loss: 0.853641  [   64/  118]
train() client id: f_00009-7-2 loss: 0.808833  [   96/  118]
train() client id: f_00009-8-0 loss: 0.912273  [   32/  118]
train() client id: f_00009-8-1 loss: 0.895832  [   64/  118]
train() client id: f_00009-8-2 loss: 0.887680  [   96/  118]
train() client id: f_00009-9-0 loss: 0.851181  [   32/  118]
train() client id: f_00009-9-1 loss: 0.981181  [   64/  118]
train() client id: f_00009-9-2 loss: 0.800065  [   96/  118]
train() client id: f_00009-10-0 loss: 0.945104  [   32/  118]
train() client id: f_00009-10-1 loss: 0.988116  [   64/  118]
train() client id: f_00009-10-2 loss: 0.656564  [   96/  118]
train() client id: f_00009-11-0 loss: 0.838247  [   32/  118]
train() client id: f_00009-11-1 loss: 0.824990  [   64/  118]
train() client id: f_00009-11-2 loss: 0.950816  [   96/  118]
At round 38 accuracy: 0.6525198938992043
At round 38 training accuracy: 0.5821596244131455
At round 38 training loss: 0.8333359960106005
update_location
xs = [  -3.9056584     4.20031788  210.00902392   18.81129433    0.97929623
    3.95640986 -172.44319194 -151.32485185  194.66397685 -137.06087855]
ys = [ 202.5879595   185.55583871    1.32061395 -172.45517586  164.35018685
  147.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [225.95826053 210.82839455 232.60596327 200.23649137 192.3848823
 178.50679634 199.35783152 181.38326009 219.5512101  169.71062514]
dists_bs = [173.30426779 179.51094579 422.3700719  397.85414041 176.30125133
 181.00957718 177.64338525 175.77931364 401.79095764 175.15670175]
uav_gains = [1.14573089e-11 1.45365625e-11 1.02368353e-11 1.69945603e-11
 1.90332055e-11 2.32656486e-11 1.72124693e-11 2.23088121e-11
 1.27076078e-11 2.65155347e-11]
bs_gains = [5.95152365e-11 5.39311234e-11 4.91307394e-12 5.80854309e-12
 5.67255845e-11 5.26901863e-11 5.55337207e-11 5.71984612e-11
 5.65058798e-12 5.77695734e-11]
Round 39
-------------------------------
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.84371614 12.07295727  5.75849455  2.07999855 13.92335007  6.69969194
  2.5762787   8.21232374  6.06245015  5.43331894]
obj_prev = 68.66258004963703
eta_min = 1.603615121775097e-16	eta_max = 0.9314544727787696
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 15.921783734685466	eta = 0.909090909090909
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 29.685339818895304	eta = 0.4875924930628838
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 22.863992502600777	eta = 0.6330630509114977
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.629256357976853	eta = 0.6692023345673616
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.563057577188566	eta = 0.6712567917560257
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.562850936268624	eta = 0.6712632245380996
eta = 0.6712632245380996
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.03289102 0.06917555 0.03236894 0.01122471 0.07987818 0.03811181
 0.01409615 0.04672613 0.03393518 0.0308027 ]
ene_total = [1.94642197 3.43896482 1.93822068 0.92238666 3.91833323 2.03975789
 1.05049566 2.49573328 2.10825374 1.704283  ]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 0 obj = 4.023474514862622
eta = 0.6712632245380996
freqs = [31667787.3789384  62754800.17478311 31370631.5489173  10594331.96949695
 72369920.453618   34595378.85521331 13296888.56956221 43600515.27273497
 34356690.70712458 27894452.74815087]
eta_min = 0.6712632245381114	eta_max = 0.6712632245380962
af = 0.008905236727720691	bf = 1.3191144648464428	zeta = 0.009795760400492761	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [1.97179310e-06 1.62852764e-05 1.90424825e-06 7.53131768e-08
 2.50088069e-05 2.72674418e-06 1.48987275e-07 5.30996467e-06
 2.39453594e-06 1.43275870e-06]
ene_total = [1.67569409 1.15879127 1.73115483 1.50519706 1.14852664 1.1620451
 1.50025809 1.40670146 2.09070673 1.14052021]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 1 obj = 4.0234745148625795
eta = 0.6712632245380962
freqs = [31667787.37893841 62754800.17478317 31370631.54891731 10594331.96949696
 72369920.4536181  34595378.85521334 13296888.56956222 43600515.27273501
 34356690.70712457 27894452.7481509 ]
Done!
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.21333132e-06 6.78348912e-05 7.93197918e-06 3.13710436e-07
 1.04171993e-04 1.13580139e-05 6.20593432e-07 2.21181924e-05
 9.97422956e-06 5.96803079e-06]
ene_total = [0.01028301 0.00715824 0.01062293 0.00923138 0.0071229  0.00713527
 0.00920132 0.00864389 0.01282956 0.00699917]
At round 39 energy consumption: 0.08922767374894407
At round 39 eta: 0.6712632245380962
At round 39 a_n: 14.823314824451002
At round 39 local rounds: 13.051990433476096
At round 39 global rounds: 45.091744918477566
gradient difference: 0.5246683955192566
train() client id: f_00000-0-0 loss: 1.109195  [   32/  126]
train() client id: f_00000-0-1 loss: 1.115490  [   64/  126]
train() client id: f_00000-0-2 loss: 0.935499  [   96/  126]
train() client id: f_00000-1-0 loss: 0.993517  [   32/  126]
train() client id: f_00000-1-1 loss: 1.041960  [   64/  126]
train() client id: f_00000-1-2 loss: 0.949184  [   96/  126]
train() client id: f_00000-2-0 loss: 0.987354  [   32/  126]
train() client id: f_00000-2-1 loss: 0.952342  [   64/  126]
train() client id: f_00000-2-2 loss: 0.922892  [   96/  126]
train() client id: f_00000-3-0 loss: 0.934081  [   32/  126]
train() client id: f_00000-3-1 loss: 0.920575  [   64/  126]
train() client id: f_00000-3-2 loss: 0.951711  [   96/  126]
train() client id: f_00000-4-0 loss: 0.857157  [   32/  126]
train() client id: f_00000-4-1 loss: 0.845301  [   64/  126]
train() client id: f_00000-4-2 loss: 0.990483  [   96/  126]
train() client id: f_00000-5-0 loss: 0.960415  [   32/  126]
train() client id: f_00000-5-1 loss: 0.927627  [   64/  126]
train() client id: f_00000-5-2 loss: 0.898825  [   96/  126]
train() client id: f_00000-6-0 loss: 1.041882  [   32/  126]
train() client id: f_00000-6-1 loss: 0.896771  [   64/  126]
train() client id: f_00000-6-2 loss: 0.870618  [   96/  126]
train() client id: f_00000-7-0 loss: 0.939041  [   32/  126]
train() client id: f_00000-7-1 loss: 0.841654  [   64/  126]
train() client id: f_00000-7-2 loss: 0.977782  [   96/  126]
train() client id: f_00000-8-0 loss: 0.907674  [   32/  126]
train() client id: f_00000-8-1 loss: 0.879034  [   64/  126]
train() client id: f_00000-8-2 loss: 0.948904  [   96/  126]
train() client id: f_00000-9-0 loss: 0.989863  [   32/  126]
train() client id: f_00000-9-1 loss: 0.888553  [   64/  126]
train() client id: f_00000-9-2 loss: 0.809625  [   96/  126]
train() client id: f_00000-10-0 loss: 0.920591  [   32/  126]
train() client id: f_00000-10-1 loss: 1.036806  [   64/  126]
train() client id: f_00000-10-2 loss: 0.908796  [   96/  126]
train() client id: f_00000-11-0 loss: 0.927702  [   32/  126]
train() client id: f_00000-11-1 loss: 0.927015  [   64/  126]
train() client id: f_00000-11-2 loss: 0.974892  [   96/  126]
train() client id: f_00000-12-0 loss: 0.833956  [   32/  126]
train() client id: f_00000-12-1 loss: 1.055603  [   64/  126]
train() client id: f_00000-12-2 loss: 0.917673  [   96/  126]
train() client id: f_00001-0-0 loss: 0.434017  [   32/  265]
train() client id: f_00001-0-1 loss: 0.527173  [   64/  265]
train() client id: f_00001-0-2 loss: 0.447249  [   96/  265]
train() client id: f_00001-0-3 loss: 0.445397  [  128/  265]
train() client id: f_00001-0-4 loss: 0.459331  [  160/  265]
train() client id: f_00001-0-5 loss: 0.617455  [  192/  265]
train() client id: f_00001-0-6 loss: 0.394878  [  224/  265]
train() client id: f_00001-0-7 loss: 0.467137  [  256/  265]
train() client id: f_00001-1-0 loss: 0.496339  [   32/  265]
train() client id: f_00001-1-1 loss: 0.428017  [   64/  265]
train() client id: f_00001-1-2 loss: 0.460608  [   96/  265]
train() client id: f_00001-1-3 loss: 0.433030  [  128/  265]
train() client id: f_00001-1-4 loss: 0.382736  [  160/  265]
train() client id: f_00001-1-5 loss: 0.513002  [  192/  265]
train() client id: f_00001-1-6 loss: 0.451753  [  224/  265]
train() client id: f_00001-1-7 loss: 0.519843  [  256/  265]
train() client id: f_00001-2-0 loss: 0.481167  [   32/  265]
train() client id: f_00001-2-1 loss: 0.455253  [   64/  265]
train() client id: f_00001-2-2 loss: 0.418750  [   96/  265]
train() client id: f_00001-2-3 loss: 0.560602  [  128/  265]
train() client id: f_00001-2-4 loss: 0.570246  [  160/  265]
train() client id: f_00001-2-5 loss: 0.428317  [  192/  265]
train() client id: f_00001-2-6 loss: 0.449501  [  224/  265]
train() client id: f_00001-2-7 loss: 0.338279  [  256/  265]
train() client id: f_00001-3-0 loss: 0.386443  [   32/  265]
train() client id: f_00001-3-1 loss: 0.505449  [   64/  265]
train() client id: f_00001-3-2 loss: 0.410051  [   96/  265]
train() client id: f_00001-3-3 loss: 0.491004  [  128/  265]
train() client id: f_00001-3-4 loss: 0.436927  [  160/  265]
train() client id: f_00001-3-5 loss: 0.464151  [  192/  265]
train() client id: f_00001-3-6 loss: 0.539502  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411575  [  256/  265]
train() client id: f_00001-4-0 loss: 0.404588  [   32/  265]
train() client id: f_00001-4-1 loss: 0.469109  [   64/  265]
train() client id: f_00001-4-2 loss: 0.498798  [   96/  265]
train() client id: f_00001-4-3 loss: 0.404291  [  128/  265]
train() client id: f_00001-4-4 loss: 0.429682  [  160/  265]
train() client id: f_00001-4-5 loss: 0.487805  [  192/  265]
train() client id: f_00001-4-6 loss: 0.437275  [  224/  265]
train() client id: f_00001-4-7 loss: 0.438682  [  256/  265]
train() client id: f_00001-5-0 loss: 0.381519  [   32/  265]
train() client id: f_00001-5-1 loss: 0.537474  [   64/  265]
train() client id: f_00001-5-2 loss: 0.461797  [   96/  265]
train() client id: f_00001-5-3 loss: 0.515481  [  128/  265]
train() client id: f_00001-5-4 loss: 0.413985  [  160/  265]
train() client id: f_00001-5-5 loss: 0.395278  [  192/  265]
train() client id: f_00001-5-6 loss: 0.375020  [  224/  265]
train() client id: f_00001-5-7 loss: 0.441061  [  256/  265]
train() client id: f_00001-6-0 loss: 0.402543  [   32/  265]
train() client id: f_00001-6-1 loss: 0.418872  [   64/  265]
train() client id: f_00001-6-2 loss: 0.458611  [   96/  265]
train() client id: f_00001-6-3 loss: 0.417709  [  128/  265]
train() client id: f_00001-6-4 loss: 0.568522  [  160/  265]
train() client id: f_00001-6-5 loss: 0.511293  [  192/  265]
train() client id: f_00001-6-6 loss: 0.458500  [  224/  265]
train() client id: f_00001-6-7 loss: 0.375210  [  256/  265]
train() client id: f_00001-7-0 loss: 0.435868  [   32/  265]
train() client id: f_00001-7-1 loss: 0.387829  [   64/  265]
train() client id: f_00001-7-2 loss: 0.379113  [   96/  265]
train() client id: f_00001-7-3 loss: 0.506502  [  128/  265]
train() client id: f_00001-7-4 loss: 0.488525  [  160/  265]
train() client id: f_00001-7-5 loss: 0.420986  [  192/  265]
train() client id: f_00001-7-6 loss: 0.460757  [  224/  265]
train() client id: f_00001-7-7 loss: 0.461218  [  256/  265]
train() client id: f_00001-8-0 loss: 0.449808  [   32/  265]
train() client id: f_00001-8-1 loss: 0.356336  [   64/  265]
train() client id: f_00001-8-2 loss: 0.347997  [   96/  265]
train() client id: f_00001-8-3 loss: 0.421591  [  128/  265]
train() client id: f_00001-8-4 loss: 0.396087  [  160/  265]
train() client id: f_00001-8-5 loss: 0.572849  [  192/  265]
train() client id: f_00001-8-6 loss: 0.536680  [  224/  265]
train() client id: f_00001-8-7 loss: 0.518739  [  256/  265]
train() client id: f_00001-9-0 loss: 0.480389  [   32/  265]
train() client id: f_00001-9-1 loss: 0.419287  [   64/  265]
train() client id: f_00001-9-2 loss: 0.499629  [   96/  265]
train() client id: f_00001-9-3 loss: 0.458100  [  128/  265]
train() client id: f_00001-9-4 loss: 0.439887  [  160/  265]
train() client id: f_00001-9-5 loss: 0.456688  [  192/  265]
train() client id: f_00001-9-6 loss: 0.355298  [  224/  265]
train() client id: f_00001-9-7 loss: 0.392477  [  256/  265]
train() client id: f_00001-10-0 loss: 0.500077  [   32/  265]
train() client id: f_00001-10-1 loss: 0.442650  [   64/  265]
train() client id: f_00001-10-2 loss: 0.397704  [   96/  265]
train() client id: f_00001-10-3 loss: 0.494673  [  128/  265]
train() client id: f_00001-10-4 loss: 0.419178  [  160/  265]
train() client id: f_00001-10-5 loss: 0.521869  [  192/  265]
train() client id: f_00001-10-6 loss: 0.369913  [  224/  265]
train() client id: f_00001-10-7 loss: 0.448443  [  256/  265]
train() client id: f_00001-11-0 loss: 0.547821  [   32/  265]
train() client id: f_00001-11-1 loss: 0.412842  [   64/  265]
train() client id: f_00001-11-2 loss: 0.363546  [   96/  265]
train() client id: f_00001-11-3 loss: 0.345339  [  128/  265]
train() client id: f_00001-11-4 loss: 0.463311  [  160/  265]
train() client id: f_00001-11-5 loss: 0.368719  [  192/  265]
train() client id: f_00001-11-6 loss: 0.353676  [  224/  265]
train() client id: f_00001-11-7 loss: 0.704597  [  256/  265]
train() client id: f_00001-12-0 loss: 0.509390  [   32/  265]
train() client id: f_00001-12-1 loss: 0.360419  [   64/  265]
train() client id: f_00001-12-2 loss: 0.355751  [   96/  265]
train() client id: f_00001-12-3 loss: 0.477039  [  128/  265]
train() client id: f_00001-12-4 loss: 0.515047  [  160/  265]
train() client id: f_00001-12-5 loss: 0.539224  [  192/  265]
train() client id: f_00001-12-6 loss: 0.445655  [  224/  265]
train() client id: f_00001-12-7 loss: 0.391467  [  256/  265]
train() client id: f_00002-0-0 loss: 1.442302  [   32/  124]
train() client id: f_00002-0-1 loss: 1.309616  [   64/  124]
train() client id: f_00002-0-2 loss: 1.152514  [   96/  124]
train() client id: f_00002-1-0 loss: 1.056957  [   32/  124]
train() client id: f_00002-1-1 loss: 1.120042  [   64/  124]
train() client id: f_00002-1-2 loss: 1.200431  [   96/  124]
train() client id: f_00002-2-0 loss: 1.254383  [   32/  124]
train() client id: f_00002-2-1 loss: 1.168708  [   64/  124]
train() client id: f_00002-2-2 loss: 1.145679  [   96/  124]
train() client id: f_00002-3-0 loss: 1.207381  [   32/  124]
train() client id: f_00002-3-1 loss: 0.929163  [   64/  124]
train() client id: f_00002-3-2 loss: 1.174244  [   96/  124]
train() client id: f_00002-4-0 loss: 1.188753  [   32/  124]
train() client id: f_00002-4-1 loss: 1.100821  [   64/  124]
train() client id: f_00002-4-2 loss: 1.049843  [   96/  124]
train() client id: f_00002-5-0 loss: 1.140506  [   32/  124]
train() client id: f_00002-5-1 loss: 0.916039  [   64/  124]
train() client id: f_00002-5-2 loss: 1.168863  [   96/  124]
train() client id: f_00002-6-0 loss: 1.157789  [   32/  124]
train() client id: f_00002-6-1 loss: 0.970264  [   64/  124]
train() client id: f_00002-6-2 loss: 0.994598  [   96/  124]
train() client id: f_00002-7-0 loss: 1.021572  [   32/  124]
train() client id: f_00002-7-1 loss: 1.141334  [   64/  124]
train() client id: f_00002-7-2 loss: 0.997054  [   96/  124]
train() client id: f_00002-8-0 loss: 1.030382  [   32/  124]
train() client id: f_00002-8-1 loss: 1.044550  [   64/  124]
train() client id: f_00002-8-2 loss: 0.937424  [   96/  124]
train() client id: f_00002-9-0 loss: 1.069080  [   32/  124]
train() client id: f_00002-9-1 loss: 1.030284  [   64/  124]
train() client id: f_00002-9-2 loss: 1.000303  [   96/  124]
train() client id: f_00002-10-0 loss: 1.190019  [   32/  124]
train() client id: f_00002-10-1 loss: 0.886660  [   64/  124]
train() client id: f_00002-10-2 loss: 1.089910  [   96/  124]
train() client id: f_00002-11-0 loss: 1.202383  [   32/  124]
train() client id: f_00002-11-1 loss: 1.016180  [   64/  124]
train() client id: f_00002-11-2 loss: 0.911228  [   96/  124]
train() client id: f_00002-12-0 loss: 0.926975  [   32/  124]
train() client id: f_00002-12-1 loss: 1.104801  [   64/  124]
train() client id: f_00002-12-2 loss: 0.968309  [   96/  124]
train() client id: f_00003-0-0 loss: 0.770824  [   32/   43]
train() client id: f_00003-1-0 loss: 0.662883  [   32/   43]
train() client id: f_00003-2-0 loss: 0.634828  [   32/   43]
train() client id: f_00003-3-0 loss: 0.899090  [   32/   43]
train() client id: f_00003-4-0 loss: 0.732943  [   32/   43]
train() client id: f_00003-5-0 loss: 0.613468  [   32/   43]
train() client id: f_00003-6-0 loss: 0.655120  [   32/   43]
train() client id: f_00003-7-0 loss: 0.762397  [   32/   43]
train() client id: f_00003-8-0 loss: 0.716649  [   32/   43]
train() client id: f_00003-9-0 loss: 0.673279  [   32/   43]
train() client id: f_00003-10-0 loss: 0.767588  [   32/   43]
train() client id: f_00003-11-0 loss: 0.746284  [   32/   43]
train() client id: f_00003-12-0 loss: 0.700088  [   32/   43]
train() client id: f_00004-0-0 loss: 1.288708  [   32/  306]
train() client id: f_00004-0-1 loss: 1.013412  [   64/  306]
train() client id: f_00004-0-2 loss: 1.015146  [   96/  306]
train() client id: f_00004-0-3 loss: 1.026524  [  128/  306]
train() client id: f_00004-0-4 loss: 1.095971  [  160/  306]
train() client id: f_00004-0-5 loss: 1.023245  [  192/  306]
train() client id: f_00004-0-6 loss: 0.945018  [  224/  306]
train() client id: f_00004-0-7 loss: 1.118205  [  256/  306]
train() client id: f_00004-0-8 loss: 0.945176  [  288/  306]
train() client id: f_00004-1-0 loss: 1.007348  [   32/  306]
train() client id: f_00004-1-1 loss: 1.096314  [   64/  306]
train() client id: f_00004-1-2 loss: 0.949777  [   96/  306]
train() client id: f_00004-1-3 loss: 0.988618  [  128/  306]
train() client id: f_00004-1-4 loss: 1.037773  [  160/  306]
train() client id: f_00004-1-5 loss: 1.071077  [  192/  306]
train() client id: f_00004-1-6 loss: 0.957484  [  224/  306]
train() client id: f_00004-1-7 loss: 1.125254  [  256/  306]
train() client id: f_00004-1-8 loss: 1.071709  [  288/  306]
train() client id: f_00004-2-0 loss: 1.090318  [   32/  306]
train() client id: f_00004-2-1 loss: 1.094551  [   64/  306]
train() client id: f_00004-2-2 loss: 1.211340  [   96/  306]
train() client id: f_00004-2-3 loss: 1.048892  [  128/  306]
train() client id: f_00004-2-4 loss: 1.005561  [  160/  306]
train() client id: f_00004-2-5 loss: 0.950143  [  192/  306]
train() client id: f_00004-2-6 loss: 0.878088  [  224/  306]
train() client id: f_00004-2-7 loss: 1.088947  [  256/  306]
train() client id: f_00004-2-8 loss: 0.897505  [  288/  306]
train() client id: f_00004-3-0 loss: 0.980188  [   32/  306]
train() client id: f_00004-3-1 loss: 0.985206  [   64/  306]
train() client id: f_00004-3-2 loss: 1.022047  [   96/  306]
train() client id: f_00004-3-3 loss: 0.901329  [  128/  306]
train() client id: f_00004-3-4 loss: 1.025627  [  160/  306]
train() client id: f_00004-3-5 loss: 1.116123  [  192/  306]
train() client id: f_00004-3-6 loss: 1.001119  [  224/  306]
train() client id: f_00004-3-7 loss: 1.040279  [  256/  306]
train() client id: f_00004-3-8 loss: 1.136860  [  288/  306]
train() client id: f_00004-4-0 loss: 1.061237  [   32/  306]
train() client id: f_00004-4-1 loss: 0.914546  [   64/  306]
train() client id: f_00004-4-2 loss: 0.990072  [   96/  306]
train() client id: f_00004-4-3 loss: 0.994554  [  128/  306]
train() client id: f_00004-4-4 loss: 1.099303  [  160/  306]
train() client id: f_00004-4-5 loss: 1.094215  [  192/  306]
train() client id: f_00004-4-6 loss: 1.032314  [  224/  306]
train() client id: f_00004-4-7 loss: 1.019749  [  256/  306]
train() client id: f_00004-4-8 loss: 1.000436  [  288/  306]
train() client id: f_00004-5-0 loss: 1.020543  [   32/  306]
train() client id: f_00004-5-1 loss: 1.114585  [   64/  306]
train() client id: f_00004-5-2 loss: 1.102857  [   96/  306]
train() client id: f_00004-5-3 loss: 1.018821  [  128/  306]
train() client id: f_00004-5-4 loss: 0.994736  [  160/  306]
train() client id: f_00004-5-5 loss: 1.021943  [  192/  306]
train() client id: f_00004-5-6 loss: 0.941067  [  224/  306]
train() client id: f_00004-5-7 loss: 0.987720  [  256/  306]
train() client id: f_00004-5-8 loss: 0.950765  [  288/  306]
train() client id: f_00004-6-0 loss: 0.964319  [   32/  306]
train() client id: f_00004-6-1 loss: 1.101448  [   64/  306]
train() client id: f_00004-6-2 loss: 0.962528  [   96/  306]
train() client id: f_00004-6-3 loss: 1.007556  [  128/  306]
train() client id: f_00004-6-4 loss: 1.109375  [  160/  306]
train() client id: f_00004-6-5 loss: 1.042820  [  192/  306]
train() client id: f_00004-6-6 loss: 1.005974  [  224/  306]
train() client id: f_00004-6-7 loss: 0.978707  [  256/  306]
train() client id: f_00004-6-8 loss: 0.997812  [  288/  306]
train() client id: f_00004-7-0 loss: 1.058887  [   32/  306]
train() client id: f_00004-7-1 loss: 0.995811  [   64/  306]
train() client id: f_00004-7-2 loss: 0.997694  [   96/  306]
train() client id: f_00004-7-3 loss: 1.107346  [  128/  306]
train() client id: f_00004-7-4 loss: 1.068309  [  160/  306]
train() client id: f_00004-7-5 loss: 0.968577  [  192/  306]
train() client id: f_00004-7-6 loss: 0.992460  [  224/  306]
train() client id: f_00004-7-7 loss: 0.980250  [  256/  306]
train() client id: f_00004-7-8 loss: 0.968006  [  288/  306]
train() client id: f_00004-8-0 loss: 1.090626  [   32/  306]
train() client id: f_00004-8-1 loss: 0.957274  [   64/  306]
train() client id: f_00004-8-2 loss: 1.035003  [   96/  306]
train() client id: f_00004-8-3 loss: 1.018826  [  128/  306]
train() client id: f_00004-8-4 loss: 1.050862  [  160/  306]
train() client id: f_00004-8-5 loss: 1.043265  [  192/  306]
train() client id: f_00004-8-6 loss: 0.981202  [  224/  306]
train() client id: f_00004-8-7 loss: 0.925469  [  256/  306]
train() client id: f_00004-8-8 loss: 1.059063  [  288/  306]
train() client id: f_00004-9-0 loss: 1.105235  [   32/  306]
train() client id: f_00004-9-1 loss: 0.869360  [   64/  306]
train() client id: f_00004-9-2 loss: 0.895932  [   96/  306]
train() client id: f_00004-9-3 loss: 1.034647  [  128/  306]
train() client id: f_00004-9-4 loss: 0.886862  [  160/  306]
train() client id: f_00004-9-5 loss: 1.077336  [  192/  306]
train() client id: f_00004-9-6 loss: 1.020183  [  224/  306]
train() client id: f_00004-9-7 loss: 1.033460  [  256/  306]
train() client id: f_00004-9-8 loss: 1.075681  [  288/  306]
train() client id: f_00004-10-0 loss: 1.003575  [   32/  306]
train() client id: f_00004-10-1 loss: 1.013749  [   64/  306]
train() client id: f_00004-10-2 loss: 0.975769  [   96/  306]
train() client id: f_00004-10-3 loss: 1.012438  [  128/  306]
train() client id: f_00004-10-4 loss: 1.009921  [  160/  306]
train() client id: f_00004-10-5 loss: 1.076236  [  192/  306]
train() client id: f_00004-10-6 loss: 0.905522  [  224/  306]
train() client id: f_00004-10-7 loss: 0.972934  [  256/  306]
train() client id: f_00004-10-8 loss: 1.065243  [  288/  306]
train() client id: f_00004-11-0 loss: 1.063725  [   32/  306]
train() client id: f_00004-11-1 loss: 0.964861  [   64/  306]
train() client id: f_00004-11-2 loss: 0.909332  [   96/  306]
train() client id: f_00004-11-3 loss: 0.950684  [  128/  306]
train() client id: f_00004-11-4 loss: 1.094257  [  160/  306]
train() client id: f_00004-11-5 loss: 1.027861  [  192/  306]
train() client id: f_00004-11-6 loss: 1.031757  [  224/  306]
train() client id: f_00004-11-7 loss: 1.055866  [  256/  306]
train() client id: f_00004-11-8 loss: 0.900304  [  288/  306]
train() client id: f_00004-12-0 loss: 0.965929  [   32/  306]
train() client id: f_00004-12-1 loss: 0.884248  [   64/  306]
train() client id: f_00004-12-2 loss: 1.010175  [   96/  306]
train() client id: f_00004-12-3 loss: 0.995628  [  128/  306]
train() client id: f_00004-12-4 loss: 1.022830  [  160/  306]
train() client id: f_00004-12-5 loss: 0.995887  [  192/  306]
train() client id: f_00004-12-6 loss: 1.022640  [  224/  306]
train() client id: f_00004-12-7 loss: 1.032124  [  256/  306]
train() client id: f_00004-12-8 loss: 1.033519  [  288/  306]
train() client id: f_00005-0-0 loss: 0.622370  [   32/  146]
train() client id: f_00005-0-1 loss: 0.528870  [   64/  146]
train() client id: f_00005-0-2 loss: 0.547729  [   96/  146]
train() client id: f_00005-0-3 loss: 0.992574  [  128/  146]
train() client id: f_00005-1-0 loss: 0.691540  [   32/  146]
train() client id: f_00005-1-1 loss: 0.623426  [   64/  146]
train() client id: f_00005-1-2 loss: 0.782239  [   96/  146]
train() client id: f_00005-1-3 loss: 0.712324  [  128/  146]
train() client id: f_00005-2-0 loss: 0.751738  [   32/  146]
train() client id: f_00005-2-1 loss: 0.835026  [   64/  146]
train() client id: f_00005-2-2 loss: 0.718226  [   96/  146]
train() client id: f_00005-2-3 loss: 0.593811  [  128/  146]
train() client id: f_00005-3-0 loss: 0.587961  [   32/  146]
train() client id: f_00005-3-1 loss: 0.852241  [   64/  146]
train() client id: f_00005-3-2 loss: 0.620178  [   96/  146]
train() client id: f_00005-3-3 loss: 0.570870  [  128/  146]
train() client id: f_00005-4-0 loss: 0.707333  [   32/  146]
train() client id: f_00005-4-1 loss: 0.557007  [   64/  146]
train() client id: f_00005-4-2 loss: 0.728041  [   96/  146]
train() client id: f_00005-4-3 loss: 0.845973  [  128/  146]
train() client id: f_00005-5-0 loss: 0.384849  [   32/  146]
train() client id: f_00005-5-1 loss: 0.558448  [   64/  146]
train() client id: f_00005-5-2 loss: 0.654832  [   96/  146]
train() client id: f_00005-5-3 loss: 0.822504  [  128/  146]
train() client id: f_00005-6-0 loss: 0.776650  [   32/  146]
train() client id: f_00005-6-1 loss: 0.629410  [   64/  146]
train() client id: f_00005-6-2 loss: 0.744684  [   96/  146]
train() client id: f_00005-6-3 loss: 0.610983  [  128/  146]
train() client id: f_00005-7-0 loss: 0.552907  [   32/  146]
train() client id: f_00005-7-1 loss: 0.794643  [   64/  146]
train() client id: f_00005-7-2 loss: 0.648679  [   96/  146]
train() client id: f_00005-7-3 loss: 0.739978  [  128/  146]
train() client id: f_00005-8-0 loss: 0.402702  [   32/  146]
train() client id: f_00005-8-1 loss: 0.893216  [   64/  146]
train() client id: f_00005-8-2 loss: 0.874964  [   96/  146]
train() client id: f_00005-8-3 loss: 0.692242  [  128/  146]
train() client id: f_00005-9-0 loss: 0.839396  [   32/  146]
train() client id: f_00005-9-1 loss: 0.671571  [   64/  146]
train() client id: f_00005-9-2 loss: 0.723735  [   96/  146]
train() client id: f_00005-9-3 loss: 0.594048  [  128/  146]
train() client id: f_00005-10-0 loss: 0.860737  [   32/  146]
train() client id: f_00005-10-1 loss: 0.724136  [   64/  146]
train() client id: f_00005-10-2 loss: 0.514931  [   96/  146]
train() client id: f_00005-10-3 loss: 0.755890  [  128/  146]
train() client id: f_00005-11-0 loss: 0.806916  [   32/  146]
train() client id: f_00005-11-1 loss: 0.421906  [   64/  146]
train() client id: f_00005-11-2 loss: 0.940983  [   96/  146]
train() client id: f_00005-11-3 loss: 0.436414  [  128/  146]
train() client id: f_00005-12-0 loss: 0.715259  [   32/  146]
train() client id: f_00005-12-1 loss: 0.773332  [   64/  146]
train() client id: f_00005-12-2 loss: 0.609023  [   96/  146]
train() client id: f_00005-12-3 loss: 0.721742  [  128/  146]
train() client id: f_00006-0-0 loss: 0.575641  [   32/   54]
train() client id: f_00006-1-0 loss: 0.627658  [   32/   54]
train() client id: f_00006-2-0 loss: 0.604299  [   32/   54]
train() client id: f_00006-3-0 loss: 0.573630  [   32/   54]
train() client id: f_00006-4-0 loss: 0.609423  [   32/   54]
train() client id: f_00006-5-0 loss: 0.540112  [   32/   54]
train() client id: f_00006-6-0 loss: 0.604362  [   32/   54]
train() client id: f_00006-7-0 loss: 0.571085  [   32/   54]
train() client id: f_00006-8-0 loss: 0.631950  [   32/   54]
train() client id: f_00006-9-0 loss: 0.573934  [   32/   54]
train() client id: f_00006-10-0 loss: 0.553673  [   32/   54]
train() client id: f_00006-11-0 loss: 0.585312  [   32/   54]
train() client id: f_00006-12-0 loss: 0.553260  [   32/   54]
train() client id: f_00007-0-0 loss: 0.472626  [   32/  179]
train() client id: f_00007-0-1 loss: 0.672887  [   64/  179]
train() client id: f_00007-0-2 loss: 0.619246  [   96/  179]
train() client id: f_00007-0-3 loss: 0.515913  [  128/  179]
train() client id: f_00007-0-4 loss: 0.450035  [  160/  179]
train() client id: f_00007-1-0 loss: 0.584727  [   32/  179]
train() client id: f_00007-1-1 loss: 0.544556  [   64/  179]
train() client id: f_00007-1-2 loss: 0.628088  [   96/  179]
train() client id: f_00007-1-3 loss: 0.441868  [  128/  179]
train() client id: f_00007-1-4 loss: 0.450195  [  160/  179]
train() client id: f_00007-2-0 loss: 0.519469  [   32/  179]
train() client id: f_00007-2-1 loss: 0.543137  [   64/  179]
train() client id: f_00007-2-2 loss: 0.533951  [   96/  179]
train() client id: f_00007-2-3 loss: 0.471563  [  128/  179]
train() client id: f_00007-2-4 loss: 0.509177  [  160/  179]
train() client id: f_00007-3-0 loss: 0.628803  [   32/  179]
train() client id: f_00007-3-1 loss: 0.342417  [   64/  179]
train() client id: f_00007-3-2 loss: 0.768347  [   96/  179]
train() client id: f_00007-3-3 loss: 0.432367  [  128/  179]
train() client id: f_00007-3-4 loss: 0.375042  [  160/  179]
train() client id: f_00007-4-0 loss: 0.439923  [   32/  179]
train() client id: f_00007-4-1 loss: 0.504309  [   64/  179]
train() client id: f_00007-4-2 loss: 0.360359  [   96/  179]
train() client id: f_00007-4-3 loss: 0.574974  [  128/  179]
train() client id: f_00007-4-4 loss: 0.556445  [  160/  179]
train() client id: f_00007-5-0 loss: 0.654638  [   32/  179]
train() client id: f_00007-5-1 loss: 0.389580  [   64/  179]
train() client id: f_00007-5-2 loss: 0.443042  [   96/  179]
train() client id: f_00007-5-3 loss: 0.413764  [  128/  179]
train() client id: f_00007-5-4 loss: 0.590073  [  160/  179]
train() client id: f_00007-6-0 loss: 0.494739  [   32/  179]
train() client id: f_00007-6-1 loss: 0.426663  [   64/  179]
train() client id: f_00007-6-2 loss: 0.468128  [   96/  179]
train() client id: f_00007-6-3 loss: 0.600013  [  128/  179]
train() client id: f_00007-6-4 loss: 0.432143  [  160/  179]
train() client id: f_00007-7-0 loss: 0.387987  [   32/  179]
train() client id: f_00007-7-1 loss: 0.512164  [   64/  179]
train() client id: f_00007-7-2 loss: 0.331103  [   96/  179]
train() client id: f_00007-7-3 loss: 0.402775  [  128/  179]
train() client id: f_00007-7-4 loss: 0.449347  [  160/  179]
train() client id: f_00007-8-0 loss: 0.490501  [   32/  179]
train() client id: f_00007-8-1 loss: 0.348475  [   64/  179]
train() client id: f_00007-8-2 loss: 0.719803  [   96/  179]
train() client id: f_00007-8-3 loss: 0.313498  [  128/  179]
train() client id: f_00007-8-4 loss: 0.428877  [  160/  179]
train() client id: f_00007-9-0 loss: 0.444933  [   32/  179]
train() client id: f_00007-9-1 loss: 0.424122  [   64/  179]
train() client id: f_00007-9-2 loss: 0.498095  [   96/  179]
train() client id: f_00007-9-3 loss: 0.588620  [  128/  179]
train() client id: f_00007-9-4 loss: 0.428369  [  160/  179]
train() client id: f_00007-10-0 loss: 0.328838  [   32/  179]
train() client id: f_00007-10-1 loss: 0.586806  [   64/  179]
train() client id: f_00007-10-2 loss: 0.452946  [   96/  179]
train() client id: f_00007-10-3 loss: 0.674381  [  128/  179]
train() client id: f_00007-10-4 loss: 0.425480  [  160/  179]
train() client id: f_00007-11-0 loss: 0.559837  [   32/  179]
train() client id: f_00007-11-1 loss: 0.337599  [   64/  179]
train() client id: f_00007-11-2 loss: 0.374067  [   96/  179]
train() client id: f_00007-11-3 loss: 0.699293  [  128/  179]
train() client id: f_00007-11-4 loss: 0.415929  [  160/  179]
train() client id: f_00007-12-0 loss: 0.513893  [   32/  179]
train() client id: f_00007-12-1 loss: 0.324006  [   64/  179]
train() client id: f_00007-12-2 loss: 0.511050  [   96/  179]
train() client id: f_00007-12-3 loss: 0.416137  [  128/  179]
train() client id: f_00007-12-4 loss: 0.570299  [  160/  179]
train() client id: f_00008-0-0 loss: 0.790258  [   32/  130]
train() client id: f_00008-0-1 loss: 0.742505  [   64/  130]
train() client id: f_00008-0-2 loss: 0.658889  [   96/  130]
train() client id: f_00008-0-3 loss: 0.761294  [  128/  130]
train() client id: f_00008-1-0 loss: 0.701691  [   32/  130]
train() client id: f_00008-1-1 loss: 0.743437  [   64/  130]
train() client id: f_00008-1-2 loss: 0.646209  [   96/  130]
train() client id: f_00008-1-3 loss: 0.827986  [  128/  130]
train() client id: f_00008-2-0 loss: 0.828860  [   32/  130]
train() client id: f_00008-2-1 loss: 0.697015  [   64/  130]
train() client id: f_00008-2-2 loss: 0.769203  [   96/  130]
train() client id: f_00008-2-3 loss: 0.659682  [  128/  130]
train() client id: f_00008-3-0 loss: 0.606040  [   32/  130]
train() client id: f_00008-3-1 loss: 0.862526  [   64/  130]
train() client id: f_00008-3-2 loss: 0.681375  [   96/  130]
train() client id: f_00008-3-3 loss: 0.802025  [  128/  130]
train() client id: f_00008-4-0 loss: 0.751931  [   32/  130]
train() client id: f_00008-4-1 loss: 0.696452  [   64/  130]
train() client id: f_00008-4-2 loss: 0.718767  [   96/  130]
train() client id: f_00008-4-3 loss: 0.766808  [  128/  130]
train() client id: f_00008-5-0 loss: 0.628829  [   32/  130]
train() client id: f_00008-5-1 loss: 0.712446  [   64/  130]
train() client id: f_00008-5-2 loss: 0.740270  [   96/  130]
train() client id: f_00008-5-3 loss: 0.850817  [  128/  130]
train() client id: f_00008-6-0 loss: 0.734664  [   32/  130]
train() client id: f_00008-6-1 loss: 0.708877  [   64/  130]
train() client id: f_00008-6-2 loss: 0.711513  [   96/  130]
train() client id: f_00008-6-3 loss: 0.791686  [  128/  130]
train() client id: f_00008-7-0 loss: 0.859352  [   32/  130]
train() client id: f_00008-7-1 loss: 0.746643  [   64/  130]
train() client id: f_00008-7-2 loss: 0.608696  [   96/  130]
train() client id: f_00008-7-3 loss: 0.693574  [  128/  130]
train() client id: f_00008-8-0 loss: 0.754433  [   32/  130]
train() client id: f_00008-8-1 loss: 0.705692  [   64/  130]
train() client id: f_00008-8-2 loss: 0.782194  [   96/  130]
train() client id: f_00008-8-3 loss: 0.701264  [  128/  130]
train() client id: f_00008-9-0 loss: 0.725116  [   32/  130]
train() client id: f_00008-9-1 loss: 0.729886  [   64/  130]
train() client id: f_00008-9-2 loss: 0.824230  [   96/  130]
train() client id: f_00008-9-3 loss: 0.652477  [  128/  130]
train() client id: f_00008-10-0 loss: 0.696942  [   32/  130]
train() client id: f_00008-10-1 loss: 0.778328  [   64/  130]
train() client id: f_00008-10-2 loss: 0.762507  [   96/  130]
train() client id: f_00008-10-3 loss: 0.697635  [  128/  130]
train() client id: f_00008-11-0 loss: 0.753354  [   32/  130]
train() client id: f_00008-11-1 loss: 0.684896  [   64/  130]
train() client id: f_00008-11-2 loss: 0.691689  [   96/  130]
train() client id: f_00008-11-3 loss: 0.795916  [  128/  130]
train() client id: f_00008-12-0 loss: 0.782198  [   32/  130]
train() client id: f_00008-12-1 loss: 0.638595  [   64/  130]
train() client id: f_00008-12-2 loss: 0.737504  [   96/  130]
train() client id: f_00008-12-3 loss: 0.773509  [  128/  130]
train() client id: f_00009-0-0 loss: 1.140430  [   32/  118]
train() client id: f_00009-0-1 loss: 0.966964  [   64/  118]
train() client id: f_00009-0-2 loss: 0.882657  [   96/  118]
train() client id: f_00009-1-0 loss: 0.805910  [   32/  118]
train() client id: f_00009-1-1 loss: 0.804005  [   64/  118]
train() client id: f_00009-1-2 loss: 0.971692  [   96/  118]
train() client id: f_00009-2-0 loss: 0.933784  [   32/  118]
train() client id: f_00009-2-1 loss: 0.762877  [   64/  118]
train() client id: f_00009-2-2 loss: 0.825141  [   96/  118]
train() client id: f_00009-3-0 loss: 0.867806  [   32/  118]
train() client id: f_00009-3-1 loss: 0.831218  [   64/  118]
train() client id: f_00009-3-2 loss: 0.842968  [   96/  118]
train() client id: f_00009-4-0 loss: 0.824141  [   32/  118]
train() client id: f_00009-4-1 loss: 0.847164  [   64/  118]
train() client id: f_00009-4-2 loss: 0.752710  [   96/  118]
train() client id: f_00009-5-0 loss: 0.924286  [   32/  118]
train() client id: f_00009-5-1 loss: 0.714062  [   64/  118]
train() client id: f_00009-5-2 loss: 0.660559  [   96/  118]
train() client id: f_00009-6-0 loss: 0.823528  [   32/  118]
train() client id: f_00009-6-1 loss: 0.815813  [   64/  118]
train() client id: f_00009-6-2 loss: 0.699079  [   96/  118]
train() client id: f_00009-7-0 loss: 0.693414  [   32/  118]
train() client id: f_00009-7-1 loss: 0.883726  [   64/  118]
train() client id: f_00009-7-2 loss: 0.813719  [   96/  118]
train() client id: f_00009-8-0 loss: 0.799184  [   32/  118]
train() client id: f_00009-8-1 loss: 0.826943  [   64/  118]
train() client id: f_00009-8-2 loss: 0.640408  [   96/  118]
train() client id: f_00009-9-0 loss: 0.732056  [   32/  118]
train() client id: f_00009-9-1 loss: 0.706923  [   64/  118]
train() client id: f_00009-9-2 loss: 0.839460  [   96/  118]
train() client id: f_00009-10-0 loss: 0.707736  [   32/  118]
train() client id: f_00009-10-1 loss: 0.798960  [   64/  118]
train() client id: f_00009-10-2 loss: 0.676684  [   96/  118]
train() client id: f_00009-11-0 loss: 0.712291  [   32/  118]
train() client id: f_00009-11-1 loss: 0.738621  [   64/  118]
train() client id: f_00009-11-2 loss: 0.781248  [   96/  118]
train() client id: f_00009-12-0 loss: 0.844328  [   32/  118]
train() client id: f_00009-12-1 loss: 0.704346  [   64/  118]
train() client id: f_00009-12-2 loss: 0.681263  [   96/  118]
At round 39 accuracy: 0.6525198938992043
At round 39 training accuracy: 0.5828303152246814
At round 39 training loss: 0.8330745824438406
update_location
xs = [  -3.9056584     4.20031788  215.00902392   18.81129433    0.97929623
    3.95640986 -177.44319194 -156.32485185  199.66397685 -142.06087855]
ys = [ 207.5879595   190.55583871    1.32061395 -177.45517586  169.35018685
  152.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [230.45176306 215.24212027 237.12997361 204.55855942 196.67344713
 182.66860115 203.69824964 185.57514802 223.99636967 173.77371801]
dists_bs = [174.17017204 179.87422841 426.93274397 402.22813029 176.06996646
 180.32639446 177.6417863  175.17595617 406.39588262 174.14212286]
uav_gains = [1.06238871e-11 1.35916922e-11 9.45055194e-12 1.59558146e-11
 1.78934679e-11 2.18960415e-11 1.61583748e-11 2.09940106e-11
 1.18323052e-11 2.49490095e-11]
bs_gains = [5.86904580e-11 5.36266962e-11 4.76746578e-12 5.63340860e-12
 5.69344720e-11 5.32510343e-11 5.55351203e-11 5.77517960e-11
 5.47313357e-12 5.87169311e-11]
Round 40
-------------------------------
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.71197888 11.79408709  5.62914561  2.03417315 13.60152567  6.544648
  2.51897531  8.02443866  5.92443305  5.30743185]
obj_prev = 67.09083726402451
eta_min = 7.023642441479729e-17	eta_max = 0.9321930357555286
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 15.5538538243213	eta = 0.909090909090909
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 29.16218476187658	eta = 0.4848699515649548
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 22.40002232600407	eta = 0.6312434383873119
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.175688042561685	eta = 0.667740622387296
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.10970401340027	eta = 0.6698278243997874
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.109495669486368	eta = 0.6698344353843775
eta = 0.6698344353843775
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.03306684 0.06954534 0.03254197 0.01128472 0.08030519 0.03831555
 0.0141715  0.04697591 0.03411658 0.03096736]
ene_total = [1.91082934 3.36153203 1.90398611 0.90652186 3.82972324 1.99226433
 1.03177315 2.44413259 2.06480829 1.66392474]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 0 obj = 3.9578926480979075
eta = 0.6698344353843775
freqs = [30996407.38459397 61282205.26212957 30716806.61521623 10361284.76037134
 70657812.12672363 33769043.53321395 13004437.39099001 42636067.25492053
 33527323.09434476 27226537.57500104]
eta_min = 0.6698344353843826	eta_max = 0.6698344353843708
af = 0.008298545367209708	bf = 1.3034344286740114	zeta = 0.00912839990393068	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [1.88907249e-06 1.55299480e-05 1.82569896e-06 7.20362393e-08
 2.38395007e-05 2.59803940e-06 1.42505706e-07 5.07764887e-06
 2.28032353e-06 1.36496714e-06]
ene_total = [1.67292505 1.13318653 1.73166738 1.49481175 1.1209777  1.13273715
 1.48988129 1.3947409  2.06499979 1.11055146]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 1 obj = 3.9578926480978276
eta = 0.6698344353843708
freqs = [30996407.384594   61282205.26212971 30716806.61521625 10361284.76037136
 70657812.12672378 33769043.53321403 13004437.39099002 42636067.2549206
 33527323.09434475 27226537.5750011 ]
Done!
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.86876585e-06 6.46886370e-05 7.60478900e-06 3.00060640e-07
 9.93013504e-05 1.08219054e-05 5.93595025e-07 2.11505013e-05
 9.49848778e-06 5.68565095e-06]
ene_total = [0.01050847 0.00716321 0.01087705 0.00938453 0.00711287 0.00711946
 0.0093538  0.00877214 0.01297112 0.00697627]
At round 40 energy consumption: 0.09023892340678759
At round 40 eta: 0.6698344353843708
At round 40 a_n: 14.480768977481684
At round 40 local rounds: 13.121762881988987
At round 40 global rounds: 43.859113515789716
gradient difference: 0.4932621121406555
train() client id: f_00000-0-0 loss: 0.879713  [   32/  126]
train() client id: f_00000-0-1 loss: 1.090100  [   64/  126]
train() client id: f_00000-0-2 loss: 0.883301  [   96/  126]
train() client id: f_00000-1-0 loss: 1.104563  [   32/  126]
train() client id: f_00000-1-1 loss: 0.689092  [   64/  126]
train() client id: f_00000-1-2 loss: 0.817933  [   96/  126]
train() client id: f_00000-2-0 loss: 0.769612  [   32/  126]
train() client id: f_00000-2-1 loss: 0.769919  [   64/  126]
train() client id: f_00000-2-2 loss: 0.889908  [   96/  126]
train() client id: f_00000-3-0 loss: 0.828379  [   32/  126]
train() client id: f_00000-3-1 loss: 0.861760  [   64/  126]
train() client id: f_00000-3-2 loss: 0.871562  [   96/  126]
train() client id: f_00000-4-0 loss: 0.826888  [   32/  126]
train() client id: f_00000-4-1 loss: 0.881808  [   64/  126]
train() client id: f_00000-4-2 loss: 0.720531  [   96/  126]
train() client id: f_00000-5-0 loss: 0.790604  [   32/  126]
train() client id: f_00000-5-1 loss: 0.810471  [   64/  126]
train() client id: f_00000-5-2 loss: 0.743902  [   96/  126]
train() client id: f_00000-6-0 loss: 0.803609  [   32/  126]
train() client id: f_00000-6-1 loss: 0.907599  [   64/  126]
train() client id: f_00000-6-2 loss: 0.795387  [   96/  126]
train() client id: f_00000-7-0 loss: 0.803775  [   32/  126]
train() client id: f_00000-7-1 loss: 0.773230  [   64/  126]
train() client id: f_00000-7-2 loss: 0.836649  [   96/  126]
train() client id: f_00000-8-0 loss: 0.834147  [   32/  126]
train() client id: f_00000-8-1 loss: 0.838421  [   64/  126]
train() client id: f_00000-8-2 loss: 0.792976  [   96/  126]
train() client id: f_00000-9-0 loss: 0.710690  [   32/  126]
train() client id: f_00000-9-1 loss: 0.831280  [   64/  126]
train() client id: f_00000-9-2 loss: 0.952661  [   96/  126]
train() client id: f_00000-10-0 loss: 0.927239  [   32/  126]
train() client id: f_00000-10-1 loss: 0.797765  [   64/  126]
train() client id: f_00000-10-2 loss: 0.804290  [   96/  126]
train() client id: f_00000-11-0 loss: 0.919732  [   32/  126]
train() client id: f_00000-11-1 loss: 0.864001  [   64/  126]
train() client id: f_00000-11-2 loss: 0.850375  [   96/  126]
train() client id: f_00000-12-0 loss: 1.140059  [   32/  126]
train() client id: f_00000-12-1 loss: 0.803775  [   64/  126]
train() client id: f_00000-12-2 loss: 0.727805  [   96/  126]
train() client id: f_00001-0-0 loss: 0.523073  [   32/  265]
train() client id: f_00001-0-1 loss: 0.454252  [   64/  265]
train() client id: f_00001-0-2 loss: 0.438656  [   96/  265]
train() client id: f_00001-0-3 loss: 0.398495  [  128/  265]
train() client id: f_00001-0-4 loss: 0.527984  [  160/  265]
train() client id: f_00001-0-5 loss: 0.421441  [  192/  265]
train() client id: f_00001-0-6 loss: 0.477160  [  224/  265]
train() client id: f_00001-0-7 loss: 0.482179  [  256/  265]
train() client id: f_00001-1-0 loss: 0.553857  [   32/  265]
train() client id: f_00001-1-1 loss: 0.425158  [   64/  265]
train() client id: f_00001-1-2 loss: 0.588552  [   96/  265]
train() client id: f_00001-1-3 loss: 0.481715  [  128/  265]
train() client id: f_00001-1-4 loss: 0.353616  [  160/  265]
train() client id: f_00001-1-5 loss: 0.435239  [  192/  265]
train() client id: f_00001-1-6 loss: 0.402479  [  224/  265]
train() client id: f_00001-1-7 loss: 0.362953  [  256/  265]
train() client id: f_00001-2-0 loss: 0.446576  [   32/  265]
train() client id: f_00001-2-1 loss: 0.700675  [   64/  265]
train() client id: f_00001-2-2 loss: 0.378706  [   96/  265]
train() client id: f_00001-2-3 loss: 0.400808  [  128/  265]
train() client id: f_00001-2-4 loss: 0.421683  [  160/  265]
train() client id: f_00001-2-5 loss: 0.388660  [  192/  265]
train() client id: f_00001-2-6 loss: 0.429863  [  224/  265]
train() client id: f_00001-2-7 loss: 0.433808  [  256/  265]
train() client id: f_00001-3-0 loss: 0.568857  [   32/  265]
train() client id: f_00001-3-1 loss: 0.412131  [   64/  265]
train() client id: f_00001-3-2 loss: 0.334533  [   96/  265]
train() client id: f_00001-3-3 loss: 0.477446  [  128/  265]
train() client id: f_00001-3-4 loss: 0.580953  [  160/  265]
train() client id: f_00001-3-5 loss: 0.428417  [  192/  265]
train() client id: f_00001-3-6 loss: 0.365117  [  224/  265]
train() client id: f_00001-3-7 loss: 0.403036  [  256/  265]
train() client id: f_00001-4-0 loss: 0.377560  [   32/  265]
train() client id: f_00001-4-1 loss: 0.472785  [   64/  265]
train() client id: f_00001-4-2 loss: 0.407616  [   96/  265]
train() client id: f_00001-4-3 loss: 0.356607  [  128/  265]
train() client id: f_00001-4-4 loss: 0.409015  [  160/  265]
train() client id: f_00001-4-5 loss: 0.590260  [  192/  265]
train() client id: f_00001-4-6 loss: 0.443984  [  224/  265]
train() client id: f_00001-4-7 loss: 0.466169  [  256/  265]
train() client id: f_00001-5-0 loss: 0.615510  [   32/  265]
train() client id: f_00001-5-1 loss: 0.338508  [   64/  265]
train() client id: f_00001-5-2 loss: 0.444485  [   96/  265]
train() client id: f_00001-5-3 loss: 0.381145  [  128/  265]
train() client id: f_00001-5-4 loss: 0.411442  [  160/  265]
train() client id: f_00001-5-5 loss: 0.422228  [  192/  265]
train() client id: f_00001-5-6 loss: 0.392308  [  224/  265]
train() client id: f_00001-5-7 loss: 0.480021  [  256/  265]
train() client id: f_00001-6-0 loss: 0.347497  [   32/  265]
train() client id: f_00001-6-1 loss: 0.421835  [   64/  265]
train() client id: f_00001-6-2 loss: 0.436348  [   96/  265]
train() client id: f_00001-6-3 loss: 0.382803  [  128/  265]
train() client id: f_00001-6-4 loss: 0.421765  [  160/  265]
train() client id: f_00001-6-5 loss: 0.457710  [  192/  265]
train() client id: f_00001-6-6 loss: 0.500473  [  224/  265]
train() client id: f_00001-6-7 loss: 0.485566  [  256/  265]
train() client id: f_00001-7-0 loss: 0.529806  [   32/  265]
train() client id: f_00001-7-1 loss: 0.452500  [   64/  265]
train() client id: f_00001-7-2 loss: 0.580437  [   96/  265]
train() client id: f_00001-7-3 loss: 0.401352  [  128/  265]
train() client id: f_00001-7-4 loss: 0.335355  [  160/  265]
train() client id: f_00001-7-5 loss: 0.425141  [  192/  265]
train() client id: f_00001-7-6 loss: 0.316460  [  224/  265]
train() client id: f_00001-7-7 loss: 0.402685  [  256/  265]
train() client id: f_00001-8-0 loss: 0.332387  [   32/  265]
train() client id: f_00001-8-1 loss: 0.553662  [   64/  265]
train() client id: f_00001-8-2 loss: 0.434766  [   96/  265]
train() client id: f_00001-8-3 loss: 0.551102  [  128/  265]
train() client id: f_00001-8-4 loss: 0.343915  [  160/  265]
train() client id: f_00001-8-5 loss: 0.376219  [  192/  265]
train() client id: f_00001-8-6 loss: 0.383290  [  224/  265]
train() client id: f_00001-8-7 loss: 0.451959  [  256/  265]
train() client id: f_00001-9-0 loss: 0.546540  [   32/  265]
train() client id: f_00001-9-1 loss: 0.409301  [   64/  265]
train() client id: f_00001-9-2 loss: 0.435492  [   96/  265]
train() client id: f_00001-9-3 loss: 0.489963  [  128/  265]
train() client id: f_00001-9-4 loss: 0.463877  [  160/  265]
train() client id: f_00001-9-5 loss: 0.347709  [  192/  265]
train() client id: f_00001-9-6 loss: 0.351204  [  224/  265]
train() client id: f_00001-9-7 loss: 0.380007  [  256/  265]
train() client id: f_00001-10-0 loss: 0.342040  [   32/  265]
train() client id: f_00001-10-1 loss: 0.387110  [   64/  265]
train() client id: f_00001-10-2 loss: 0.427334  [   96/  265]
train() client id: f_00001-10-3 loss: 0.441698  [  128/  265]
train() client id: f_00001-10-4 loss: 0.401107  [  160/  265]
train() client id: f_00001-10-5 loss: 0.464344  [  192/  265]
train() client id: f_00001-10-6 loss: 0.414178  [  224/  265]
train() client id: f_00001-10-7 loss: 0.459538  [  256/  265]
train() client id: f_00001-11-0 loss: 0.461529  [   32/  265]
train() client id: f_00001-11-1 loss: 0.427789  [   64/  265]
train() client id: f_00001-11-2 loss: 0.407108  [   96/  265]
train() client id: f_00001-11-3 loss: 0.616432  [  128/  265]
train() client id: f_00001-11-4 loss: 0.412584  [  160/  265]
train() client id: f_00001-11-5 loss: 0.395505  [  192/  265]
train() client id: f_00001-11-6 loss: 0.362925  [  224/  265]
train() client id: f_00001-11-7 loss: 0.327055  [  256/  265]
train() client id: f_00001-12-0 loss: 0.503270  [   32/  265]
train() client id: f_00001-12-1 loss: 0.475977  [   64/  265]
train() client id: f_00001-12-2 loss: 0.478202  [   96/  265]
train() client id: f_00001-12-3 loss: 0.410732  [  128/  265]
train() client id: f_00001-12-4 loss: 0.326830  [  160/  265]
train() client id: f_00001-12-5 loss: 0.410093  [  192/  265]
train() client id: f_00001-12-6 loss: 0.411673  [  224/  265]
train() client id: f_00001-12-7 loss: 0.384620  [  256/  265]
train() client id: f_00002-0-0 loss: 1.174690  [   32/  124]
train() client id: f_00002-0-1 loss: 1.371316  [   64/  124]
train() client id: f_00002-0-2 loss: 1.288768  [   96/  124]
train() client id: f_00002-1-0 loss: 1.186709  [   32/  124]
train() client id: f_00002-1-1 loss: 1.328843  [   64/  124]
train() client id: f_00002-1-2 loss: 1.295996  [   96/  124]
train() client id: f_00002-2-0 loss: 1.127164  [   32/  124]
train() client id: f_00002-2-1 loss: 1.120674  [   64/  124]
train() client id: f_00002-2-2 loss: 1.289750  [   96/  124]
train() client id: f_00002-3-0 loss: 1.214377  [   32/  124]
train() client id: f_00002-3-1 loss: 1.135607  [   64/  124]
train() client id: f_00002-3-2 loss: 1.181161  [   96/  124]
train() client id: f_00002-4-0 loss: 1.020895  [   32/  124]
train() client id: f_00002-4-1 loss: 1.190913  [   64/  124]
train() client id: f_00002-4-2 loss: 1.233217  [   96/  124]
train() client id: f_00002-5-0 loss: 1.058095  [   32/  124]
train() client id: f_00002-5-1 loss: 1.057512  [   64/  124]
train() client id: f_00002-5-2 loss: 1.206124  [   96/  124]
train() client id: f_00002-6-0 loss: 1.034800  [   32/  124]
train() client id: f_00002-6-1 loss: 0.997640  [   64/  124]
train() client id: f_00002-6-2 loss: 1.149660  [   96/  124]
train() client id: f_00002-7-0 loss: 1.066286  [   32/  124]
train() client id: f_00002-7-1 loss: 1.008420  [   64/  124]
train() client id: f_00002-7-2 loss: 1.001434  [   96/  124]
train() client id: f_00002-8-0 loss: 1.081921  [   32/  124]
train() client id: f_00002-8-1 loss: 0.983030  [   64/  124]
train() client id: f_00002-8-2 loss: 1.123196  [   96/  124]
train() client id: f_00002-9-0 loss: 0.950409  [   32/  124]
train() client id: f_00002-9-1 loss: 1.014025  [   64/  124]
train() client id: f_00002-9-2 loss: 1.132403  [   96/  124]
train() client id: f_00002-10-0 loss: 1.028783  [   32/  124]
train() client id: f_00002-10-1 loss: 0.841528  [   64/  124]
train() client id: f_00002-10-2 loss: 1.128987  [   96/  124]
train() client id: f_00002-11-0 loss: 1.031863  [   32/  124]
train() client id: f_00002-11-1 loss: 0.909444  [   64/  124]
train() client id: f_00002-11-2 loss: 1.190032  [   96/  124]
train() client id: f_00002-12-0 loss: 1.011569  [   32/  124]
train() client id: f_00002-12-1 loss: 1.085924  [   64/  124]
train() client id: f_00002-12-2 loss: 1.105103  [   96/  124]
train() client id: f_00003-0-0 loss: 0.835468  [   32/   43]
train() client id: f_00003-1-0 loss: 0.773260  [   32/   43]
train() client id: f_00003-2-0 loss: 0.665373  [   32/   43]
train() client id: f_00003-3-0 loss: 0.737961  [   32/   43]
train() client id: f_00003-4-0 loss: 0.675050  [   32/   43]
train() client id: f_00003-5-0 loss: 0.818627  [   32/   43]
train() client id: f_00003-6-0 loss: 0.688860  [   32/   43]
train() client id: f_00003-7-0 loss: 0.712345  [   32/   43]
train() client id: f_00003-8-0 loss: 0.667936  [   32/   43]
train() client id: f_00003-9-0 loss: 0.691059  [   32/   43]
train() client id: f_00003-10-0 loss: 0.761140  [   32/   43]
train() client id: f_00003-11-0 loss: 0.851106  [   32/   43]
train() client id: f_00003-12-0 loss: 0.659864  [   32/   43]
train() client id: f_00004-0-0 loss: 0.744366  [   32/  306]
train() client id: f_00004-0-1 loss: 0.797651  [   64/  306]
train() client id: f_00004-0-2 loss: 0.985896  [   96/  306]
train() client id: f_00004-0-3 loss: 0.766247  [  128/  306]
train() client id: f_00004-0-4 loss: 1.029585  [  160/  306]
train() client id: f_00004-0-5 loss: 0.729547  [  192/  306]
train() client id: f_00004-0-6 loss: 0.796762  [  224/  306]
train() client id: f_00004-0-7 loss: 0.784670  [  256/  306]
train() client id: f_00004-0-8 loss: 0.870080  [  288/  306]
train() client id: f_00004-1-0 loss: 0.825772  [   32/  306]
train() client id: f_00004-1-1 loss: 0.832486  [   64/  306]
train() client id: f_00004-1-2 loss: 0.907662  [   96/  306]
train() client id: f_00004-1-3 loss: 0.833563  [  128/  306]
train() client id: f_00004-1-4 loss: 0.704591  [  160/  306]
train() client id: f_00004-1-5 loss: 0.886757  [  192/  306]
train() client id: f_00004-1-6 loss: 0.847104  [  224/  306]
train() client id: f_00004-1-7 loss: 0.948781  [  256/  306]
train() client id: f_00004-1-8 loss: 0.708310  [  288/  306]
train() client id: f_00004-2-0 loss: 0.846672  [   32/  306]
train() client id: f_00004-2-1 loss: 0.687898  [   64/  306]
train() client id: f_00004-2-2 loss: 0.829715  [   96/  306]
train() client id: f_00004-2-3 loss: 0.813110  [  128/  306]
train() client id: f_00004-2-4 loss: 0.749932  [  160/  306]
train() client id: f_00004-2-5 loss: 0.953275  [  192/  306]
train() client id: f_00004-2-6 loss: 1.006139  [  224/  306]
train() client id: f_00004-2-7 loss: 0.768166  [  256/  306]
train() client id: f_00004-2-8 loss: 0.819709  [  288/  306]
train() client id: f_00004-3-0 loss: 0.943633  [   32/  306]
train() client id: f_00004-3-1 loss: 0.972390  [   64/  306]
train() client id: f_00004-3-2 loss: 0.914624  [   96/  306]
train() client id: f_00004-3-3 loss: 0.734595  [  128/  306]
train() client id: f_00004-3-4 loss: 0.652606  [  160/  306]
train() client id: f_00004-3-5 loss: 0.886469  [  192/  306]
train() client id: f_00004-3-6 loss: 0.782364  [  224/  306]
train() client id: f_00004-3-7 loss: 0.751873  [  256/  306]
train() client id: f_00004-3-8 loss: 0.894983  [  288/  306]
train() client id: f_00004-4-0 loss: 0.688902  [   32/  306]
train() client id: f_00004-4-1 loss: 0.812207  [   64/  306]
train() client id: f_00004-4-2 loss: 0.789296  [   96/  306]
train() client id: f_00004-4-3 loss: 0.972630  [  128/  306]
train() client id: f_00004-4-4 loss: 0.762504  [  160/  306]
train() client id: f_00004-4-5 loss: 0.821378  [  192/  306]
train() client id: f_00004-4-6 loss: 0.874866  [  224/  306]
train() client id: f_00004-4-7 loss: 0.782958  [  256/  306]
train() client id: f_00004-4-8 loss: 0.866071  [  288/  306]
train() client id: f_00004-5-0 loss: 0.853717  [   32/  306]
train() client id: f_00004-5-1 loss: 0.845484  [   64/  306]
train() client id: f_00004-5-2 loss: 0.936105  [   96/  306]
train() client id: f_00004-5-3 loss: 0.735497  [  128/  306]
train() client id: f_00004-5-4 loss: 0.814170  [  160/  306]
train() client id: f_00004-5-5 loss: 0.807513  [  192/  306]
train() client id: f_00004-5-6 loss: 0.736096  [  224/  306]
train() client id: f_00004-5-7 loss: 0.959286  [  256/  306]
train() client id: f_00004-5-8 loss: 0.807233  [  288/  306]
train() client id: f_00004-6-0 loss: 0.831300  [   32/  306]
train() client id: f_00004-6-1 loss: 0.861639  [   64/  306]
train() client id: f_00004-6-2 loss: 0.766331  [   96/  306]
train() client id: f_00004-6-3 loss: 1.000874  [  128/  306]
train() client id: f_00004-6-4 loss: 0.827668  [  160/  306]
train() client id: f_00004-6-5 loss: 0.828453  [  192/  306]
train() client id: f_00004-6-6 loss: 0.789730  [  224/  306]
train() client id: f_00004-6-7 loss: 0.790702  [  256/  306]
train() client id: f_00004-6-8 loss: 0.723667  [  288/  306]
train() client id: f_00004-7-0 loss: 0.915046  [   32/  306]
train() client id: f_00004-7-1 loss: 0.879660  [   64/  306]
train() client id: f_00004-7-2 loss: 0.825998  [   96/  306]
train() client id: f_00004-7-3 loss: 0.868101  [  128/  306]
train() client id: f_00004-7-4 loss: 0.799257  [  160/  306]
train() client id: f_00004-7-5 loss: 0.848767  [  192/  306]
train() client id: f_00004-7-6 loss: 0.810787  [  224/  306]
train() client id: f_00004-7-7 loss: 0.723674  [  256/  306]
train() client id: f_00004-7-8 loss: 0.838407  [  288/  306]
train() client id: f_00004-8-0 loss: 0.740478  [   32/  306]
train() client id: f_00004-8-1 loss: 0.724492  [   64/  306]
train() client id: f_00004-8-2 loss: 0.848174  [   96/  306]
train() client id: f_00004-8-3 loss: 0.845254  [  128/  306]
train() client id: f_00004-8-4 loss: 0.808433  [  160/  306]
train() client id: f_00004-8-5 loss: 0.876685  [  192/  306]
train() client id: f_00004-8-6 loss: 0.837452  [  224/  306]
train() client id: f_00004-8-7 loss: 0.812442  [  256/  306]
train() client id: f_00004-8-8 loss: 0.815142  [  288/  306]
train() client id: f_00004-9-0 loss: 0.757000  [   32/  306]
train() client id: f_00004-9-1 loss: 0.910751  [   64/  306]
train() client id: f_00004-9-2 loss: 0.744398  [   96/  306]
train() client id: f_00004-9-3 loss: 1.020911  [  128/  306]
train() client id: f_00004-9-4 loss: 0.720726  [  160/  306]
train() client id: f_00004-9-5 loss: 0.873556  [  192/  306]
train() client id: f_00004-9-6 loss: 0.723234  [  224/  306]
train() client id: f_00004-9-7 loss: 0.836551  [  256/  306]
train() client id: f_00004-9-8 loss: 0.794900  [  288/  306]
train() client id: f_00004-10-0 loss: 0.831384  [   32/  306]
train() client id: f_00004-10-1 loss: 0.660520  [   64/  306]
train() client id: f_00004-10-2 loss: 0.809275  [   96/  306]
train() client id: f_00004-10-3 loss: 0.933517  [  128/  306]
train() client id: f_00004-10-4 loss: 0.855241  [  160/  306]
train() client id: f_00004-10-5 loss: 0.797818  [  192/  306]
train() client id: f_00004-10-6 loss: 0.842340  [  224/  306]
train() client id: f_00004-10-7 loss: 0.722097  [  256/  306]
train() client id: f_00004-10-8 loss: 0.899930  [  288/  306]
train() client id: f_00004-11-0 loss: 0.791663  [   32/  306]
train() client id: f_00004-11-1 loss: 0.951960  [   64/  306]
train() client id: f_00004-11-2 loss: 0.810884  [   96/  306]
train() client id: f_00004-11-3 loss: 0.816667  [  128/  306]
train() client id: f_00004-11-4 loss: 0.805817  [  160/  306]
train() client id: f_00004-11-5 loss: 0.847697  [  192/  306]
train() client id: f_00004-11-6 loss: 0.743122  [  224/  306]
train() client id: f_00004-11-7 loss: 0.840054  [  256/  306]
train() client id: f_00004-11-8 loss: 0.798116  [  288/  306]
train() client id: f_00004-12-0 loss: 0.802458  [   32/  306]
train() client id: f_00004-12-1 loss: 0.784673  [   64/  306]
train() client id: f_00004-12-2 loss: 0.748660  [   96/  306]
train() client id: f_00004-12-3 loss: 0.952353  [  128/  306]
train() client id: f_00004-12-4 loss: 0.766137  [  160/  306]
train() client id: f_00004-12-5 loss: 0.721312  [  192/  306]
train() client id: f_00004-12-6 loss: 0.847706  [  224/  306]
train() client id: f_00004-12-7 loss: 0.824504  [  256/  306]
train() client id: f_00004-12-8 loss: 0.825999  [  288/  306]
train() client id: f_00005-0-0 loss: 0.563966  [   32/  146]
train() client id: f_00005-0-1 loss: 0.408069  [   64/  146]
train() client id: f_00005-0-2 loss: 0.657378  [   96/  146]
train() client id: f_00005-0-3 loss: 0.730368  [  128/  146]
train() client id: f_00005-1-0 loss: 0.706299  [   32/  146]
train() client id: f_00005-1-1 loss: 0.515141  [   64/  146]
train() client id: f_00005-1-2 loss: 0.730253  [   96/  146]
train() client id: f_00005-1-3 loss: 0.422734  [  128/  146]
train() client id: f_00005-2-0 loss: 0.782121  [   32/  146]
train() client id: f_00005-2-1 loss: 0.640810  [   64/  146]
train() client id: f_00005-2-2 loss: 0.597618  [   96/  146]
train() client id: f_00005-2-3 loss: 0.415672  [  128/  146]
train() client id: f_00005-3-0 loss: 0.723597  [   32/  146]
train() client id: f_00005-3-1 loss: 0.488112  [   64/  146]
train() client id: f_00005-3-2 loss: 0.722438  [   96/  146]
train() client id: f_00005-3-3 loss: 0.540117  [  128/  146]
train() client id: f_00005-4-0 loss: 0.393272  [   32/  146]
train() client id: f_00005-4-1 loss: 0.403921  [   64/  146]
train() client id: f_00005-4-2 loss: 0.565729  [   96/  146]
train() client id: f_00005-4-3 loss: 1.012033  [  128/  146]
train() client id: f_00005-5-0 loss: 0.415121  [   32/  146]
train() client id: f_00005-5-1 loss: 0.691576  [   64/  146]
train() client id: f_00005-5-2 loss: 0.620693  [   96/  146]
train() client id: f_00005-5-3 loss: 0.610769  [  128/  146]
train() client id: f_00005-6-0 loss: 0.717204  [   32/  146]
train() client id: f_00005-6-1 loss: 0.397640  [   64/  146]
train() client id: f_00005-6-2 loss: 0.952275  [   96/  146]
train() client id: f_00005-6-3 loss: 0.365197  [  128/  146]
train() client id: f_00005-7-0 loss: 0.587655  [   32/  146]
train() client id: f_00005-7-1 loss: 0.624348  [   64/  146]
train() client id: f_00005-7-2 loss: 0.566472  [   96/  146]
train() client id: f_00005-7-3 loss: 0.656068  [  128/  146]
train() client id: f_00005-8-0 loss: 0.617317  [   32/  146]
train() client id: f_00005-8-1 loss: 0.597491  [   64/  146]
train() client id: f_00005-8-2 loss: 0.507546  [   96/  146]
train() client id: f_00005-8-3 loss: 0.794025  [  128/  146]
train() client id: f_00005-9-0 loss: 0.377180  [   32/  146]
train() client id: f_00005-9-1 loss: 0.784256  [   64/  146]
train() client id: f_00005-9-2 loss: 0.620505  [   96/  146]
train() client id: f_00005-9-3 loss: 0.561334  [  128/  146]
train() client id: f_00005-10-0 loss: 0.625003  [   32/  146]
train() client id: f_00005-10-1 loss: 0.741829  [   64/  146]
train() client id: f_00005-10-2 loss: 0.692757  [   96/  146]
train() client id: f_00005-10-3 loss: 0.488075  [  128/  146]
train() client id: f_00005-11-0 loss: 0.479752  [   32/  146]
train() client id: f_00005-11-1 loss: 0.727247  [   64/  146]
train() client id: f_00005-11-2 loss: 0.586255  [   96/  146]
train() client id: f_00005-11-3 loss: 0.592680  [  128/  146]
train() client id: f_00005-12-0 loss: 0.649346  [   32/  146]
train() client id: f_00005-12-1 loss: 0.652189  [   64/  146]
train() client id: f_00005-12-2 loss: 0.493514  [   96/  146]
train() client id: f_00005-12-3 loss: 0.588463  [  128/  146]
train() client id: f_00006-0-0 loss: 0.493038  [   32/   54]
train() client id: f_00006-1-0 loss: 0.477688  [   32/   54]
train() client id: f_00006-2-0 loss: 0.522142  [   32/   54]
train() client id: f_00006-3-0 loss: 0.570152  [   32/   54]
train() client id: f_00006-4-0 loss: 0.524394  [   32/   54]
train() client id: f_00006-5-0 loss: 0.594782  [   32/   54]
train() client id: f_00006-6-0 loss: 0.588522  [   32/   54]
train() client id: f_00006-7-0 loss: 0.574333  [   32/   54]
train() client id: f_00006-8-0 loss: 0.504751  [   32/   54]
train() client id: f_00006-9-0 loss: 0.534553  [   32/   54]
train() client id: f_00006-10-0 loss: 0.532771  [   32/   54]
train() client id: f_00006-11-0 loss: 0.590757  [   32/   54]
train() client id: f_00006-12-0 loss: 0.536162  [   32/   54]
train() client id: f_00007-0-0 loss: 0.587081  [   32/  179]
train() client id: f_00007-0-1 loss: 0.683351  [   64/  179]
train() client id: f_00007-0-2 loss: 0.375972  [   96/  179]
train() client id: f_00007-0-3 loss: 0.365551  [  128/  179]
train() client id: f_00007-0-4 loss: 0.424929  [  160/  179]
train() client id: f_00007-1-0 loss: 0.526202  [   32/  179]
train() client id: f_00007-1-1 loss: 0.623645  [   64/  179]
train() client id: f_00007-1-2 loss: 0.382911  [   96/  179]
train() client id: f_00007-1-3 loss: 0.617840  [  128/  179]
train() client id: f_00007-1-4 loss: 0.371181  [  160/  179]
train() client id: f_00007-2-0 loss: 0.350685  [   32/  179]
train() client id: f_00007-2-1 loss: 0.442271  [   64/  179]
train() client id: f_00007-2-2 loss: 0.467523  [   96/  179]
train() client id: f_00007-2-3 loss: 0.651347  [  128/  179]
train() client id: f_00007-2-4 loss: 0.625564  [  160/  179]
train() client id: f_00007-3-0 loss: 0.459768  [   32/  179]
train() client id: f_00007-3-1 loss: 0.435632  [   64/  179]
train() client id: f_00007-3-2 loss: 0.482424  [   96/  179]
train() client id: f_00007-3-3 loss: 0.649339  [  128/  179]
train() client id: f_00007-3-4 loss: 0.381389  [  160/  179]
train() client id: f_00007-4-0 loss: 0.391377  [   32/  179]
train() client id: f_00007-4-1 loss: 0.653301  [   64/  179]
train() client id: f_00007-4-2 loss: 0.502637  [   96/  179]
train() client id: f_00007-4-3 loss: 0.316331  [  128/  179]
train() client id: f_00007-4-4 loss: 0.501637  [  160/  179]
train() client id: f_00007-5-0 loss: 0.469245  [   32/  179]
train() client id: f_00007-5-1 loss: 0.446422  [   64/  179]
train() client id: f_00007-5-2 loss: 0.418093  [   96/  179]
train() client id: f_00007-5-3 loss: 0.593224  [  128/  179]
train() client id: f_00007-5-4 loss: 0.502759  [  160/  179]
train() client id: f_00007-6-0 loss: 0.394688  [   32/  179]
train() client id: f_00007-6-1 loss: 0.425146  [   64/  179]
train() client id: f_00007-6-2 loss: 0.430215  [   96/  179]
train() client id: f_00007-6-3 loss: 0.428015  [  128/  179]
train() client id: f_00007-6-4 loss: 0.639526  [  160/  179]
train() client id: f_00007-7-0 loss: 0.421184  [   32/  179]
train() client id: f_00007-7-1 loss: 0.642469  [   64/  179]
train() client id: f_00007-7-2 loss: 0.344268  [   96/  179]
train() client id: f_00007-7-3 loss: 0.394606  [  128/  179]
train() client id: f_00007-7-4 loss: 0.482327  [  160/  179]
train() client id: f_00007-8-0 loss: 0.539882  [   32/  179]
train() client id: f_00007-8-1 loss: 0.459025  [   64/  179]
train() client id: f_00007-8-2 loss: 0.394542  [   96/  179]
train() client id: f_00007-8-3 loss: 0.481654  [  128/  179]
train() client id: f_00007-8-4 loss: 0.325715  [  160/  179]
train() client id: f_00007-9-0 loss: 0.283134  [   32/  179]
train() client id: f_00007-9-1 loss: 0.373354  [   64/  179]
train() client id: f_00007-9-2 loss: 0.582123  [   96/  179]
train() client id: f_00007-9-3 loss: 0.431629  [  128/  179]
train() client id: f_00007-9-4 loss: 0.472021  [  160/  179]
train() client id: f_00007-10-0 loss: 0.446977  [   32/  179]
train() client id: f_00007-10-1 loss: 0.500230  [   64/  179]
train() client id: f_00007-10-2 loss: 0.307156  [   96/  179]
train() client id: f_00007-10-3 loss: 0.521773  [  128/  179]
train() client id: f_00007-10-4 loss: 0.488955  [  160/  179]
train() client id: f_00007-11-0 loss: 0.516299  [   32/  179]
train() client id: f_00007-11-1 loss: 0.388081  [   64/  179]
train() client id: f_00007-11-2 loss: 0.321563  [   96/  179]
train() client id: f_00007-11-3 loss: 0.578255  [  128/  179]
train() client id: f_00007-11-4 loss: 0.465492  [  160/  179]
train() client id: f_00007-12-0 loss: 0.374589  [   32/  179]
train() client id: f_00007-12-1 loss: 0.643282  [   64/  179]
train() client id: f_00007-12-2 loss: 0.402862  [   96/  179]
train() client id: f_00007-12-3 loss: 0.361855  [  128/  179]
train() client id: f_00007-12-4 loss: 0.379796  [  160/  179]
train() client id: f_00008-0-0 loss: 0.743405  [   32/  130]
train() client id: f_00008-0-1 loss: 0.582592  [   64/  130]
train() client id: f_00008-0-2 loss: 0.744251  [   96/  130]
train() client id: f_00008-0-3 loss: 0.693894  [  128/  130]
train() client id: f_00008-1-0 loss: 0.775975  [   32/  130]
train() client id: f_00008-1-1 loss: 0.588270  [   64/  130]
train() client id: f_00008-1-2 loss: 0.687808  [   96/  130]
train() client id: f_00008-1-3 loss: 0.707373  [  128/  130]
train() client id: f_00008-2-0 loss: 0.670155  [   32/  130]
train() client id: f_00008-2-1 loss: 0.590848  [   64/  130]
train() client id: f_00008-2-2 loss: 0.698955  [   96/  130]
train() client id: f_00008-2-3 loss: 0.773524  [  128/  130]
train() client id: f_00008-3-0 loss: 0.763537  [   32/  130]
train() client id: f_00008-3-1 loss: 0.561589  [   64/  130]
train() client id: f_00008-3-2 loss: 0.650767  [   96/  130]
train() client id: f_00008-3-3 loss: 0.759855  [  128/  130]
train() client id: f_00008-4-0 loss: 0.663342  [   32/  130]
train() client id: f_00008-4-1 loss: 0.666291  [   64/  130]
train() client id: f_00008-4-2 loss: 0.660283  [   96/  130]
train() client id: f_00008-4-3 loss: 0.780045  [  128/  130]
train() client id: f_00008-5-0 loss: 0.757220  [   32/  130]
train() client id: f_00008-5-1 loss: 0.645928  [   64/  130]
train() client id: f_00008-5-2 loss: 0.819739  [   96/  130]
train() client id: f_00008-5-3 loss: 0.527532  [  128/  130]
train() client id: f_00008-6-0 loss: 0.667919  [   32/  130]
train() client id: f_00008-6-1 loss: 0.675481  [   64/  130]
train() client id: f_00008-6-2 loss: 0.710674  [   96/  130]
train() client id: f_00008-6-3 loss: 0.695653  [  128/  130]
train() client id: f_00008-7-0 loss: 0.702817  [   32/  130]
train() client id: f_00008-7-1 loss: 0.691852  [   64/  130]
train() client id: f_00008-7-2 loss: 0.608286  [   96/  130]
train() client id: f_00008-7-3 loss: 0.764814  [  128/  130]
train() client id: f_00008-8-0 loss: 0.637711  [   32/  130]
train() client id: f_00008-8-1 loss: 0.691315  [   64/  130]
train() client id: f_00008-8-2 loss: 0.777296  [   96/  130]
train() client id: f_00008-8-3 loss: 0.672907  [  128/  130]
train() client id: f_00008-9-0 loss: 0.654996  [   32/  130]
train() client id: f_00008-9-1 loss: 0.766295  [   64/  130]
train() client id: f_00008-9-2 loss: 0.696819  [   96/  130]
train() client id: f_00008-9-3 loss: 0.665985  [  128/  130]
train() client id: f_00008-10-0 loss: 0.585202  [   32/  130]
train() client id: f_00008-10-1 loss: 0.795115  [   64/  130]
train() client id: f_00008-10-2 loss: 0.679604  [   96/  130]
train() client id: f_00008-10-3 loss: 0.716442  [  128/  130]
train() client id: f_00008-11-0 loss: 0.632699  [   32/  130]
train() client id: f_00008-11-1 loss: 0.778374  [   64/  130]
train() client id: f_00008-11-2 loss: 0.650531  [   96/  130]
train() client id: f_00008-11-3 loss: 0.727838  [  128/  130]
train() client id: f_00008-12-0 loss: 0.777762  [   32/  130]
train() client id: f_00008-12-1 loss: 0.765364  [   64/  130]
train() client id: f_00008-12-2 loss: 0.640716  [   96/  130]
train() client id: f_00008-12-3 loss: 0.614219  [  128/  130]
train() client id: f_00009-0-0 loss: 1.041650  [   32/  118]
train() client id: f_00009-0-1 loss: 1.089215  [   64/  118]
train() client id: f_00009-0-2 loss: 1.112229  [   96/  118]
train() client id: f_00009-1-0 loss: 1.074609  [   32/  118]
train() client id: f_00009-1-1 loss: 0.945253  [   64/  118]
train() client id: f_00009-1-2 loss: 1.234478  [   96/  118]
train() client id: f_00009-2-0 loss: 1.215531  [   32/  118]
train() client id: f_00009-2-1 loss: 1.166905  [   64/  118]
train() client id: f_00009-2-2 loss: 0.924341  [   96/  118]
train() client id: f_00009-3-0 loss: 0.876963  [   32/  118]
train() client id: f_00009-3-1 loss: 1.041377  [   64/  118]
train() client id: f_00009-3-2 loss: 1.113570  [   96/  118]
train() client id: f_00009-4-0 loss: 0.973561  [   32/  118]
train() client id: f_00009-4-1 loss: 1.160770  [   64/  118]
train() client id: f_00009-4-2 loss: 0.842941  [   96/  118]
train() client id: f_00009-5-0 loss: 1.100051  [   32/  118]
train() client id: f_00009-5-1 loss: 0.887141  [   64/  118]
train() client id: f_00009-5-2 loss: 0.904188  [   96/  118]
train() client id: f_00009-6-0 loss: 0.894105  [   32/  118]
train() client id: f_00009-6-1 loss: 0.922880  [   64/  118]
train() client id: f_00009-6-2 loss: 0.989658  [   96/  118]
train() client id: f_00009-7-0 loss: 0.862856  [   32/  118]
train() client id: f_00009-7-1 loss: 1.108441  [   64/  118]
train() client id: f_00009-7-2 loss: 0.854632  [   96/  118]
train() client id: f_00009-8-0 loss: 0.858973  [   32/  118]
train() client id: f_00009-8-1 loss: 0.895791  [   64/  118]
train() client id: f_00009-8-2 loss: 1.021449  [   96/  118]
train() client id: f_00009-9-0 loss: 0.798417  [   32/  118]
train() client id: f_00009-9-1 loss: 0.970581  [   64/  118]
train() client id: f_00009-9-2 loss: 0.940793  [   96/  118]
train() client id: f_00009-10-0 loss: 0.862083  [   32/  118]
train() client id: f_00009-10-1 loss: 0.871696  [   64/  118]
train() client id: f_00009-10-2 loss: 1.011230  [   96/  118]
train() client id: f_00009-11-0 loss: 0.848933  [   32/  118]
train() client id: f_00009-11-1 loss: 0.985963  [   64/  118]
train() client id: f_00009-11-2 loss: 0.996848  [   96/  118]
train() client id: f_00009-12-0 loss: 0.861267  [   32/  118]
train() client id: f_00009-12-1 loss: 1.017718  [   64/  118]
train() client id: f_00009-12-2 loss: 0.808236  [   96/  118]
At round 40 accuracy: 0.6525198938992043
At round 40 training accuracy: 0.5888665325285044
At round 40 training loss: 0.8300536864380321
update_location
xs = [  -3.9056584     4.20031788  220.00902392   18.81129433    0.97929623
    3.95640986 -182.44319194 -161.32485185  204.66397685 -147.06087855]
ys = [ 212.5879595   195.55583871    1.32061395 -182.45517586  174.35018685
  157.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [234.96573089 219.68097033 241.67274283 208.9108805  200.99489216
 186.87150493 208.06827924 189.80617503 228.46446856 177.88455205]
dists_bs = [175.17456558 180.3754319  431.50510786 406.61655224 175.98049596
 179.77972646 177.78086556 174.71366329 411.01004024 173.26594513]
uav_gains = [9.82218151e-12 1.26815403e-11 8.69804006e-12 1.49604850e-11
 1.68083716e-11 2.06049621e-11 1.51494992e-11 1.97523587e-11
 1.09881270e-11 2.34789484e-11]
bs_gains = [5.77530797e-11 5.32105086e-11 4.62736173e-12 5.46482092e-12
 5.70155582e-11 5.37056630e-11 5.54135584e-11 5.81806876e-11
 5.30282480e-12 5.95521015e-11]
Round 41
-------------------------------
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.58035445 11.51525536  5.49993513  1.98836081 13.27975002  6.38965832
  2.46168242  7.8365251   5.78633913  5.1816046 ]
obj_prev = 65.51946532847218
eta_min = 2.9576516171485466e-17	eta_max = 0.9329613748543103
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 15.185923913957124	eta = 0.9090909090909091
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 28.643068262013102	eta = 0.4819799767971639
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 21.937413464592197	eta = 0.6293077986886223
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.723057688485657	eta = 0.6661847678972268
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65725512436301	eta = 0.668306863289048
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65704485277882	eta = 0.6683136660986402
eta = 0.6683136660986402
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.0332544  0.06993981 0.03272655 0.01134872 0.08076069 0.03853288
 0.01425189 0.04724236 0.0343101  0.03114301]
ene_total = [1.87549698 3.28425833 1.87007596 0.89063245 3.74131718 1.94496118
 1.01302102 2.39242243 2.02108855 1.62377077]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 0 obj = 3.892935261137077
eta = 0.6683136660986402
freqs = [30327342.94858874 59817206.41115162 30065677.93401769 10128218.84644976
 68956163.97393337 32948387.69791197 12711921.71227483 41669829.72547625
 32697548.88052001 26563493.52335148]
eta_min = 0.6683136660986426	eta_max = 0.6683136660986362
af = 0.007721858371378171	bf = 1.2881215340611454	zeta = 0.008494044208515988	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [1.80840054e-06 1.47963121e-05 1.74911774e-06 6.88319331e-08
 2.27050766e-05 2.47329858e-06 1.36166896e-07 4.85011281e-06
 2.16884794e-06 1.29929494e-06]
ene_total = [1.67138562 1.10800963 1.73368868 1.48456874 1.09397392 1.1040216
 1.47961889 1.38247263 2.03845756 1.08123529]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 1 obj = 3.8929352611370294
eta = 0.6683136660986362
freqs = [30327342.94858875 59817206.41115168 30065677.9340177  10128218.84644976
 68956163.97393346 32948387.697912   12711921.71227484 41669829.72547627
 32697548.88052    26563493.52335151]
Done!
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.53273391e-06 6.16327412e-05 7.28579662e-06 2.86713384e-07
 9.45760062e-05 1.03023084e-05 5.67191267e-07 2.02027197e-05
 9.03414599e-06 5.41209917e-06]
ene_total = [0.01075277 0.00717136 0.01115319 0.00954603 0.00710615 0.00710671
 0.00951441 0.00890468 0.01311419 0.00695648]
At round 41 energy consumption: 0.09132598543578574
At round 41 eta: 0.6683136660986362
At round 41 a_n: 14.138223130512367
At round 41 local rounds: 13.19619071883817
At round 41 global rounds: 42.625280831488105
gradient difference: 0.5582128763198853
train() client id: f_00000-0-0 loss: 0.863537  [   32/  126]
train() client id: f_00000-0-1 loss: 1.159699  [   64/  126]
train() client id: f_00000-0-2 loss: 1.061009  [   96/  126]
train() client id: f_00000-1-0 loss: 0.852354  [   32/  126]
train() client id: f_00000-1-1 loss: 0.884145  [   64/  126]
train() client id: f_00000-1-2 loss: 1.045503  [   96/  126]
train() client id: f_00000-2-0 loss: 0.968115  [   32/  126]
train() client id: f_00000-2-1 loss: 0.895963  [   64/  126]
train() client id: f_00000-2-2 loss: 0.972000  [   96/  126]
train() client id: f_00000-3-0 loss: 0.949663  [   32/  126]
train() client id: f_00000-3-1 loss: 0.995300  [   64/  126]
train() client id: f_00000-3-2 loss: 0.895370  [   96/  126]
train() client id: f_00000-4-0 loss: 0.797030  [   32/  126]
train() client id: f_00000-4-1 loss: 0.971360  [   64/  126]
train() client id: f_00000-4-2 loss: 0.877299  [   96/  126]
train() client id: f_00000-5-0 loss: 0.870435  [   32/  126]
train() client id: f_00000-5-1 loss: 0.891401  [   64/  126]
train() client id: f_00000-5-2 loss: 0.931570  [   96/  126]
train() client id: f_00000-6-0 loss: 0.864467  [   32/  126]
train() client id: f_00000-6-1 loss: 0.940734  [   64/  126]
train() client id: f_00000-6-2 loss: 0.933101  [   96/  126]
train() client id: f_00000-7-0 loss: 0.912997  [   32/  126]
train() client id: f_00000-7-1 loss: 0.859342  [   64/  126]
train() client id: f_00000-7-2 loss: 1.058235  [   96/  126]
train() client id: f_00000-8-0 loss: 0.930933  [   32/  126]
train() client id: f_00000-8-1 loss: 0.913762  [   64/  126]
train() client id: f_00000-8-2 loss: 0.913369  [   96/  126]
train() client id: f_00000-9-0 loss: 0.837132  [   32/  126]
train() client id: f_00000-9-1 loss: 1.043405  [   64/  126]
train() client id: f_00000-9-2 loss: 0.851981  [   96/  126]
train() client id: f_00000-10-0 loss: 0.952313  [   32/  126]
train() client id: f_00000-10-1 loss: 0.939921  [   64/  126]
train() client id: f_00000-10-2 loss: 0.972900  [   96/  126]
train() client id: f_00000-11-0 loss: 0.941060  [   32/  126]
train() client id: f_00000-11-1 loss: 0.898414  [   64/  126]
train() client id: f_00000-11-2 loss: 0.950732  [   96/  126]
train() client id: f_00000-12-0 loss: 0.847956  [   32/  126]
train() client id: f_00000-12-1 loss: 0.935259  [   64/  126]
train() client id: f_00000-12-2 loss: 0.922205  [   96/  126]
train() client id: f_00001-0-0 loss: 0.382681  [   32/  265]
train() client id: f_00001-0-1 loss: 0.457025  [   64/  265]
train() client id: f_00001-0-2 loss: 0.550775  [   96/  265]
train() client id: f_00001-0-3 loss: 0.495327  [  128/  265]
train() client id: f_00001-0-4 loss: 0.444583  [  160/  265]
train() client id: f_00001-0-5 loss: 0.344898  [  192/  265]
train() client id: f_00001-0-6 loss: 0.483450  [  224/  265]
train() client id: f_00001-0-7 loss: 0.451290  [  256/  265]
train() client id: f_00001-1-0 loss: 0.368691  [   32/  265]
train() client id: f_00001-1-1 loss: 0.510868  [   64/  265]
train() client id: f_00001-1-2 loss: 0.424554  [   96/  265]
train() client id: f_00001-1-3 loss: 0.457241  [  128/  265]
train() client id: f_00001-1-4 loss: 0.364136  [  160/  265]
train() client id: f_00001-1-5 loss: 0.542994  [  192/  265]
train() client id: f_00001-1-6 loss: 0.498990  [  224/  265]
train() client id: f_00001-1-7 loss: 0.373434  [  256/  265]
train() client id: f_00001-2-0 loss: 0.384973  [   32/  265]
train() client id: f_00001-2-1 loss: 0.442645  [   64/  265]
train() client id: f_00001-2-2 loss: 0.488444  [   96/  265]
train() client id: f_00001-2-3 loss: 0.474536  [  128/  265]
train() client id: f_00001-2-4 loss: 0.348219  [  160/  265]
train() client id: f_00001-2-5 loss: 0.421163  [  192/  265]
train() client id: f_00001-2-6 loss: 0.484343  [  224/  265]
train() client id: f_00001-2-7 loss: 0.425426  [  256/  265]
train() client id: f_00001-3-0 loss: 0.416303  [   32/  265]
train() client id: f_00001-3-1 loss: 0.386578  [   64/  265]
train() client id: f_00001-3-2 loss: 0.394849  [   96/  265]
train() client id: f_00001-3-3 loss: 0.341092  [  128/  265]
train() client id: f_00001-3-4 loss: 0.337592  [  160/  265]
train() client id: f_00001-3-5 loss: 0.603219  [  192/  265]
train() client id: f_00001-3-6 loss: 0.441855  [  224/  265]
train() client id: f_00001-3-7 loss: 0.461681  [  256/  265]
train() client id: f_00001-4-0 loss: 0.502044  [   32/  265]
train() client id: f_00001-4-1 loss: 0.476569  [   64/  265]
train() client id: f_00001-4-2 loss: 0.356650  [   96/  265]
train() client id: f_00001-4-3 loss: 0.406421  [  128/  265]
train() client id: f_00001-4-4 loss: 0.403908  [  160/  265]
train() client id: f_00001-4-5 loss: 0.462682  [  192/  265]
train() client id: f_00001-4-6 loss: 0.407717  [  224/  265]
train() client id: f_00001-4-7 loss: 0.408486  [  256/  265]
train() client id: f_00001-5-0 loss: 0.337187  [   32/  265]
train() client id: f_00001-5-1 loss: 0.370856  [   64/  265]
train() client id: f_00001-5-2 loss: 0.415390  [   96/  265]
train() client id: f_00001-5-3 loss: 0.418100  [  128/  265]
train() client id: f_00001-5-4 loss: 0.480600  [  160/  265]
train() client id: f_00001-5-5 loss: 0.340500  [  192/  265]
train() client id: f_00001-5-6 loss: 0.411937  [  224/  265]
train() client id: f_00001-5-7 loss: 0.632218  [  256/  265]
train() client id: f_00001-6-0 loss: 0.449284  [   32/  265]
train() client id: f_00001-6-1 loss: 0.353241  [   64/  265]
train() client id: f_00001-6-2 loss: 0.395304  [   96/  265]
train() client id: f_00001-6-3 loss: 0.336127  [  128/  265]
train() client id: f_00001-6-4 loss: 0.367000  [  160/  265]
train() client id: f_00001-6-5 loss: 0.450905  [  192/  265]
train() client id: f_00001-6-6 loss: 0.500978  [  224/  265]
train() client id: f_00001-6-7 loss: 0.518346  [  256/  265]
train() client id: f_00001-7-0 loss: 0.379091  [   32/  265]
train() client id: f_00001-7-1 loss: 0.371250  [   64/  265]
train() client id: f_00001-7-2 loss: 0.316319  [   96/  265]
train() client id: f_00001-7-3 loss: 0.394824  [  128/  265]
train() client id: f_00001-7-4 loss: 0.498326  [  160/  265]
train() client id: f_00001-7-5 loss: 0.611667  [  192/  265]
train() client id: f_00001-7-6 loss: 0.321760  [  224/  265]
train() client id: f_00001-7-7 loss: 0.483602  [  256/  265]
train() client id: f_00001-8-0 loss: 0.442382  [   32/  265]
train() client id: f_00001-8-1 loss: 0.495765  [   64/  265]
train() client id: f_00001-8-2 loss: 0.495557  [   96/  265]
train() client id: f_00001-8-3 loss: 0.419960  [  128/  265]
train() client id: f_00001-8-4 loss: 0.497486  [  160/  265]
train() client id: f_00001-8-5 loss: 0.324405  [  192/  265]
train() client id: f_00001-8-6 loss: 0.367514  [  224/  265]
train() client id: f_00001-8-7 loss: 0.316088  [  256/  265]
train() client id: f_00001-9-0 loss: 0.374523  [   32/  265]
train() client id: f_00001-9-1 loss: 0.497210  [   64/  265]
train() client id: f_00001-9-2 loss: 0.494577  [   96/  265]
train() client id: f_00001-9-3 loss: 0.380382  [  128/  265]
train() client id: f_00001-9-4 loss: 0.504688  [  160/  265]
train() client id: f_00001-9-5 loss: 0.375582  [  192/  265]
train() client id: f_00001-9-6 loss: 0.396392  [  224/  265]
train() client id: f_00001-9-7 loss: 0.318456  [  256/  265]
train() client id: f_00001-10-0 loss: 0.318872  [   32/  265]
train() client id: f_00001-10-1 loss: 0.446078  [   64/  265]
train() client id: f_00001-10-2 loss: 0.376587  [   96/  265]
train() client id: f_00001-10-3 loss: 0.322852  [  128/  265]
train() client id: f_00001-10-4 loss: 0.435751  [  160/  265]
train() client id: f_00001-10-5 loss: 0.470998  [  192/  265]
train() client id: f_00001-10-6 loss: 0.322661  [  224/  265]
train() client id: f_00001-10-7 loss: 0.608757  [  256/  265]
train() client id: f_00001-11-0 loss: 0.412001  [   32/  265]
train() client id: f_00001-11-1 loss: 0.414477  [   64/  265]
train() client id: f_00001-11-2 loss: 0.335425  [   96/  265]
train() client id: f_00001-11-3 loss: 0.438930  [  128/  265]
train() client id: f_00001-11-4 loss: 0.356141  [  160/  265]
train() client id: f_00001-11-5 loss: 0.368479  [  192/  265]
train() client id: f_00001-11-6 loss: 0.460931  [  224/  265]
train() client id: f_00001-11-7 loss: 0.479734  [  256/  265]
train() client id: f_00001-12-0 loss: 0.314824  [   32/  265]
train() client id: f_00001-12-1 loss: 0.376603  [   64/  265]
train() client id: f_00001-12-2 loss: 0.399651  [   96/  265]
train() client id: f_00001-12-3 loss: 0.379815  [  128/  265]
train() client id: f_00001-12-4 loss: 0.313963  [  160/  265]
train() client id: f_00001-12-5 loss: 0.499972  [  192/  265]
train() client id: f_00001-12-6 loss: 0.497648  [  224/  265]
train() client id: f_00001-12-7 loss: 0.507571  [  256/  265]
train() client id: f_00002-0-0 loss: 1.006436  [   32/  124]
train() client id: f_00002-0-1 loss: 1.345899  [   64/  124]
train() client id: f_00002-0-2 loss: 1.211282  [   96/  124]
train() client id: f_00002-1-0 loss: 1.168072  [   32/  124]
train() client id: f_00002-1-1 loss: 1.024333  [   64/  124]
train() client id: f_00002-1-2 loss: 1.084877  [   96/  124]
train() client id: f_00002-2-0 loss: 1.071519  [   32/  124]
train() client id: f_00002-2-1 loss: 0.996880  [   64/  124]
train() client id: f_00002-2-2 loss: 1.121085  [   96/  124]
train() client id: f_00002-3-0 loss: 1.044480  [   32/  124]
train() client id: f_00002-3-1 loss: 1.019031  [   64/  124]
train() client id: f_00002-3-2 loss: 1.059175  [   96/  124]
train() client id: f_00002-4-0 loss: 0.955052  [   32/  124]
train() client id: f_00002-4-1 loss: 1.052999  [   64/  124]
train() client id: f_00002-4-2 loss: 1.072624  [   96/  124]
train() client id: f_00002-5-0 loss: 0.858925  [   32/  124]
train() client id: f_00002-5-1 loss: 1.111209  [   64/  124]
train() client id: f_00002-5-2 loss: 0.935456  [   96/  124]
train() client id: f_00002-6-0 loss: 0.991749  [   32/  124]
train() client id: f_00002-6-1 loss: 0.892931  [   64/  124]
train() client id: f_00002-6-2 loss: 0.816705  [   96/  124]
train() client id: f_00002-7-0 loss: 1.052902  [   32/  124]
train() client id: f_00002-7-1 loss: 0.773932  [   64/  124]
train() client id: f_00002-7-2 loss: 0.947808  [   96/  124]
train() client id: f_00002-8-0 loss: 0.783958  [   32/  124]
train() client id: f_00002-8-1 loss: 1.064116  [   64/  124]
train() client id: f_00002-8-2 loss: 0.890521  [   96/  124]
train() client id: f_00002-9-0 loss: 0.883949  [   32/  124]
train() client id: f_00002-9-1 loss: 0.787979  [   64/  124]
train() client id: f_00002-9-2 loss: 0.841930  [   96/  124]
train() client id: f_00002-10-0 loss: 1.013050  [   32/  124]
train() client id: f_00002-10-1 loss: 1.021542  [   64/  124]
train() client id: f_00002-10-2 loss: 0.722693  [   96/  124]
train() client id: f_00002-11-0 loss: 0.659501  [   32/  124]
train() client id: f_00002-11-1 loss: 1.071573  [   64/  124]
train() client id: f_00002-11-2 loss: 0.911422  [   96/  124]
train() client id: f_00002-12-0 loss: 1.082883  [   32/  124]
train() client id: f_00002-12-1 loss: 0.655856  [   64/  124]
train() client id: f_00002-12-2 loss: 0.925516  [   96/  124]
train() client id: f_00003-0-0 loss: 0.876076  [   32/   43]
train() client id: f_00003-1-0 loss: 0.814624  [   32/   43]
train() client id: f_00003-2-0 loss: 0.844931  [   32/   43]
train() client id: f_00003-3-0 loss: 0.947060  [   32/   43]
train() client id: f_00003-4-0 loss: 0.799716  [   32/   43]
train() client id: f_00003-5-0 loss: 0.708041  [   32/   43]
train() client id: f_00003-6-0 loss: 0.983918  [   32/   43]
train() client id: f_00003-7-0 loss: 0.718877  [   32/   43]
train() client id: f_00003-8-0 loss: 0.825798  [   32/   43]
train() client id: f_00003-9-0 loss: 0.878349  [   32/   43]
train() client id: f_00003-10-0 loss: 0.841571  [   32/   43]
train() client id: f_00003-11-0 loss: 0.655348  [   32/   43]
train() client id: f_00003-12-0 loss: 0.684670  [   32/   43]
train() client id: f_00004-0-0 loss: 0.862863  [   32/  306]
train() client id: f_00004-0-1 loss: 0.605816  [   64/  306]
train() client id: f_00004-0-2 loss: 0.801005  [   96/  306]
train() client id: f_00004-0-3 loss: 0.799356  [  128/  306]
train() client id: f_00004-0-4 loss: 0.771914  [  160/  306]
train() client id: f_00004-0-5 loss: 0.969844  [  192/  306]
train() client id: f_00004-0-6 loss: 0.996217  [  224/  306]
train() client id: f_00004-0-7 loss: 0.900665  [  256/  306]
train() client id: f_00004-0-8 loss: 0.816728  [  288/  306]
train() client id: f_00004-1-0 loss: 0.747452  [   32/  306]
train() client id: f_00004-1-1 loss: 0.784292  [   64/  306]
train() client id: f_00004-1-2 loss: 0.807474  [   96/  306]
train() client id: f_00004-1-3 loss: 0.935068  [  128/  306]
train() client id: f_00004-1-4 loss: 0.911191  [  160/  306]
train() client id: f_00004-1-5 loss: 0.839870  [  192/  306]
train() client id: f_00004-1-6 loss: 0.743319  [  224/  306]
train() client id: f_00004-1-7 loss: 0.869741  [  256/  306]
train() client id: f_00004-1-8 loss: 0.875891  [  288/  306]
train() client id: f_00004-2-0 loss: 0.843482  [   32/  306]
train() client id: f_00004-2-1 loss: 0.835735  [   64/  306]
train() client id: f_00004-2-2 loss: 0.642177  [   96/  306]
train() client id: f_00004-2-3 loss: 0.906470  [  128/  306]
train() client id: f_00004-2-4 loss: 0.843903  [  160/  306]
train() client id: f_00004-2-5 loss: 0.836442  [  192/  306]
train() client id: f_00004-2-6 loss: 0.821287  [  224/  306]
train() client id: f_00004-2-7 loss: 0.751553  [  256/  306]
train() client id: f_00004-2-8 loss: 0.996265  [  288/  306]
train() client id: f_00004-3-0 loss: 0.880353  [   32/  306]
train() client id: f_00004-3-1 loss: 0.973128  [   64/  306]
train() client id: f_00004-3-2 loss: 0.906679  [   96/  306]
train() client id: f_00004-3-3 loss: 0.855993  [  128/  306]
train() client id: f_00004-3-4 loss: 0.720878  [  160/  306]
train() client id: f_00004-3-5 loss: 0.720729  [  192/  306]
train() client id: f_00004-3-6 loss: 0.865624  [  224/  306]
train() client id: f_00004-3-7 loss: 0.890363  [  256/  306]
train() client id: f_00004-3-8 loss: 0.796322  [  288/  306]
train() client id: f_00004-4-0 loss: 0.921053  [   32/  306]
train() client id: f_00004-4-1 loss: 0.865235  [   64/  306]
train() client id: f_00004-4-2 loss: 0.758740  [   96/  306]
train() client id: f_00004-4-3 loss: 0.856454  [  128/  306]
train() client id: f_00004-4-4 loss: 0.772250  [  160/  306]
train() client id: f_00004-4-5 loss: 0.828647  [  192/  306]
train() client id: f_00004-4-6 loss: 0.828155  [  224/  306]
train() client id: f_00004-4-7 loss: 0.782748  [  256/  306]
train() client id: f_00004-4-8 loss: 0.902155  [  288/  306]
train() client id: f_00004-5-0 loss: 0.818439  [   32/  306]
train() client id: f_00004-5-1 loss: 0.725764  [   64/  306]
train() client id: f_00004-5-2 loss: 0.864437  [   96/  306]
train() client id: f_00004-5-3 loss: 0.817440  [  128/  306]
train() client id: f_00004-5-4 loss: 0.850001  [  160/  306]
train() client id: f_00004-5-5 loss: 0.845101  [  192/  306]
train() client id: f_00004-5-6 loss: 0.927361  [  224/  306]
train() client id: f_00004-5-7 loss: 0.882062  [  256/  306]
train() client id: f_00004-5-8 loss: 0.861410  [  288/  306]
train() client id: f_00004-6-0 loss: 0.782619  [   32/  306]
train() client id: f_00004-6-1 loss: 0.890535  [   64/  306]
train() client id: f_00004-6-2 loss: 0.819014  [   96/  306]
train() client id: f_00004-6-3 loss: 0.790862  [  128/  306]
train() client id: f_00004-6-4 loss: 0.799021  [  160/  306]
train() client id: f_00004-6-5 loss: 0.954072  [  192/  306]
train() client id: f_00004-6-6 loss: 0.917727  [  224/  306]
train() client id: f_00004-6-7 loss: 0.803630  [  256/  306]
train() client id: f_00004-6-8 loss: 0.776149  [  288/  306]
train() client id: f_00004-7-0 loss: 0.815161  [   32/  306]
train() client id: f_00004-7-1 loss: 0.830754  [   64/  306]
train() client id: f_00004-7-2 loss: 0.792875  [   96/  306]
train() client id: f_00004-7-3 loss: 0.880006  [  128/  306]
train() client id: f_00004-7-4 loss: 0.829184  [  160/  306]
train() client id: f_00004-7-5 loss: 0.916056  [  192/  306]
train() client id: f_00004-7-6 loss: 0.956389  [  224/  306]
train() client id: f_00004-7-7 loss: 0.859774  [  256/  306]
train() client id: f_00004-7-8 loss: 0.813157  [  288/  306]
train() client id: f_00004-8-0 loss: 0.845853  [   32/  306]
train() client id: f_00004-8-1 loss: 0.758121  [   64/  306]
train() client id: f_00004-8-2 loss: 0.939495  [   96/  306]
train() client id: f_00004-8-3 loss: 0.756243  [  128/  306]
train() client id: f_00004-8-4 loss: 0.789609  [  160/  306]
train() client id: f_00004-8-5 loss: 0.906101  [  192/  306]
train() client id: f_00004-8-6 loss: 0.863717  [  224/  306]
train() client id: f_00004-8-7 loss: 0.827636  [  256/  306]
train() client id: f_00004-8-8 loss: 0.917869  [  288/  306]
train() client id: f_00004-9-0 loss: 0.822870  [   32/  306]
train() client id: f_00004-9-1 loss: 0.882279  [   64/  306]
train() client id: f_00004-9-2 loss: 0.757340  [   96/  306]
train() client id: f_00004-9-3 loss: 0.863053  [  128/  306]
train() client id: f_00004-9-4 loss: 0.852507  [  160/  306]
train() client id: f_00004-9-5 loss: 0.890547  [  192/  306]
train() client id: f_00004-9-6 loss: 0.849339  [  224/  306]
train() client id: f_00004-9-7 loss: 0.940206  [  256/  306]
train() client id: f_00004-9-8 loss: 0.758707  [  288/  306]
train() client id: f_00004-10-0 loss: 0.852662  [   32/  306]
train() client id: f_00004-10-1 loss: 0.871716  [   64/  306]
train() client id: f_00004-10-2 loss: 0.886804  [   96/  306]
train() client id: f_00004-10-3 loss: 0.879277  [  128/  306]
train() client id: f_00004-10-4 loss: 0.812059  [  160/  306]
train() client id: f_00004-10-5 loss: 0.808925  [  192/  306]
train() client id: f_00004-10-6 loss: 0.854553  [  224/  306]
train() client id: f_00004-10-7 loss: 0.806670  [  256/  306]
train() client id: f_00004-10-8 loss: 0.837595  [  288/  306]
train() client id: f_00004-11-0 loss: 0.730119  [   32/  306]
train() client id: f_00004-11-1 loss: 0.777567  [   64/  306]
train() client id: f_00004-11-2 loss: 0.858648  [   96/  306]
train() client id: f_00004-11-3 loss: 0.878809  [  128/  306]
train() client id: f_00004-11-4 loss: 0.894768  [  160/  306]
train() client id: f_00004-11-5 loss: 0.969600  [  192/  306]
train() client id: f_00004-11-6 loss: 0.837748  [  224/  306]
train() client id: f_00004-11-7 loss: 0.868115  [  256/  306]
train() client id: f_00004-11-8 loss: 0.845869  [  288/  306]
train() client id: f_00004-12-0 loss: 0.848773  [   32/  306]
train() client id: f_00004-12-1 loss: 0.920791  [   64/  306]
train() client id: f_00004-12-2 loss: 0.891518  [   96/  306]
train() client id: f_00004-12-3 loss: 0.895574  [  128/  306]
train() client id: f_00004-12-4 loss: 0.904995  [  160/  306]
train() client id: f_00004-12-5 loss: 0.798630  [  192/  306]
train() client id: f_00004-12-6 loss: 0.709722  [  224/  306]
train() client id: f_00004-12-7 loss: 0.852935  [  256/  306]
train() client id: f_00004-12-8 loss: 0.766577  [  288/  306]
train() client id: f_00005-0-0 loss: 0.631979  [   32/  146]
train() client id: f_00005-0-1 loss: 0.886441  [   64/  146]
train() client id: f_00005-0-2 loss: 0.545650  [   96/  146]
train() client id: f_00005-0-3 loss: 0.536836  [  128/  146]
train() client id: f_00005-1-0 loss: 0.637047  [   32/  146]
train() client id: f_00005-1-1 loss: 0.648767  [   64/  146]
train() client id: f_00005-1-2 loss: 0.954985  [   96/  146]
train() client id: f_00005-1-3 loss: 0.464985  [  128/  146]
train() client id: f_00005-2-0 loss: 0.615090  [   32/  146]
train() client id: f_00005-2-1 loss: 0.817496  [   64/  146]
train() client id: f_00005-2-2 loss: 0.596448  [   96/  146]
train() client id: f_00005-2-3 loss: 0.571220  [  128/  146]
train() client id: f_00005-3-0 loss: 0.616954  [   32/  146]
train() client id: f_00005-3-1 loss: 0.746574  [   64/  146]
train() client id: f_00005-3-2 loss: 0.732299  [   96/  146]
train() client id: f_00005-3-3 loss: 0.569947  [  128/  146]
train() client id: f_00005-4-0 loss: 0.654166  [   32/  146]
train() client id: f_00005-4-1 loss: 0.550384  [   64/  146]
train() client id: f_00005-4-2 loss: 0.648874  [   96/  146]
train() client id: f_00005-4-3 loss: 0.772370  [  128/  146]
train() client id: f_00005-5-0 loss: 0.388351  [   32/  146]
train() client id: f_00005-5-1 loss: 0.746880  [   64/  146]
train() client id: f_00005-5-2 loss: 0.704349  [   96/  146]
train() client id: f_00005-5-3 loss: 0.558199  [  128/  146]
train() client id: f_00005-6-0 loss: 0.505407  [   32/  146]
train() client id: f_00005-6-1 loss: 0.823169  [   64/  146]
train() client id: f_00005-6-2 loss: 0.570211  [   96/  146]
train() client id: f_00005-6-3 loss: 0.550560  [  128/  146]
train() client id: f_00005-7-0 loss: 0.637876  [   32/  146]
train() client id: f_00005-7-1 loss: 0.542440  [   64/  146]
train() client id: f_00005-7-2 loss: 0.404184  [   96/  146]
train() client id: f_00005-7-3 loss: 0.906536  [  128/  146]
train() client id: f_00005-8-0 loss: 0.732397  [   32/  146]
train() client id: f_00005-8-1 loss: 0.602445  [   64/  146]
train() client id: f_00005-8-2 loss: 0.800774  [   96/  146]
train() client id: f_00005-8-3 loss: 0.533632  [  128/  146]
train() client id: f_00005-9-0 loss: 0.730885  [   32/  146]
train() client id: f_00005-9-1 loss: 0.833724  [   64/  146]
train() client id: f_00005-9-2 loss: 0.584733  [   96/  146]
train() client id: f_00005-9-3 loss: 0.594307  [  128/  146]
train() client id: f_00005-10-0 loss: 0.735369  [   32/  146]
train() client id: f_00005-10-1 loss: 0.523386  [   64/  146]
train() client id: f_00005-10-2 loss: 0.457870  [   96/  146]
train() client id: f_00005-10-3 loss: 0.875644  [  128/  146]
train() client id: f_00005-11-0 loss: 0.712177  [   32/  146]
train() client id: f_00005-11-1 loss: 0.852957  [   64/  146]
train() client id: f_00005-11-2 loss: 0.594097  [   96/  146]
train() client id: f_00005-11-3 loss: 0.543748  [  128/  146]
train() client id: f_00005-12-0 loss: 0.568815  [   32/  146]
train() client id: f_00005-12-1 loss: 0.593314  [   64/  146]
train() client id: f_00005-12-2 loss: 0.675402  [   96/  146]
train() client id: f_00005-12-3 loss: 0.806081  [  128/  146]
train() client id: f_00006-0-0 loss: 0.479794  [   32/   54]
train() client id: f_00006-1-0 loss: 0.451859  [   32/   54]
train() client id: f_00006-2-0 loss: 0.534660  [   32/   54]
train() client id: f_00006-3-0 loss: 0.466254  [   32/   54]
train() client id: f_00006-4-0 loss: 0.559028  [   32/   54]
train() client id: f_00006-5-0 loss: 0.545571  [   32/   54]
train() client id: f_00006-6-0 loss: 0.508630  [   32/   54]
train() client id: f_00006-7-0 loss: 0.572614  [   32/   54]
train() client id: f_00006-8-0 loss: 0.556078  [   32/   54]
train() client id: f_00006-9-0 loss: 0.483843  [   32/   54]
train() client id: f_00006-10-0 loss: 0.436644  [   32/   54]
train() client id: f_00006-11-0 loss: 0.509601  [   32/   54]
train() client id: f_00006-12-0 loss: 0.492971  [   32/   54]
train() client id: f_00007-0-0 loss: 0.686855  [   32/  179]
train() client id: f_00007-0-1 loss: 0.748097  [   64/  179]
train() client id: f_00007-0-2 loss: 0.743717  [   96/  179]
train() client id: f_00007-0-3 loss: 0.554892  [  128/  179]
train() client id: f_00007-0-4 loss: 0.639031  [  160/  179]
train() client id: f_00007-1-0 loss: 0.696781  [   32/  179]
train() client id: f_00007-1-1 loss: 0.783124  [   64/  179]
train() client id: f_00007-1-2 loss: 0.604550  [   96/  179]
train() client id: f_00007-1-3 loss: 0.601110  [  128/  179]
train() client id: f_00007-1-4 loss: 0.544721  [  160/  179]
train() client id: f_00007-2-0 loss: 0.616022  [   32/  179]
train() client id: f_00007-2-1 loss: 0.660968  [   64/  179]
train() client id: f_00007-2-2 loss: 0.755645  [   96/  179]
train() client id: f_00007-2-3 loss: 0.506292  [  128/  179]
train() client id: f_00007-2-4 loss: 0.634889  [  160/  179]
train() client id: f_00007-3-0 loss: 0.601534  [   32/  179]
train() client id: f_00007-3-1 loss: 0.846517  [   64/  179]
train() client id: f_00007-3-2 loss: 0.635003  [   96/  179]
train() client id: f_00007-3-3 loss: 0.591157  [  128/  179]
train() client id: f_00007-3-4 loss: 0.540807  [  160/  179]
train() client id: f_00007-4-0 loss: 0.690307  [   32/  179]
train() client id: f_00007-4-1 loss: 0.670321  [   64/  179]
train() client id: f_00007-4-2 loss: 0.616578  [   96/  179]
train() client id: f_00007-4-3 loss: 0.550383  [  128/  179]
train() client id: f_00007-4-4 loss: 0.550115  [  160/  179]
train() client id: f_00007-5-0 loss: 0.531190  [   32/  179]
train() client id: f_00007-5-1 loss: 0.529615  [   64/  179]
train() client id: f_00007-5-2 loss: 0.604453  [   96/  179]
train() client id: f_00007-5-3 loss: 0.581246  [  128/  179]
train() client id: f_00007-5-4 loss: 0.796886  [  160/  179]
train() client id: f_00007-6-0 loss: 0.525330  [   32/  179]
train() client id: f_00007-6-1 loss: 0.511143  [   64/  179]
train() client id: f_00007-6-2 loss: 0.617019  [   96/  179]
train() client id: f_00007-6-3 loss: 0.885405  [  128/  179]
train() client id: f_00007-6-4 loss: 0.548292  [  160/  179]
train() client id: f_00007-7-0 loss: 0.642836  [   32/  179]
train() client id: f_00007-7-1 loss: 0.674641  [   64/  179]
train() client id: f_00007-7-2 loss: 0.412192  [   96/  179]
train() client id: f_00007-7-3 loss: 0.553374  [  128/  179]
train() client id: f_00007-7-4 loss: 0.611512  [  160/  179]
train() client id: f_00007-8-0 loss: 0.475921  [   32/  179]
train() client id: f_00007-8-1 loss: 0.601983  [   64/  179]
train() client id: f_00007-8-2 loss: 0.602467  [   96/  179]
train() client id: f_00007-8-3 loss: 0.746123  [  128/  179]
train() client id: f_00007-8-4 loss: 0.573291  [  160/  179]
train() client id: f_00007-9-0 loss: 0.509104  [   32/  179]
train() client id: f_00007-9-1 loss: 0.825573  [   64/  179]
train() client id: f_00007-9-2 loss: 0.634990  [   96/  179]
train() client id: f_00007-9-3 loss: 0.534054  [  128/  179]
train() client id: f_00007-9-4 loss: 0.535214  [  160/  179]
train() client id: f_00007-10-0 loss: 0.513584  [   32/  179]
train() client id: f_00007-10-1 loss: 0.617314  [   64/  179]
train() client id: f_00007-10-2 loss: 0.541420  [   96/  179]
train() client id: f_00007-10-3 loss: 0.543625  [  128/  179]
train() client id: f_00007-10-4 loss: 0.618145  [  160/  179]
train() client id: f_00007-11-0 loss: 0.608821  [   32/  179]
train() client id: f_00007-11-1 loss: 0.434275  [   64/  179]
train() client id: f_00007-11-2 loss: 0.545142  [   96/  179]
train() client id: f_00007-11-3 loss: 0.650919  [  128/  179]
train() client id: f_00007-11-4 loss: 0.764955  [  160/  179]
train() client id: f_00007-12-0 loss: 0.663115  [   32/  179]
train() client id: f_00007-12-1 loss: 0.456861  [   64/  179]
train() client id: f_00007-12-2 loss: 0.770836  [   96/  179]
train() client id: f_00007-12-3 loss: 0.409837  [  128/  179]
train() client id: f_00007-12-4 loss: 0.705509  [  160/  179]
train() client id: f_00008-0-0 loss: 0.776718  [   32/  130]
train() client id: f_00008-0-1 loss: 0.769411  [   64/  130]
train() client id: f_00008-0-2 loss: 0.772893  [   96/  130]
train() client id: f_00008-0-3 loss: 0.907796  [  128/  130]
train() client id: f_00008-1-0 loss: 0.778097  [   32/  130]
train() client id: f_00008-1-1 loss: 0.762971  [   64/  130]
train() client id: f_00008-1-2 loss: 0.845949  [   96/  130]
train() client id: f_00008-1-3 loss: 0.845857  [  128/  130]
train() client id: f_00008-2-0 loss: 0.852683  [   32/  130]
train() client id: f_00008-2-1 loss: 0.762932  [   64/  130]
train() client id: f_00008-2-2 loss: 0.780400  [   96/  130]
train() client id: f_00008-2-3 loss: 0.801657  [  128/  130]
train() client id: f_00008-3-0 loss: 0.808868  [   32/  130]
train() client id: f_00008-3-1 loss: 0.813677  [   64/  130]
train() client id: f_00008-3-2 loss: 0.836458  [   96/  130]
train() client id: f_00008-3-3 loss: 0.757322  [  128/  130]
train() client id: f_00008-4-0 loss: 0.833985  [   32/  130]
train() client id: f_00008-4-1 loss: 0.822656  [   64/  130]
train() client id: f_00008-4-2 loss: 0.694564  [   96/  130]
train() client id: f_00008-4-3 loss: 0.837358  [  128/  130]
train() client id: f_00008-5-0 loss: 0.795146  [   32/  130]
train() client id: f_00008-5-1 loss: 0.778216  [   64/  130]
train() client id: f_00008-5-2 loss: 0.840720  [   96/  130]
train() client id: f_00008-5-3 loss: 0.814467  [  128/  130]
train() client id: f_00008-6-0 loss: 0.946498  [   32/  130]
train() client id: f_00008-6-1 loss: 0.710354  [   64/  130]
train() client id: f_00008-6-2 loss: 0.751448  [   96/  130]
train() client id: f_00008-6-3 loss: 0.778283  [  128/  130]
train() client id: f_00008-7-0 loss: 0.795874  [   32/  130]
train() client id: f_00008-7-1 loss: 0.797671  [   64/  130]
train() client id: f_00008-7-2 loss: 0.753920  [   96/  130]
train() client id: f_00008-7-3 loss: 0.875244  [  128/  130]
train() client id: f_00008-8-0 loss: 0.801579  [   32/  130]
train() client id: f_00008-8-1 loss: 0.665350  [   64/  130]
train() client id: f_00008-8-2 loss: 0.950995  [   96/  130]
train() client id: f_00008-8-3 loss: 0.807874  [  128/  130]
train() client id: f_00008-9-0 loss: 0.777691  [   32/  130]
train() client id: f_00008-9-1 loss: 0.794501  [   64/  130]
train() client id: f_00008-9-2 loss: 0.783843  [   96/  130]
train() client id: f_00008-9-3 loss: 0.866982  [  128/  130]
train() client id: f_00008-10-0 loss: 0.758569  [   32/  130]
train() client id: f_00008-10-1 loss: 0.802112  [   64/  130]
train() client id: f_00008-10-2 loss: 0.755122  [   96/  130]
train() client id: f_00008-10-3 loss: 0.873792  [  128/  130]
train() client id: f_00008-11-0 loss: 0.751033  [   32/  130]
train() client id: f_00008-11-1 loss: 0.741747  [   64/  130]
train() client id: f_00008-11-2 loss: 0.786774  [   96/  130]
train() client id: f_00008-11-3 loss: 0.941817  [  128/  130]
train() client id: f_00008-12-0 loss: 0.667919  [   32/  130]
train() client id: f_00008-12-1 loss: 0.908249  [   64/  130]
train() client id: f_00008-12-2 loss: 0.784281  [   96/  130]
train() client id: f_00008-12-3 loss: 0.792934  [  128/  130]
train() client id: f_00009-0-0 loss: 0.994599  [   32/  118]
train() client id: f_00009-0-1 loss: 1.105064  [   64/  118]
train() client id: f_00009-0-2 loss: 1.270751  [   96/  118]
train() client id: f_00009-1-0 loss: 1.092995  [   32/  118]
train() client id: f_00009-1-1 loss: 1.000613  [   64/  118]
train() client id: f_00009-1-2 loss: 1.051585  [   96/  118]
train() client id: f_00009-2-0 loss: 0.893354  [   32/  118]
train() client id: f_00009-2-1 loss: 1.039074  [   64/  118]
train() client id: f_00009-2-2 loss: 0.967923  [   96/  118]
train() client id: f_00009-3-0 loss: 0.965698  [   32/  118]
train() client id: f_00009-3-1 loss: 0.987327  [   64/  118]
train() client id: f_00009-3-2 loss: 1.014600  [   96/  118]
train() client id: f_00009-4-0 loss: 0.945893  [   32/  118]
train() client id: f_00009-4-1 loss: 1.023610  [   64/  118]
train() client id: f_00009-4-2 loss: 0.880764  [   96/  118]
train() client id: f_00009-5-0 loss: 0.991420  [   32/  118]
train() client id: f_00009-5-1 loss: 0.919562  [   64/  118]
train() client id: f_00009-5-2 loss: 0.884626  [   96/  118]
train() client id: f_00009-6-0 loss: 0.933067  [   32/  118]
train() client id: f_00009-6-1 loss: 0.923412  [   64/  118]
train() client id: f_00009-6-2 loss: 0.897823  [   96/  118]
train() client id: f_00009-7-0 loss: 0.920894  [   32/  118]
train() client id: f_00009-7-1 loss: 0.916320  [   64/  118]
train() client id: f_00009-7-2 loss: 0.874240  [   96/  118]
train() client id: f_00009-8-0 loss: 0.813695  [   32/  118]
train() client id: f_00009-8-1 loss: 0.929715  [   64/  118]
train() client id: f_00009-8-2 loss: 0.866268  [   96/  118]
train() client id: f_00009-9-0 loss: 0.958096  [   32/  118]
train() client id: f_00009-9-1 loss: 0.945773  [   64/  118]
train() client id: f_00009-9-2 loss: 0.809659  [   96/  118]
train() client id: f_00009-10-0 loss: 0.820183  [   32/  118]
train() client id: f_00009-10-1 loss: 1.061883  [   64/  118]
train() client id: f_00009-10-2 loss: 0.917033  [   96/  118]
train() client id: f_00009-11-0 loss: 0.908298  [   32/  118]
train() client id: f_00009-11-1 loss: 0.951338  [   64/  118]
train() client id: f_00009-11-2 loss: 0.792130  [   96/  118]
train() client id: f_00009-12-0 loss: 1.123695  [   32/  118]
train() client id: f_00009-12-1 loss: 0.839955  [   64/  118]
train() client id: f_00009-12-2 loss: 0.814090  [   96/  118]
At round 41 accuracy: 0.6525198938992043
At round 41 training accuracy: 0.5821596244131455
At round 41 training loss: 0.83115609906299
update_location
xs = [  -3.9056584     4.20031788  225.00902392   18.81129433    0.97929623
    3.95640986 -187.44319194 -166.32485185  209.66397685 -152.06087855]
ys = [ 217.5879595   200.55583871    1.32061395 -187.45517586  179.35018685
  162.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [239.49900686 224.14345208 246.23323266 213.29160263 205.34714155
 191.11279617 212.46609317 194.07378133 232.95418683 182.039893  ]
dists_bs = [176.31508166 181.0134106  436.08685872 411.01894399 176.03305606
 179.37082135 178.06029338 174.39355681 415.63312302 172.53027713]
uav_gains = [9.05340846e-12 1.18039549e-11 7.98136893e-12 1.40041215e-11
 1.57718869e-11 1.93846067e-11 1.41810871e-11 1.85763382e-11
 1.01750306e-11 2.20968418e-11]
bs_gains = [5.67131265e-11 5.26870620e-11 4.49251656e-12 5.30250316e-12
 5.69679045e-11 5.40491731e-11 5.51704147e-11 5.84802023e-11
 5.13932005e-12 6.02658366e-11]
Round 42
-------------------------------
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.44885328 11.23645857  5.37086954  1.94257213 12.9580199   6.23472006
  2.4044106   7.64858783  5.64816701  5.05583458]
obj_prev = 63.948493486950056
eta_min = 1.1938832184881957e-17	eta_max = 0.9337601214025776
af = 13.47090363962996	bf = 1.273204732538192	zeta = 14.817994003592956	eta = 0.9090909090909091
af = 13.47090363962996	bf = 1.273204732538192	zeta = 28.128308786138636	eta = 0.47890912112953954
af = 13.47090363962996	bf = 1.273204732538192	zeta = 21.476258560043068	eta = 0.6272462962749386
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.271427562519737	eta = 0.6645266396796146
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.2057708129807	eta = 0.666685956418743
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.20555837007598	eta = 0.6666929660097932
eta = 0.6666929660097932
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.03345475 0.07036119 0.03292373 0.0114171  0.08124726 0.03876503
 0.01433775 0.04752699 0.03451681 0.03133064]
ene_total = [1.84044551 3.20713505 1.83649691 0.87474524 3.65310778 1.89784042
 0.99426624 2.34061661 1.97709129 1.58381332]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 0 obj = 3.828642052039432
eta = 0.6666929660097932
freqs = [29660676.66717937 58359378.23819214 29417104.30254062  9895264.30741952
 67264422.3037599  32133134.64789411 12419507.6458075  40702220.97084436
 31867673.54934066 25905086.18443851]
eta_min = 0.6666929660097993	eta_max = 0.6666929660097998
af = 0.007174173529547853	bf = 1.273204732538192	zeta = 0.007891590882502639	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [1.72976861e-06 1.40838874e-05 1.67446812e-06 6.57020031e-08
 2.16046690e-05 2.35241749e-06 1.29974416e-07 4.62748057e-06
 2.06015278e-06 1.23568407e-06]
ene_total = [1.67118944 1.08322193 1.73728876 1.47458345 1.06747966 1.07586755
 1.46958603 1.36994863 2.01106501 1.0525431 ]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 1 obj = 3.8286420520395077
eta = 0.6666929660097998
freqs = [29660676.66717935 58359378.23819202 29417104.30254061  9895264.30741951
 67264422.30375975 32133134.64789404 12419507.64580748 40702220.97084429
 31867673.54934067 25905086.18443845]
Done!
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.20519951e-06 5.86651983e-05 6.97485017e-06 2.73675935e-07
 8.99923549e-05 9.79878887e-06 5.41397037e-07 1.92753647e-05
 8.58138582e-06 5.14713367e-06]
ene_total = [0.01101808 0.00718267 0.01145348 0.00971724 0.00710274 0.00709707
 0.00968451 0.00904217 0.01325879 0.00693983]
At round 42 energy consumption: 0.09249657683909618
At round 42 eta: 0.6666929660097998
At round 42 a_n: 13.79567728354305
At round 42 local rounds: 13.275695913565364
At round 42 global rounds: 41.390297463535276
gradient difference: 0.357733815908432
train() client id: f_00000-0-0 loss: 1.137371  [   32/  126]
train() client id: f_00000-0-1 loss: 1.294727  [   64/  126]
train() client id: f_00000-0-2 loss: 1.261077  [   96/  126]
train() client id: f_00000-1-0 loss: 1.152571  [   32/  126]
train() client id: f_00000-1-1 loss: 1.048604  [   64/  126]
train() client id: f_00000-1-2 loss: 1.149641  [   96/  126]
train() client id: f_00000-2-0 loss: 1.115928  [   32/  126]
train() client id: f_00000-2-1 loss: 0.962288  [   64/  126]
train() client id: f_00000-2-2 loss: 1.113761  [   96/  126]
train() client id: f_00000-3-0 loss: 0.989183  [   32/  126]
train() client id: f_00000-3-1 loss: 0.988554  [   64/  126]
train() client id: f_00000-3-2 loss: 0.948218  [   96/  126]
train() client id: f_00000-4-0 loss: 1.002077  [   32/  126]
train() client id: f_00000-4-1 loss: 0.934878  [   64/  126]
train() client id: f_00000-4-2 loss: 0.976056  [   96/  126]
train() client id: f_00000-5-0 loss: 0.869730  [   32/  126]
train() client id: f_00000-5-1 loss: 1.037659  [   64/  126]
train() client id: f_00000-5-2 loss: 1.006071  [   96/  126]
train() client id: f_00000-6-0 loss: 0.897305  [   32/  126]
train() client id: f_00000-6-1 loss: 0.978539  [   64/  126]
train() client id: f_00000-6-2 loss: 0.872505  [   96/  126]
train() client id: f_00000-7-0 loss: 0.959351  [   32/  126]
train() client id: f_00000-7-1 loss: 0.857711  [   64/  126]
train() client id: f_00000-7-2 loss: 0.974817  [   96/  126]
train() client id: f_00000-8-0 loss: 0.877907  [   32/  126]
train() client id: f_00000-8-1 loss: 0.884317  [   64/  126]
train() client id: f_00000-8-2 loss: 0.904019  [   96/  126]
train() client id: f_00000-9-0 loss: 0.864668  [   32/  126]
train() client id: f_00000-9-1 loss: 0.937811  [   64/  126]
train() client id: f_00000-9-2 loss: 0.992564  [   96/  126]
train() client id: f_00000-10-0 loss: 0.912154  [   32/  126]
train() client id: f_00000-10-1 loss: 0.904012  [   64/  126]
train() client id: f_00000-10-2 loss: 0.871188  [   96/  126]
train() client id: f_00000-11-0 loss: 0.871978  [   32/  126]
train() client id: f_00000-11-1 loss: 0.797741  [   64/  126]
train() client id: f_00000-11-2 loss: 0.948779  [   96/  126]
train() client id: f_00000-12-0 loss: 0.911959  [   32/  126]
train() client id: f_00000-12-1 loss: 0.900135  [   64/  126]
train() client id: f_00000-12-2 loss: 0.803178  [   96/  126]
train() client id: f_00001-0-0 loss: 0.406267  [   32/  265]
train() client id: f_00001-0-1 loss: 0.216328  [   64/  265]
train() client id: f_00001-0-2 loss: 0.352660  [   96/  265]
train() client id: f_00001-0-3 loss: 0.432976  [  128/  265]
train() client id: f_00001-0-4 loss: 0.286197  [  160/  265]
train() client id: f_00001-0-5 loss: 0.262964  [  192/  265]
train() client id: f_00001-0-6 loss: 0.379878  [  224/  265]
train() client id: f_00001-0-7 loss: 0.278761  [  256/  265]
train() client id: f_00001-1-0 loss: 0.218346  [   32/  265]
train() client id: f_00001-1-1 loss: 0.318865  [   64/  265]
train() client id: f_00001-1-2 loss: 0.482781  [   96/  265]
train() client id: f_00001-1-3 loss: 0.306142  [  128/  265]
train() client id: f_00001-1-4 loss: 0.316566  [  160/  265]
train() client id: f_00001-1-5 loss: 0.266169  [  192/  265]
train() client id: f_00001-1-6 loss: 0.280465  [  224/  265]
train() client id: f_00001-1-7 loss: 0.388297  [  256/  265]
train() client id: f_00001-2-0 loss: 0.271329  [   32/  265]
train() client id: f_00001-2-1 loss: 0.324417  [   64/  265]
train() client id: f_00001-2-2 loss: 0.396081  [   96/  265]
train() client id: f_00001-2-3 loss: 0.405633  [  128/  265]
train() client id: f_00001-2-4 loss: 0.222783  [  160/  265]
train() client id: f_00001-2-5 loss: 0.349459  [  192/  265]
train() client id: f_00001-2-6 loss: 0.235615  [  224/  265]
train() client id: f_00001-2-7 loss: 0.373549  [  256/  265]
train() client id: f_00001-3-0 loss: 0.290792  [   32/  265]
train() client id: f_00001-3-1 loss: 0.407162  [   64/  265]
train() client id: f_00001-3-2 loss: 0.388816  [   96/  265]
train() client id: f_00001-3-3 loss: 0.237600  [  128/  265]
train() client id: f_00001-3-4 loss: 0.332488  [  160/  265]
train() client id: f_00001-3-5 loss: 0.318693  [  192/  265]
train() client id: f_00001-3-6 loss: 0.196943  [  224/  265]
train() client id: f_00001-3-7 loss: 0.364457  [  256/  265]
train() client id: f_00001-4-0 loss: 0.306480  [   32/  265]
train() client id: f_00001-4-1 loss: 0.332941  [   64/  265]
train() client id: f_00001-4-2 loss: 0.290238  [   96/  265]
train() client id: f_00001-4-3 loss: 0.250371  [  128/  265]
train() client id: f_00001-4-4 loss: 0.335973  [  160/  265]
train() client id: f_00001-4-5 loss: 0.366773  [  192/  265]
train() client id: f_00001-4-6 loss: 0.319408  [  224/  265]
train() client id: f_00001-4-7 loss: 0.254485  [  256/  265]
train() client id: f_00001-5-0 loss: 0.283626  [   32/  265]
train() client id: f_00001-5-1 loss: 0.272256  [   64/  265]
train() client id: f_00001-5-2 loss: 0.461850  [   96/  265]
train() client id: f_00001-5-3 loss: 0.237524  [  128/  265]
train() client id: f_00001-5-4 loss: 0.356260  [  160/  265]
train() client id: f_00001-5-5 loss: 0.255513  [  192/  265]
train() client id: f_00001-5-6 loss: 0.402049  [  224/  265]
train() client id: f_00001-5-7 loss: 0.205058  [  256/  265]
train() client id: f_00001-6-0 loss: 0.362904  [   32/  265]
train() client id: f_00001-6-1 loss: 0.326297  [   64/  265]
train() client id: f_00001-6-2 loss: 0.336721  [   96/  265]
train() client id: f_00001-6-3 loss: 0.267126  [  128/  265]
train() client id: f_00001-6-4 loss: 0.357015  [  160/  265]
train() client id: f_00001-6-5 loss: 0.267068  [  192/  265]
train() client id: f_00001-6-6 loss: 0.191249  [  224/  265]
train() client id: f_00001-6-7 loss: 0.331666  [  256/  265]
train() client id: f_00001-7-0 loss: 0.213298  [   32/  265]
train() client id: f_00001-7-1 loss: 0.209427  [   64/  265]
train() client id: f_00001-7-2 loss: 0.374112  [   96/  265]
train() client id: f_00001-7-3 loss: 0.247722  [  128/  265]
train() client id: f_00001-7-4 loss: 0.386868  [  160/  265]
train() client id: f_00001-7-5 loss: 0.360645  [  192/  265]
train() client id: f_00001-7-6 loss: 0.407159  [  224/  265]
train() client id: f_00001-7-7 loss: 0.218158  [  256/  265]
train() client id: f_00001-8-0 loss: 0.421285  [   32/  265]
train() client id: f_00001-8-1 loss: 0.258848  [   64/  265]
train() client id: f_00001-8-2 loss: 0.380591  [   96/  265]
train() client id: f_00001-8-3 loss: 0.246777  [  128/  265]
train() client id: f_00001-8-4 loss: 0.340019  [  160/  265]
train() client id: f_00001-8-5 loss: 0.225599  [  192/  265]
train() client id: f_00001-8-6 loss: 0.277937  [  224/  265]
train() client id: f_00001-8-7 loss: 0.252528  [  256/  265]
train() client id: f_00001-9-0 loss: 0.268623  [   32/  265]
train() client id: f_00001-9-1 loss: 0.321878  [   64/  265]
train() client id: f_00001-9-2 loss: 0.293881  [   96/  265]
train() client id: f_00001-9-3 loss: 0.387109  [  128/  265]
train() client id: f_00001-9-4 loss: 0.197629  [  160/  265]
train() client id: f_00001-9-5 loss: 0.234756  [  192/  265]
train() client id: f_00001-9-6 loss: 0.299612  [  224/  265]
train() client id: f_00001-9-7 loss: 0.293461  [  256/  265]
train() client id: f_00001-10-0 loss: 0.310021  [   32/  265]
train() client id: f_00001-10-1 loss: 0.305115  [   64/  265]
train() client id: f_00001-10-2 loss: 0.365425  [   96/  265]
train() client id: f_00001-10-3 loss: 0.214164  [  128/  265]
train() client id: f_00001-10-4 loss: 0.288177  [  160/  265]
train() client id: f_00001-10-5 loss: 0.420313  [  192/  265]
train() client id: f_00001-10-6 loss: 0.201146  [  224/  265]
train() client id: f_00001-10-7 loss: 0.266197  [  256/  265]
train() client id: f_00001-11-0 loss: 0.285313  [   32/  265]
train() client id: f_00001-11-1 loss: 0.247329  [   64/  265]
train() client id: f_00001-11-2 loss: 0.396085  [   96/  265]
train() client id: f_00001-11-3 loss: 0.273678  [  128/  265]
train() client id: f_00001-11-4 loss: 0.281883  [  160/  265]
train() client id: f_00001-11-5 loss: 0.408149  [  192/  265]
train() client id: f_00001-11-6 loss: 0.206063  [  224/  265]
train() client id: f_00001-11-7 loss: 0.204180  [  256/  265]
train() client id: f_00001-12-0 loss: 0.205668  [   32/  265]
train() client id: f_00001-12-1 loss: 0.218565  [   64/  265]
train() client id: f_00001-12-2 loss: 0.301276  [   96/  265]
train() client id: f_00001-12-3 loss: 0.357530  [  128/  265]
train() client id: f_00001-12-4 loss: 0.207435  [  160/  265]
train() client id: f_00001-12-5 loss: 0.402247  [  192/  265]
train() client id: f_00001-12-6 loss: 0.387023  [  224/  265]
train() client id: f_00001-12-7 loss: 0.276573  [  256/  265]
train() client id: f_00002-0-0 loss: 1.141442  [   32/  124]
train() client id: f_00002-0-1 loss: 1.223683  [   64/  124]
train() client id: f_00002-0-2 loss: 1.244272  [   96/  124]
train() client id: f_00002-1-0 loss: 1.135723  [   32/  124]
train() client id: f_00002-1-1 loss: 0.955627  [   64/  124]
train() client id: f_00002-1-2 loss: 1.190530  [   96/  124]
train() client id: f_00002-2-0 loss: 1.055503  [   32/  124]
train() client id: f_00002-2-1 loss: 0.978147  [   64/  124]
train() client id: f_00002-2-2 loss: 1.212085  [   96/  124]
train() client id: f_00002-3-0 loss: 1.214745  [   32/  124]
train() client id: f_00002-3-1 loss: 1.063014  [   64/  124]
train() client id: f_00002-3-2 loss: 0.929352  [   96/  124]
train() client id: f_00002-4-0 loss: 0.927613  [   32/  124]
train() client id: f_00002-4-1 loss: 1.093180  [   64/  124]
train() client id: f_00002-4-2 loss: 0.955224  [   96/  124]
train() client id: f_00002-5-0 loss: 1.028025  [   32/  124]
train() client id: f_00002-5-1 loss: 0.905393  [   64/  124]
train() client id: f_00002-5-2 loss: 0.906096  [   96/  124]
train() client id: f_00002-6-0 loss: 0.845937  [   32/  124]
train() client id: f_00002-6-1 loss: 0.884857  [   64/  124]
train() client id: f_00002-6-2 loss: 0.934077  [   96/  124]
train() client id: f_00002-7-0 loss: 0.874889  [   32/  124]
train() client id: f_00002-7-1 loss: 1.050620  [   64/  124]
train() client id: f_00002-7-2 loss: 0.738987  [   96/  124]
train() client id: f_00002-8-0 loss: 0.795660  [   32/  124]
train() client id: f_00002-8-1 loss: 0.894448  [   64/  124]
train() client id: f_00002-8-2 loss: 1.000073  [   96/  124]
train() client id: f_00002-9-0 loss: 0.977430  [   32/  124]
train() client id: f_00002-9-1 loss: 0.812450  [   64/  124]
train() client id: f_00002-9-2 loss: 0.887752  [   96/  124]
train() client id: f_00002-10-0 loss: 0.795313  [   32/  124]
train() client id: f_00002-10-1 loss: 0.849816  [   64/  124]
train() client id: f_00002-10-2 loss: 0.989973  [   96/  124]
train() client id: f_00002-11-0 loss: 0.748935  [   32/  124]
train() client id: f_00002-11-1 loss: 0.813840  [   64/  124]
train() client id: f_00002-11-2 loss: 1.038994  [   96/  124]
train() client id: f_00002-12-0 loss: 0.761867  [   32/  124]
train() client id: f_00002-12-1 loss: 1.020398  [   64/  124]
train() client id: f_00002-12-2 loss: 0.696599  [   96/  124]
train() client id: f_00003-0-0 loss: 0.744861  [   32/   43]
train() client id: f_00003-1-0 loss: 0.774758  [   32/   43]
train() client id: f_00003-2-0 loss: 0.592977  [   32/   43]
train() client id: f_00003-3-0 loss: 0.602720  [   32/   43]
train() client id: f_00003-4-0 loss: 0.705365  [   32/   43]
train() client id: f_00003-5-0 loss: 0.873942  [   32/   43]
train() client id: f_00003-6-0 loss: 0.743282  [   32/   43]
train() client id: f_00003-7-0 loss: 0.793214  [   32/   43]
train() client id: f_00003-8-0 loss: 0.772623  [   32/   43]
train() client id: f_00003-9-0 loss: 0.801521  [   32/   43]
train() client id: f_00003-10-0 loss: 0.825895  [   32/   43]
train() client id: f_00003-11-0 loss: 0.754604  [   32/   43]
train() client id: f_00003-12-0 loss: 0.775228  [   32/   43]
train() client id: f_00004-0-0 loss: 0.996310  [   32/  306]
train() client id: f_00004-0-1 loss: 0.940717  [   64/  306]
train() client id: f_00004-0-2 loss: 0.951975  [   96/  306]
train() client id: f_00004-0-3 loss: 0.834805  [  128/  306]
train() client id: f_00004-0-4 loss: 1.095171  [  160/  306]
train() client id: f_00004-0-5 loss: 0.942990  [  192/  306]
train() client id: f_00004-0-6 loss: 0.923312  [  224/  306]
train() client id: f_00004-0-7 loss: 0.779027  [  256/  306]
train() client id: f_00004-0-8 loss: 1.010748  [  288/  306]
train() client id: f_00004-1-0 loss: 0.975249  [   32/  306]
train() client id: f_00004-1-1 loss: 1.051857  [   64/  306]
train() client id: f_00004-1-2 loss: 1.006252  [   96/  306]
train() client id: f_00004-1-3 loss: 0.843616  [  128/  306]
train() client id: f_00004-1-4 loss: 0.894473  [  160/  306]
train() client id: f_00004-1-5 loss: 0.820918  [  192/  306]
train() client id: f_00004-1-6 loss: 0.922533  [  224/  306]
train() client id: f_00004-1-7 loss: 0.791101  [  256/  306]
train() client id: f_00004-1-8 loss: 0.978876  [  288/  306]
train() client id: f_00004-2-0 loss: 1.110279  [   32/  306]
train() client id: f_00004-2-1 loss: 0.927409  [   64/  306]
train() client id: f_00004-2-2 loss: 0.908653  [   96/  306]
train() client id: f_00004-2-3 loss: 1.056219  [  128/  306]
train() client id: f_00004-2-4 loss: 0.910130  [  160/  306]
train() client id: f_00004-2-5 loss: 0.934945  [  192/  306]
train() client id: f_00004-2-6 loss: 0.929407  [  224/  306]
train() client id: f_00004-2-7 loss: 0.872396  [  256/  306]
train() client id: f_00004-2-8 loss: 0.864914  [  288/  306]
train() client id: f_00004-3-0 loss: 1.008565  [   32/  306]
train() client id: f_00004-3-1 loss: 0.925501  [   64/  306]
train() client id: f_00004-3-2 loss: 0.919129  [   96/  306]
train() client id: f_00004-3-3 loss: 0.877930  [  128/  306]
train() client id: f_00004-3-4 loss: 0.972180  [  160/  306]
train() client id: f_00004-3-5 loss: 0.962897  [  192/  306]
train() client id: f_00004-3-6 loss: 0.938411  [  224/  306]
train() client id: f_00004-3-7 loss: 0.885267  [  256/  306]
train() client id: f_00004-3-8 loss: 0.883309  [  288/  306]
train() client id: f_00004-4-0 loss: 0.858207  [   32/  306]
train() client id: f_00004-4-1 loss: 0.896205  [   64/  306]
train() client id: f_00004-4-2 loss: 0.976976  [   96/  306]
train() client id: f_00004-4-3 loss: 0.881840  [  128/  306]
train() client id: f_00004-4-4 loss: 0.743155  [  160/  306]
train() client id: f_00004-4-5 loss: 0.827453  [  192/  306]
train() client id: f_00004-4-6 loss: 1.046254  [  224/  306]
train() client id: f_00004-4-7 loss: 1.035616  [  256/  306]
train() client id: f_00004-4-8 loss: 0.978381  [  288/  306]
train() client id: f_00004-5-0 loss: 0.935698  [   32/  306]
train() client id: f_00004-5-1 loss: 0.980261  [   64/  306]
train() client id: f_00004-5-2 loss: 0.907445  [   96/  306]
train() client id: f_00004-5-3 loss: 0.853320  [  128/  306]
train() client id: f_00004-5-4 loss: 0.920037  [  160/  306]
train() client id: f_00004-5-5 loss: 0.999584  [  192/  306]
train() client id: f_00004-5-6 loss: 0.945720  [  224/  306]
train() client id: f_00004-5-7 loss: 0.818616  [  256/  306]
train() client id: f_00004-5-8 loss: 0.933311  [  288/  306]
train() client id: f_00004-6-0 loss: 0.891192  [   32/  306]
train() client id: f_00004-6-1 loss: 0.971785  [   64/  306]
train() client id: f_00004-6-2 loss: 0.830868  [   96/  306]
train() client id: f_00004-6-3 loss: 1.006491  [  128/  306]
train() client id: f_00004-6-4 loss: 0.919218  [  160/  306]
train() client id: f_00004-6-5 loss: 0.926347  [  192/  306]
train() client id: f_00004-6-6 loss: 1.065969  [  224/  306]
train() client id: f_00004-6-7 loss: 0.851012  [  256/  306]
train() client id: f_00004-6-8 loss: 0.866994  [  288/  306]
train() client id: f_00004-7-0 loss: 0.907841  [   32/  306]
train() client id: f_00004-7-1 loss: 0.813885  [   64/  306]
train() client id: f_00004-7-2 loss: 0.878716  [   96/  306]
train() client id: f_00004-7-3 loss: 0.902776  [  128/  306]
train() client id: f_00004-7-4 loss: 0.805635  [  160/  306]
train() client id: f_00004-7-5 loss: 0.875605  [  192/  306]
train() client id: f_00004-7-6 loss: 1.069795  [  224/  306]
train() client id: f_00004-7-7 loss: 0.968627  [  256/  306]
train() client id: f_00004-7-8 loss: 1.002081  [  288/  306]
train() client id: f_00004-8-0 loss: 0.906032  [   32/  306]
train() client id: f_00004-8-1 loss: 0.961602  [   64/  306]
train() client id: f_00004-8-2 loss: 0.813360  [   96/  306]
train() client id: f_00004-8-3 loss: 0.778899  [  128/  306]
train() client id: f_00004-8-4 loss: 0.907763  [  160/  306]
train() client id: f_00004-8-5 loss: 0.882539  [  192/  306]
train() client id: f_00004-8-6 loss: 0.879012  [  224/  306]
train() client id: f_00004-8-7 loss: 1.096607  [  256/  306]
train() client id: f_00004-8-8 loss: 0.993334  [  288/  306]
train() client id: f_00004-9-0 loss: 0.975332  [   32/  306]
train() client id: f_00004-9-1 loss: 0.898037  [   64/  306]
train() client id: f_00004-9-2 loss: 0.874998  [   96/  306]
train() client id: f_00004-9-3 loss: 0.940636  [  128/  306]
train() client id: f_00004-9-4 loss: 0.951433  [  160/  306]
train() client id: f_00004-9-5 loss: 0.831966  [  192/  306]
train() client id: f_00004-9-6 loss: 1.026122  [  224/  306]
train() client id: f_00004-9-7 loss: 0.877011  [  256/  306]
train() client id: f_00004-9-8 loss: 0.807975  [  288/  306]
train() client id: f_00004-10-0 loss: 0.965147  [   32/  306]
train() client id: f_00004-10-1 loss: 0.827817  [   64/  306]
train() client id: f_00004-10-2 loss: 0.911404  [   96/  306]
train() client id: f_00004-10-3 loss: 0.905622  [  128/  306]
train() client id: f_00004-10-4 loss: 0.981661  [  160/  306]
train() client id: f_00004-10-5 loss: 0.968967  [  192/  306]
train() client id: f_00004-10-6 loss: 0.920704  [  224/  306]
train() client id: f_00004-10-7 loss: 0.801433  [  256/  306]
train() client id: f_00004-10-8 loss: 0.948688  [  288/  306]
train() client id: f_00004-11-0 loss: 0.971554  [   32/  306]
train() client id: f_00004-11-1 loss: 0.898310  [   64/  306]
train() client id: f_00004-11-2 loss: 0.938174  [   96/  306]
train() client id: f_00004-11-3 loss: 0.986769  [  128/  306]
train() client id: f_00004-11-4 loss: 0.877173  [  160/  306]
train() client id: f_00004-11-5 loss: 0.807078  [  192/  306]
train() client id: f_00004-11-6 loss: 0.893673  [  224/  306]
train() client id: f_00004-11-7 loss: 0.965289  [  256/  306]
train() client id: f_00004-11-8 loss: 0.859763  [  288/  306]
train() client id: f_00004-12-0 loss: 0.916958  [   32/  306]
train() client id: f_00004-12-1 loss: 0.971220  [   64/  306]
train() client id: f_00004-12-2 loss: 0.986662  [   96/  306]
train() client id: f_00004-12-3 loss: 0.865345  [  128/  306]
train() client id: f_00004-12-4 loss: 0.931541  [  160/  306]
train() client id: f_00004-12-5 loss: 0.943739  [  192/  306]
train() client id: f_00004-12-6 loss: 0.871058  [  224/  306]
train() client id: f_00004-12-7 loss: 0.885415  [  256/  306]
train() client id: f_00004-12-8 loss: 0.840405  [  288/  306]
train() client id: f_00005-0-0 loss: 0.314845  [   32/  146]
train() client id: f_00005-0-1 loss: 0.705969  [   64/  146]
train() client id: f_00005-0-2 loss: 0.184471  [   96/  146]
train() client id: f_00005-0-3 loss: 0.658494  [  128/  146]
train() client id: f_00005-1-0 loss: 0.520657  [   32/  146]
train() client id: f_00005-1-1 loss: 0.536016  [   64/  146]
train() client id: f_00005-1-2 loss: 0.605544  [   96/  146]
train() client id: f_00005-1-3 loss: 0.171404  [  128/  146]
train() client id: f_00005-2-0 loss: 0.367638  [   32/  146]
train() client id: f_00005-2-1 loss: 0.427000  [   64/  146]
train() client id: f_00005-2-2 loss: 0.615141  [   96/  146]
train() client id: f_00005-2-3 loss: 0.371011  [  128/  146]
train() client id: f_00005-3-0 loss: 0.535970  [   32/  146]
train() client id: f_00005-3-1 loss: 0.514293  [   64/  146]
train() client id: f_00005-3-2 loss: 0.617169  [   96/  146]
train() client id: f_00005-3-3 loss: 0.166860  [  128/  146]
train() client id: f_00005-4-0 loss: 0.767055  [   32/  146]
train() client id: f_00005-4-1 loss: 0.520200  [   64/  146]
train() client id: f_00005-4-2 loss: 0.301824  [   96/  146]
train() client id: f_00005-4-3 loss: 0.209247  [  128/  146]
train() client id: f_00005-5-0 loss: 0.295732  [   32/  146]
train() client id: f_00005-5-1 loss: 0.522118  [   64/  146]
train() client id: f_00005-5-2 loss: 0.467218  [   96/  146]
train() client id: f_00005-5-3 loss: 0.458365  [  128/  146]
train() client id: f_00005-6-0 loss: 0.498441  [   32/  146]
train() client id: f_00005-6-1 loss: 0.436132  [   64/  146]
train() client id: f_00005-6-2 loss: 0.529133  [   96/  146]
train() client id: f_00005-6-3 loss: 0.418687  [  128/  146]
train() client id: f_00005-7-0 loss: 0.415514  [   32/  146]
train() client id: f_00005-7-1 loss: 0.511910  [   64/  146]
train() client id: f_00005-7-2 loss: 0.615149  [   96/  146]
train() client id: f_00005-7-3 loss: 0.306681  [  128/  146]
train() client id: f_00005-8-0 loss: 0.265856  [   32/  146]
train() client id: f_00005-8-1 loss: 0.526269  [   64/  146]
train() client id: f_00005-8-2 loss: 0.745187  [   96/  146]
train() client id: f_00005-8-3 loss: 0.293485  [  128/  146]
train() client id: f_00005-9-0 loss: 0.481602  [   32/  146]
train() client id: f_00005-9-1 loss: 0.187296  [   64/  146]
train() client id: f_00005-9-2 loss: 0.418461  [   96/  146]
train() client id: f_00005-9-3 loss: 0.367089  [  128/  146]
train() client id: f_00005-10-0 loss: 0.472695  [   32/  146]
train() client id: f_00005-10-1 loss: 0.271326  [   64/  146]
train() client id: f_00005-10-2 loss: 0.398865  [   96/  146]
train() client id: f_00005-10-3 loss: 0.587195  [  128/  146]
train() client id: f_00005-11-0 loss: 0.469574  [   32/  146]
train() client id: f_00005-11-1 loss: 0.398099  [   64/  146]
train() client id: f_00005-11-2 loss: 0.427026  [   96/  146]
train() client id: f_00005-11-3 loss: 0.143624  [  128/  146]
train() client id: f_00005-12-0 loss: 0.460601  [   32/  146]
train() client id: f_00005-12-1 loss: 0.195404  [   64/  146]
train() client id: f_00005-12-2 loss: 0.816691  [   96/  146]
train() client id: f_00005-12-3 loss: 0.241712  [  128/  146]
train() client id: f_00006-0-0 loss: 0.573510  [   32/   54]
train() client id: f_00006-1-0 loss: 0.517374  [   32/   54]
train() client id: f_00006-2-0 loss: 0.584338  [   32/   54]
train() client id: f_00006-3-0 loss: 0.534435  [   32/   54]
train() client id: f_00006-4-0 loss: 0.582922  [   32/   54]
train() client id: f_00006-5-0 loss: 0.480659  [   32/   54]
train() client id: f_00006-6-0 loss: 0.582078  [   32/   54]
train() client id: f_00006-7-0 loss: 0.517601  [   32/   54]
train() client id: f_00006-8-0 loss: 0.534095  [   32/   54]
train() client id: f_00006-9-0 loss: 0.571667  [   32/   54]
train() client id: f_00006-10-0 loss: 0.585240  [   32/   54]
train() client id: f_00006-11-0 loss: 0.560577  [   32/   54]
train() client id: f_00006-12-0 loss: 0.532492  [   32/   54]
train() client id: f_00007-0-0 loss: 0.344749  [   32/  179]
train() client id: f_00007-0-1 loss: 0.438652  [   64/  179]
train() client id: f_00007-0-2 loss: 0.606299  [   96/  179]
train() client id: f_00007-0-3 loss: 0.654588  [  128/  179]
train() client id: f_00007-0-4 loss: 0.415357  [  160/  179]
train() client id: f_00007-1-0 loss: 0.511370  [   32/  179]
train() client id: f_00007-1-1 loss: 0.550974  [   64/  179]
train() client id: f_00007-1-2 loss: 0.391771  [   96/  179]
train() client id: f_00007-1-3 loss: 0.349218  [  128/  179]
train() client id: f_00007-1-4 loss: 0.595559  [  160/  179]
train() client id: f_00007-2-0 loss: 0.593034  [   32/  179]
train() client id: f_00007-2-1 loss: 0.406415  [   64/  179]
train() client id: f_00007-2-2 loss: 0.542980  [   96/  179]
train() client id: f_00007-2-3 loss: 0.451766  [  128/  179]
train() client id: f_00007-2-4 loss: 0.293357  [  160/  179]
train() client id: f_00007-3-0 loss: 0.343255  [   32/  179]
train() client id: f_00007-3-1 loss: 0.613407  [   64/  179]
train() client id: f_00007-3-2 loss: 0.446203  [   96/  179]
train() client id: f_00007-3-3 loss: 0.453260  [  128/  179]
train() client id: f_00007-3-4 loss: 0.305163  [  160/  179]
train() client id: f_00007-4-0 loss: 0.315046  [   32/  179]
train() client id: f_00007-4-1 loss: 0.385745  [   64/  179]
train() client id: f_00007-4-2 loss: 0.433834  [   96/  179]
train() client id: f_00007-4-3 loss: 0.409156  [  128/  179]
train() client id: f_00007-4-4 loss: 0.659405  [  160/  179]
train() client id: f_00007-5-0 loss: 0.406779  [   32/  179]
train() client id: f_00007-5-1 loss: 0.578159  [   64/  179]
train() client id: f_00007-5-2 loss: 0.489941  [   96/  179]
train() client id: f_00007-5-3 loss: 0.269029  [  128/  179]
train() client id: f_00007-5-4 loss: 0.498180  [  160/  179]
train() client id: f_00007-6-0 loss: 0.487370  [   32/  179]
train() client id: f_00007-6-1 loss: 0.443101  [   64/  179]
train() client id: f_00007-6-2 loss: 0.517466  [   96/  179]
train() client id: f_00007-6-3 loss: 0.342233  [  128/  179]
train() client id: f_00007-6-4 loss: 0.283458  [  160/  179]
train() client id: f_00007-7-0 loss: 0.581543  [   32/  179]
train() client id: f_00007-7-1 loss: 0.551458  [   64/  179]
train() client id: f_00007-7-2 loss: 0.383321  [   96/  179]
train() client id: f_00007-7-3 loss: 0.256634  [  128/  179]
train() client id: f_00007-7-4 loss: 0.444752  [  160/  179]
train() client id: f_00007-8-0 loss: 0.458570  [   32/  179]
train() client id: f_00007-8-1 loss: 0.328419  [   64/  179]
train() client id: f_00007-8-2 loss: 0.565651  [   96/  179]
train() client id: f_00007-8-3 loss: 0.440594  [  128/  179]
train() client id: f_00007-8-4 loss: 0.394846  [  160/  179]
train() client id: f_00007-9-0 loss: 0.470578  [   32/  179]
train() client id: f_00007-9-1 loss: 0.481037  [   64/  179]
train() client id: f_00007-9-2 loss: 0.345427  [   96/  179]
train() client id: f_00007-9-3 loss: 0.344980  [  128/  179]
train() client id: f_00007-9-4 loss: 0.476203  [  160/  179]
train() client id: f_00007-10-0 loss: 0.289844  [   32/  179]
train() client id: f_00007-10-1 loss: 0.269306  [   64/  179]
train() client id: f_00007-10-2 loss: 0.377994  [   96/  179]
train() client id: f_00007-10-3 loss: 0.446314  [  128/  179]
train() client id: f_00007-10-4 loss: 0.547628  [  160/  179]
train() client id: f_00007-11-0 loss: 0.350516  [   32/  179]
train() client id: f_00007-11-1 loss: 0.707724  [   64/  179]
train() client id: f_00007-11-2 loss: 0.569769  [   96/  179]
train() client id: f_00007-11-3 loss: 0.288239  [  128/  179]
train() client id: f_00007-11-4 loss: 0.283617  [  160/  179]
train() client id: f_00007-12-0 loss: 0.613442  [   32/  179]
train() client id: f_00007-12-1 loss: 0.572162  [   64/  179]
train() client id: f_00007-12-2 loss: 0.298349  [   96/  179]
train() client id: f_00007-12-3 loss: 0.248395  [  128/  179]
train() client id: f_00007-12-4 loss: 0.454242  [  160/  179]
train() client id: f_00008-0-0 loss: 0.784695  [   32/  130]
train() client id: f_00008-0-1 loss: 0.630668  [   64/  130]
train() client id: f_00008-0-2 loss: 0.827441  [   96/  130]
train() client id: f_00008-0-3 loss: 0.684904  [  128/  130]
train() client id: f_00008-1-0 loss: 0.742424  [   32/  130]
train() client id: f_00008-1-1 loss: 0.792121  [   64/  130]
train() client id: f_00008-1-2 loss: 0.746078  [   96/  130]
train() client id: f_00008-1-3 loss: 0.626082  [  128/  130]
train() client id: f_00008-2-0 loss: 0.687887  [   32/  130]
train() client id: f_00008-2-1 loss: 0.775789  [   64/  130]
train() client id: f_00008-2-2 loss: 0.558674  [   96/  130]
train() client id: f_00008-2-3 loss: 0.900473  [  128/  130]
train() client id: f_00008-3-0 loss: 0.854173  [   32/  130]
train() client id: f_00008-3-1 loss: 0.635933  [   64/  130]
train() client id: f_00008-3-2 loss: 0.797175  [   96/  130]
train() client id: f_00008-3-3 loss: 0.631155  [  128/  130]
train() client id: f_00008-4-0 loss: 0.766121  [   32/  130]
train() client id: f_00008-4-1 loss: 0.620342  [   64/  130]
train() client id: f_00008-4-2 loss: 0.740116  [   96/  130]
train() client id: f_00008-4-3 loss: 0.799082  [  128/  130]
train() client id: f_00008-5-0 loss: 0.706115  [   32/  130]
train() client id: f_00008-5-1 loss: 0.812674  [   64/  130]
train() client id: f_00008-5-2 loss: 0.702503  [   96/  130]
train() client id: f_00008-5-3 loss: 0.714236  [  128/  130]
train() client id: f_00008-6-0 loss: 0.735033  [   32/  130]
train() client id: f_00008-6-1 loss: 0.645084  [   64/  130]
train() client id: f_00008-6-2 loss: 0.696593  [   96/  130]
train() client id: f_00008-6-3 loss: 0.826911  [  128/  130]
train() client id: f_00008-7-0 loss: 0.722485  [   32/  130]
train() client id: f_00008-7-1 loss: 0.742136  [   64/  130]
train() client id: f_00008-7-2 loss: 0.780350  [   96/  130]
train() client id: f_00008-7-3 loss: 0.676855  [  128/  130]
train() client id: f_00008-8-0 loss: 0.726053  [   32/  130]
train() client id: f_00008-8-1 loss: 0.781245  [   64/  130]
train() client id: f_00008-8-2 loss: 0.788516  [   96/  130]
train() client id: f_00008-8-3 loss: 0.630539  [  128/  130]
train() client id: f_00008-9-0 loss: 0.703355  [   32/  130]
train() client id: f_00008-9-1 loss: 0.768201  [   64/  130]
train() client id: f_00008-9-2 loss: 0.732864  [   96/  130]
train() client id: f_00008-9-3 loss: 0.715948  [  128/  130]
train() client id: f_00008-10-0 loss: 0.754818  [   32/  130]
train() client id: f_00008-10-1 loss: 0.712314  [   64/  130]
train() client id: f_00008-10-2 loss: 0.697225  [   96/  130]
train() client id: f_00008-10-3 loss: 0.716066  [  128/  130]
train() client id: f_00008-11-0 loss: 0.740824  [   32/  130]
train() client id: f_00008-11-1 loss: 0.764707  [   64/  130]
train() client id: f_00008-11-2 loss: 0.716487  [   96/  130]
train() client id: f_00008-11-3 loss: 0.719327  [  128/  130]
train() client id: f_00008-12-0 loss: 0.747637  [   32/  130]
train() client id: f_00008-12-1 loss: 0.832152  [   64/  130]
train() client id: f_00008-12-2 loss: 0.651868  [   96/  130]
train() client id: f_00008-12-3 loss: 0.702435  [  128/  130]
train() client id: f_00009-0-0 loss: 1.097576  [   32/  118]
train() client id: f_00009-0-1 loss: 1.229488  [   64/  118]
train() client id: f_00009-0-2 loss: 1.202401  [   96/  118]
train() client id: f_00009-1-0 loss: 1.171420  [   32/  118]
train() client id: f_00009-1-1 loss: 1.176770  [   64/  118]
train() client id: f_00009-1-2 loss: 1.014548  [   96/  118]
train() client id: f_00009-2-0 loss: 1.141525  [   32/  118]
train() client id: f_00009-2-1 loss: 0.964223  [   64/  118]
train() client id: f_00009-2-2 loss: 1.138884  [   96/  118]
train() client id: f_00009-3-0 loss: 1.046380  [   32/  118]
train() client id: f_00009-3-1 loss: 1.073229  [   64/  118]
train() client id: f_00009-3-2 loss: 1.033802  [   96/  118]
train() client id: f_00009-4-0 loss: 1.215537  [   32/  118]
train() client id: f_00009-4-1 loss: 0.939334  [   64/  118]
train() client id: f_00009-4-2 loss: 0.996783  [   96/  118]
train() client id: f_00009-5-0 loss: 1.015635  [   32/  118]
train() client id: f_00009-5-1 loss: 1.027749  [   64/  118]
train() client id: f_00009-5-2 loss: 0.923024  [   96/  118]
train() client id: f_00009-6-0 loss: 0.899069  [   32/  118]
train() client id: f_00009-6-1 loss: 1.065600  [   64/  118]
train() client id: f_00009-6-2 loss: 0.956143  [   96/  118]
train() client id: f_00009-7-0 loss: 0.920817  [   32/  118]
train() client id: f_00009-7-1 loss: 0.878093  [   64/  118]
train() client id: f_00009-7-2 loss: 1.013301  [   96/  118]
train() client id: f_00009-8-0 loss: 0.852695  [   32/  118]
train() client id: f_00009-8-1 loss: 0.990137  [   64/  118]
train() client id: f_00009-8-2 loss: 1.024229  [   96/  118]
train() client id: f_00009-9-0 loss: 1.099468  [   32/  118]
train() client id: f_00009-9-1 loss: 0.872290  [   64/  118]
train() client id: f_00009-9-2 loss: 0.917865  [   96/  118]
train() client id: f_00009-10-0 loss: 0.888495  [   32/  118]
train() client id: f_00009-10-1 loss: 1.021317  [   64/  118]
train() client id: f_00009-10-2 loss: 0.861256  [   96/  118]
train() client id: f_00009-11-0 loss: 0.845138  [   32/  118]
train() client id: f_00009-11-1 loss: 0.753845  [   64/  118]
train() client id: f_00009-11-2 loss: 1.206991  [   96/  118]
train() client id: f_00009-12-0 loss: 0.868271  [   32/  118]
train() client id: f_00009-12-1 loss: 0.970234  [   64/  118]
train() client id: f_00009-12-2 loss: 0.879762  [   96/  118]
At round 42 accuracy: 0.649867374005305
At round 42 training accuracy: 0.5895372233400402
At round 42 training loss: 0.8296210325486488
update_location
xs = [  -3.9056584     4.20031788  230.00902392   18.81129433    0.97929623
    3.95640986 -192.44319194 -171.32485185  214.66397685 -157.06087855]
ys = [ 222.5879595   205.55583871    1.32061395 -192.45517586  184.35018685
  167.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [244.05051502 228.62818177 250.81047646 217.69901127 209.72827757
 195.3899751  216.8900013  198.37560615 237.46429822 186.23676175]
dists_bs = [177.58909768 181.7867245  440.67770376 415.43486141 176.22751969
 179.10062273 178.47941057 174.21642051 420.2648364  171.93692248]
uav_gains = [8.31942576e-12 1.09578607e-11 7.30282910e-12 1.30831832e-11
 1.47787344e-11 1.82276138e-11 1.32493055e-11 1.74589294e-11
 9.39389080e-12 2.07944683e-11]
bs_gains = [5.55812671e-11 5.20619013e-11 4.36269716e-12 5.14619051e-12
 5.67920630e-11 5.42777976e-11 5.48084274e-11 5.86468436e-11
 4.98229610e-12 6.08499840e-11]
Round 43
-------------------------------
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.31748257 10.95769314  5.24195078  1.89681827 12.63633198  6.07983027
  2.34717104  7.46063269  5.5099153   4.93011902]
obj_prev = 62.377945058813665
eta_min = 4.6045000649351174e-18	eta_max = 0.9345899157842099
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 14.450064093228786	eta = 0.9090909090909091
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 27.618160282034836	eta = 0.47564435026761315
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 21.01662000340652	eta = 0.6250492182285267
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.820837826510346	eta = 0.6627581547216592
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755289483233774	eta = 0.6649571961010281
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755074608659463	eta = 0.6649644287942615
eta = 0.6649644287942615
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.03366898 0.07081173 0.03313455 0.01149021 0.08176751 0.03901326
 0.01442956 0.04783132 0.03473783 0.03153126]
ene_total = [1.80568477 3.13015273 1.80324111 0.85888841 3.56508653 1.85089364
 0.97553732 2.28873162 1.93281417 1.54404431]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 0 obj = 3.765039909361509
eta = 0.6649644287942615
freqs = [28996330.2200803  56908313.47423983 28770753.14460626  9662542.53169571
 65582048.83208953 31323013.60893005 12127351.29718995 39733680.81777115
 31038012.71809572 25251084.61040274]
eta_min = 0.6649644287942639	eta_max = 0.6649644287943239
af = 0.006654514151181828	bf = 1.2587071102671177	zeta = 0.007319965566300011	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [1.65314891e-06 1.33922227e-05 1.60169383e-06 6.26479190e-08
 2.05374606e-05 2.23529729e-06 1.23931308e-07 4.40987202e-06
 1.95427881e-06 1.17407932e-06]
ene_total = [1.6724149  1.05878427 1.74248852 1.46497738 1.04145823 1.04824288
 1.45990451 1.35723249 1.98280693 1.02444478]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 1 obj = 3.765039909362207
eta = 0.6649644287943239
freqs = [28996330.22008007 56908313.47423852 28770753.14460608  9662542.53169558
 65582048.832088   31323013.60892932 12127351.29718979 39733680.81777052
 31038012.71809573 25251084.61040214]
Done!
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [6.88604689e-06 5.57841296e-05 6.67171527e-06 2.60954415e-07
 8.55469920e-05 9.31093491e-06 5.16225000e-07 1.83689353e-05
 8.14037707e-06 4.89052449e-06]
ene_total = [0.01130652 0.00719709 0.01177988 0.00989973 0.00710263 0.00709054
 0.00986565 0.00918541 0.01340494 0.00692637]
At round 43 energy consumption: 0.09375876535404022
At round 43 eta: 0.6649644287943239
At round 43 a_n: 13.453131436573733
At round 43 local rounds: 13.360704448743727
At round 43 global rounds: 40.15433760708037
gradient difference: 0.40808162093162537
train() client id: f_00000-0-0 loss: 1.453138  [   32/  126]
train() client id: f_00000-0-1 loss: 1.113637  [   64/  126]
train() client id: f_00000-0-2 loss: 1.390984  [   96/  126]
train() client id: f_00000-1-0 loss: 1.192586  [   32/  126]
train() client id: f_00000-1-1 loss: 1.194238  [   64/  126]
train() client id: f_00000-1-2 loss: 1.211969  [   96/  126]
train() client id: f_00000-2-0 loss: 1.126396  [   32/  126]
train() client id: f_00000-2-1 loss: 1.111762  [   64/  126]
train() client id: f_00000-2-2 loss: 1.211596  [   96/  126]
train() client id: f_00000-3-0 loss: 1.024151  [   32/  126]
train() client id: f_00000-3-1 loss: 1.073609  [   64/  126]
train() client id: f_00000-3-2 loss: 1.030123  [   96/  126]
train() client id: f_00000-4-0 loss: 1.051599  [   32/  126]
train() client id: f_00000-4-1 loss: 1.083921  [   64/  126]
train() client id: f_00000-4-2 loss: 1.034211  [   96/  126]
train() client id: f_00000-5-0 loss: 1.040494  [   32/  126]
train() client id: f_00000-5-1 loss: 1.033631  [   64/  126]
train() client id: f_00000-5-2 loss: 0.997544  [   96/  126]
train() client id: f_00000-6-0 loss: 0.964019  [   32/  126]
train() client id: f_00000-6-1 loss: 0.972262  [   64/  126]
train() client id: f_00000-6-2 loss: 0.965697  [   96/  126]
train() client id: f_00000-7-0 loss: 0.989969  [   32/  126]
train() client id: f_00000-7-1 loss: 0.912164  [   64/  126]
train() client id: f_00000-7-2 loss: 0.952829  [   96/  126]
train() client id: f_00000-8-0 loss: 0.927757  [   32/  126]
train() client id: f_00000-8-1 loss: 0.897469  [   64/  126]
train() client id: f_00000-8-2 loss: 1.062279  [   96/  126]
train() client id: f_00000-9-0 loss: 0.884431  [   32/  126]
train() client id: f_00000-9-1 loss: 0.940612  [   64/  126]
train() client id: f_00000-9-2 loss: 0.950924  [   96/  126]
train() client id: f_00000-10-0 loss: 0.910420  [   32/  126]
train() client id: f_00000-10-1 loss: 0.918865  [   64/  126]
train() client id: f_00000-10-2 loss: 0.858860  [   96/  126]
train() client id: f_00000-11-0 loss: 0.837496  [   32/  126]
train() client id: f_00000-11-1 loss: 1.000529  [   64/  126]
train() client id: f_00000-11-2 loss: 0.992030  [   96/  126]
train() client id: f_00000-12-0 loss: 0.913820  [   32/  126]
train() client id: f_00000-12-1 loss: 1.014418  [   64/  126]
train() client id: f_00000-12-2 loss: 0.838265  [   96/  126]
train() client id: f_00001-0-0 loss: 0.448368  [   32/  265]
train() client id: f_00001-0-1 loss: 0.389676  [   64/  265]
train() client id: f_00001-0-2 loss: 0.472773  [   96/  265]
train() client id: f_00001-0-3 loss: 0.376150  [  128/  265]
train() client id: f_00001-0-4 loss: 0.475236  [  160/  265]
train() client id: f_00001-0-5 loss: 0.442989  [  192/  265]
train() client id: f_00001-0-6 loss: 0.438861  [  224/  265]
train() client id: f_00001-0-7 loss: 0.359407  [  256/  265]
train() client id: f_00001-1-0 loss: 0.385759  [   32/  265]
train() client id: f_00001-1-1 loss: 0.352613  [   64/  265]
train() client id: f_00001-1-2 loss: 0.500329  [   96/  265]
train() client id: f_00001-1-3 loss: 0.398026  [  128/  265]
train() client id: f_00001-1-4 loss: 0.491607  [  160/  265]
train() client id: f_00001-1-5 loss: 0.340190  [  192/  265]
train() client id: f_00001-1-6 loss: 0.429292  [  224/  265]
train() client id: f_00001-1-7 loss: 0.468876  [  256/  265]
train() client id: f_00001-2-0 loss: 0.338855  [   32/  265]
train() client id: f_00001-2-1 loss: 0.398313  [   64/  265]
train() client id: f_00001-2-2 loss: 0.324524  [   96/  265]
train() client id: f_00001-2-3 loss: 0.452448  [  128/  265]
train() client id: f_00001-2-4 loss: 0.372268  [  160/  265]
train() client id: f_00001-2-5 loss: 0.474594  [  192/  265]
train() client id: f_00001-2-6 loss: 0.492429  [  224/  265]
train() client id: f_00001-2-7 loss: 0.471886  [  256/  265]
train() client id: f_00001-3-0 loss: 0.471995  [   32/  265]
train() client id: f_00001-3-1 loss: 0.341164  [   64/  265]
train() client id: f_00001-3-2 loss: 0.370954  [   96/  265]
train() client id: f_00001-3-3 loss: 0.411754  [  128/  265]
train() client id: f_00001-3-4 loss: 0.372914  [  160/  265]
train() client id: f_00001-3-5 loss: 0.463011  [  192/  265]
train() client id: f_00001-3-6 loss: 0.455295  [  224/  265]
train() client id: f_00001-3-7 loss: 0.364847  [  256/  265]
train() client id: f_00001-4-0 loss: 0.356479  [   32/  265]
train() client id: f_00001-4-1 loss: 0.462390  [   64/  265]
train() client id: f_00001-4-2 loss: 0.407044  [   96/  265]
train() client id: f_00001-4-3 loss: 0.443879  [  128/  265]
train() client id: f_00001-4-4 loss: 0.430023  [  160/  265]
train() client id: f_00001-4-5 loss: 0.337555  [  192/  265]
train() client id: f_00001-4-6 loss: 0.405585  [  224/  265]
train() client id: f_00001-4-7 loss: 0.388378  [  256/  265]
train() client id: f_00001-5-0 loss: 0.502083  [   32/  265]
train() client id: f_00001-5-1 loss: 0.456465  [   64/  265]
train() client id: f_00001-5-2 loss: 0.490691  [   96/  265]
train() client id: f_00001-5-3 loss: 0.362812  [  128/  265]
train() client id: f_00001-5-4 loss: 0.355725  [  160/  265]
train() client id: f_00001-5-5 loss: 0.297017  [  192/  265]
train() client id: f_00001-5-6 loss: 0.334941  [  224/  265]
train() client id: f_00001-5-7 loss: 0.373228  [  256/  265]
train() client id: f_00001-6-0 loss: 0.404691  [   32/  265]
train() client id: f_00001-6-1 loss: 0.285623  [   64/  265]
train() client id: f_00001-6-2 loss: 0.390727  [   96/  265]
train() client id: f_00001-6-3 loss: 0.449075  [  128/  265]
train() client id: f_00001-6-4 loss: 0.401193  [  160/  265]
train() client id: f_00001-6-5 loss: 0.332396  [  192/  265]
train() client id: f_00001-6-6 loss: 0.418562  [  224/  265]
train() client id: f_00001-6-7 loss: 0.540452  [  256/  265]
train() client id: f_00001-7-0 loss: 0.347772  [   32/  265]
train() client id: f_00001-7-1 loss: 0.440774  [   64/  265]
train() client id: f_00001-7-2 loss: 0.377400  [   96/  265]
train() client id: f_00001-7-3 loss: 0.306033  [  128/  265]
train() client id: f_00001-7-4 loss: 0.483739  [  160/  265]
train() client id: f_00001-7-5 loss: 0.428910  [  192/  265]
train() client id: f_00001-7-6 loss: 0.425196  [  224/  265]
train() client id: f_00001-7-7 loss: 0.381017  [  256/  265]
train() client id: f_00001-8-0 loss: 0.486293  [   32/  265]
train() client id: f_00001-8-1 loss: 0.320863  [   64/  265]
train() client id: f_00001-8-2 loss: 0.380978  [   96/  265]
train() client id: f_00001-8-3 loss: 0.348054  [  128/  265]
train() client id: f_00001-8-4 loss: 0.356352  [  160/  265]
train() client id: f_00001-8-5 loss: 0.508738  [  192/  265]
train() client id: f_00001-8-6 loss: 0.371070  [  224/  265]
train() client id: f_00001-8-7 loss: 0.343698  [  256/  265]
train() client id: f_00001-9-0 loss: 0.454091  [   32/  265]
train() client id: f_00001-9-1 loss: 0.306160  [   64/  265]
train() client id: f_00001-9-2 loss: 0.377279  [   96/  265]
train() client id: f_00001-9-3 loss: 0.455821  [  128/  265]
train() client id: f_00001-9-4 loss: 0.316605  [  160/  265]
train() client id: f_00001-9-5 loss: 0.510465  [  192/  265]
train() client id: f_00001-9-6 loss: 0.353822  [  224/  265]
train() client id: f_00001-9-7 loss: 0.402676  [  256/  265]
train() client id: f_00001-10-0 loss: 0.401381  [   32/  265]
train() client id: f_00001-10-1 loss: 0.295086  [   64/  265]
train() client id: f_00001-10-2 loss: 0.412439  [   96/  265]
train() client id: f_00001-10-3 loss: 0.477845  [  128/  265]
train() client id: f_00001-10-4 loss: 0.301927  [  160/  265]
train() client id: f_00001-10-5 loss: 0.507841  [  192/  265]
train() client id: f_00001-10-6 loss: 0.407692  [  224/  265]
train() client id: f_00001-10-7 loss: 0.361070  [  256/  265]
train() client id: f_00001-11-0 loss: 0.297055  [   32/  265]
train() client id: f_00001-11-1 loss: 0.384076  [   64/  265]
train() client id: f_00001-11-2 loss: 0.373588  [   96/  265]
train() client id: f_00001-11-3 loss: 0.437235  [  128/  265]
train() client id: f_00001-11-4 loss: 0.410272  [  160/  265]
train() client id: f_00001-11-5 loss: 0.344698  [  192/  265]
train() client id: f_00001-11-6 loss: 0.453969  [  224/  265]
train() client id: f_00001-11-7 loss: 0.441911  [  256/  265]
train() client id: f_00001-12-0 loss: 0.382186  [   32/  265]
train() client id: f_00001-12-1 loss: 0.354826  [   64/  265]
train() client id: f_00001-12-2 loss: 0.479464  [   96/  265]
train() client id: f_00001-12-3 loss: 0.293785  [  128/  265]
train() client id: f_00001-12-4 loss: 0.452421  [  160/  265]
train() client id: f_00001-12-5 loss: 0.295458  [  192/  265]
train() client id: f_00001-12-6 loss: 0.491450  [  224/  265]
train() client id: f_00001-12-7 loss: 0.330408  [  256/  265]
train() client id: f_00002-0-0 loss: 1.302642  [   32/  124]
train() client id: f_00002-0-1 loss: 1.344924  [   64/  124]
train() client id: f_00002-0-2 loss: 1.475657  [   96/  124]
train() client id: f_00002-1-0 loss: 1.297077  [   32/  124]
train() client id: f_00002-1-1 loss: 1.127318  [   64/  124]
train() client id: f_00002-1-2 loss: 1.393972  [   96/  124]
train() client id: f_00002-2-0 loss: 1.204567  [   32/  124]
train() client id: f_00002-2-1 loss: 1.454924  [   64/  124]
train() client id: f_00002-2-2 loss: 1.033349  [   96/  124]
train() client id: f_00002-3-0 loss: 1.106025  [   32/  124]
train() client id: f_00002-3-1 loss: 1.388598  [   64/  124]
train() client id: f_00002-3-2 loss: 1.195478  [   96/  124]
train() client id: f_00002-4-0 loss: 1.383852  [   32/  124]
train() client id: f_00002-4-1 loss: 1.061012  [   64/  124]
train() client id: f_00002-4-2 loss: 1.331266  [   96/  124]
train() client id: f_00002-5-0 loss: 1.123609  [   32/  124]
train() client id: f_00002-5-1 loss: 1.277246  [   64/  124]
train() client id: f_00002-5-2 loss: 1.176452  [   96/  124]
train() client id: f_00002-6-0 loss: 1.183739  [   32/  124]
train() client id: f_00002-6-1 loss: 1.243768  [   64/  124]
train() client id: f_00002-6-2 loss: 1.032500  [   96/  124]
train() client id: f_00002-7-0 loss: 1.110501  [   32/  124]
train() client id: f_00002-7-1 loss: 1.150182  [   64/  124]
train() client id: f_00002-7-2 loss: 1.123507  [   96/  124]
train() client id: f_00002-8-0 loss: 1.143815  [   32/  124]
train() client id: f_00002-8-1 loss: 1.045906  [   64/  124]
train() client id: f_00002-8-2 loss: 1.207426  [   96/  124]
train() client id: f_00002-9-0 loss: 1.080762  [   32/  124]
train() client id: f_00002-9-1 loss: 1.280625  [   64/  124]
train() client id: f_00002-9-2 loss: 1.049623  [   96/  124]
train() client id: f_00002-10-0 loss: 1.232685  [   32/  124]
train() client id: f_00002-10-1 loss: 1.018111  [   64/  124]
train() client id: f_00002-10-2 loss: 1.008560  [   96/  124]
train() client id: f_00002-11-0 loss: 1.124231  [   32/  124]
train() client id: f_00002-11-1 loss: 1.162996  [   64/  124]
train() client id: f_00002-11-2 loss: 0.905528  [   96/  124]
train() client id: f_00002-12-0 loss: 1.169333  [   32/  124]
train() client id: f_00002-12-1 loss: 0.972414  [   64/  124]
train() client id: f_00002-12-2 loss: 1.010872  [   96/  124]
train() client id: f_00003-0-0 loss: 0.639021  [   32/   43]
train() client id: f_00003-1-0 loss: 0.784915  [   32/   43]
train() client id: f_00003-2-0 loss: 0.689364  [   32/   43]
train() client id: f_00003-3-0 loss: 0.650402  [   32/   43]
train() client id: f_00003-4-0 loss: 0.693447  [   32/   43]
train() client id: f_00003-5-0 loss: 0.765274  [   32/   43]
train() client id: f_00003-6-0 loss: 0.709077  [   32/   43]
train() client id: f_00003-7-0 loss: 0.537500  [   32/   43]
train() client id: f_00003-8-0 loss: 0.635260  [   32/   43]
train() client id: f_00003-9-0 loss: 0.639363  [   32/   43]
train() client id: f_00003-10-0 loss: 0.582419  [   32/   43]
train() client id: f_00003-11-0 loss: 0.599372  [   32/   43]
train() client id: f_00003-12-0 loss: 0.572292  [   32/   43]
train() client id: f_00004-0-0 loss: 1.070132  [   32/  306]
train() client id: f_00004-0-1 loss: 1.005383  [   64/  306]
train() client id: f_00004-0-2 loss: 0.839864  [   96/  306]
train() client id: f_00004-0-3 loss: 0.892298  [  128/  306]
train() client id: f_00004-0-4 loss: 0.792898  [  160/  306]
train() client id: f_00004-0-5 loss: 0.966367  [  192/  306]
train() client id: f_00004-0-6 loss: 0.818278  [  224/  306]
train() client id: f_00004-0-7 loss: 0.940245  [  256/  306]
train() client id: f_00004-0-8 loss: 1.020877  [  288/  306]
train() client id: f_00004-1-0 loss: 0.925678  [   32/  306]
train() client id: f_00004-1-1 loss: 0.861164  [   64/  306]
train() client id: f_00004-1-2 loss: 0.961771  [   96/  306]
train() client id: f_00004-1-3 loss: 0.826679  [  128/  306]
train() client id: f_00004-1-4 loss: 1.004536  [  160/  306]
train() client id: f_00004-1-5 loss: 0.958816  [  192/  306]
train() client id: f_00004-1-6 loss: 0.915012  [  224/  306]
train() client id: f_00004-1-7 loss: 0.905053  [  256/  306]
train() client id: f_00004-1-8 loss: 0.993024  [  288/  306]
train() client id: f_00004-2-0 loss: 1.026475  [   32/  306]
train() client id: f_00004-2-1 loss: 0.935242  [   64/  306]
train() client id: f_00004-2-2 loss: 0.769755  [   96/  306]
train() client id: f_00004-2-3 loss: 0.941953  [  128/  306]
train() client id: f_00004-2-4 loss: 0.879997  [  160/  306]
train() client id: f_00004-2-5 loss: 0.999832  [  192/  306]
train() client id: f_00004-2-6 loss: 0.867481  [  224/  306]
train() client id: f_00004-2-7 loss: 0.929035  [  256/  306]
train() client id: f_00004-2-8 loss: 1.019908  [  288/  306]
train() client id: f_00004-3-0 loss: 0.872224  [   32/  306]
train() client id: f_00004-3-1 loss: 0.969724  [   64/  306]
train() client id: f_00004-3-2 loss: 0.959638  [   96/  306]
train() client id: f_00004-3-3 loss: 0.952501  [  128/  306]
train() client id: f_00004-3-4 loss: 0.925903  [  160/  306]
train() client id: f_00004-3-5 loss: 0.899926  [  192/  306]
train() client id: f_00004-3-6 loss: 0.830636  [  224/  306]
train() client id: f_00004-3-7 loss: 0.970697  [  256/  306]
train() client id: f_00004-3-8 loss: 1.034272  [  288/  306]
train() client id: f_00004-4-0 loss: 0.817307  [   32/  306]
train() client id: f_00004-4-1 loss: 0.822321  [   64/  306]
train() client id: f_00004-4-2 loss: 1.021994  [   96/  306]
train() client id: f_00004-4-3 loss: 0.890184  [  128/  306]
train() client id: f_00004-4-4 loss: 0.940013  [  160/  306]
train() client id: f_00004-4-5 loss: 0.907644  [  192/  306]
train() client id: f_00004-4-6 loss: 0.936315  [  224/  306]
train() client id: f_00004-4-7 loss: 1.083791  [  256/  306]
train() client id: f_00004-4-8 loss: 0.869338  [  288/  306]
train() client id: f_00004-5-0 loss: 0.855608  [   32/  306]
train() client id: f_00004-5-1 loss: 0.918059  [   64/  306]
train() client id: f_00004-5-2 loss: 1.016041  [   96/  306]
train() client id: f_00004-5-3 loss: 0.830556  [  128/  306]
train() client id: f_00004-5-4 loss: 0.914265  [  160/  306]
train() client id: f_00004-5-5 loss: 0.961236  [  192/  306]
train() client id: f_00004-5-6 loss: 1.019603  [  224/  306]
train() client id: f_00004-5-7 loss: 0.909009  [  256/  306]
train() client id: f_00004-5-8 loss: 0.984120  [  288/  306]
train() client id: f_00004-6-0 loss: 0.803558  [   32/  306]
train() client id: f_00004-6-1 loss: 0.926385  [   64/  306]
train() client id: f_00004-6-2 loss: 1.023526  [   96/  306]
train() client id: f_00004-6-3 loss: 0.852053  [  128/  306]
train() client id: f_00004-6-4 loss: 1.031117  [  160/  306]
train() client id: f_00004-6-5 loss: 0.883367  [  192/  306]
train() client id: f_00004-6-6 loss: 0.922155  [  224/  306]
train() client id: f_00004-6-7 loss: 0.921798  [  256/  306]
train() client id: f_00004-6-8 loss: 0.905223  [  288/  306]
train() client id: f_00004-7-0 loss: 0.917979  [   32/  306]
train() client id: f_00004-7-1 loss: 0.934175  [   64/  306]
train() client id: f_00004-7-2 loss: 0.901855  [   96/  306]
train() client id: f_00004-7-3 loss: 0.900380  [  128/  306]
train() client id: f_00004-7-4 loss: 0.948212  [  160/  306]
train() client id: f_00004-7-5 loss: 0.940105  [  192/  306]
train() client id: f_00004-7-6 loss: 0.922955  [  224/  306]
train() client id: f_00004-7-7 loss: 0.887180  [  256/  306]
train() client id: f_00004-7-8 loss: 0.981745  [  288/  306]
train() client id: f_00004-8-0 loss: 0.924016  [   32/  306]
train() client id: f_00004-8-1 loss: 0.810165  [   64/  306]
train() client id: f_00004-8-2 loss: 0.924650  [   96/  306]
train() client id: f_00004-8-3 loss: 0.896457  [  128/  306]
train() client id: f_00004-8-4 loss: 0.956740  [  160/  306]
train() client id: f_00004-8-5 loss: 0.914367  [  192/  306]
train() client id: f_00004-8-6 loss: 0.963804  [  224/  306]
train() client id: f_00004-8-7 loss: 0.924643  [  256/  306]
train() client id: f_00004-8-8 loss: 0.986826  [  288/  306]
train() client id: f_00004-9-0 loss: 1.009021  [   32/  306]
train() client id: f_00004-9-1 loss: 0.906597  [   64/  306]
train() client id: f_00004-9-2 loss: 0.877372  [   96/  306]
train() client id: f_00004-9-3 loss: 0.953661  [  128/  306]
train() client id: f_00004-9-4 loss: 1.006856  [  160/  306]
train() client id: f_00004-9-5 loss: 0.917055  [  192/  306]
train() client id: f_00004-9-6 loss: 0.875781  [  224/  306]
train() client id: f_00004-9-7 loss: 0.942554  [  256/  306]
train() client id: f_00004-9-8 loss: 0.847956  [  288/  306]
train() client id: f_00004-10-0 loss: 0.908874  [   32/  306]
train() client id: f_00004-10-1 loss: 0.810820  [   64/  306]
train() client id: f_00004-10-2 loss: 0.893873  [   96/  306]
train() client id: f_00004-10-3 loss: 1.007849  [  128/  306]
train() client id: f_00004-10-4 loss: 0.953100  [  160/  306]
train() client id: f_00004-10-5 loss: 0.844162  [  192/  306]
train() client id: f_00004-10-6 loss: 0.895385  [  224/  306]
train() client id: f_00004-10-7 loss: 1.026266  [  256/  306]
train() client id: f_00004-10-8 loss: 0.948987  [  288/  306]
train() client id: f_00004-11-0 loss: 0.871979  [   32/  306]
train() client id: f_00004-11-1 loss: 0.992689  [   64/  306]
train() client id: f_00004-11-2 loss: 0.818067  [   96/  306]
train() client id: f_00004-11-3 loss: 1.034976  [  128/  306]
train() client id: f_00004-11-4 loss: 0.873782  [  160/  306]
train() client id: f_00004-11-5 loss: 0.986590  [  192/  306]
train() client id: f_00004-11-6 loss: 0.869845  [  224/  306]
train() client id: f_00004-11-7 loss: 0.841357  [  256/  306]
train() client id: f_00004-11-8 loss: 0.940413  [  288/  306]
train() client id: f_00004-12-0 loss: 0.962136  [   32/  306]
train() client id: f_00004-12-1 loss: 0.851276  [   64/  306]
train() client id: f_00004-12-2 loss: 0.869114  [   96/  306]
train() client id: f_00004-12-3 loss: 0.871356  [  128/  306]
train() client id: f_00004-12-4 loss: 0.965323  [  160/  306]
train() client id: f_00004-12-5 loss: 0.825014  [  192/  306]
train() client id: f_00004-12-6 loss: 0.903017  [  224/  306]
train() client id: f_00004-12-7 loss: 1.059289  [  256/  306]
train() client id: f_00004-12-8 loss: 0.993096  [  288/  306]
train() client id: f_00005-0-0 loss: 0.729223  [   32/  146]
train() client id: f_00005-0-1 loss: 0.825139  [   64/  146]
train() client id: f_00005-0-2 loss: 0.877082  [   96/  146]
train() client id: f_00005-0-3 loss: 0.654176  [  128/  146]
train() client id: f_00005-1-0 loss: 0.932404  [   32/  146]
train() client id: f_00005-1-1 loss: 0.596210  [   64/  146]
train() client id: f_00005-1-2 loss: 0.750176  [   96/  146]
train() client id: f_00005-1-3 loss: 1.008909  [  128/  146]
train() client id: f_00005-2-0 loss: 0.643892  [   32/  146]
train() client id: f_00005-2-1 loss: 0.925072  [   64/  146]
train() client id: f_00005-2-2 loss: 0.633709  [   96/  146]
train() client id: f_00005-2-3 loss: 0.835665  [  128/  146]
train() client id: f_00005-3-0 loss: 0.934470  [   32/  146]
train() client id: f_00005-3-1 loss: 0.719375  [   64/  146]
train() client id: f_00005-3-2 loss: 0.762700  [   96/  146]
train() client id: f_00005-3-3 loss: 0.773710  [  128/  146]
train() client id: f_00005-4-0 loss: 0.791067  [   32/  146]
train() client id: f_00005-4-1 loss: 0.860552  [   64/  146]
train() client id: f_00005-4-2 loss: 0.715114  [   96/  146]
train() client id: f_00005-4-3 loss: 0.793141  [  128/  146]
train() client id: f_00005-5-0 loss: 0.790984  [   32/  146]
train() client id: f_00005-5-1 loss: 0.838514  [   64/  146]
train() client id: f_00005-5-2 loss: 0.711050  [   96/  146]
train() client id: f_00005-5-3 loss: 0.869595  [  128/  146]
train() client id: f_00005-6-0 loss: 0.939973  [   32/  146]
train() client id: f_00005-6-1 loss: 0.727320  [   64/  146]
train() client id: f_00005-6-2 loss: 0.646971  [   96/  146]
train() client id: f_00005-6-3 loss: 0.737329  [  128/  146]
train() client id: f_00005-7-0 loss: 0.891387  [   32/  146]
train() client id: f_00005-7-1 loss: 0.712994  [   64/  146]
train() client id: f_00005-7-2 loss: 0.752553  [   96/  146]
train() client id: f_00005-7-3 loss: 0.817028  [  128/  146]
train() client id: f_00005-8-0 loss: 0.643853  [   32/  146]
train() client id: f_00005-8-1 loss: 0.592403  [   64/  146]
train() client id: f_00005-8-2 loss: 0.984140  [   96/  146]
train() client id: f_00005-8-3 loss: 0.867802  [  128/  146]
train() client id: f_00005-9-0 loss: 1.187760  [   32/  146]
train() client id: f_00005-9-1 loss: 0.804883  [   64/  146]
train() client id: f_00005-9-2 loss: 0.723350  [   96/  146]
train() client id: f_00005-9-3 loss: 0.581213  [  128/  146]
train() client id: f_00005-10-0 loss: 0.821016  [   32/  146]
train() client id: f_00005-10-1 loss: 0.834881  [   64/  146]
train() client id: f_00005-10-2 loss: 0.749848  [   96/  146]
train() client id: f_00005-10-3 loss: 0.845907  [  128/  146]
train() client id: f_00005-11-0 loss: 0.818523  [   32/  146]
train() client id: f_00005-11-1 loss: 0.760952  [   64/  146]
train() client id: f_00005-11-2 loss: 0.764232  [   96/  146]
train() client id: f_00005-11-3 loss: 0.825652  [  128/  146]
train() client id: f_00005-12-0 loss: 0.812699  [   32/  146]
train() client id: f_00005-12-1 loss: 0.798487  [   64/  146]
train() client id: f_00005-12-2 loss: 0.924306  [   96/  146]
train() client id: f_00005-12-3 loss: 0.634739  [  128/  146]
train() client id: f_00006-0-0 loss: 0.548598  [   32/   54]
train() client id: f_00006-1-0 loss: 0.474121  [   32/   54]
train() client id: f_00006-2-0 loss: 0.569852  [   32/   54]
train() client id: f_00006-3-0 loss: 0.514108  [   32/   54]
train() client id: f_00006-4-0 loss: 0.534018  [   32/   54]
train() client id: f_00006-5-0 loss: 0.545194  [   32/   54]
train() client id: f_00006-6-0 loss: 0.552460  [   32/   54]
train() client id: f_00006-7-0 loss: 0.527348  [   32/   54]
train() client id: f_00006-8-0 loss: 0.595742  [   32/   54]
train() client id: f_00006-9-0 loss: 0.486432  [   32/   54]
train() client id: f_00006-10-0 loss: 0.493115  [   32/   54]
train() client id: f_00006-11-0 loss: 0.544650  [   32/   54]
train() client id: f_00006-12-0 loss: 0.534315  [   32/   54]
train() client id: f_00007-0-0 loss: 0.456880  [   32/  179]
train() client id: f_00007-0-1 loss: 0.818517  [   64/  179]
train() client id: f_00007-0-2 loss: 0.561828  [   96/  179]
train() client id: f_00007-0-3 loss: 0.662763  [  128/  179]
train() client id: f_00007-0-4 loss: 0.408584  [  160/  179]
train() client id: f_00007-1-0 loss: 0.469012  [   32/  179]
train() client id: f_00007-1-1 loss: 0.514055  [   64/  179]
train() client id: f_00007-1-2 loss: 0.458391  [   96/  179]
train() client id: f_00007-1-3 loss: 0.629898  [  128/  179]
train() client id: f_00007-1-4 loss: 0.710686  [  160/  179]
train() client id: f_00007-2-0 loss: 0.588850  [   32/  179]
train() client id: f_00007-2-1 loss: 0.532628  [   64/  179]
train() client id: f_00007-2-2 loss: 0.494856  [   96/  179]
train() client id: f_00007-2-3 loss: 0.542429  [  128/  179]
train() client id: f_00007-2-4 loss: 0.549168  [  160/  179]
train() client id: f_00007-3-0 loss: 0.410387  [   32/  179]
train() client id: f_00007-3-1 loss: 0.427925  [   64/  179]
train() client id: f_00007-3-2 loss: 0.685446  [   96/  179]
train() client id: f_00007-3-3 loss: 0.433361  [  128/  179]
train() client id: f_00007-3-4 loss: 0.673382  [  160/  179]
train() client id: f_00007-4-0 loss: 0.678633  [   32/  179]
train() client id: f_00007-4-1 loss: 0.462873  [   64/  179]
train() client id: f_00007-4-2 loss: 0.524819  [   96/  179]
train() client id: f_00007-4-3 loss: 0.397665  [  128/  179]
train() client id: f_00007-4-4 loss: 0.631898  [  160/  179]
train() client id: f_00007-5-0 loss: 0.585842  [   32/  179]
train() client id: f_00007-5-1 loss: 0.582938  [   64/  179]
train() client id: f_00007-5-2 loss: 0.413005  [   96/  179]
train() client id: f_00007-5-3 loss: 0.541892  [  128/  179]
train() client id: f_00007-5-4 loss: 0.525005  [  160/  179]
train() client id: f_00007-6-0 loss: 0.652689  [   32/  179]
train() client id: f_00007-6-1 loss: 0.370758  [   64/  179]
train() client id: f_00007-6-2 loss: 0.415412  [   96/  179]
train() client id: f_00007-6-3 loss: 0.722161  [  128/  179]
train() client id: f_00007-6-4 loss: 0.474787  [  160/  179]
train() client id: f_00007-7-0 loss: 0.347971  [   32/  179]
train() client id: f_00007-7-1 loss: 0.742552  [   64/  179]
train() client id: f_00007-7-2 loss: 0.445759  [   96/  179]
train() client id: f_00007-7-3 loss: 0.355720  [  128/  179]
train() client id: f_00007-7-4 loss: 0.756709  [  160/  179]
train() client id: f_00007-8-0 loss: 0.468894  [   32/  179]
train() client id: f_00007-8-1 loss: 0.422005  [   64/  179]
train() client id: f_00007-8-2 loss: 0.619049  [   96/  179]
train() client id: f_00007-8-3 loss: 0.626274  [  128/  179]
train() client id: f_00007-8-4 loss: 0.491198  [  160/  179]
train() client id: f_00007-9-0 loss: 0.439201  [   32/  179]
train() client id: f_00007-9-1 loss: 0.365764  [   64/  179]
train() client id: f_00007-9-2 loss: 0.432410  [   96/  179]
train() client id: f_00007-9-3 loss: 0.754575  [  128/  179]
train() client id: f_00007-9-4 loss: 0.446977  [  160/  179]
train() client id: f_00007-10-0 loss: 0.382828  [   32/  179]
train() client id: f_00007-10-1 loss: 0.457983  [   64/  179]
train() client id: f_00007-10-2 loss: 0.615663  [   96/  179]
train() client id: f_00007-10-3 loss: 0.494510  [  128/  179]
train() client id: f_00007-10-4 loss: 0.629311  [  160/  179]
train() client id: f_00007-11-0 loss: 0.547283  [   32/  179]
train() client id: f_00007-11-1 loss: 0.519164  [   64/  179]
train() client id: f_00007-11-2 loss: 0.455738  [   96/  179]
train() client id: f_00007-11-3 loss: 0.376295  [  128/  179]
train() client id: f_00007-11-4 loss: 0.636302  [  160/  179]
train() client id: f_00007-12-0 loss: 0.366976  [   32/  179]
train() client id: f_00007-12-1 loss: 0.435573  [   64/  179]
train() client id: f_00007-12-2 loss: 0.522468  [   96/  179]
train() client id: f_00007-12-3 loss: 0.344539  [  128/  179]
train() client id: f_00007-12-4 loss: 0.474488  [  160/  179]
train() client id: f_00008-0-0 loss: 0.739019  [   32/  130]
train() client id: f_00008-0-1 loss: 0.709306  [   64/  130]
train() client id: f_00008-0-2 loss: 0.770485  [   96/  130]
train() client id: f_00008-0-3 loss: 0.689743  [  128/  130]
train() client id: f_00008-1-0 loss: 0.530015  [   32/  130]
train() client id: f_00008-1-1 loss: 0.672536  [   64/  130]
train() client id: f_00008-1-2 loss: 0.864334  [   96/  130]
train() client id: f_00008-1-3 loss: 0.802762  [  128/  130]
train() client id: f_00008-2-0 loss: 0.680488  [   32/  130]
train() client id: f_00008-2-1 loss: 0.727414  [   64/  130]
train() client id: f_00008-2-2 loss: 0.808628  [   96/  130]
train() client id: f_00008-2-3 loss: 0.688580  [  128/  130]
train() client id: f_00008-3-0 loss: 0.791093  [   32/  130]
train() client id: f_00008-3-1 loss: 0.705381  [   64/  130]
train() client id: f_00008-3-2 loss: 0.687343  [   96/  130]
train() client id: f_00008-3-3 loss: 0.692115  [  128/  130]
train() client id: f_00008-4-0 loss: 0.688124  [   32/  130]
train() client id: f_00008-4-1 loss: 0.756616  [   64/  130]
train() client id: f_00008-4-2 loss: 0.648041  [   96/  130]
train() client id: f_00008-4-3 loss: 0.789694  [  128/  130]
train() client id: f_00008-5-0 loss: 0.710749  [   32/  130]
train() client id: f_00008-5-1 loss: 0.659345  [   64/  130]
train() client id: f_00008-5-2 loss: 0.809827  [   96/  130]
train() client id: f_00008-5-3 loss: 0.730306  [  128/  130]
train() client id: f_00008-6-0 loss: 0.784391  [   32/  130]
train() client id: f_00008-6-1 loss: 0.804117  [   64/  130]
train() client id: f_00008-6-2 loss: 0.680512  [   96/  130]
train() client id: f_00008-6-3 loss: 0.599171  [  128/  130]
train() client id: f_00008-7-0 loss: 0.835457  [   32/  130]
train() client id: f_00008-7-1 loss: 0.675387  [   64/  130]
train() client id: f_00008-7-2 loss: 0.712077  [   96/  130]
train() client id: f_00008-7-3 loss: 0.651832  [  128/  130]
train() client id: f_00008-8-0 loss: 0.654839  [   32/  130]
train() client id: f_00008-8-1 loss: 0.714033  [   64/  130]
train() client id: f_00008-8-2 loss: 0.747005  [   96/  130]
train() client id: f_00008-8-3 loss: 0.780201  [  128/  130]
train() client id: f_00008-9-0 loss: 0.736648  [   32/  130]
train() client id: f_00008-9-1 loss: 0.586031  [   64/  130]
train() client id: f_00008-9-2 loss: 0.716673  [   96/  130]
train() client id: f_00008-9-3 loss: 0.828329  [  128/  130]
train() client id: f_00008-10-0 loss: 0.577123  [   32/  130]
train() client id: f_00008-10-1 loss: 0.788719  [   64/  130]
train() client id: f_00008-10-2 loss: 0.835711  [   96/  130]
train() client id: f_00008-10-3 loss: 0.709108  [  128/  130]
train() client id: f_00008-11-0 loss: 0.814270  [   32/  130]
train() client id: f_00008-11-1 loss: 0.656980  [   64/  130]
train() client id: f_00008-11-2 loss: 0.709094  [   96/  130]
train() client id: f_00008-11-3 loss: 0.740393  [  128/  130]
train() client id: f_00008-12-0 loss: 0.620418  [   32/  130]
train() client id: f_00008-12-1 loss: 0.704740  [   64/  130]
train() client id: f_00008-12-2 loss: 0.864801  [   96/  130]
train() client id: f_00008-12-3 loss: 0.733089  [  128/  130]
train() client id: f_00009-0-0 loss: 1.243273  [   32/  118]
train() client id: f_00009-0-1 loss: 1.020036  [   64/  118]
train() client id: f_00009-0-2 loss: 1.098753  [   96/  118]
train() client id: f_00009-1-0 loss: 0.958872  [   32/  118]
train() client id: f_00009-1-1 loss: 1.138193  [   64/  118]
train() client id: f_00009-1-2 loss: 1.025051  [   96/  118]
train() client id: f_00009-2-0 loss: 1.067716  [   32/  118]
train() client id: f_00009-2-1 loss: 1.098560  [   64/  118]
train() client id: f_00009-2-2 loss: 0.926795  [   96/  118]
train() client id: f_00009-3-0 loss: 1.074719  [   32/  118]
train() client id: f_00009-3-1 loss: 0.952392  [   64/  118]
train() client id: f_00009-3-2 loss: 0.918374  [   96/  118]
train() client id: f_00009-4-0 loss: 0.908258  [   32/  118]
train() client id: f_00009-4-1 loss: 0.948716  [   64/  118]
train() client id: f_00009-4-2 loss: 1.027349  [   96/  118]
train() client id: f_00009-5-0 loss: 0.936631  [   32/  118]
train() client id: f_00009-5-1 loss: 1.042232  [   64/  118]
train() client id: f_00009-5-2 loss: 0.852830  [   96/  118]
train() client id: f_00009-6-0 loss: 1.038403  [   32/  118]
train() client id: f_00009-6-1 loss: 0.812886  [   64/  118]
train() client id: f_00009-6-2 loss: 1.021417  [   96/  118]
train() client id: f_00009-7-0 loss: 0.938818  [   32/  118]
train() client id: f_00009-7-1 loss: 0.868859  [   64/  118]
train() client id: f_00009-7-2 loss: 0.876207  [   96/  118]
train() client id: f_00009-8-0 loss: 0.839863  [   32/  118]
train() client id: f_00009-8-1 loss: 0.963241  [   64/  118]
train() client id: f_00009-8-2 loss: 0.874997  [   96/  118]
train() client id: f_00009-9-0 loss: 0.915860  [   32/  118]
train() client id: f_00009-9-1 loss: 0.845009  [   64/  118]
train() client id: f_00009-9-2 loss: 0.992790  [   96/  118]
train() client id: f_00009-10-0 loss: 0.863883  [   32/  118]
train() client id: f_00009-10-1 loss: 0.925359  [   64/  118]
train() client id: f_00009-10-2 loss: 0.788785  [   96/  118]
train() client id: f_00009-11-0 loss: 0.896557  [   32/  118]
train() client id: f_00009-11-1 loss: 0.811356  [   64/  118]
train() client id: f_00009-11-2 loss: 0.942198  [   96/  118]
train() client id: f_00009-12-0 loss: 0.707565  [   32/  118]
train() client id: f_00009-12-1 loss: 0.973223  [   64/  118]
train() client id: f_00009-12-2 loss: 0.905030  [   96/  118]
At round 43 accuracy: 0.649867374005305
At round 43 training accuracy: 0.5835010060362174
At round 43 training loss: 0.8426593785436327
update_location
xs = [  -3.9056584     4.20031788  235.00902392   18.81129433    0.97929623
    3.95640986 -197.44319194 -176.32485185  219.66397685 -162.06087855]
ys = [ 227.5879595   210.55583871    1.32061395 -197.45517586  189.35018685
  172.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [248.61925403 233.13387545 255.40357347 222.13151795 214.1365272
 199.70073579 221.33843901 202.709471   241.99366252 190.47241326]
dists_bs = [178.99376305 182.69365504 445.27736169 419.86387774 176.56341796
 178.96975881 179.03723612 174.18269057 424.90489817 171.48735842]
uav_gains = [7.62245662e-12 1.01432215e-11 6.66462561e-12 1.21951248e-11
 1.38244935e-11 1.71271443e-11 1.23513336e-11 1.63936996e-11
 8.64631835e-12 1.95639647e-11]
bs_gains = [5.43685774e-11 5.13414801e-11 4.23768198e-12 4.99562983e-12
 5.64900616e-11 5.43889979e-11 5.43316212e-11 5.86786482e-11
 4.83144700e-12 6.12976995e-11]
Round 44
-------------------------------
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.18624546 10.67895549  5.11317553  1.85111052 12.31468286  5.9249859
  2.28997512  7.27266655  5.37158258  4.80445503]
obj_prev = 60.80783503027712
eta_min = 1.6905775058448652e-18	eta_max = 0.9354514077356543
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 14.082134182864616	eta = 0.9090909090909091
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 27.112785949573404	eta = 0.4721735416659381
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 20.558519671827195	eta = 0.622707294620243
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.371299283019138	eta = 0.6608715285021035
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305821021572196	eta = 0.6631129622477986
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305603441902093	eta = 0.6631204357204616
eta = 0.6631204357204616
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.03389812 0.07129367 0.03336005 0.01156841 0.08232401 0.03927877
 0.01452777 0.04815685 0.03497425 0.03174586]
ene_total = [1.77121153 3.05330088 1.77028457 0.8430904  3.47724339 1.80411205
 0.95686313 2.2367863  1.88825588 1.50445532]
ti_comp = [0.59818702 0.64270661 0.59304455 0.61337277 0.644077   0.64353969
 0.6137322  0.62114382 0.5788735  0.64520803]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.80350798e-06 5.48286200e-05 6.59759497e-06 2.57188538e-07
 8.40589437e-05 9.14540875e-06 5.08766715e-07 1.80913087e-05
 7.97917407e-06 4.80332220e-06]
ene_total = [0.45223865 0.28084755 0.47224396 0.39288439 0.2766519  0.27582751
 0.39149537 0.36333529 0.52744819 0.26916572]
optimize_network iter = 0 obj = 3.7021385292782734
eta = 0.6631204357204616
freqs = [28334048.79448884 55463616.087887   28126095.25989743  9430159.49291534
 63908512.8336163  30517755.77420575 11835590.3817379  38764656.20847187
 30208889.06833503 24601258.06337142]
eta_min = 0.6631204357204632	eta_max = 0.6660036163441293
af = 0.006161926359054621	bf = 1.2446435035998953	zeta = 0.006778118994960084	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57849488e-06 1.27208928e-05 1.53072061e-06 5.96708039e-08
 1.95026760e-05 2.12184375e-06 1.18039937e-07 4.19739907e-06
 1.85126341e-06 1.11442796e-06]
ene_total = [1.67509603 1.03465718 1.74925201 1.45587345 1.015872   1.02111426
 1.45069832 1.34439885 1.95366795 0.99690874]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 1 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
eta_min = 0.6660036163441323	eta_max = 0.6660036163441293
af = 0.006150348470560822	bf = 1.2446435035998953	zeta = 0.006765383317616905	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57740747e-06 1.26939645e-05 1.52994007e-06 5.95992217e-08
 1.94605963e-05 2.11729946e-06 1.17896944e-07 4.19130791e-06
 1.85126341e-06 1.11198606e-06]
ene_total = [1.67509587 1.0346533  1.74925189 1.45587344 1.01586593 1.0211136
 1.4506983  1.34439798 1.95366795 0.99690839]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 2 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
Done!
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.57055255e-06 5.28755963e-05 6.37283125e-06 2.48255333e-07
 8.10614081e-05 8.81942529e-06 4.91089385e-07 1.74585257e-05
 7.71127547e-06 4.63188047e-06]
ene_total = [0.01162015 0.00721449 0.01213419 0.01009525 0.00710564 0.00708713
 0.01005955 0.00933535 0.01355264 0.0069161 ]
At round 44 energy consumption: 0.09512048272349369
At round 44 eta: 0.6660036163441293
At round 44 a_n: 13.110585589604415
At round 44 local rounds: 13.309571309784454
At round 44 global rounds: 39.25367528264245
gradient difference: 0.4026576578617096
train() client id: f_00000-0-0 loss: 1.133481  [   32/  126]
train() client id: f_00000-0-1 loss: 1.022144  [   64/  126]
train() client id: f_00000-0-2 loss: 1.171372  [   96/  126]
train() client id: f_00000-1-0 loss: 1.007654  [   32/  126]
train() client id: f_00000-1-1 loss: 0.969525  [   64/  126]
train() client id: f_00000-1-2 loss: 1.096301  [   96/  126]
train() client id: f_00000-2-0 loss: 1.109753  [   32/  126]
train() client id: f_00000-2-1 loss: 1.009637  [   64/  126]
train() client id: f_00000-2-2 loss: 1.104730  [   96/  126]
train() client id: f_00000-3-0 loss: 0.957209  [   32/  126]
train() client id: f_00000-3-1 loss: 1.204021  [   64/  126]
train() client id: f_00000-3-2 loss: 0.847981  [   96/  126]
train() client id: f_00000-4-0 loss: 0.897741  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896269  [   64/  126]
train() client id: f_00000-4-2 loss: 0.872405  [   96/  126]
train() client id: f_00000-5-0 loss: 0.899204  [   32/  126]
train() client id: f_00000-5-1 loss: 0.864243  [   64/  126]
train() client id: f_00000-5-2 loss: 0.926714  [   96/  126]
train() client id: f_00000-6-0 loss: 0.984715  [   32/  126]
train() client id: f_00000-6-1 loss: 0.856954  [   64/  126]
train() client id: f_00000-6-2 loss: 0.858006  [   96/  126]
train() client id: f_00000-7-0 loss: 0.892902  [   32/  126]
train() client id: f_00000-7-1 loss: 0.965096  [   64/  126]
train() client id: f_00000-7-2 loss: 0.770895  [   96/  126]
train() client id: f_00000-8-0 loss: 0.927561  [   32/  126]
train() client id: f_00000-8-1 loss: 0.841385  [   64/  126]
train() client id: f_00000-8-2 loss: 0.855800  [   96/  126]
train() client id: f_00000-9-0 loss: 0.873915  [   32/  126]
train() client id: f_00000-9-1 loss: 0.836596  [   64/  126]
train() client id: f_00000-9-2 loss: 0.800886  [   96/  126]
train() client id: f_00000-10-0 loss: 1.020081  [   32/  126]
train() client id: f_00000-10-1 loss: 0.826145  [   64/  126]
train() client id: f_00000-10-2 loss: 0.752904  [   96/  126]
train() client id: f_00000-11-0 loss: 0.999008  [   32/  126]
train() client id: f_00000-11-1 loss: 0.723645  [   64/  126]
train() client id: f_00000-11-2 loss: 0.879564  [   96/  126]
train() client id: f_00000-12-0 loss: 0.757504  [   32/  126]
train() client id: f_00000-12-1 loss: 0.936646  [   64/  126]
train() client id: f_00000-12-2 loss: 0.860117  [   96/  126]
train() client id: f_00001-0-0 loss: 0.578857  [   32/  265]
train() client id: f_00001-0-1 loss: 0.420666  [   64/  265]
train() client id: f_00001-0-2 loss: 0.390054  [   96/  265]
train() client id: f_00001-0-3 loss: 0.405873  [  128/  265]
train() client id: f_00001-0-4 loss: 0.447157  [  160/  265]
train() client id: f_00001-0-5 loss: 0.508173  [  192/  265]
train() client id: f_00001-0-6 loss: 0.628048  [  224/  265]
train() client id: f_00001-0-7 loss: 0.530039  [  256/  265]
train() client id: f_00001-1-0 loss: 0.474602  [   32/  265]
train() client id: f_00001-1-1 loss: 0.508234  [   64/  265]
train() client id: f_00001-1-2 loss: 0.456818  [   96/  265]
train() client id: f_00001-1-3 loss: 0.539109  [  128/  265]
train() client id: f_00001-1-4 loss: 0.468843  [  160/  265]
train() client id: f_00001-1-5 loss: 0.420172  [  192/  265]
train() client id: f_00001-1-6 loss: 0.465905  [  224/  265]
train() client id: f_00001-1-7 loss: 0.478414  [  256/  265]
train() client id: f_00001-2-0 loss: 0.449682  [   32/  265]
train() client id: f_00001-2-1 loss: 0.500331  [   64/  265]
train() client id: f_00001-2-2 loss: 0.536275  [   96/  265]
train() client id: f_00001-2-3 loss: 0.385164  [  128/  265]
train() client id: f_00001-2-4 loss: 0.499797  [  160/  265]
train() client id: f_00001-2-5 loss: 0.592136  [  192/  265]
train() client id: f_00001-2-6 loss: 0.386755  [  224/  265]
train() client id: f_00001-2-7 loss: 0.361877  [  256/  265]
train() client id: f_00001-3-0 loss: 0.443566  [   32/  265]
train() client id: f_00001-3-1 loss: 0.505982  [   64/  265]
train() client id: f_00001-3-2 loss: 0.436567  [   96/  265]
train() client id: f_00001-3-3 loss: 0.585562  [  128/  265]
train() client id: f_00001-3-4 loss: 0.528967  [  160/  265]
train() client id: f_00001-3-5 loss: 0.410653  [  192/  265]
train() client id: f_00001-3-6 loss: 0.434651  [  224/  265]
train() client id: f_00001-3-7 loss: 0.417562  [  256/  265]
train() client id: f_00001-4-0 loss: 0.411126  [   32/  265]
train() client id: f_00001-4-1 loss: 0.473503  [   64/  265]
train() client id: f_00001-4-2 loss: 0.522276  [   96/  265]
train() client id: f_00001-4-3 loss: 0.499694  [  128/  265]
train() client id: f_00001-4-4 loss: 0.602347  [  160/  265]
train() client id: f_00001-4-5 loss: 0.404192  [  192/  265]
train() client id: f_00001-4-6 loss: 0.375520  [  224/  265]
train() client id: f_00001-4-7 loss: 0.379206  [  256/  265]
train() client id: f_00001-5-0 loss: 0.397189  [   32/  265]
train() client id: f_00001-5-1 loss: 0.545227  [   64/  265]
train() client id: f_00001-5-2 loss: 0.435358  [   96/  265]
train() client id: f_00001-5-3 loss: 0.464644  [  128/  265]
train() client id: f_00001-5-4 loss: 0.368251  [  160/  265]
train() client id: f_00001-5-5 loss: 0.461010  [  192/  265]
train() client id: f_00001-5-6 loss: 0.503839  [  224/  265]
train() client id: f_00001-5-7 loss: 0.485065  [  256/  265]
train() client id: f_00001-6-0 loss: 0.460274  [   32/  265]
train() client id: f_00001-6-1 loss: 0.498296  [   64/  265]
train() client id: f_00001-6-2 loss: 0.391749  [   96/  265]
train() client id: f_00001-6-3 loss: 0.484508  [  128/  265]
train() client id: f_00001-6-4 loss: 0.617490  [  160/  265]
train() client id: f_00001-6-5 loss: 0.515239  [  192/  265]
train() client id: f_00001-6-6 loss: 0.378518  [  224/  265]
train() client id: f_00001-6-7 loss: 0.365161  [  256/  265]
train() client id: f_00001-7-0 loss: 0.378020  [   32/  265]
train() client id: f_00001-7-1 loss: 0.415437  [   64/  265]
train() client id: f_00001-7-2 loss: 0.569743  [   96/  265]
train() client id: f_00001-7-3 loss: 0.416754  [  128/  265]
train() client id: f_00001-7-4 loss: 0.526932  [  160/  265]
train() client id: f_00001-7-5 loss: 0.576430  [  192/  265]
train() client id: f_00001-7-6 loss: 0.415127  [  224/  265]
train() client id: f_00001-7-7 loss: 0.407148  [  256/  265]
train() client id: f_00001-8-0 loss: 0.498949  [   32/  265]
train() client id: f_00001-8-1 loss: 0.469501  [   64/  265]
train() client id: f_00001-8-2 loss: 0.429175  [   96/  265]
train() client id: f_00001-8-3 loss: 0.371881  [  128/  265]
train() client id: f_00001-8-4 loss: 0.456262  [  160/  265]
train() client id: f_00001-8-5 loss: 0.514611  [  192/  265]
train() client id: f_00001-8-6 loss: 0.475674  [  224/  265]
train() client id: f_00001-8-7 loss: 0.439836  [  256/  265]
train() client id: f_00001-9-0 loss: 0.461727  [   32/  265]
train() client id: f_00001-9-1 loss: 0.454809  [   64/  265]
train() client id: f_00001-9-2 loss: 0.420791  [   96/  265]
train() client id: f_00001-9-3 loss: 0.536122  [  128/  265]
train() client id: f_00001-9-4 loss: 0.403220  [  160/  265]
train() client id: f_00001-9-5 loss: 0.411099  [  192/  265]
train() client id: f_00001-9-6 loss: 0.375461  [  224/  265]
train() client id: f_00001-9-7 loss: 0.571994  [  256/  265]
train() client id: f_00001-10-0 loss: 0.369098  [   32/  265]
train() client id: f_00001-10-1 loss: 0.566191  [   64/  265]
train() client id: f_00001-10-2 loss: 0.417155  [   96/  265]
train() client id: f_00001-10-3 loss: 0.418761  [  128/  265]
train() client id: f_00001-10-4 loss: 0.383572  [  160/  265]
train() client id: f_00001-10-5 loss: 0.531568  [  192/  265]
train() client id: f_00001-10-6 loss: 0.403361  [  224/  265]
train() client id: f_00001-10-7 loss: 0.540251  [  256/  265]
train() client id: f_00001-11-0 loss: 0.484886  [   32/  265]
train() client id: f_00001-11-1 loss: 0.456564  [   64/  265]
train() client id: f_00001-11-2 loss: 0.468794  [   96/  265]
train() client id: f_00001-11-3 loss: 0.524548  [  128/  265]
train() client id: f_00001-11-4 loss: 0.448579  [  160/  265]
train() client id: f_00001-11-5 loss: 0.357884  [  192/  265]
train() client id: f_00001-11-6 loss: 0.463711  [  224/  265]
train() client id: f_00001-11-7 loss: 0.416831  [  256/  265]
train() client id: f_00001-12-0 loss: 0.423626  [   32/  265]
train() client id: f_00001-12-1 loss: 0.619842  [   64/  265]
train() client id: f_00001-12-2 loss: 0.360984  [   96/  265]
train() client id: f_00001-12-3 loss: 0.374276  [  128/  265]
train() client id: f_00001-12-4 loss: 0.458753  [  160/  265]
train() client id: f_00001-12-5 loss: 0.503499  [  192/  265]
train() client id: f_00001-12-6 loss: 0.455314  [  224/  265]
train() client id: f_00001-12-7 loss: 0.464186  [  256/  265]
train() client id: f_00002-0-0 loss: 1.375470  [   32/  124]
train() client id: f_00002-0-1 loss: 1.324224  [   64/  124]
train() client id: f_00002-0-2 loss: 1.106805  [   96/  124]
train() client id: f_00002-1-0 loss: 1.323627  [   32/  124]
train() client id: f_00002-1-1 loss: 1.254887  [   64/  124]
train() client id: f_00002-1-2 loss: 1.292823  [   96/  124]
train() client id: f_00002-2-0 loss: 1.549902  [   32/  124]
train() client id: f_00002-2-1 loss: 1.119532  [   64/  124]
train() client id: f_00002-2-2 loss: 1.036899  [   96/  124]
train() client id: f_00002-3-0 loss: 1.056418  [   32/  124]
train() client id: f_00002-3-1 loss: 1.095430  [   64/  124]
train() client id: f_00002-3-2 loss: 1.281636  [   96/  124]
train() client id: f_00002-4-0 loss: 1.106668  [   32/  124]
train() client id: f_00002-4-1 loss: 1.187775  [   64/  124]
train() client id: f_00002-4-2 loss: 1.198256  [   96/  124]
train() client id: f_00002-5-0 loss: 1.203642  [   32/  124]
train() client id: f_00002-5-1 loss: 1.182370  [   64/  124]
train() client id: f_00002-5-2 loss: 1.001937  [   96/  124]
train() client id: f_00002-6-0 loss: 0.896685  [   32/  124]
train() client id: f_00002-6-1 loss: 1.293959  [   64/  124]
train() client id: f_00002-6-2 loss: 1.030433  [   96/  124]
train() client id: f_00002-7-0 loss: 1.100309  [   32/  124]
train() client id: f_00002-7-1 loss: 1.116674  [   64/  124]
train() client id: f_00002-7-2 loss: 1.129576  [   96/  124]
train() client id: f_00002-8-0 loss: 1.116994  [   32/  124]
train() client id: f_00002-8-1 loss: 1.219757  [   64/  124]
train() client id: f_00002-8-2 loss: 1.077834  [   96/  124]
train() client id: f_00002-9-0 loss: 0.946723  [   32/  124]
train() client id: f_00002-9-1 loss: 1.147469  [   64/  124]
train() client id: f_00002-9-2 loss: 0.923902  [   96/  124]
train() client id: f_00002-10-0 loss: 1.072087  [   32/  124]
train() client id: f_00002-10-1 loss: 1.036723  [   64/  124]
train() client id: f_00002-10-2 loss: 1.072650  [   96/  124]
train() client id: f_00002-11-0 loss: 1.109212  [   32/  124]
train() client id: f_00002-11-1 loss: 0.904051  [   64/  124]
train() client id: f_00002-11-2 loss: 1.171111  [   96/  124]
train() client id: f_00002-12-0 loss: 1.266746  [   32/  124]
train() client id: f_00002-12-1 loss: 0.959077  [   64/  124]
train() client id: f_00002-12-2 loss: 0.928528  [   96/  124]
train() client id: f_00003-0-0 loss: 0.655983  [   32/   43]
train() client id: f_00003-1-0 loss: 0.712615  [   32/   43]
train() client id: f_00003-2-0 loss: 0.775498  [   32/   43]
train() client id: f_00003-3-0 loss: 0.829721  [   32/   43]
train() client id: f_00003-4-0 loss: 0.709948  [   32/   43]
train() client id: f_00003-5-0 loss: 0.790498  [   32/   43]
train() client id: f_00003-6-0 loss: 0.758027  [   32/   43]
train() client id: f_00003-7-0 loss: 0.761768  [   32/   43]
train() client id: f_00003-8-0 loss: 0.846304  [   32/   43]
train() client id: f_00003-9-0 loss: 0.807947  [   32/   43]
train() client id: f_00003-10-0 loss: 0.920388  [   32/   43]
train() client id: f_00003-11-0 loss: 0.962498  [   32/   43]
train() client id: f_00003-12-0 loss: 0.852599  [   32/   43]
train() client id: f_00004-0-0 loss: 0.697410  [   32/  306]
train() client id: f_00004-0-1 loss: 0.683870  [   64/  306]
train() client id: f_00004-0-2 loss: 0.805628  [   96/  306]
train() client id: f_00004-0-3 loss: 0.711138  [  128/  306]
train() client id: f_00004-0-4 loss: 0.823092  [  160/  306]
train() client id: f_00004-0-5 loss: 0.795776  [  192/  306]
train() client id: f_00004-0-6 loss: 0.712534  [  224/  306]
train() client id: f_00004-0-7 loss: 0.703896  [  256/  306]
train() client id: f_00004-0-8 loss: 0.734153  [  288/  306]
train() client id: f_00004-1-0 loss: 0.697468  [   32/  306]
train() client id: f_00004-1-1 loss: 0.731603  [   64/  306]
train() client id: f_00004-1-2 loss: 0.821947  [   96/  306]
train() client id: f_00004-1-3 loss: 0.717055  [  128/  306]
train() client id: f_00004-1-4 loss: 0.779052  [  160/  306]
train() client id: f_00004-1-5 loss: 0.781406  [  192/  306]
train() client id: f_00004-1-6 loss: 0.723877  [  224/  306]
train() client id: f_00004-1-7 loss: 0.733144  [  256/  306]
train() client id: f_00004-1-8 loss: 0.674303  [  288/  306]
train() client id: f_00004-2-0 loss: 0.800641  [   32/  306]
train() client id: f_00004-2-1 loss: 0.685527  [   64/  306]
train() client id: f_00004-2-2 loss: 0.861809  [   96/  306]
train() client id: f_00004-2-3 loss: 0.771303  [  128/  306]
train() client id: f_00004-2-4 loss: 0.705369  [  160/  306]
train() client id: f_00004-2-5 loss: 0.826577  [  192/  306]
train() client id: f_00004-2-6 loss: 0.706724  [  224/  306]
train() client id: f_00004-2-7 loss: 0.595893  [  256/  306]
train() client id: f_00004-2-8 loss: 0.767644  [  288/  306]
train() client id: f_00004-3-0 loss: 0.716070  [   32/  306]
train() client id: f_00004-3-1 loss: 0.604744  [   64/  306]
train() client id: f_00004-3-2 loss: 0.766198  [   96/  306]
train() client id: f_00004-3-3 loss: 0.921144  [  128/  306]
train() client id: f_00004-3-4 loss: 0.760074  [  160/  306]
train() client id: f_00004-3-5 loss: 0.863847  [  192/  306]
train() client id: f_00004-3-6 loss: 0.744692  [  224/  306]
train() client id: f_00004-3-7 loss: 0.811102  [  256/  306]
train() client id: f_00004-3-8 loss: 0.653311  [  288/  306]
train() client id: f_00004-4-0 loss: 0.776439  [   32/  306]
train() client id: f_00004-4-1 loss: 0.775217  [   64/  306]
train() client id: f_00004-4-2 loss: 0.663459  [   96/  306]
train() client id: f_00004-4-3 loss: 0.656275  [  128/  306]
train() client id: f_00004-4-4 loss: 0.726585  [  160/  306]
train() client id: f_00004-4-5 loss: 0.717321  [  192/  306]
train() client id: f_00004-4-6 loss: 0.832563  [  224/  306]
train() client id: f_00004-4-7 loss: 0.792839  [  256/  306]
train() client id: f_00004-4-8 loss: 0.787937  [  288/  306]
train() client id: f_00004-5-0 loss: 0.747232  [   32/  306]
train() client id: f_00004-5-1 loss: 0.773628  [   64/  306]
train() client id: f_00004-5-2 loss: 0.815753  [   96/  306]
train() client id: f_00004-5-3 loss: 0.694168  [  128/  306]
train() client id: f_00004-5-4 loss: 0.667486  [  160/  306]
train() client id: f_00004-5-5 loss: 0.664780  [  192/  306]
train() client id: f_00004-5-6 loss: 0.819691  [  224/  306]
train() client id: f_00004-5-7 loss: 0.767265  [  256/  306]
train() client id: f_00004-5-8 loss: 0.854229  [  288/  306]
train() client id: f_00004-6-0 loss: 0.891692  [   32/  306]
train() client id: f_00004-6-1 loss: 0.797244  [   64/  306]
train() client id: f_00004-6-2 loss: 0.828506  [   96/  306]
train() client id: f_00004-6-3 loss: 0.768688  [  128/  306]
train() client id: f_00004-6-4 loss: 0.742942  [  160/  306]
train() client id: f_00004-6-5 loss: 0.656188  [  192/  306]
train() client id: f_00004-6-6 loss: 0.760232  [  224/  306]
train() client id: f_00004-6-7 loss: 0.661323  [  256/  306]
train() client id: f_00004-6-8 loss: 0.674935  [  288/  306]
train() client id: f_00004-7-0 loss: 0.729057  [   32/  306]
train() client id: f_00004-7-1 loss: 0.773876  [   64/  306]
train() client id: f_00004-7-2 loss: 0.655844  [   96/  306]
train() client id: f_00004-7-3 loss: 0.676755  [  128/  306]
train() client id: f_00004-7-4 loss: 0.802645  [  160/  306]
train() client id: f_00004-7-5 loss: 0.850111  [  192/  306]
train() client id: f_00004-7-6 loss: 0.812780  [  224/  306]
train() client id: f_00004-7-7 loss: 0.733743  [  256/  306]
train() client id: f_00004-7-8 loss: 0.786902  [  288/  306]
train() client id: f_00004-8-0 loss: 0.792656  [   32/  306]
train() client id: f_00004-8-1 loss: 0.762322  [   64/  306]
train() client id: f_00004-8-2 loss: 0.716296  [   96/  306]
train() client id: f_00004-8-3 loss: 0.853051  [  128/  306]
train() client id: f_00004-8-4 loss: 0.741893  [  160/  306]
train() client id: f_00004-8-5 loss: 0.793535  [  192/  306]
train() client id: f_00004-8-6 loss: 0.839667  [  224/  306]
train() client id: f_00004-8-7 loss: 0.714466  [  256/  306]
train() client id: f_00004-8-8 loss: 0.640162  [  288/  306]
train() client id: f_00004-9-0 loss: 0.685512  [   32/  306]
train() client id: f_00004-9-1 loss: 0.751747  [   64/  306]
train() client id: f_00004-9-2 loss: 0.810107  [   96/  306]
train() client id: f_00004-9-3 loss: 0.608323  [  128/  306]
train() client id: f_00004-9-4 loss: 0.844982  [  160/  306]
train() client id: f_00004-9-5 loss: 0.815249  [  192/  306]
train() client id: f_00004-9-6 loss: 0.671990  [  224/  306]
train() client id: f_00004-9-7 loss: 0.847981  [  256/  306]
train() client id: f_00004-9-8 loss: 0.811861  [  288/  306]
train() client id: f_00004-10-0 loss: 0.629543  [   32/  306]
train() client id: f_00004-10-1 loss: 0.937733  [   64/  306]
train() client id: f_00004-10-2 loss: 0.753026  [   96/  306]
train() client id: f_00004-10-3 loss: 0.725952  [  128/  306]
train() client id: f_00004-10-4 loss: 0.730436  [  160/  306]
train() client id: f_00004-10-5 loss: 0.767049  [  192/  306]
train() client id: f_00004-10-6 loss: 0.728622  [  224/  306]
train() client id: f_00004-10-7 loss: 0.777903  [  256/  306]
train() client id: f_00004-10-8 loss: 0.730661  [  288/  306]
train() client id: f_00004-11-0 loss: 0.745445  [   32/  306]
train() client id: f_00004-11-1 loss: 0.816954  [   64/  306]
train() client id: f_00004-11-2 loss: 0.743067  [   96/  306]
train() client id: f_00004-11-3 loss: 0.727913  [  128/  306]
train() client id: f_00004-11-4 loss: 0.776699  [  160/  306]
train() client id: f_00004-11-5 loss: 0.626804  [  192/  306]
train() client id: f_00004-11-6 loss: 0.681668  [  224/  306]
train() client id: f_00004-11-7 loss: 0.909744  [  256/  306]
train() client id: f_00004-11-8 loss: 0.790096  [  288/  306]
train() client id: f_00004-12-0 loss: 0.893364  [   32/  306]
train() client id: f_00004-12-1 loss: 0.699699  [   64/  306]
train() client id: f_00004-12-2 loss: 0.728101  [   96/  306]
train() client id: f_00004-12-3 loss: 0.685042  [  128/  306]
train() client id: f_00004-12-4 loss: 0.880210  [  160/  306]
train() client id: f_00004-12-5 loss: 0.667289  [  192/  306]
train() client id: f_00004-12-6 loss: 0.793786  [  224/  306]
train() client id: f_00004-12-7 loss: 0.712838  [  256/  306]
train() client id: f_00004-12-8 loss: 0.712035  [  288/  306]
train() client id: f_00005-0-0 loss: 0.800427  [   32/  146]
train() client id: f_00005-0-1 loss: 0.860232  [   64/  146]
train() client id: f_00005-0-2 loss: 0.737086  [   96/  146]
train() client id: f_00005-0-3 loss: 0.688591  [  128/  146]
train() client id: f_00005-1-0 loss: 0.828512  [   32/  146]
train() client id: f_00005-1-1 loss: 0.940515  [   64/  146]
train() client id: f_00005-1-2 loss: 0.803992  [   96/  146]
train() client id: f_00005-1-3 loss: 0.596976  [  128/  146]
train() client id: f_00005-2-0 loss: 0.610451  [   32/  146]
train() client id: f_00005-2-1 loss: 0.948353  [   64/  146]
train() client id: f_00005-2-2 loss: 0.836316  [   96/  146]
train() client id: f_00005-2-3 loss: 0.565899  [  128/  146]
train() client id: f_00005-3-0 loss: 0.792108  [   32/  146]
train() client id: f_00005-3-1 loss: 0.757215  [   64/  146]
train() client id: f_00005-3-2 loss: 0.760369  [   96/  146]
train() client id: f_00005-3-3 loss: 0.833572  [  128/  146]
train() client id: f_00005-4-0 loss: 0.708403  [   32/  146]
train() client id: f_00005-4-1 loss: 0.846637  [   64/  146]
train() client id: f_00005-4-2 loss: 0.435203  [   96/  146]
train() client id: f_00005-4-3 loss: 1.070636  [  128/  146]
train() client id: f_00005-5-0 loss: 0.966545  [   32/  146]
train() client id: f_00005-5-1 loss: 0.659696  [   64/  146]
train() client id: f_00005-5-2 loss: 0.633901  [   96/  146]
train() client id: f_00005-5-3 loss: 0.910558  [  128/  146]
train() client id: f_00005-6-0 loss: 0.760596  [   32/  146]
train() client id: f_00005-6-1 loss: 0.947480  [   64/  146]
train() client id: f_00005-6-2 loss: 0.561219  [   96/  146]
train() client id: f_00005-6-3 loss: 0.881827  [  128/  146]
train() client id: f_00005-7-0 loss: 0.846632  [   32/  146]
train() client id: f_00005-7-1 loss: 0.930936  [   64/  146]
train() client id: f_00005-7-2 loss: 0.675731  [   96/  146]
train() client id: f_00005-7-3 loss: 0.624269  [  128/  146]
train() client id: f_00005-8-0 loss: 0.612731  [   32/  146]
train() client id: f_00005-8-1 loss: 0.675133  [   64/  146]
train() client id: f_00005-8-2 loss: 0.947307  [   96/  146]
train() client id: f_00005-8-3 loss: 0.859575  [  128/  146]
train() client id: f_00005-9-0 loss: 0.619748  [   32/  146]
train() client id: f_00005-9-1 loss: 0.833249  [   64/  146]
train() client id: f_00005-9-2 loss: 0.840612  [   96/  146]
train() client id: f_00005-9-3 loss: 0.817804  [  128/  146]
train() client id: f_00005-10-0 loss: 0.892210  [   32/  146]
train() client id: f_00005-10-1 loss: 0.717235  [   64/  146]
train() client id: f_00005-10-2 loss: 0.743995  [   96/  146]
train() client id: f_00005-10-3 loss: 0.658113  [  128/  146]
train() client id: f_00005-11-0 loss: 0.583289  [   32/  146]
train() client id: f_00005-11-1 loss: 0.915641  [   64/  146]
train() client id: f_00005-11-2 loss: 0.730826  [   96/  146]
train() client id: f_00005-11-3 loss: 0.802932  [  128/  146]
train() client id: f_00005-12-0 loss: 0.894051  [   32/  146]
train() client id: f_00005-12-1 loss: 0.772729  [   64/  146]
train() client id: f_00005-12-2 loss: 0.767484  [   96/  146]
train() client id: f_00005-12-3 loss: 0.920640  [  128/  146]
train() client id: f_00006-0-0 loss: 0.475636  [   32/   54]
train() client id: f_00006-1-0 loss: 0.512585  [   32/   54]
train() client id: f_00006-2-0 loss: 0.534047  [   32/   54]
train() client id: f_00006-3-0 loss: 0.469176  [   32/   54]
train() client id: f_00006-4-0 loss: 0.552081  [   32/   54]
train() client id: f_00006-5-0 loss: 0.488244  [   32/   54]
train() client id: f_00006-6-0 loss: 0.523400  [   32/   54]
train() client id: f_00006-7-0 loss: 0.517485  [   32/   54]
train() client id: f_00006-8-0 loss: 0.541787  [   32/   54]
train() client id: f_00006-9-0 loss: 0.486533  [   32/   54]
train() client id: f_00006-10-0 loss: 0.479459  [   32/   54]
train() client id: f_00006-11-0 loss: 0.490907  [   32/   54]
train() client id: f_00006-12-0 loss: 0.547645  [   32/   54]
train() client id: f_00007-0-0 loss: 0.465442  [   32/  179]
train() client id: f_00007-0-1 loss: 0.577770  [   64/  179]
train() client id: f_00007-0-2 loss: 0.759999  [   96/  179]
train() client id: f_00007-0-3 loss: 0.558462  [  128/  179]
train() client id: f_00007-0-4 loss: 0.708827  [  160/  179]
train() client id: f_00007-1-0 loss: 0.609017  [   32/  179]
train() client id: f_00007-1-1 loss: 0.520319  [   64/  179]
train() client id: f_00007-1-2 loss: 0.496023  [   96/  179]
train() client id: f_00007-1-3 loss: 0.616150  [  128/  179]
train() client id: f_00007-1-4 loss: 0.824425  [  160/  179]
train() client id: f_00007-2-0 loss: 0.508112  [   32/  179]
train() client id: f_00007-2-1 loss: 0.507129  [   64/  179]
train() client id: f_00007-2-2 loss: 0.624145  [   96/  179]
train() client id: f_00007-2-3 loss: 0.500219  [  128/  179]
train() client id: f_00007-2-4 loss: 0.952691  [  160/  179]
train() client id: f_00007-3-0 loss: 0.764452  [   32/  179]
train() client id: f_00007-3-1 loss: 0.724164  [   64/  179]
train() client id: f_00007-3-2 loss: 0.527753  [   96/  179]
train() client id: f_00007-3-3 loss: 0.462231  [  128/  179]
train() client id: f_00007-3-4 loss: 0.531326  [  160/  179]
train() client id: f_00007-4-0 loss: 0.498366  [   32/  179]
train() client id: f_00007-4-1 loss: 0.541126  [   64/  179]
train() client id: f_00007-4-2 loss: 0.433571  [   96/  179]
train() client id: f_00007-4-3 loss: 0.846599  [  128/  179]
train() client id: f_00007-4-4 loss: 0.563487  [  160/  179]
train() client id: f_00007-5-0 loss: 0.601626  [   32/  179]
train() client id: f_00007-5-1 loss: 0.666537  [   64/  179]
train() client id: f_00007-5-2 loss: 0.432485  [   96/  179]
train() client id: f_00007-5-3 loss: 0.555481  [  128/  179]
train() client id: f_00007-5-4 loss: 0.661928  [  160/  179]
train() client id: f_00007-6-0 loss: 0.601141  [   32/  179]
train() client id: f_00007-6-1 loss: 0.579883  [   64/  179]
train() client id: f_00007-6-2 loss: 0.763637  [   96/  179]
train() client id: f_00007-6-3 loss: 0.465197  [  128/  179]
train() client id: f_00007-6-4 loss: 0.560207  [  160/  179]
train() client id: f_00007-7-0 loss: 0.601018  [   32/  179]
train() client id: f_00007-7-1 loss: 0.455801  [   64/  179]
train() client id: f_00007-7-2 loss: 0.801852  [   96/  179]
train() client id: f_00007-7-3 loss: 0.545635  [  128/  179]
train() client id: f_00007-7-4 loss: 0.545482  [  160/  179]
train() client id: f_00007-8-0 loss: 0.625415  [   32/  179]
train() client id: f_00007-8-1 loss: 0.691076  [   64/  179]
train() client id: f_00007-8-2 loss: 0.417897  [   96/  179]
train() client id: f_00007-8-3 loss: 0.529203  [  128/  179]
train() client id: f_00007-8-4 loss: 0.567081  [  160/  179]
train() client id: f_00007-9-0 loss: 0.548422  [   32/  179]
train() client id: f_00007-9-1 loss: 0.437797  [   64/  179]
train() client id: f_00007-9-2 loss: 0.493148  [   96/  179]
train() client id: f_00007-9-3 loss: 0.627873  [  128/  179]
train() client id: f_00007-9-4 loss: 0.833464  [  160/  179]
train() client id: f_00007-10-0 loss: 0.757872  [   32/  179]
train() client id: f_00007-10-1 loss: 0.462232  [   64/  179]
train() client id: f_00007-10-2 loss: 0.522724  [   96/  179]
train() client id: f_00007-10-3 loss: 0.659821  [  128/  179]
train() client id: f_00007-10-4 loss: 0.439824  [  160/  179]
train() client id: f_00007-11-0 loss: 0.711690  [   32/  179]
train() client id: f_00007-11-1 loss: 0.666083  [   64/  179]
train() client id: f_00007-11-2 loss: 0.584235  [   96/  179]
train() client id: f_00007-11-3 loss: 0.441574  [  128/  179]
train() client id: f_00007-11-4 loss: 0.426813  [  160/  179]
train() client id: f_00007-12-0 loss: 0.415342  [   32/  179]
train() client id: f_00007-12-1 loss: 0.455692  [   64/  179]
train() client id: f_00007-12-2 loss: 0.696900  [   96/  179]
train() client id: f_00007-12-3 loss: 0.686482  [  128/  179]
train() client id: f_00007-12-4 loss: 0.499234  [  160/  179]
train() client id: f_00008-0-0 loss: 0.637349  [   32/  130]
train() client id: f_00008-0-1 loss: 0.714921  [   64/  130]
train() client id: f_00008-0-2 loss: 0.692294  [   96/  130]
train() client id: f_00008-0-3 loss: 0.700913  [  128/  130]
train() client id: f_00008-1-0 loss: 0.669715  [   32/  130]
train() client id: f_00008-1-1 loss: 0.689679  [   64/  130]
train() client id: f_00008-1-2 loss: 0.696052  [   96/  130]
train() client id: f_00008-1-3 loss: 0.684974  [  128/  130]
train() client id: f_00008-2-0 loss: 0.672830  [   32/  130]
train() client id: f_00008-2-1 loss: 0.728570  [   64/  130]
train() client id: f_00008-2-2 loss: 0.633915  [   96/  130]
train() client id: f_00008-2-3 loss: 0.688136  [  128/  130]
train() client id: f_00008-3-0 loss: 0.601898  [   32/  130]
train() client id: f_00008-3-1 loss: 0.819127  [   64/  130]
train() client id: f_00008-3-2 loss: 0.630996  [   96/  130]
train() client id: f_00008-3-3 loss: 0.688589  [  128/  130]
train() client id: f_00008-4-0 loss: 0.825242  [   32/  130]
train() client id: f_00008-4-1 loss: 0.666258  [   64/  130]
train() client id: f_00008-4-2 loss: 0.664668  [   96/  130]
train() client id: f_00008-4-3 loss: 0.583231  [  128/  130]
train() client id: f_00008-5-0 loss: 0.806845  [   32/  130]
train() client id: f_00008-5-1 loss: 0.673005  [   64/  130]
train() client id: f_00008-5-2 loss: 0.671317  [   96/  130]
train() client id: f_00008-5-3 loss: 0.585063  [  128/  130]
train() client id: f_00008-6-0 loss: 0.730375  [   32/  130]
train() client id: f_00008-6-1 loss: 0.637604  [   64/  130]
train() client id: f_00008-6-2 loss: 0.621707  [   96/  130]
train() client id: f_00008-6-3 loss: 0.726185  [  128/  130]
train() client id: f_00008-7-0 loss: 0.641007  [   32/  130]
train() client id: f_00008-7-1 loss: 0.781655  [   64/  130]
train() client id: f_00008-7-2 loss: 0.510946  [   96/  130]
train() client id: f_00008-7-3 loss: 0.790488  [  128/  130]
train() client id: f_00008-8-0 loss: 0.735847  [   32/  130]
train() client id: f_00008-8-1 loss: 0.676229  [   64/  130]
train() client id: f_00008-8-2 loss: 0.553969  [   96/  130]
train() client id: f_00008-8-3 loss: 0.729702  [  128/  130]
train() client id: f_00008-9-0 loss: 0.765423  [   32/  130]
train() client id: f_00008-9-1 loss: 0.661963  [   64/  130]
train() client id: f_00008-9-2 loss: 0.652386  [   96/  130]
train() client id: f_00008-9-3 loss: 0.610446  [  128/  130]
train() client id: f_00008-10-0 loss: 0.573445  [   32/  130]
train() client id: f_00008-10-1 loss: 0.742160  [   64/  130]
train() client id: f_00008-10-2 loss: 0.669209  [   96/  130]
train() client id: f_00008-10-3 loss: 0.726953  [  128/  130]
train() client id: f_00008-11-0 loss: 0.637786  [   32/  130]
train() client id: f_00008-11-1 loss: 0.713328  [   64/  130]
train() client id: f_00008-11-2 loss: 0.670677  [   96/  130]
train() client id: f_00008-11-3 loss: 0.667877  [  128/  130]
train() client id: f_00008-12-0 loss: 0.694594  [   32/  130]
train() client id: f_00008-12-1 loss: 0.639886  [   64/  130]
train() client id: f_00008-12-2 loss: 0.645726  [   96/  130]
train() client id: f_00008-12-3 loss: 0.743904  [  128/  130]
train() client id: f_00009-0-0 loss: 1.172933  [   32/  118]
train() client id: f_00009-0-1 loss: 0.950998  [   64/  118]
train() client id: f_00009-0-2 loss: 1.053798  [   96/  118]
train() client id: f_00009-1-0 loss: 0.983887  [   32/  118]
train() client id: f_00009-1-1 loss: 1.061039  [   64/  118]
train() client id: f_00009-1-2 loss: 0.969547  [   96/  118]
train() client id: f_00009-2-0 loss: 1.062977  [   32/  118]
train() client id: f_00009-2-1 loss: 0.868384  [   64/  118]
train() client id: f_00009-2-2 loss: 0.961104  [   96/  118]
train() client id: f_00009-3-0 loss: 0.855750  [   32/  118]
train() client id: f_00009-3-1 loss: 0.904053  [   64/  118]
train() client id: f_00009-3-2 loss: 0.911546  [   96/  118]
train() client id: f_00009-4-0 loss: 0.866887  [   32/  118]
train() client id: f_00009-4-1 loss: 0.913379  [   64/  118]
train() client id: f_00009-4-2 loss: 0.845063  [   96/  118]
train() client id: f_00009-5-0 loss: 0.757935  [   32/  118]
train() client id: f_00009-5-1 loss: 0.875360  [   64/  118]
train() client id: f_00009-5-2 loss: 0.897169  [   96/  118]
train() client id: f_00009-6-0 loss: 0.906417  [   32/  118]
train() client id: f_00009-6-1 loss: 0.869830  [   64/  118]
train() client id: f_00009-6-2 loss: 0.869172  [   96/  118]
train() client id: f_00009-7-0 loss: 0.766735  [   32/  118]
train() client id: f_00009-7-1 loss: 0.883020  [   64/  118]
train() client id: f_00009-7-2 loss: 0.813017  [   96/  118]
train() client id: f_00009-8-0 loss: 0.906616  [   32/  118]
train() client id: f_00009-8-1 loss: 0.765223  [   64/  118]
train() client id: f_00009-8-2 loss: 0.730309  [   96/  118]
train() client id: f_00009-9-0 loss: 0.860203  [   32/  118]
train() client id: f_00009-9-1 loss: 0.879326  [   64/  118]
train() client id: f_00009-9-2 loss: 0.776146  [   96/  118]
train() client id: f_00009-10-0 loss: 0.860683  [   32/  118]
train() client id: f_00009-10-1 loss: 0.916881  [   64/  118]
train() client id: f_00009-10-2 loss: 0.690748  [   96/  118]
train() client id: f_00009-11-0 loss: 0.712627  [   32/  118]
train() client id: f_00009-11-1 loss: 0.840084  [   64/  118]
train() client id: f_00009-11-2 loss: 0.828257  [   96/  118]
train() client id: f_00009-12-0 loss: 0.850541  [   32/  118]
train() client id: f_00009-12-1 loss: 0.769089  [   64/  118]
train() client id: f_00009-12-2 loss: 0.684887  [   96/  118]
At round 44 accuracy: 0.649867374005305
At round 44 training accuracy: 0.5895372233400402
At round 44 training loss: 0.8357247665234655
update_location
xs = [  -3.9056584     4.20031788  240.00902392   18.81129433    0.97929623
    3.95640986 -202.44319194 -181.32485185  224.66397685 -167.06087855]
ys = [ 232.5879595   215.55583871    1.32061395 -202.45517586  194.35018685
  177.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [253.20429118 237.6593408  260.01168355 226.58764977 218.57024991
 204.04294985 225.80995661 207.07336418 246.5412186  194.74431699]
dists_bs = [180.52602805 183.73222358 449.88556219 424.3055828  177.03994586
 178.97853524 179.73247853 174.29245025 429.55303777 171.18271783]
uav_gains = [6.96478461e-12 9.36092802e-12 6.06859569e-12 1.13384472e-11
 1.29057031e-11 1.60769736e-11 1.14854179e-11 1.53749023e-11
 7.93441177e-12 1.83978984e-11]
bs_gains = [5.30863138e-11 5.05330089e-11 4.11726034e-12 4.85057916e-12
 5.60653496e-11 5.43815306e-11 5.37452031e-11 5.85752397e-11
 4.68648315e-12 6.16036319e-11]
Round 45
-------------------------------
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.05514024 10.40024199  4.98453475  1.80545975 11.99306906  5.77018381
  2.2328339   7.08469715  5.23316744  4.67883957]
obj_prev = 59.2381676466099
eta_min = 5.885406011502381e-19	eta_max = 0.9363452566333951
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 13.714204272500444	eta = 0.9090909090909091
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 26.6122320197388	eta = 0.4684860112559707
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 20.101928982645475	eta = 0.6202120423522212
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.922786406509566	eta = 0.6588595443458035
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857340066002127	eta = 0.6611461842396013
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857119499819667	eta = 0.6611539174721296
eta = 0.6611539174721296
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.03414319 0.0718091  0.03360124 0.01165204 0.08291919 0.03956275
 0.0146328  0.04850501 0.0352271  0.03197537]
ene_total = [1.73700778 2.97656775 1.73758614 0.82737842 3.38956642 1.75748632
 0.93827146 2.18480132 1.84341633 1.46503756]
ti_comp = [0.61689568 0.66459269 0.61132264 0.63338719 0.66608943 0.66565651
 0.63376527 0.64167568 0.59949544 0.66739459]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.53685718e-06 5.23971089e-05 6.34460979e-06 2.46460673e-07
 8.03119208e-05 8.73451965e-06 4.87534362e-07 1.73224277e-05
 7.60219691e-06 4.58734577e-06]
ene_total = [0.45070637 0.27270761 0.47169887 0.38832777 0.26811959 0.26705379
 0.38691223 0.35773942 0.51631227 0.26034825]
optimize_network iter = 0 obj = 3.639926174690373
eta = 0.6611539174721296
freqs = [27673393.660334   54024893.67528107 27482409.80128485  9198198.6140707
 62243282.60071961 29717090.05574555 11544335.26103987 37795583.8923452
 29380628.13581736 23955372.57683051]
eta_min = 0.6611539174721324	eta_max = 0.6721599416563945
af = 0.005695477061159853	bf = 1.2310181153532935	zeta = 0.006265024767275839	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50574262e-06 1.20694943e-05 1.46145910e-06 5.67713705e-08
 1.84995754e-05 2.01196663e-06 1.12301867e-07 3.99016180e-06
 1.75113998e-06 1.05667936e-06]
ene_total = [1.67921435 1.010801   1.75748063 1.44739012 0.99068256 0.99444728
 1.44208793 1.33153211 1.92363253 0.96990197]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 1 obj = 3.761787113624325
eta = 0.6721599416563945
freqs = [27640992.43136498 53805865.01782712 27460330.69138759  9177789.36033805
 61985734.40435877 29594844.6264841  11518450.63917791 37692613.12413883
 29380628.13581735 23854513.15446249]
eta_min = 0.6721599416563961	eta_max = 0.6721599416563635
af = 0.0056547282477792775	bf = 1.2310181153532935	zeta = 0.006220201072557206	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50221871e-06 1.19718279e-05 1.45911179e-06 5.65197178e-08
 1.83467983e-05 1.99544766e-06 1.11798827e-07 3.96844972e-06
 1.75113998e-06 1.04780021e-06]
ene_total = [1.67921385 1.01078729 1.7574803  1.44739009 0.9906611  0.99444496
 1.44208786 1.33152906 1.92363253 0.96990072]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 2 obj = 3.761787113623971
eta = 0.6721599416563635
freqs = [27640992.43136505 53805865.01782771 27460330.69138763  9177789.3603381
 61985734.40435947 29594844.62648444 11518450.63917798 37692613.1241391
 29380628.13581733 23854513.15446277]
Done!
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.25736035e-06 4.98675999e-05 6.07780228e-06 2.35427930e-07
 7.64219800e-05 8.31186232e-06 4.65688215e-07 1.65302294e-05
 7.29422007e-06 4.36451991e-06]
ene_total = [0.01196084 0.00723475 0.01251797 0.01030567 0.00711163 0.00708682
 0.01026809 0.00949312 0.0137019  0.00690906]
At round 45 energy consumption: 0.09658985266196866
At round 45 eta: 0.6721599416563635
At round 45 a_n: 12.768039742635098
At round 45 local rounds: 13.008276614299735
At round 45 global rounds: 38.94594152753551
gradient difference: 0.47253069281578064
train() client id: f_00000-0-0 loss: 1.173146  [   32/  126]
train() client id: f_00000-0-1 loss: 1.063388  [   64/  126]
train() client id: f_00000-0-2 loss: 0.923402  [   96/  126]
train() client id: f_00000-1-0 loss: 1.256214  [   32/  126]
train() client id: f_00000-1-1 loss: 1.070876  [   64/  126]
train() client id: f_00000-1-2 loss: 0.898551  [   96/  126]
train() client id: f_00000-2-0 loss: 0.967715  [   32/  126]
train() client id: f_00000-2-1 loss: 0.980518  [   64/  126]
train() client id: f_00000-2-2 loss: 0.851371  [   96/  126]
train() client id: f_00000-3-0 loss: 0.947049  [   32/  126]
train() client id: f_00000-3-1 loss: 0.860947  [   64/  126]
train() client id: f_00000-3-2 loss: 0.896089  [   96/  126]
train() client id: f_00000-4-0 loss: 0.789508  [   32/  126]
train() client id: f_00000-4-1 loss: 0.987964  [   64/  126]
train() client id: f_00000-4-2 loss: 0.838428  [   96/  126]
train() client id: f_00000-5-0 loss: 0.842489  [   32/  126]
train() client id: f_00000-5-1 loss: 0.850524  [   64/  126]
train() client id: f_00000-5-2 loss: 0.909487  [   96/  126]
train() client id: f_00000-6-0 loss: 0.898071  [   32/  126]
train() client id: f_00000-6-1 loss: 0.857684  [   64/  126]
train() client id: f_00000-6-2 loss: 0.824020  [   96/  126]
train() client id: f_00000-7-0 loss: 0.867292  [   32/  126]
train() client id: f_00000-7-1 loss: 0.976553  [   64/  126]
train() client id: f_00000-7-2 loss: 0.872532  [   96/  126]
train() client id: f_00000-8-0 loss: 0.920119  [   32/  126]
train() client id: f_00000-8-1 loss: 0.865913  [   64/  126]
train() client id: f_00000-8-2 loss: 0.779203  [   96/  126]
train() client id: f_00000-9-0 loss: 0.966825  [   32/  126]
train() client id: f_00000-9-1 loss: 0.805866  [   64/  126]
train() client id: f_00000-9-2 loss: 0.746643  [   96/  126]
train() client id: f_00000-10-0 loss: 0.865747  [   32/  126]
train() client id: f_00000-10-1 loss: 1.011824  [   64/  126]
train() client id: f_00000-10-2 loss: 0.787609  [   96/  126]
train() client id: f_00000-11-0 loss: 0.898288  [   32/  126]
train() client id: f_00000-11-1 loss: 0.922516  [   64/  126]
train() client id: f_00000-11-2 loss: 0.862071  [   96/  126]
train() client id: f_00000-12-0 loss: 0.983810  [   32/  126]
train() client id: f_00000-12-1 loss: 0.813034  [   64/  126]
train() client id: f_00000-12-2 loss: 0.798979  [   96/  126]
train() client id: f_00001-0-0 loss: 0.469147  [   32/  265]
train() client id: f_00001-0-1 loss: 0.426367  [   64/  265]
train() client id: f_00001-0-2 loss: 0.465877  [   96/  265]
train() client id: f_00001-0-3 loss: 0.440710  [  128/  265]
train() client id: f_00001-0-4 loss: 0.403028  [  160/  265]
train() client id: f_00001-0-5 loss: 0.512244  [  192/  265]
train() client id: f_00001-0-6 loss: 0.458713  [  224/  265]
train() client id: f_00001-0-7 loss: 0.391437  [  256/  265]
train() client id: f_00001-1-0 loss: 0.422718  [   32/  265]
train() client id: f_00001-1-1 loss: 0.398563  [   64/  265]
train() client id: f_00001-1-2 loss: 0.353666  [   96/  265]
train() client id: f_00001-1-3 loss: 0.595135  [  128/  265]
train() client id: f_00001-1-4 loss: 0.353052  [  160/  265]
train() client id: f_00001-1-5 loss: 0.419868  [  192/  265]
train() client id: f_00001-1-6 loss: 0.457545  [  224/  265]
train() client id: f_00001-1-7 loss: 0.451955  [  256/  265]
train() client id: f_00001-2-0 loss: 0.455311  [   32/  265]
train() client id: f_00001-2-1 loss: 0.427573  [   64/  265]
train() client id: f_00001-2-2 loss: 0.548505  [   96/  265]
train() client id: f_00001-2-3 loss: 0.423947  [  128/  265]
train() client id: f_00001-2-4 loss: 0.325810  [  160/  265]
train() client id: f_00001-2-5 loss: 0.408570  [  192/  265]
train() client id: f_00001-2-6 loss: 0.375407  [  224/  265]
train() client id: f_00001-2-7 loss: 0.416893  [  256/  265]
train() client id: f_00001-3-0 loss: 0.423050  [   32/  265]
train() client id: f_00001-3-1 loss: 0.475573  [   64/  265]
train() client id: f_00001-3-2 loss: 0.380163  [   96/  265]
train() client id: f_00001-3-3 loss: 0.462966  [  128/  265]
train() client id: f_00001-3-4 loss: 0.323463  [  160/  265]
train() client id: f_00001-3-5 loss: 0.339986  [  192/  265]
train() client id: f_00001-3-6 loss: 0.482689  [  224/  265]
train() client id: f_00001-3-7 loss: 0.491918  [  256/  265]
train() client id: f_00001-4-0 loss: 0.342450  [   32/  265]
train() client id: f_00001-4-1 loss: 0.447916  [   64/  265]
train() client id: f_00001-4-2 loss: 0.331012  [   96/  265]
train() client id: f_00001-4-3 loss: 0.472842  [  128/  265]
train() client id: f_00001-4-4 loss: 0.405873  [  160/  265]
train() client id: f_00001-4-5 loss: 0.363639  [  192/  265]
train() client id: f_00001-4-6 loss: 0.424548  [  224/  265]
train() client id: f_00001-4-7 loss: 0.431467  [  256/  265]
train() client id: f_00001-5-0 loss: 0.414161  [   32/  265]
train() client id: f_00001-5-1 loss: 0.384913  [   64/  265]
train() client id: f_00001-5-2 loss: 0.383135  [   96/  265]
train() client id: f_00001-5-3 loss: 0.439037  [  128/  265]
train() client id: f_00001-5-4 loss: 0.415402  [  160/  265]
train() client id: f_00001-5-5 loss: 0.485955  [  192/  265]
train() client id: f_00001-5-6 loss: 0.456708  [  224/  265]
train() client id: f_00001-5-7 loss: 0.335918  [  256/  265]
train() client id: f_00001-6-0 loss: 0.441750  [   32/  265]
train() client id: f_00001-6-1 loss: 0.413555  [   64/  265]
train() client id: f_00001-6-2 loss: 0.336369  [   96/  265]
train() client id: f_00001-6-3 loss: 0.387015  [  128/  265]
train() client id: f_00001-6-4 loss: 0.384795  [  160/  265]
train() client id: f_00001-6-5 loss: 0.426452  [  192/  265]
train() client id: f_00001-6-6 loss: 0.520600  [  224/  265]
train() client id: f_00001-6-7 loss: 0.401708  [  256/  265]
train() client id: f_00001-7-0 loss: 0.432203  [   32/  265]
train() client id: f_00001-7-1 loss: 0.382156  [   64/  265]
train() client id: f_00001-7-2 loss: 0.351803  [   96/  265]
train() client id: f_00001-7-3 loss: 0.403093  [  128/  265]
train() client id: f_00001-7-4 loss: 0.443929  [  160/  265]
train() client id: f_00001-7-5 loss: 0.351730  [  192/  265]
train() client id: f_00001-7-6 loss: 0.398120  [  224/  265]
train() client id: f_00001-7-7 loss: 0.536500  [  256/  265]
train() client id: f_00001-8-0 loss: 0.406002  [   32/  265]
train() client id: f_00001-8-1 loss: 0.505863  [   64/  265]
train() client id: f_00001-8-2 loss: 0.356598  [   96/  265]
train() client id: f_00001-8-3 loss: 0.356942  [  128/  265]
train() client id: f_00001-8-4 loss: 0.348613  [  160/  265]
train() client id: f_00001-8-5 loss: 0.388299  [  192/  265]
train() client id: f_00001-8-6 loss: 0.410945  [  224/  265]
train() client id: f_00001-8-7 loss: 0.422938  [  256/  265]
train() client id: f_00001-9-0 loss: 0.333696  [   32/  265]
train() client id: f_00001-9-1 loss: 0.406396  [   64/  265]
train() client id: f_00001-9-2 loss: 0.344168  [   96/  265]
train() client id: f_00001-9-3 loss: 0.404028  [  128/  265]
train() client id: f_00001-9-4 loss: 0.497558  [  160/  265]
train() client id: f_00001-9-5 loss: 0.402504  [  192/  265]
train() client id: f_00001-9-6 loss: 0.386612  [  224/  265]
train() client id: f_00001-9-7 loss: 0.503471  [  256/  265]
train() client id: f_00001-10-0 loss: 0.416371  [   32/  265]
train() client id: f_00001-10-1 loss: 0.485042  [   64/  265]
train() client id: f_00001-10-2 loss: 0.502304  [   96/  265]
train() client id: f_00001-10-3 loss: 0.306565  [  128/  265]
train() client id: f_00001-10-4 loss: 0.348773  [  160/  265]
train() client id: f_00001-10-5 loss: 0.314964  [  192/  265]
train() client id: f_00001-10-6 loss: 0.576878  [  224/  265]
train() client id: f_00001-10-7 loss: 0.322234  [  256/  265]
train() client id: f_00001-11-0 loss: 0.344106  [   32/  265]
train() client id: f_00001-11-1 loss: 0.467110  [   64/  265]
train() client id: f_00001-11-2 loss: 0.385957  [   96/  265]
train() client id: f_00001-11-3 loss: 0.405967  [  128/  265]
train() client id: f_00001-11-4 loss: 0.403484  [  160/  265]
train() client id: f_00001-11-5 loss: 0.609464  [  192/  265]
train() client id: f_00001-11-6 loss: 0.305087  [  224/  265]
train() client id: f_00001-11-7 loss: 0.342261  [  256/  265]
train() client id: f_00001-12-0 loss: 0.298415  [   32/  265]
train() client id: f_00001-12-1 loss: 0.397360  [   64/  265]
train() client id: f_00001-12-2 loss: 0.354970  [   96/  265]
train() client id: f_00001-12-3 loss: 0.325066  [  128/  265]
train() client id: f_00001-12-4 loss: 0.561163  [  160/  265]
train() client id: f_00001-12-5 loss: 0.382091  [  192/  265]
train() client id: f_00001-12-6 loss: 0.487438  [  224/  265]
train() client id: f_00001-12-7 loss: 0.471889  [  256/  265]
train() client id: f_00002-0-0 loss: 1.458365  [   32/  124]
train() client id: f_00002-0-1 loss: 1.326448  [   64/  124]
train() client id: f_00002-0-2 loss: 1.121764  [   96/  124]
train() client id: f_00002-1-0 loss: 1.239634  [   32/  124]
train() client id: f_00002-1-1 loss: 1.088138  [   64/  124]
train() client id: f_00002-1-2 loss: 1.294725  [   96/  124]
train() client id: f_00002-2-0 loss: 1.179020  [   32/  124]
train() client id: f_00002-2-1 loss: 1.218315  [   64/  124]
train() client id: f_00002-2-2 loss: 1.196351  [   96/  124]
train() client id: f_00002-3-0 loss: 1.095070  [   32/  124]
train() client id: f_00002-3-1 loss: 1.187288  [   64/  124]
train() client id: f_00002-3-2 loss: 1.008756  [   96/  124]
train() client id: f_00002-4-0 loss: 1.121299  [   32/  124]
train() client id: f_00002-4-1 loss: 1.125336  [   64/  124]
train() client id: f_00002-4-2 loss: 1.067647  [   96/  124]
train() client id: f_00002-5-0 loss: 1.079232  [   32/  124]
train() client id: f_00002-5-1 loss: 1.166497  [   64/  124]
train() client id: f_00002-5-2 loss: 1.145193  [   96/  124]
train() client id: f_00002-6-0 loss: 1.246081  [   32/  124]
train() client id: f_00002-6-1 loss: 1.046476  [   64/  124]
train() client id: f_00002-6-2 loss: 1.021847  [   96/  124]
train() client id: f_00002-7-0 loss: 1.002358  [   32/  124]
train() client id: f_00002-7-1 loss: 1.087893  [   64/  124]
train() client id: f_00002-7-2 loss: 1.003294  [   96/  124]
train() client id: f_00002-8-0 loss: 1.127834  [   32/  124]
train() client id: f_00002-8-1 loss: 1.010974  [   64/  124]
train() client id: f_00002-8-2 loss: 1.093095  [   96/  124]
train() client id: f_00002-9-0 loss: 1.150579  [   32/  124]
train() client id: f_00002-9-1 loss: 1.141033  [   64/  124]
train() client id: f_00002-9-2 loss: 1.066227  [   96/  124]
train() client id: f_00002-10-0 loss: 1.144540  [   32/  124]
train() client id: f_00002-10-1 loss: 1.039646  [   64/  124]
train() client id: f_00002-10-2 loss: 1.084024  [   96/  124]
train() client id: f_00002-11-0 loss: 1.012996  [   32/  124]
train() client id: f_00002-11-1 loss: 1.074576  [   64/  124]
train() client id: f_00002-11-2 loss: 1.052541  [   96/  124]
train() client id: f_00002-12-0 loss: 1.046815  [   32/  124]
train() client id: f_00002-12-1 loss: 0.893408  [   64/  124]
train() client id: f_00002-12-2 loss: 1.082195  [   96/  124]
train() client id: f_00003-0-0 loss: 0.562160  [   32/   43]
train() client id: f_00003-1-0 loss: 0.574347  [   32/   43]
train() client id: f_00003-2-0 loss: 0.740195  [   32/   43]
train() client id: f_00003-3-0 loss: 0.568332  [   32/   43]
train() client id: f_00003-4-0 loss: 0.776159  [   32/   43]
train() client id: f_00003-5-0 loss: 0.681083  [   32/   43]
train() client id: f_00003-6-0 loss: 0.509724  [   32/   43]
train() client id: f_00003-7-0 loss: 0.711352  [   32/   43]
train() client id: f_00003-8-0 loss: 0.689146  [   32/   43]
train() client id: f_00003-9-0 loss: 0.382029  [   32/   43]
train() client id: f_00003-10-0 loss: 0.482880  [   32/   43]
train() client id: f_00003-11-0 loss: 0.634142  [   32/   43]
train() client id: f_00003-12-0 loss: 0.637579  [   32/   43]
train() client id: f_00004-0-0 loss: 0.822231  [   32/  306]
train() client id: f_00004-0-1 loss: 0.882788  [   64/  306]
train() client id: f_00004-0-2 loss: 0.824767  [   96/  306]
train() client id: f_00004-0-3 loss: 0.818982  [  128/  306]
train() client id: f_00004-0-4 loss: 0.875667  [  160/  306]
train() client id: f_00004-0-5 loss: 0.852087  [  192/  306]
train() client id: f_00004-0-6 loss: 1.135923  [  224/  306]
train() client id: f_00004-0-7 loss: 0.872383  [  256/  306]
train() client id: f_00004-0-8 loss: 0.929668  [  288/  306]
train() client id: f_00004-1-0 loss: 0.778765  [   32/  306]
train() client id: f_00004-1-1 loss: 1.060987  [   64/  306]
train() client id: f_00004-1-2 loss: 0.876705  [   96/  306]
train() client id: f_00004-1-3 loss: 0.972622  [  128/  306]
train() client id: f_00004-1-4 loss: 0.939918  [  160/  306]
train() client id: f_00004-1-5 loss: 0.949229  [  192/  306]
train() client id: f_00004-1-6 loss: 0.720434  [  224/  306]
train() client id: f_00004-1-7 loss: 0.868799  [  256/  306]
train() client id: f_00004-1-8 loss: 0.709733  [  288/  306]
train() client id: f_00004-2-0 loss: 0.923113  [   32/  306]
train() client id: f_00004-2-1 loss: 0.956984  [   64/  306]
train() client id: f_00004-2-2 loss: 0.922098  [   96/  306]
train() client id: f_00004-2-3 loss: 0.829528  [  128/  306]
train() client id: f_00004-2-4 loss: 0.729171  [  160/  306]
train() client id: f_00004-2-5 loss: 0.821614  [  192/  306]
train() client id: f_00004-2-6 loss: 0.806041  [  224/  306]
train() client id: f_00004-2-7 loss: 0.944251  [  256/  306]
train() client id: f_00004-2-8 loss: 0.903863  [  288/  306]
train() client id: f_00004-3-0 loss: 0.966054  [   32/  306]
train() client id: f_00004-3-1 loss: 0.855081  [   64/  306]
train() client id: f_00004-3-2 loss: 0.858021  [   96/  306]
train() client id: f_00004-3-3 loss: 0.918383  [  128/  306]
train() client id: f_00004-3-4 loss: 0.914466  [  160/  306]
train() client id: f_00004-3-5 loss: 0.882226  [  192/  306]
train() client id: f_00004-3-6 loss: 0.887887  [  224/  306]
train() client id: f_00004-3-7 loss: 0.891481  [  256/  306]
train() client id: f_00004-3-8 loss: 0.750298  [  288/  306]
train() client id: f_00004-4-0 loss: 0.875349  [   32/  306]
train() client id: f_00004-4-1 loss: 0.849892  [   64/  306]
train() client id: f_00004-4-2 loss: 0.901476  [   96/  306]
train() client id: f_00004-4-3 loss: 0.824087  [  128/  306]
train() client id: f_00004-4-4 loss: 0.950677  [  160/  306]
train() client id: f_00004-4-5 loss: 0.914502  [  192/  306]
train() client id: f_00004-4-6 loss: 0.830090  [  224/  306]
train() client id: f_00004-4-7 loss: 0.903576  [  256/  306]
train() client id: f_00004-4-8 loss: 0.761371  [  288/  306]
train() client id: f_00004-5-0 loss: 0.778459  [   32/  306]
train() client id: f_00004-5-1 loss: 0.935236  [   64/  306]
train() client id: f_00004-5-2 loss: 0.874660  [   96/  306]
train() client id: f_00004-5-3 loss: 0.911798  [  128/  306]
train() client id: f_00004-5-4 loss: 0.931699  [  160/  306]
train() client id: f_00004-5-5 loss: 0.790719  [  192/  306]
train() client id: f_00004-5-6 loss: 0.884906  [  224/  306]
train() client id: f_00004-5-7 loss: 0.837034  [  256/  306]
train() client id: f_00004-5-8 loss: 0.873145  [  288/  306]
train() client id: f_00004-6-0 loss: 0.801131  [   32/  306]
train() client id: f_00004-6-1 loss: 0.872607  [   64/  306]
train() client id: f_00004-6-2 loss: 0.871320  [   96/  306]
train() client id: f_00004-6-3 loss: 0.823352  [  128/  306]
train() client id: f_00004-6-4 loss: 0.903082  [  160/  306]
train() client id: f_00004-6-5 loss: 0.880561  [  192/  306]
train() client id: f_00004-6-6 loss: 0.920074  [  224/  306]
train() client id: f_00004-6-7 loss: 0.904501  [  256/  306]
train() client id: f_00004-6-8 loss: 0.776736  [  288/  306]
train() client id: f_00004-7-0 loss: 0.985094  [   32/  306]
train() client id: f_00004-7-1 loss: 0.781524  [   64/  306]
train() client id: f_00004-7-2 loss: 0.941184  [   96/  306]
train() client id: f_00004-7-3 loss: 0.825267  [  128/  306]
train() client id: f_00004-7-4 loss: 0.849549  [  160/  306]
train() client id: f_00004-7-5 loss: 0.830318  [  192/  306]
train() client id: f_00004-7-6 loss: 0.788835  [  224/  306]
train() client id: f_00004-7-7 loss: 0.889637  [  256/  306]
train() client id: f_00004-7-8 loss: 0.916382  [  288/  306]
train() client id: f_00004-8-0 loss: 0.797711  [   32/  306]
train() client id: f_00004-8-1 loss: 0.861139  [   64/  306]
train() client id: f_00004-8-2 loss: 0.990347  [   96/  306]
train() client id: f_00004-8-3 loss: 0.828477  [  128/  306]
train() client id: f_00004-8-4 loss: 0.904653  [  160/  306]
train() client id: f_00004-8-5 loss: 0.821223  [  192/  306]
train() client id: f_00004-8-6 loss: 0.794519  [  224/  306]
train() client id: f_00004-8-7 loss: 0.780582  [  256/  306]
train() client id: f_00004-8-8 loss: 0.943319  [  288/  306]
train() client id: f_00004-9-0 loss: 0.885077  [   32/  306]
train() client id: f_00004-9-1 loss: 0.889491  [   64/  306]
train() client id: f_00004-9-2 loss: 0.702110  [   96/  306]
train() client id: f_00004-9-3 loss: 0.993395  [  128/  306]
train() client id: f_00004-9-4 loss: 0.840561  [  160/  306]
train() client id: f_00004-9-5 loss: 0.915527  [  192/  306]
train() client id: f_00004-9-6 loss: 0.744453  [  224/  306]
train() client id: f_00004-9-7 loss: 0.920495  [  256/  306]
train() client id: f_00004-9-8 loss: 0.859245  [  288/  306]
train() client id: f_00004-10-0 loss: 0.963019  [   32/  306]
train() client id: f_00004-10-1 loss: 0.888189  [   64/  306]
train() client id: f_00004-10-2 loss: 0.776614  [   96/  306]
train() client id: f_00004-10-3 loss: 0.749320  [  128/  306]
train() client id: f_00004-10-4 loss: 0.901061  [  160/  306]
train() client id: f_00004-10-5 loss: 0.863239  [  192/  306]
train() client id: f_00004-10-6 loss: 0.804906  [  224/  306]
train() client id: f_00004-10-7 loss: 0.827690  [  256/  306]
train() client id: f_00004-10-8 loss: 0.851971  [  288/  306]
train() client id: f_00004-11-0 loss: 0.882100  [   32/  306]
train() client id: f_00004-11-1 loss: 0.742068  [   64/  306]
train() client id: f_00004-11-2 loss: 0.797163  [   96/  306]
train() client id: f_00004-11-3 loss: 0.651876  [  128/  306]
train() client id: f_00004-11-4 loss: 0.870668  [  160/  306]
train() client id: f_00004-11-5 loss: 0.979695  [  192/  306]
train() client id: f_00004-11-6 loss: 0.932879  [  224/  306]
train() client id: f_00004-11-7 loss: 0.867803  [  256/  306]
train() client id: f_00004-11-8 loss: 0.924053  [  288/  306]
train() client id: f_00004-12-0 loss: 0.817669  [   32/  306]
train() client id: f_00004-12-1 loss: 0.784436  [   64/  306]
train() client id: f_00004-12-2 loss: 0.959935  [   96/  306]
train() client id: f_00004-12-3 loss: 0.823467  [  128/  306]
train() client id: f_00004-12-4 loss: 0.806044  [  160/  306]
train() client id: f_00004-12-5 loss: 0.834621  [  192/  306]
train() client id: f_00004-12-6 loss: 0.966060  [  224/  306]
train() client id: f_00004-12-7 loss: 0.789260  [  256/  306]
train() client id: f_00004-12-8 loss: 0.891079  [  288/  306]
train() client id: f_00005-0-0 loss: 0.450209  [   32/  146]
train() client id: f_00005-0-1 loss: 0.426957  [   64/  146]
train() client id: f_00005-0-2 loss: 0.609855  [   96/  146]
train() client id: f_00005-0-3 loss: 0.546482  [  128/  146]
train() client id: f_00005-1-0 loss: 0.639832  [   32/  146]
train() client id: f_00005-1-1 loss: 0.622836  [   64/  146]
train() client id: f_00005-1-2 loss: 0.589521  [   96/  146]
train() client id: f_00005-1-3 loss: 0.286693  [  128/  146]
train() client id: f_00005-2-0 loss: 0.487316  [   32/  146]
train() client id: f_00005-2-1 loss: 0.550213  [   64/  146]
train() client id: f_00005-2-2 loss: 0.460307  [   96/  146]
train() client id: f_00005-2-3 loss: 0.560194  [  128/  146]
train() client id: f_00005-3-0 loss: 0.366278  [   32/  146]
train() client id: f_00005-3-1 loss: 0.495530  [   64/  146]
train() client id: f_00005-3-2 loss: 0.562779  [   96/  146]
train() client id: f_00005-3-3 loss: 0.374771  [  128/  146]
train() client id: f_00005-4-0 loss: 0.405520  [   32/  146]
train() client id: f_00005-4-1 loss: 0.581320  [   64/  146]
train() client id: f_00005-4-2 loss: 0.379160  [   96/  146]
train() client id: f_00005-4-3 loss: 0.617652  [  128/  146]
train() client id: f_00005-5-0 loss: 0.415359  [   32/  146]
train() client id: f_00005-5-1 loss: 0.599914  [   64/  146]
train() client id: f_00005-5-2 loss: 0.543849  [   96/  146]
train() client id: f_00005-5-3 loss: 0.396079  [  128/  146]
train() client id: f_00005-6-0 loss: 0.819564  [   32/  146]
train() client id: f_00005-6-1 loss: 0.411376  [   64/  146]
train() client id: f_00005-6-2 loss: 0.410594  [   96/  146]
train() client id: f_00005-6-3 loss: 0.451492  [  128/  146]
train() client id: f_00005-7-0 loss: 0.457322  [   32/  146]
train() client id: f_00005-7-1 loss: 0.636613  [   64/  146]
train() client id: f_00005-7-2 loss: 0.572757  [   96/  146]
train() client id: f_00005-7-3 loss: 0.533884  [  128/  146]
train() client id: f_00005-8-0 loss: 0.320614  [   32/  146]
train() client id: f_00005-8-1 loss: 0.530046  [   64/  146]
train() client id: f_00005-8-2 loss: 0.527312  [   96/  146]
train() client id: f_00005-8-3 loss: 0.806667  [  128/  146]
train() client id: f_00005-9-0 loss: 0.456027  [   32/  146]
train() client id: f_00005-9-1 loss: 0.426728  [   64/  146]
train() client id: f_00005-9-2 loss: 0.618197  [   96/  146]
train() client id: f_00005-9-3 loss: 0.471535  [  128/  146]
train() client id: f_00005-10-0 loss: 0.553810  [   32/  146]
train() client id: f_00005-10-1 loss: 0.416825  [   64/  146]
train() client id: f_00005-10-2 loss: 0.567349  [   96/  146]
train() client id: f_00005-10-3 loss: 0.468684  [  128/  146]
train() client id: f_00005-11-0 loss: 0.387726  [   32/  146]
train() client id: f_00005-11-1 loss: 0.494364  [   64/  146]
train() client id: f_00005-11-2 loss: 0.418398  [   96/  146]
train() client id: f_00005-11-3 loss: 0.490392  [  128/  146]
train() client id: f_00005-12-0 loss: 0.419853  [   32/  146]
train() client id: f_00005-12-1 loss: 0.409499  [   64/  146]
train() client id: f_00005-12-2 loss: 0.781507  [   96/  146]
train() client id: f_00005-12-3 loss: 0.390684  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507825  [   32/   54]
train() client id: f_00006-1-0 loss: 0.497981  [   32/   54]
train() client id: f_00006-2-0 loss: 0.489100  [   32/   54]
train() client id: f_00006-3-0 loss: 0.452952  [   32/   54]
train() client id: f_00006-4-0 loss: 0.458701  [   32/   54]
train() client id: f_00006-5-0 loss: 0.472592  [   32/   54]
train() client id: f_00006-6-0 loss: 0.491627  [   32/   54]
train() client id: f_00006-7-0 loss: 0.424839  [   32/   54]
train() client id: f_00006-8-0 loss: 0.443599  [   32/   54]
train() client id: f_00006-9-0 loss: 0.501423  [   32/   54]
train() client id: f_00006-10-0 loss: 0.444005  [   32/   54]
train() client id: f_00006-11-0 loss: 0.511532  [   32/   54]
train() client id: f_00006-12-0 loss: 0.402471  [   32/   54]
train() client id: f_00007-0-0 loss: 0.631176  [   32/  179]
train() client id: f_00007-0-1 loss: 0.664731  [   64/  179]
train() client id: f_00007-0-2 loss: 0.639272  [   96/  179]
train() client id: f_00007-0-3 loss: 0.590172  [  128/  179]
train() client id: f_00007-0-4 loss: 0.577576  [  160/  179]
train() client id: f_00007-1-0 loss: 0.590215  [   32/  179]
train() client id: f_00007-1-1 loss: 0.512575  [   64/  179]
train() client id: f_00007-1-2 loss: 0.689809  [   96/  179]
train() client id: f_00007-1-3 loss: 0.608963  [  128/  179]
train() client id: f_00007-1-4 loss: 0.784377  [  160/  179]
train() client id: f_00007-2-0 loss: 0.569414  [   32/  179]
train() client id: f_00007-2-1 loss: 0.711424  [   64/  179]
train() client id: f_00007-2-2 loss: 0.562134  [   96/  179]
train() client id: f_00007-2-3 loss: 0.634571  [  128/  179]
train() client id: f_00007-2-4 loss: 0.498140  [  160/  179]
train() client id: f_00007-3-0 loss: 0.666148  [   32/  179]
train() client id: f_00007-3-1 loss: 0.530355  [   64/  179]
train() client id: f_00007-3-2 loss: 0.555368  [   96/  179]
train() client id: f_00007-3-3 loss: 0.435037  [  128/  179]
train() client id: f_00007-3-4 loss: 0.654797  [  160/  179]
train() client id: f_00007-4-0 loss: 0.502527  [   32/  179]
train() client id: f_00007-4-1 loss: 0.460047  [   64/  179]
train() client id: f_00007-4-2 loss: 0.522369  [   96/  179]
train() client id: f_00007-4-3 loss: 0.772169  [  128/  179]
train() client id: f_00007-4-4 loss: 0.746282  [  160/  179]
train() client id: f_00007-5-0 loss: 0.622164  [   32/  179]
train() client id: f_00007-5-1 loss: 0.547075  [   64/  179]
train() client id: f_00007-5-2 loss: 0.535989  [   96/  179]
train() client id: f_00007-5-3 loss: 0.586167  [  128/  179]
train() client id: f_00007-5-4 loss: 0.630960  [  160/  179]
train() client id: f_00007-6-0 loss: 0.496337  [   32/  179]
train() client id: f_00007-6-1 loss: 0.707588  [   64/  179]
train() client id: f_00007-6-2 loss: 0.521367  [   96/  179]
train() client id: f_00007-6-3 loss: 0.495437  [  128/  179]
train() client id: f_00007-6-4 loss: 0.702556  [  160/  179]
train() client id: f_00007-7-0 loss: 0.425659  [   32/  179]
train() client id: f_00007-7-1 loss: 0.508342  [   64/  179]
train() client id: f_00007-7-2 loss: 0.577134  [   96/  179]
train() client id: f_00007-7-3 loss: 0.659181  [  128/  179]
train() client id: f_00007-7-4 loss: 0.590093  [  160/  179]
train() client id: f_00007-8-0 loss: 0.454898  [   32/  179]
train() client id: f_00007-8-1 loss: 0.418409  [   64/  179]
train() client id: f_00007-8-2 loss: 0.576142  [   96/  179]
train() client id: f_00007-8-3 loss: 0.616427  [  128/  179]
train() client id: f_00007-8-4 loss: 0.893600  [  160/  179]
train() client id: f_00007-9-0 loss: 0.773938  [   32/  179]
train() client id: f_00007-9-1 loss: 0.639850  [   64/  179]
train() client id: f_00007-9-2 loss: 0.512459  [   96/  179]
train() client id: f_00007-9-3 loss: 0.487308  [  128/  179]
train() client id: f_00007-9-4 loss: 0.516031  [  160/  179]
train() client id: f_00007-10-0 loss: 0.409767  [   32/  179]
train() client id: f_00007-10-1 loss: 0.417163  [   64/  179]
train() client id: f_00007-10-2 loss: 0.427771  [   96/  179]
train() client id: f_00007-10-3 loss: 0.606179  [  128/  179]
train() client id: f_00007-10-4 loss: 0.934437  [  160/  179]
train() client id: f_00007-11-0 loss: 0.529952  [   32/  179]
train() client id: f_00007-11-1 loss: 0.817819  [   64/  179]
train() client id: f_00007-11-2 loss: 0.525896  [   96/  179]
train() client id: f_00007-11-3 loss: 0.392257  [  128/  179]
train() client id: f_00007-11-4 loss: 0.528987  [  160/  179]
train() client id: f_00007-12-0 loss: 0.565120  [   32/  179]
train() client id: f_00007-12-1 loss: 0.712780  [   64/  179]
train() client id: f_00007-12-2 loss: 0.522020  [   96/  179]
train() client id: f_00007-12-3 loss: 0.568626  [  128/  179]
train() client id: f_00007-12-4 loss: 0.406457  [  160/  179]
train() client id: f_00008-0-0 loss: 0.693362  [   32/  130]
train() client id: f_00008-0-1 loss: 0.764959  [   64/  130]
train() client id: f_00008-0-2 loss: 0.550645  [   96/  130]
train() client id: f_00008-0-3 loss: 0.638335  [  128/  130]
train() client id: f_00008-1-0 loss: 0.619133  [   32/  130]
train() client id: f_00008-1-1 loss: 0.632224  [   64/  130]
train() client id: f_00008-1-2 loss: 0.693249  [   96/  130]
train() client id: f_00008-1-3 loss: 0.629679  [  128/  130]
train() client id: f_00008-2-0 loss: 0.580603  [   32/  130]
train() client id: f_00008-2-1 loss: 0.766185  [   64/  130]
train() client id: f_00008-2-2 loss: 0.680270  [   96/  130]
train() client id: f_00008-2-3 loss: 0.596144  [  128/  130]
train() client id: f_00008-3-0 loss: 0.704341  [   32/  130]
train() client id: f_00008-3-1 loss: 0.607249  [   64/  130]
train() client id: f_00008-3-2 loss: 0.666028  [   96/  130]
train() client id: f_00008-3-3 loss: 0.663698  [  128/  130]
train() client id: f_00008-4-0 loss: 0.748562  [   32/  130]
train() client id: f_00008-4-1 loss: 0.588414  [   64/  130]
train() client id: f_00008-4-2 loss: 0.583599  [   96/  130]
train() client id: f_00008-4-3 loss: 0.709573  [  128/  130]
train() client id: f_00008-5-0 loss: 0.734183  [   32/  130]
train() client id: f_00008-5-1 loss: 0.685335  [   64/  130]
train() client id: f_00008-5-2 loss: 0.671349  [   96/  130]
train() client id: f_00008-5-3 loss: 0.538564  [  128/  130]
train() client id: f_00008-6-0 loss: 0.608382  [   32/  130]
train() client id: f_00008-6-1 loss: 0.657389  [   64/  130]
train() client id: f_00008-6-2 loss: 0.624308  [   96/  130]
train() client id: f_00008-6-3 loss: 0.742304  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708436  [   32/  130]
train() client id: f_00008-7-1 loss: 0.587594  [   64/  130]
train() client id: f_00008-7-2 loss: 0.595704  [   96/  130]
train() client id: f_00008-7-3 loss: 0.738034  [  128/  130]
train() client id: f_00008-8-0 loss: 0.606296  [   32/  130]
train() client id: f_00008-8-1 loss: 0.662379  [   64/  130]
train() client id: f_00008-8-2 loss: 0.568268  [   96/  130]
train() client id: f_00008-8-3 loss: 0.796190  [  128/  130]
train() client id: f_00008-9-0 loss: 0.574839  [   32/  130]
train() client id: f_00008-9-1 loss: 0.735110  [   64/  130]
train() client id: f_00008-9-2 loss: 0.723830  [   96/  130]
train() client id: f_00008-9-3 loss: 0.604527  [  128/  130]
train() client id: f_00008-10-0 loss: 0.596292  [   32/  130]
train() client id: f_00008-10-1 loss: 0.612118  [   64/  130]
train() client id: f_00008-10-2 loss: 0.693509  [   96/  130]
train() client id: f_00008-10-3 loss: 0.697403  [  128/  130]
train() client id: f_00008-11-0 loss: 0.639083  [   32/  130]
train() client id: f_00008-11-1 loss: 0.623435  [   64/  130]
train() client id: f_00008-11-2 loss: 0.673256  [   96/  130]
train() client id: f_00008-11-3 loss: 0.670893  [  128/  130]
train() client id: f_00008-12-0 loss: 0.657531  [   32/  130]
train() client id: f_00008-12-1 loss: 0.697929  [   64/  130]
train() client id: f_00008-12-2 loss: 0.590329  [   96/  130]
train() client id: f_00008-12-3 loss: 0.687306  [  128/  130]
train() client id: f_00009-0-0 loss: 1.149706  [   32/  118]
train() client id: f_00009-0-1 loss: 1.038134  [   64/  118]
train() client id: f_00009-0-2 loss: 1.051629  [   96/  118]
train() client id: f_00009-1-0 loss: 0.960557  [   32/  118]
train() client id: f_00009-1-1 loss: 1.069291  [   64/  118]
train() client id: f_00009-1-2 loss: 1.159538  [   96/  118]
train() client id: f_00009-2-0 loss: 0.913144  [   32/  118]
train() client id: f_00009-2-1 loss: 1.115836  [   64/  118]
train() client id: f_00009-2-2 loss: 1.078923  [   96/  118]
train() client id: f_00009-3-0 loss: 0.950994  [   32/  118]
train() client id: f_00009-3-1 loss: 1.047010  [   64/  118]
train() client id: f_00009-3-2 loss: 0.978031  [   96/  118]
train() client id: f_00009-4-0 loss: 1.041306  [   32/  118]
train() client id: f_00009-4-1 loss: 1.017128  [   64/  118]
train() client id: f_00009-4-2 loss: 0.854906  [   96/  118]
train() client id: f_00009-5-0 loss: 1.100245  [   32/  118]
train() client id: f_00009-5-1 loss: 0.832770  [   64/  118]
train() client id: f_00009-5-2 loss: 0.921314  [   96/  118]
train() client id: f_00009-6-0 loss: 1.136126  [   32/  118]
train() client id: f_00009-6-1 loss: 0.878031  [   64/  118]
train() client id: f_00009-6-2 loss: 0.828704  [   96/  118]
train() client id: f_00009-7-0 loss: 0.933678  [   32/  118]
train() client id: f_00009-7-1 loss: 0.916870  [   64/  118]
train() client id: f_00009-7-2 loss: 0.937624  [   96/  118]
train() client id: f_00009-8-0 loss: 0.968919  [   32/  118]
train() client id: f_00009-8-1 loss: 0.930568  [   64/  118]
train() client id: f_00009-8-2 loss: 1.056990  [   96/  118]
train() client id: f_00009-9-0 loss: 0.937733  [   32/  118]
train() client id: f_00009-9-1 loss: 0.866353  [   64/  118]
train() client id: f_00009-9-2 loss: 0.963759  [   96/  118]
train() client id: f_00009-10-0 loss: 1.062524  [   32/  118]
train() client id: f_00009-10-1 loss: 0.735325  [   64/  118]
train() client id: f_00009-10-2 loss: 0.987290  [   96/  118]
train() client id: f_00009-11-0 loss: 1.041229  [   32/  118]
train() client id: f_00009-11-1 loss: 0.886467  [   64/  118]
train() client id: f_00009-11-2 loss: 0.918617  [   96/  118]
train() client id: f_00009-12-0 loss: 0.971690  [   32/  118]
train() client id: f_00009-12-1 loss: 1.012678  [   64/  118]
train() client id: f_00009-12-2 loss: 0.897119  [   96/  118]
At round 45 accuracy: 0.649867374005305
At round 45 training accuracy: 0.5935613682092555
At round 45 training loss: 0.8254184771265869
update_location
xs = [  -3.9056584     4.20031788  245.00902392   18.81129433    0.97929623
    3.95640986 -207.44319194 -186.32485185  229.66397685 -172.06087855]
ys = [ 237.5879595   220.55583871    1.32061395 -207.45517586  199.35018685
  182.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [257.80475687 242.20346954 264.63402242 231.06603988 223.02792654
 208.41465133 230.30320976 211.46542666 251.1059781  199.05013887]
dists_bs = [182.18267316 184.90021192 454.50204544 428.75958223 177.65597175
 179.12693148 180.56355047 174.54542885 434.20899579 171.02377515]
uav_gains = [6.34845528e-12 8.61260857e-12 5.51598422e-12 1.05126925e-11
 1.20199406e-11 1.50715939e-11 1.06508766e-11 1.43975841e-11
 7.26046655e-12 1.72893462e-11]
bs_gains = [5.17457027e-11 4.96442942e-11 4.00123195e-12 4.71080726e-12
 5.55227049e-11 5.42554792e-11 5.30554308e-11 5.83378396e-11
 4.54713029e-12 6.17640715e-11]
Round 46
-------------------------------
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.92415982 10.12154902  4.85601333  1.75987582 11.67148702  5.61542073
  2.1757575   6.89673295  5.09466844  4.55326949]
obj_prev = 57.668934107649186
eta_min = 1.934063127362615e-19	eta_max = 0.9372721317732582
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 13.346274362136274	eta = 0.9090909090909091
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 26.11640265875819	eta = 0.46457304443429304
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 19.646759764324543	eta = 0.617556118077178
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.475231020287687	eta = 0.6567158310241378
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40977991233988	eta = 0.6590506106332402
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40955607686498	eta = 0.6590586238034545
eta = 0.6590586238034545
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.03440512 0.07235997 0.03385901 0.01174143 0.08355529 0.03986625
 0.01474505 0.04887711 0.03549735 0.03222067]
ene_total = [1.70303953 2.89994011 1.70508731 0.81177669 3.30204155 1.7110066
 0.91978726 2.13279846 1.79829675 1.42578181]
ti_comp = [0.63680768 0.68794036 0.63078466 0.65472313 0.68956148 0.68923291
 0.65512248 0.66360668 0.62159244 0.69103952]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [6.27671845e-06 5.00349702e-05 6.09733991e-06 2.36008472e-07
 7.66753725e-05 8.33612381e-06 4.66846244e-07 1.65720103e-05
 7.23529432e-06 4.37802116e-06]
ene_total = [0.44938525 0.26462869 0.47132941 0.38387283 0.25969147 0.25839833
 0.38242583 0.35209241 0.50487161 0.25166996]
optimize_network iter = 0 obj = 3.578365787451716
eta = 0.6590586238034545
freqs = [27013744.23254671 52591749.45149608 26838799.91285678  8966713.64201406
 60585816.6007772  28920738.74380525 11253659.91650407 36826870.45355151
 28553553.02519245 23313187.49305714]
eta_min = 0.6590586238034565	eta_max = 0.6785433277513145
af = 0.005254252612989919	bf = 1.2178222333661453	zeta = 0.005779677874288912	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.43481355e-06 1.14376411e-05 1.39380888e-06 5.39498714e-08
 1.75274491e-05 1.90557908e-06 1.06717757e-07 3.78824463e-06
 1.65393723e-06 1.00078475e-06]
ene_total = [1.6846925  0.98717606 1.76700979 1.43963457 0.96585086 0.96820657
 1.43418346 1.3187243  1.89268496 0.94339011]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 1 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
eta_min = 0.6785433277513172	eta_max = 0.6785433277513145
af = 0.005187942505206725	bf = 1.2178222333661453	zeta = 0.005706736755727398	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.42967619e-06 1.12736748e-05 1.39076189e-06 5.35419898e-08
 1.72707080e-05 1.87778663e-06 1.05901756e-07 3.75246172e-06
 1.65393723e-06 9.85841782e-07]
ene_total = [1.6846918  0.98715364 1.76700937 1.43963452 0.96581577 0.96820277
 1.43418335 1.31871941 1.89268496 0.94338807]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 2 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
Done!
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.49709926e-06 4.33472348e-05 5.34747390e-06 2.05868737e-07
 6.64058034e-05 7.22008208e-06 4.07191829e-07 1.44282003e-05
 6.35938204e-06 3.79055772e-06]
ene_total = [0.01232984 0.00725442 0.01293199 0.010533   0.00711537 0.00708904
 0.01049327 0.00965887 0.01385222 0.00690495]
At round 46 energy consumption: 0.0981629748231883
At round 46 eta: 0.6785433277513145
At round 46 a_n: 12.425493895665781
At round 46 local rounds: 12.69876962905909
At round 46 global rounds: 38.653712827752926
gradient difference: 0.4051823019981384
train() client id: f_00000-0-0 loss: 1.070709  [   32/  126]
train() client id: f_00000-0-1 loss: 1.070713  [   64/  126]
train() client id: f_00000-0-2 loss: 1.286789  [   96/  126]
train() client id: f_00000-1-0 loss: 1.096523  [   32/  126]
train() client id: f_00000-1-1 loss: 1.162446  [   64/  126]
train() client id: f_00000-1-2 loss: 0.992593  [   96/  126]
train() client id: f_00000-2-0 loss: 0.899251  [   32/  126]
train() client id: f_00000-2-1 loss: 0.914470  [   64/  126]
train() client id: f_00000-2-2 loss: 1.120072  [   96/  126]
train() client id: f_00000-3-0 loss: 0.886091  [   32/  126]
train() client id: f_00000-3-1 loss: 1.044002  [   64/  126]
train() client id: f_00000-3-2 loss: 0.844736  [   96/  126]
train() client id: f_00000-4-0 loss: 1.067155  [   32/  126]
train() client id: f_00000-4-1 loss: 0.850800  [   64/  126]
train() client id: f_00000-4-2 loss: 0.866139  [   96/  126]
train() client id: f_00000-5-0 loss: 0.874615  [   32/  126]
train() client id: f_00000-5-1 loss: 1.072122  [   64/  126]
train() client id: f_00000-5-2 loss: 0.809517  [   96/  126]
train() client id: f_00000-6-0 loss: 0.952973  [   32/  126]
train() client id: f_00000-6-1 loss: 0.897397  [   64/  126]
train() client id: f_00000-6-2 loss: 0.837367  [   96/  126]
train() client id: f_00000-7-0 loss: 0.754858  [   32/  126]
train() client id: f_00000-7-1 loss: 0.941359  [   64/  126]
train() client id: f_00000-7-2 loss: 0.888378  [   96/  126]
train() client id: f_00000-8-0 loss: 0.861823  [   32/  126]
train() client id: f_00000-8-1 loss: 0.973101  [   64/  126]
train() client id: f_00000-8-2 loss: 0.825617  [   96/  126]
train() client id: f_00000-9-0 loss: 0.779049  [   32/  126]
train() client id: f_00000-9-1 loss: 0.972321  [   64/  126]
train() client id: f_00000-9-2 loss: 0.917666  [   96/  126]
train() client id: f_00000-10-0 loss: 0.899918  [   32/  126]
train() client id: f_00000-10-1 loss: 0.913910  [   64/  126]
train() client id: f_00000-10-2 loss: 0.765459  [   96/  126]
train() client id: f_00000-11-0 loss: 0.760936  [   32/  126]
train() client id: f_00000-11-1 loss: 0.893024  [   64/  126]
train() client id: f_00000-11-2 loss: 0.889133  [   96/  126]
train() client id: f_00001-0-0 loss: 0.346149  [   32/  265]
train() client id: f_00001-0-1 loss: 0.366186  [   64/  265]
train() client id: f_00001-0-2 loss: 0.458177  [   96/  265]
train() client id: f_00001-0-3 loss: 0.584395  [  128/  265]
train() client id: f_00001-0-4 loss: 0.331556  [  160/  265]
train() client id: f_00001-0-5 loss: 0.386474  [  192/  265]
train() client id: f_00001-0-6 loss: 0.314455  [  224/  265]
train() client id: f_00001-0-7 loss: 0.356232  [  256/  265]
train() client id: f_00001-1-0 loss: 0.567883  [   32/  265]
train() client id: f_00001-1-1 loss: 0.314876  [   64/  265]
train() client id: f_00001-1-2 loss: 0.480858  [   96/  265]
train() client id: f_00001-1-3 loss: 0.371579  [  128/  265]
train() client id: f_00001-1-4 loss: 0.471570  [  160/  265]
train() client id: f_00001-1-5 loss: 0.289232  [  192/  265]
train() client id: f_00001-1-6 loss: 0.281766  [  224/  265]
train() client id: f_00001-1-7 loss: 0.416136  [  256/  265]
train() client id: f_00001-2-0 loss: 0.317368  [   32/  265]
train() client id: f_00001-2-1 loss: 0.515617  [   64/  265]
train() client id: f_00001-2-2 loss: 0.410512  [   96/  265]
train() client id: f_00001-2-3 loss: 0.313342  [  128/  265]
train() client id: f_00001-2-4 loss: 0.296806  [  160/  265]
train() client id: f_00001-2-5 loss: 0.395860  [  192/  265]
train() client id: f_00001-2-6 loss: 0.475880  [  224/  265]
train() client id: f_00001-2-7 loss: 0.426727  [  256/  265]
train() client id: f_00001-3-0 loss: 0.472000  [   32/  265]
train() client id: f_00001-3-1 loss: 0.406181  [   64/  265]
train() client id: f_00001-3-2 loss: 0.332874  [   96/  265]
train() client id: f_00001-3-3 loss: 0.377977  [  128/  265]
train() client id: f_00001-3-4 loss: 0.380983  [  160/  265]
train() client id: f_00001-3-5 loss: 0.281553  [  192/  265]
train() client id: f_00001-3-6 loss: 0.441455  [  224/  265]
train() client id: f_00001-3-7 loss: 0.400299  [  256/  265]
train() client id: f_00001-4-0 loss: 0.403419  [   32/  265]
train() client id: f_00001-4-1 loss: 0.646768  [   64/  265]
train() client id: f_00001-4-2 loss: 0.413107  [   96/  265]
train() client id: f_00001-4-3 loss: 0.288025  [  128/  265]
train() client id: f_00001-4-4 loss: 0.293456  [  160/  265]
train() client id: f_00001-4-5 loss: 0.341964  [  192/  265]
train() client id: f_00001-4-6 loss: 0.300516  [  224/  265]
train() client id: f_00001-4-7 loss: 0.366634  [  256/  265]
train() client id: f_00001-5-0 loss: 0.371738  [   32/  265]
train() client id: f_00001-5-1 loss: 0.430602  [   64/  265]
train() client id: f_00001-5-2 loss: 0.280134  [   96/  265]
train() client id: f_00001-5-3 loss: 0.330846  [  128/  265]
train() client id: f_00001-5-4 loss: 0.306603  [  160/  265]
train() client id: f_00001-5-5 loss: 0.297244  [  192/  265]
train() client id: f_00001-5-6 loss: 0.390540  [  224/  265]
train() client id: f_00001-5-7 loss: 0.544295  [  256/  265]
train() client id: f_00001-6-0 loss: 0.411815  [   32/  265]
train() client id: f_00001-6-1 loss: 0.459911  [   64/  265]
train() client id: f_00001-6-2 loss: 0.395711  [   96/  265]
train() client id: f_00001-6-3 loss: 0.343143  [  128/  265]
train() client id: f_00001-6-4 loss: 0.285133  [  160/  265]
train() client id: f_00001-6-5 loss: 0.417884  [  192/  265]
train() client id: f_00001-6-6 loss: 0.376401  [  224/  265]
train() client id: f_00001-6-7 loss: 0.318498  [  256/  265]
train() client id: f_00001-7-0 loss: 0.332887  [   32/  265]
train() client id: f_00001-7-1 loss: 0.267628  [   64/  265]
train() client id: f_00001-7-2 loss: 0.482169  [   96/  265]
train() client id: f_00001-7-3 loss: 0.360355  [  128/  265]
train() client id: f_00001-7-4 loss: 0.439548  [  160/  265]
train() client id: f_00001-7-5 loss: 0.420692  [  192/  265]
train() client id: f_00001-7-6 loss: 0.358007  [  224/  265]
train() client id: f_00001-7-7 loss: 0.322226  [  256/  265]
train() client id: f_00001-8-0 loss: 0.420426  [   32/  265]
train() client id: f_00001-8-1 loss: 0.496542  [   64/  265]
train() client id: f_00001-8-2 loss: 0.409612  [   96/  265]
train() client id: f_00001-8-3 loss: 0.407959  [  128/  265]
train() client id: f_00001-8-4 loss: 0.295662  [  160/  265]
train() client id: f_00001-8-5 loss: 0.392553  [  192/  265]
train() client id: f_00001-8-6 loss: 0.252192  [  224/  265]
train() client id: f_00001-8-7 loss: 0.310556  [  256/  265]
train() client id: f_00001-9-0 loss: 0.527524  [   32/  265]
train() client id: f_00001-9-1 loss: 0.400159  [   64/  265]
train() client id: f_00001-9-2 loss: 0.272363  [   96/  265]
train() client id: f_00001-9-3 loss: 0.369018  [  128/  265]
train() client id: f_00001-9-4 loss: 0.344104  [  160/  265]
train() client id: f_00001-9-5 loss: 0.312651  [  192/  265]
train() client id: f_00001-9-6 loss: 0.424988  [  224/  265]
train() client id: f_00001-9-7 loss: 0.266605  [  256/  265]
train() client id: f_00001-10-0 loss: 0.348016  [   32/  265]
train() client id: f_00001-10-1 loss: 0.312310  [   64/  265]
train() client id: f_00001-10-2 loss: 0.314944  [   96/  265]
train() client id: f_00001-10-3 loss: 0.402507  [  128/  265]
train() client id: f_00001-10-4 loss: 0.354361  [  160/  265]
train() client id: f_00001-10-5 loss: 0.481379  [  192/  265]
train() client id: f_00001-10-6 loss: 0.357479  [  224/  265]
train() client id: f_00001-10-7 loss: 0.390237  [  256/  265]
train() client id: f_00001-11-0 loss: 0.276013  [   32/  265]
train() client id: f_00001-11-1 loss: 0.338371  [   64/  265]
train() client id: f_00001-11-2 loss: 0.315508  [   96/  265]
train() client id: f_00001-11-3 loss: 0.349479  [  128/  265]
train() client id: f_00001-11-4 loss: 0.404055  [  160/  265]
train() client id: f_00001-11-5 loss: 0.414056  [  192/  265]
train() client id: f_00001-11-6 loss: 0.395545  [  224/  265]
train() client id: f_00001-11-7 loss: 0.393102  [  256/  265]
train() client id: f_00002-0-0 loss: 1.385426  [   32/  124]
train() client id: f_00002-0-1 loss: 1.203423  [   64/  124]
train() client id: f_00002-0-2 loss: 0.968922  [   96/  124]
train() client id: f_00002-1-0 loss: 1.026419  [   32/  124]
train() client id: f_00002-1-1 loss: 1.173258  [   64/  124]
train() client id: f_00002-1-2 loss: 1.257598  [   96/  124]
train() client id: f_00002-2-0 loss: 1.201513  [   32/  124]
train() client id: f_00002-2-1 loss: 1.071794  [   64/  124]
train() client id: f_00002-2-2 loss: 0.933216  [   96/  124]
train() client id: f_00002-3-0 loss: 1.182863  [   32/  124]
train() client id: f_00002-3-1 loss: 1.046954  [   64/  124]
train() client id: f_00002-3-2 loss: 1.002291  [   96/  124]
train() client id: f_00002-4-0 loss: 0.979809  [   32/  124]
train() client id: f_00002-4-1 loss: 1.141795  [   64/  124]
train() client id: f_00002-4-2 loss: 0.951728  [   96/  124]
train() client id: f_00002-5-0 loss: 1.017139  [   32/  124]
train() client id: f_00002-5-1 loss: 1.023093  [   64/  124]
train() client id: f_00002-5-2 loss: 1.080965  [   96/  124]
train() client id: f_00002-6-0 loss: 0.929111  [   32/  124]
train() client id: f_00002-6-1 loss: 0.932415  [   64/  124]
train() client id: f_00002-6-2 loss: 1.091736  [   96/  124]
train() client id: f_00002-7-0 loss: 0.837173  [   32/  124]
train() client id: f_00002-7-1 loss: 1.002941  [   64/  124]
train() client id: f_00002-7-2 loss: 0.979888  [   96/  124]
train() client id: f_00002-8-0 loss: 1.199169  [   32/  124]
train() client id: f_00002-8-1 loss: 0.867118  [   64/  124]
train() client id: f_00002-8-2 loss: 0.935469  [   96/  124]
train() client id: f_00002-9-0 loss: 0.895930  [   32/  124]
train() client id: f_00002-9-1 loss: 0.980013  [   64/  124]
train() client id: f_00002-9-2 loss: 0.928221  [   96/  124]
train() client id: f_00002-10-0 loss: 0.675373  [   32/  124]
train() client id: f_00002-10-1 loss: 1.018058  [   64/  124]
train() client id: f_00002-10-2 loss: 1.304139  [   96/  124]
train() client id: f_00002-11-0 loss: 0.966626  [   32/  124]
train() client id: f_00002-11-1 loss: 1.109693  [   64/  124]
train() client id: f_00002-11-2 loss: 0.765070  [   96/  124]
train() client id: f_00003-0-0 loss: 0.828445  [   32/   43]
train() client id: f_00003-1-0 loss: 0.604971  [   32/   43]
train() client id: f_00003-2-0 loss: 0.564724  [   32/   43]
train() client id: f_00003-3-0 loss: 0.500700  [   32/   43]
train() client id: f_00003-4-0 loss: 0.652305  [   32/   43]
train() client id: f_00003-5-0 loss: 0.665784  [   32/   43]
train() client id: f_00003-6-0 loss: 0.549537  [   32/   43]
train() client id: f_00003-7-0 loss: 0.610453  [   32/   43]
train() client id: f_00003-8-0 loss: 0.487164  [   32/   43]
train() client id: f_00003-9-0 loss: 0.615645  [   32/   43]
train() client id: f_00003-10-0 loss: 0.635023  [   32/   43]
train() client id: f_00003-11-0 loss: 0.639901  [   32/   43]
train() client id: f_00004-0-0 loss: 0.873548  [   32/  306]
train() client id: f_00004-0-1 loss: 0.868044  [   64/  306]
train() client id: f_00004-0-2 loss: 0.907766  [   96/  306]
train() client id: f_00004-0-3 loss: 0.781877  [  128/  306]
train() client id: f_00004-0-4 loss: 0.907005  [  160/  306]
train() client id: f_00004-0-5 loss: 0.831800  [  192/  306]
train() client id: f_00004-0-6 loss: 0.853646  [  224/  306]
train() client id: f_00004-0-7 loss: 0.716184  [  256/  306]
train() client id: f_00004-0-8 loss: 0.792826  [  288/  306]
train() client id: f_00004-1-0 loss: 0.773236  [   32/  306]
train() client id: f_00004-1-1 loss: 0.739932  [   64/  306]
train() client id: f_00004-1-2 loss: 0.876192  [   96/  306]
train() client id: f_00004-1-3 loss: 0.767508  [  128/  306]
train() client id: f_00004-1-4 loss: 0.845166  [  160/  306]
train() client id: f_00004-1-5 loss: 0.888146  [  192/  306]
train() client id: f_00004-1-6 loss: 0.841999  [  224/  306]
train() client id: f_00004-1-7 loss: 0.854761  [  256/  306]
train() client id: f_00004-1-8 loss: 0.848694  [  288/  306]
train() client id: f_00004-2-0 loss: 0.975555  [   32/  306]
train() client id: f_00004-2-1 loss: 0.921308  [   64/  306]
train() client id: f_00004-2-2 loss: 0.855301  [   96/  306]
train() client id: f_00004-2-3 loss: 0.816021  [  128/  306]
train() client id: f_00004-2-4 loss: 0.888409  [  160/  306]
train() client id: f_00004-2-5 loss: 0.761450  [  192/  306]
train() client id: f_00004-2-6 loss: 0.614584  [  224/  306]
train() client id: f_00004-2-7 loss: 0.783002  [  256/  306]
train() client id: f_00004-2-8 loss: 0.898369  [  288/  306]
train() client id: f_00004-3-0 loss: 0.783122  [   32/  306]
train() client id: f_00004-3-1 loss: 0.943813  [   64/  306]
train() client id: f_00004-3-2 loss: 0.879883  [   96/  306]
train() client id: f_00004-3-3 loss: 0.839372  [  128/  306]
train() client id: f_00004-3-4 loss: 0.882416  [  160/  306]
train() client id: f_00004-3-5 loss: 0.749584  [  192/  306]
train() client id: f_00004-3-6 loss: 0.790417  [  224/  306]
train() client id: f_00004-3-7 loss: 0.739315  [  256/  306]
train() client id: f_00004-3-8 loss: 0.725215  [  288/  306]
train() client id: f_00004-4-0 loss: 0.754986  [   32/  306]
train() client id: f_00004-4-1 loss: 0.830930  [   64/  306]
train() client id: f_00004-4-2 loss: 0.813170  [   96/  306]
train() client id: f_00004-4-3 loss: 0.838061  [  128/  306]
train() client id: f_00004-4-4 loss: 0.728723  [  160/  306]
train() client id: f_00004-4-5 loss: 0.745673  [  192/  306]
train() client id: f_00004-4-6 loss: 0.815479  [  224/  306]
train() client id: f_00004-4-7 loss: 0.954966  [  256/  306]
train() client id: f_00004-4-8 loss: 0.894418  [  288/  306]
train() client id: f_00004-5-0 loss: 0.812695  [   32/  306]
train() client id: f_00004-5-1 loss: 0.792058  [   64/  306]
train() client id: f_00004-5-2 loss: 0.722397  [   96/  306]
train() client id: f_00004-5-3 loss: 0.934982  [  128/  306]
train() client id: f_00004-5-4 loss: 0.860498  [  160/  306]
train() client id: f_00004-5-5 loss: 0.796305  [  192/  306]
train() client id: f_00004-5-6 loss: 0.794196  [  224/  306]
train() client id: f_00004-5-7 loss: 0.796180  [  256/  306]
train() client id: f_00004-5-8 loss: 0.882965  [  288/  306]
train() client id: f_00004-6-0 loss: 0.822133  [   32/  306]
train() client id: f_00004-6-1 loss: 0.844327  [   64/  306]
train() client id: f_00004-6-2 loss: 0.805843  [   96/  306]
train() client id: f_00004-6-3 loss: 0.730108  [  128/  306]
train() client id: f_00004-6-4 loss: 0.738544  [  160/  306]
train() client id: f_00004-6-5 loss: 0.910488  [  192/  306]
train() client id: f_00004-6-6 loss: 0.802341  [  224/  306]
train() client id: f_00004-6-7 loss: 0.810536  [  256/  306]
train() client id: f_00004-6-8 loss: 0.949151  [  288/  306]
train() client id: f_00004-7-0 loss: 0.899897  [   32/  306]
train() client id: f_00004-7-1 loss: 0.933918  [   64/  306]
train() client id: f_00004-7-2 loss: 0.885339  [   96/  306]
train() client id: f_00004-7-3 loss: 0.755012  [  128/  306]
train() client id: f_00004-7-4 loss: 0.856913  [  160/  306]
train() client id: f_00004-7-5 loss: 0.823116  [  192/  306]
train() client id: f_00004-7-6 loss: 0.718899  [  224/  306]
train() client id: f_00004-7-7 loss: 0.749435  [  256/  306]
train() client id: f_00004-7-8 loss: 0.755747  [  288/  306]
train() client id: f_00004-8-0 loss: 1.014294  [   32/  306]
train() client id: f_00004-8-1 loss: 0.739156  [   64/  306]
train() client id: f_00004-8-2 loss: 0.738772  [   96/  306]
train() client id: f_00004-8-3 loss: 0.895137  [  128/  306]
train() client id: f_00004-8-4 loss: 0.757407  [  160/  306]
train() client id: f_00004-8-5 loss: 0.809564  [  192/  306]
train() client id: f_00004-8-6 loss: 0.849594  [  224/  306]
train() client id: f_00004-8-7 loss: 0.793865  [  256/  306]
train() client id: f_00004-8-8 loss: 0.801459  [  288/  306]
train() client id: f_00004-9-0 loss: 0.688530  [   32/  306]
train() client id: f_00004-9-1 loss: 0.924269  [   64/  306]
train() client id: f_00004-9-2 loss: 0.836271  [   96/  306]
train() client id: f_00004-9-3 loss: 0.936695  [  128/  306]
train() client id: f_00004-9-4 loss: 0.788662  [  160/  306]
train() client id: f_00004-9-5 loss: 0.690402  [  192/  306]
train() client id: f_00004-9-6 loss: 0.850996  [  224/  306]
train() client id: f_00004-9-7 loss: 0.789088  [  256/  306]
train() client id: f_00004-9-8 loss: 0.788738  [  288/  306]
train() client id: f_00004-10-0 loss: 0.842384  [   32/  306]
train() client id: f_00004-10-1 loss: 0.815335  [   64/  306]
train() client id: f_00004-10-2 loss: 0.795268  [   96/  306]
train() client id: f_00004-10-3 loss: 0.785899  [  128/  306]
train() client id: f_00004-10-4 loss: 0.856500  [  160/  306]
train() client id: f_00004-10-5 loss: 0.751072  [  192/  306]
train() client id: f_00004-10-6 loss: 0.844808  [  224/  306]
train() client id: f_00004-10-7 loss: 0.829381  [  256/  306]
train() client id: f_00004-10-8 loss: 0.809551  [  288/  306]
train() client id: f_00004-11-0 loss: 0.923088  [   32/  306]
train() client id: f_00004-11-1 loss: 0.852667  [   64/  306]
train() client id: f_00004-11-2 loss: 0.782581  [   96/  306]
train() client id: f_00004-11-3 loss: 0.731790  [  128/  306]
train() client id: f_00004-11-4 loss: 0.816532  [  160/  306]
train() client id: f_00004-11-5 loss: 0.751116  [  192/  306]
train() client id: f_00004-11-6 loss: 0.787187  [  224/  306]
train() client id: f_00004-11-7 loss: 0.892206  [  256/  306]
train() client id: f_00004-11-8 loss: 0.722221  [  288/  306]
train() client id: f_00005-0-0 loss: 0.758179  [   32/  146]
train() client id: f_00005-0-1 loss: 0.652975  [   64/  146]
train() client id: f_00005-0-2 loss: 0.850713  [   96/  146]
train() client id: f_00005-0-3 loss: 0.588002  [  128/  146]
train() client id: f_00005-1-0 loss: 0.751803  [   32/  146]
train() client id: f_00005-1-1 loss: 0.890623  [   64/  146]
train() client id: f_00005-1-2 loss: 0.653728  [   96/  146]
train() client id: f_00005-1-3 loss: 0.545895  [  128/  146]
train() client id: f_00005-2-0 loss: 0.676010  [   32/  146]
train() client id: f_00005-2-1 loss: 0.729992  [   64/  146]
train() client id: f_00005-2-2 loss: 0.797774  [   96/  146]
train() client id: f_00005-2-3 loss: 0.640333  [  128/  146]
train() client id: f_00005-3-0 loss: 0.658958  [   32/  146]
train() client id: f_00005-3-1 loss: 1.000163  [   64/  146]
train() client id: f_00005-3-2 loss: 0.488857  [   96/  146]
train() client id: f_00005-3-3 loss: 0.678096  [  128/  146]
train() client id: f_00005-4-0 loss: 0.692479  [   32/  146]
train() client id: f_00005-4-1 loss: 0.857326  [   64/  146]
train() client id: f_00005-4-2 loss: 0.595218  [   96/  146]
train() client id: f_00005-4-3 loss: 0.479721  [  128/  146]
train() client id: f_00005-5-0 loss: 0.525616  [   32/  146]
train() client id: f_00005-5-1 loss: 0.730185  [   64/  146]
train() client id: f_00005-5-2 loss: 0.570184  [   96/  146]
train() client id: f_00005-5-3 loss: 1.003734  [  128/  146]
train() client id: f_00005-6-0 loss: 0.769789  [   32/  146]
train() client id: f_00005-6-1 loss: 0.820256  [   64/  146]
train() client id: f_00005-6-2 loss: 0.539365  [   96/  146]
train() client id: f_00005-6-3 loss: 0.720412  [  128/  146]
train() client id: f_00005-7-0 loss: 0.560562  [   32/  146]
train() client id: f_00005-7-1 loss: 0.883495  [   64/  146]
train() client id: f_00005-7-2 loss: 0.567234  [   96/  146]
train() client id: f_00005-7-3 loss: 0.648233  [  128/  146]
train() client id: f_00005-8-0 loss: 0.628171  [   32/  146]
train() client id: f_00005-8-1 loss: 0.637964  [   64/  146]
train() client id: f_00005-8-2 loss: 0.756202  [   96/  146]
train() client id: f_00005-8-3 loss: 0.769012  [  128/  146]
train() client id: f_00005-9-0 loss: 0.871689  [   32/  146]
train() client id: f_00005-9-1 loss: 0.611840  [   64/  146]
train() client id: f_00005-9-2 loss: 0.665198  [   96/  146]
train() client id: f_00005-9-3 loss: 0.770941  [  128/  146]
train() client id: f_00005-10-0 loss: 0.897314  [   32/  146]
train() client id: f_00005-10-1 loss: 0.708743  [   64/  146]
train() client id: f_00005-10-2 loss: 0.758010  [   96/  146]
train() client id: f_00005-10-3 loss: 0.448265  [  128/  146]
train() client id: f_00005-11-0 loss: 0.495941  [   32/  146]
train() client id: f_00005-11-1 loss: 0.819846  [   64/  146]
train() client id: f_00005-11-2 loss: 0.854315  [   96/  146]
train() client id: f_00005-11-3 loss: 0.727304  [  128/  146]
train() client id: f_00006-0-0 loss: 0.618467  [   32/   54]
train() client id: f_00006-1-0 loss: 0.582226  [   32/   54]
train() client id: f_00006-2-0 loss: 0.550611  [   32/   54]
train() client id: f_00006-3-0 loss: 0.582834  [   32/   54]
train() client id: f_00006-4-0 loss: 0.602261  [   32/   54]
train() client id: f_00006-5-0 loss: 0.511463  [   32/   54]
train() client id: f_00006-6-0 loss: 0.621195  [   32/   54]
train() client id: f_00006-7-0 loss: 0.621535  [   32/   54]
train() client id: f_00006-8-0 loss: 0.537312  [   32/   54]
train() client id: f_00006-9-0 loss: 0.512596  [   32/   54]
train() client id: f_00006-10-0 loss: 0.551493  [   32/   54]
train() client id: f_00006-11-0 loss: 0.545618  [   32/   54]
train() client id: f_00007-0-0 loss: 0.605249  [   32/  179]
train() client id: f_00007-0-1 loss: 0.797871  [   64/  179]
train() client id: f_00007-0-2 loss: 0.756597  [   96/  179]
train() client id: f_00007-0-3 loss: 0.774867  [  128/  179]
train() client id: f_00007-0-4 loss: 0.629621  [  160/  179]
train() client id: f_00007-1-0 loss: 0.727156  [   32/  179]
train() client id: f_00007-1-1 loss: 0.771953  [   64/  179]
train() client id: f_00007-1-2 loss: 0.694869  [   96/  179]
train() client id: f_00007-1-3 loss: 0.745164  [  128/  179]
train() client id: f_00007-1-4 loss: 0.637427  [  160/  179]
train() client id: f_00007-2-0 loss: 0.624745  [   32/  179]
train() client id: f_00007-2-1 loss: 0.701000  [   64/  179]
train() client id: f_00007-2-2 loss: 0.580828  [   96/  179]
train() client id: f_00007-2-3 loss: 0.848270  [  128/  179]
train() client id: f_00007-2-4 loss: 0.682514  [  160/  179]
train() client id: f_00007-3-0 loss: 0.749477  [   32/  179]
train() client id: f_00007-3-1 loss: 0.855206  [   64/  179]
train() client id: f_00007-3-2 loss: 0.549391  [   96/  179]
train() client id: f_00007-3-3 loss: 0.642898  [  128/  179]
train() client id: f_00007-3-4 loss: 0.613473  [  160/  179]
train() client id: f_00007-4-0 loss: 0.557754  [   32/  179]
train() client id: f_00007-4-1 loss: 0.998795  [   64/  179]
train() client id: f_00007-4-2 loss: 0.530940  [   96/  179]
train() client id: f_00007-4-3 loss: 0.619948  [  128/  179]
train() client id: f_00007-4-4 loss: 0.613054  [  160/  179]
train() client id: f_00007-5-0 loss: 0.740641  [   32/  179]
train() client id: f_00007-5-1 loss: 0.766542  [   64/  179]
train() client id: f_00007-5-2 loss: 0.651610  [   96/  179]
train() client id: f_00007-5-3 loss: 0.635748  [  128/  179]
train() client id: f_00007-5-4 loss: 0.544658  [  160/  179]
train() client id: f_00007-6-0 loss: 0.715217  [   32/  179]
train() client id: f_00007-6-1 loss: 0.777819  [   64/  179]
train() client id: f_00007-6-2 loss: 0.626107  [   96/  179]
train() client id: f_00007-6-3 loss: 0.670199  [  128/  179]
train() client id: f_00007-6-4 loss: 0.589524  [  160/  179]
train() client id: f_00007-7-0 loss: 0.666301  [   32/  179]
train() client id: f_00007-7-1 loss: 0.521629  [   64/  179]
train() client id: f_00007-7-2 loss: 0.603918  [   96/  179]
train() client id: f_00007-7-3 loss: 0.691817  [  128/  179]
train() client id: f_00007-7-4 loss: 0.888562  [  160/  179]
train() client id: f_00007-8-0 loss: 0.517785  [   32/  179]
train() client id: f_00007-8-1 loss: 0.513265  [   64/  179]
train() client id: f_00007-8-2 loss: 0.716854  [   96/  179]
train() client id: f_00007-8-3 loss: 0.767523  [  128/  179]
train() client id: f_00007-8-4 loss: 0.797909  [  160/  179]
train() client id: f_00007-9-0 loss: 0.594057  [   32/  179]
train() client id: f_00007-9-1 loss: 0.812403  [   64/  179]
train() client id: f_00007-9-2 loss: 0.711009  [   96/  179]
train() client id: f_00007-9-3 loss: 0.617186  [  128/  179]
train() client id: f_00007-9-4 loss: 0.626809  [  160/  179]
train() client id: f_00007-10-0 loss: 0.846881  [   32/  179]
train() client id: f_00007-10-1 loss: 0.645570  [   64/  179]
train() client id: f_00007-10-2 loss: 0.692194  [   96/  179]
train() client id: f_00007-10-3 loss: 0.592149  [  128/  179]
train() client id: f_00007-10-4 loss: 0.595534  [  160/  179]
train() client id: f_00007-11-0 loss: 0.519612  [   32/  179]
train() client id: f_00007-11-1 loss: 0.908453  [   64/  179]
train() client id: f_00007-11-2 loss: 0.615999  [   96/  179]
train() client id: f_00007-11-3 loss: 0.744439  [  128/  179]
train() client id: f_00007-11-4 loss: 0.561391  [  160/  179]
train() client id: f_00008-0-0 loss: 0.686599  [   32/  130]
train() client id: f_00008-0-1 loss: 0.732182  [   64/  130]
train() client id: f_00008-0-2 loss: 0.616733  [   96/  130]
train() client id: f_00008-0-3 loss: 0.793706  [  128/  130]
train() client id: f_00008-1-0 loss: 0.637836  [   32/  130]
train() client id: f_00008-1-1 loss: 0.775865  [   64/  130]
train() client id: f_00008-1-2 loss: 0.641549  [   96/  130]
train() client id: f_00008-1-3 loss: 0.736369  [  128/  130]
train() client id: f_00008-2-0 loss: 0.666662  [   32/  130]
train() client id: f_00008-2-1 loss: 0.816716  [   64/  130]
train() client id: f_00008-2-2 loss: 0.662664  [   96/  130]
train() client id: f_00008-2-3 loss: 0.685569  [  128/  130]
train() client id: f_00008-3-0 loss: 0.773010  [   32/  130]
train() client id: f_00008-3-1 loss: 0.649288  [   64/  130]
train() client id: f_00008-3-2 loss: 0.652458  [   96/  130]
train() client id: f_00008-3-3 loss: 0.753382  [  128/  130]
train() client id: f_00008-4-0 loss: 0.675164  [   32/  130]
train() client id: f_00008-4-1 loss: 0.628046  [   64/  130]
train() client id: f_00008-4-2 loss: 0.753679  [   96/  130]
train() client id: f_00008-4-3 loss: 0.734223  [  128/  130]
train() client id: f_00008-5-0 loss: 0.581242  [   32/  130]
train() client id: f_00008-5-1 loss: 0.685616  [   64/  130]
train() client id: f_00008-5-2 loss: 0.766895  [   96/  130]
train() client id: f_00008-5-3 loss: 0.775668  [  128/  130]
train() client id: f_00008-6-0 loss: 0.632711  [   32/  130]
train() client id: f_00008-6-1 loss: 0.750533  [   64/  130]
train() client id: f_00008-6-2 loss: 0.657411  [   96/  130]
train() client id: f_00008-6-3 loss: 0.776626  [  128/  130]
train() client id: f_00008-7-0 loss: 0.629994  [   32/  130]
train() client id: f_00008-7-1 loss: 0.700726  [   64/  130]
train() client id: f_00008-7-2 loss: 0.808056  [   96/  130]
train() client id: f_00008-7-3 loss: 0.676748  [  128/  130]
train() client id: f_00008-8-0 loss: 0.712035  [   32/  130]
train() client id: f_00008-8-1 loss: 0.633469  [   64/  130]
train() client id: f_00008-8-2 loss: 0.673541  [   96/  130]
train() client id: f_00008-8-3 loss: 0.779770  [  128/  130]
train() client id: f_00008-9-0 loss: 0.711741  [   32/  130]
train() client id: f_00008-9-1 loss: 0.683662  [   64/  130]
train() client id: f_00008-9-2 loss: 0.642618  [   96/  130]
train() client id: f_00008-9-3 loss: 0.777136  [  128/  130]
train() client id: f_00008-10-0 loss: 0.748082  [   32/  130]
train() client id: f_00008-10-1 loss: 0.664765  [   64/  130]
train() client id: f_00008-10-2 loss: 0.728884  [   96/  130]
train() client id: f_00008-10-3 loss: 0.648645  [  128/  130]
train() client id: f_00008-11-0 loss: 0.754582  [   32/  130]
train() client id: f_00008-11-1 loss: 0.701801  [   64/  130]
train() client id: f_00008-11-2 loss: 0.622506  [   96/  130]
train() client id: f_00008-11-3 loss: 0.694209  [  128/  130]
train() client id: f_00009-0-0 loss: 1.073021  [   32/  118]
train() client id: f_00009-0-1 loss: 1.155649  [   64/  118]
train() client id: f_00009-0-2 loss: 1.043672  [   96/  118]
train() client id: f_00009-1-0 loss: 1.045925  [   32/  118]
train() client id: f_00009-1-1 loss: 1.080551  [   64/  118]
train() client id: f_00009-1-2 loss: 0.976942  [   96/  118]
train() client id: f_00009-2-0 loss: 1.011001  [   32/  118]
train() client id: f_00009-2-1 loss: 1.018694  [   64/  118]
train() client id: f_00009-2-2 loss: 0.980721  [   96/  118]
train() client id: f_00009-3-0 loss: 0.946637  [   32/  118]
train() client id: f_00009-3-1 loss: 1.036837  [   64/  118]
train() client id: f_00009-3-2 loss: 1.010025  [   96/  118]
train() client id: f_00009-4-0 loss: 1.062746  [   32/  118]
train() client id: f_00009-4-1 loss: 0.824182  [   64/  118]
train() client id: f_00009-4-2 loss: 0.856562  [   96/  118]
train() client id: f_00009-5-0 loss: 0.874444  [   32/  118]
train() client id: f_00009-5-1 loss: 0.806332  [   64/  118]
train() client id: f_00009-5-2 loss: 0.968679  [   96/  118]
train() client id: f_00009-6-0 loss: 0.785102  [   32/  118]
train() client id: f_00009-6-1 loss: 0.771958  [   64/  118]
train() client id: f_00009-6-2 loss: 0.949358  [   96/  118]
train() client id: f_00009-7-0 loss: 0.845638  [   32/  118]
train() client id: f_00009-7-1 loss: 0.764277  [   64/  118]
train() client id: f_00009-7-2 loss: 0.910300  [   96/  118]
train() client id: f_00009-8-0 loss: 0.926097  [   32/  118]
train() client id: f_00009-8-1 loss: 0.808280  [   64/  118]
train() client id: f_00009-8-2 loss: 0.767674  [   96/  118]
train() client id: f_00009-9-0 loss: 0.815709  [   32/  118]
train() client id: f_00009-9-1 loss: 0.883035  [   64/  118]
train() client id: f_00009-9-2 loss: 0.881812  [   96/  118]
train() client id: f_00009-10-0 loss: 0.768154  [   32/  118]
train() client id: f_00009-10-1 loss: 0.902980  [   64/  118]
train() client id: f_00009-10-2 loss: 0.892908  [   96/  118]
train() client id: f_00009-11-0 loss: 0.788543  [   32/  118]
train() client id: f_00009-11-1 loss: 0.884546  [   64/  118]
train() client id: f_00009-11-2 loss: 0.758821  [   96/  118]
At round 46 accuracy: 0.6445623342175066
At round 46 training accuracy: 0.5935613682092555
At round 46 training loss: 0.8257419715839732
update_location
xs = [  -3.9056584     4.20031788  250.00902392   18.81129433    0.97929623
    3.95640986 -212.44319194 -191.32485185  234.66397685 -177.06087855]
ys = [ 242.5879595   225.55583871    1.32061395 -212.45517586  204.35018685
  187.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [262.41983969 246.76523063 269.26985732 235.56541882 227.50814905
 212.81402303 234.81695071 215.88393917 255.68701962 203.38772473]
dists_bs = [183.9603381  186.19518456 459.12656158 433.22549684 178.41005064
 179.41460111 181.52858639 174.94100506 438.8725234  171.01093665]
uav_gains = [5.77502052e-12 7.90037418e-12 5.00729947e-12 9.71837173e-12
 1.11658603e-11 1.41063225e-11 9.84803495e-12 1.34576919e-11
 6.62667355e-12 1.62319850e-11]
bs_gains = [5.03577512e-11 4.86835724e-11 3.88940630e-12 4.57609321e-12
 5.48681098e-11 5.40122526e-11 5.22694600e-11 5.79692334e-11
 4.41312865e-12 6.17770556e-11]
Round 47
-------------------------------
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.7932913   9.84287299  4.72759004  1.71436679 11.34993316  5.46069336
  2.11875437  6.70878283  4.95608411  4.42774153]
obj_prev = 56.1001104698108
eta_min = 5.969830954362688e-20	eta_max = 0.9382327126425405
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 12.978344451772108	eta = 0.9090909090909091
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 25.625037120999544	eta = 0.4604284044719558
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 19.192856434060143	eta = 0.6147336638864522
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 18.0285169675076	eta = 0.6544351361468406
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.963027394397052	eta = 0.6568210745943969
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.962800013672382	eta = 0.6568293889135345
eta = 0.6568293889135345
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.0346847  0.07294799 0.03413415 0.01183684 0.08423428 0.04019021
 0.01486487 0.0492743  0.03578581 0.0324825 ]
ene_total = [1.66925633 2.82340307 1.67271292 0.79630461 3.21465228 1.66466235
 0.90143074 2.08079959 1.75289976 1.38667837]
ti_comp = [0.65804615 0.71288711 0.65155892 0.67749658 0.71463044 0.71440598
 0.67791976 0.68705877 0.6453014  0.71627974]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [6.02256253e-06 4.77395682e-05 5.85517725e-06 2.25825789e-07
 7.31448314e-05 7.94970091e-06 4.46690869e-07 1.58399461e-05
 6.87839679e-06 4.17506343e-06]
ene_total = [0.44822894 0.2566042  0.4710644  0.37954037 0.25136048 0.24985529
 0.37805813 0.34642191 0.493133   0.24312494]
optimize_network iter = 0 obj = 3.5173916587449052
eta = 0.6568293889135345
freqs = [26354310.22875877 51163774.23769676 26194218.65211249  8735722.05600123
 58935554.78523942 28128413.29923534 10963593.51009882 35858870.20763274
 27727978.21502854 22674452.1605431 ]
eta_min = 0.6568293889135383	eta_max = 0.6851522482532448
af = 0.0048373581457900995	bf = 1.2050321534900845	zeta = 0.00532109396036911	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.36561793e-06 1.08249620e-05 1.32766327e-06 5.12060679e-08
 1.65856133e-05 1.80259716e-06 1.01287294e-07 3.59171272e-06
 1.55967862e-06 9.46696933e-07]
ene_total = [1.69139003 0.96374279 1.77760827 1.4326949  0.9413374  0.94235589
 1.42707694 1.30607207 1.86080934 0.91733759]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 1 obj = 3.8330520913049613
eta = 0.6851522482532448
freqs = [26297444.52410097 50627871.73999517 26166161.04026021  8689617.22720056
 58304024.4554098  27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656411]
eta_min = 0.6851522482532453	eta_max = 0.6851522482532441
af = 0.0047490407946122245	bf = 1.2050321534900845	zeta = 0.005223944874073448	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.35973100e-06 1.05993827e-05 1.32482057e-06 5.06669901e-08
 1.62320679e-05 1.76428286e-06 1.00207745e-07 3.54345912e-06
 1.55967862e-06 9.26091353e-07]
ene_total = [1.69138925 0.96371281 1.77760789 1.43269482 0.94129041 0.9423508
 1.4270768  1.30606566 1.86080934 0.91733486]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 2 obj = 3.8330520913049524
eta = 0.6851522482532441
freqs = [26297444.52410097 50627871.73999519 26166161.04026021  8689617.22720056
 58304024.45540981 27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656412]
Done!
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.22816025e-06 4.07545842e-05 5.09392980e-06 1.94814375e-07
 6.24122362e-05 6.78366056e-06 3.85298380e-07 1.36245862e-05
 5.99695805e-06 3.56081756e-06]
ene_total = [0.01272946 0.00728089 0.01337805 0.01077938 0.00712821 0.00709503
 0.01073726 0.00983659 0.0140047  0.00690443]
At round 47 energy consumption: 0.09987401390775219
At round 47 eta: 0.6851522482532441
At round 47 a_n: 12.082948048696464
At round 47 local rounds: 12.381380128537735
At round 47 global rounds: 38.37711395955986
gradient difference: 0.35888296365737915
train() client id: f_00000-0-0 loss: 1.163191  [   32/  126]
train() client id: f_00000-0-1 loss: 1.141229  [   64/  126]
train() client id: f_00000-0-2 loss: 1.233222  [   96/  126]
train() client id: f_00000-1-0 loss: 0.903008  [   32/  126]
train() client id: f_00000-1-1 loss: 1.220615  [   64/  126]
train() client id: f_00000-1-2 loss: 0.955518  [   96/  126]
train() client id: f_00000-2-0 loss: 0.850657  [   32/  126]
train() client id: f_00000-2-1 loss: 1.015043  [   64/  126]
train() client id: f_00000-2-2 loss: 0.890749  [   96/  126]
train() client id: f_00000-3-0 loss: 1.046917  [   32/  126]
train() client id: f_00000-3-1 loss: 0.824265  [   64/  126]
train() client id: f_00000-3-2 loss: 0.819085  [   96/  126]
train() client id: f_00000-4-0 loss: 0.929673  [   32/  126]
train() client id: f_00000-4-1 loss: 0.806692  [   64/  126]
train() client id: f_00000-4-2 loss: 0.888558  [   96/  126]
train() client id: f_00000-5-0 loss: 0.848550  [   32/  126]
train() client id: f_00000-5-1 loss: 0.821426  [   64/  126]
train() client id: f_00000-5-2 loss: 0.768970  [   96/  126]
train() client id: f_00000-6-0 loss: 0.929764  [   32/  126]
train() client id: f_00000-6-1 loss: 0.757811  [   64/  126]
train() client id: f_00000-6-2 loss: 0.741799  [   96/  126]
train() client id: f_00000-7-0 loss: 0.972755  [   32/  126]
train() client id: f_00000-7-1 loss: 0.744631  [   64/  126]
train() client id: f_00000-7-2 loss: 0.795578  [   96/  126]
train() client id: f_00000-8-0 loss: 0.840920  [   32/  126]
train() client id: f_00000-8-1 loss: 0.760693  [   64/  126]
train() client id: f_00000-8-2 loss: 0.762895  [   96/  126]
train() client id: f_00000-9-0 loss: 0.780137  [   32/  126]
train() client id: f_00000-9-1 loss: 0.753827  [   64/  126]
train() client id: f_00000-9-2 loss: 0.748895  [   96/  126]
train() client id: f_00000-10-0 loss: 0.816620  [   32/  126]
train() client id: f_00000-10-1 loss: 0.799244  [   64/  126]
train() client id: f_00000-10-2 loss: 0.880295  [   96/  126]
train() client id: f_00000-11-0 loss: 0.894381  [   32/  126]
train() client id: f_00000-11-1 loss: 0.762626  [   64/  126]
train() client id: f_00000-11-2 loss: 0.790673  [   96/  126]
train() client id: f_00001-0-0 loss: 0.469154  [   32/  265]
train() client id: f_00001-0-1 loss: 0.442167  [   64/  265]
train() client id: f_00001-0-2 loss: 0.526995  [   96/  265]
train() client id: f_00001-0-3 loss: 0.366358  [  128/  265]
train() client id: f_00001-0-4 loss: 0.413237  [  160/  265]
train() client id: f_00001-0-5 loss: 0.425569  [  192/  265]
train() client id: f_00001-0-6 loss: 0.345708  [  224/  265]
train() client id: f_00001-0-7 loss: 0.540619  [  256/  265]
train() client id: f_00001-1-0 loss: 0.471621  [   32/  265]
train() client id: f_00001-1-1 loss: 0.350949  [   64/  265]
train() client id: f_00001-1-2 loss: 0.515531  [   96/  265]
train() client id: f_00001-1-3 loss: 0.408371  [  128/  265]
train() client id: f_00001-1-4 loss: 0.372164  [  160/  265]
train() client id: f_00001-1-5 loss: 0.446359  [  192/  265]
train() client id: f_00001-1-6 loss: 0.470068  [  224/  265]
train() client id: f_00001-1-7 loss: 0.431061  [  256/  265]
train() client id: f_00001-2-0 loss: 0.518474  [   32/  265]
train() client id: f_00001-2-1 loss: 0.377190  [   64/  265]
train() client id: f_00001-2-2 loss: 0.411364  [   96/  265]
train() client id: f_00001-2-3 loss: 0.420492  [  128/  265]
train() client id: f_00001-2-4 loss: 0.395918  [  160/  265]
train() client id: f_00001-2-5 loss: 0.374310  [  192/  265]
train() client id: f_00001-2-6 loss: 0.413307  [  224/  265]
train() client id: f_00001-2-7 loss: 0.480719  [  256/  265]
train() client id: f_00001-3-0 loss: 0.497774  [   32/  265]
train() client id: f_00001-3-1 loss: 0.407061  [   64/  265]
train() client id: f_00001-3-2 loss: 0.396397  [   96/  265]
train() client id: f_00001-3-3 loss: 0.469656  [  128/  265]
train() client id: f_00001-3-4 loss: 0.467560  [  160/  265]
train() client id: f_00001-3-5 loss: 0.352253  [  192/  265]
train() client id: f_00001-3-6 loss: 0.344649  [  224/  265]
train() client id: f_00001-3-7 loss: 0.456670  [  256/  265]
train() client id: f_00001-4-0 loss: 0.338451  [   32/  265]
train() client id: f_00001-4-1 loss: 0.314468  [   64/  265]
train() client id: f_00001-4-2 loss: 0.460295  [   96/  265]
train() client id: f_00001-4-3 loss: 0.346424  [  128/  265]
train() client id: f_00001-4-4 loss: 0.465802  [  160/  265]
train() client id: f_00001-4-5 loss: 0.436801  [  192/  265]
train() client id: f_00001-4-6 loss: 0.558255  [  224/  265]
train() client id: f_00001-4-7 loss: 0.456378  [  256/  265]
train() client id: f_00001-5-0 loss: 0.507305  [   32/  265]
train() client id: f_00001-5-1 loss: 0.425525  [   64/  265]
train() client id: f_00001-5-2 loss: 0.423279  [   96/  265]
train() client id: f_00001-5-3 loss: 0.421558  [  128/  265]
train() client id: f_00001-5-4 loss: 0.332662  [  160/  265]
train() client id: f_00001-5-5 loss: 0.400454  [  192/  265]
train() client id: f_00001-5-6 loss: 0.513956  [  224/  265]
train() client id: f_00001-5-7 loss: 0.327474  [  256/  265]
train() client id: f_00001-6-0 loss: 0.410554  [   32/  265]
train() client id: f_00001-6-1 loss: 0.459055  [   64/  265]
train() client id: f_00001-6-2 loss: 0.424028  [   96/  265]
train() client id: f_00001-6-3 loss: 0.489496  [  128/  265]
train() client id: f_00001-6-4 loss: 0.387393  [  160/  265]
train() client id: f_00001-6-5 loss: 0.346970  [  192/  265]
train() client id: f_00001-6-6 loss: 0.436529  [  224/  265]
train() client id: f_00001-6-7 loss: 0.365037  [  256/  265]
train() client id: f_00001-7-0 loss: 0.316556  [   32/  265]
train() client id: f_00001-7-1 loss: 0.311941  [   64/  265]
train() client id: f_00001-7-2 loss: 0.415292  [   96/  265]
train() client id: f_00001-7-3 loss: 0.515974  [  128/  265]
train() client id: f_00001-7-4 loss: 0.455238  [  160/  265]
train() client id: f_00001-7-5 loss: 0.548795  [  192/  265]
train() client id: f_00001-7-6 loss: 0.463390  [  224/  265]
train() client id: f_00001-7-7 loss: 0.314875  [  256/  265]
train() client id: f_00001-8-0 loss: 0.399187  [   32/  265]
train() client id: f_00001-8-1 loss: 0.512311  [   64/  265]
train() client id: f_00001-8-2 loss: 0.430263  [   96/  265]
train() client id: f_00001-8-3 loss: 0.424570  [  128/  265]
train() client id: f_00001-8-4 loss: 0.376575  [  160/  265]
train() client id: f_00001-8-5 loss: 0.327321  [  192/  265]
train() client id: f_00001-8-6 loss: 0.486058  [  224/  265]
train() client id: f_00001-8-7 loss: 0.324478  [  256/  265]
train() client id: f_00001-9-0 loss: 0.361043  [   32/  265]
train() client id: f_00001-9-1 loss: 0.381867  [   64/  265]
train() client id: f_00001-9-2 loss: 0.408524  [   96/  265]
train() client id: f_00001-9-3 loss: 0.448849  [  128/  265]
train() client id: f_00001-9-4 loss: 0.369640  [  160/  265]
train() client id: f_00001-9-5 loss: 0.421713  [  192/  265]
train() client id: f_00001-9-6 loss: 0.518849  [  224/  265]
train() client id: f_00001-9-7 loss: 0.415719  [  256/  265]
train() client id: f_00001-10-0 loss: 0.561456  [   32/  265]
train() client id: f_00001-10-1 loss: 0.454566  [   64/  265]
train() client id: f_00001-10-2 loss: 0.462107  [   96/  265]
train() client id: f_00001-10-3 loss: 0.360271  [  128/  265]
train() client id: f_00001-10-4 loss: 0.385143  [  160/  265]
train() client id: f_00001-10-5 loss: 0.437121  [  192/  265]
train() client id: f_00001-10-6 loss: 0.363598  [  224/  265]
train() client id: f_00001-10-7 loss: 0.318657  [  256/  265]
train() client id: f_00001-11-0 loss: 0.520586  [   32/  265]
train() client id: f_00001-11-1 loss: 0.307464  [   64/  265]
train() client id: f_00001-11-2 loss: 0.422295  [   96/  265]
train() client id: f_00001-11-3 loss: 0.464533  [  128/  265]
train() client id: f_00001-11-4 loss: 0.401551  [  160/  265]
train() client id: f_00001-11-5 loss: 0.370812  [  192/  265]
train() client id: f_00001-11-6 loss: 0.391843  [  224/  265]
train() client id: f_00001-11-7 loss: 0.470218  [  256/  265]
train() client id: f_00002-0-0 loss: 1.483033  [   32/  124]
train() client id: f_00002-0-1 loss: 1.195574  [   64/  124]
train() client id: f_00002-0-2 loss: 1.326475  [   96/  124]
train() client id: f_00002-1-0 loss: 1.215677  [   32/  124]
train() client id: f_00002-1-1 loss: 1.255920  [   64/  124]
train() client id: f_00002-1-2 loss: 1.447838  [   96/  124]
train() client id: f_00002-2-0 loss: 1.301462  [   32/  124]
train() client id: f_00002-2-1 loss: 1.258786  [   64/  124]
train() client id: f_00002-2-2 loss: 1.215590  [   96/  124]
train() client id: f_00002-3-0 loss: 1.268096  [   32/  124]
train() client id: f_00002-3-1 loss: 1.281474  [   64/  124]
train() client id: f_00002-3-2 loss: 1.135622  [   96/  124]
train() client id: f_00002-4-0 loss: 1.295176  [   32/  124]
train() client id: f_00002-4-1 loss: 1.231385  [   64/  124]
train() client id: f_00002-4-2 loss: 1.023046  [   96/  124]
train() client id: f_00002-5-0 loss: 1.142083  [   32/  124]
train() client id: f_00002-5-1 loss: 1.233831  [   64/  124]
train() client id: f_00002-5-2 loss: 1.101155  [   96/  124]
train() client id: f_00002-6-0 loss: 0.993998  [   32/  124]
train() client id: f_00002-6-1 loss: 1.197798  [   64/  124]
train() client id: f_00002-6-2 loss: 1.208422  [   96/  124]
train() client id: f_00002-7-0 loss: 1.062015  [   32/  124]
train() client id: f_00002-7-1 loss: 1.097991  [   64/  124]
train() client id: f_00002-7-2 loss: 1.120124  [   96/  124]
train() client id: f_00002-8-0 loss: 0.985944  [   32/  124]
train() client id: f_00002-8-1 loss: 1.066569  [   64/  124]
train() client id: f_00002-8-2 loss: 1.359923  [   96/  124]
train() client id: f_00002-9-0 loss: 1.138879  [   32/  124]
train() client id: f_00002-9-1 loss: 1.157146  [   64/  124]
train() client id: f_00002-9-2 loss: 1.195335  [   96/  124]
train() client id: f_00002-10-0 loss: 0.994003  [   32/  124]
train() client id: f_00002-10-1 loss: 1.122479  [   64/  124]
train() client id: f_00002-10-2 loss: 1.103741  [   96/  124]
train() client id: f_00002-11-0 loss: 0.911538  [   32/  124]
train() client id: f_00002-11-1 loss: 1.176691  [   64/  124]
train() client id: f_00002-11-2 loss: 1.143987  [   96/  124]
train() client id: f_00003-0-0 loss: 0.655735  [   32/   43]
train() client id: f_00003-1-0 loss: 0.599356  [   32/   43]
train() client id: f_00003-2-0 loss: 0.711928  [   32/   43]
train() client id: f_00003-3-0 loss: 0.428834  [   32/   43]
train() client id: f_00003-4-0 loss: 0.726391  [   32/   43]
train() client id: f_00003-5-0 loss: 0.629043  [   32/   43]
train() client id: f_00003-6-0 loss: 0.579464  [   32/   43]
train() client id: f_00003-7-0 loss: 0.522283  [   32/   43]
train() client id: f_00003-8-0 loss: 0.752573  [   32/   43]
train() client id: f_00003-9-0 loss: 0.611743  [   32/   43]
train() client id: f_00003-10-0 loss: 0.705275  [   32/   43]
train() client id: f_00003-11-0 loss: 0.657156  [   32/   43]
train() client id: f_00004-0-0 loss: 0.853716  [   32/  306]
train() client id: f_00004-0-1 loss: 0.945294  [   64/  306]
train() client id: f_00004-0-2 loss: 1.006505  [   96/  306]
train() client id: f_00004-0-3 loss: 0.958344  [  128/  306]
train() client id: f_00004-0-4 loss: 0.976840  [  160/  306]
train() client id: f_00004-0-5 loss: 0.907812  [  192/  306]
train() client id: f_00004-0-6 loss: 0.928225  [  224/  306]
train() client id: f_00004-0-7 loss: 0.968915  [  256/  306]
train() client id: f_00004-0-8 loss: 1.007883  [  288/  306]
train() client id: f_00004-1-0 loss: 1.054607  [   32/  306]
train() client id: f_00004-1-1 loss: 0.866100  [   64/  306]
train() client id: f_00004-1-2 loss: 0.917784  [   96/  306]
train() client id: f_00004-1-3 loss: 0.784642  [  128/  306]
train() client id: f_00004-1-4 loss: 0.920352  [  160/  306]
train() client id: f_00004-1-5 loss: 0.973287  [  192/  306]
train() client id: f_00004-1-6 loss: 0.983536  [  224/  306]
train() client id: f_00004-1-7 loss: 0.948579  [  256/  306]
train() client id: f_00004-1-8 loss: 1.113229  [  288/  306]
train() client id: f_00004-2-0 loss: 0.971708  [   32/  306]
train() client id: f_00004-2-1 loss: 1.099735  [   64/  306]
train() client id: f_00004-2-2 loss: 1.052446  [   96/  306]
train() client id: f_00004-2-3 loss: 0.958004  [  128/  306]
train() client id: f_00004-2-4 loss: 0.886859  [  160/  306]
train() client id: f_00004-2-5 loss: 1.090085  [  192/  306]
train() client id: f_00004-2-6 loss: 0.922949  [  224/  306]
train() client id: f_00004-2-7 loss: 0.762393  [  256/  306]
train() client id: f_00004-2-8 loss: 0.848297  [  288/  306]
train() client id: f_00004-3-0 loss: 1.027220  [   32/  306]
train() client id: f_00004-3-1 loss: 1.064078  [   64/  306]
train() client id: f_00004-3-2 loss: 0.798635  [   96/  306]
train() client id: f_00004-3-3 loss: 0.843206  [  128/  306]
train() client id: f_00004-3-4 loss: 1.012812  [  160/  306]
train() client id: f_00004-3-5 loss: 0.940202  [  192/  306]
train() client id: f_00004-3-6 loss: 0.933998  [  224/  306]
train() client id: f_00004-3-7 loss: 0.982106  [  256/  306]
train() client id: f_00004-3-8 loss: 0.978647  [  288/  306]
train() client id: f_00004-4-0 loss: 0.970699  [   32/  306]
train() client id: f_00004-4-1 loss: 0.863394  [   64/  306]
train() client id: f_00004-4-2 loss: 0.846826  [   96/  306]
train() client id: f_00004-4-3 loss: 0.986960  [  128/  306]
train() client id: f_00004-4-4 loss: 0.991185  [  160/  306]
train() client id: f_00004-4-5 loss: 0.985510  [  192/  306]
train() client id: f_00004-4-6 loss: 1.000860  [  224/  306]
train() client id: f_00004-4-7 loss: 0.924910  [  256/  306]
train() client id: f_00004-4-8 loss: 0.993181  [  288/  306]
train() client id: f_00004-5-0 loss: 0.994286  [   32/  306]
train() client id: f_00004-5-1 loss: 0.987733  [   64/  306]
train() client id: f_00004-5-2 loss: 0.911628  [   96/  306]
train() client id: f_00004-5-3 loss: 0.955701  [  128/  306]
train() client id: f_00004-5-4 loss: 0.801360  [  160/  306]
train() client id: f_00004-5-5 loss: 0.960875  [  192/  306]
train() client id: f_00004-5-6 loss: 0.950800  [  224/  306]
train() client id: f_00004-5-7 loss: 1.040484  [  256/  306]
train() client id: f_00004-5-8 loss: 0.962582  [  288/  306]
train() client id: f_00004-6-0 loss: 1.010045  [   32/  306]
train() client id: f_00004-6-1 loss: 0.952260  [   64/  306]
train() client id: f_00004-6-2 loss: 0.981725  [   96/  306]
train() client id: f_00004-6-3 loss: 0.968353  [  128/  306]
train() client id: f_00004-6-4 loss: 0.792535  [  160/  306]
train() client id: f_00004-6-5 loss: 0.901074  [  192/  306]
train() client id: f_00004-6-6 loss: 0.844627  [  224/  306]
train() client id: f_00004-6-7 loss: 0.970078  [  256/  306]
train() client id: f_00004-6-8 loss: 0.999054  [  288/  306]
train() client id: f_00004-7-0 loss: 1.046269  [   32/  306]
train() client id: f_00004-7-1 loss: 0.889351  [   64/  306]
train() client id: f_00004-7-2 loss: 0.944715  [   96/  306]
train() client id: f_00004-7-3 loss: 0.932275  [  128/  306]
train() client id: f_00004-7-4 loss: 0.976384  [  160/  306]
train() client id: f_00004-7-5 loss: 0.898523  [  192/  306]
train() client id: f_00004-7-6 loss: 0.930360  [  224/  306]
train() client id: f_00004-7-7 loss: 0.879706  [  256/  306]
train() client id: f_00004-7-8 loss: 0.927187  [  288/  306]
train() client id: f_00004-8-0 loss: 0.920900  [   32/  306]
train() client id: f_00004-8-1 loss: 0.821884  [   64/  306]
train() client id: f_00004-8-2 loss: 0.848300  [   96/  306]
train() client id: f_00004-8-3 loss: 1.001424  [  128/  306]
train() client id: f_00004-8-4 loss: 0.943844  [  160/  306]
train() client id: f_00004-8-5 loss: 1.037158  [  192/  306]
train() client id: f_00004-8-6 loss: 0.985676  [  224/  306]
train() client id: f_00004-8-7 loss: 0.974176  [  256/  306]
train() client id: f_00004-8-8 loss: 0.928481  [  288/  306]
train() client id: f_00004-9-0 loss: 1.107582  [   32/  306]
train() client id: f_00004-9-1 loss: 0.910378  [   64/  306]
train() client id: f_00004-9-2 loss: 0.888572  [   96/  306]
train() client id: f_00004-9-3 loss: 0.910028  [  128/  306]
train() client id: f_00004-9-4 loss: 0.937993  [  160/  306]
train() client id: f_00004-9-5 loss: 0.867490  [  192/  306]
train() client id: f_00004-9-6 loss: 0.951593  [  224/  306]
train() client id: f_00004-9-7 loss: 0.954610  [  256/  306]
train() client id: f_00004-9-8 loss: 0.889470  [  288/  306]
train() client id: f_00004-10-0 loss: 0.911674  [   32/  306]
train() client id: f_00004-10-1 loss: 0.980723  [   64/  306]
train() client id: f_00004-10-2 loss: 0.898463  [   96/  306]
train() client id: f_00004-10-3 loss: 0.930072  [  128/  306]
train() client id: f_00004-10-4 loss: 1.090885  [  160/  306]
train() client id: f_00004-10-5 loss: 1.021466  [  192/  306]
train() client id: f_00004-10-6 loss: 0.979132  [  224/  306]
train() client id: f_00004-10-7 loss: 0.798769  [  256/  306]
train() client id: f_00004-10-8 loss: 0.863869  [  288/  306]
train() client id: f_00004-11-0 loss: 0.936595  [   32/  306]
train() client id: f_00004-11-1 loss: 1.057868  [   64/  306]
train() client id: f_00004-11-2 loss: 0.858565  [   96/  306]
train() client id: f_00004-11-3 loss: 0.824591  [  128/  306]
train() client id: f_00004-11-4 loss: 0.890480  [  160/  306]
train() client id: f_00004-11-5 loss: 1.033814  [  192/  306]
train() client id: f_00004-11-6 loss: 0.879760  [  224/  306]
train() client id: f_00004-11-7 loss: 0.923457  [  256/  306]
train() client id: f_00004-11-8 loss: 0.951146  [  288/  306]
train() client id: f_00005-0-0 loss: 0.344332  [   32/  146]
train() client id: f_00005-0-1 loss: 0.800240  [   64/  146]
train() client id: f_00005-0-2 loss: 0.772196  [   96/  146]
train() client id: f_00005-0-3 loss: 0.458153  [  128/  146]
train() client id: f_00005-1-0 loss: 0.540721  [   32/  146]
train() client id: f_00005-1-1 loss: 0.569519  [   64/  146]
train() client id: f_00005-1-2 loss: 0.635774  [   96/  146]
train() client id: f_00005-1-3 loss: 0.413482  [  128/  146]
train() client id: f_00005-2-0 loss: 0.666800  [   32/  146]
train() client id: f_00005-2-1 loss: 0.772212  [   64/  146]
train() client id: f_00005-2-2 loss: 0.590963  [   96/  146]
train() client id: f_00005-2-3 loss: 0.418844  [  128/  146]
train() client id: f_00005-3-0 loss: 0.609857  [   32/  146]
train() client id: f_00005-3-1 loss: 0.913049  [   64/  146]
train() client id: f_00005-3-2 loss: 0.591175  [   96/  146]
train() client id: f_00005-3-3 loss: 0.505976  [  128/  146]
train() client id: f_00005-4-0 loss: 0.576420  [   32/  146]
train() client id: f_00005-4-1 loss: 0.582688  [   64/  146]
train() client id: f_00005-4-2 loss: 0.680827  [   96/  146]
train() client id: f_00005-4-3 loss: 0.531819  [  128/  146]
train() client id: f_00005-5-0 loss: 0.624645  [   32/  146]
train() client id: f_00005-5-1 loss: 0.660005  [   64/  146]
train() client id: f_00005-5-2 loss: 0.501763  [   96/  146]
train() client id: f_00005-5-3 loss: 0.530598  [  128/  146]
train() client id: f_00005-6-0 loss: 0.688870  [   32/  146]
train() client id: f_00005-6-1 loss: 0.774540  [   64/  146]
train() client id: f_00005-6-2 loss: 0.369974  [   96/  146]
train() client id: f_00005-6-3 loss: 0.473972  [  128/  146]
train() client id: f_00005-7-0 loss: 0.502388  [   32/  146]
train() client id: f_00005-7-1 loss: 0.749346  [   64/  146]
train() client id: f_00005-7-2 loss: 0.602648  [   96/  146]
train() client id: f_00005-7-3 loss: 0.472688  [  128/  146]
train() client id: f_00005-8-0 loss: 0.616771  [   32/  146]
train() client id: f_00005-8-1 loss: 0.650545  [   64/  146]
train() client id: f_00005-8-2 loss: 0.440552  [   96/  146]
train() client id: f_00005-8-3 loss: 0.808588  [  128/  146]
train() client id: f_00005-9-0 loss: 0.629331  [   32/  146]
train() client id: f_00005-9-1 loss: 0.620417  [   64/  146]
train() client id: f_00005-9-2 loss: 0.616342  [   96/  146]
train() client id: f_00005-9-3 loss: 0.602395  [  128/  146]
train() client id: f_00005-10-0 loss: 0.575715  [   32/  146]
train() client id: f_00005-10-1 loss: 0.847527  [   64/  146]
train() client id: f_00005-10-2 loss: 0.510849  [   96/  146]
train() client id: f_00005-10-3 loss: 0.472028  [  128/  146]
train() client id: f_00005-11-0 loss: 0.549452  [   32/  146]
train() client id: f_00005-11-1 loss: 0.826713  [   64/  146]
train() client id: f_00005-11-2 loss: 0.405716  [   96/  146]
train() client id: f_00005-11-3 loss: 0.663234  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497921  [   32/   54]
train() client id: f_00006-1-0 loss: 0.424637  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469705  [   32/   54]
train() client id: f_00006-3-0 loss: 0.463655  [   32/   54]
train() client id: f_00006-4-0 loss: 0.435596  [   32/   54]
train() client id: f_00006-5-0 loss: 0.460954  [   32/   54]
train() client id: f_00006-6-0 loss: 0.479452  [   32/   54]
train() client id: f_00006-7-0 loss: 0.465563  [   32/   54]
train() client id: f_00006-8-0 loss: 0.463338  [   32/   54]
train() client id: f_00006-9-0 loss: 0.429331  [   32/   54]
train() client id: f_00006-10-0 loss: 0.423520  [   32/   54]
train() client id: f_00006-11-0 loss: 0.457615  [   32/   54]
train() client id: f_00007-0-0 loss: 0.564852  [   32/  179]
train() client id: f_00007-0-1 loss: 0.586272  [   64/  179]
train() client id: f_00007-0-2 loss: 0.541625  [   96/  179]
train() client id: f_00007-0-3 loss: 0.579294  [  128/  179]
train() client id: f_00007-0-4 loss: 0.790206  [  160/  179]
train() client id: f_00007-1-0 loss: 0.559312  [   32/  179]
train() client id: f_00007-1-1 loss: 0.475881  [   64/  179]
train() client id: f_00007-1-2 loss: 0.793660  [   96/  179]
train() client id: f_00007-1-3 loss: 0.729444  [  128/  179]
train() client id: f_00007-1-4 loss: 0.624228  [  160/  179]
train() client id: f_00007-2-0 loss: 0.450140  [   32/  179]
train() client id: f_00007-2-1 loss: 0.561156  [   64/  179]
train() client id: f_00007-2-2 loss: 0.615775  [   96/  179]
train() client id: f_00007-2-3 loss: 0.688479  [  128/  179]
train() client id: f_00007-2-4 loss: 0.717129  [  160/  179]
train() client id: f_00007-3-0 loss: 0.758412  [   32/  179]
train() client id: f_00007-3-1 loss: 0.497237  [   64/  179]
train() client id: f_00007-3-2 loss: 0.462448  [   96/  179]
train() client id: f_00007-3-3 loss: 0.584599  [  128/  179]
train() client id: f_00007-3-4 loss: 0.823605  [  160/  179]
train() client id: f_00007-4-0 loss: 0.484712  [   32/  179]
train() client id: f_00007-4-1 loss: 0.526761  [   64/  179]
train() client id: f_00007-4-2 loss: 0.936728  [   96/  179]
train() client id: f_00007-4-3 loss: 0.630630  [  128/  179]
train() client id: f_00007-4-4 loss: 0.553349  [  160/  179]
train() client id: f_00007-5-0 loss: 0.558571  [   32/  179]
train() client id: f_00007-5-1 loss: 0.788751  [   64/  179]
train() client id: f_00007-5-2 loss: 0.480764  [   96/  179]
train() client id: f_00007-5-3 loss: 0.709030  [  128/  179]
train() client id: f_00007-5-4 loss: 0.549250  [  160/  179]
train() client id: f_00007-6-0 loss: 0.530455  [   32/  179]
train() client id: f_00007-6-1 loss: 0.526622  [   64/  179]
train() client id: f_00007-6-2 loss: 0.658491  [   96/  179]
train() client id: f_00007-6-3 loss: 0.706494  [  128/  179]
train() client id: f_00007-6-4 loss: 0.483939  [  160/  179]
train() client id: f_00007-7-0 loss: 0.696953  [   32/  179]
train() client id: f_00007-7-1 loss: 0.803346  [   64/  179]
train() client id: f_00007-7-2 loss: 0.482663  [   96/  179]
train() client id: f_00007-7-3 loss: 0.513386  [  128/  179]
train() client id: f_00007-7-4 loss: 0.558235  [  160/  179]
train() client id: f_00007-8-0 loss: 0.615469  [   32/  179]
train() client id: f_00007-8-1 loss: 0.572852  [   64/  179]
train() client id: f_00007-8-2 loss: 0.530616  [   96/  179]
train() client id: f_00007-8-3 loss: 0.506697  [  128/  179]
train() client id: f_00007-8-4 loss: 0.611524  [  160/  179]
train() client id: f_00007-9-0 loss: 0.709117  [   32/  179]
train() client id: f_00007-9-1 loss: 0.430196  [   64/  179]
train() client id: f_00007-9-2 loss: 0.665886  [   96/  179]
train() client id: f_00007-9-3 loss: 0.503109  [  128/  179]
train() client id: f_00007-9-4 loss: 0.720402  [  160/  179]
train() client id: f_00007-10-0 loss: 0.590066  [   32/  179]
train() client id: f_00007-10-1 loss: 0.544425  [   64/  179]
train() client id: f_00007-10-2 loss: 0.525998  [   96/  179]
train() client id: f_00007-10-3 loss: 0.866874  [  128/  179]
train() client id: f_00007-10-4 loss: 0.497694  [  160/  179]
train() client id: f_00007-11-0 loss: 0.497680  [   32/  179]
train() client id: f_00007-11-1 loss: 0.453270  [   64/  179]
train() client id: f_00007-11-2 loss: 0.528470  [   96/  179]
train() client id: f_00007-11-3 loss: 0.682049  [  128/  179]
train() client id: f_00007-11-4 loss: 0.641613  [  160/  179]
train() client id: f_00008-0-0 loss: 0.657589  [   32/  130]
train() client id: f_00008-0-1 loss: 0.766217  [   64/  130]
train() client id: f_00008-0-2 loss: 0.640977  [   96/  130]
train() client id: f_00008-0-3 loss: 0.810537  [  128/  130]
train() client id: f_00008-1-0 loss: 0.713283  [   32/  130]
train() client id: f_00008-1-1 loss: 0.734493  [   64/  130]
train() client id: f_00008-1-2 loss: 0.773293  [   96/  130]
train() client id: f_00008-1-3 loss: 0.654250  [  128/  130]
train() client id: f_00008-2-0 loss: 0.679785  [   32/  130]
train() client id: f_00008-2-1 loss: 0.671903  [   64/  130]
train() client id: f_00008-2-2 loss: 0.720162  [   96/  130]
train() client id: f_00008-2-3 loss: 0.797177  [  128/  130]
train() client id: f_00008-3-0 loss: 0.613966  [   32/  130]
train() client id: f_00008-3-1 loss: 0.751005  [   64/  130]
train() client id: f_00008-3-2 loss: 0.821421  [   96/  130]
train() client id: f_00008-3-3 loss: 0.687312  [  128/  130]
train() client id: f_00008-4-0 loss: 0.662852  [   32/  130]
train() client id: f_00008-4-1 loss: 0.763556  [   64/  130]
train() client id: f_00008-4-2 loss: 0.781176  [   96/  130]
train() client id: f_00008-4-3 loss: 0.661825  [  128/  130]
train() client id: f_00008-5-0 loss: 0.740438  [   32/  130]
train() client id: f_00008-5-1 loss: 0.715006  [   64/  130]
train() client id: f_00008-5-2 loss: 0.666127  [   96/  130]
train() client id: f_00008-5-3 loss: 0.737929  [  128/  130]
train() client id: f_00008-6-0 loss: 0.635988  [   32/  130]
train() client id: f_00008-6-1 loss: 0.711859  [   64/  130]
train() client id: f_00008-6-2 loss: 0.798091  [   96/  130]
train() client id: f_00008-6-3 loss: 0.688993  [  128/  130]
train() client id: f_00008-7-0 loss: 0.689815  [   32/  130]
train() client id: f_00008-7-1 loss: 0.743617  [   64/  130]
train() client id: f_00008-7-2 loss: 0.797657  [   96/  130]
train() client id: f_00008-7-3 loss: 0.631373  [  128/  130]
train() client id: f_00008-8-0 loss: 0.703193  [   32/  130]
train() client id: f_00008-8-1 loss: 0.697706  [   64/  130]
train() client id: f_00008-8-2 loss: 0.650573  [   96/  130]
train() client id: f_00008-8-3 loss: 0.816007  [  128/  130]
train() client id: f_00008-9-0 loss: 0.837492  [   32/  130]
train() client id: f_00008-9-1 loss: 0.645677  [   64/  130]
train() client id: f_00008-9-2 loss: 0.638964  [   96/  130]
train() client id: f_00008-9-3 loss: 0.713030  [  128/  130]
train() client id: f_00008-10-0 loss: 0.698577  [   32/  130]
train() client id: f_00008-10-1 loss: 0.730853  [   64/  130]
train() client id: f_00008-10-2 loss: 0.692041  [   96/  130]
train() client id: f_00008-10-3 loss: 0.742527  [  128/  130]
train() client id: f_00008-11-0 loss: 0.612155  [   32/  130]
train() client id: f_00008-11-1 loss: 0.743834  [   64/  130]
train() client id: f_00008-11-2 loss: 0.804499  [   96/  130]
train() client id: f_00008-11-3 loss: 0.696412  [  128/  130]
train() client id: f_00009-0-0 loss: 1.286476  [   32/  118]
train() client id: f_00009-0-1 loss: 1.158854  [   64/  118]
train() client id: f_00009-0-2 loss: 1.082289  [   96/  118]
train() client id: f_00009-1-0 loss: 1.154106  [   32/  118]
train() client id: f_00009-1-1 loss: 0.981406  [   64/  118]
train() client id: f_00009-1-2 loss: 1.001672  [   96/  118]
train() client id: f_00009-2-0 loss: 1.173587  [   32/  118]
train() client id: f_00009-2-1 loss: 1.083946  [   64/  118]
train() client id: f_00009-2-2 loss: 0.994923  [   96/  118]
train() client id: f_00009-3-0 loss: 0.951388  [   32/  118]
train() client id: f_00009-3-1 loss: 0.923229  [   64/  118]
train() client id: f_00009-3-2 loss: 1.150676  [   96/  118]
train() client id: f_00009-4-0 loss: 0.983205  [   32/  118]
train() client id: f_00009-4-1 loss: 1.094567  [   64/  118]
train() client id: f_00009-4-2 loss: 1.065489  [   96/  118]
train() client id: f_00009-5-0 loss: 0.960791  [   32/  118]
train() client id: f_00009-5-1 loss: 0.995529  [   64/  118]
train() client id: f_00009-5-2 loss: 0.908014  [   96/  118]
train() client id: f_00009-6-0 loss: 0.848687  [   32/  118]
train() client id: f_00009-6-1 loss: 0.880350  [   64/  118]
train() client id: f_00009-6-2 loss: 1.051809  [   96/  118]
train() client id: f_00009-7-0 loss: 0.874568  [   32/  118]
train() client id: f_00009-7-1 loss: 0.941772  [   64/  118]
train() client id: f_00009-7-2 loss: 0.814298  [   96/  118]
train() client id: f_00009-8-0 loss: 1.095526  [   32/  118]
train() client id: f_00009-8-1 loss: 0.825824  [   64/  118]
train() client id: f_00009-8-2 loss: 0.782592  [   96/  118]
train() client id: f_00009-9-0 loss: 0.820340  [   32/  118]
train() client id: f_00009-9-1 loss: 0.898946  [   64/  118]
train() client id: f_00009-9-2 loss: 0.999734  [   96/  118]
train() client id: f_00009-10-0 loss: 0.773064  [   32/  118]
train() client id: f_00009-10-1 loss: 0.711012  [   64/  118]
train() client id: f_00009-10-2 loss: 1.082975  [   96/  118]
train() client id: f_00009-11-0 loss: 0.839016  [   32/  118]
train() client id: f_00009-11-1 loss: 0.705984  [   64/  118]
train() client id: f_00009-11-2 loss: 0.798167  [   96/  118]
At round 47 accuracy: 0.6445623342175066
At round 47 training accuracy: 0.590878604963112
At round 47 training loss: 0.8309045568286785
update_location
xs = [  -3.9056584     4.20031788  255.00902392   18.81129433    0.97929623
    3.95640986 -217.44319194 -196.32485185  239.66397685 -182.06087855]
ys = [ 247.5879595   230.55583871    1.32061395 -217.45517586  209.35018685
  192.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [267.04878179 251.343664   273.91850303 240.08460655 232.00961134
 217.23938387 239.35002039 220.3273104  260.28348348 207.75508503]
dists_bs = [185.85555033 187.61451208 463.75887031 437.70296191 179.3004407
 179.84087577 182.62546262 175.47821451 443.54338182 171.14423519]
uav_gains = [5.24535404e-12 7.22652115e-12 4.54225750e-12 8.95681473e-12
 1.03431723e-11 1.31774062e-11 9.07808384e-12 1.25521670e-11
 6.03484339e-12 1.52201927e-11]
bs_gains = [4.89330838e-11 4.76593468e-11 3.78160221e-12 4.44622592e-12
 5.41085986e-11 5.36545480e-11 5.13951767e-11 5.74736933e-11
 4.28423215e-12 6.16424250e-11]
Round 48
-------------------------------
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.66251584  9.56421031  4.59923771  1.66893827 11.02840386  5.30599829
  2.06183055  6.52085571  4.81741298  4.30225233]
obj_prev = 54.53165585566928
eta_min = 1.7212465954185627e-20	eta_max = 0.9392276891852349
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 12.610414541407936	eta = 0.909090909090909
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 25.13769025130001	eta = 0.4560487898791294
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 18.739989943316214	eta = 0.6117406281506839
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.582476098023903	eta = 0.6520135819067145
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516919047777254	eta = 0.6544537420190021
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516687862363067	eta = 0.6544623795057578
eta = 0.6544623795057578
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.03498261 0.07357454 0.03442733 0.01193851 0.08495777 0.0405354
 0.01499255 0.04969752 0.03609317 0.03276149]
ene_total = [1.63559168 2.74693988 1.64037262 0.78097472 3.12737952 1.61844234
 0.88321539 2.02882541 1.7072293  1.347717  ]
ti_comp = [0.68075038 0.73958322 0.67379048 0.70183595 0.74144644 0.74132564
 0.70228536 0.71216487 0.67077194 0.74326503]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [5.77378821e-06 4.55080801e-05 5.61746411e-06 2.15903231e-07
 6.97155574e-05 7.57470152e-06 4.27050309e-07 1.51259614e-05
 6.53138483e-06 3.97817259e-06]
ene_total = [0.4471766  0.2486291  0.47081906 0.37534488 0.24312091 0.24142
 0.37382507 0.34075668 0.48110627 0.23470829]
optimize_network iter = 0 obj = 3.4569068484449996
eta = 0.6544623795057578
freqs = [25694153.73552614 49740538.88381484 25547504.06148466  8505199.59149158
 57291910.54818947 27339810.52250793 10674113.26419133 34891861.85896511
 26904202.71882671 22038902.98735746]
eta_min = 0.6544623795057585	eta_max = 0.691983191142256
af = 0.004443917503062845	bf = 1.1926074070739463	zeta = 0.00488830925336913	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29805932e-06 1.02310970e-05 1.26291463e-06 4.85392243e-08
 1.56734064e-05 1.70293948e-06 9.60091734e-08 3.40060882e-06
 1.46838170e-06 8.94370180e-07]
ene_total = [1.6991017  0.94046194 1.78898032 1.42663193 0.91710237 0.91685829
 1.42083414 1.2936725  1.8279896  0.89170773]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 1 obj = 3.877075061802886
eta = 0.691983191142256
freqs = [25637255.917499   49049571.69376329 25530186.95114224  8448581.49608057
 56476825.65917366 26951443.47001215 10602081.72934842 34587483.18659778
 26904202.71882672 21718183.03696004]
eta_min = 0.6919831911422635	eta_max = 0.6919831911422551
af = 0.004337069151020109	bf = 1.1926074070739463	zeta = 0.004770776066122121	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29231677e-06 9.94882221e-06 1.26120311e-06 4.78951357e-08
 1.52306115e-05 1.65490197e-06 9.47177586e-08 3.34153728e-06
 1.46838170e-06 8.68529037e-07]
ene_total = [1.69910096 0.94042549 1.7889801  1.42663185 0.91704519 0.91685209
 1.42083397 1.29366487 1.8279896  0.8917044 ]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 2 obj = 3.8770750618028753
eta = 0.6919831911422551
freqs = [25637255.91749901 49049571.69376332 25530186.95114225  8448581.49608057
 56476825.6591737  26951443.47001216 10602081.72934843 34587483.18659779
 26904202.71882672 21718183.03696005]
Done!
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.96895282e-06 3.82531817e-05 4.84932092e-06 1.84156606e-07
 5.85616405e-05 6.36309147e-06 3.64189404e-07 1.28481975e-05
 5.64592177e-06 3.33949068e-06]
ene_total = [0.01316027 0.00731027 0.01385614 0.01104693 0.00714426 0.00710414
 0.01100217 0.0100267  0.01415879 0.00690718]
At round 48 energy consumption: 0.10171685606737808
At round 48 eta: 0.6919831911422551
At round 48 a_n: 11.740402201727147
At round 48 local rounds: 12.056529567513225
At round 48 global rounds: 38.11610880998822
gradient difference: 0.5039137005805969
train() client id: f_00000-0-0 loss: 1.118229  [   32/  126]
train() client id: f_00000-0-1 loss: 1.434864  [   64/  126]
train() client id: f_00000-0-2 loss: 1.050284  [   96/  126]
train() client id: f_00000-1-0 loss: 1.135049  [   32/  126]
train() client id: f_00000-1-1 loss: 1.137382  [   64/  126]
train() client id: f_00000-1-2 loss: 1.034640  [   96/  126]
train() client id: f_00000-2-0 loss: 1.086956  [   32/  126]
train() client id: f_00000-2-1 loss: 0.979394  [   64/  126]
train() client id: f_00000-2-2 loss: 1.133366  [   96/  126]
train() client id: f_00000-3-0 loss: 1.095832  [   32/  126]
train() client id: f_00000-3-1 loss: 0.990933  [   64/  126]
train() client id: f_00000-3-2 loss: 0.919845  [   96/  126]
train() client id: f_00000-4-0 loss: 0.883172  [   32/  126]
train() client id: f_00000-4-1 loss: 0.970530  [   64/  126]
train() client id: f_00000-4-2 loss: 1.099571  [   96/  126]
train() client id: f_00000-5-0 loss: 0.903309  [   32/  126]
train() client id: f_00000-5-1 loss: 0.949167  [   64/  126]
train() client id: f_00000-5-2 loss: 0.963908  [   96/  126]
train() client id: f_00000-6-0 loss: 0.922266  [   32/  126]
train() client id: f_00000-6-1 loss: 0.915532  [   64/  126]
train() client id: f_00000-6-2 loss: 0.906196  [   96/  126]
train() client id: f_00000-7-0 loss: 0.790066  [   32/  126]
train() client id: f_00000-7-1 loss: 0.862076  [   64/  126]
train() client id: f_00000-7-2 loss: 0.949091  [   96/  126]
train() client id: f_00000-8-0 loss: 0.909657  [   32/  126]
train() client id: f_00000-8-1 loss: 0.781067  [   64/  126]
train() client id: f_00000-8-2 loss: 0.900741  [   96/  126]
train() client id: f_00000-9-0 loss: 0.862586  [   32/  126]
train() client id: f_00000-9-1 loss: 0.942718  [   64/  126]
train() client id: f_00000-9-2 loss: 0.921435  [   96/  126]
train() client id: f_00000-10-0 loss: 0.852269  [   32/  126]
train() client id: f_00000-10-1 loss: 0.999051  [   64/  126]
train() client id: f_00000-10-2 loss: 0.868853  [   96/  126]
train() client id: f_00000-11-0 loss: 0.909548  [   32/  126]
train() client id: f_00000-11-1 loss: 0.852312  [   64/  126]
train() client id: f_00000-11-2 loss: 0.890464  [   96/  126]
train() client id: f_00001-0-0 loss: 0.443080  [   32/  265]
train() client id: f_00001-0-1 loss: 0.516626  [   64/  265]
train() client id: f_00001-0-2 loss: 0.357460  [   96/  265]
train() client id: f_00001-0-3 loss: 0.333302  [  128/  265]
train() client id: f_00001-0-4 loss: 0.480081  [  160/  265]
train() client id: f_00001-0-5 loss: 0.460209  [  192/  265]
train() client id: f_00001-0-6 loss: 0.345499  [  224/  265]
train() client id: f_00001-0-7 loss: 0.449532  [  256/  265]
train() client id: f_00001-1-0 loss: 0.398668  [   32/  265]
train() client id: f_00001-1-1 loss: 0.444423  [   64/  265]
train() client id: f_00001-1-2 loss: 0.487489  [   96/  265]
train() client id: f_00001-1-3 loss: 0.378187  [  128/  265]
train() client id: f_00001-1-4 loss: 0.486650  [  160/  265]
train() client id: f_00001-1-5 loss: 0.374400  [  192/  265]
train() client id: f_00001-1-6 loss: 0.316027  [  224/  265]
train() client id: f_00001-1-7 loss: 0.404294  [  256/  265]
train() client id: f_00001-2-0 loss: 0.399201  [   32/  265]
train() client id: f_00001-2-1 loss: 0.464719  [   64/  265]
train() client id: f_00001-2-2 loss: 0.431050  [   96/  265]
train() client id: f_00001-2-3 loss: 0.360516  [  128/  265]
train() client id: f_00001-2-4 loss: 0.394751  [  160/  265]
train() client id: f_00001-2-5 loss: 0.333209  [  192/  265]
train() client id: f_00001-2-6 loss: 0.317209  [  224/  265]
train() client id: f_00001-2-7 loss: 0.557259  [  256/  265]
train() client id: f_00001-3-0 loss: 0.454178  [   32/  265]
train() client id: f_00001-3-1 loss: 0.390916  [   64/  265]
train() client id: f_00001-3-2 loss: 0.429857  [   96/  265]
train() client id: f_00001-3-3 loss: 0.339731  [  128/  265]
train() client id: f_00001-3-4 loss: 0.465608  [  160/  265]
train() client id: f_00001-3-5 loss: 0.427145  [  192/  265]
train() client id: f_00001-3-6 loss: 0.309490  [  224/  265]
train() client id: f_00001-3-7 loss: 0.403698  [  256/  265]
train() client id: f_00001-4-0 loss: 0.373080  [   32/  265]
train() client id: f_00001-4-1 loss: 0.345189  [   64/  265]
train() client id: f_00001-4-2 loss: 0.431950  [   96/  265]
train() client id: f_00001-4-3 loss: 0.312018  [  128/  265]
train() client id: f_00001-4-4 loss: 0.442195  [  160/  265]
train() client id: f_00001-4-5 loss: 0.408441  [  192/  265]
train() client id: f_00001-4-6 loss: 0.440952  [  224/  265]
train() client id: f_00001-4-7 loss: 0.424271  [  256/  265]
train() client id: f_00001-5-0 loss: 0.373858  [   32/  265]
train() client id: f_00001-5-1 loss: 0.293151  [   64/  265]
train() client id: f_00001-5-2 loss: 0.405539  [   96/  265]
train() client id: f_00001-5-3 loss: 0.294294  [  128/  265]
train() client id: f_00001-5-4 loss: 0.478314  [  160/  265]
train() client id: f_00001-5-5 loss: 0.547984  [  192/  265]
train() client id: f_00001-5-6 loss: 0.432019  [  224/  265]
train() client id: f_00001-5-7 loss: 0.316236  [  256/  265]
train() client id: f_00001-6-0 loss: 0.496000  [   32/  265]
train() client id: f_00001-6-1 loss: 0.449136  [   64/  265]
train() client id: f_00001-6-2 loss: 0.426567  [   96/  265]
train() client id: f_00001-6-3 loss: 0.288975  [  128/  265]
train() client id: f_00001-6-4 loss: 0.411308  [  160/  265]
train() client id: f_00001-6-5 loss: 0.375380  [  192/  265]
train() client id: f_00001-6-6 loss: 0.341195  [  224/  265]
train() client id: f_00001-6-7 loss: 0.332082  [  256/  265]
train() client id: f_00001-7-0 loss: 0.366704  [   32/  265]
train() client id: f_00001-7-1 loss: 0.300762  [   64/  265]
train() client id: f_00001-7-2 loss: 0.369907  [   96/  265]
train() client id: f_00001-7-3 loss: 0.361047  [  128/  265]
train() client id: f_00001-7-4 loss: 0.451360  [  160/  265]
train() client id: f_00001-7-5 loss: 0.378370  [  192/  265]
train() client id: f_00001-7-6 loss: 0.472739  [  224/  265]
train() client id: f_00001-7-7 loss: 0.420304  [  256/  265]
train() client id: f_00001-8-0 loss: 0.399201  [   32/  265]
train() client id: f_00001-8-1 loss: 0.351356  [   64/  265]
train() client id: f_00001-8-2 loss: 0.559161  [   96/  265]
train() client id: f_00001-8-3 loss: 0.358146  [  128/  265]
train() client id: f_00001-8-4 loss: 0.419847  [  160/  265]
train() client id: f_00001-8-5 loss: 0.350965  [  192/  265]
train() client id: f_00001-8-6 loss: 0.281706  [  224/  265]
train() client id: f_00001-8-7 loss: 0.375176  [  256/  265]
train() client id: f_00001-9-0 loss: 0.416084  [   32/  265]
train() client id: f_00001-9-1 loss: 0.501702  [   64/  265]
train() client id: f_00001-9-2 loss: 0.357047  [   96/  265]
train() client id: f_00001-9-3 loss: 0.327468  [  128/  265]
train() client id: f_00001-9-4 loss: 0.359916  [  160/  265]
train() client id: f_00001-9-5 loss: 0.367042  [  192/  265]
train() client id: f_00001-9-6 loss: 0.384155  [  224/  265]
train() client id: f_00001-9-7 loss: 0.309646  [  256/  265]
train() client id: f_00001-10-0 loss: 0.423152  [   32/  265]
train() client id: f_00001-10-1 loss: 0.425678  [   64/  265]
train() client id: f_00001-10-2 loss: 0.392295  [   96/  265]
train() client id: f_00001-10-3 loss: 0.366363  [  128/  265]
train() client id: f_00001-10-4 loss: 0.287609  [  160/  265]
train() client id: f_00001-10-5 loss: 0.271798  [  192/  265]
train() client id: f_00001-10-6 loss: 0.325443  [  224/  265]
train() client id: f_00001-10-7 loss: 0.451512  [  256/  265]
train() client id: f_00001-11-0 loss: 0.435308  [   32/  265]
train() client id: f_00001-11-1 loss: 0.285761  [   64/  265]
train() client id: f_00001-11-2 loss: 0.275042  [   96/  265]
train() client id: f_00001-11-3 loss: 0.504398  [  128/  265]
train() client id: f_00001-11-4 loss: 0.428982  [  160/  265]
train() client id: f_00001-11-5 loss: 0.295180  [  192/  265]
train() client id: f_00001-11-6 loss: 0.373133  [  224/  265]
train() client id: f_00001-11-7 loss: 0.334521  [  256/  265]
train() client id: f_00002-0-0 loss: 1.065694  [   32/  124]
train() client id: f_00002-0-1 loss: 1.249815  [   64/  124]
train() client id: f_00002-0-2 loss: 1.001350  [   96/  124]
train() client id: f_00002-1-0 loss: 0.936626  [   32/  124]
train() client id: f_00002-1-1 loss: 1.073907  [   64/  124]
train() client id: f_00002-1-2 loss: 1.037403  [   96/  124]
train() client id: f_00002-2-0 loss: 1.114597  [   32/  124]
train() client id: f_00002-2-1 loss: 0.904484  [   64/  124]
train() client id: f_00002-2-2 loss: 1.129977  [   96/  124]
train() client id: f_00002-3-0 loss: 0.951774  [   32/  124]
train() client id: f_00002-3-1 loss: 1.041824  [   64/  124]
train() client id: f_00002-3-2 loss: 1.186519  [   96/  124]
train() client id: f_00002-4-0 loss: 0.925951  [   32/  124]
train() client id: f_00002-4-1 loss: 0.920833  [   64/  124]
train() client id: f_00002-4-2 loss: 0.875669  [   96/  124]
train() client id: f_00002-5-0 loss: 0.893657  [   32/  124]
train() client id: f_00002-5-1 loss: 0.778592  [   64/  124]
train() client id: f_00002-5-2 loss: 1.049228  [   96/  124]
train() client id: f_00002-6-0 loss: 1.000800  [   32/  124]
train() client id: f_00002-6-1 loss: 1.085723  [   64/  124]
train() client id: f_00002-6-2 loss: 0.782251  [   96/  124]
train() client id: f_00002-7-0 loss: 1.055189  [   32/  124]
train() client id: f_00002-7-1 loss: 0.948645  [   64/  124]
train() client id: f_00002-7-2 loss: 0.909397  [   96/  124]
train() client id: f_00002-8-0 loss: 1.033324  [   32/  124]
train() client id: f_00002-8-1 loss: 1.010832  [   64/  124]
train() client id: f_00002-8-2 loss: 0.787354  [   96/  124]
train() client id: f_00002-9-0 loss: 1.071366  [   32/  124]
train() client id: f_00002-9-1 loss: 0.863112  [   64/  124]
train() client id: f_00002-9-2 loss: 1.045818  [   96/  124]
train() client id: f_00002-10-0 loss: 1.041621  [   32/  124]
train() client id: f_00002-10-1 loss: 0.766217  [   64/  124]
train() client id: f_00002-10-2 loss: 0.967885  [   96/  124]
train() client id: f_00002-11-0 loss: 0.935927  [   32/  124]
train() client id: f_00002-11-1 loss: 0.879643  [   64/  124]
train() client id: f_00002-11-2 loss: 1.056247  [   96/  124]
train() client id: f_00003-0-0 loss: 0.672876  [   32/   43]
train() client id: f_00003-1-0 loss: 0.675353  [   32/   43]
train() client id: f_00003-2-0 loss: 0.675527  [   32/   43]
train() client id: f_00003-3-0 loss: 0.517822  [   32/   43]
train() client id: f_00003-4-0 loss: 0.899430  [   32/   43]
train() client id: f_00003-5-0 loss: 0.755699  [   32/   43]
train() client id: f_00003-6-0 loss: 0.345461  [   32/   43]
train() client id: f_00003-7-0 loss: 0.525380  [   32/   43]
train() client id: f_00003-8-0 loss: 0.648111  [   32/   43]
train() client id: f_00003-9-0 loss: 0.667039  [   32/   43]
train() client id: f_00003-10-0 loss: 0.714819  [   32/   43]
train() client id: f_00003-11-0 loss: 0.531859  [   32/   43]
train() client id: f_00004-0-0 loss: 0.721428  [   32/  306]
train() client id: f_00004-0-1 loss: 0.768582  [   64/  306]
train() client id: f_00004-0-2 loss: 0.760355  [   96/  306]
train() client id: f_00004-0-3 loss: 0.682305  [  128/  306]
train() client id: f_00004-0-4 loss: 0.730441  [  160/  306]
train() client id: f_00004-0-5 loss: 0.960660  [  192/  306]
train() client id: f_00004-0-6 loss: 0.853859  [  224/  306]
train() client id: f_00004-0-7 loss: 0.848766  [  256/  306]
train() client id: f_00004-0-8 loss: 0.842013  [  288/  306]
train() client id: f_00004-1-0 loss: 0.877170  [   32/  306]
train() client id: f_00004-1-1 loss: 0.798238  [   64/  306]
train() client id: f_00004-1-2 loss: 0.724784  [   96/  306]
train() client id: f_00004-1-3 loss: 0.701768  [  128/  306]
train() client id: f_00004-1-4 loss: 0.856176  [  160/  306]
train() client id: f_00004-1-5 loss: 0.787795  [  192/  306]
train() client id: f_00004-1-6 loss: 0.879679  [  224/  306]
train() client id: f_00004-1-7 loss: 0.760075  [  256/  306]
train() client id: f_00004-1-8 loss: 0.812951  [  288/  306]
train() client id: f_00004-2-0 loss: 0.771628  [   32/  306]
train() client id: f_00004-2-1 loss: 0.789082  [   64/  306]
train() client id: f_00004-2-2 loss: 0.780717  [   96/  306]
train() client id: f_00004-2-3 loss: 0.917687  [  128/  306]
train() client id: f_00004-2-4 loss: 0.964106  [  160/  306]
train() client id: f_00004-2-5 loss: 0.778425  [  192/  306]
train() client id: f_00004-2-6 loss: 0.718455  [  224/  306]
train() client id: f_00004-2-7 loss: 0.758375  [  256/  306]
train() client id: f_00004-2-8 loss: 0.766356  [  288/  306]
train() client id: f_00004-3-0 loss: 0.753046  [   32/  306]
train() client id: f_00004-3-1 loss: 0.792648  [   64/  306]
train() client id: f_00004-3-2 loss: 0.832335  [   96/  306]
train() client id: f_00004-3-3 loss: 0.740156  [  128/  306]
train() client id: f_00004-3-4 loss: 0.807389  [  160/  306]
train() client id: f_00004-3-5 loss: 0.764496  [  192/  306]
train() client id: f_00004-3-6 loss: 0.793814  [  224/  306]
train() client id: f_00004-3-7 loss: 0.784197  [  256/  306]
train() client id: f_00004-3-8 loss: 0.999989  [  288/  306]
train() client id: f_00004-4-0 loss: 0.797299  [   32/  306]
train() client id: f_00004-4-1 loss: 0.818389  [   64/  306]
train() client id: f_00004-4-2 loss: 0.718387  [   96/  306]
train() client id: f_00004-4-3 loss: 0.869805  [  128/  306]
train() client id: f_00004-4-4 loss: 0.790697  [  160/  306]
train() client id: f_00004-4-5 loss: 0.818235  [  192/  306]
train() client id: f_00004-4-6 loss: 0.723768  [  224/  306]
train() client id: f_00004-4-7 loss: 0.758002  [  256/  306]
train() client id: f_00004-4-8 loss: 0.917515  [  288/  306]
train() client id: f_00004-5-0 loss: 0.818904  [   32/  306]
train() client id: f_00004-5-1 loss: 0.771842  [   64/  306]
train() client id: f_00004-5-2 loss: 0.818848  [   96/  306]
train() client id: f_00004-5-3 loss: 0.871327  [  128/  306]
train() client id: f_00004-5-4 loss: 0.816104  [  160/  306]
train() client id: f_00004-5-5 loss: 0.769559  [  192/  306]
train() client id: f_00004-5-6 loss: 0.784277  [  224/  306]
train() client id: f_00004-5-7 loss: 0.861179  [  256/  306]
train() client id: f_00004-5-8 loss: 0.773408  [  288/  306]
train() client id: f_00004-6-0 loss: 0.798178  [   32/  306]
train() client id: f_00004-6-1 loss: 0.809714  [   64/  306]
train() client id: f_00004-6-2 loss: 0.630172  [   96/  306]
train() client id: f_00004-6-3 loss: 0.859957  [  128/  306]
train() client id: f_00004-6-4 loss: 0.827070  [  160/  306]
train() client id: f_00004-6-5 loss: 0.889330  [  192/  306]
train() client id: f_00004-6-6 loss: 0.805931  [  224/  306]
train() client id: f_00004-6-7 loss: 0.860191  [  256/  306]
train() client id: f_00004-6-8 loss: 0.731625  [  288/  306]
train() client id: f_00004-7-0 loss: 0.804600  [   32/  306]
train() client id: f_00004-7-1 loss: 0.862512  [   64/  306]
train() client id: f_00004-7-2 loss: 0.805581  [   96/  306]
train() client id: f_00004-7-3 loss: 0.743197  [  128/  306]
train() client id: f_00004-7-4 loss: 0.868380  [  160/  306]
train() client id: f_00004-7-5 loss: 0.795534  [  192/  306]
train() client id: f_00004-7-6 loss: 0.809906  [  224/  306]
train() client id: f_00004-7-7 loss: 0.782833  [  256/  306]
train() client id: f_00004-7-8 loss: 0.804521  [  288/  306]
train() client id: f_00004-8-0 loss: 0.879570  [   32/  306]
train() client id: f_00004-8-1 loss: 0.801362  [   64/  306]
train() client id: f_00004-8-2 loss: 0.709695  [   96/  306]
train() client id: f_00004-8-3 loss: 0.822345  [  128/  306]
train() client id: f_00004-8-4 loss: 0.745586  [  160/  306]
train() client id: f_00004-8-5 loss: 0.890009  [  192/  306]
train() client id: f_00004-8-6 loss: 0.778101  [  224/  306]
train() client id: f_00004-8-7 loss: 0.975221  [  256/  306]
train() client id: f_00004-8-8 loss: 0.770125  [  288/  306]
train() client id: f_00004-9-0 loss: 0.812696  [   32/  306]
train() client id: f_00004-9-1 loss: 0.733939  [   64/  306]
train() client id: f_00004-9-2 loss: 0.969641  [   96/  306]
train() client id: f_00004-9-3 loss: 0.773088  [  128/  306]
train() client id: f_00004-9-4 loss: 0.763728  [  160/  306]
train() client id: f_00004-9-5 loss: 0.770288  [  192/  306]
train() client id: f_00004-9-6 loss: 0.819166  [  224/  306]
train() client id: f_00004-9-7 loss: 0.927137  [  256/  306]
train() client id: f_00004-9-8 loss: 0.768562  [  288/  306]
train() client id: f_00004-10-0 loss: 0.901256  [   32/  306]
train() client id: f_00004-10-1 loss: 0.736866  [   64/  306]
train() client id: f_00004-10-2 loss: 0.827569  [   96/  306]
train() client id: f_00004-10-3 loss: 0.680848  [  128/  306]
train() client id: f_00004-10-4 loss: 0.853481  [  160/  306]
train() client id: f_00004-10-5 loss: 0.824247  [  192/  306]
train() client id: f_00004-10-6 loss: 0.857362  [  224/  306]
train() client id: f_00004-10-7 loss: 0.867041  [  256/  306]
train() client id: f_00004-10-8 loss: 0.740138  [  288/  306]
train() client id: f_00004-11-0 loss: 0.831109  [   32/  306]
train() client id: f_00004-11-1 loss: 0.913011  [   64/  306]
train() client id: f_00004-11-2 loss: 0.934837  [   96/  306]
train() client id: f_00004-11-3 loss: 0.762564  [  128/  306]
train() client id: f_00004-11-4 loss: 0.776268  [  160/  306]
train() client id: f_00004-11-5 loss: 0.845673  [  192/  306]
train() client id: f_00004-11-6 loss: 0.718852  [  224/  306]
train() client id: f_00004-11-7 loss: 0.768705  [  256/  306]
train() client id: f_00004-11-8 loss: 0.783950  [  288/  306]
train() client id: f_00005-0-0 loss: 0.573899  [   32/  146]
train() client id: f_00005-0-1 loss: 0.527224  [   64/  146]
train() client id: f_00005-0-2 loss: 0.612089  [   96/  146]
train() client id: f_00005-0-3 loss: 0.429870  [  128/  146]
train() client id: f_00005-1-0 loss: 0.585957  [   32/  146]
train() client id: f_00005-1-1 loss: 0.452322  [   64/  146]
train() client id: f_00005-1-2 loss: 0.324060  [   96/  146]
train() client id: f_00005-1-3 loss: 0.683531  [  128/  146]
train() client id: f_00005-2-0 loss: 0.543892  [   32/  146]
train() client id: f_00005-2-1 loss: 0.559014  [   64/  146]
train() client id: f_00005-2-2 loss: 0.673329  [   96/  146]
train() client id: f_00005-2-3 loss: 0.388969  [  128/  146]
train() client id: f_00005-3-0 loss: 0.621829  [   32/  146]
train() client id: f_00005-3-1 loss: 0.413085  [   64/  146]
train() client id: f_00005-3-2 loss: 0.446722  [   96/  146]
train() client id: f_00005-3-3 loss: 0.734775  [  128/  146]
train() client id: f_00005-4-0 loss: 0.436839  [   32/  146]
train() client id: f_00005-4-1 loss: 0.458215  [   64/  146]
train() client id: f_00005-4-2 loss: 0.886172  [   96/  146]
train() client id: f_00005-4-3 loss: 0.316110  [  128/  146]
train() client id: f_00005-5-0 loss: 0.370057  [   32/  146]
train() client id: f_00005-5-1 loss: 0.651524  [   64/  146]
train() client id: f_00005-5-2 loss: 0.454550  [   96/  146]
train() client id: f_00005-5-3 loss: 0.801245  [  128/  146]
train() client id: f_00005-6-0 loss: 0.516128  [   32/  146]
train() client id: f_00005-6-1 loss: 0.557368  [   64/  146]
train() client id: f_00005-6-2 loss: 0.507588  [   96/  146]
train() client id: f_00005-6-3 loss: 0.579327  [  128/  146]
train() client id: f_00005-7-0 loss: 0.444461  [   32/  146]
train() client id: f_00005-7-1 loss: 0.579231  [   64/  146]
train() client id: f_00005-7-2 loss: 0.607916  [   96/  146]
train() client id: f_00005-7-3 loss: 0.660572  [  128/  146]
train() client id: f_00005-8-0 loss: 0.643000  [   32/  146]
train() client id: f_00005-8-1 loss: 0.507577  [   64/  146]
train() client id: f_00005-8-2 loss: 0.489446  [   96/  146]
train() client id: f_00005-8-3 loss: 0.585451  [  128/  146]
train() client id: f_00005-9-0 loss: 0.340091  [   32/  146]
train() client id: f_00005-9-1 loss: 0.480577  [   64/  146]
train() client id: f_00005-9-2 loss: 0.644172  [   96/  146]
train() client id: f_00005-9-3 loss: 0.858095  [  128/  146]
train() client id: f_00005-10-0 loss: 0.518362  [   32/  146]
train() client id: f_00005-10-1 loss: 0.590150  [   64/  146]
train() client id: f_00005-10-2 loss: 0.482637  [   96/  146]
train() client id: f_00005-10-3 loss: 0.520101  [  128/  146]
train() client id: f_00005-11-0 loss: 0.699822  [   32/  146]
train() client id: f_00005-11-1 loss: 0.632605  [   64/  146]
train() client id: f_00005-11-2 loss: 0.333267  [   96/  146]
train() client id: f_00005-11-3 loss: 0.523584  [  128/  146]
train() client id: f_00006-0-0 loss: 0.549193  [   32/   54]
train() client id: f_00006-1-0 loss: 0.529599  [   32/   54]
train() client id: f_00006-2-0 loss: 0.543945  [   32/   54]
train() client id: f_00006-3-0 loss: 0.479429  [   32/   54]
train() client id: f_00006-4-0 loss: 0.508703  [   32/   54]
train() client id: f_00006-5-0 loss: 0.523486  [   32/   54]
train() client id: f_00006-6-0 loss: 0.533592  [   32/   54]
train() client id: f_00006-7-0 loss: 0.568025  [   32/   54]
train() client id: f_00006-8-0 loss: 0.578593  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586384  [   32/   54]
train() client id: f_00006-10-0 loss: 0.570700  [   32/   54]
train() client id: f_00006-11-0 loss: 0.576150  [   32/   54]
train() client id: f_00007-0-0 loss: 0.532984  [   32/  179]
train() client id: f_00007-0-1 loss: 0.503987  [   64/  179]
train() client id: f_00007-0-2 loss: 0.431763  [   96/  179]
train() client id: f_00007-0-3 loss: 0.721457  [  128/  179]
train() client id: f_00007-0-4 loss: 0.428733  [  160/  179]
train() client id: f_00007-1-0 loss: 0.574380  [   32/  179]
train() client id: f_00007-1-1 loss: 0.540487  [   64/  179]
train() client id: f_00007-1-2 loss: 0.540634  [   96/  179]
train() client id: f_00007-1-3 loss: 0.490030  [  128/  179]
train() client id: f_00007-1-4 loss: 0.565296  [  160/  179]
train() client id: f_00007-2-0 loss: 0.628467  [   32/  179]
train() client id: f_00007-2-1 loss: 0.526013  [   64/  179]
train() client id: f_00007-2-2 loss: 0.599470  [   96/  179]
train() client id: f_00007-2-3 loss: 0.427950  [  128/  179]
train() client id: f_00007-2-4 loss: 0.551011  [  160/  179]
train() client id: f_00007-3-0 loss: 0.362379  [   32/  179]
train() client id: f_00007-3-1 loss: 0.456499  [   64/  179]
train() client id: f_00007-3-2 loss: 0.604139  [   96/  179]
train() client id: f_00007-3-3 loss: 0.650365  [  128/  179]
train() client id: f_00007-3-4 loss: 0.603538  [  160/  179]
train() client id: f_00007-4-0 loss: 0.480135  [   32/  179]
train() client id: f_00007-4-1 loss: 0.371050  [   64/  179]
train() client id: f_00007-4-2 loss: 0.691661  [   96/  179]
train() client id: f_00007-4-3 loss: 0.734821  [  128/  179]
train() client id: f_00007-4-4 loss: 0.384215  [  160/  179]
train() client id: f_00007-5-0 loss: 0.641494  [   32/  179]
train() client id: f_00007-5-1 loss: 0.465247  [   64/  179]
train() client id: f_00007-5-2 loss: 0.380883  [   96/  179]
train() client id: f_00007-5-3 loss: 0.590038  [  128/  179]
train() client id: f_00007-5-4 loss: 0.547691  [  160/  179]
train() client id: f_00007-6-0 loss: 0.511885  [   32/  179]
train() client id: f_00007-6-1 loss: 0.560342  [   64/  179]
train() client id: f_00007-6-2 loss: 0.336518  [   96/  179]
train() client id: f_00007-6-3 loss: 0.728127  [  128/  179]
train() client id: f_00007-6-4 loss: 0.495091  [  160/  179]
train() client id: f_00007-7-0 loss: 0.499160  [   32/  179]
train() client id: f_00007-7-1 loss: 0.520597  [   64/  179]
train() client id: f_00007-7-2 loss: 0.445070  [   96/  179]
train() client id: f_00007-7-3 loss: 0.518473  [  128/  179]
train() client id: f_00007-7-4 loss: 0.562121  [  160/  179]
train() client id: f_00007-8-0 loss: 0.786880  [   32/  179]
train() client id: f_00007-8-1 loss: 0.385238  [   64/  179]
train() client id: f_00007-8-2 loss: 0.349367  [   96/  179]
train() client id: f_00007-8-3 loss: 0.451927  [  128/  179]
train() client id: f_00007-8-4 loss: 0.468333  [  160/  179]
train() client id: f_00007-9-0 loss: 0.631751  [   32/  179]
train() client id: f_00007-9-1 loss: 0.374788  [   64/  179]
train() client id: f_00007-9-2 loss: 0.667480  [   96/  179]
train() client id: f_00007-9-3 loss: 0.412689  [  128/  179]
train() client id: f_00007-9-4 loss: 0.527545  [  160/  179]
train() client id: f_00007-10-0 loss: 0.450654  [   32/  179]
train() client id: f_00007-10-1 loss: 0.502767  [   64/  179]
train() client id: f_00007-10-2 loss: 0.361904  [   96/  179]
train() client id: f_00007-10-3 loss: 0.439666  [  128/  179]
train() client id: f_00007-10-4 loss: 0.550550  [  160/  179]
train() client id: f_00007-11-0 loss: 0.582515  [   32/  179]
train() client id: f_00007-11-1 loss: 0.516991  [   64/  179]
train() client id: f_00007-11-2 loss: 0.473445  [   96/  179]
train() client id: f_00007-11-3 loss: 0.356297  [  128/  179]
train() client id: f_00007-11-4 loss: 0.388516  [  160/  179]
train() client id: f_00008-0-0 loss: 0.732296  [   32/  130]
train() client id: f_00008-0-1 loss: 0.750087  [   64/  130]
train() client id: f_00008-0-2 loss: 0.614327  [   96/  130]
train() client id: f_00008-0-3 loss: 0.668941  [  128/  130]
train() client id: f_00008-1-0 loss: 0.712407  [   32/  130]
train() client id: f_00008-1-1 loss: 0.804816  [   64/  130]
train() client id: f_00008-1-2 loss: 0.618078  [   96/  130]
train() client id: f_00008-1-3 loss: 0.677069  [  128/  130]
train() client id: f_00008-2-0 loss: 0.744897  [   32/  130]
train() client id: f_00008-2-1 loss: 0.678752  [   64/  130]
train() client id: f_00008-2-2 loss: 0.647043  [   96/  130]
train() client id: f_00008-2-3 loss: 0.736911  [  128/  130]
train() client id: f_00008-3-0 loss: 0.660254  [   32/  130]
train() client id: f_00008-3-1 loss: 0.616045  [   64/  130]
train() client id: f_00008-3-2 loss: 0.775202  [   96/  130]
train() client id: f_00008-3-3 loss: 0.712595  [  128/  130]
train() client id: f_00008-4-0 loss: 0.629306  [   32/  130]
train() client id: f_00008-4-1 loss: 0.748767  [   64/  130]
train() client id: f_00008-4-2 loss: 0.732248  [   96/  130]
train() client id: f_00008-4-3 loss: 0.655456  [  128/  130]
train() client id: f_00008-5-0 loss: 0.716101  [   32/  130]
train() client id: f_00008-5-1 loss: 0.683702  [   64/  130]
train() client id: f_00008-5-2 loss: 0.614368  [   96/  130]
train() client id: f_00008-5-3 loss: 0.792217  [  128/  130]
train() client id: f_00008-6-0 loss: 0.777531  [   32/  130]
train() client id: f_00008-6-1 loss: 0.687171  [   64/  130]
train() client id: f_00008-6-2 loss: 0.617677  [   96/  130]
train() client id: f_00008-6-3 loss: 0.710926  [  128/  130]
train() client id: f_00008-7-0 loss: 0.675978  [   32/  130]
train() client id: f_00008-7-1 loss: 0.739267  [   64/  130]
train() client id: f_00008-7-2 loss: 0.663194  [   96/  130]
train() client id: f_00008-7-3 loss: 0.722364  [  128/  130]
train() client id: f_00008-8-0 loss: 0.641603  [   32/  130]
train() client id: f_00008-8-1 loss: 0.750627  [   64/  130]
train() client id: f_00008-8-2 loss: 0.648389  [   96/  130]
train() client id: f_00008-8-3 loss: 0.725821  [  128/  130]
train() client id: f_00008-9-0 loss: 0.537244  [   32/  130]
train() client id: f_00008-9-1 loss: 0.772457  [   64/  130]
train() client id: f_00008-9-2 loss: 0.761633  [   96/  130]
train() client id: f_00008-9-3 loss: 0.701222  [  128/  130]
train() client id: f_00008-10-0 loss: 0.678701  [   32/  130]
train() client id: f_00008-10-1 loss: 0.764161  [   64/  130]
train() client id: f_00008-10-2 loss: 0.690060  [   96/  130]
train() client id: f_00008-10-3 loss: 0.636594  [  128/  130]
train() client id: f_00008-11-0 loss: 0.575823  [   32/  130]
train() client id: f_00008-11-1 loss: 0.702615  [   64/  130]
train() client id: f_00008-11-2 loss: 0.770451  [   96/  130]
train() client id: f_00008-11-3 loss: 0.744752  [  128/  130]
train() client id: f_00009-0-0 loss: 0.934207  [   32/  118]
train() client id: f_00009-0-1 loss: 0.906118  [   64/  118]
train() client id: f_00009-0-2 loss: 1.065423  [   96/  118]
train() client id: f_00009-1-0 loss: 0.930977  [   32/  118]
train() client id: f_00009-1-1 loss: 1.172323  [   64/  118]
train() client id: f_00009-1-2 loss: 0.806365  [   96/  118]
train() client id: f_00009-2-0 loss: 0.981906  [   32/  118]
train() client id: f_00009-2-1 loss: 0.738170  [   64/  118]
train() client id: f_00009-2-2 loss: 0.988260  [   96/  118]
train() client id: f_00009-3-0 loss: 0.936354  [   32/  118]
train() client id: f_00009-3-1 loss: 0.883511  [   64/  118]
train() client id: f_00009-3-2 loss: 0.811148  [   96/  118]
train() client id: f_00009-4-0 loss: 0.930629  [   32/  118]
train() client id: f_00009-4-1 loss: 0.864989  [   64/  118]
train() client id: f_00009-4-2 loss: 0.823151  [   96/  118]
train() client id: f_00009-5-0 loss: 0.919489  [   32/  118]
train() client id: f_00009-5-1 loss: 0.827166  [   64/  118]
train() client id: f_00009-5-2 loss: 0.826709  [   96/  118]
train() client id: f_00009-6-0 loss: 0.902562  [   32/  118]
train() client id: f_00009-6-1 loss: 0.945837  [   64/  118]
train() client id: f_00009-6-2 loss: 0.805607  [   96/  118]
train() client id: f_00009-7-0 loss: 1.068319  [   32/  118]
train() client id: f_00009-7-1 loss: 0.701752  [   64/  118]
train() client id: f_00009-7-2 loss: 0.917783  [   96/  118]
train() client id: f_00009-8-0 loss: 0.736749  [   32/  118]
train() client id: f_00009-8-1 loss: 0.952229  [   64/  118]
train() client id: f_00009-8-2 loss: 0.833560  [   96/  118]
train() client id: f_00009-9-0 loss: 0.881110  [   32/  118]
train() client id: f_00009-9-1 loss: 0.867223  [   64/  118]
train() client id: f_00009-9-2 loss: 0.894992  [   96/  118]
train() client id: f_00009-10-0 loss: 0.802636  [   32/  118]
train() client id: f_00009-10-1 loss: 1.025595  [   64/  118]
train() client id: f_00009-10-2 loss: 0.865015  [   96/  118]
train() client id: f_00009-11-0 loss: 0.931500  [   32/  118]
train() client id: f_00009-11-1 loss: 0.794414  [   64/  118]
train() client id: f_00009-11-2 loss: 0.908887  [   96/  118]
At round 48 accuracy: 0.6445623342175066
At round 48 training accuracy: 0.5928906773977196
At round 48 training loss: 0.8291380586327889
update_location
xs = [  -3.9056584     4.20031788  260.00902392   18.81129433    0.97929623
    3.95640986 -222.44319194 -201.32485185  244.66397685 -187.06087855]
ys = [ 252.5879595   235.55583871    1.32061395 -222.45517586  214.35018685
  197.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [271.6908748  255.93787492 278.57931822 244.62250522 236.53110075
 221.68917748 243.90134108 224.79406626 264.89456684 212.15038096]
dists_bs = [187.86475237 189.15539519 468.39874042 442.19162659 180.32512278
 180.40477296 183.85181945 176.15576144 448.22134189 171.42332987]
uav_gains = [4.75955329e-12 6.59322551e-12 4.11980872e-12 8.22994915e-12
 9.55255008e-12 1.22821086e-11 8.34286394e-12 1.16790101e-11
 5.48618892e-12 1.42491555e-11]
bs_gains = [4.74818064e-11 4.65802305e-11 3.67764727e-12 4.32100377e-12
 5.32520858e-11 5.31862805e-11 5.04410218e-11 5.68568637e-11
 4.16020754e-12 6.13618281e-11]
Round 49
-------------------------------
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.53180875  9.28555743  4.47092374  1.6235926  10.7068955   5.1513321
  2.0049889   6.33296008  4.67865356  4.17679842]
obj_prev = 52.96351106757935
eta_min = 4.606985121778528e-21	eta_max = 0.9400638118405741
af = 11.129531482767057	bf = 1.180489388448874	zeta = 12.242484631043764	eta = 0.909090909090909
af = 11.129531482767057	bf = 1.180489388448874	zeta = 24.65371738730223	eta = 0.45143421204703454
af = 11.129531482767057	bf = 1.180489388448874	zeta = 18.287853905358034	eta = 0.608575043324591
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.136885852504452	eta = 0.6494488892881634
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.071238826404898	eta = 0.6519463289068677
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.07100360444631	eta = 0.6519553120982448
eta = 0.6519553120982448
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.03529932 0.07424063 0.03473901 0.01204659 0.08572692 0.04090239
 0.01512828 0.05014745 0.03641993 0.03305809]
ene_total = [1.60196419 2.67053184 1.60796312 0.76579092 3.04020143 1.57233453
 0.86514606 1.976894   1.66129063 1.30888688]
ti_comp = [0.70507767 0.76819322 0.69764277 0.72788409 0.77017386 0.77015604
 0.7283619  0.73907045 0.6981679  0.77215943]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [5.52974468e-06 4.33375200e-05 5.38351868e-06 2.06228308e-07
 6.63825764e-05 7.21055148e-06 4.07900477e-07 1.44296163e-05
 6.19409234e-06 3.78703601e-06]
ene_total = [0.44615416 0.2406999  0.47049736 0.37129277 0.23496834 0.23308889
 0.36973465 0.33512519 0.46880422 0.22641604]
optimize_network iter = 0 obj = 3.396781522582919
eta = 0.6519553120982448
freqs = [25032220.23451398 48321587.59240132 24897421.60469212  8275076.45541563
 55654263.85367705 26554609.35010472 10385139.39706882 33926025.18152012
 26082502.95796476 21406261.0487775 ]
eta_min = 0.6519553120982468	eta_max = 0.6979914102198819
af = 0.004073073698070901	bf = 1.180489388448874	zeta = 0.004480381067877992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.23203954e-06 9.65569680e-06 1.19946005e-06 4.59481302e-08
 1.47901871e-05 1.60652707e-06 9.08811424e-08 3.21495093e-06
 1.38005769e-06 8.43760128e-07]
ene_total = [1.70755859 0.91729463 1.80077061 1.42147114 0.89310582 0.89167624
 1.41548626 1.28161791 1.79420944 0.86646288]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 1 obj = 3.9135003002771818
eta = 0.6979914102198819
freqs = [24982229.19460944 47493855.941672   24897421.60469214  8210343.90468077
 54677108.13463603 26088475.84559726 10302681.03588401 33568945.36426646
 26078780.65421337 21021289.55092173]
eta_min = 0.697991410219886	eta_max = 0.6979914102198779
af = 0.003953064498406836	bf = 1.180489388448874	zeta = 0.004348370948247519	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.22712352e-06 9.32773270e-06 1.19946005e-06 4.52320749e-08
 1.42753859e-05 1.55062088e-06 8.94436732e-08 3.14763077e-06
 1.37966381e-06 8.13684556e-07]
ene_total = [1.70755797 0.91725351 1.80077061 1.42147105 0.89304128 0.89166923
 1.41548608 1.28160947 1.79420939 0.86645911]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 2 obj = 3.9135003002771303
eta = 0.6979914102198779
freqs = [24982229.19460944 47493855.94167206 24897421.60469212  8210343.90468077
 54677108.13463611 26088475.8455973  10302681.03588402 33568945.36426648
 26078780.65421337 21021289.55092177]
Done!
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.32509460e-06 3.28763371e-05 4.22759253e-06 1.59424052e-07
 5.03147350e-05 5.46528683e-06 3.15251352e-07 1.10940754e-05
 4.86273502e-06 2.86789605e-06]
ene_total = [0.01362255 0.00733955 0.01436594 0.01133774 0.00715892 0.00711585
 0.01129012 0.01023004 0.01431406 0.00691292]
At round 49 energy consumption: 0.10368768459529902
At round 49 eta: 0.6979914102198779
At round 49 a_n: 11.397856354757828
At round 49 local rounds: 11.773444044723618
At round 49 global rounds: 37.74017276480797
gradient difference: 0.418490469455719
train() client id: f_00000-0-0 loss: 1.269491  [   32/  126]
train() client id: f_00000-0-1 loss: 1.335805  [   64/  126]
train() client id: f_00000-0-2 loss: 1.169755  [   96/  126]
train() client id: f_00000-1-0 loss: 0.951418  [   32/  126]
train() client id: f_00000-1-1 loss: 1.154853  [   64/  126]
train() client id: f_00000-1-2 loss: 1.104024  [   96/  126]
train() client id: f_00000-2-0 loss: 1.081385  [   32/  126]
train() client id: f_00000-2-1 loss: 0.994657  [   64/  126]
train() client id: f_00000-2-2 loss: 1.105606  [   96/  126]
train() client id: f_00000-3-0 loss: 0.998841  [   32/  126]
train() client id: f_00000-3-1 loss: 0.835678  [   64/  126]
train() client id: f_00000-3-2 loss: 1.102244  [   96/  126]
train() client id: f_00000-4-0 loss: 0.894881  [   32/  126]
train() client id: f_00000-4-1 loss: 0.955026  [   64/  126]
train() client id: f_00000-4-2 loss: 0.978454  [   96/  126]
train() client id: f_00000-5-0 loss: 0.901590  [   32/  126]
train() client id: f_00000-5-1 loss: 0.772216  [   64/  126]
train() client id: f_00000-5-2 loss: 0.853744  [   96/  126]
train() client id: f_00000-6-0 loss: 0.848443  [   32/  126]
train() client id: f_00000-6-1 loss: 0.804680  [   64/  126]
train() client id: f_00000-6-2 loss: 0.894158  [   96/  126]
train() client id: f_00000-7-0 loss: 0.886473  [   32/  126]
train() client id: f_00000-7-1 loss: 0.709496  [   64/  126]
train() client id: f_00000-7-2 loss: 0.958053  [   96/  126]
train() client id: f_00000-8-0 loss: 0.836981  [   32/  126]
train() client id: f_00000-8-1 loss: 0.934780  [   64/  126]
train() client id: f_00000-8-2 loss: 0.785306  [   96/  126]
train() client id: f_00000-9-0 loss: 0.787001  [   32/  126]
train() client id: f_00000-9-1 loss: 0.663803  [   64/  126]
train() client id: f_00000-9-2 loss: 0.979367  [   96/  126]
train() client id: f_00000-10-0 loss: 0.844474  [   32/  126]
train() client id: f_00000-10-1 loss: 0.793750  [   64/  126]
train() client id: f_00000-10-2 loss: 0.911743  [   96/  126]
train() client id: f_00001-0-0 loss: 0.370175  [   32/  265]
train() client id: f_00001-0-1 loss: 0.495953  [   64/  265]
train() client id: f_00001-0-2 loss: 0.464907  [   96/  265]
train() client id: f_00001-0-3 loss: 0.440562  [  128/  265]
train() client id: f_00001-0-4 loss: 0.446697  [  160/  265]
train() client id: f_00001-0-5 loss: 0.497942  [  192/  265]
train() client id: f_00001-0-6 loss: 0.421712  [  224/  265]
train() client id: f_00001-0-7 loss: 0.614471  [  256/  265]
train() client id: f_00001-1-0 loss: 0.486341  [   32/  265]
train() client id: f_00001-1-1 loss: 0.433377  [   64/  265]
train() client id: f_00001-1-2 loss: 0.399524  [   96/  265]
train() client id: f_00001-1-3 loss: 0.382258  [  128/  265]
train() client id: f_00001-1-4 loss: 0.409293  [  160/  265]
train() client id: f_00001-1-5 loss: 0.480385  [  192/  265]
train() client id: f_00001-1-6 loss: 0.528777  [  224/  265]
train() client id: f_00001-1-7 loss: 0.558958  [  256/  265]
train() client id: f_00001-2-0 loss: 0.418189  [   32/  265]
train() client id: f_00001-2-1 loss: 0.488584  [   64/  265]
train() client id: f_00001-2-2 loss: 0.518424  [   96/  265]
train() client id: f_00001-2-3 loss: 0.410582  [  128/  265]
train() client id: f_00001-2-4 loss: 0.506252  [  160/  265]
train() client id: f_00001-2-5 loss: 0.364993  [  192/  265]
train() client id: f_00001-2-6 loss: 0.448916  [  224/  265]
train() client id: f_00001-2-7 loss: 0.400448  [  256/  265]
train() client id: f_00001-3-0 loss: 0.411465  [   32/  265]
train() client id: f_00001-3-1 loss: 0.419054  [   64/  265]
train() client id: f_00001-3-2 loss: 0.394605  [   96/  265]
train() client id: f_00001-3-3 loss: 0.436272  [  128/  265]
train() client id: f_00001-3-4 loss: 0.447487  [  160/  265]
train() client id: f_00001-3-5 loss: 0.503111  [  192/  265]
train() client id: f_00001-3-6 loss: 0.427365  [  224/  265]
train() client id: f_00001-3-7 loss: 0.480303  [  256/  265]
train() client id: f_00001-4-0 loss: 0.461259  [   32/  265]
train() client id: f_00001-4-1 loss: 0.439483  [   64/  265]
train() client id: f_00001-4-2 loss: 0.510366  [   96/  265]
train() client id: f_00001-4-3 loss: 0.469364  [  128/  265]
train() client id: f_00001-4-4 loss: 0.495148  [  160/  265]
train() client id: f_00001-4-5 loss: 0.414875  [  192/  265]
train() client id: f_00001-4-6 loss: 0.345161  [  224/  265]
train() client id: f_00001-4-7 loss: 0.417716  [  256/  265]
train() client id: f_00001-5-0 loss: 0.561249  [   32/  265]
train() client id: f_00001-5-1 loss: 0.374998  [   64/  265]
train() client id: f_00001-5-2 loss: 0.378353  [   96/  265]
train() client id: f_00001-5-3 loss: 0.481105  [  128/  265]
train() client id: f_00001-5-4 loss: 0.484440  [  160/  265]
train() client id: f_00001-5-5 loss: 0.399809  [  192/  265]
train() client id: f_00001-5-6 loss: 0.337169  [  224/  265]
train() client id: f_00001-5-7 loss: 0.337125  [  256/  265]
train() client id: f_00001-6-0 loss: 0.449324  [   32/  265]
train() client id: f_00001-6-1 loss: 0.479639  [   64/  265]
train() client id: f_00001-6-2 loss: 0.478829  [   96/  265]
train() client id: f_00001-6-3 loss: 0.595150  [  128/  265]
train() client id: f_00001-6-4 loss: 0.434794  [  160/  265]
train() client id: f_00001-6-5 loss: 0.366200  [  192/  265]
train() client id: f_00001-6-6 loss: 0.396086  [  224/  265]
train() client id: f_00001-6-7 loss: 0.343641  [  256/  265]
train() client id: f_00001-7-0 loss: 0.354775  [   32/  265]
train() client id: f_00001-7-1 loss: 0.470276  [   64/  265]
train() client id: f_00001-7-2 loss: 0.335341  [   96/  265]
train() client id: f_00001-7-3 loss: 0.428755  [  128/  265]
train() client id: f_00001-7-4 loss: 0.476426  [  160/  265]
train() client id: f_00001-7-5 loss: 0.495709  [  192/  265]
train() client id: f_00001-7-6 loss: 0.419514  [  224/  265]
train() client id: f_00001-7-7 loss: 0.540322  [  256/  265]
train() client id: f_00001-8-0 loss: 0.397324  [   32/  265]
train() client id: f_00001-8-1 loss: 0.363787  [   64/  265]
train() client id: f_00001-8-2 loss: 0.358488  [   96/  265]
train() client id: f_00001-8-3 loss: 0.540814  [  128/  265]
train() client id: f_00001-8-4 loss: 0.399251  [  160/  265]
train() client id: f_00001-8-5 loss: 0.431998  [  192/  265]
train() client id: f_00001-8-6 loss: 0.439304  [  224/  265]
train() client id: f_00001-8-7 loss: 0.546217  [  256/  265]
train() client id: f_00001-9-0 loss: 0.345465  [   32/  265]
train() client id: f_00001-9-1 loss: 0.466152  [   64/  265]
train() client id: f_00001-9-2 loss: 0.349174  [   96/  265]
train() client id: f_00001-9-3 loss: 0.470689  [  128/  265]
train() client id: f_00001-9-4 loss: 0.495129  [  160/  265]
train() client id: f_00001-9-5 loss: 0.552885  [  192/  265]
train() client id: f_00001-9-6 loss: 0.460095  [  224/  265]
train() client id: f_00001-9-7 loss: 0.329731  [  256/  265]
train() client id: f_00001-10-0 loss: 0.577011  [   32/  265]
train() client id: f_00001-10-1 loss: 0.459628  [   64/  265]
train() client id: f_00001-10-2 loss: 0.427171  [   96/  265]
train() client id: f_00001-10-3 loss: 0.348401  [  128/  265]
train() client id: f_00001-10-4 loss: 0.391575  [  160/  265]
train() client id: f_00001-10-5 loss: 0.526062  [  192/  265]
train() client id: f_00001-10-6 loss: 0.351719  [  224/  265]
train() client id: f_00001-10-7 loss: 0.423569  [  256/  265]
train() client id: f_00002-0-0 loss: 1.010159  [   32/  124]
train() client id: f_00002-0-1 loss: 1.058219  [   64/  124]
train() client id: f_00002-0-2 loss: 1.079647  [   96/  124]
train() client id: f_00002-1-0 loss: 1.017138  [   32/  124]
train() client id: f_00002-1-1 loss: 0.982526  [   64/  124]
train() client id: f_00002-1-2 loss: 1.010523  [   96/  124]
train() client id: f_00002-2-0 loss: 0.827278  [   32/  124]
train() client id: f_00002-2-1 loss: 0.964364  [   64/  124]
train() client id: f_00002-2-2 loss: 1.085212  [   96/  124]
train() client id: f_00002-3-0 loss: 0.946743  [   32/  124]
train() client id: f_00002-3-1 loss: 0.984792  [   64/  124]
train() client id: f_00002-3-2 loss: 0.912605  [   96/  124]
train() client id: f_00002-4-0 loss: 0.883098  [   32/  124]
train() client id: f_00002-4-1 loss: 0.904014  [   64/  124]
train() client id: f_00002-4-2 loss: 0.775206  [   96/  124]
train() client id: f_00002-5-0 loss: 0.902131  [   32/  124]
train() client id: f_00002-5-1 loss: 0.903010  [   64/  124]
train() client id: f_00002-5-2 loss: 0.738888  [   96/  124]
train() client id: f_00002-6-0 loss: 0.983677  [   32/  124]
train() client id: f_00002-6-1 loss: 0.708935  [   64/  124]
train() client id: f_00002-6-2 loss: 0.823551  [   96/  124]
train() client id: f_00002-7-0 loss: 0.774715  [   32/  124]
train() client id: f_00002-7-1 loss: 1.015672  [   64/  124]
train() client id: f_00002-7-2 loss: 0.959642  [   96/  124]
train() client id: f_00002-8-0 loss: 0.999936  [   32/  124]
train() client id: f_00002-8-1 loss: 0.907859  [   64/  124]
train() client id: f_00002-8-2 loss: 0.743180  [   96/  124]
train() client id: f_00002-9-0 loss: 0.945076  [   32/  124]
train() client id: f_00002-9-1 loss: 0.854490  [   64/  124]
train() client id: f_00002-9-2 loss: 0.885008  [   96/  124]
train() client id: f_00002-10-0 loss: 0.969082  [   32/  124]
train() client id: f_00002-10-1 loss: 0.900263  [   64/  124]
train() client id: f_00002-10-2 loss: 0.792298  [   96/  124]
train() client id: f_00003-0-0 loss: 0.889942  [   32/   43]
train() client id: f_00003-1-0 loss: 0.793864  [   32/   43]
train() client id: f_00003-2-0 loss: 0.961696  [   32/   43]
train() client id: f_00003-3-0 loss: 0.756682  [   32/   43]
train() client id: f_00003-4-0 loss: 0.841471  [   32/   43]
train() client id: f_00003-5-0 loss: 0.799320  [   32/   43]
train() client id: f_00003-6-0 loss: 0.906203  [   32/   43]
train() client id: f_00003-7-0 loss: 1.003044  [   32/   43]
train() client id: f_00003-8-0 loss: 0.808337  [   32/   43]
train() client id: f_00003-9-0 loss: 0.957521  [   32/   43]
train() client id: f_00003-10-0 loss: 0.937358  [   32/   43]
train() client id: f_00004-0-0 loss: 0.927610  [   32/  306]
train() client id: f_00004-0-1 loss: 0.840822  [   64/  306]
train() client id: f_00004-0-2 loss: 0.934906  [   96/  306]
train() client id: f_00004-0-3 loss: 0.809013  [  128/  306]
train() client id: f_00004-0-4 loss: 0.860023  [  160/  306]
train() client id: f_00004-0-5 loss: 0.880224  [  192/  306]
train() client id: f_00004-0-6 loss: 0.926869  [  224/  306]
train() client id: f_00004-0-7 loss: 0.799136  [  256/  306]
train() client id: f_00004-0-8 loss: 0.768553  [  288/  306]
train() client id: f_00004-1-0 loss: 0.904799  [   32/  306]
train() client id: f_00004-1-1 loss: 0.891798  [   64/  306]
train() client id: f_00004-1-2 loss: 0.732368  [   96/  306]
train() client id: f_00004-1-3 loss: 0.788589  [  128/  306]
train() client id: f_00004-1-4 loss: 0.943751  [  160/  306]
train() client id: f_00004-1-5 loss: 0.747621  [  192/  306]
train() client id: f_00004-1-6 loss: 0.888973  [  224/  306]
train() client id: f_00004-1-7 loss: 0.697927  [  256/  306]
train() client id: f_00004-1-8 loss: 1.052390  [  288/  306]
train() client id: f_00004-2-0 loss: 0.865712  [   32/  306]
train() client id: f_00004-2-1 loss: 0.809405  [   64/  306]
train() client id: f_00004-2-2 loss: 0.778020  [   96/  306]
train() client id: f_00004-2-3 loss: 0.888357  [  128/  306]
train() client id: f_00004-2-4 loss: 0.791296  [  160/  306]
train() client id: f_00004-2-5 loss: 0.832788  [  192/  306]
train() client id: f_00004-2-6 loss: 0.903384  [  224/  306]
train() client id: f_00004-2-7 loss: 0.834328  [  256/  306]
train() client id: f_00004-2-8 loss: 0.895947  [  288/  306]
train() client id: f_00004-3-0 loss: 0.849847  [   32/  306]
train() client id: f_00004-3-1 loss: 0.839153  [   64/  306]
train() client id: f_00004-3-2 loss: 0.898924  [   96/  306]
train() client id: f_00004-3-3 loss: 0.854213  [  128/  306]
train() client id: f_00004-3-4 loss: 0.752462  [  160/  306]
train() client id: f_00004-3-5 loss: 0.862810  [  192/  306]
train() client id: f_00004-3-6 loss: 0.896656  [  224/  306]
train() client id: f_00004-3-7 loss: 0.885809  [  256/  306]
train() client id: f_00004-3-8 loss: 0.886001  [  288/  306]
train() client id: f_00004-4-0 loss: 1.037880  [   32/  306]
train() client id: f_00004-4-1 loss: 0.903095  [   64/  306]
train() client id: f_00004-4-2 loss: 0.926188  [   96/  306]
train() client id: f_00004-4-3 loss: 0.816793  [  128/  306]
train() client id: f_00004-4-4 loss: 0.943389  [  160/  306]
train() client id: f_00004-4-5 loss: 0.801070  [  192/  306]
train() client id: f_00004-4-6 loss: 0.727528  [  224/  306]
train() client id: f_00004-4-7 loss: 0.696784  [  256/  306]
train() client id: f_00004-4-8 loss: 0.891584  [  288/  306]
train() client id: f_00004-5-0 loss: 0.845542  [   32/  306]
train() client id: f_00004-5-1 loss: 0.975074  [   64/  306]
train() client id: f_00004-5-2 loss: 0.757382  [   96/  306]
train() client id: f_00004-5-3 loss: 0.916778  [  128/  306]
train() client id: f_00004-5-4 loss: 0.669619  [  160/  306]
train() client id: f_00004-5-5 loss: 0.949624  [  192/  306]
train() client id: f_00004-5-6 loss: 0.909152  [  224/  306]
train() client id: f_00004-5-7 loss: 0.852088  [  256/  306]
train() client id: f_00004-5-8 loss: 0.915673  [  288/  306]
train() client id: f_00004-6-0 loss: 0.831627  [   32/  306]
train() client id: f_00004-6-1 loss: 0.772053  [   64/  306]
train() client id: f_00004-6-2 loss: 0.992344  [   96/  306]
train() client id: f_00004-6-3 loss: 0.764679  [  128/  306]
train() client id: f_00004-6-4 loss: 0.770713  [  160/  306]
train() client id: f_00004-6-5 loss: 0.804033  [  192/  306]
train() client id: f_00004-6-6 loss: 0.880321  [  224/  306]
train() client id: f_00004-6-7 loss: 0.927093  [  256/  306]
train() client id: f_00004-6-8 loss: 0.928661  [  288/  306]
train() client id: f_00004-7-0 loss: 0.769370  [   32/  306]
train() client id: f_00004-7-1 loss: 0.926986  [   64/  306]
train() client id: f_00004-7-2 loss: 0.866413  [   96/  306]
train() client id: f_00004-7-3 loss: 0.753824  [  128/  306]
train() client id: f_00004-7-4 loss: 0.914899  [  160/  306]
train() client id: f_00004-7-5 loss: 0.864361  [  192/  306]
train() client id: f_00004-7-6 loss: 0.831961  [  224/  306]
train() client id: f_00004-7-7 loss: 0.833376  [  256/  306]
train() client id: f_00004-7-8 loss: 0.864782  [  288/  306]
train() client id: f_00004-8-0 loss: 0.882230  [   32/  306]
train() client id: f_00004-8-1 loss: 0.861603  [   64/  306]
train() client id: f_00004-8-2 loss: 1.082215  [   96/  306]
train() client id: f_00004-8-3 loss: 0.809287  [  128/  306]
train() client id: f_00004-8-4 loss: 0.813964  [  160/  306]
train() client id: f_00004-8-5 loss: 0.761580  [  192/  306]
train() client id: f_00004-8-6 loss: 0.832273  [  224/  306]
train() client id: f_00004-8-7 loss: 0.926567  [  256/  306]
train() client id: f_00004-8-8 loss: 0.790391  [  288/  306]
train() client id: f_00004-9-0 loss: 0.916074  [   32/  306]
train() client id: f_00004-9-1 loss: 0.811264  [   64/  306]
train() client id: f_00004-9-2 loss: 0.898064  [   96/  306]
train() client id: f_00004-9-3 loss: 0.744891  [  128/  306]
train() client id: f_00004-9-4 loss: 0.849371  [  160/  306]
train() client id: f_00004-9-5 loss: 0.772588  [  192/  306]
train() client id: f_00004-9-6 loss: 0.868739  [  224/  306]
train() client id: f_00004-9-7 loss: 0.894947  [  256/  306]
train() client id: f_00004-9-8 loss: 0.960578  [  288/  306]
train() client id: f_00004-10-0 loss: 0.837000  [   32/  306]
train() client id: f_00004-10-1 loss: 0.744102  [   64/  306]
train() client id: f_00004-10-2 loss: 0.956745  [   96/  306]
train() client id: f_00004-10-3 loss: 0.772230  [  128/  306]
train() client id: f_00004-10-4 loss: 0.871985  [  160/  306]
train() client id: f_00004-10-5 loss: 0.869437  [  192/  306]
train() client id: f_00004-10-6 loss: 0.875489  [  224/  306]
train() client id: f_00004-10-7 loss: 0.950690  [  256/  306]
train() client id: f_00004-10-8 loss: 0.824673  [  288/  306]
train() client id: f_00005-0-0 loss: 0.627675  [   32/  146]
train() client id: f_00005-0-1 loss: 0.820836  [   64/  146]
train() client id: f_00005-0-2 loss: 0.671679  [   96/  146]
train() client id: f_00005-0-3 loss: 0.551197  [  128/  146]
train() client id: f_00005-1-0 loss: 0.670070  [   32/  146]
train() client id: f_00005-1-1 loss: 0.494242  [   64/  146]
train() client id: f_00005-1-2 loss: 0.782582  [   96/  146]
train() client id: f_00005-1-3 loss: 0.705391  [  128/  146]
train() client id: f_00005-2-0 loss: 0.639789  [   32/  146]
train() client id: f_00005-2-1 loss: 0.728355  [   64/  146]
train() client id: f_00005-2-2 loss: 0.721073  [   96/  146]
train() client id: f_00005-2-3 loss: 0.392654  [  128/  146]
train() client id: f_00005-3-0 loss: 0.644851  [   32/  146]
train() client id: f_00005-3-1 loss: 0.722539  [   64/  146]
train() client id: f_00005-3-2 loss: 0.817286  [   96/  146]
train() client id: f_00005-3-3 loss: 0.495558  [  128/  146]
train() client id: f_00005-4-0 loss: 0.730884  [   32/  146]
train() client id: f_00005-4-1 loss: 0.565311  [   64/  146]
train() client id: f_00005-4-2 loss: 0.387912  [   96/  146]
train() client id: f_00005-4-3 loss: 0.900243  [  128/  146]
train() client id: f_00005-5-0 loss: 0.562887  [   32/  146]
train() client id: f_00005-5-1 loss: 0.736053  [   64/  146]
train() client id: f_00005-5-2 loss: 0.740537  [   96/  146]
train() client id: f_00005-5-3 loss: 0.574805  [  128/  146]
train() client id: f_00005-6-0 loss: 0.529153  [   32/  146]
train() client id: f_00005-6-1 loss: 0.739040  [   64/  146]
train() client id: f_00005-6-2 loss: 0.412017  [   96/  146]
train() client id: f_00005-6-3 loss: 0.825292  [  128/  146]
train() client id: f_00005-7-0 loss: 0.745442  [   32/  146]
train() client id: f_00005-7-1 loss: 0.688725  [   64/  146]
train() client id: f_00005-7-2 loss: 0.662997  [   96/  146]
train() client id: f_00005-7-3 loss: 0.444005  [  128/  146]
train() client id: f_00005-8-0 loss: 0.690781  [   32/  146]
train() client id: f_00005-8-1 loss: 0.646701  [   64/  146]
train() client id: f_00005-8-2 loss: 0.460963  [   96/  146]
train() client id: f_00005-8-3 loss: 0.605303  [  128/  146]
train() client id: f_00005-9-0 loss: 0.629065  [   32/  146]
train() client id: f_00005-9-1 loss: 0.605188  [   64/  146]
train() client id: f_00005-9-2 loss: 0.811922  [   96/  146]
train() client id: f_00005-9-3 loss: 0.601218  [  128/  146]
train() client id: f_00005-10-0 loss: 0.631322  [   32/  146]
train() client id: f_00005-10-1 loss: 0.709637  [   64/  146]
train() client id: f_00005-10-2 loss: 0.565148  [   96/  146]
train() client id: f_00005-10-3 loss: 0.686973  [  128/  146]
train() client id: f_00006-0-0 loss: 0.487619  [   32/   54]
train() client id: f_00006-1-0 loss: 0.465791  [   32/   54]
train() client id: f_00006-2-0 loss: 0.529477  [   32/   54]
train() client id: f_00006-3-0 loss: 0.457901  [   32/   54]
train() client id: f_00006-4-0 loss: 0.518019  [   32/   54]
train() client id: f_00006-5-0 loss: 0.471179  [   32/   54]
train() client id: f_00006-6-0 loss: 0.440575  [   32/   54]
train() client id: f_00006-7-0 loss: 0.454092  [   32/   54]
train() client id: f_00006-8-0 loss: 0.433472  [   32/   54]
train() client id: f_00006-9-0 loss: 0.482237  [   32/   54]
train() client id: f_00006-10-0 loss: 0.506340  [   32/   54]
train() client id: f_00007-0-0 loss: 0.813190  [   32/  179]
train() client id: f_00007-0-1 loss: 0.808657  [   64/  179]
train() client id: f_00007-0-2 loss: 0.686922  [   96/  179]
train() client id: f_00007-0-3 loss: 0.771816  [  128/  179]
train() client id: f_00007-0-4 loss: 0.673056  [  160/  179]
train() client id: f_00007-1-0 loss: 0.833573  [   32/  179]
train() client id: f_00007-1-1 loss: 0.589345  [   64/  179]
train() client id: f_00007-1-2 loss: 0.637872  [   96/  179]
train() client id: f_00007-1-3 loss: 0.768143  [  128/  179]
train() client id: f_00007-1-4 loss: 0.599491  [  160/  179]
train() client id: f_00007-2-0 loss: 0.580795  [   32/  179]
train() client id: f_00007-2-1 loss: 0.958639  [   64/  179]
train() client id: f_00007-2-2 loss: 0.737551  [   96/  179]
train() client id: f_00007-2-3 loss: 0.736959  [  128/  179]
train() client id: f_00007-2-4 loss: 0.639192  [  160/  179]
train() client id: f_00007-3-0 loss: 0.628359  [   32/  179]
train() client id: f_00007-3-1 loss: 0.742486  [   64/  179]
train() client id: f_00007-3-2 loss: 0.806285  [   96/  179]
train() client id: f_00007-3-3 loss: 0.643129  [  128/  179]
train() client id: f_00007-3-4 loss: 0.802421  [  160/  179]
train() client id: f_00007-4-0 loss: 1.052573  [   32/  179]
train() client id: f_00007-4-1 loss: 0.601822  [   64/  179]
train() client id: f_00007-4-2 loss: 0.683867  [   96/  179]
train() client id: f_00007-4-3 loss: 0.565610  [  128/  179]
train() client id: f_00007-4-4 loss: 0.632229  [  160/  179]
train() client id: f_00007-5-0 loss: 0.554209  [   32/  179]
train() client id: f_00007-5-1 loss: 0.778066  [   64/  179]
train() client id: f_00007-5-2 loss: 0.770581  [   96/  179]
train() client id: f_00007-5-3 loss: 0.758203  [  128/  179]
train() client id: f_00007-5-4 loss: 0.738236  [  160/  179]
train() client id: f_00007-6-0 loss: 0.599501  [   32/  179]
train() client id: f_00007-6-1 loss: 0.809228  [   64/  179]
train() client id: f_00007-6-2 loss: 0.745222  [   96/  179]
train() client id: f_00007-6-3 loss: 0.766628  [  128/  179]
train() client id: f_00007-6-4 loss: 0.718843  [  160/  179]
train() client id: f_00007-7-0 loss: 0.753204  [   32/  179]
train() client id: f_00007-7-1 loss: 0.626133  [   64/  179]
train() client id: f_00007-7-2 loss: 0.812108  [   96/  179]
train() client id: f_00007-7-3 loss: 0.689373  [  128/  179]
train() client id: f_00007-7-4 loss: 0.643938  [  160/  179]
train() client id: f_00007-8-0 loss: 0.614522  [   32/  179]
train() client id: f_00007-8-1 loss: 0.597792  [   64/  179]
train() client id: f_00007-8-2 loss: 0.733884  [   96/  179]
train() client id: f_00007-8-3 loss: 0.727664  [  128/  179]
train() client id: f_00007-8-4 loss: 0.739578  [  160/  179]
train() client id: f_00007-9-0 loss: 0.710878  [   32/  179]
train() client id: f_00007-9-1 loss: 1.072074  [   64/  179]
train() client id: f_00007-9-2 loss: 0.570979  [   96/  179]
train() client id: f_00007-9-3 loss: 0.528401  [  128/  179]
train() client id: f_00007-9-4 loss: 0.721549  [  160/  179]
train() client id: f_00007-10-0 loss: 0.554048  [   32/  179]
train() client id: f_00007-10-1 loss: 0.833265  [   64/  179]
train() client id: f_00007-10-2 loss: 0.880728  [   96/  179]
train() client id: f_00007-10-3 loss: 0.663126  [  128/  179]
train() client id: f_00007-10-4 loss: 0.667398  [  160/  179]
train() client id: f_00008-0-0 loss: 0.700719  [   32/  130]
train() client id: f_00008-0-1 loss: 0.667761  [   64/  130]
train() client id: f_00008-0-2 loss: 0.592534  [   96/  130]
train() client id: f_00008-0-3 loss: 0.613624  [  128/  130]
train() client id: f_00008-1-0 loss: 0.676800  [   32/  130]
train() client id: f_00008-1-1 loss: 0.559714  [   64/  130]
train() client id: f_00008-1-2 loss: 0.697534  [   96/  130]
train() client id: f_00008-1-3 loss: 0.648643  [  128/  130]
train() client id: f_00008-2-0 loss: 0.674689  [   32/  130]
train() client id: f_00008-2-1 loss: 0.627748  [   64/  130]
train() client id: f_00008-2-2 loss: 0.650769  [   96/  130]
train() client id: f_00008-2-3 loss: 0.633611  [  128/  130]
train() client id: f_00008-3-0 loss: 0.605417  [   32/  130]
train() client id: f_00008-3-1 loss: 0.730630  [   64/  130]
train() client id: f_00008-3-2 loss: 0.625880  [   96/  130]
train() client id: f_00008-3-3 loss: 0.586970  [  128/  130]
train() client id: f_00008-4-0 loss: 0.679025  [   32/  130]
train() client id: f_00008-4-1 loss: 0.771443  [   64/  130]
train() client id: f_00008-4-2 loss: 0.561282  [   96/  130]
train() client id: f_00008-4-3 loss: 0.571852  [  128/  130]
train() client id: f_00008-5-0 loss: 0.778513  [   32/  130]
train() client id: f_00008-5-1 loss: 0.671128  [   64/  130]
train() client id: f_00008-5-2 loss: 0.512163  [   96/  130]
train() client id: f_00008-5-3 loss: 0.612262  [  128/  130]
train() client id: f_00008-6-0 loss: 0.560801  [   32/  130]
train() client id: f_00008-6-1 loss: 0.665203  [   64/  130]
train() client id: f_00008-6-2 loss: 0.731345  [   96/  130]
train() client id: f_00008-6-3 loss: 0.622544  [  128/  130]
train() client id: f_00008-7-0 loss: 0.691247  [   32/  130]
train() client id: f_00008-7-1 loss: 0.627694  [   64/  130]
train() client id: f_00008-7-2 loss: 0.558767  [   96/  130]
train() client id: f_00008-7-3 loss: 0.699067  [  128/  130]
train() client id: f_00008-8-0 loss: 0.644173  [   32/  130]
train() client id: f_00008-8-1 loss: 0.545129  [   64/  130]
train() client id: f_00008-8-2 loss: 0.730031  [   96/  130]
train() client id: f_00008-8-3 loss: 0.628748  [  128/  130]
train() client id: f_00008-9-0 loss: 0.573368  [   32/  130]
train() client id: f_00008-9-1 loss: 0.638277  [   64/  130]
train() client id: f_00008-9-2 loss: 0.663208  [   96/  130]
train() client id: f_00008-9-3 loss: 0.709988  [  128/  130]
train() client id: f_00008-10-0 loss: 0.656340  [   32/  130]
train() client id: f_00008-10-1 loss: 0.657696  [   64/  130]
train() client id: f_00008-10-2 loss: 0.748700  [   96/  130]
train() client id: f_00008-10-3 loss: 0.530015  [  128/  130]
train() client id: f_00009-0-0 loss: 1.126640  [   32/  118]
train() client id: f_00009-0-1 loss: 1.019813  [   64/  118]
train() client id: f_00009-0-2 loss: 0.881932  [   96/  118]
train() client id: f_00009-1-0 loss: 1.095119  [   32/  118]
train() client id: f_00009-1-1 loss: 1.006817  [   64/  118]
train() client id: f_00009-1-2 loss: 0.822119  [   96/  118]
train() client id: f_00009-2-0 loss: 0.963001  [   32/  118]
train() client id: f_00009-2-1 loss: 0.942628  [   64/  118]
train() client id: f_00009-2-2 loss: 0.766661  [   96/  118]
train() client id: f_00009-3-0 loss: 0.719645  [   32/  118]
train() client id: f_00009-3-1 loss: 0.918351  [   64/  118]
train() client id: f_00009-3-2 loss: 1.059183  [   96/  118]
train() client id: f_00009-4-0 loss: 0.979593  [   32/  118]
train() client id: f_00009-4-1 loss: 0.941144  [   64/  118]
train() client id: f_00009-4-2 loss: 0.659901  [   96/  118]
train() client id: f_00009-5-0 loss: 0.840274  [   32/  118]
train() client id: f_00009-5-1 loss: 0.752014  [   64/  118]
train() client id: f_00009-5-2 loss: 0.966175  [   96/  118]
train() client id: f_00009-6-0 loss: 1.033918  [   32/  118]
train() client id: f_00009-6-1 loss: 0.730584  [   64/  118]
train() client id: f_00009-6-2 loss: 0.700605  [   96/  118]
train() client id: f_00009-7-0 loss: 0.716886  [   32/  118]
train() client id: f_00009-7-1 loss: 0.920516  [   64/  118]
train() client id: f_00009-7-2 loss: 0.727930  [   96/  118]
train() client id: f_00009-8-0 loss: 0.783058  [   32/  118]
train() client id: f_00009-8-1 loss: 0.912833  [   64/  118]
train() client id: f_00009-8-2 loss: 0.739222  [   96/  118]
train() client id: f_00009-9-0 loss: 0.896479  [   32/  118]
train() client id: f_00009-9-1 loss: 0.833168  [   64/  118]
train() client id: f_00009-9-2 loss: 0.765363  [   96/  118]
train() client id: f_00009-10-0 loss: 0.944395  [   32/  118]
train() client id: f_00009-10-1 loss: 0.658822  [   64/  118]
train() client id: f_00009-10-2 loss: 0.791147  [   96/  118]
At round 49 accuracy: 0.6445623342175066
At round 49 training accuracy: 0.5888665325285044
At round 49 training loss: 0.8331934497365842
update_location
xs = [  -3.9056584     4.20031788  265.00902392   18.81129433    0.97929623
    3.95640986 -227.44319194 -206.32485185  249.66397685 -192.06087855]
ys = [ 257.5879595   240.55583871    1.32061395 -227.45517586  219.35018685
  202.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [276.34545599 260.54702878 283.25170216 249.17809258 241.07149041
 226.1619617  248.46990985 229.28284006 269.51951935 216.57191167]
dists_bs = [189.98432772 190.81488913 473.04594942 446.69115324 181.48182216
 181.10500714 185.2050848  176.97203396 452.90618355 171.84751034]
uav_gains = [4.31692826e-12 6.00226608e-12 3.73822966e-12 7.54002475e-12
 8.79545935e-12 1.14187632e-11 7.64459119e-12 1.08372987e-11
 4.98118754e-12 1.33149743e-11]
bs_gains = [4.60133985e-11 4.54548008e-11 3.57737746e-12 4.20023415e-12
 5.23071814e-11 5.26124836e-11 4.94158134e-11 5.61256116e-11
 4.04083377e-12 6.09386741e-11]
Round 50
-------------------------------
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.40113982  9.00691085  4.34261068  1.57832816 10.38540444  4.99669129
  1.94822841  6.14510341  4.53980435  4.05137628]
obj_prev = 51.395597693307295
eta_min = 1.1367158248091635e-21	eta_max = 0.9395757044516898
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 11.874554720679596	eta = 0.909090909090909
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 24.172264619780965	eta = 0.44658826617504477
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 17.836063247717608	eta = 0.6052372430028105
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.691468667850707	eta = 0.6467405571604739
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62571758399257	eta = 0.6492982748886553
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62547813342434	eta = 0.6493076264898318
eta = 0.6493076264898318
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.03563512 0.07494687 0.03506948 0.01216119 0.08654243 0.04129149
 0.01527219 0.05062449 0.03676639 0.03337257]
ene_total = [1.56827942 2.59415826 1.57537116 0.75074681 2.95309342 1.52632602
 0.84721729 1.92501916 1.61509008 1.2701765 ]
ti_comp = [0.73120545 0.79889761 0.72329946 0.75580078 0.8009931  0.80107742
 0.7563088  0.76793556 0.72766908 0.80314305]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [5.28975756e-06 4.12247795e-05 5.15266296e-06 1.96785757e-07
 6.31407441e-05 6.85665898e-06 3.89211744e-07 1.37503103e-05
 5.86631230e-06 3.60133211e-06]
ene_total = [0.44507619 0.2328146  0.46999487 0.36738071 0.22689962 0.22485949
 0.36578528 0.32955399 0.45624248 0.21824512]
optimize_network iter = 0 obj = 3.3368523467960083
eta = 0.6493076264898318
freqs = [24367376.98591928 46906432.60757383 24242711.66907599  8045235.73109414
 54021956.04098655 25772468.52031963 10096532.76307652 32961419.31716986
 25263125.77228831 20776230.43734453]
eta_min = 0.6493076264898333	eta_max = 0.6979381775060964
af = 0.0037239897789525765	bf = 1.1686004695034842	zeta = 0.004096388756847835	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16746392e-06 9.09842125e-06 1.13720677e-06 4.34311531e-08
 1.39353344e-05 1.51328333e-06 8.59000931e-08 3.03473097e-06
 1.29471112e-06 7.94823816e-07]
ene_total = [1.71643177 0.89420252 1.81257148 1.41719511 0.86930783 0.86677177
 1.41102229 1.26998969 1.7594524  0.84156459]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 1 obj = 3.8730371713241745
eta = 0.6979381775060964
freqs = [24314579.32952386 46031634.30360291 24242711.66907599  7976349.99245549
 52989718.17503818 25279541.36753655 10008802.12815238 32581091.06223162
 25232695.58126215 20369543.75424322]
eta_min = 0.6979381775061025	eta_max = 0.697938177506091
af = 0.0036043043465234475	bf = 1.1686004695034842	zeta = 0.003964734781175793	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16241023e-06 8.76221731e-06 1.13720677e-06 4.26905959e-08
 1.34078766e-05 1.45595045e-06 8.44137753e-08 2.96510198e-06
 1.29159396e-06 7.64011626e-07]
ene_total = [1.71643115 0.89416163 1.81257148 1.41719502 0.86924368 0.8667648
 1.41102211 1.26998122 1.75945202 0.84156084]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 2 obj = 3.873037171324106
eta = 0.697938177506091
freqs = [24314579.32952385 46031634.30360299 24242711.66907597  7976349.99245549
 52989718.17503828 25279541.3675366  10008802.12815239 32581091.06223165
 25232695.58126213 20369543.75424326]
Done!
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.09700746e-06 3.08831330e-05 4.00817585e-06 1.50466407e-07
 4.72571292e-05 5.13161335e-06 2.97523077e-07 1.04507382e-05
 4.55232582e-06 2.69281985e-06]
ene_total = [0.01411735 0.00737491 0.01490785 0.01165387 0.00718174 0.00713118
 0.01160321 0.01045069 0.01447144 0.00692218]
At round 50 energy consumption: 0.10581441892494492
At round 50 eta: 0.697938177506091
At round 50 a_n: 11.055310507788512
At round 50 local rounds: 11.775941462244061
At round 50 global rounds: 36.59949614457299
gradient difference: 0.5073786377906799
train() client id: f_00000-0-0 loss: 1.212975  [   32/  126]
train() client id: f_00000-0-1 loss: 1.166238  [   64/  126]
train() client id: f_00000-0-2 loss: 1.068787  [   96/  126]
train() client id: f_00000-1-0 loss: 1.057249  [   32/  126]
train() client id: f_00000-1-1 loss: 1.174354  [   64/  126]
train() client id: f_00000-1-2 loss: 0.951590  [   96/  126]
train() client id: f_00000-2-0 loss: 1.039137  [   32/  126]
train() client id: f_00000-2-1 loss: 1.101068  [   64/  126]
train() client id: f_00000-2-2 loss: 0.794911  [   96/  126]
train() client id: f_00000-3-0 loss: 0.971230  [   32/  126]
train() client id: f_00000-3-1 loss: 1.042341  [   64/  126]
train() client id: f_00000-3-2 loss: 0.800030  [   96/  126]
train() client id: f_00000-4-0 loss: 0.789166  [   32/  126]
train() client id: f_00000-4-1 loss: 0.815357  [   64/  126]
train() client id: f_00000-4-2 loss: 0.991789  [   96/  126]
train() client id: f_00000-5-0 loss: 0.816465  [   32/  126]
train() client id: f_00000-5-1 loss: 0.775261  [   64/  126]
train() client id: f_00000-5-2 loss: 0.869390  [   96/  126]
train() client id: f_00000-6-0 loss: 0.799924  [   32/  126]
train() client id: f_00000-6-1 loss: 0.872495  [   64/  126]
train() client id: f_00000-6-2 loss: 0.768357  [   96/  126]
train() client id: f_00000-7-0 loss: 0.790850  [   32/  126]
train() client id: f_00000-7-1 loss: 0.827659  [   64/  126]
train() client id: f_00000-7-2 loss: 0.777411  [   96/  126]
train() client id: f_00000-8-0 loss: 0.759092  [   32/  126]
train() client id: f_00000-8-1 loss: 0.676272  [   64/  126]
train() client id: f_00000-8-2 loss: 0.781671  [   96/  126]
train() client id: f_00000-9-0 loss: 0.810297  [   32/  126]
train() client id: f_00000-9-1 loss: 0.692330  [   64/  126]
train() client id: f_00000-9-2 loss: 0.605862  [   96/  126]
train() client id: f_00000-10-0 loss: 0.738801  [   32/  126]
train() client id: f_00000-10-1 loss: 0.730837  [   64/  126]
train() client id: f_00000-10-2 loss: 0.719509  [   96/  126]
train() client id: f_00001-0-0 loss: 0.528625  [   32/  265]
train() client id: f_00001-0-1 loss: 0.451772  [   64/  265]
train() client id: f_00001-0-2 loss: 0.465959  [   96/  265]
train() client id: f_00001-0-3 loss: 0.427194  [  128/  265]
train() client id: f_00001-0-4 loss: 0.453110  [  160/  265]
train() client id: f_00001-0-5 loss: 0.431485  [  192/  265]
train() client id: f_00001-0-6 loss: 0.401427  [  224/  265]
train() client id: f_00001-0-7 loss: 0.499081  [  256/  265]
train() client id: f_00001-1-0 loss: 0.411866  [   32/  265]
train() client id: f_00001-1-1 loss: 0.483759  [   64/  265]
train() client id: f_00001-1-2 loss: 0.473092  [   96/  265]
train() client id: f_00001-1-3 loss: 0.576685  [  128/  265]
train() client id: f_00001-1-4 loss: 0.355826  [  160/  265]
train() client id: f_00001-1-5 loss: 0.371550  [  192/  265]
train() client id: f_00001-1-6 loss: 0.432099  [  224/  265]
train() client id: f_00001-1-7 loss: 0.419895  [  256/  265]
train() client id: f_00001-2-0 loss: 0.356730  [   32/  265]
train() client id: f_00001-2-1 loss: 0.392529  [   64/  265]
train() client id: f_00001-2-2 loss: 0.354736  [   96/  265]
train() client id: f_00001-2-3 loss: 0.368488  [  128/  265]
train() client id: f_00001-2-4 loss: 0.383879  [  160/  265]
train() client id: f_00001-2-5 loss: 0.543637  [  192/  265]
train() client id: f_00001-2-6 loss: 0.420853  [  224/  265]
train() client id: f_00001-2-7 loss: 0.553257  [  256/  265]
train() client id: f_00001-3-0 loss: 0.530314  [   32/  265]
train() client id: f_00001-3-1 loss: 0.341622  [   64/  265]
train() client id: f_00001-3-2 loss: 0.364405  [   96/  265]
train() client id: f_00001-3-3 loss: 0.377246  [  128/  265]
train() client id: f_00001-3-4 loss: 0.421981  [  160/  265]
train() client id: f_00001-3-5 loss: 0.390984  [  192/  265]
train() client id: f_00001-3-6 loss: 0.557725  [  224/  265]
train() client id: f_00001-3-7 loss: 0.401941  [  256/  265]
train() client id: f_00001-4-0 loss: 0.361515  [   32/  265]
train() client id: f_00001-4-1 loss: 0.457595  [   64/  265]
train() client id: f_00001-4-2 loss: 0.391212  [   96/  265]
train() client id: f_00001-4-3 loss: 0.418800  [  128/  265]
train() client id: f_00001-4-4 loss: 0.503270  [  160/  265]
train() client id: f_00001-4-5 loss: 0.391997  [  192/  265]
train() client id: f_00001-4-6 loss: 0.456786  [  224/  265]
train() client id: f_00001-4-7 loss: 0.451101  [  256/  265]
train() client id: f_00001-5-0 loss: 0.397559  [   32/  265]
train() client id: f_00001-5-1 loss: 0.385682  [   64/  265]
train() client id: f_00001-5-2 loss: 0.409680  [   96/  265]
train() client id: f_00001-5-3 loss: 0.394457  [  128/  265]
train() client id: f_00001-5-4 loss: 0.493288  [  160/  265]
train() client id: f_00001-5-5 loss: 0.483490  [  192/  265]
train() client id: f_00001-5-6 loss: 0.343463  [  224/  265]
train() client id: f_00001-5-7 loss: 0.382495  [  256/  265]
train() client id: f_00001-6-0 loss: 0.359033  [   32/  265]
train() client id: f_00001-6-1 loss: 0.475286  [   64/  265]
train() client id: f_00001-6-2 loss: 0.383840  [   96/  265]
train() client id: f_00001-6-3 loss: 0.375468  [  128/  265]
train() client id: f_00001-6-4 loss: 0.326422  [  160/  265]
train() client id: f_00001-6-5 loss: 0.552682  [  192/  265]
train() client id: f_00001-6-6 loss: 0.449096  [  224/  265]
train() client id: f_00001-6-7 loss: 0.396294  [  256/  265]
train() client id: f_00001-7-0 loss: 0.321082  [   32/  265]
train() client id: f_00001-7-1 loss: 0.377035  [   64/  265]
train() client id: f_00001-7-2 loss: 0.322430  [   96/  265]
train() client id: f_00001-7-3 loss: 0.408518  [  128/  265]
train() client id: f_00001-7-4 loss: 0.501743  [  160/  265]
train() client id: f_00001-7-5 loss: 0.334379  [  192/  265]
train() client id: f_00001-7-6 loss: 0.585558  [  224/  265]
train() client id: f_00001-7-7 loss: 0.507414  [  256/  265]
train() client id: f_00001-8-0 loss: 0.493517  [   32/  265]
train() client id: f_00001-8-1 loss: 0.548565  [   64/  265]
train() client id: f_00001-8-2 loss: 0.448324  [   96/  265]
train() client id: f_00001-8-3 loss: 0.328650  [  128/  265]
train() client id: f_00001-8-4 loss: 0.383470  [  160/  265]
train() client id: f_00001-8-5 loss: 0.335528  [  192/  265]
train() client id: f_00001-8-6 loss: 0.412867  [  224/  265]
train() client id: f_00001-8-7 loss: 0.374402  [  256/  265]
train() client id: f_00001-9-0 loss: 0.563213  [   32/  265]
train() client id: f_00001-9-1 loss: 0.377554  [   64/  265]
train() client id: f_00001-9-2 loss: 0.321144  [   96/  265]
train() client id: f_00001-9-3 loss: 0.331387  [  128/  265]
train() client id: f_00001-9-4 loss: 0.428330  [  160/  265]
train() client id: f_00001-9-5 loss: 0.384597  [  192/  265]
train() client id: f_00001-9-6 loss: 0.539046  [  224/  265]
train() client id: f_00001-9-7 loss: 0.366867  [  256/  265]
train() client id: f_00001-10-0 loss: 0.519434  [   32/  265]
train() client id: f_00001-10-1 loss: 0.459027  [   64/  265]
train() client id: f_00001-10-2 loss: 0.521523  [   96/  265]
train() client id: f_00001-10-3 loss: 0.381274  [  128/  265]
train() client id: f_00001-10-4 loss: 0.349645  [  160/  265]
train() client id: f_00001-10-5 loss: 0.330073  [  192/  265]
train() client id: f_00001-10-6 loss: 0.314229  [  224/  265]
train() client id: f_00001-10-7 loss: 0.356094  [  256/  265]
train() client id: f_00002-0-0 loss: 1.149016  [   32/  124]
train() client id: f_00002-0-1 loss: 1.050179  [   64/  124]
train() client id: f_00002-0-2 loss: 1.389334  [   96/  124]
train() client id: f_00002-1-0 loss: 1.032426  [   32/  124]
train() client id: f_00002-1-1 loss: 1.322374  [   64/  124]
train() client id: f_00002-1-2 loss: 1.106434  [   96/  124]
train() client id: f_00002-2-0 loss: 1.254078  [   32/  124]
train() client id: f_00002-2-1 loss: 1.083723  [   64/  124]
train() client id: f_00002-2-2 loss: 1.088509  [   96/  124]
train() client id: f_00002-3-0 loss: 1.116286  [   32/  124]
train() client id: f_00002-3-1 loss: 1.027001  [   64/  124]
train() client id: f_00002-3-2 loss: 1.182932  [   96/  124]
train() client id: f_00002-4-0 loss: 1.111370  [   32/  124]
train() client id: f_00002-4-1 loss: 1.176103  [   64/  124]
train() client id: f_00002-4-2 loss: 0.961931  [   96/  124]
train() client id: f_00002-5-0 loss: 0.889287  [   32/  124]
train() client id: f_00002-5-1 loss: 1.213103  [   64/  124]
train() client id: f_00002-5-2 loss: 1.074073  [   96/  124]
train() client id: f_00002-6-0 loss: 1.229798  [   32/  124]
train() client id: f_00002-6-1 loss: 0.905873  [   64/  124]
train() client id: f_00002-6-2 loss: 0.952713  [   96/  124]
train() client id: f_00002-7-0 loss: 1.189054  [   32/  124]
train() client id: f_00002-7-1 loss: 1.018694  [   64/  124]
train() client id: f_00002-7-2 loss: 0.842361  [   96/  124]
train() client id: f_00002-8-0 loss: 1.104113  [   32/  124]
train() client id: f_00002-8-1 loss: 1.001156  [   64/  124]
train() client id: f_00002-8-2 loss: 0.983141  [   96/  124]
train() client id: f_00002-9-0 loss: 0.955760  [   32/  124]
train() client id: f_00002-9-1 loss: 0.934063  [   64/  124]
train() client id: f_00002-9-2 loss: 1.022897  [   96/  124]
train() client id: f_00002-10-0 loss: 0.918599  [   32/  124]
train() client id: f_00002-10-1 loss: 0.820034  [   64/  124]
train() client id: f_00002-10-2 loss: 1.254749  [   96/  124]
train() client id: f_00003-0-0 loss: 0.632583  [   32/   43]
train() client id: f_00003-1-0 loss: 0.468003  [   32/   43]
train() client id: f_00003-2-0 loss: 0.506921  [   32/   43]
train() client id: f_00003-3-0 loss: 0.492075  [   32/   43]
train() client id: f_00003-4-0 loss: 0.498296  [   32/   43]
train() client id: f_00003-5-0 loss: 0.450623  [   32/   43]
train() client id: f_00003-6-0 loss: 0.232295  [   32/   43]
train() client id: f_00003-7-0 loss: 0.373960  [   32/   43]
train() client id: f_00003-8-0 loss: 0.616017  [   32/   43]
train() client id: f_00003-9-0 loss: 0.634345  [   32/   43]
train() client id: f_00003-10-0 loss: 0.346603  [   32/   43]
train() client id: f_00004-0-0 loss: 0.799924  [   32/  306]
train() client id: f_00004-0-1 loss: 0.901580  [   64/  306]
train() client id: f_00004-0-2 loss: 0.867954  [   96/  306]
train() client id: f_00004-0-3 loss: 0.869058  [  128/  306]
train() client id: f_00004-0-4 loss: 0.927858  [  160/  306]
train() client id: f_00004-0-5 loss: 0.952162  [  192/  306]
train() client id: f_00004-0-6 loss: 0.953042  [  224/  306]
train() client id: f_00004-0-7 loss: 1.040363  [  256/  306]
train() client id: f_00004-0-8 loss: 0.860614  [  288/  306]
train() client id: f_00004-1-0 loss: 0.934352  [   32/  306]
train() client id: f_00004-1-1 loss: 1.004109  [   64/  306]
train() client id: f_00004-1-2 loss: 0.844327  [   96/  306]
train() client id: f_00004-1-3 loss: 0.897566  [  128/  306]
train() client id: f_00004-1-4 loss: 0.913645  [  160/  306]
train() client id: f_00004-1-5 loss: 0.850835  [  192/  306]
train() client id: f_00004-1-6 loss: 0.932802  [  224/  306]
train() client id: f_00004-1-7 loss: 0.898956  [  256/  306]
train() client id: f_00004-1-8 loss: 0.837828  [  288/  306]
train() client id: f_00004-2-0 loss: 0.811316  [   32/  306]
train() client id: f_00004-2-1 loss: 0.887502  [   64/  306]
train() client id: f_00004-2-2 loss: 0.869877  [   96/  306]
train() client id: f_00004-2-3 loss: 0.809018  [  128/  306]
train() client id: f_00004-2-4 loss: 0.932265  [  160/  306]
train() client id: f_00004-2-5 loss: 1.022324  [  192/  306]
train() client id: f_00004-2-6 loss: 0.789997  [  224/  306]
train() client id: f_00004-2-7 loss: 1.107934  [  256/  306]
train() client id: f_00004-2-8 loss: 0.802167  [  288/  306]
train() client id: f_00004-3-0 loss: 0.896908  [   32/  306]
train() client id: f_00004-3-1 loss: 0.774117  [   64/  306]
train() client id: f_00004-3-2 loss: 0.923303  [   96/  306]
train() client id: f_00004-3-3 loss: 1.007991  [  128/  306]
train() client id: f_00004-3-4 loss: 0.848377  [  160/  306]
train() client id: f_00004-3-5 loss: 0.899883  [  192/  306]
train() client id: f_00004-3-6 loss: 1.006237  [  224/  306]
train() client id: f_00004-3-7 loss: 0.901443  [  256/  306]
train() client id: f_00004-3-8 loss: 0.874884  [  288/  306]
train() client id: f_00004-4-0 loss: 0.826350  [   32/  306]
train() client id: f_00004-4-1 loss: 0.718944  [   64/  306]
train() client id: f_00004-4-2 loss: 1.064802  [   96/  306]
train() client id: f_00004-4-3 loss: 0.976728  [  128/  306]
train() client id: f_00004-4-4 loss: 0.986785  [  160/  306]
train() client id: f_00004-4-5 loss: 0.880790  [  192/  306]
train() client id: f_00004-4-6 loss: 0.874147  [  224/  306]
train() client id: f_00004-4-7 loss: 0.917076  [  256/  306]
train() client id: f_00004-4-8 loss: 0.861960  [  288/  306]
train() client id: f_00004-5-0 loss: 0.763873  [   32/  306]
train() client id: f_00004-5-1 loss: 1.023993  [   64/  306]
train() client id: f_00004-5-2 loss: 0.773849  [   96/  306]
train() client id: f_00004-5-3 loss: 0.951503  [  128/  306]
train() client id: f_00004-5-4 loss: 0.946561  [  160/  306]
train() client id: f_00004-5-5 loss: 0.994272  [  192/  306]
train() client id: f_00004-5-6 loss: 0.942906  [  224/  306]
train() client id: f_00004-5-7 loss: 0.890546  [  256/  306]
train() client id: f_00004-5-8 loss: 0.848457  [  288/  306]
train() client id: f_00004-6-0 loss: 0.931466  [   32/  306]
train() client id: f_00004-6-1 loss: 0.937600  [   64/  306]
train() client id: f_00004-6-2 loss: 0.964750  [   96/  306]
train() client id: f_00004-6-3 loss: 0.706633  [  128/  306]
train() client id: f_00004-6-4 loss: 0.844275  [  160/  306]
train() client id: f_00004-6-5 loss: 0.985009  [  192/  306]
train() client id: f_00004-6-6 loss: 0.868028  [  224/  306]
train() client id: f_00004-6-7 loss: 0.939141  [  256/  306]
train() client id: f_00004-6-8 loss: 0.960139  [  288/  306]
train() client id: f_00004-7-0 loss: 0.928059  [   32/  306]
train() client id: f_00004-7-1 loss: 1.060515  [   64/  306]
train() client id: f_00004-7-2 loss: 0.915993  [   96/  306]
train() client id: f_00004-7-3 loss: 0.860345  [  128/  306]
train() client id: f_00004-7-4 loss: 0.863304  [  160/  306]
train() client id: f_00004-7-5 loss: 0.830462  [  192/  306]
train() client id: f_00004-7-6 loss: 0.805165  [  224/  306]
train() client id: f_00004-7-7 loss: 0.869370  [  256/  306]
train() client id: f_00004-7-8 loss: 0.913240  [  288/  306]
train() client id: f_00004-8-0 loss: 0.848238  [   32/  306]
train() client id: f_00004-8-1 loss: 0.869389  [   64/  306]
train() client id: f_00004-8-2 loss: 0.888608  [   96/  306]
train() client id: f_00004-8-3 loss: 0.723269  [  128/  306]
train() client id: f_00004-8-4 loss: 0.859489  [  160/  306]
train() client id: f_00004-8-5 loss: 1.002068  [  192/  306]
train() client id: f_00004-8-6 loss: 0.952177  [  224/  306]
train() client id: f_00004-8-7 loss: 0.845792  [  256/  306]
train() client id: f_00004-8-8 loss: 1.055424  [  288/  306]
train() client id: f_00004-9-0 loss: 0.985201  [   32/  306]
train() client id: f_00004-9-1 loss: 0.785445  [   64/  306]
train() client id: f_00004-9-2 loss: 0.934390  [   96/  306]
train() client id: f_00004-9-3 loss: 1.011894  [  128/  306]
train() client id: f_00004-9-4 loss: 0.843100  [  160/  306]
train() client id: f_00004-9-5 loss: 0.828780  [  192/  306]
train() client id: f_00004-9-6 loss: 0.805919  [  224/  306]
train() client id: f_00004-9-7 loss: 0.942083  [  256/  306]
train() client id: f_00004-9-8 loss: 0.861001  [  288/  306]
train() client id: f_00004-10-0 loss: 0.958507  [   32/  306]
train() client id: f_00004-10-1 loss: 0.986734  [   64/  306]
train() client id: f_00004-10-2 loss: 0.803550  [   96/  306]
train() client id: f_00004-10-3 loss: 0.706844  [  128/  306]
train() client id: f_00004-10-4 loss: 0.959349  [  160/  306]
train() client id: f_00004-10-5 loss: 0.863223  [  192/  306]
train() client id: f_00004-10-6 loss: 0.908169  [  224/  306]
train() client id: f_00004-10-7 loss: 1.030092  [  256/  306]
train() client id: f_00004-10-8 loss: 0.800078  [  288/  306]
train() client id: f_00005-0-0 loss: 0.627979  [   32/  146]
train() client id: f_00005-0-1 loss: 0.560070  [   64/  146]
train() client id: f_00005-0-2 loss: 0.599075  [   96/  146]
train() client id: f_00005-0-3 loss: 0.296687  [  128/  146]
train() client id: f_00005-1-0 loss: 0.804293  [   32/  146]
train() client id: f_00005-1-1 loss: 0.334007  [   64/  146]
train() client id: f_00005-1-2 loss: 0.533234  [   96/  146]
train() client id: f_00005-1-3 loss: 0.421963  [  128/  146]
train() client id: f_00005-2-0 loss: 0.230575  [   32/  146]
train() client id: f_00005-2-1 loss: 0.629661  [   64/  146]
train() client id: f_00005-2-2 loss: 0.599414  [   96/  146]
train() client id: f_00005-2-3 loss: 0.652398  [  128/  146]
train() client id: f_00005-3-0 loss: 0.486969  [   32/  146]
train() client id: f_00005-3-1 loss: 0.378888  [   64/  146]
train() client id: f_00005-3-2 loss: 0.762826  [   96/  146]
train() client id: f_00005-3-3 loss: 0.604317  [  128/  146]
train() client id: f_00005-4-0 loss: 0.388342  [   32/  146]
train() client id: f_00005-4-1 loss: 0.714984  [   64/  146]
train() client id: f_00005-4-2 loss: 0.732217  [   96/  146]
train() client id: f_00005-4-3 loss: 0.398107  [  128/  146]
train() client id: f_00005-5-0 loss: 0.499700  [   32/  146]
train() client id: f_00005-5-1 loss: 0.421115  [   64/  146]
train() client id: f_00005-5-2 loss: 0.611873  [   96/  146]
train() client id: f_00005-5-3 loss: 0.491682  [  128/  146]
train() client id: f_00005-6-0 loss: 0.571999  [   32/  146]
train() client id: f_00005-6-1 loss: 0.653070  [   64/  146]
train() client id: f_00005-6-2 loss: 0.528054  [   96/  146]
train() client id: f_00005-6-3 loss: 0.261110  [  128/  146]
train() client id: f_00005-7-0 loss: 0.539088  [   32/  146]
train() client id: f_00005-7-1 loss: 0.672538  [   64/  146]
train() client id: f_00005-7-2 loss: 0.564957  [   96/  146]
train() client id: f_00005-7-3 loss: 0.503505  [  128/  146]
train() client id: f_00005-8-0 loss: 0.853928  [   32/  146]
train() client id: f_00005-8-1 loss: 0.240176  [   64/  146]
train() client id: f_00005-8-2 loss: 0.583228  [   96/  146]
train() client id: f_00005-8-3 loss: 0.566245  [  128/  146]
train() client id: f_00005-9-0 loss: 0.403331  [   32/  146]
train() client id: f_00005-9-1 loss: 0.734909  [   64/  146]
train() client id: f_00005-9-2 loss: 0.562196  [   96/  146]
train() client id: f_00005-9-3 loss: 0.454920  [  128/  146]
train() client id: f_00005-10-0 loss: 0.532307  [   32/  146]
train() client id: f_00005-10-1 loss: 0.637204  [   64/  146]
train() client id: f_00005-10-2 loss: 0.525445  [   96/  146]
train() client id: f_00005-10-3 loss: 0.515435  [  128/  146]
train() client id: f_00006-0-0 loss: 0.473186  [   32/   54]
train() client id: f_00006-1-0 loss: 0.477861  [   32/   54]
train() client id: f_00006-2-0 loss: 0.409915  [   32/   54]
train() client id: f_00006-3-0 loss: 0.388578  [   32/   54]
train() client id: f_00006-4-0 loss: 0.514717  [   32/   54]
train() client id: f_00006-5-0 loss: 0.510868  [   32/   54]
train() client id: f_00006-6-0 loss: 0.525587  [   32/   54]
train() client id: f_00006-7-0 loss: 0.464057  [   32/   54]
train() client id: f_00006-8-0 loss: 0.440334  [   32/   54]
train() client id: f_00006-9-0 loss: 0.467827  [   32/   54]
train() client id: f_00006-10-0 loss: 0.498682  [   32/   54]
train() client id: f_00007-0-0 loss: 0.850604  [   32/  179]
train() client id: f_00007-0-1 loss: 0.601301  [   64/  179]
train() client id: f_00007-0-2 loss: 0.526299  [   96/  179]
train() client id: f_00007-0-3 loss: 0.601981  [  128/  179]
train() client id: f_00007-0-4 loss: 0.568033  [  160/  179]
train() client id: f_00007-1-0 loss: 0.574515  [   32/  179]
train() client id: f_00007-1-1 loss: 0.646506  [   64/  179]
train() client id: f_00007-1-2 loss: 0.625019  [   96/  179]
train() client id: f_00007-1-3 loss: 0.466232  [  128/  179]
train() client id: f_00007-1-4 loss: 0.786072  [  160/  179]
train() client id: f_00007-2-0 loss: 0.656154  [   32/  179]
train() client id: f_00007-2-1 loss: 0.758088  [   64/  179]
train() client id: f_00007-2-2 loss: 0.644472  [   96/  179]
train() client id: f_00007-2-3 loss: 0.551847  [  128/  179]
train() client id: f_00007-2-4 loss: 0.531088  [  160/  179]
train() client id: f_00007-3-0 loss: 0.952766  [   32/  179]
train() client id: f_00007-3-1 loss: 0.597602  [   64/  179]
train() client id: f_00007-3-2 loss: 0.463208  [   96/  179]
train() client id: f_00007-3-3 loss: 0.433408  [  128/  179]
train() client id: f_00007-3-4 loss: 0.676224  [  160/  179]
train() client id: f_00007-4-0 loss: 0.655636  [   32/  179]
train() client id: f_00007-4-1 loss: 0.624556  [   64/  179]
train() client id: f_00007-4-2 loss: 0.555113  [   96/  179]
train() client id: f_00007-4-3 loss: 0.514470  [  128/  179]
train() client id: f_00007-4-4 loss: 0.697658  [  160/  179]
train() client id: f_00007-5-0 loss: 0.753299  [   32/  179]
train() client id: f_00007-5-1 loss: 0.725271  [   64/  179]
train() client id: f_00007-5-2 loss: 0.633905  [   96/  179]
train() client id: f_00007-5-3 loss: 0.479599  [  128/  179]
train() client id: f_00007-5-4 loss: 0.593431  [  160/  179]
train() client id: f_00007-6-0 loss: 0.769170  [   32/  179]
train() client id: f_00007-6-1 loss: 0.493357  [   64/  179]
train() client id: f_00007-6-2 loss: 0.615873  [   96/  179]
train() client id: f_00007-6-3 loss: 0.482346  [  128/  179]
train() client id: f_00007-6-4 loss: 0.736189  [  160/  179]
train() client id: f_00007-7-0 loss: 0.465709  [   32/  179]
train() client id: f_00007-7-1 loss: 1.093348  [   64/  179]
train() client id: f_00007-7-2 loss: 0.553371  [   96/  179]
train() client id: f_00007-7-3 loss: 0.590097  [  128/  179]
train() client id: f_00007-7-4 loss: 0.468184  [  160/  179]
train() client id: f_00007-8-0 loss: 0.464700  [   32/  179]
train() client id: f_00007-8-1 loss: 0.763269  [   64/  179]
train() client id: f_00007-8-2 loss: 0.765568  [   96/  179]
train() client id: f_00007-8-3 loss: 0.665465  [  128/  179]
train() client id: f_00007-8-4 loss: 0.548681  [  160/  179]
train() client id: f_00007-9-0 loss: 0.701982  [   32/  179]
train() client id: f_00007-9-1 loss: 0.532988  [   64/  179]
train() client id: f_00007-9-2 loss: 0.893701  [   96/  179]
train() client id: f_00007-9-3 loss: 0.497690  [  128/  179]
train() client id: f_00007-9-4 loss: 0.563215  [  160/  179]
train() client id: f_00007-10-0 loss: 0.491332  [   32/  179]
train() client id: f_00007-10-1 loss: 0.602066  [   64/  179]
train() client id: f_00007-10-2 loss: 0.801581  [   96/  179]
train() client id: f_00007-10-3 loss: 0.570532  [  128/  179]
train() client id: f_00007-10-4 loss: 0.562705  [  160/  179]
train() client id: f_00008-0-0 loss: 0.710891  [   32/  130]
train() client id: f_00008-0-1 loss: 0.790701  [   64/  130]
train() client id: f_00008-0-2 loss: 0.905426  [   96/  130]
train() client id: f_00008-0-3 loss: 0.847936  [  128/  130]
train() client id: f_00008-1-0 loss: 0.731064  [   32/  130]
train() client id: f_00008-1-1 loss: 1.013700  [   64/  130]
train() client id: f_00008-1-2 loss: 0.690661  [   96/  130]
train() client id: f_00008-1-3 loss: 0.779843  [  128/  130]
train() client id: f_00008-2-0 loss: 0.855255  [   32/  130]
train() client id: f_00008-2-1 loss: 0.785361  [   64/  130]
train() client id: f_00008-2-2 loss: 0.836301  [   96/  130]
train() client id: f_00008-2-3 loss: 0.775196  [  128/  130]
train() client id: f_00008-3-0 loss: 0.908118  [   32/  130]
train() client id: f_00008-3-1 loss: 0.779909  [   64/  130]
train() client id: f_00008-3-2 loss: 0.801241  [   96/  130]
train() client id: f_00008-3-3 loss: 0.752980  [  128/  130]
train() client id: f_00008-4-0 loss: 0.781277  [   32/  130]
train() client id: f_00008-4-1 loss: 0.829015  [   64/  130]
train() client id: f_00008-4-2 loss: 0.796914  [   96/  130]
train() client id: f_00008-4-3 loss: 0.797701  [  128/  130]
train() client id: f_00008-5-0 loss: 0.879941  [   32/  130]
train() client id: f_00008-5-1 loss: 0.818569  [   64/  130]
train() client id: f_00008-5-2 loss: 0.771524  [   96/  130]
train() client id: f_00008-5-3 loss: 0.774525  [  128/  130]
train() client id: f_00008-6-0 loss: 0.794932  [   32/  130]
train() client id: f_00008-6-1 loss: 0.818419  [   64/  130]
train() client id: f_00008-6-2 loss: 0.793571  [   96/  130]
train() client id: f_00008-6-3 loss: 0.847800  [  128/  130]
train() client id: f_00008-7-0 loss: 0.786308  [   32/  130]
train() client id: f_00008-7-1 loss: 0.906360  [   64/  130]
train() client id: f_00008-7-2 loss: 0.800742  [   96/  130]
train() client id: f_00008-7-3 loss: 0.736394  [  128/  130]
train() client id: f_00008-8-0 loss: 0.786050  [   32/  130]
train() client id: f_00008-8-1 loss: 0.882813  [   64/  130]
train() client id: f_00008-8-2 loss: 0.795395  [   96/  130]
train() client id: f_00008-8-3 loss: 0.778732  [  128/  130]
train() client id: f_00008-9-0 loss: 0.671970  [   32/  130]
train() client id: f_00008-9-1 loss: 0.949996  [   64/  130]
train() client id: f_00008-9-2 loss: 0.801413  [   96/  130]
train() client id: f_00008-9-3 loss: 0.811845  [  128/  130]
train() client id: f_00008-10-0 loss: 0.834578  [   32/  130]
train() client id: f_00008-10-1 loss: 0.828631  [   64/  130]
train() client id: f_00008-10-2 loss: 0.677468  [   96/  130]
train() client id: f_00008-10-3 loss: 0.897282  [  128/  130]
train() client id: f_00009-0-0 loss: 0.852668  [   32/  118]
train() client id: f_00009-0-1 loss: 0.931082  [   64/  118]
train() client id: f_00009-0-2 loss: 0.839301  [   96/  118]
train() client id: f_00009-1-0 loss: 0.913039  [   32/  118]
train() client id: f_00009-1-1 loss: 0.771577  [   64/  118]
train() client id: f_00009-1-2 loss: 0.877791  [   96/  118]
train() client id: f_00009-2-0 loss: 0.943948  [   32/  118]
train() client id: f_00009-2-1 loss: 0.786277  [   64/  118]
train() client id: f_00009-2-2 loss: 0.737103  [   96/  118]
train() client id: f_00009-3-0 loss: 0.912792  [   32/  118]
train() client id: f_00009-3-1 loss: 0.756448  [   64/  118]
train() client id: f_00009-3-2 loss: 0.855530  [   96/  118]
train() client id: f_00009-4-0 loss: 0.805489  [   32/  118]
train() client id: f_00009-4-1 loss: 0.652302  [   64/  118]
train() client id: f_00009-4-2 loss: 0.852129  [   96/  118]
train() client id: f_00009-5-0 loss: 0.856233  [   32/  118]
train() client id: f_00009-5-1 loss: 0.733653  [   64/  118]
train() client id: f_00009-5-2 loss: 0.918282  [   96/  118]
train() client id: f_00009-6-0 loss: 0.804614  [   32/  118]
train() client id: f_00009-6-1 loss: 0.787828  [   64/  118]
train() client id: f_00009-6-2 loss: 0.683751  [   96/  118]
train() client id: f_00009-7-0 loss: 0.691486  [   32/  118]
train() client id: f_00009-7-1 loss: 0.741927  [   64/  118]
train() client id: f_00009-7-2 loss: 0.916339  [   96/  118]
train() client id: f_00009-8-0 loss: 0.771819  [   32/  118]
train() client id: f_00009-8-1 loss: 0.817701  [   64/  118]
train() client id: f_00009-8-2 loss: 0.743856  [   96/  118]
train() client id: f_00009-9-0 loss: 0.765001  [   32/  118]
train() client id: f_00009-9-1 loss: 0.776068  [   64/  118]
train() client id: f_00009-9-2 loss: 0.842080  [   96/  118]
train() client id: f_00009-10-0 loss: 0.834472  [   32/  118]
train() client id: f_00009-10-1 loss: 0.700845  [   64/  118]
train() client id: f_00009-10-2 loss: 0.732122  [   96/  118]
At round 50 accuracy: 0.6472148541114059
At round 50 training accuracy: 0.5888665325285044
At round 50 training loss: 0.8321697883441187
update_location
xs = [  -3.9056584     4.20031788  270.00902392   18.81129433    0.97929623
    3.95640986 -232.44319194 -211.32485185  254.66397685 -197.06087855]
ys = [ 262.5879595   245.55583871    1.32061395 -232.45517586  224.35018685
  207.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [281.0119048  265.17034637 287.93509167 253.75041592 245.62973224
 230.65639906 253.05479253 233.79236357 274.1576391  221.01810268]
dists_bs = [192.21062503 192.58992783 477.70028313 451.20121691 182.76803233
 181.94000418 186.68249879 177.9251228  457.59769543 172.41570577]
uav_gains = [3.91606458e-12 5.45480845e-12 3.39525697e-12 6.88931259e-12
 8.07391748e-12 1.05867714e-11 6.98555204e-12 1.00271385e-11
 4.51953049e-12 1.24147554e-11]
bs_gains = [4.45366340e-11 4.42914694e-11 3.48063665e-12 4.08373311e-12
 5.12830001e-11 5.19391842e-11 4.83285750e-11 5.52878520e-11
 3.92590119e-12 6.03780354e-11]
Round 51
-------------------------------
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.27047392  8.72826714  4.21425716  1.53313884 10.06392708  4.84207237
  1.89154359  5.95729153  4.4008638   3.92598233]
obj_prev = 49.827817776908454
eta_min = 2.5652490042097954e-22	eta_max = 0.9391007800776031
af = 10.460568009377656	bf = 1.156843673852587	zeta = 11.506624810315422	eta = 0.9090909090909091
af = 10.460568009377656	bf = 1.156843673852587	zeta = 23.692265208499137	eta = 0.44151827262279386
af = 10.460568009377656	bf = 1.156843673852587	zeta = 17.38415562977596	eta = 0.6017300024316722
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.245893349082294	eta = 0.6438899840474798
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.180034455220028	eta = 0.6465108611683364
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.179790636766835	eta = 0.6465206036478086
eta = 0.6465206036478086
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.03599007 0.0756934  0.0354188  0.01228233 0.08740446 0.04170278
 0.01542432 0.05112875 0.03713261 0.03370499]
ene_total = [1.53443253 2.51779645 1.54247681 0.73582445 2.86602817 1.48040303
 0.82941199 1.87320864 1.56863488 1.23157367]
ti_comp = [0.75933363 0.83189525 0.75096662 0.78576566 0.83410293 0.83428835
 0.78630533 0.79893754 0.75947361 0.83641438]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [5.05315743e-06 3.91666828e-05 4.92425175e-06 1.87558063e-07
 5.99848321e-05 6.51242403e-06 3.70949938e-07 1.30872950e-05
 5.54780444e-06 3.42073551e-06]
ene_total = [0.44384841 0.22497254 0.46920207 0.3635944  0.21891275 0.21673023
 0.36196441 0.32406582 0.44343917 0.21019324]
optimize_network iter = 0 obj = 3.2769230299910435
eta = 0.6465206036478086
freqs = [23698456.65782105 45494550.69374827 23582139.4675787   7815514.32290755
 52394286.76461308 24993025.32151997  9808095.66767819 31997964.51374336
 24446282.03194968 20148497.51725332]
eta_min = 0.6465206036478099	eta_max = 0.6980699554597802
af = 0.0033958499648260683	bf = 1.156843673852587	zeta = 0.003735434961308675	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.10424650e-06 8.55894019e-06 1.07607725e-06 4.09863212e-08
 1.31082480e-05 1.42313425e-06 8.10622219e-08 2.85991478e-06
 1.21233975e-06 7.47519793e-07]
ene_total = [1.7253386  0.8711479  1.82393243 1.41373717 0.84566865 0.84210666
 1.4073824  1.2588513  1.7237018  0.81697375]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 1 obj = 3.8353964400634717
eta = 0.6980699554597802
freqs = [23642854.46224663 44569165.40272366 23582139.46757871  7742332.9461439
 51302927.41865983 24471403.78525164  9714909.2256883  31593097.86188618
 24387978.69085684 19718556.51146085]
eta_min = 0.698069955459783	eta_max = 0.6980699554597762
af = 0.0032766452472554383	bf = 1.156843673852587	zeta = 0.003604309771980982	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.09907093e-06 8.21429384e-06 1.07607725e-06 4.02223555e-08
 1.25678526e-05 1.36435058e-06 7.95291995e-08 2.78800028e-06
 1.20656389e-06 7.15958094e-07]
ene_total = [1.72533799 0.87110728 1.82393243 1.41373708 0.84560497 0.84209973
 1.40738222 1.25884283 1.72370112 0.81697003]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 2 obj = 3.8353964400634206
eta = 0.6980699554597762
freqs = [23642854.46224662 44569165.40272372 23582139.46757869  7742332.9461439
 51302927.41865991 24471403.78525168  9714909.2256883  31593097.8618862
 24387978.69085682 19718556.51146088]
Done!
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [3.87376305e-06 2.89519331e-05 3.79272000e-06 1.41766896e-07
 4.42963978e-05 4.80876231e-06 2.80307000e-07 9.82652913e-06
 4.25263056e-06 2.52345135e-06]
ene_total = [0.01464408 0.007413   0.0154807  0.01199715 0.00720758 0.00714955
 0.01194332 0.01068965 0.01463047 0.00693466]
At round 51 energy consumption: 0.10809015491855846
At round 51 eta: 0.6980699554597762
At round 51 a_n: 10.712764660819193
At round 51 local rounds: 11.76975943561131
At round 51 global rounds: 35.48094949322612
gradient difference: 0.4024025797843933
train() client id: f_00000-0-0 loss: 1.136982  [   32/  126]
train() client id: f_00000-0-1 loss: 1.173598  [   64/  126]
train() client id: f_00000-0-2 loss: 1.028527  [   96/  126]
train() client id: f_00000-1-0 loss: 1.154179  [   32/  126]
train() client id: f_00000-1-1 loss: 1.047433  [   64/  126]
train() client id: f_00000-1-2 loss: 1.040777  [   96/  126]
train() client id: f_00000-2-0 loss: 0.997218  [   32/  126]
train() client id: f_00000-2-1 loss: 0.989363  [   64/  126]
train() client id: f_00000-2-2 loss: 0.997198  [   96/  126]
train() client id: f_00000-3-0 loss: 0.987889  [   32/  126]
train() client id: f_00000-3-1 loss: 1.028917  [   64/  126]
train() client id: f_00000-3-2 loss: 0.918543  [   96/  126]
train() client id: f_00000-4-0 loss: 0.948193  [   32/  126]
train() client id: f_00000-4-1 loss: 0.963055  [   64/  126]
train() client id: f_00000-4-2 loss: 0.881276  [   96/  126]
train() client id: f_00000-5-0 loss: 0.779285  [   32/  126]
train() client id: f_00000-5-1 loss: 0.959616  [   64/  126]
train() client id: f_00000-5-2 loss: 0.956768  [   96/  126]
train() client id: f_00000-6-0 loss: 0.895407  [   32/  126]
train() client id: f_00000-6-1 loss: 0.831637  [   64/  126]
train() client id: f_00000-6-2 loss: 0.910612  [   96/  126]
train() client id: f_00000-7-0 loss: 0.852504  [   32/  126]
train() client id: f_00000-7-1 loss: 0.914857  [   64/  126]
train() client id: f_00000-7-2 loss: 0.836365  [   96/  126]
train() client id: f_00000-8-0 loss: 0.803365  [   32/  126]
train() client id: f_00000-8-1 loss: 0.948281  [   64/  126]
train() client id: f_00000-8-2 loss: 0.786082  [   96/  126]
train() client id: f_00000-9-0 loss: 0.843722  [   32/  126]
train() client id: f_00000-9-1 loss: 0.884864  [   64/  126]
train() client id: f_00000-9-2 loss: 0.922841  [   96/  126]
train() client id: f_00000-10-0 loss: 0.760719  [   32/  126]
train() client id: f_00000-10-1 loss: 0.919303  [   64/  126]
train() client id: f_00000-10-2 loss: 0.874770  [   96/  126]
train() client id: f_00001-0-0 loss: 0.481896  [   32/  265]
train() client id: f_00001-0-1 loss: 0.479353  [   64/  265]
train() client id: f_00001-0-2 loss: 0.457853  [   96/  265]
train() client id: f_00001-0-3 loss: 0.364450  [  128/  265]
train() client id: f_00001-0-4 loss: 0.451693  [  160/  265]
train() client id: f_00001-0-5 loss: 0.414730  [  192/  265]
train() client id: f_00001-0-6 loss: 0.525479  [  224/  265]
train() client id: f_00001-0-7 loss: 0.583033  [  256/  265]
train() client id: f_00001-1-0 loss: 0.501966  [   32/  265]
train() client id: f_00001-1-1 loss: 0.460254  [   64/  265]
train() client id: f_00001-1-2 loss: 0.399893  [   96/  265]
train() client id: f_00001-1-3 loss: 0.570932  [  128/  265]
train() client id: f_00001-1-4 loss: 0.363211  [  160/  265]
train() client id: f_00001-1-5 loss: 0.457136  [  192/  265]
train() client id: f_00001-1-6 loss: 0.616934  [  224/  265]
train() client id: f_00001-1-7 loss: 0.423585  [  256/  265]
train() client id: f_00001-2-0 loss: 0.611446  [   32/  265]
train() client id: f_00001-2-1 loss: 0.450363  [   64/  265]
train() client id: f_00001-2-2 loss: 0.424583  [   96/  265]
train() client id: f_00001-2-3 loss: 0.454034  [  128/  265]
train() client id: f_00001-2-4 loss: 0.449347  [  160/  265]
train() client id: f_00001-2-5 loss: 0.415289  [  192/  265]
train() client id: f_00001-2-6 loss: 0.586476  [  224/  265]
train() client id: f_00001-2-7 loss: 0.387290  [  256/  265]
train() client id: f_00001-3-0 loss: 0.502545  [   32/  265]
train() client id: f_00001-3-1 loss: 0.400665  [   64/  265]
train() client id: f_00001-3-2 loss: 0.445853  [   96/  265]
train() client id: f_00001-3-3 loss: 0.442586  [  128/  265]
train() client id: f_00001-3-4 loss: 0.499739  [  160/  265]
train() client id: f_00001-3-5 loss: 0.455694  [  192/  265]
train() client id: f_00001-3-6 loss: 0.460427  [  224/  265]
train() client id: f_00001-3-7 loss: 0.547666  [  256/  265]
train() client id: f_00001-4-0 loss: 0.464238  [   32/  265]
train() client id: f_00001-4-1 loss: 0.528447  [   64/  265]
train() client id: f_00001-4-2 loss: 0.542368  [   96/  265]
train() client id: f_00001-4-3 loss: 0.475629  [  128/  265]
train() client id: f_00001-4-4 loss: 0.367086  [  160/  265]
train() client id: f_00001-4-5 loss: 0.455340  [  192/  265]
train() client id: f_00001-4-6 loss: 0.446524  [  224/  265]
train() client id: f_00001-4-7 loss: 0.477453  [  256/  265]
train() client id: f_00001-5-0 loss: 0.387614  [   32/  265]
train() client id: f_00001-5-1 loss: 0.429694  [   64/  265]
train() client id: f_00001-5-2 loss: 0.535561  [   96/  265]
train() client id: f_00001-5-3 loss: 0.400821  [  128/  265]
train() client id: f_00001-5-4 loss: 0.604572  [  160/  265]
train() client id: f_00001-5-5 loss: 0.500305  [  192/  265]
train() client id: f_00001-5-6 loss: 0.491581  [  224/  265]
train() client id: f_00001-5-7 loss: 0.380186  [  256/  265]
train() client id: f_00001-6-0 loss: 0.436036  [   32/  265]
train() client id: f_00001-6-1 loss: 0.454312  [   64/  265]
train() client id: f_00001-6-2 loss: 0.409495  [   96/  265]
train() client id: f_00001-6-3 loss: 0.415409  [  128/  265]
train() client id: f_00001-6-4 loss: 0.462676  [  160/  265]
train() client id: f_00001-6-5 loss: 0.507797  [  192/  265]
train() client id: f_00001-6-6 loss: 0.576159  [  224/  265]
train() client id: f_00001-6-7 loss: 0.465571  [  256/  265]
train() client id: f_00001-7-0 loss: 0.435368  [   32/  265]
train() client id: f_00001-7-1 loss: 0.369132  [   64/  265]
train() client id: f_00001-7-2 loss: 0.355803  [   96/  265]
train() client id: f_00001-7-3 loss: 0.556766  [  128/  265]
train() client id: f_00001-7-4 loss: 0.484786  [  160/  265]
train() client id: f_00001-7-5 loss: 0.532455  [  192/  265]
train() client id: f_00001-7-6 loss: 0.465865  [  224/  265]
train() client id: f_00001-7-7 loss: 0.473407  [  256/  265]
train() client id: f_00001-8-0 loss: 0.484048  [   32/  265]
train() client id: f_00001-8-1 loss: 0.458002  [   64/  265]
train() client id: f_00001-8-2 loss: 0.480688  [   96/  265]
train() client id: f_00001-8-3 loss: 0.471404  [  128/  265]
train() client id: f_00001-8-4 loss: 0.472119  [  160/  265]
train() client id: f_00001-8-5 loss: 0.366254  [  192/  265]
train() client id: f_00001-8-6 loss: 0.453609  [  224/  265]
train() client id: f_00001-8-7 loss: 0.548977  [  256/  265]
train() client id: f_00001-9-0 loss: 0.389167  [   32/  265]
train() client id: f_00001-9-1 loss: 0.635359  [   64/  265]
train() client id: f_00001-9-2 loss: 0.345766  [   96/  265]
train() client id: f_00001-9-3 loss: 0.468243  [  128/  265]
train() client id: f_00001-9-4 loss: 0.513938  [  160/  265]
train() client id: f_00001-9-5 loss: 0.428809  [  192/  265]
train() client id: f_00001-9-6 loss: 0.508197  [  224/  265]
train() client id: f_00001-9-7 loss: 0.381287  [  256/  265]
train() client id: f_00001-10-0 loss: 0.423625  [   32/  265]
train() client id: f_00001-10-1 loss: 0.441077  [   64/  265]
train() client id: f_00001-10-2 loss: 0.541404  [   96/  265]
train() client id: f_00001-10-3 loss: 0.418901  [  128/  265]
train() client id: f_00001-10-4 loss: 0.407145  [  160/  265]
train() client id: f_00001-10-5 loss: 0.469856  [  192/  265]
train() client id: f_00001-10-6 loss: 0.623229  [  224/  265]
train() client id: f_00001-10-7 loss: 0.426100  [  256/  265]
train() client id: f_00002-0-0 loss: 0.955071  [   32/  124]
train() client id: f_00002-0-1 loss: 0.946065  [   64/  124]
train() client id: f_00002-0-2 loss: 1.046135  [   96/  124]
train() client id: f_00002-1-0 loss: 0.844708  [   32/  124]
train() client id: f_00002-1-1 loss: 1.075454  [   64/  124]
train() client id: f_00002-1-2 loss: 0.936098  [   96/  124]
train() client id: f_00002-2-0 loss: 0.817425  [   32/  124]
train() client id: f_00002-2-1 loss: 0.878733  [   64/  124]
train() client id: f_00002-2-2 loss: 0.889552  [   96/  124]
train() client id: f_00002-3-0 loss: 0.790023  [   32/  124]
train() client id: f_00002-3-1 loss: 0.830559  [   64/  124]
train() client id: f_00002-3-2 loss: 0.977866  [   96/  124]
train() client id: f_00002-4-0 loss: 0.955589  [   32/  124]
train() client id: f_00002-4-1 loss: 0.684155  [   64/  124]
train() client id: f_00002-4-2 loss: 0.656262  [   96/  124]
train() client id: f_00002-5-0 loss: 0.721385  [   32/  124]
train() client id: f_00002-5-1 loss: 0.732042  [   64/  124]
train() client id: f_00002-5-2 loss: 0.877909  [   96/  124]
train() client id: f_00002-6-0 loss: 0.645819  [   32/  124]
train() client id: f_00002-6-1 loss: 0.845760  [   64/  124]
train() client id: f_00002-6-2 loss: 0.896992  [   96/  124]
train() client id: f_00002-7-0 loss: 0.657718  [   32/  124]
train() client id: f_00002-7-1 loss: 0.788479  [   64/  124]
train() client id: f_00002-7-2 loss: 0.943181  [   96/  124]
train() client id: f_00002-8-0 loss: 0.524173  [   32/  124]
train() client id: f_00002-8-1 loss: 0.835858  [   64/  124]
train() client id: f_00002-8-2 loss: 0.694049  [   96/  124]
train() client id: f_00002-9-0 loss: 0.716541  [   32/  124]
train() client id: f_00002-9-1 loss: 0.558884  [   64/  124]
train() client id: f_00002-9-2 loss: 0.844542  [   96/  124]
train() client id: f_00002-10-0 loss: 0.702435  [   32/  124]
train() client id: f_00002-10-1 loss: 0.617038  [   64/  124]
train() client id: f_00002-10-2 loss: 0.732651  [   96/  124]
train() client id: f_00003-0-0 loss: 1.151534  [   32/   43]
train() client id: f_00003-1-0 loss: 0.937293  [   32/   43]
train() client id: f_00003-2-0 loss: 0.678084  [   32/   43]
train() client id: f_00003-3-0 loss: 0.838472  [   32/   43]
train() client id: f_00003-4-0 loss: 1.037037  [   32/   43]
train() client id: f_00003-5-0 loss: 0.836906  [   32/   43]
train() client id: f_00003-6-0 loss: 0.761024  [   32/   43]
train() client id: f_00003-7-0 loss: 0.787743  [   32/   43]
train() client id: f_00003-8-0 loss: 1.006829  [   32/   43]
train() client id: f_00003-9-0 loss: 0.891583  [   32/   43]
train() client id: f_00003-10-0 loss: 0.892965  [   32/   43]
train() client id: f_00004-0-0 loss: 0.977357  [   32/  306]
train() client id: f_00004-0-1 loss: 1.026668  [   64/  306]
train() client id: f_00004-0-2 loss: 0.975040  [   96/  306]
train() client id: f_00004-0-3 loss: 0.814153  [  128/  306]
train() client id: f_00004-0-4 loss: 0.938083  [  160/  306]
train() client id: f_00004-0-5 loss: 0.901490  [  192/  306]
train() client id: f_00004-0-6 loss: 0.942564  [  224/  306]
train() client id: f_00004-0-7 loss: 0.995579  [  256/  306]
train() client id: f_00004-0-8 loss: 0.889760  [  288/  306]
train() client id: f_00004-1-0 loss: 0.948670  [   32/  306]
train() client id: f_00004-1-1 loss: 0.938390  [   64/  306]
train() client id: f_00004-1-2 loss: 0.907636  [   96/  306]
train() client id: f_00004-1-3 loss: 0.921141  [  128/  306]
train() client id: f_00004-1-4 loss: 0.984352  [  160/  306]
train() client id: f_00004-1-5 loss: 0.806548  [  192/  306]
train() client id: f_00004-1-6 loss: 0.970514  [  224/  306]
train() client id: f_00004-1-7 loss: 1.023628  [  256/  306]
train() client id: f_00004-1-8 loss: 0.852568  [  288/  306]
train() client id: f_00004-2-0 loss: 0.826229  [   32/  306]
train() client id: f_00004-2-1 loss: 0.948232  [   64/  306]
train() client id: f_00004-2-2 loss: 0.910880  [   96/  306]
train() client id: f_00004-2-3 loss: 0.832903  [  128/  306]
train() client id: f_00004-2-4 loss: 0.772372  [  160/  306]
train() client id: f_00004-2-5 loss: 1.125825  [  192/  306]
train() client id: f_00004-2-6 loss: 0.952772  [  224/  306]
train() client id: f_00004-2-7 loss: 0.975950  [  256/  306]
train() client id: f_00004-2-8 loss: 0.967580  [  288/  306]
train() client id: f_00004-3-0 loss: 0.879941  [   32/  306]
train() client id: f_00004-3-1 loss: 0.907008  [   64/  306]
train() client id: f_00004-3-2 loss: 0.980469  [   96/  306]
train() client id: f_00004-3-3 loss: 1.011022  [  128/  306]
train() client id: f_00004-3-4 loss: 0.910373  [  160/  306]
train() client id: f_00004-3-5 loss: 0.814811  [  192/  306]
train() client id: f_00004-3-6 loss: 0.833206  [  224/  306]
train() client id: f_00004-3-7 loss: 0.945308  [  256/  306]
train() client id: f_00004-3-8 loss: 0.886401  [  288/  306]
train() client id: f_00004-4-0 loss: 0.881919  [   32/  306]
train() client id: f_00004-4-1 loss: 0.939800  [   64/  306]
train() client id: f_00004-4-2 loss: 0.764695  [   96/  306]
train() client id: f_00004-4-3 loss: 0.961597  [  128/  306]
train() client id: f_00004-4-4 loss: 0.963966  [  160/  306]
train() client id: f_00004-4-5 loss: 0.956232  [  192/  306]
train() client id: f_00004-4-6 loss: 0.903686  [  224/  306]
train() client id: f_00004-4-7 loss: 0.831785  [  256/  306]
train() client id: f_00004-4-8 loss: 0.958151  [  288/  306]
train() client id: f_00004-5-0 loss: 0.862492  [   32/  306]
train() client id: f_00004-5-1 loss: 0.850905  [   64/  306]
train() client id: f_00004-5-2 loss: 0.886772  [   96/  306]
train() client id: f_00004-5-3 loss: 0.813896  [  128/  306]
train() client id: f_00004-5-4 loss: 0.991227  [  160/  306]
train() client id: f_00004-5-5 loss: 0.928690  [  192/  306]
train() client id: f_00004-5-6 loss: 0.985494  [  224/  306]
train() client id: f_00004-5-7 loss: 0.898930  [  256/  306]
train() client id: f_00004-5-8 loss: 0.874340  [  288/  306]
train() client id: f_00004-6-0 loss: 1.025542  [   32/  306]
train() client id: f_00004-6-1 loss: 0.872480  [   64/  306]
train() client id: f_00004-6-2 loss: 0.961183  [   96/  306]
train() client id: f_00004-6-3 loss: 0.920213  [  128/  306]
train() client id: f_00004-6-4 loss: 0.771656  [  160/  306]
train() client id: f_00004-6-5 loss: 0.937264  [  192/  306]
train() client id: f_00004-6-6 loss: 0.854361  [  224/  306]
train() client id: f_00004-6-7 loss: 0.937606  [  256/  306]
train() client id: f_00004-6-8 loss: 0.929753  [  288/  306]
train() client id: f_00004-7-0 loss: 0.804659  [   32/  306]
train() client id: f_00004-7-1 loss: 0.879425  [   64/  306]
train() client id: f_00004-7-2 loss: 0.949216  [   96/  306]
train() client id: f_00004-7-3 loss: 0.866821  [  128/  306]
train() client id: f_00004-7-4 loss: 0.965742  [  160/  306]
train() client id: f_00004-7-5 loss: 0.954628  [  192/  306]
train() client id: f_00004-7-6 loss: 0.893233  [  224/  306]
train() client id: f_00004-7-7 loss: 0.878254  [  256/  306]
train() client id: f_00004-7-8 loss: 0.842057  [  288/  306]
train() client id: f_00004-8-0 loss: 0.974046  [   32/  306]
train() client id: f_00004-8-1 loss: 0.876547  [   64/  306]
train() client id: f_00004-8-2 loss: 0.902212  [   96/  306]
train() client id: f_00004-8-3 loss: 0.911309  [  128/  306]
train() client id: f_00004-8-4 loss: 1.097665  [  160/  306]
train() client id: f_00004-8-5 loss: 0.861704  [  192/  306]
train() client id: f_00004-8-6 loss: 0.747270  [  224/  306]
train() client id: f_00004-8-7 loss: 0.718284  [  256/  306]
train() client id: f_00004-8-8 loss: 1.057189  [  288/  306]
train() client id: f_00004-9-0 loss: 0.869293  [   32/  306]
train() client id: f_00004-9-1 loss: 0.871390  [   64/  306]
train() client id: f_00004-9-2 loss: 0.898027  [   96/  306]
train() client id: f_00004-9-3 loss: 1.035522  [  128/  306]
train() client id: f_00004-9-4 loss: 0.893135  [  160/  306]
train() client id: f_00004-9-5 loss: 0.777471  [  192/  306]
train() client id: f_00004-9-6 loss: 0.894836  [  224/  306]
train() client id: f_00004-9-7 loss: 0.932086  [  256/  306]
train() client id: f_00004-9-8 loss: 0.972024  [  288/  306]
train() client id: f_00004-10-0 loss: 0.899338  [   32/  306]
train() client id: f_00004-10-1 loss: 0.864633  [   64/  306]
train() client id: f_00004-10-2 loss: 0.824072  [   96/  306]
train() client id: f_00004-10-3 loss: 0.858229  [  128/  306]
train() client id: f_00004-10-4 loss: 0.855462  [  160/  306]
train() client id: f_00004-10-5 loss: 0.959563  [  192/  306]
train() client id: f_00004-10-6 loss: 0.920097  [  224/  306]
train() client id: f_00004-10-7 loss: 0.861253  [  256/  306]
train() client id: f_00004-10-8 loss: 1.002064  [  288/  306]
train() client id: f_00005-0-0 loss: 0.565601  [   32/  146]
train() client id: f_00005-0-1 loss: 0.900509  [   64/  146]
train() client id: f_00005-0-2 loss: 0.618515  [   96/  146]
train() client id: f_00005-0-3 loss: 0.661226  [  128/  146]
train() client id: f_00005-1-0 loss: 1.056480  [   32/  146]
train() client id: f_00005-1-1 loss: 0.488943  [   64/  146]
train() client id: f_00005-1-2 loss: 0.662119  [   96/  146]
train() client id: f_00005-1-3 loss: 0.637406  [  128/  146]
train() client id: f_00005-2-0 loss: 0.427098  [   32/  146]
train() client id: f_00005-2-1 loss: 0.774374  [   64/  146]
train() client id: f_00005-2-2 loss: 0.550241  [   96/  146]
train() client id: f_00005-2-3 loss: 0.722268  [  128/  146]
train() client id: f_00005-3-0 loss: 0.784022  [   32/  146]
train() client id: f_00005-3-1 loss: 0.664544  [   64/  146]
train() client id: f_00005-3-2 loss: 0.415115  [   96/  146]
train() client id: f_00005-3-3 loss: 0.826348  [  128/  146]
train() client id: f_00005-4-0 loss: 0.742654  [   32/  146]
train() client id: f_00005-4-1 loss: 0.563314  [   64/  146]
train() client id: f_00005-4-2 loss: 0.598542  [   96/  146]
train() client id: f_00005-4-3 loss: 0.767453  [  128/  146]
train() client id: f_00005-5-0 loss: 0.590739  [   32/  146]
train() client id: f_00005-5-1 loss: 0.739904  [   64/  146]
train() client id: f_00005-5-2 loss: 0.517094  [   96/  146]
train() client id: f_00005-5-3 loss: 0.685883  [  128/  146]
train() client id: f_00005-6-0 loss: 0.505072  [   32/  146]
train() client id: f_00005-6-1 loss: 0.910606  [   64/  146]
train() client id: f_00005-6-2 loss: 0.503813  [   96/  146]
train() client id: f_00005-6-3 loss: 0.768847  [  128/  146]
train() client id: f_00005-7-0 loss: 0.674522  [   32/  146]
train() client id: f_00005-7-1 loss: 0.837145  [   64/  146]
train() client id: f_00005-7-2 loss: 0.657869  [   96/  146]
train() client id: f_00005-7-3 loss: 0.505303  [  128/  146]
train() client id: f_00005-8-0 loss: 0.596713  [   32/  146]
train() client id: f_00005-8-1 loss: 0.701104  [   64/  146]
train() client id: f_00005-8-2 loss: 0.539976  [   96/  146]
train() client id: f_00005-8-3 loss: 0.865059  [  128/  146]
train() client id: f_00005-9-0 loss: 0.627191  [   32/  146]
train() client id: f_00005-9-1 loss: 0.714146  [   64/  146]
train() client id: f_00005-9-2 loss: 0.575492  [   96/  146]
train() client id: f_00005-9-3 loss: 0.560919  [  128/  146]
train() client id: f_00005-10-0 loss: 0.420570  [   32/  146]
train() client id: f_00005-10-1 loss: 0.818515  [   64/  146]
train() client id: f_00005-10-2 loss: 0.751653  [   96/  146]
train() client id: f_00005-10-3 loss: 0.726346  [  128/  146]
train() client id: f_00006-0-0 loss: 0.515051  [   32/   54]
train() client id: f_00006-1-0 loss: 0.499390  [   32/   54]
train() client id: f_00006-2-0 loss: 0.487526  [   32/   54]
train() client id: f_00006-3-0 loss: 0.426423  [   32/   54]
train() client id: f_00006-4-0 loss: 0.442274  [   32/   54]
train() client id: f_00006-5-0 loss: 0.403775  [   32/   54]
train() client id: f_00006-6-0 loss: 0.449727  [   32/   54]
train() client id: f_00006-7-0 loss: 0.438423  [   32/   54]
train() client id: f_00006-8-0 loss: 0.447368  [   32/   54]
train() client id: f_00006-9-0 loss: 0.495654  [   32/   54]
train() client id: f_00006-10-0 loss: 0.464228  [   32/   54]
train() client id: f_00007-0-0 loss: 0.754430  [   32/  179]
train() client id: f_00007-0-1 loss: 0.578513  [   64/  179]
train() client id: f_00007-0-2 loss: 0.703580  [   96/  179]
train() client id: f_00007-0-3 loss: 0.604600  [  128/  179]
train() client id: f_00007-0-4 loss: 0.689010  [  160/  179]
train() client id: f_00007-1-0 loss: 0.468428  [   32/  179]
train() client id: f_00007-1-1 loss: 0.626818  [   64/  179]
train() client id: f_00007-1-2 loss: 0.676777  [   96/  179]
train() client id: f_00007-1-3 loss: 0.661718  [  128/  179]
train() client id: f_00007-1-4 loss: 0.695598  [  160/  179]
train() client id: f_00007-2-0 loss: 0.675038  [   32/  179]
train() client id: f_00007-2-1 loss: 0.562745  [   64/  179]
train() client id: f_00007-2-2 loss: 0.691619  [   96/  179]
train() client id: f_00007-2-3 loss: 0.546289  [  128/  179]
train() client id: f_00007-2-4 loss: 0.593516  [  160/  179]
train() client id: f_00007-3-0 loss: 0.479510  [   32/  179]
train() client id: f_00007-3-1 loss: 0.781441  [   64/  179]
train() client id: f_00007-3-2 loss: 0.773992  [   96/  179]
train() client id: f_00007-3-3 loss: 0.549749  [  128/  179]
train() client id: f_00007-3-4 loss: 0.595743  [  160/  179]
train() client id: f_00007-4-0 loss: 0.573465  [   32/  179]
train() client id: f_00007-4-1 loss: 0.758775  [   64/  179]
train() client id: f_00007-4-2 loss: 0.607530  [   96/  179]
train() client id: f_00007-4-3 loss: 0.520399  [  128/  179]
train() client id: f_00007-4-4 loss: 0.465939  [  160/  179]
train() client id: f_00007-5-0 loss: 0.596256  [   32/  179]
train() client id: f_00007-5-1 loss: 0.563117  [   64/  179]
train() client id: f_00007-5-2 loss: 0.497288  [   96/  179]
train() client id: f_00007-5-3 loss: 0.712041  [  128/  179]
train() client id: f_00007-5-4 loss: 0.653977  [  160/  179]
train() client id: f_00007-6-0 loss: 0.541413  [   32/  179]
train() client id: f_00007-6-1 loss: 0.584285  [   64/  179]
train() client id: f_00007-6-2 loss: 0.768213  [   96/  179]
train() client id: f_00007-6-3 loss: 0.455003  [  128/  179]
train() client id: f_00007-6-4 loss: 0.548456  [  160/  179]
train() client id: f_00007-7-0 loss: 0.504011  [   32/  179]
train() client id: f_00007-7-1 loss: 0.775538  [   64/  179]
train() client id: f_00007-7-2 loss: 0.533018  [   96/  179]
train() client id: f_00007-7-3 loss: 0.630916  [  128/  179]
train() client id: f_00007-7-4 loss: 0.619111  [  160/  179]
train() client id: f_00007-8-0 loss: 0.585600  [   32/  179]
train() client id: f_00007-8-1 loss: 0.487148  [   64/  179]
train() client id: f_00007-8-2 loss: 0.560962  [   96/  179]
train() client id: f_00007-8-3 loss: 0.614900  [  128/  179]
train() client id: f_00007-8-4 loss: 0.621823  [  160/  179]
train() client id: f_00007-9-0 loss: 0.462122  [   32/  179]
train() client id: f_00007-9-1 loss: 1.006017  [   64/  179]
train() client id: f_00007-9-2 loss: 0.531646  [   96/  179]
train() client id: f_00007-9-3 loss: 0.549162  [  128/  179]
train() client id: f_00007-9-4 loss: 0.526831  [  160/  179]
train() client id: f_00007-10-0 loss: 0.598826  [   32/  179]
train() client id: f_00007-10-1 loss: 0.540062  [   64/  179]
train() client id: f_00007-10-2 loss: 0.731789  [   96/  179]
train() client id: f_00007-10-3 loss: 0.558121  [  128/  179]
train() client id: f_00007-10-4 loss: 0.622279  [  160/  179]
train() client id: f_00008-0-0 loss: 0.571495  [   32/  130]
train() client id: f_00008-0-1 loss: 0.702115  [   64/  130]
train() client id: f_00008-0-2 loss: 0.582693  [   96/  130]
train() client id: f_00008-0-3 loss: 0.584281  [  128/  130]
train() client id: f_00008-1-0 loss: 0.485080  [   32/  130]
train() client id: f_00008-1-1 loss: 0.696009  [   64/  130]
train() client id: f_00008-1-2 loss: 0.657534  [   96/  130]
train() client id: f_00008-1-3 loss: 0.565694  [  128/  130]
train() client id: f_00008-2-0 loss: 0.605502  [   32/  130]
train() client id: f_00008-2-1 loss: 0.620460  [   64/  130]
train() client id: f_00008-2-2 loss: 0.646866  [   96/  130]
train() client id: f_00008-2-3 loss: 0.558020  [  128/  130]
train() client id: f_00008-3-0 loss: 0.476722  [   32/  130]
train() client id: f_00008-3-1 loss: 0.616846  [   64/  130]
train() client id: f_00008-3-2 loss: 0.612426  [   96/  130]
train() client id: f_00008-3-3 loss: 0.712964  [  128/  130]
train() client id: f_00008-4-0 loss: 0.632198  [   32/  130]
train() client id: f_00008-4-1 loss: 0.558953  [   64/  130]
train() client id: f_00008-4-2 loss: 0.611985  [   96/  130]
train() client id: f_00008-4-3 loss: 0.626231  [  128/  130]
train() client id: f_00008-5-0 loss: 0.641423  [   32/  130]
train() client id: f_00008-5-1 loss: 0.674658  [   64/  130]
train() client id: f_00008-5-2 loss: 0.578969  [   96/  130]
train() client id: f_00008-5-3 loss: 0.527236  [  128/  130]
train() client id: f_00008-6-0 loss: 0.608931  [   32/  130]
train() client id: f_00008-6-1 loss: 0.590329  [   64/  130]
train() client id: f_00008-6-2 loss: 0.580598  [   96/  130]
train() client id: f_00008-6-3 loss: 0.659243  [  128/  130]
train() client id: f_00008-7-0 loss: 0.555091  [   32/  130]
train() client id: f_00008-7-1 loss: 0.541147  [   64/  130]
train() client id: f_00008-7-2 loss: 0.684781  [   96/  130]
train() client id: f_00008-7-3 loss: 0.669182  [  128/  130]
train() client id: f_00008-8-0 loss: 0.622081  [   32/  130]
train() client id: f_00008-8-1 loss: 0.575781  [   64/  130]
train() client id: f_00008-8-2 loss: 0.616672  [   96/  130]
train() client id: f_00008-8-3 loss: 0.588500  [  128/  130]
train() client id: f_00008-9-0 loss: 0.740864  [   32/  130]
train() client id: f_00008-9-1 loss: 0.637864  [   64/  130]
train() client id: f_00008-9-2 loss: 0.529102  [   96/  130]
train() client id: f_00008-9-3 loss: 0.531055  [  128/  130]
train() client id: f_00008-10-0 loss: 0.523636  [   32/  130]
train() client id: f_00008-10-1 loss: 0.658008  [   64/  130]
train() client id: f_00008-10-2 loss: 0.495274  [   96/  130]
train() client id: f_00008-10-3 loss: 0.754262  [  128/  130]
train() client id: f_00009-0-0 loss: 1.080850  [   32/  118]
train() client id: f_00009-0-1 loss: 1.141523  [   64/  118]
train() client id: f_00009-0-2 loss: 1.008905  [   96/  118]
train() client id: f_00009-1-0 loss: 0.984576  [   32/  118]
train() client id: f_00009-1-1 loss: 1.192612  [   64/  118]
train() client id: f_00009-1-2 loss: 0.970813  [   96/  118]
train() client id: f_00009-2-0 loss: 0.951758  [   32/  118]
train() client id: f_00009-2-1 loss: 0.948441  [   64/  118]
train() client id: f_00009-2-2 loss: 1.045929  [   96/  118]
train() client id: f_00009-3-0 loss: 0.868651  [   32/  118]
train() client id: f_00009-3-1 loss: 0.968187  [   64/  118]
train() client id: f_00009-3-2 loss: 1.021921  [   96/  118]
train() client id: f_00009-4-0 loss: 0.926396  [   32/  118]
train() client id: f_00009-4-1 loss: 0.863761  [   64/  118]
train() client id: f_00009-4-2 loss: 0.882476  [   96/  118]
train() client id: f_00009-5-0 loss: 1.007289  [   32/  118]
train() client id: f_00009-5-1 loss: 0.890941  [   64/  118]
train() client id: f_00009-5-2 loss: 0.803035  [   96/  118]
train() client id: f_00009-6-0 loss: 1.072385  [   32/  118]
train() client id: f_00009-6-1 loss: 0.774532  [   64/  118]
train() client id: f_00009-6-2 loss: 0.851934  [   96/  118]
train() client id: f_00009-7-0 loss: 0.924920  [   32/  118]
train() client id: f_00009-7-1 loss: 0.864751  [   64/  118]
train() client id: f_00009-7-2 loss: 0.945889  [   96/  118]
train() client id: f_00009-8-0 loss: 0.916087  [   32/  118]
train() client id: f_00009-8-1 loss: 0.926348  [   64/  118]
train() client id: f_00009-8-2 loss: 0.708877  [   96/  118]
train() client id: f_00009-9-0 loss: 0.782640  [   32/  118]
train() client id: f_00009-9-1 loss: 0.751652  [   64/  118]
train() client id: f_00009-9-2 loss: 0.915601  [   96/  118]
train() client id: f_00009-10-0 loss: 0.838834  [   32/  118]
train() client id: f_00009-10-1 loss: 1.030584  [   64/  118]
train() client id: f_00009-10-2 loss: 0.809384  [   96/  118]
At round 51 accuracy: 0.6472148541114059
At round 51 training accuracy: 0.5915492957746479
At round 51 training loss: 0.8242653359513061
update_location
xs = [  -3.9056584     4.20031788  275.00902392   18.81129433    0.97929623
    3.95640986 -237.44319194 -216.32485185  259.66397685 -202.06087855]
ys = [ 267.5879595   250.55583871    1.32061395 -237.45517586  229.35018685
  212.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [285.68963971 269.80709958 292.62895834 258.33858662 250.20485053
 235.1712481  257.65511821 238.32145892 278.80826897 225.48749521]
dists_bs = [194.53998039 194.4773475  482.36153531 455.72150476 184.18104004
 182.90791844 188.28113892 179.01284267 462.29567447 173.12649821]
uav_gains = [3.55494025e-12 4.95127103e-12 3.08824037e-12 6.27981480e-12
 7.39020348e-12 9.78653336e-12 6.36780404e-12 9.24954034e-12
 4.10015262e-12 1.15466699e-11]
bs_gains = [4.30595271e-11 4.30983677e-11 3.38727622e-12 3.97132493e-12
 5.01889727e-11 5.11732577e-11 4.71883724e-11 5.43523537e-11
 3.81521100e-12 5.96865065e-11]
Round 52
-------------------------------
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.13977177 8.44962292 4.08581891 1.48801359 9.74245982 4.68747181
 1.834924   5.76952797 4.26183039 3.80061293]
obj_prev = 48.26005410532363
eta_min = 5.2478248998997527e-23	eta_max = 0.9386552346452276
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 11.138694899951254	eta = 0.9090909090909091
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 23.212442703241933	eta = 0.43623527270004686
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 16.931595727685714	eta = 0.598058590314986
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.799778450901695	eta = 0.6409005230136667
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733820172197483	eta = 0.6435872637324471
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733571912722388	eta = 0.6435974188731335
eta = 0.6435974188731335
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.03636401 0.07647987 0.03578681 0.01240994 0.0883126  0.04213608
 0.01558458 0.05165999 0.03751843 0.03405519]
ene_total = [1.50031121 2.44142183 1.50915709 0.72099371 2.77897583 1.43455086
 0.8117007  1.8214624  1.52193281 1.19306548]
ti_comp = [0.78968755 0.8674062  0.78087534 0.81798185 0.86972334 0.87000865
 0.81855413 0.83227438 0.79380079 0.87219318]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [4.81930945e-06 3.71600544e-05 4.69770128e-06 1.78526131e-07
 5.69096341e-05 6.17725006e-06 3.53077655e-07 1.24396969e-05
 5.23830472e-06 3.24492313e-06]
ene_total = [0.44237055 0.21717421 0.46800782 0.35990781 0.2110067  0.20870035
 0.35824773 0.31867769 0.43041446 0.20225874]
optimize_network iter = 0 obj = 3.2167660550334363
eta = 0.6435974188731335
freqs = [23024303.73895735 44085381.74515043 22914544.46834145  7585706.59027375
 50770513.42376783 24215895.58340987  9519576.07846752 31035429.18235327
 23632141.3100814  19522731.20042242]
eta_min = 0.6435974188731354	eta_max = 0.698454705717541
af = 0.003087860896766924	bf = 1.1451029605675662	zeta = 0.003396646986443617	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.04231482e-06 8.03693475e-06 1.01601355e-06 3.86114307e-08
 1.23083516e-05 1.33600869e-06 7.63632379e-08 2.69044364e-06
 1.13293465e-06 7.01808322e-07]
ene_total = [1.7338512  0.84809379 1.83437118 1.41097682 0.82214886 0.81764254
 1.40445301 1.24824095 1.68694076 0.79265082]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 1 obj = 3.8009831688814426
eta = 0.698454705717541
freqs = [22965892.7521965  43105303.11724808 22914544.46834147  7508078.8429111
 49615295.72405244 23663342.1290412   9420739.8305015  30604647.12122058
 23544761.66323882 19067718.62692055]
eta_min = 0.698454705717542	eta_max = 0.6984547057175405
af = 0.002969273754740996	bf = 1.1451029605675662	zeta = 0.003266201130215096	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.03703298e-06 7.68356267e-06 1.01601355e-06 3.78252199e-08
 1.17546026e-05 1.27573473e-06 7.47857989e-08 2.61627349e-06
 1.12457210e-06 6.69475724e-07]
ene_total = [1.73385059 0.84805348 1.83437118 1.41097673 0.8220857  0.81763566
 1.40445283 1.24823249 1.68693981 0.79264713]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 2 obj = 3.800983168881437
eta = 0.6984547057175405
freqs = [22965892.7521965  43105303.11724809 22914544.46834146  7508078.8429111
 49615295.72405244 23663342.12904121  9420739.8305015  30604647.12122059
 23544761.66323882 19067718.62692055]
Done!
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.65510535e-06 2.70813288e-05 3.58102069e-06 1.33318001e-07
 4.14300335e-05 4.49642870e-06 2.63588507e-07 9.22126437e-06
 3.96364398e-06 2.35962053e-06]
ene_total = [0.01520218 0.00745374 0.01608333 0.01236923 0.00723638 0.00717091
 0.01231213 0.01094906 0.01479116 0.00695032]
At round 52 energy consumption: 0.11051844173128197
At round 52 eta: 0.6984547057175405
At round 52 a_n: 10.370218813849878
At round 52 local rounds: 11.751716534008594
At round 52 global rounds: 34.39025251090811
gradient difference: 0.5681652426719666
train() client id: f_00000-0-0 loss: 0.896701  [   32/  126]
train() client id: f_00000-0-1 loss: 1.180451  [   64/  126]
train() client id: f_00000-0-2 loss: 1.050250  [   96/  126]
train() client id: f_00000-1-0 loss: 0.943517  [   32/  126]
train() client id: f_00000-1-1 loss: 0.871717  [   64/  126]
train() client id: f_00000-1-2 loss: 0.989177  [   96/  126]
train() client id: f_00000-2-0 loss: 0.892264  [   32/  126]
train() client id: f_00000-2-1 loss: 0.931555  [   64/  126]
train() client id: f_00000-2-2 loss: 0.876623  [   96/  126]
train() client id: f_00000-3-0 loss: 0.899587  [   32/  126]
train() client id: f_00000-3-1 loss: 0.853950  [   64/  126]
train() client id: f_00000-3-2 loss: 0.861244  [   96/  126]
train() client id: f_00000-4-0 loss: 0.865470  [   32/  126]
train() client id: f_00000-4-1 loss: 0.858669  [   64/  126]
train() client id: f_00000-4-2 loss: 0.873086  [   96/  126]
train() client id: f_00000-5-0 loss: 0.888018  [   32/  126]
train() client id: f_00000-5-1 loss: 0.833781  [   64/  126]
train() client id: f_00000-5-2 loss: 0.724144  [   96/  126]
train() client id: f_00000-6-0 loss: 0.830405  [   32/  126]
train() client id: f_00000-6-1 loss: 0.752425  [   64/  126]
train() client id: f_00000-6-2 loss: 0.929691  [   96/  126]
train() client id: f_00000-7-0 loss: 0.818337  [   32/  126]
train() client id: f_00000-7-1 loss: 0.742594  [   64/  126]
train() client id: f_00000-7-2 loss: 0.859479  [   96/  126]
train() client id: f_00000-8-0 loss: 0.822688  [   32/  126]
train() client id: f_00000-8-1 loss: 0.857105  [   64/  126]
train() client id: f_00000-8-2 loss: 0.760868  [   96/  126]
train() client id: f_00000-9-0 loss: 0.711877  [   32/  126]
train() client id: f_00000-9-1 loss: 0.808621  [   64/  126]
train() client id: f_00000-9-2 loss: 0.930794  [   96/  126]
train() client id: f_00000-10-0 loss: 0.818729  [   32/  126]
train() client id: f_00000-10-1 loss: 0.848911  [   64/  126]
train() client id: f_00000-10-2 loss: 0.809026  [   96/  126]
train() client id: f_00001-0-0 loss: 0.378896  [   32/  265]
train() client id: f_00001-0-1 loss: 0.432394  [   64/  265]
train() client id: f_00001-0-2 loss: 0.504507  [   96/  265]
train() client id: f_00001-0-3 loss: 0.644749  [  128/  265]
train() client id: f_00001-0-4 loss: 0.596271  [  160/  265]
train() client id: f_00001-0-5 loss: 0.459590  [  192/  265]
train() client id: f_00001-0-6 loss: 0.440558  [  224/  265]
train() client id: f_00001-0-7 loss: 0.470850  [  256/  265]
train() client id: f_00001-1-0 loss: 0.483935  [   32/  265]
train() client id: f_00001-1-1 loss: 0.509661  [   64/  265]
train() client id: f_00001-1-2 loss: 0.426856  [   96/  265]
train() client id: f_00001-1-3 loss: 0.569570  [  128/  265]
train() client id: f_00001-1-4 loss: 0.407064  [  160/  265]
train() client id: f_00001-1-5 loss: 0.475477  [  192/  265]
train() client id: f_00001-1-6 loss: 0.457888  [  224/  265]
train() client id: f_00001-1-7 loss: 0.564528  [  256/  265]
train() client id: f_00001-2-0 loss: 0.518914  [   32/  265]
train() client id: f_00001-2-1 loss: 0.412124  [   64/  265]
train() client id: f_00001-2-2 loss: 0.374749  [   96/  265]
train() client id: f_00001-2-3 loss: 0.386652  [  128/  265]
train() client id: f_00001-2-4 loss: 0.520476  [  160/  265]
train() client id: f_00001-2-5 loss: 0.506345  [  192/  265]
train() client id: f_00001-2-6 loss: 0.531717  [  224/  265]
train() client id: f_00001-2-7 loss: 0.619811  [  256/  265]
train() client id: f_00001-3-0 loss: 0.452908  [   32/  265]
train() client id: f_00001-3-1 loss: 0.430173  [   64/  265]
train() client id: f_00001-3-2 loss: 0.450529  [   96/  265]
train() client id: f_00001-3-3 loss: 0.432413  [  128/  265]
train() client id: f_00001-3-4 loss: 0.546813  [  160/  265]
train() client id: f_00001-3-5 loss: 0.373050  [  192/  265]
train() client id: f_00001-3-6 loss: 0.551822  [  224/  265]
train() client id: f_00001-3-7 loss: 0.580336  [  256/  265]
train() client id: f_00001-4-0 loss: 0.605258  [   32/  265]
train() client id: f_00001-4-1 loss: 0.451081  [   64/  265]
train() client id: f_00001-4-2 loss: 0.365410  [   96/  265]
train() client id: f_00001-4-3 loss: 0.441730  [  128/  265]
train() client id: f_00001-4-4 loss: 0.412369  [  160/  265]
train() client id: f_00001-4-5 loss: 0.508117  [  192/  265]
train() client id: f_00001-4-6 loss: 0.495288  [  224/  265]
train() client id: f_00001-4-7 loss: 0.467514  [  256/  265]
train() client id: f_00001-5-0 loss: 0.363003  [   32/  265]
train() client id: f_00001-5-1 loss: 0.435743  [   64/  265]
train() client id: f_00001-5-2 loss: 0.460949  [   96/  265]
train() client id: f_00001-5-3 loss: 0.451966  [  128/  265]
train() client id: f_00001-5-4 loss: 0.369101  [  160/  265]
train() client id: f_00001-5-5 loss: 0.589058  [  192/  265]
train() client id: f_00001-5-6 loss: 0.454503  [  224/  265]
train() client id: f_00001-5-7 loss: 0.605952  [  256/  265]
train() client id: f_00001-6-0 loss: 0.407645  [   32/  265]
train() client id: f_00001-6-1 loss: 0.518072  [   64/  265]
train() client id: f_00001-6-2 loss: 0.424531  [   96/  265]
train() client id: f_00001-6-3 loss: 0.625051  [  128/  265]
train() client id: f_00001-6-4 loss: 0.385863  [  160/  265]
train() client id: f_00001-6-5 loss: 0.453346  [  192/  265]
train() client id: f_00001-6-6 loss: 0.523848  [  224/  265]
train() client id: f_00001-6-7 loss: 0.443437  [  256/  265]
train() client id: f_00001-7-0 loss: 0.528398  [   32/  265]
train() client id: f_00001-7-1 loss: 0.409792  [   64/  265]
train() client id: f_00001-7-2 loss: 0.563563  [   96/  265]
train() client id: f_00001-7-3 loss: 0.477494  [  128/  265]
train() client id: f_00001-7-4 loss: 0.465495  [  160/  265]
train() client id: f_00001-7-5 loss: 0.369448  [  192/  265]
train() client id: f_00001-7-6 loss: 0.451132  [  224/  265]
train() client id: f_00001-7-7 loss: 0.443795  [  256/  265]
train() client id: f_00001-8-0 loss: 0.382034  [   32/  265]
train() client id: f_00001-8-1 loss: 0.496318  [   64/  265]
train() client id: f_00001-8-2 loss: 0.454230  [   96/  265]
train() client id: f_00001-8-3 loss: 0.662638  [  128/  265]
train() client id: f_00001-8-4 loss: 0.402747  [  160/  265]
train() client id: f_00001-8-5 loss: 0.385210  [  192/  265]
train() client id: f_00001-8-6 loss: 0.541247  [  224/  265]
train() client id: f_00001-8-7 loss: 0.438089  [  256/  265]
train() client id: f_00001-9-0 loss: 0.391984  [   32/  265]
train() client id: f_00001-9-1 loss: 0.619045  [   64/  265]
train() client id: f_00001-9-2 loss: 0.518622  [   96/  265]
train() client id: f_00001-9-3 loss: 0.444298  [  128/  265]
train() client id: f_00001-9-4 loss: 0.452959  [  160/  265]
train() client id: f_00001-9-5 loss: 0.360272  [  192/  265]
train() client id: f_00001-9-6 loss: 0.479751  [  224/  265]
train() client id: f_00001-9-7 loss: 0.498219  [  256/  265]
train() client id: f_00001-10-0 loss: 0.432545  [   32/  265]
train() client id: f_00001-10-1 loss: 0.408586  [   64/  265]
train() client id: f_00001-10-2 loss: 0.439026  [   96/  265]
train() client id: f_00001-10-3 loss: 0.547960  [  128/  265]
train() client id: f_00001-10-4 loss: 0.380160  [  160/  265]
train() client id: f_00001-10-5 loss: 0.549966  [  192/  265]
train() client id: f_00001-10-6 loss: 0.482619  [  224/  265]
train() client id: f_00001-10-7 loss: 0.531578  [  256/  265]
train() client id: f_00002-0-0 loss: 1.071823  [   32/  124]
train() client id: f_00002-0-1 loss: 0.861629  [   64/  124]
train() client id: f_00002-0-2 loss: 1.068448  [   96/  124]
train() client id: f_00002-1-0 loss: 1.123359  [   32/  124]
train() client id: f_00002-1-1 loss: 0.848061  [   64/  124]
train() client id: f_00002-1-2 loss: 1.014441  [   96/  124]
train() client id: f_00002-2-0 loss: 1.104385  [   32/  124]
train() client id: f_00002-2-1 loss: 0.943951  [   64/  124]
train() client id: f_00002-2-2 loss: 0.858195  [   96/  124]
train() client id: f_00002-3-0 loss: 0.964169  [   32/  124]
train() client id: f_00002-3-1 loss: 1.132534  [   64/  124]
train() client id: f_00002-3-2 loss: 0.786700  [   96/  124]
train() client id: f_00002-4-0 loss: 1.036981  [   32/  124]
train() client id: f_00002-4-1 loss: 1.015569  [   64/  124]
train() client id: f_00002-4-2 loss: 0.836623  [   96/  124]
train() client id: f_00002-5-0 loss: 0.984951  [   32/  124]
train() client id: f_00002-5-1 loss: 0.981886  [   64/  124]
train() client id: f_00002-5-2 loss: 0.680953  [   96/  124]
train() client id: f_00002-6-0 loss: 0.928814  [   32/  124]
train() client id: f_00002-6-1 loss: 0.920733  [   64/  124]
train() client id: f_00002-6-2 loss: 0.818577  [   96/  124]
train() client id: f_00002-7-0 loss: 0.893585  [   32/  124]
train() client id: f_00002-7-1 loss: 0.992767  [   64/  124]
train() client id: f_00002-7-2 loss: 0.836672  [   96/  124]
train() client id: f_00002-8-0 loss: 0.709835  [   32/  124]
train() client id: f_00002-8-1 loss: 0.985577  [   64/  124]
train() client id: f_00002-8-2 loss: 0.853141  [   96/  124]
train() client id: f_00002-9-0 loss: 0.998363  [   32/  124]
train() client id: f_00002-9-1 loss: 0.884205  [   64/  124]
train() client id: f_00002-9-2 loss: 0.907222  [   96/  124]
train() client id: f_00002-10-0 loss: 0.888004  [   32/  124]
train() client id: f_00002-10-1 loss: 0.916379  [   64/  124]
train() client id: f_00002-10-2 loss: 0.903197  [   96/  124]
train() client id: f_00003-0-0 loss: 0.606290  [   32/   43]
train() client id: f_00003-1-0 loss: 0.637611  [   32/   43]
train() client id: f_00003-2-0 loss: 0.578922  [   32/   43]
train() client id: f_00003-3-0 loss: 0.578906  [   32/   43]
train() client id: f_00003-4-0 loss: 0.651590  [   32/   43]
train() client id: f_00003-5-0 loss: 0.638237  [   32/   43]
train() client id: f_00003-6-0 loss: 0.621599  [   32/   43]
train() client id: f_00003-7-0 loss: 0.574662  [   32/   43]
train() client id: f_00003-8-0 loss: 0.646955  [   32/   43]
train() client id: f_00003-9-0 loss: 0.607368  [   32/   43]
train() client id: f_00003-10-0 loss: 0.824396  [   32/   43]
train() client id: f_00004-0-0 loss: 0.708253  [   32/  306]
train() client id: f_00004-0-1 loss: 0.682215  [   64/  306]
train() client id: f_00004-0-2 loss: 0.748136  [   96/  306]
train() client id: f_00004-0-3 loss: 0.617952  [  128/  306]
train() client id: f_00004-0-4 loss: 0.850542  [  160/  306]
train() client id: f_00004-0-5 loss: 0.658313  [  192/  306]
train() client id: f_00004-0-6 loss: 0.803530  [  224/  306]
train() client id: f_00004-0-7 loss: 0.667050  [  256/  306]
train() client id: f_00004-0-8 loss: 0.617251  [  288/  306]
train() client id: f_00004-1-0 loss: 0.596507  [   32/  306]
train() client id: f_00004-1-1 loss: 0.675702  [   64/  306]
train() client id: f_00004-1-2 loss: 0.852385  [   96/  306]
train() client id: f_00004-1-3 loss: 0.626099  [  128/  306]
train() client id: f_00004-1-4 loss: 0.649586  [  160/  306]
train() client id: f_00004-1-5 loss: 0.905118  [  192/  306]
train() client id: f_00004-1-6 loss: 0.733320  [  224/  306]
train() client id: f_00004-1-7 loss: 0.681275  [  256/  306]
train() client id: f_00004-1-8 loss: 0.648859  [  288/  306]
train() client id: f_00004-2-0 loss: 0.787469  [   32/  306]
train() client id: f_00004-2-1 loss: 0.457727  [   64/  306]
train() client id: f_00004-2-2 loss: 0.758964  [   96/  306]
train() client id: f_00004-2-3 loss: 0.756986  [  128/  306]
train() client id: f_00004-2-4 loss: 0.707771  [  160/  306]
train() client id: f_00004-2-5 loss: 0.686127  [  192/  306]
train() client id: f_00004-2-6 loss: 0.751553  [  224/  306]
train() client id: f_00004-2-7 loss: 0.768497  [  256/  306]
train() client id: f_00004-2-8 loss: 0.713732  [  288/  306]
train() client id: f_00004-3-0 loss: 0.625851  [   32/  306]
train() client id: f_00004-3-1 loss: 0.849900  [   64/  306]
train() client id: f_00004-3-2 loss: 0.673649  [   96/  306]
train() client id: f_00004-3-3 loss: 0.756360  [  128/  306]
train() client id: f_00004-3-4 loss: 0.647470  [  160/  306]
train() client id: f_00004-3-5 loss: 0.651740  [  192/  306]
train() client id: f_00004-3-6 loss: 0.683080  [  224/  306]
train() client id: f_00004-3-7 loss: 0.704331  [  256/  306]
train() client id: f_00004-3-8 loss: 0.689593  [  288/  306]
train() client id: f_00004-4-0 loss: 0.714943  [   32/  306]
train() client id: f_00004-4-1 loss: 0.666730  [   64/  306]
train() client id: f_00004-4-2 loss: 0.809606  [   96/  306]
train() client id: f_00004-4-3 loss: 0.744691  [  128/  306]
train() client id: f_00004-4-4 loss: 0.763573  [  160/  306]
train() client id: f_00004-4-5 loss: 0.613918  [  192/  306]
train() client id: f_00004-4-6 loss: 0.715787  [  224/  306]
train() client id: f_00004-4-7 loss: 0.662736  [  256/  306]
train() client id: f_00004-4-8 loss: 0.725366  [  288/  306]
train() client id: f_00004-5-0 loss: 0.562318  [   32/  306]
train() client id: f_00004-5-1 loss: 0.790441  [   64/  306]
train() client id: f_00004-5-2 loss: 0.816330  [   96/  306]
train() client id: f_00004-5-3 loss: 0.730186  [  128/  306]
train() client id: f_00004-5-4 loss: 0.761850  [  160/  306]
train() client id: f_00004-5-5 loss: 0.643643  [  192/  306]
train() client id: f_00004-5-6 loss: 0.724243  [  224/  306]
train() client id: f_00004-5-7 loss: 0.759312  [  256/  306]
train() client id: f_00004-5-8 loss: 0.609619  [  288/  306]
train() client id: f_00004-6-0 loss: 0.748039  [   32/  306]
train() client id: f_00004-6-1 loss: 0.742686  [   64/  306]
train() client id: f_00004-6-2 loss: 0.677561  [   96/  306]
train() client id: f_00004-6-3 loss: 0.729143  [  128/  306]
train() client id: f_00004-6-4 loss: 0.652552  [  160/  306]
train() client id: f_00004-6-5 loss: 0.718696  [  192/  306]
train() client id: f_00004-6-6 loss: 0.773669  [  224/  306]
train() client id: f_00004-6-7 loss: 0.873286  [  256/  306]
train() client id: f_00004-6-8 loss: 0.597647  [  288/  306]
train() client id: f_00004-7-0 loss: 0.769565  [   32/  306]
train() client id: f_00004-7-1 loss: 0.881551  [   64/  306]
train() client id: f_00004-7-2 loss: 0.574291  [   96/  306]
train() client id: f_00004-7-3 loss: 0.732518  [  128/  306]
train() client id: f_00004-7-4 loss: 0.620320  [  160/  306]
train() client id: f_00004-7-5 loss: 0.743010  [  192/  306]
train() client id: f_00004-7-6 loss: 0.650535  [  224/  306]
train() client id: f_00004-7-7 loss: 0.832263  [  256/  306]
train() client id: f_00004-7-8 loss: 0.661880  [  288/  306]
train() client id: f_00004-8-0 loss: 0.739381  [   32/  306]
train() client id: f_00004-8-1 loss: 0.686539  [   64/  306]
train() client id: f_00004-8-2 loss: 0.721664  [   96/  306]
train() client id: f_00004-8-3 loss: 0.796966  [  128/  306]
train() client id: f_00004-8-4 loss: 0.771875  [  160/  306]
train() client id: f_00004-8-5 loss: 0.670634  [  192/  306]
train() client id: f_00004-8-6 loss: 0.654409  [  224/  306]
train() client id: f_00004-8-7 loss: 0.794995  [  256/  306]
train() client id: f_00004-8-8 loss: 0.711150  [  288/  306]
train() client id: f_00004-9-0 loss: 0.739947  [   32/  306]
train() client id: f_00004-9-1 loss: 0.816191  [   64/  306]
train() client id: f_00004-9-2 loss: 0.559194  [   96/  306]
train() client id: f_00004-9-3 loss: 0.726923  [  128/  306]
train() client id: f_00004-9-4 loss: 0.740233  [  160/  306]
train() client id: f_00004-9-5 loss: 0.798877  [  192/  306]
train() client id: f_00004-9-6 loss: 0.701347  [  224/  306]
train() client id: f_00004-9-7 loss: 0.662966  [  256/  306]
train() client id: f_00004-9-8 loss: 0.666883  [  288/  306]
train() client id: f_00004-10-0 loss: 0.749168  [   32/  306]
train() client id: f_00004-10-1 loss: 0.688725  [   64/  306]
train() client id: f_00004-10-2 loss: 0.729238  [   96/  306]
train() client id: f_00004-10-3 loss: 0.718695  [  128/  306]
train() client id: f_00004-10-4 loss: 0.666700  [  160/  306]
train() client id: f_00004-10-5 loss: 0.635269  [  192/  306]
train() client id: f_00004-10-6 loss: 0.848072  [  224/  306]
train() client id: f_00004-10-7 loss: 0.652972  [  256/  306]
train() client id: f_00004-10-8 loss: 0.759332  [  288/  306]
train() client id: f_00005-0-0 loss: 0.650308  [   32/  146]
train() client id: f_00005-0-1 loss: 0.589318  [   64/  146]
train() client id: f_00005-0-2 loss: 0.543865  [   96/  146]
train() client id: f_00005-0-3 loss: 0.522917  [  128/  146]
train() client id: f_00005-1-0 loss: 0.550021  [   32/  146]
train() client id: f_00005-1-1 loss: 0.524929  [   64/  146]
train() client id: f_00005-1-2 loss: 0.487206  [   96/  146]
train() client id: f_00005-1-3 loss: 0.724406  [  128/  146]
train() client id: f_00005-2-0 loss: 0.331470  [   32/  146]
train() client id: f_00005-2-1 loss: 0.599009  [   64/  146]
train() client id: f_00005-2-2 loss: 0.715191  [   96/  146]
train() client id: f_00005-2-3 loss: 0.602068  [  128/  146]
train() client id: f_00005-3-0 loss: 0.645227  [   32/  146]
train() client id: f_00005-3-1 loss: 0.650891  [   64/  146]
train() client id: f_00005-3-2 loss: 0.486073  [   96/  146]
train() client id: f_00005-3-3 loss: 0.563973  [  128/  146]
train() client id: f_00005-4-0 loss: 0.574468  [   32/  146]
train() client id: f_00005-4-1 loss: 0.388956  [   64/  146]
train() client id: f_00005-4-2 loss: 0.488881  [   96/  146]
train() client id: f_00005-4-3 loss: 0.821742  [  128/  146]
train() client id: f_00005-5-0 loss: 0.460476  [   32/  146]
train() client id: f_00005-5-1 loss: 0.576552  [   64/  146]
train() client id: f_00005-5-2 loss: 0.666540  [   96/  146]
train() client id: f_00005-5-3 loss: 0.532568  [  128/  146]
train() client id: f_00005-6-0 loss: 0.342153  [   32/  146]
train() client id: f_00005-6-1 loss: 0.568919  [   64/  146]
train() client id: f_00005-6-2 loss: 0.589644  [   96/  146]
train() client id: f_00005-6-3 loss: 0.538879  [  128/  146]
train() client id: f_00005-7-0 loss: 0.383353  [   32/  146]
train() client id: f_00005-7-1 loss: 0.667300  [   64/  146]
train() client id: f_00005-7-2 loss: 0.517235  [   96/  146]
train() client id: f_00005-7-3 loss: 0.563379  [  128/  146]
train() client id: f_00005-8-0 loss: 0.339186  [   32/  146]
train() client id: f_00005-8-1 loss: 0.827922  [   64/  146]
train() client id: f_00005-8-2 loss: 0.552048  [   96/  146]
train() client id: f_00005-8-3 loss: 0.514430  [  128/  146]
train() client id: f_00005-9-0 loss: 0.785773  [   32/  146]
train() client id: f_00005-9-1 loss: 0.537100  [   64/  146]
train() client id: f_00005-9-2 loss: 0.191906  [   96/  146]
train() client id: f_00005-9-3 loss: 0.595947  [  128/  146]
train() client id: f_00005-10-0 loss: 0.374928  [   32/  146]
train() client id: f_00005-10-1 loss: 0.886527  [   64/  146]
train() client id: f_00005-10-2 loss: 0.796020  [   96/  146]
train() client id: f_00005-10-3 loss: 0.320322  [  128/  146]
train() client id: f_00006-0-0 loss: 0.482444  [   32/   54]
train() client id: f_00006-1-0 loss: 0.580344  [   32/   54]
train() client id: f_00006-2-0 loss: 0.483727  [   32/   54]
train() client id: f_00006-3-0 loss: 0.473268  [   32/   54]
train() client id: f_00006-4-0 loss: 0.555803  [   32/   54]
train() client id: f_00006-5-0 loss: 0.466973  [   32/   54]
train() client id: f_00006-6-0 loss: 0.538980  [   32/   54]
train() client id: f_00006-7-0 loss: 0.545233  [   32/   54]
train() client id: f_00006-8-0 loss: 0.471388  [   32/   54]
train() client id: f_00006-9-0 loss: 0.567781  [   32/   54]
train() client id: f_00006-10-0 loss: 0.579309  [   32/   54]
train() client id: f_00007-0-0 loss: 0.520334  [   32/  179]
train() client id: f_00007-0-1 loss: 0.579510  [   64/  179]
train() client id: f_00007-0-2 loss: 0.521815  [   96/  179]
train() client id: f_00007-0-3 loss: 0.666966  [  128/  179]
train() client id: f_00007-0-4 loss: 0.389465  [  160/  179]
train() client id: f_00007-1-0 loss: 0.662720  [   32/  179]
train() client id: f_00007-1-1 loss: 0.504195  [   64/  179]
train() client id: f_00007-1-2 loss: 0.620580  [   96/  179]
train() client id: f_00007-1-3 loss: 0.426924  [  128/  179]
train() client id: f_00007-1-4 loss: 0.593039  [  160/  179]
train() client id: f_00007-2-0 loss: 0.536012  [   32/  179]
train() client id: f_00007-2-1 loss: 0.725703  [   64/  179]
train() client id: f_00007-2-2 loss: 0.406060  [   96/  179]
train() client id: f_00007-2-3 loss: 0.416172  [  128/  179]
train() client id: f_00007-2-4 loss: 0.699401  [  160/  179]
train() client id: f_00007-3-0 loss: 0.489011  [   32/  179]
train() client id: f_00007-3-1 loss: 0.513452  [   64/  179]
train() client id: f_00007-3-2 loss: 0.444816  [   96/  179]
train() client id: f_00007-3-3 loss: 0.557605  [  128/  179]
train() client id: f_00007-3-4 loss: 0.461937  [  160/  179]
train() client id: f_00007-4-0 loss: 0.449548  [   32/  179]
train() client id: f_00007-4-1 loss: 0.655136  [   64/  179]
train() client id: f_00007-4-2 loss: 0.349204  [   96/  179]
train() client id: f_00007-4-3 loss: 0.778817  [  128/  179]
train() client id: f_00007-4-4 loss: 0.494801  [  160/  179]
train() client id: f_00007-5-0 loss: 0.751859  [   32/  179]
train() client id: f_00007-5-1 loss: 0.566702  [   64/  179]
train() client id: f_00007-5-2 loss: 0.340466  [   96/  179]
train() client id: f_00007-5-3 loss: 0.570548  [  128/  179]
train() client id: f_00007-5-4 loss: 0.344944  [  160/  179]
train() client id: f_00007-6-0 loss: 0.571715  [   32/  179]
train() client id: f_00007-6-1 loss: 0.554053  [   64/  179]
train() client id: f_00007-6-2 loss: 0.706520  [   96/  179]
train() client id: f_00007-6-3 loss: 0.398099  [  128/  179]
train() client id: f_00007-6-4 loss: 0.465747  [  160/  179]
train() client id: f_00007-7-0 loss: 0.611467  [   32/  179]
train() client id: f_00007-7-1 loss: 0.455842  [   64/  179]
train() client id: f_00007-7-2 loss: 0.683918  [   96/  179]
train() client id: f_00007-7-3 loss: 0.354471  [  128/  179]
train() client id: f_00007-7-4 loss: 0.610259  [  160/  179]
train() client id: f_00007-8-0 loss: 0.668489  [   32/  179]
train() client id: f_00007-8-1 loss: 0.600733  [   64/  179]
train() client id: f_00007-8-2 loss: 0.488643  [   96/  179]
train() client id: f_00007-8-3 loss: 0.406315  [  128/  179]
train() client id: f_00007-8-4 loss: 0.542958  [  160/  179]
train() client id: f_00007-9-0 loss: 0.414996  [   32/  179]
train() client id: f_00007-9-1 loss: 0.607197  [   64/  179]
train() client id: f_00007-9-2 loss: 0.525763  [   96/  179]
train() client id: f_00007-9-3 loss: 0.464671  [  128/  179]
train() client id: f_00007-9-4 loss: 0.481888  [  160/  179]
train() client id: f_00007-10-0 loss: 0.673653  [   32/  179]
train() client id: f_00007-10-1 loss: 0.423467  [   64/  179]
train() client id: f_00007-10-2 loss: 0.732689  [   96/  179]
train() client id: f_00007-10-3 loss: 0.362595  [  128/  179]
train() client id: f_00007-10-4 loss: 0.397625  [  160/  179]
train() client id: f_00008-0-0 loss: 0.690215  [   32/  130]
train() client id: f_00008-0-1 loss: 0.559562  [   64/  130]
train() client id: f_00008-0-2 loss: 0.604313  [   96/  130]
train() client id: f_00008-0-3 loss: 0.617158  [  128/  130]
train() client id: f_00008-1-0 loss: 0.635636  [   32/  130]
train() client id: f_00008-1-1 loss: 0.654411  [   64/  130]
train() client id: f_00008-1-2 loss: 0.698437  [   96/  130]
train() client id: f_00008-1-3 loss: 0.520656  [  128/  130]
train() client id: f_00008-2-0 loss: 0.631257  [   32/  130]
train() client id: f_00008-2-1 loss: 0.572093  [   64/  130]
train() client id: f_00008-2-2 loss: 0.698367  [   96/  130]
train() client id: f_00008-2-3 loss: 0.608650  [  128/  130]
train() client id: f_00008-3-0 loss: 0.629756  [   32/  130]
train() client id: f_00008-3-1 loss: 0.755005  [   64/  130]
train() client id: f_00008-3-2 loss: 0.505570  [   96/  130]
train() client id: f_00008-3-3 loss: 0.620846  [  128/  130]
train() client id: f_00008-4-0 loss: 0.703797  [   32/  130]
train() client id: f_00008-4-1 loss: 0.619133  [   64/  130]
train() client id: f_00008-4-2 loss: 0.586048  [   96/  130]
train() client id: f_00008-4-3 loss: 0.590019  [  128/  130]
train() client id: f_00008-5-0 loss: 0.621434  [   32/  130]
train() client id: f_00008-5-1 loss: 0.588573  [   64/  130]
train() client id: f_00008-5-2 loss: 0.613313  [   96/  130]
train() client id: f_00008-5-3 loss: 0.681541  [  128/  130]
train() client id: f_00008-6-0 loss: 0.619538  [   32/  130]
train() client id: f_00008-6-1 loss: 0.617017  [   64/  130]
train() client id: f_00008-6-2 loss: 0.615854  [   96/  130]
train() client id: f_00008-6-3 loss: 0.623911  [  128/  130]
train() client id: f_00008-7-0 loss: 0.570542  [   32/  130]
train() client id: f_00008-7-1 loss: 0.650490  [   64/  130]
train() client id: f_00008-7-2 loss: 0.639612  [   96/  130]
train() client id: f_00008-7-3 loss: 0.610655  [  128/  130]
train() client id: f_00008-8-0 loss: 0.553498  [   32/  130]
train() client id: f_00008-8-1 loss: 0.713235  [   64/  130]
train() client id: f_00008-8-2 loss: 0.598763  [   96/  130]
train() client id: f_00008-8-3 loss: 0.643560  [  128/  130]
train() client id: f_00008-9-0 loss: 0.617901  [   32/  130]
train() client id: f_00008-9-1 loss: 0.528973  [   64/  130]
train() client id: f_00008-9-2 loss: 0.668445  [   96/  130]
train() client id: f_00008-9-3 loss: 0.689448  [  128/  130]
train() client id: f_00008-10-0 loss: 0.609042  [   32/  130]
train() client id: f_00008-10-1 loss: 0.600782  [   64/  130]
train() client id: f_00008-10-2 loss: 0.638218  [   96/  130]
train() client id: f_00008-10-3 loss: 0.661359  [  128/  130]
train() client id: f_00009-0-0 loss: 1.081977  [   32/  118]
train() client id: f_00009-0-1 loss: 1.074783  [   64/  118]
train() client id: f_00009-0-2 loss: 1.071892  [   96/  118]
train() client id: f_00009-1-0 loss: 1.090502  [   32/  118]
train() client id: f_00009-1-1 loss: 0.927113  [   64/  118]
train() client id: f_00009-1-2 loss: 0.939774  [   96/  118]
train() client id: f_00009-2-0 loss: 0.970592  [   32/  118]
train() client id: f_00009-2-1 loss: 0.958628  [   64/  118]
train() client id: f_00009-2-2 loss: 1.027644  [   96/  118]
train() client id: f_00009-3-0 loss: 1.111400  [   32/  118]
train() client id: f_00009-3-1 loss: 0.907651  [   64/  118]
train() client id: f_00009-3-2 loss: 0.953529  [   96/  118]
train() client id: f_00009-4-0 loss: 0.821171  [   32/  118]
train() client id: f_00009-4-1 loss: 0.926674  [   64/  118]
train() client id: f_00009-4-2 loss: 1.066980  [   96/  118]
train() client id: f_00009-5-0 loss: 0.932348  [   32/  118]
train() client id: f_00009-5-1 loss: 0.911367  [   64/  118]
train() client id: f_00009-5-2 loss: 0.820845  [   96/  118]
train() client id: f_00009-6-0 loss: 0.865270  [   32/  118]
train() client id: f_00009-6-1 loss: 0.952244  [   64/  118]
train() client id: f_00009-6-2 loss: 0.922327  [   96/  118]
train() client id: f_00009-7-0 loss: 1.044664  [   32/  118]
train() client id: f_00009-7-1 loss: 0.851126  [   64/  118]
train() client id: f_00009-7-2 loss: 0.869785  [   96/  118]
train() client id: f_00009-8-0 loss: 1.037985  [   32/  118]
train() client id: f_00009-8-1 loss: 0.797184  [   64/  118]
train() client id: f_00009-8-2 loss: 0.834766  [   96/  118]
train() client id: f_00009-9-0 loss: 0.729278  [   32/  118]
train() client id: f_00009-9-1 loss: 0.931598  [   64/  118]
train() client id: f_00009-9-2 loss: 0.812579  [   96/  118]
train() client id: f_00009-10-0 loss: 0.811543  [   32/  118]
train() client id: f_00009-10-1 loss: 1.051140  [   64/  118]
train() client id: f_00009-10-2 loss: 0.821459  [   96/  118]
At round 52 accuracy: 0.6472148541114059
At round 52 training accuracy: 0.590878604963112
At round 52 training loss: 0.8318591578292441
update_location
xs = [  -3.9056584     4.20031788  280.00902392   18.81129433    0.97929623
    3.95640986 -242.44319194 -221.32485185  264.66397685 -207.06087855]
ys = [ 272.5879595   255.55583871    1.32061395 -242.45517586  234.35018685
  217.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [290.37811527 274.45660744 297.33280595 262.94177511 254.79593618
 239.70535547 262.27007427 242.86903117 283.47079323 229.97873659]
dists_bs = [196.96873753 196.4739094  487.0295073  460.25171554 185.71795115
 184.00665242 189.99794523 180.23275607 466.99992549 173.97813991]
uav_gains = [3.23107212e-12 4.49127941e-12 2.81429478e-12 5.71301961e-12
 6.74655076e-12 9.01929979e-12 5.79291830e-12 8.50621989e-12
 3.72132531e-12 1.07099613e-11]
bs_gains = [4.15893030e-11 4.18832527e-11 3.29715465e-12 3.86284178e-12
 4.90346648e-11 5.03222692e-11 4.60041651e-11 5.33285356e-11
 3.70857455e-12 5.88720247e-11]
Round 53
-------------------------------
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.00899084 8.17097489 3.95724982 1.44293614 9.42099912 4.53288609
 1.77835404 5.58181324 4.12270253 3.67526442]
obj_prev = 46.692171130454035
eta_min = 9.633908147429921e-24	eta_max = 0.9382560279743308
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 10.770764989587082	eta = 0.9090909090909091
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 22.73132098521351	eta = 0.4307538722609915
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 16.477782321424517	eta = 0.5942307250446651
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.352697592199675	eta = 0.6377774640049659
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286662235887265	eta = 0.6405325364618376
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286409541549396	eta = 0.6405431248831895
eta = 0.6405431248831895
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.03675654 0.07730543 0.03617311 0.0125439  0.08926589 0.04259092
 0.0157528  0.05221763 0.03792342 0.0344228 ]
ene_total = [1.46579906 2.36500809 1.47528965 0.70621205 2.69190422 1.38875393
 0.79404133 1.76977105 1.47499189 1.15463827]
ti_comp = [0.82252146 0.90567545 0.81328504 0.85268029 0.90809929 0.90848315
 0.85328562 0.86816903 0.8308949  0.91072419]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [4.58764259e-06 3.52017978e-05 4.47251598e-06 1.69670107e-07
 5.39100853e-05 5.85055712e-06 3.35555868e-07 1.18065477e-05
 4.93753617e-06 3.07358121e-06]
ene_total = [0.44053948 0.20942103 0.46630285 0.35628294 0.2031812  0.20076965
 0.35459888 0.31339904 0.41719008 0.19444042]
optimize_network iter = 0 obj = 3.1561255646288915
eta = 0.6405431248831895
freqs = [22343821.0822694  42678329.73940749 22238886.46292163  7355570.59611931
 49149853.28596357 23440674.99681275  9230675.16871088 30073423.9891545
 22820828.02431027 18898584.29975834]
eta_min = 0.6405431248831939	eta_max = 0.6991595698398908
af = 0.002799252834533405	bf = 1.1332441370306174	zeta = 0.0030791781179867455	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.81614110e-07 7.53209970e-06 9.56980564e-07 3.63041731e-08
 1.15350966e-05 1.25183889e-06 7.17986128e-08 2.52623730e-06
 1.05648055e-06 6.57651642e-07]
ene_total = [1.74150662 0.82500401 1.84338565 1.40873715 0.79870952 0.79334107
 1.40206398 1.23816416 1.64915218 0.76855593]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 1 obj = 3.770117456851034
eta = 0.6991595698398908
freqs = [22282599.06518497 41639030.99946182 22238886.46292161  7273350.56555898
 47925539.38367743 22854708.69293805  9126001.3904321  29615341.56378339
 22703099.87437893 18416482.47146711]
eta_min = 0.6991595698399091	eta_max = 0.6991595698398846
af = 0.002681430697799304	bf = 1.1332441370306174	zeta = 0.0029495737675792346	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.76242239e-07 7.16972436e-06 9.56980564e-07 3.54970983e-08
 1.09675798e-05 1.19003461e-06 7.01794852e-08 2.44986346e-06
 1.04560832e-06 6.24526300e-07]
ene_total = [1.74150603 0.82496404 1.84338565 1.40873707 0.79864692 0.79333425
 1.4020638  1.23815573 1.64915098 0.76855228]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 2 obj = 3.770117456850956
eta = 0.6991595698398846
freqs = [22282599.06518495 41639030.99946193 22238886.46292159  7273350.56555898
 47925539.38367755 22854708.6929381   9126001.39043211 29615341.56378343
 22703099.87437893 18416482.47146716]
Done!
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.44084355e-06 2.52702648e-05 3.37295424e-06 1.25112351e-07
 3.86561088e-05 4.19437181e-06 2.47353187e-07 8.63473895e-06
 3.68532982e-06 2.20119270e-06]
ene_total = [0.01579065 0.00749708 0.01671422 0.01277145 0.00726808 0.00719523
 0.01271104 0.01123108 0.01495355 0.00696913]
At round 53 energy consumption: 0.11310150513411767
At round 53 eta: 0.6991595698398846
At round 53 a_n: 10.027672966880559
At round 53 local rounds: 11.718687629159392
At round 53 global rounds: 33.33219860622975
gradient difference: 0.5058578252792358
train() client id: f_00000-0-0 loss: 1.072328  [   32/  126]
train() client id: f_00000-0-1 loss: 0.976701  [   64/  126]
train() client id: f_00000-0-2 loss: 1.092133  [   96/  126]
train() client id: f_00000-1-0 loss: 1.036666  [   32/  126]
train() client id: f_00000-1-1 loss: 1.034973  [   64/  126]
train() client id: f_00000-1-2 loss: 0.849213  [   96/  126]
train() client id: f_00000-2-0 loss: 0.955924  [   32/  126]
train() client id: f_00000-2-1 loss: 0.904674  [   64/  126]
train() client id: f_00000-2-2 loss: 0.875358  [   96/  126]
train() client id: f_00000-3-0 loss: 0.763766  [   32/  126]
train() client id: f_00000-3-1 loss: 0.858928  [   64/  126]
train() client id: f_00000-3-2 loss: 1.040336  [   96/  126]
train() client id: f_00000-4-0 loss: 0.919660  [   32/  126]
train() client id: f_00000-4-1 loss: 0.875363  [   64/  126]
train() client id: f_00000-4-2 loss: 0.851516  [   96/  126]
train() client id: f_00000-5-0 loss: 0.904136  [   32/  126]
train() client id: f_00000-5-1 loss: 0.821503  [   64/  126]
train() client id: f_00000-5-2 loss: 0.815009  [   96/  126]
train() client id: f_00000-6-0 loss: 0.901313  [   32/  126]
train() client id: f_00000-6-1 loss: 0.887765  [   64/  126]
train() client id: f_00000-6-2 loss: 0.755508  [   96/  126]
train() client id: f_00000-7-0 loss: 0.876502  [   32/  126]
train() client id: f_00000-7-1 loss: 0.818777  [   64/  126]
train() client id: f_00000-7-2 loss: 0.781797  [   96/  126]
train() client id: f_00000-8-0 loss: 0.833395  [   32/  126]
train() client id: f_00000-8-1 loss: 0.745296  [   64/  126]
train() client id: f_00000-8-2 loss: 0.750137  [   96/  126]
train() client id: f_00000-9-0 loss: 0.767538  [   32/  126]
train() client id: f_00000-9-1 loss: 0.800772  [   64/  126]
train() client id: f_00000-9-2 loss: 0.805753  [   96/  126]
train() client id: f_00000-10-0 loss: 0.864740  [   32/  126]
train() client id: f_00000-10-1 loss: 0.670522  [   64/  126]
train() client id: f_00000-10-2 loss: 0.983439  [   96/  126]
train() client id: f_00001-0-0 loss: 0.364667  [   32/  265]
train() client id: f_00001-0-1 loss: 0.502518  [   64/  265]
train() client id: f_00001-0-2 loss: 0.354840  [   96/  265]
train() client id: f_00001-0-3 loss: 0.362488  [  128/  265]
train() client id: f_00001-0-4 loss: 0.480737  [  160/  265]
train() client id: f_00001-0-5 loss: 0.494256  [  192/  265]
train() client id: f_00001-0-6 loss: 0.480816  [  224/  265]
train() client id: f_00001-0-7 loss: 0.419788  [  256/  265]
train() client id: f_00001-1-0 loss: 0.407757  [   32/  265]
train() client id: f_00001-1-1 loss: 0.497569  [   64/  265]
train() client id: f_00001-1-2 loss: 0.469718  [   96/  265]
train() client id: f_00001-1-3 loss: 0.373124  [  128/  265]
train() client id: f_00001-1-4 loss: 0.357054  [  160/  265]
train() client id: f_00001-1-5 loss: 0.426440  [  192/  265]
train() client id: f_00001-1-6 loss: 0.480687  [  224/  265]
train() client id: f_00001-1-7 loss: 0.400319  [  256/  265]
train() client id: f_00001-2-0 loss: 0.428564  [   32/  265]
train() client id: f_00001-2-1 loss: 0.483384  [   64/  265]
train() client id: f_00001-2-2 loss: 0.456264  [   96/  265]
train() client id: f_00001-2-3 loss: 0.414509  [  128/  265]
train() client id: f_00001-2-4 loss: 0.455626  [  160/  265]
train() client id: f_00001-2-5 loss: 0.339554  [  192/  265]
train() client id: f_00001-2-6 loss: 0.402714  [  224/  265]
train() client id: f_00001-2-7 loss: 0.406731  [  256/  265]
train() client id: f_00001-3-0 loss: 0.428061  [   32/  265]
train() client id: f_00001-3-1 loss: 0.589950  [   64/  265]
train() client id: f_00001-3-2 loss: 0.419980  [   96/  265]
train() client id: f_00001-3-3 loss: 0.323739  [  128/  265]
train() client id: f_00001-3-4 loss: 0.320272  [  160/  265]
train() client id: f_00001-3-5 loss: 0.416337  [  192/  265]
train() client id: f_00001-3-6 loss: 0.470104  [  224/  265]
train() client id: f_00001-3-7 loss: 0.380643  [  256/  265]
train() client id: f_00001-4-0 loss: 0.403947  [   32/  265]
train() client id: f_00001-4-1 loss: 0.358096  [   64/  265]
train() client id: f_00001-4-2 loss: 0.403038  [   96/  265]
train() client id: f_00001-4-3 loss: 0.428858  [  128/  265]
train() client id: f_00001-4-4 loss: 0.612434  [  160/  265]
train() client id: f_00001-4-5 loss: 0.400113  [  192/  265]
train() client id: f_00001-4-6 loss: 0.331606  [  224/  265]
train() client id: f_00001-4-7 loss: 0.325331  [  256/  265]
train() client id: f_00001-5-0 loss: 0.415750  [   32/  265]
train() client id: f_00001-5-1 loss: 0.377012  [   64/  265]
train() client id: f_00001-5-2 loss: 0.411478  [   96/  265]
train() client id: f_00001-5-3 loss: 0.482420  [  128/  265]
train() client id: f_00001-5-4 loss: 0.343411  [  160/  265]
train() client id: f_00001-5-5 loss: 0.433716  [  192/  265]
train() client id: f_00001-5-6 loss: 0.498901  [  224/  265]
train() client id: f_00001-5-7 loss: 0.346939  [  256/  265]
train() client id: f_00001-6-0 loss: 0.360550  [   32/  265]
train() client id: f_00001-6-1 loss: 0.574042  [   64/  265]
train() client id: f_00001-6-2 loss: 0.401983  [   96/  265]
train() client id: f_00001-6-3 loss: 0.352870  [  128/  265]
train() client id: f_00001-6-4 loss: 0.391141  [  160/  265]
train() client id: f_00001-6-5 loss: 0.445183  [  192/  265]
train() client id: f_00001-6-6 loss: 0.414767  [  224/  265]
train() client id: f_00001-6-7 loss: 0.383027  [  256/  265]
train() client id: f_00001-7-0 loss: 0.452064  [   32/  265]
train() client id: f_00001-7-1 loss: 0.499539  [   64/  265]
train() client id: f_00001-7-2 loss: 0.296223  [   96/  265]
train() client id: f_00001-7-3 loss: 0.410796  [  128/  265]
train() client id: f_00001-7-4 loss: 0.547181  [  160/  265]
train() client id: f_00001-7-5 loss: 0.314775  [  192/  265]
train() client id: f_00001-7-6 loss: 0.422710  [  224/  265]
train() client id: f_00001-7-7 loss: 0.374511  [  256/  265]
train() client id: f_00001-8-0 loss: 0.517073  [   32/  265]
train() client id: f_00001-8-1 loss: 0.408767  [   64/  265]
train() client id: f_00001-8-2 loss: 0.379802  [   96/  265]
train() client id: f_00001-8-3 loss: 0.366083  [  128/  265]
train() client id: f_00001-8-4 loss: 0.327905  [  160/  265]
train() client id: f_00001-8-5 loss: 0.436403  [  192/  265]
train() client id: f_00001-8-6 loss: 0.582623  [  224/  265]
train() client id: f_00001-8-7 loss: 0.302469  [  256/  265]
train() client id: f_00001-9-0 loss: 0.412579  [   32/  265]
train() client id: f_00001-9-1 loss: 0.534102  [   64/  265]
train() client id: f_00001-9-2 loss: 0.322473  [   96/  265]
train() client id: f_00001-9-3 loss: 0.444923  [  128/  265]
train() client id: f_00001-9-4 loss: 0.331696  [  160/  265]
train() client id: f_00001-9-5 loss: 0.348683  [  192/  265]
train() client id: f_00001-9-6 loss: 0.547830  [  224/  265]
train() client id: f_00001-9-7 loss: 0.383072  [  256/  265]
train() client id: f_00001-10-0 loss: 0.434528  [   32/  265]
train() client id: f_00001-10-1 loss: 0.323548  [   64/  265]
train() client id: f_00001-10-2 loss: 0.550290  [   96/  265]
train() client id: f_00001-10-3 loss: 0.437781  [  128/  265]
train() client id: f_00001-10-4 loss: 0.331129  [  160/  265]
train() client id: f_00001-10-5 loss: 0.446162  [  192/  265]
train() client id: f_00001-10-6 loss: 0.456869  [  224/  265]
train() client id: f_00001-10-7 loss: 0.354632  [  256/  265]
train() client id: f_00002-0-0 loss: 1.121153  [   32/  124]
train() client id: f_00002-0-1 loss: 1.138698  [   64/  124]
train() client id: f_00002-0-2 loss: 1.357320  [   96/  124]
train() client id: f_00002-1-0 loss: 1.258903  [   32/  124]
train() client id: f_00002-1-1 loss: 1.046504  [   64/  124]
train() client id: f_00002-1-2 loss: 1.311446  [   96/  124]
train() client id: f_00002-2-0 loss: 0.999933  [   32/  124]
train() client id: f_00002-2-1 loss: 1.113385  [   64/  124]
train() client id: f_00002-2-2 loss: 1.288576  [   96/  124]
train() client id: f_00002-3-0 loss: 0.922997  [   32/  124]
train() client id: f_00002-3-1 loss: 1.202681  [   64/  124]
train() client id: f_00002-3-2 loss: 1.175085  [   96/  124]
train() client id: f_00002-4-0 loss: 1.245205  [   32/  124]
train() client id: f_00002-4-1 loss: 0.922905  [   64/  124]
train() client id: f_00002-4-2 loss: 1.131744  [   96/  124]
train() client id: f_00002-5-0 loss: 1.214505  [   32/  124]
train() client id: f_00002-5-1 loss: 0.864612  [   64/  124]
train() client id: f_00002-5-2 loss: 1.056278  [   96/  124]
train() client id: f_00002-6-0 loss: 1.245968  [   32/  124]
train() client id: f_00002-6-1 loss: 0.983827  [   64/  124]
train() client id: f_00002-6-2 loss: 0.929787  [   96/  124]
train() client id: f_00002-7-0 loss: 0.997943  [   32/  124]
train() client id: f_00002-7-1 loss: 1.103583  [   64/  124]
train() client id: f_00002-7-2 loss: 1.166204  [   96/  124]
train() client id: f_00002-8-0 loss: 1.175187  [   32/  124]
train() client id: f_00002-8-1 loss: 0.922906  [   64/  124]
train() client id: f_00002-8-2 loss: 1.052851  [   96/  124]
train() client id: f_00002-9-0 loss: 0.997339  [   32/  124]
train() client id: f_00002-9-1 loss: 0.958780  [   64/  124]
train() client id: f_00002-9-2 loss: 1.079373  [   96/  124]
train() client id: f_00002-10-0 loss: 0.936449  [   32/  124]
train() client id: f_00002-10-1 loss: 0.932434  [   64/  124]
train() client id: f_00002-10-2 loss: 1.032233  [   96/  124]
train() client id: f_00003-0-0 loss: 0.422868  [   32/   43]
train() client id: f_00003-1-0 loss: 0.578982  [   32/   43]
train() client id: f_00003-2-0 loss: 0.438433  [   32/   43]
train() client id: f_00003-3-0 loss: 0.309713  [   32/   43]
train() client id: f_00003-4-0 loss: 0.515298  [   32/   43]
train() client id: f_00003-5-0 loss: 0.505265  [   32/   43]
train() client id: f_00003-6-0 loss: 0.330196  [   32/   43]
train() client id: f_00003-7-0 loss: 0.422520  [   32/   43]
train() client id: f_00003-8-0 loss: 0.519943  [   32/   43]
train() client id: f_00003-9-0 loss: 0.577260  [   32/   43]
train() client id: f_00003-10-0 loss: 0.475504  [   32/   43]
train() client id: f_00004-0-0 loss: 0.997315  [   32/  306]
train() client id: f_00004-0-1 loss: 0.696502  [   64/  306]
train() client id: f_00004-0-2 loss: 0.838807  [   96/  306]
train() client id: f_00004-0-3 loss: 0.963258  [  128/  306]
train() client id: f_00004-0-4 loss: 0.790212  [  160/  306]
train() client id: f_00004-0-5 loss: 0.932399  [  192/  306]
train() client id: f_00004-0-6 loss: 0.807363  [  224/  306]
train() client id: f_00004-0-7 loss: 0.838172  [  256/  306]
train() client id: f_00004-0-8 loss: 0.765816  [  288/  306]
train() client id: f_00004-1-0 loss: 0.771625  [   32/  306]
train() client id: f_00004-1-1 loss: 0.776178  [   64/  306]
train() client id: f_00004-1-2 loss: 0.892670  [   96/  306]
train() client id: f_00004-1-3 loss: 0.662324  [  128/  306]
train() client id: f_00004-1-4 loss: 0.941104  [  160/  306]
train() client id: f_00004-1-5 loss: 0.830670  [  192/  306]
train() client id: f_00004-1-6 loss: 0.853929  [  224/  306]
train() client id: f_00004-1-7 loss: 0.892157  [  256/  306]
train() client id: f_00004-1-8 loss: 0.993843  [  288/  306]
train() client id: f_00004-2-0 loss: 0.695942  [   32/  306]
train() client id: f_00004-2-1 loss: 0.960850  [   64/  306]
train() client id: f_00004-2-2 loss: 0.852516  [   96/  306]
train() client id: f_00004-2-3 loss: 0.875882  [  128/  306]
train() client id: f_00004-2-4 loss: 0.838459  [  160/  306]
train() client id: f_00004-2-5 loss: 0.796272  [  192/  306]
train() client id: f_00004-2-6 loss: 0.871224  [  224/  306]
train() client id: f_00004-2-7 loss: 0.906963  [  256/  306]
train() client id: f_00004-2-8 loss: 0.761288  [  288/  306]
train() client id: f_00004-3-0 loss: 0.889247  [   32/  306]
train() client id: f_00004-3-1 loss: 0.847959  [   64/  306]
train() client id: f_00004-3-2 loss: 0.846839  [   96/  306]
train() client id: f_00004-3-3 loss: 0.805975  [  128/  306]
train() client id: f_00004-3-4 loss: 0.894506  [  160/  306]
train() client id: f_00004-3-5 loss: 0.967928  [  192/  306]
train() client id: f_00004-3-6 loss: 0.711748  [  224/  306]
train() client id: f_00004-3-7 loss: 0.770460  [  256/  306]
train() client id: f_00004-3-8 loss: 0.817194  [  288/  306]
train() client id: f_00004-4-0 loss: 0.879491  [   32/  306]
train() client id: f_00004-4-1 loss: 0.832056  [   64/  306]
train() client id: f_00004-4-2 loss: 0.848166  [   96/  306]
train() client id: f_00004-4-3 loss: 0.838577  [  128/  306]
train() client id: f_00004-4-4 loss: 0.844497  [  160/  306]
train() client id: f_00004-4-5 loss: 0.807612  [  192/  306]
train() client id: f_00004-4-6 loss: 0.899960  [  224/  306]
train() client id: f_00004-4-7 loss: 0.820752  [  256/  306]
train() client id: f_00004-4-8 loss: 0.728330  [  288/  306]
train() client id: f_00004-5-0 loss: 0.757931  [   32/  306]
train() client id: f_00004-5-1 loss: 0.985811  [   64/  306]
train() client id: f_00004-5-2 loss: 0.777980  [   96/  306]
train() client id: f_00004-5-3 loss: 0.846385  [  128/  306]
train() client id: f_00004-5-4 loss: 0.831155  [  160/  306]
train() client id: f_00004-5-5 loss: 0.800128  [  192/  306]
train() client id: f_00004-5-6 loss: 0.789600  [  224/  306]
train() client id: f_00004-5-7 loss: 0.951163  [  256/  306]
train() client id: f_00004-5-8 loss: 0.786132  [  288/  306]
train() client id: f_00004-6-0 loss: 0.871164  [   32/  306]
train() client id: f_00004-6-1 loss: 0.801009  [   64/  306]
train() client id: f_00004-6-2 loss: 0.755490  [   96/  306]
train() client id: f_00004-6-3 loss: 0.779408  [  128/  306]
train() client id: f_00004-6-4 loss: 0.854388  [  160/  306]
train() client id: f_00004-6-5 loss: 0.814899  [  192/  306]
train() client id: f_00004-6-6 loss: 0.866299  [  224/  306]
train() client id: f_00004-6-7 loss: 0.835915  [  256/  306]
train() client id: f_00004-6-8 loss: 1.034711  [  288/  306]
train() client id: f_00004-7-0 loss: 0.786039  [   32/  306]
train() client id: f_00004-7-1 loss: 0.914150  [   64/  306]
train() client id: f_00004-7-2 loss: 0.825126  [   96/  306]
train() client id: f_00004-7-3 loss: 0.788614  [  128/  306]
train() client id: f_00004-7-4 loss: 0.823472  [  160/  306]
train() client id: f_00004-7-5 loss: 0.687200  [  192/  306]
train() client id: f_00004-7-6 loss: 0.793108  [  224/  306]
train() client id: f_00004-7-7 loss: 0.871035  [  256/  306]
train() client id: f_00004-7-8 loss: 0.979519  [  288/  306]
train() client id: f_00004-8-0 loss: 0.794264  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825391  [   64/  306]
train() client id: f_00004-8-2 loss: 0.792467  [   96/  306]
train() client id: f_00004-8-3 loss: 0.900827  [  128/  306]
train() client id: f_00004-8-4 loss: 0.799769  [  160/  306]
train() client id: f_00004-8-5 loss: 0.873884  [  192/  306]
train() client id: f_00004-8-6 loss: 0.842459  [  224/  306]
train() client id: f_00004-8-7 loss: 0.881749  [  256/  306]
train() client id: f_00004-8-8 loss: 0.751069  [  288/  306]
train() client id: f_00004-9-0 loss: 0.845772  [   32/  306]
train() client id: f_00004-9-1 loss: 0.827896  [   64/  306]
train() client id: f_00004-9-2 loss: 0.702917  [   96/  306]
train() client id: f_00004-9-3 loss: 0.876112  [  128/  306]
train() client id: f_00004-9-4 loss: 0.819071  [  160/  306]
train() client id: f_00004-9-5 loss: 0.757402  [  192/  306]
train() client id: f_00004-9-6 loss: 0.875156  [  224/  306]
train() client id: f_00004-9-7 loss: 0.882623  [  256/  306]
train() client id: f_00004-9-8 loss: 0.902419  [  288/  306]
train() client id: f_00004-10-0 loss: 0.774920  [   32/  306]
train() client id: f_00004-10-1 loss: 0.743436  [   64/  306]
train() client id: f_00004-10-2 loss: 0.920748  [   96/  306]
train() client id: f_00004-10-3 loss: 0.772784  [  128/  306]
train() client id: f_00004-10-4 loss: 1.081764  [  160/  306]
train() client id: f_00004-10-5 loss: 0.777354  [  192/  306]
train() client id: f_00004-10-6 loss: 0.827891  [  224/  306]
train() client id: f_00004-10-7 loss: 0.869277  [  256/  306]
train() client id: f_00004-10-8 loss: 0.744334  [  288/  306]
train() client id: f_00005-0-0 loss: 0.742798  [   32/  146]
train() client id: f_00005-0-1 loss: 0.952220  [   64/  146]
train() client id: f_00005-0-2 loss: 0.659134  [   96/  146]
train() client id: f_00005-0-3 loss: 0.898898  [  128/  146]
train() client id: f_00005-1-0 loss: 0.692659  [   32/  146]
train() client id: f_00005-1-1 loss: 0.935993  [   64/  146]
train() client id: f_00005-1-2 loss: 0.931386  [   96/  146]
train() client id: f_00005-1-3 loss: 0.917159  [  128/  146]
train() client id: f_00005-2-0 loss: 0.838648  [   32/  146]
train() client id: f_00005-2-1 loss: 0.917752  [   64/  146]
train() client id: f_00005-2-2 loss: 0.771105  [   96/  146]
train() client id: f_00005-2-3 loss: 0.849322  [  128/  146]
train() client id: f_00005-3-0 loss: 0.785529  [   32/  146]
train() client id: f_00005-3-1 loss: 0.910406  [   64/  146]
train() client id: f_00005-3-2 loss: 0.966642  [   96/  146]
train() client id: f_00005-3-3 loss: 0.836034  [  128/  146]
train() client id: f_00005-4-0 loss: 0.996275  [   32/  146]
train() client id: f_00005-4-1 loss: 0.685976  [   64/  146]
train() client id: f_00005-4-2 loss: 0.948173  [   96/  146]
train() client id: f_00005-4-3 loss: 0.768964  [  128/  146]
train() client id: f_00005-5-0 loss: 0.803571  [   32/  146]
train() client id: f_00005-5-1 loss: 0.935921  [   64/  146]
train() client id: f_00005-5-2 loss: 0.705257  [   96/  146]
train() client id: f_00005-5-3 loss: 0.780546  [  128/  146]
train() client id: f_00005-6-0 loss: 0.731773  [   32/  146]
train() client id: f_00005-6-1 loss: 0.674831  [   64/  146]
train() client id: f_00005-6-2 loss: 0.794638  [   96/  146]
train() client id: f_00005-6-3 loss: 1.307119  [  128/  146]
train() client id: f_00005-7-0 loss: 0.561753  [   32/  146]
train() client id: f_00005-7-1 loss: 0.854251  [   64/  146]
train() client id: f_00005-7-2 loss: 1.237895  [   96/  146]
train() client id: f_00005-7-3 loss: 0.809450  [  128/  146]
train() client id: f_00005-8-0 loss: 0.680680  [   32/  146]
train() client id: f_00005-8-1 loss: 0.879694  [   64/  146]
train() client id: f_00005-8-2 loss: 0.999192  [   96/  146]
train() client id: f_00005-8-3 loss: 0.834402  [  128/  146]
train() client id: f_00005-9-0 loss: 0.897318  [   32/  146]
train() client id: f_00005-9-1 loss: 0.825794  [   64/  146]
train() client id: f_00005-9-2 loss: 0.900802  [   96/  146]
train() client id: f_00005-9-3 loss: 0.863874  [  128/  146]
train() client id: f_00005-10-0 loss: 0.662041  [   32/  146]
train() client id: f_00005-10-1 loss: 0.785401  [   64/  146]
train() client id: f_00005-10-2 loss: 0.848813  [   96/  146]
train() client id: f_00005-10-3 loss: 0.976256  [  128/  146]
train() client id: f_00006-0-0 loss: 0.428305  [   32/   54]
train() client id: f_00006-1-0 loss: 0.371080  [   32/   54]
train() client id: f_00006-2-0 loss: 0.388510  [   32/   54]
train() client id: f_00006-3-0 loss: 0.377813  [   32/   54]
train() client id: f_00006-4-0 loss: 0.486244  [   32/   54]
train() client id: f_00006-5-0 loss: 0.486897  [   32/   54]
train() client id: f_00006-6-0 loss: 0.379593  [   32/   54]
train() client id: f_00006-7-0 loss: 0.424188  [   32/   54]
train() client id: f_00006-8-0 loss: 0.407643  [   32/   54]
train() client id: f_00006-9-0 loss: 0.409002  [   32/   54]
train() client id: f_00006-10-0 loss: 0.421396  [   32/   54]
train() client id: f_00007-0-0 loss: 0.672042  [   32/  179]
train() client id: f_00007-0-1 loss: 0.663947  [   64/  179]
train() client id: f_00007-0-2 loss: 0.629282  [   96/  179]
train() client id: f_00007-0-3 loss: 0.778374  [  128/  179]
train() client id: f_00007-0-4 loss: 0.665942  [  160/  179]
train() client id: f_00007-1-0 loss: 0.567816  [   32/  179]
train() client id: f_00007-1-1 loss: 0.750107  [   64/  179]
train() client id: f_00007-1-2 loss: 0.558193  [   96/  179]
train() client id: f_00007-1-3 loss: 0.890450  [  128/  179]
train() client id: f_00007-1-4 loss: 0.586562  [  160/  179]
train() client id: f_00007-2-0 loss: 0.696941  [   32/  179]
train() client id: f_00007-2-1 loss: 0.666090  [   64/  179]
train() client id: f_00007-2-2 loss: 0.700027  [   96/  179]
train() client id: f_00007-2-3 loss: 0.758378  [  128/  179]
train() client id: f_00007-2-4 loss: 0.661413  [  160/  179]
train() client id: f_00007-3-0 loss: 0.754704  [   32/  179]
train() client id: f_00007-3-1 loss: 0.687920  [   64/  179]
train() client id: f_00007-3-2 loss: 0.542720  [   96/  179]
train() client id: f_00007-3-3 loss: 0.844918  [  128/  179]
train() client id: f_00007-3-4 loss: 0.533503  [  160/  179]
train() client id: f_00007-4-0 loss: 0.620001  [   32/  179]
train() client id: f_00007-4-1 loss: 0.999698  [   64/  179]
train() client id: f_00007-4-2 loss: 0.583882  [   96/  179]
train() client id: f_00007-4-3 loss: 0.650363  [  128/  179]
train() client id: f_00007-4-4 loss: 0.520529  [  160/  179]
train() client id: f_00007-5-0 loss: 0.591934  [   32/  179]
train() client id: f_00007-5-1 loss: 0.699959  [   64/  179]
train() client id: f_00007-5-2 loss: 0.798948  [   96/  179]
train() client id: f_00007-5-3 loss: 0.751778  [  128/  179]
train() client id: f_00007-5-4 loss: 0.635589  [  160/  179]
train() client id: f_00007-6-0 loss: 0.893673  [   32/  179]
train() client id: f_00007-6-1 loss: 0.528301  [   64/  179]
train() client id: f_00007-6-2 loss: 0.784233  [   96/  179]
train() client id: f_00007-6-3 loss: 0.564121  [  128/  179]
train() client id: f_00007-6-4 loss: 0.498892  [  160/  179]
train() client id: f_00007-7-0 loss: 0.600858  [   32/  179]
train() client id: f_00007-7-1 loss: 0.947571  [   64/  179]
train() client id: f_00007-7-2 loss: 0.824641  [   96/  179]
train() client id: f_00007-7-3 loss: 0.540013  [  128/  179]
train() client id: f_00007-7-4 loss: 0.567330  [  160/  179]
train() client id: f_00007-8-0 loss: 0.638634  [   32/  179]
train() client id: f_00007-8-1 loss: 0.537182  [   64/  179]
train() client id: f_00007-8-2 loss: 0.742877  [   96/  179]
train() client id: f_00007-8-3 loss: 0.727802  [  128/  179]
train() client id: f_00007-8-4 loss: 0.523243  [  160/  179]
train() client id: f_00007-9-0 loss: 0.767468  [   32/  179]
train() client id: f_00007-9-1 loss: 0.606014  [   64/  179]
train() client id: f_00007-9-2 loss: 0.632490  [   96/  179]
train() client id: f_00007-9-3 loss: 0.720358  [  128/  179]
train() client id: f_00007-9-4 loss: 0.775221  [  160/  179]
train() client id: f_00007-10-0 loss: 0.534423  [   32/  179]
train() client id: f_00007-10-1 loss: 0.859436  [   64/  179]
train() client id: f_00007-10-2 loss: 0.494145  [   96/  179]
train() client id: f_00007-10-3 loss: 0.615968  [  128/  179]
train() client id: f_00007-10-4 loss: 0.629096  [  160/  179]
train() client id: f_00008-0-0 loss: 0.743464  [   32/  130]
train() client id: f_00008-0-1 loss: 0.606431  [   64/  130]
train() client id: f_00008-0-2 loss: 0.630498  [   96/  130]
train() client id: f_00008-0-3 loss: 0.761028  [  128/  130]
train() client id: f_00008-1-0 loss: 0.791897  [   32/  130]
train() client id: f_00008-1-1 loss: 0.693580  [   64/  130]
train() client id: f_00008-1-2 loss: 0.636413  [   96/  130]
train() client id: f_00008-1-3 loss: 0.627351  [  128/  130]
train() client id: f_00008-2-0 loss: 0.729915  [   32/  130]
train() client id: f_00008-2-1 loss: 0.750354  [   64/  130]
train() client id: f_00008-2-2 loss: 0.650482  [   96/  130]
train() client id: f_00008-2-3 loss: 0.638800  [  128/  130]
train() client id: f_00008-3-0 loss: 0.611333  [   32/  130]
train() client id: f_00008-3-1 loss: 0.701818  [   64/  130]
train() client id: f_00008-3-2 loss: 0.684083  [   96/  130]
train() client id: f_00008-3-3 loss: 0.769748  [  128/  130]
train() client id: f_00008-4-0 loss: 0.800224  [   32/  130]
train() client id: f_00008-4-1 loss: 0.663339  [   64/  130]
train() client id: f_00008-4-2 loss: 0.607778  [   96/  130]
train() client id: f_00008-4-3 loss: 0.695153  [  128/  130]
train() client id: f_00008-5-0 loss: 0.556918  [   32/  130]
train() client id: f_00008-5-1 loss: 0.807831  [   64/  130]
train() client id: f_00008-5-2 loss: 0.679723  [   96/  130]
train() client id: f_00008-5-3 loss: 0.717394  [  128/  130]
train() client id: f_00008-6-0 loss: 0.689802  [   32/  130]
train() client id: f_00008-6-1 loss: 0.714312  [   64/  130]
train() client id: f_00008-6-2 loss: 0.669222  [   96/  130]
train() client id: f_00008-6-3 loss: 0.694721  [  128/  130]
train() client id: f_00008-7-0 loss: 0.656314  [   32/  130]
train() client id: f_00008-7-1 loss: 0.770809  [   64/  130]
train() client id: f_00008-7-2 loss: 0.642477  [   96/  130]
train() client id: f_00008-7-3 loss: 0.698127  [  128/  130]
train() client id: f_00008-8-0 loss: 0.591807  [   32/  130]
train() client id: f_00008-8-1 loss: 0.663904  [   64/  130]
train() client id: f_00008-8-2 loss: 0.711196  [   96/  130]
train() client id: f_00008-8-3 loss: 0.806179  [  128/  130]
train() client id: f_00008-9-0 loss: 0.737028  [   32/  130]
train() client id: f_00008-9-1 loss: 0.736457  [   64/  130]
train() client id: f_00008-9-2 loss: 0.653328  [   96/  130]
train() client id: f_00008-9-3 loss: 0.633024  [  128/  130]
train() client id: f_00008-10-0 loss: 0.667369  [   32/  130]
train() client id: f_00008-10-1 loss: 0.677656  [   64/  130]
train() client id: f_00008-10-2 loss: 0.574889  [   96/  130]
train() client id: f_00008-10-3 loss: 0.848646  [  128/  130]
train() client id: f_00009-0-0 loss: 1.070174  [   32/  118]
train() client id: f_00009-0-1 loss: 1.043268  [   64/  118]
train() client id: f_00009-0-2 loss: 0.983941  [   96/  118]
train() client id: f_00009-1-0 loss: 0.798428  [   32/  118]
train() client id: f_00009-1-1 loss: 0.908195  [   64/  118]
train() client id: f_00009-1-2 loss: 1.038866  [   96/  118]
train() client id: f_00009-2-0 loss: 0.925055  [   32/  118]
train() client id: f_00009-2-1 loss: 0.872039  [   64/  118]
train() client id: f_00009-2-2 loss: 0.970321  [   96/  118]
train() client id: f_00009-3-0 loss: 0.860729  [   32/  118]
train() client id: f_00009-3-1 loss: 0.909315  [   64/  118]
train() client id: f_00009-3-2 loss: 0.970920  [   96/  118]
train() client id: f_00009-4-0 loss: 0.887569  [   32/  118]
train() client id: f_00009-4-1 loss: 0.844335  [   64/  118]
train() client id: f_00009-4-2 loss: 0.864382  [   96/  118]
train() client id: f_00009-5-0 loss: 0.930739  [   32/  118]
train() client id: f_00009-5-1 loss: 0.865704  [   64/  118]
train() client id: f_00009-5-2 loss: 0.812508  [   96/  118]
train() client id: f_00009-6-0 loss: 0.989808  [   32/  118]
train() client id: f_00009-6-1 loss: 0.824422  [   64/  118]
train() client id: f_00009-6-2 loss: 0.863724  [   96/  118]
train() client id: f_00009-7-0 loss: 0.795468  [   32/  118]
train() client id: f_00009-7-1 loss: 0.841370  [   64/  118]
train() client id: f_00009-7-2 loss: 0.913115  [   96/  118]
train() client id: f_00009-8-0 loss: 0.927798  [   32/  118]
train() client id: f_00009-8-1 loss: 0.656904  [   64/  118]
train() client id: f_00009-8-2 loss: 0.856568  [   96/  118]
train() client id: f_00009-9-0 loss: 0.756424  [   32/  118]
train() client id: f_00009-9-1 loss: 0.675411  [   64/  118]
train() client id: f_00009-9-2 loss: 0.802894  [   96/  118]
train() client id: f_00009-10-0 loss: 0.840873  [   32/  118]
train() client id: f_00009-10-1 loss: 0.744081  [   64/  118]
train() client id: f_00009-10-2 loss: 0.801646  [   96/  118]
At round 53 accuracy: 0.6472148541114059
At round 53 training accuracy: 0.5928906773977196
At round 53 training loss: 0.8261399945518615
update_location
xs = [  -3.9056584     4.20031788  285.00902392   18.81129433    0.97929623
    3.95640986 -247.44319194 -226.32485185  269.66397685 -212.06087855]
ys = [ 277.5879595   260.55583871    1.32061395 -247.45517586  239.35018685
  222.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [295.07681953 279.11823257 302.04616822 267.55920626 259.4021414
 244.2576487  266.89890179 247.43406156 288.14463449 234.49057138]
dists_bs = [199.49326595 198.57632151 491.70400773 464.79155911 187.3757168
 185.23387822 191.82974512 181.58219868 471.71026083 174.96857419]
uav_gains = [2.94167056e-12 4.07370156e-12 2.57043737e-12 5.18972845e-12
 6.14485954e-12 8.28695143e-12 5.26179225e-12 7.79933417e-12
 3.38079010e-12 9.90488682e-12]
bs_gains = [4.01323889e-11 4.06534300e-11 3.21013717e-12 3.75812338e-12
 4.78296084e-11 4.93943086e-11 4.47846749e-11 5.22262602e-11
 3.60581283e-12 5.79436618e-11]
Round 54
-------------------------------
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.87808648 7.89231983 3.82850312 1.39788506 9.0995415  4.37831172
 1.72181282 5.39414421 3.98347864 3.54993313]
obj_prev = 45.124016510283404
eta_min = 1.5688388831448793e-24	eta_max = 0.9379204733020944
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 10.402835079222914	eta = 0.9090909090909091
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 22.247241049856704	eta = 0.4250919373822522
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 16.022057938092566	eta = 0.5902564349620266
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.904186500929747	eta = 0.6345279427832983
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.838111741409746	eta = 0.6373535234204303
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.83785471002873	eta = 0.6373645640903601
eta = 0.6373645640903601
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.03716704 0.07816878 0.03657709 0.01268399 0.09026281 0.04306657
 0.01592873 0.0528008  0.03834695 0.03480723]
ene_total = [1.43077897 2.28852743 1.44075621 0.69142509 2.60477925 1.34299583
 0.77637967 1.71811452 1.42782004 1.11627769]
ti_comp = [0.85812284 0.94697766 0.84848758 0.89012496 0.94950542 0.94998635
 0.89076325 0.90687475 0.8710298  0.95228188]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [4.35767693e-06 3.32889784e-05 4.24831188e-06 1.60970289e-07
 5.09813926e-05 5.53179598e-06 3.18345717e-07 1.11868234e-05
 4.64522066e-06 2.90641279e-06]
ene_total = [0.4382524  0.20171506 0.46398296 0.35267029 0.19543653 0.19293825
 0.35096977 0.30823006 0.40378868 0.18673731]
optimize_network iter = 0 obj = 3.094721297443309
eta = 0.6373645640903601
freqs = [21656013.90690634 41272766.08711902 21554285.58755618  7124836.67893045
 47531488.32308537 22666941.75621849  8941057.85172046 29111404.29436724
 22012419.35744325 18275695.9412735 ]
eta_min = 0.6373645640903616	eta_max = 0.700248974656232
af = 0.0025292806273427074	bf = 1.1211163846456331	zeta = 0.002782208690076978	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.22110454e-07 7.04414656e-06 8.98968156e-07 3.40622741e-08
 1.07879670e-05 1.17056105e-06 6.73638541e-08 2.36719862e-06
 9.82956424e-07 6.15014298e-07]
ene_total = [1.74781835 0.80184327 1.85046646 1.40678466 0.77531226 0.76916407
 1.39998808 1.22858694 1.61031875 0.74464912]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 1 obj = 3.7430263059334705
eta = 0.700248974656232
freqs = [21591991.81873228 40169519.43771622 21554285.58755618  7037900.85535273
 46232598.93038776 22044962.3818897   8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
eta_min = 0.7002489746562424	eta_max = 0.7002489746562317
af = 0.0024124103758265034	bf = 1.1211163846456331	zeta = 0.002653651413409154	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.16666407e-07 6.67259097e-06 8.98968156e-07 3.32361034e-08
 1.02064191e-05 1.10720219e-06 6.57065467e-08 2.28871285e-06
 9.69657495e-07 5.81082673e-07]
ene_total = [1.74781777 0.80180369 1.85046646 1.40678457 0.7752503  0.76915732
 1.3999879  1.22857858 1.61031734 0.7446455 ]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 2 obj = 3.743026305933466
eta = 0.7002489746562317
freqs = [21591991.81873227 40169519.4377162  21554285.58755618  7037900.85535273
 46232598.93038776 22044962.38188971  8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
Done!
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.23086379e-06 2.35180785e-05 3.16848489e-06 1.17143295e-07
 3.59733374e-05 3.90242234e-06 2.31587959e-07 8.06675079e-06
 3.41763510e-06 2.04807218e-06]
ene_total = [0.01640814 0.00754295 0.01737161 0.01320482 0.00730263 0.00722246
 0.0131411  0.01153779 0.01511763 0.00699105]
At round 54 energy consumption: 0.11584017098855656
At round 54 eta: 0.7002489746562317
At round 54 a_n: 9.685127119911243
At round 54 local rounds: 11.66770515392171
At round 54 global rounds: 32.310572111651304
gradient difference: 0.4647432863712311
train() client id: f_00000-0-0 loss: 1.011119  [   32/  126]
train() client id: f_00000-0-1 loss: 1.407146  [   64/  126]
train() client id: f_00000-0-2 loss: 0.843451  [   96/  126]
train() client id: f_00000-1-0 loss: 0.896945  [   32/  126]
train() client id: f_00000-1-1 loss: 0.881687  [   64/  126]
train() client id: f_00000-1-2 loss: 1.140978  [   96/  126]
train() client id: f_00000-2-0 loss: 0.893413  [   32/  126]
train() client id: f_00000-2-1 loss: 0.900205  [   64/  126]
train() client id: f_00000-2-2 loss: 0.941534  [   96/  126]
train() client id: f_00000-3-0 loss: 0.836282  [   32/  126]
train() client id: f_00000-3-1 loss: 0.795155  [   64/  126]
train() client id: f_00000-3-2 loss: 0.895874  [   96/  126]
train() client id: f_00000-4-0 loss: 0.680544  [   32/  126]
train() client id: f_00000-4-1 loss: 0.825664  [   64/  126]
train() client id: f_00000-4-2 loss: 0.890253  [   96/  126]
train() client id: f_00000-5-0 loss: 0.803690  [   32/  126]
train() client id: f_00000-5-1 loss: 0.770714  [   64/  126]
train() client id: f_00000-5-2 loss: 0.780004  [   96/  126]
train() client id: f_00000-6-0 loss: 0.831976  [   32/  126]
train() client id: f_00000-6-1 loss: 0.821549  [   64/  126]
train() client id: f_00000-6-2 loss: 0.687744  [   96/  126]
train() client id: f_00000-7-0 loss: 0.772474  [   32/  126]
train() client id: f_00000-7-1 loss: 0.764945  [   64/  126]
train() client id: f_00000-7-2 loss: 0.876484  [   96/  126]
train() client id: f_00000-8-0 loss: 0.801412  [   32/  126]
train() client id: f_00000-8-1 loss: 0.701871  [   64/  126]
train() client id: f_00000-8-2 loss: 0.770190  [   96/  126]
train() client id: f_00000-9-0 loss: 0.842944  [   32/  126]
train() client id: f_00000-9-1 loss: 0.677199  [   64/  126]
train() client id: f_00000-9-2 loss: 0.627917  [   96/  126]
train() client id: f_00000-10-0 loss: 0.801017  [   32/  126]
train() client id: f_00000-10-1 loss: 0.631323  [   64/  126]
train() client id: f_00000-10-2 loss: 0.650532  [   96/  126]
train() client id: f_00001-0-0 loss: 0.424260  [   32/  265]
train() client id: f_00001-0-1 loss: 0.472036  [   64/  265]
train() client id: f_00001-0-2 loss: 0.471076  [   96/  265]
train() client id: f_00001-0-3 loss: 0.582879  [  128/  265]
train() client id: f_00001-0-4 loss: 0.409513  [  160/  265]
train() client id: f_00001-0-5 loss: 0.549742  [  192/  265]
train() client id: f_00001-0-6 loss: 0.493177  [  224/  265]
train() client id: f_00001-0-7 loss: 0.501853  [  256/  265]
train() client id: f_00001-1-0 loss: 0.583568  [   32/  265]
train() client id: f_00001-1-1 loss: 0.467906  [   64/  265]
train() client id: f_00001-1-2 loss: 0.497538  [   96/  265]
train() client id: f_00001-1-3 loss: 0.403087  [  128/  265]
train() client id: f_00001-1-4 loss: 0.452069  [  160/  265]
train() client id: f_00001-1-5 loss: 0.521122  [  192/  265]
train() client id: f_00001-1-6 loss: 0.489664  [  224/  265]
train() client id: f_00001-1-7 loss: 0.443045  [  256/  265]
train() client id: f_00001-2-0 loss: 0.526120  [   32/  265]
train() client id: f_00001-2-1 loss: 0.450614  [   64/  265]
train() client id: f_00001-2-2 loss: 0.366331  [   96/  265]
train() client id: f_00001-2-3 loss: 0.609012  [  128/  265]
train() client id: f_00001-2-4 loss: 0.445950  [  160/  265]
train() client id: f_00001-2-5 loss: 0.437890  [  192/  265]
train() client id: f_00001-2-6 loss: 0.452862  [  224/  265]
train() client id: f_00001-2-7 loss: 0.524866  [  256/  265]
train() client id: f_00001-3-0 loss: 0.512056  [   32/  265]
train() client id: f_00001-3-1 loss: 0.558591  [   64/  265]
train() client id: f_00001-3-2 loss: 0.529795  [   96/  265]
train() client id: f_00001-3-3 loss: 0.451197  [  128/  265]
train() client id: f_00001-3-4 loss: 0.409837  [  160/  265]
train() client id: f_00001-3-5 loss: 0.385925  [  192/  265]
train() client id: f_00001-3-6 loss: 0.450621  [  224/  265]
train() client id: f_00001-3-7 loss: 0.496035  [  256/  265]
train() client id: f_00001-4-0 loss: 0.488958  [   32/  265]
train() client id: f_00001-4-1 loss: 0.393766  [   64/  265]
train() client id: f_00001-4-2 loss: 0.413368  [   96/  265]
train() client id: f_00001-4-3 loss: 0.650706  [  128/  265]
train() client id: f_00001-4-4 loss: 0.463132  [  160/  265]
train() client id: f_00001-4-5 loss: 0.509087  [  192/  265]
train() client id: f_00001-4-6 loss: 0.431346  [  224/  265]
train() client id: f_00001-4-7 loss: 0.418124  [  256/  265]
train() client id: f_00001-5-0 loss: 0.468677  [   32/  265]
train() client id: f_00001-5-1 loss: 0.388443  [   64/  265]
train() client id: f_00001-5-2 loss: 0.381624  [   96/  265]
train() client id: f_00001-5-3 loss: 0.448343  [  128/  265]
train() client id: f_00001-5-4 loss: 0.535188  [  160/  265]
train() client id: f_00001-5-5 loss: 0.427184  [  192/  265]
train() client id: f_00001-5-6 loss: 0.611924  [  224/  265]
train() client id: f_00001-5-7 loss: 0.500280  [  256/  265]
train() client id: f_00001-6-0 loss: 0.435786  [   32/  265]
train() client id: f_00001-6-1 loss: 0.530291  [   64/  265]
train() client id: f_00001-6-2 loss: 0.390690  [   96/  265]
train() client id: f_00001-6-3 loss: 0.468457  [  128/  265]
train() client id: f_00001-6-4 loss: 0.600917  [  160/  265]
train() client id: f_00001-6-5 loss: 0.425822  [  192/  265]
train() client id: f_00001-6-6 loss: 0.467880  [  224/  265]
train() client id: f_00001-6-7 loss: 0.417216  [  256/  265]
train() client id: f_00001-7-0 loss: 0.429425  [   32/  265]
train() client id: f_00001-7-1 loss: 0.466917  [   64/  265]
train() client id: f_00001-7-2 loss: 0.389244  [   96/  265]
train() client id: f_00001-7-3 loss: 0.500769  [  128/  265]
train() client id: f_00001-7-4 loss: 0.427851  [  160/  265]
train() client id: f_00001-7-5 loss: 0.645187  [  192/  265]
train() client id: f_00001-7-6 loss: 0.523845  [  224/  265]
train() client id: f_00001-7-7 loss: 0.359636  [  256/  265]
train() client id: f_00001-8-0 loss: 0.426280  [   32/  265]
train() client id: f_00001-8-1 loss: 0.378632  [   64/  265]
train() client id: f_00001-8-2 loss: 0.648803  [   96/  265]
train() client id: f_00001-8-3 loss: 0.482591  [  128/  265]
train() client id: f_00001-8-4 loss: 0.430576  [  160/  265]
train() client id: f_00001-8-5 loss: 0.449862  [  192/  265]
train() client id: f_00001-8-6 loss: 0.473622  [  224/  265]
train() client id: f_00001-8-7 loss: 0.371683  [  256/  265]
train() client id: f_00001-9-0 loss: 0.537850  [   32/  265]
train() client id: f_00001-9-1 loss: 0.485803  [   64/  265]
train() client id: f_00001-9-2 loss: 0.482935  [   96/  265]
train() client id: f_00001-9-3 loss: 0.428363  [  128/  265]
train() client id: f_00001-9-4 loss: 0.430947  [  160/  265]
train() client id: f_00001-9-5 loss: 0.525858  [  192/  265]
train() client id: f_00001-9-6 loss: 0.498405  [  224/  265]
train() client id: f_00001-9-7 loss: 0.364415  [  256/  265]
train() client id: f_00001-10-0 loss: 0.499051  [   32/  265]
train() client id: f_00001-10-1 loss: 0.528602  [   64/  265]
train() client id: f_00001-10-2 loss: 0.634346  [   96/  265]
train() client id: f_00001-10-3 loss: 0.521976  [  128/  265]
train() client id: f_00001-10-4 loss: 0.415901  [  160/  265]
train() client id: f_00001-10-5 loss: 0.412175  [  192/  265]
train() client id: f_00001-10-6 loss: 0.378011  [  224/  265]
train() client id: f_00001-10-7 loss: 0.364584  [  256/  265]
train() client id: f_00002-0-0 loss: 1.186728  [   32/  124]
train() client id: f_00002-0-1 loss: 1.084779  [   64/  124]
train() client id: f_00002-0-2 loss: 1.007863  [   96/  124]
train() client id: f_00002-1-0 loss: 1.053792  [   32/  124]
train() client id: f_00002-1-1 loss: 1.079004  [   64/  124]
train() client id: f_00002-1-2 loss: 1.023408  [   96/  124]
train() client id: f_00002-2-0 loss: 1.045043  [   32/  124]
train() client id: f_00002-2-1 loss: 0.877745  [   64/  124]
train() client id: f_00002-2-2 loss: 0.951904  [   96/  124]
train() client id: f_00002-3-0 loss: 0.829754  [   32/  124]
train() client id: f_00002-3-1 loss: 0.894539  [   64/  124]
train() client id: f_00002-3-2 loss: 0.948112  [   96/  124]
train() client id: f_00002-4-0 loss: 0.938126  [   32/  124]
train() client id: f_00002-4-1 loss: 0.895581  [   64/  124]
train() client id: f_00002-4-2 loss: 0.923688  [   96/  124]
train() client id: f_00002-5-0 loss: 0.947486  [   32/  124]
train() client id: f_00002-5-1 loss: 0.693961  [   64/  124]
train() client id: f_00002-5-2 loss: 1.169343  [   96/  124]
train() client id: f_00002-6-0 loss: 1.054555  [   32/  124]
train() client id: f_00002-6-1 loss: 0.809136  [   64/  124]
train() client id: f_00002-6-2 loss: 0.968490  [   96/  124]
train() client id: f_00002-7-0 loss: 0.926534  [   32/  124]
train() client id: f_00002-7-1 loss: 0.803147  [   64/  124]
train() client id: f_00002-7-2 loss: 1.064084  [   96/  124]
train() client id: f_00002-8-0 loss: 0.957508  [   32/  124]
train() client id: f_00002-8-1 loss: 0.942904  [   64/  124]
train() client id: f_00002-8-2 loss: 0.837994  [   96/  124]
train() client id: f_00002-9-0 loss: 1.047153  [   32/  124]
train() client id: f_00002-9-1 loss: 0.798207  [   64/  124]
train() client id: f_00002-9-2 loss: 0.893930  [   96/  124]
train() client id: f_00002-10-0 loss: 1.105546  [   32/  124]
train() client id: f_00002-10-1 loss: 0.763655  [   64/  124]
train() client id: f_00002-10-2 loss: 0.811043  [   96/  124]
train() client id: f_00003-0-0 loss: 0.589675  [   32/   43]
train() client id: f_00003-1-0 loss: 0.592542  [   32/   43]
train() client id: f_00003-2-0 loss: 0.432026  [   32/   43]
train() client id: f_00003-3-0 loss: 0.605579  [   32/   43]
train() client id: f_00003-4-0 loss: 0.769014  [   32/   43]
train() client id: f_00003-5-0 loss: 0.667937  [   32/   43]
train() client id: f_00003-6-0 loss: 0.537686  [   32/   43]
train() client id: f_00003-7-0 loss: 0.576817  [   32/   43]
train() client id: f_00003-8-0 loss: 0.455790  [   32/   43]
train() client id: f_00003-9-0 loss: 0.544715  [   32/   43]
train() client id: f_00003-10-0 loss: 0.678656  [   32/   43]
train() client id: f_00004-0-0 loss: 0.976469  [   32/  306]
train() client id: f_00004-0-1 loss: 1.049213  [   64/  306]
train() client id: f_00004-0-2 loss: 1.102779  [   96/  306]
train() client id: f_00004-0-3 loss: 1.025480  [  128/  306]
train() client id: f_00004-0-4 loss: 0.845343  [  160/  306]
train() client id: f_00004-0-5 loss: 0.903046  [  192/  306]
train() client id: f_00004-0-6 loss: 0.865641  [  224/  306]
train() client id: f_00004-0-7 loss: 1.142318  [  256/  306]
train() client id: f_00004-0-8 loss: 0.924762  [  288/  306]
train() client id: f_00004-1-0 loss: 0.968964  [   32/  306]
train() client id: f_00004-1-1 loss: 0.993124  [   64/  306]
train() client id: f_00004-1-2 loss: 1.063111  [   96/  306]
train() client id: f_00004-1-3 loss: 0.980315  [  128/  306]
train() client id: f_00004-1-4 loss: 1.028681  [  160/  306]
train() client id: f_00004-1-5 loss: 0.933315  [  192/  306]
train() client id: f_00004-1-6 loss: 1.081551  [  224/  306]
train() client id: f_00004-1-7 loss: 0.795644  [  256/  306]
train() client id: f_00004-1-8 loss: 0.864413  [  288/  306]
train() client id: f_00004-2-0 loss: 0.842816  [   32/  306]
train() client id: f_00004-2-1 loss: 0.946378  [   64/  306]
train() client id: f_00004-2-2 loss: 0.968044  [   96/  306]
train() client id: f_00004-2-3 loss: 0.918060  [  128/  306]
train() client id: f_00004-2-4 loss: 0.915988  [  160/  306]
train() client id: f_00004-2-5 loss: 1.093071  [  192/  306]
train() client id: f_00004-2-6 loss: 0.970340  [  224/  306]
train() client id: f_00004-2-7 loss: 1.024605  [  256/  306]
train() client id: f_00004-2-8 loss: 0.978622  [  288/  306]
train() client id: f_00004-3-0 loss: 0.959232  [   32/  306]
train() client id: f_00004-3-1 loss: 0.922035  [   64/  306]
train() client id: f_00004-3-2 loss: 1.005238  [   96/  306]
train() client id: f_00004-3-3 loss: 0.901740  [  128/  306]
train() client id: f_00004-3-4 loss: 0.968559  [  160/  306]
train() client id: f_00004-3-5 loss: 1.050264  [  192/  306]
train() client id: f_00004-3-6 loss: 0.928843  [  224/  306]
train() client id: f_00004-3-7 loss: 0.971237  [  256/  306]
train() client id: f_00004-3-8 loss: 0.954034  [  288/  306]
train() client id: f_00004-4-0 loss: 0.957109  [   32/  306]
train() client id: f_00004-4-1 loss: 0.939697  [   64/  306]
train() client id: f_00004-4-2 loss: 0.865526  [   96/  306]
train() client id: f_00004-4-3 loss: 0.823960  [  128/  306]
train() client id: f_00004-4-4 loss: 0.998208  [  160/  306]
train() client id: f_00004-4-5 loss: 0.867685  [  192/  306]
train() client id: f_00004-4-6 loss: 1.030256  [  224/  306]
train() client id: f_00004-4-7 loss: 1.026872  [  256/  306]
train() client id: f_00004-4-8 loss: 0.992303  [  288/  306]
train() client id: f_00004-5-0 loss: 0.947539  [   32/  306]
train() client id: f_00004-5-1 loss: 1.038137  [   64/  306]
train() client id: f_00004-5-2 loss: 0.894371  [   96/  306]
train() client id: f_00004-5-3 loss: 0.889785  [  128/  306]
train() client id: f_00004-5-4 loss: 0.971459  [  160/  306]
train() client id: f_00004-5-5 loss: 0.980257  [  192/  306]
train() client id: f_00004-5-6 loss: 1.040905  [  224/  306]
train() client id: f_00004-5-7 loss: 1.029282  [  256/  306]
train() client id: f_00004-5-8 loss: 0.831818  [  288/  306]
train() client id: f_00004-6-0 loss: 0.893090  [   32/  306]
train() client id: f_00004-6-1 loss: 0.958019  [   64/  306]
train() client id: f_00004-6-2 loss: 0.937527  [   96/  306]
train() client id: f_00004-6-3 loss: 1.014677  [  128/  306]
train() client id: f_00004-6-4 loss: 1.016348  [  160/  306]
train() client id: f_00004-6-5 loss: 0.918173  [  192/  306]
train() client id: f_00004-6-6 loss: 0.956070  [  224/  306]
train() client id: f_00004-6-7 loss: 0.945321  [  256/  306]
train() client id: f_00004-6-8 loss: 0.965417  [  288/  306]
train() client id: f_00004-7-0 loss: 0.921344  [   32/  306]
train() client id: f_00004-7-1 loss: 0.936338  [   64/  306]
train() client id: f_00004-7-2 loss: 0.957433  [   96/  306]
train() client id: f_00004-7-3 loss: 1.061325  [  128/  306]
train() client id: f_00004-7-4 loss: 0.880344  [  160/  306]
train() client id: f_00004-7-5 loss: 0.947162  [  192/  306]
train() client id: f_00004-7-6 loss: 1.010787  [  224/  306]
train() client id: f_00004-7-7 loss: 0.931116  [  256/  306]
train() client id: f_00004-7-8 loss: 0.903660  [  288/  306]
train() client id: f_00004-8-0 loss: 1.031797  [   32/  306]
train() client id: f_00004-8-1 loss: 0.912009  [   64/  306]
train() client id: f_00004-8-2 loss: 0.978439  [   96/  306]
train() client id: f_00004-8-3 loss: 0.928056  [  128/  306]
train() client id: f_00004-8-4 loss: 0.880447  [  160/  306]
train() client id: f_00004-8-5 loss: 0.968025  [  192/  306]
train() client id: f_00004-8-6 loss: 0.947156  [  224/  306]
train() client id: f_00004-8-7 loss: 1.017103  [  256/  306]
train() client id: f_00004-8-8 loss: 0.829804  [  288/  306]
train() client id: f_00004-9-0 loss: 0.811946  [   32/  306]
train() client id: f_00004-9-1 loss: 1.031685  [   64/  306]
train() client id: f_00004-9-2 loss: 0.896484  [   96/  306]
train() client id: f_00004-9-3 loss: 0.951521  [  128/  306]
train() client id: f_00004-9-4 loss: 0.979528  [  160/  306]
train() client id: f_00004-9-5 loss: 0.878549  [  192/  306]
train() client id: f_00004-9-6 loss: 0.820818  [  224/  306]
train() client id: f_00004-9-7 loss: 1.079684  [  256/  306]
train() client id: f_00004-9-8 loss: 1.065639  [  288/  306]
train() client id: f_00004-10-0 loss: 0.941991  [   32/  306]
train() client id: f_00004-10-1 loss: 0.903447  [   64/  306]
train() client id: f_00004-10-2 loss: 1.229295  [   96/  306]
train() client id: f_00004-10-3 loss: 0.884066  [  128/  306]
train() client id: f_00004-10-4 loss: 1.114009  [  160/  306]
train() client id: f_00004-10-5 loss: 0.803852  [  192/  306]
train() client id: f_00004-10-6 loss: 0.916265  [  224/  306]
train() client id: f_00004-10-7 loss: 0.767927  [  256/  306]
train() client id: f_00004-10-8 loss: 0.946626  [  288/  306]
train() client id: f_00005-0-0 loss: 0.613093  [   32/  146]
train() client id: f_00005-0-1 loss: 0.416902  [   64/  146]
train() client id: f_00005-0-2 loss: 0.322575  [   96/  146]
train() client id: f_00005-0-3 loss: 0.570796  [  128/  146]
train() client id: f_00005-1-0 loss: 0.707776  [   32/  146]
train() client id: f_00005-1-1 loss: 0.397283  [   64/  146]
train() client id: f_00005-1-2 loss: 0.569832  [   96/  146]
train() client id: f_00005-1-3 loss: 0.435590  [  128/  146]
train() client id: f_00005-2-0 loss: 0.389395  [   32/  146]
train() client id: f_00005-2-1 loss: 0.503073  [   64/  146]
train() client id: f_00005-2-2 loss: 0.647280  [   96/  146]
train() client id: f_00005-2-3 loss: 0.468687  [  128/  146]
train() client id: f_00005-3-0 loss: 0.539391  [   32/  146]
train() client id: f_00005-3-1 loss: 0.277553  [   64/  146]
train() client id: f_00005-3-2 loss: 0.491723  [   96/  146]
train() client id: f_00005-3-3 loss: 0.641825  [  128/  146]
train() client id: f_00005-4-0 loss: 0.788264  [   32/  146]
train() client id: f_00005-4-1 loss: 0.422859  [   64/  146]
train() client id: f_00005-4-2 loss: 0.468157  [   96/  146]
train() client id: f_00005-4-3 loss: 0.500230  [  128/  146]
train() client id: f_00005-5-0 loss: 0.343588  [   32/  146]
train() client id: f_00005-5-1 loss: 0.435990  [   64/  146]
train() client id: f_00005-5-2 loss: 0.693039  [   96/  146]
train() client id: f_00005-5-3 loss: 0.548878  [  128/  146]
train() client id: f_00005-6-0 loss: 0.368797  [   32/  146]
train() client id: f_00005-6-1 loss: 0.641511  [   64/  146]
train() client id: f_00005-6-2 loss: 0.456938  [   96/  146]
train() client id: f_00005-6-3 loss: 0.554510  [  128/  146]
train() client id: f_00005-7-0 loss: 0.545435  [   32/  146]
train() client id: f_00005-7-1 loss: 0.533605  [   64/  146]
train() client id: f_00005-7-2 loss: 0.670530  [   96/  146]
train() client id: f_00005-7-3 loss: 0.305086  [  128/  146]
train() client id: f_00005-8-0 loss: 0.495548  [   32/  146]
train() client id: f_00005-8-1 loss: 0.552721  [   64/  146]
train() client id: f_00005-8-2 loss: 0.466357  [   96/  146]
train() client id: f_00005-8-3 loss: 0.605162  [  128/  146]
train() client id: f_00005-9-0 loss: 0.540407  [   32/  146]
train() client id: f_00005-9-1 loss: 0.461990  [   64/  146]
train() client id: f_00005-9-2 loss: 0.586621  [   96/  146]
train() client id: f_00005-9-3 loss: 0.462359  [  128/  146]
train() client id: f_00005-10-0 loss: 0.314760  [   32/  146]
train() client id: f_00005-10-1 loss: 0.536049  [   64/  146]
train() client id: f_00005-10-2 loss: 0.671491  [   96/  146]
train() client id: f_00005-10-3 loss: 0.625717  [  128/  146]
train() client id: f_00006-0-0 loss: 0.508513  [   32/   54]
train() client id: f_00006-1-0 loss: 0.568286  [   32/   54]
train() client id: f_00006-2-0 loss: 0.457000  [   32/   54]
train() client id: f_00006-3-0 loss: 0.561618  [   32/   54]
train() client id: f_00006-4-0 loss: 0.488286  [   32/   54]
train() client id: f_00006-5-0 loss: 0.536056  [   32/   54]
train() client id: f_00006-6-0 loss: 0.558015  [   32/   54]
train() client id: f_00006-7-0 loss: 0.518734  [   32/   54]
train() client id: f_00006-8-0 loss: 0.460264  [   32/   54]
train() client id: f_00006-9-0 loss: 0.522912  [   32/   54]
train() client id: f_00006-10-0 loss: 0.560827  [   32/   54]
train() client id: f_00007-0-0 loss: 0.538617  [   32/  179]
train() client id: f_00007-0-1 loss: 0.881237  [   64/  179]
train() client id: f_00007-0-2 loss: 0.762694  [   96/  179]
train() client id: f_00007-0-3 loss: 0.588647  [  128/  179]
train() client id: f_00007-0-4 loss: 0.523207  [  160/  179]
train() client id: f_00007-1-0 loss: 0.534012  [   32/  179]
train() client id: f_00007-1-1 loss: 0.686362  [   64/  179]
train() client id: f_00007-1-2 loss: 0.518965  [   96/  179]
train() client id: f_00007-1-3 loss: 0.756514  [  128/  179]
train() client id: f_00007-1-4 loss: 0.732582  [  160/  179]
train() client id: f_00007-2-0 loss: 0.929559  [   32/  179]
train() client id: f_00007-2-1 loss: 0.619225  [   64/  179]
train() client id: f_00007-2-2 loss: 0.495866  [   96/  179]
train() client id: f_00007-2-3 loss: 0.551341  [  128/  179]
train() client id: f_00007-2-4 loss: 0.642791  [  160/  179]
train() client id: f_00007-3-0 loss: 0.755383  [   32/  179]
train() client id: f_00007-3-1 loss: 0.603275  [   64/  179]
train() client id: f_00007-3-2 loss: 0.665053  [   96/  179]
train() client id: f_00007-3-3 loss: 0.682928  [  128/  179]
train() client id: f_00007-3-4 loss: 0.488923  [  160/  179]
train() client id: f_00007-4-0 loss: 0.550665  [   32/  179]
train() client id: f_00007-4-1 loss: 0.785369  [   64/  179]
train() client id: f_00007-4-2 loss: 0.748115  [   96/  179]
train() client id: f_00007-4-3 loss: 0.470605  [  128/  179]
train() client id: f_00007-4-4 loss: 0.436271  [  160/  179]
train() client id: f_00007-5-0 loss: 0.619625  [   32/  179]
train() client id: f_00007-5-1 loss: 0.482320  [   64/  179]
train() client id: f_00007-5-2 loss: 0.552370  [   96/  179]
train() client id: f_00007-5-3 loss: 0.828976  [  128/  179]
train() client id: f_00007-5-4 loss: 0.649874  [  160/  179]
train() client id: f_00007-6-0 loss: 0.704587  [   32/  179]
train() client id: f_00007-6-1 loss: 0.610031  [   64/  179]
train() client id: f_00007-6-2 loss: 0.451411  [   96/  179]
train() client id: f_00007-6-3 loss: 0.691744  [  128/  179]
train() client id: f_00007-6-4 loss: 0.706084  [  160/  179]
train() client id: f_00007-7-0 loss: 0.752461  [   32/  179]
train() client id: f_00007-7-1 loss: 0.646807  [   64/  179]
train() client id: f_00007-7-2 loss: 0.463144  [   96/  179]
train() client id: f_00007-7-3 loss: 0.464054  [  128/  179]
train() client id: f_00007-7-4 loss: 0.824958  [  160/  179]
train() client id: f_00007-8-0 loss: 0.494708  [   32/  179]
train() client id: f_00007-8-1 loss: 0.534286  [   64/  179]
train() client id: f_00007-8-2 loss: 0.849475  [   96/  179]
train() client id: f_00007-8-3 loss: 0.537790  [  128/  179]
train() client id: f_00007-8-4 loss: 0.561591  [  160/  179]
train() client id: f_00007-9-0 loss: 0.763041  [   32/  179]
train() client id: f_00007-9-1 loss: 0.481851  [   64/  179]
train() client id: f_00007-9-2 loss: 0.726174  [   96/  179]
train() client id: f_00007-9-3 loss: 0.729895  [  128/  179]
train() client id: f_00007-9-4 loss: 0.469187  [  160/  179]
train() client id: f_00007-10-0 loss: 0.514530  [   32/  179]
train() client id: f_00007-10-1 loss: 0.462897  [   64/  179]
train() client id: f_00007-10-2 loss: 0.922883  [   96/  179]
train() client id: f_00007-10-3 loss: 0.465728  [  128/  179]
train() client id: f_00007-10-4 loss: 0.660737  [  160/  179]
train() client id: f_00008-0-0 loss: 0.532263  [   32/  130]
train() client id: f_00008-0-1 loss: 0.562552  [   64/  130]
train() client id: f_00008-0-2 loss: 0.550376  [   96/  130]
train() client id: f_00008-0-3 loss: 0.570512  [  128/  130]
train() client id: f_00008-1-0 loss: 0.554233  [   32/  130]
train() client id: f_00008-1-1 loss: 0.522320  [   64/  130]
train() client id: f_00008-1-2 loss: 0.525741  [   96/  130]
train() client id: f_00008-1-3 loss: 0.653933  [  128/  130]
train() client id: f_00008-2-0 loss: 0.498966  [   32/  130]
train() client id: f_00008-2-1 loss: 0.579626  [   64/  130]
train() client id: f_00008-2-2 loss: 0.648999  [   96/  130]
train() client id: f_00008-2-3 loss: 0.525943  [  128/  130]
train() client id: f_00008-3-0 loss: 0.544370  [   32/  130]
train() client id: f_00008-3-1 loss: 0.710153  [   64/  130]
train() client id: f_00008-3-2 loss: 0.587611  [   96/  130]
train() client id: f_00008-3-3 loss: 0.407138  [  128/  130]
train() client id: f_00008-4-0 loss: 0.657701  [   32/  130]
train() client id: f_00008-4-1 loss: 0.540039  [   64/  130]
train() client id: f_00008-4-2 loss: 0.531860  [   96/  130]
train() client id: f_00008-4-3 loss: 0.513950  [  128/  130]
train() client id: f_00008-5-0 loss: 0.484624  [   32/  130]
train() client id: f_00008-5-1 loss: 0.554603  [   64/  130]
train() client id: f_00008-5-2 loss: 0.670061  [   96/  130]
train() client id: f_00008-5-3 loss: 0.512338  [  128/  130]
train() client id: f_00008-6-0 loss: 0.517326  [   32/  130]
train() client id: f_00008-6-1 loss: 0.516058  [   64/  130]
train() client id: f_00008-6-2 loss: 0.621169  [   96/  130]
train() client id: f_00008-6-3 loss: 0.608229  [  128/  130]
train() client id: f_00008-7-0 loss: 0.593092  [   32/  130]
train() client id: f_00008-7-1 loss: 0.688239  [   64/  130]
train() client id: f_00008-7-2 loss: 0.487811  [   96/  130]
train() client id: f_00008-7-3 loss: 0.498696  [  128/  130]
train() client id: f_00008-8-0 loss: 0.488431  [   32/  130]
train() client id: f_00008-8-1 loss: 0.552111  [   64/  130]
train() client id: f_00008-8-2 loss: 0.583125  [   96/  130]
train() client id: f_00008-8-3 loss: 0.617090  [  128/  130]
train() client id: f_00008-9-0 loss: 0.591277  [   32/  130]
train() client id: f_00008-9-1 loss: 0.681716  [   64/  130]
train() client id: f_00008-9-2 loss: 0.557476  [   96/  130]
train() client id: f_00008-9-3 loss: 0.451332  [  128/  130]
train() client id: f_00008-10-0 loss: 0.559703  [   32/  130]
train() client id: f_00008-10-1 loss: 0.672676  [   64/  130]
train() client id: f_00008-10-2 loss: 0.595242  [   96/  130]
train() client id: f_00008-10-3 loss: 0.460049  [  128/  130]
train() client id: f_00009-0-0 loss: 1.147171  [   32/  118]
train() client id: f_00009-0-1 loss: 1.154402  [   64/  118]
train() client id: f_00009-0-2 loss: 0.925126  [   96/  118]
train() client id: f_00009-1-0 loss: 1.016380  [   32/  118]
train() client id: f_00009-1-1 loss: 1.029655  [   64/  118]
train() client id: f_00009-1-2 loss: 1.010635  [   96/  118]
train() client id: f_00009-2-0 loss: 1.028613  [   32/  118]
train() client id: f_00009-2-1 loss: 1.001617  [   64/  118]
train() client id: f_00009-2-2 loss: 0.858735  [   96/  118]
train() client id: f_00009-3-0 loss: 0.946586  [   32/  118]
train() client id: f_00009-3-1 loss: 1.015861  [   64/  118]
train() client id: f_00009-3-2 loss: 0.930542  [   96/  118]
train() client id: f_00009-4-0 loss: 0.910388  [   32/  118]
train() client id: f_00009-4-1 loss: 0.855171  [   64/  118]
train() client id: f_00009-4-2 loss: 0.995155  [   96/  118]
train() client id: f_00009-5-0 loss: 0.970390  [   32/  118]
train() client id: f_00009-5-1 loss: 0.831390  [   64/  118]
train() client id: f_00009-5-2 loss: 0.780751  [   96/  118]
train() client id: f_00009-6-0 loss: 0.975845  [   32/  118]
train() client id: f_00009-6-1 loss: 0.819950  [   64/  118]
train() client id: f_00009-6-2 loss: 0.837735  [   96/  118]
train() client id: f_00009-7-0 loss: 0.900200  [   32/  118]
train() client id: f_00009-7-1 loss: 0.979014  [   64/  118]
train() client id: f_00009-7-2 loss: 0.813295  [   96/  118]
train() client id: f_00009-8-0 loss: 1.011331  [   32/  118]
train() client id: f_00009-8-1 loss: 0.785001  [   64/  118]
train() client id: f_00009-8-2 loss: 0.757682  [   96/  118]
train() client id: f_00009-9-0 loss: 0.945582  [   32/  118]
train() client id: f_00009-9-1 loss: 0.898718  [   64/  118]
train() client id: f_00009-9-2 loss: 0.752946  [   96/  118]
train() client id: f_00009-10-0 loss: 0.828458  [   32/  118]
train() client id: f_00009-10-1 loss: 0.811240  [   64/  118]
train() client id: f_00009-10-2 loss: 0.942701  [   96/  118]
At round 54 accuracy: 0.6472148541114059
At round 54 training accuracy: 0.5928906773977196
At round 54 training loss: 0.8175726782389796
update_location
xs = [  -3.9056584     4.20031788  290.00902392   18.81129433    0.97929623
    3.95640986 -252.44319194 -231.32485185  274.66397685 -217.06087855]
ys = [ 282.5879595   265.55583871    1.32061395 -252.45517586  244.35018685
  227.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [299.78527152 283.79137785 306.76860657 272.19015525 264.02267484
 248.82712967 271.54089139 252.01560138 292.82925085 239.02183343]
dists_bs = [202.10997688 200.78125871 496.38485217 469.34075593 189.15115944
 186.58706051 193.77327739 183.05830601 476.42650004 176.09545916]
uav_gains = [2.68378500e-12 3.69674554e-12 2.35370123e-12 4.70996853e-12
 5.58646308e-12 7.59172095e-12 4.77454866e-12 7.13118064e-12
 3.07591018e-12 9.13258179e-12]
bs_gains = [3.86944233e-11 3.94156962e-11 3.12609541e-12 3.65701662e-12
 4.65831512e-11 4.83978224e-11 4.35382740e-11 5.10556319e-11
 3.50675591e-12 5.69113968e-11]
Round 55
-------------------------------
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.74701293 7.6136546  3.69953246 1.35283391 8.77808353 4.22374522
 1.66527442 5.20651362 3.84415712 3.42461541]
obj_prev = 43.55542321522954
eta_min = 2.2364048538474024e-25	eta_max = 0.9376658335407816
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 10.034905168858742	eta = 0.909090909090909
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 21.758383945021716	eta = 0.4192701574551497
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 15.56372063459429	eta = 0.5861478290943805
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.453751471416155	eta = 0.6311607806900414
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387691546337155	eta = 0.6340587044987988
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387430378338086	eta = 0.6340702142568856
eta = 0.6340702142568856
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.03759466 0.07906812 0.03699791 0.01282992 0.09130131 0.04356206
 0.016112   0.05340828 0.03878814 0.03520769]
ene_total = [1.39513647 2.21195089 1.40544566 0.67656761 2.51756531 1.29725943
 0.75865034 1.66646122 1.38042471 1.07796874]
ti_comp = [0.89681784 0.99162304 0.88681254 0.93061922 0.99425194 0.99482833
 0.93128983 0.94868174 0.91451495 0.99717632]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [4.12904738e-06 3.14189100e-05 4.02483533e-06 1.52408097e-07
 4.81191659e-05 5.22046259e-06 3.01410426e-07 1.05794898e-05
 4.36109102e-06 2.74314532e-06]
ene_total = [0.43540987 0.19405873 0.46095183 0.34900978 0.18777323 0.1852064
 0.34730146 0.30316045 0.39023318 0.17914848]
optimize_network iter = 0 obj = 3.032253401302107
eta = 0.6340702142568856
freqs = [20960028.75390409 39868035.25301737 20860054.05324307  6893217.87674232
 45914572.57827595 21894260.42256957  8650365.72831919 28148681.6418485
 21206945.13364052 17653694.93957705]
eta_min = 0.6340702142568873	eta_max = 0.7017829950267004
af = 0.0022772242984259367	bf = 1.1085543441262717	zeta = 0.0025049467282685306	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.63792940e-07 6.57280727e-06 8.41991876e-07 3.18836346e-08
 1.00664856e-05 1.09211601e-06 6.30547859e-08 2.21321959e-06
 9.12336258e-07 5.73863496e-07]
ene_total = [1.75228827 0.77857718 1.85510894 1.40483142 0.75191939 0.74507368
 1.39794292 1.21943002 1.57042295 0.72089048]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 1 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
eta_min = 0.7017829950267082	eta_max = 0.7017829950267004
af = 0.0021615562993590274	bf = 1.1085543441262717	zeta = 0.0023777119292949303	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.58297057e-07 6.19209011e-06 8.41991876e-07 3.10406904e-08
 9.47094369e-06 1.02721042e-06 6.13639241e-08 2.13277124e-06
 8.96703470e-07 5.39129116e-07]
ene_total = [1.75228771 0.77853806 1.85510894 1.40483134 0.75185818 0.74506701
 1.39794275 1.21942176 1.57042134 0.72088691]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 2 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
Done!
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.02513636e-06 2.18245150e-05 2.96766745e-06 1.09405386e-07
 3.33810956e-05 3.62048496e-06 2.16282040e-07 7.51712216e-06
 3.16050283e-06 1.90020353e-06]
ene_total = [0.01705301 0.00759129 0.01805348 0.01366996 0.00733996 0.00725256
 0.013603   0.01187111 0.01528343 0.00701604]
At round 55 energy consumption: 0.11873383165749558
At round 55 eta: 0.7017829950267004
At round 55 a_n: 9.342581272941924
At round 55 local rounds: 11.596049668428781
At round 55 global rounds: 31.328130579872187
gradient difference: 0.4756650924682617
train() client id: f_00000-0-0 loss: 1.419495  [   32/  126]
train() client id: f_00000-0-1 loss: 1.260014  [   64/  126]
train() client id: f_00000-0-2 loss: 1.081851  [   96/  126]
train() client id: f_00000-1-0 loss: 1.105619  [   32/  126]
train() client id: f_00000-1-1 loss: 1.329162  [   64/  126]
train() client id: f_00000-1-2 loss: 1.063104  [   96/  126]
train() client id: f_00000-2-0 loss: 1.043227  [   32/  126]
train() client id: f_00000-2-1 loss: 1.012698  [   64/  126]
train() client id: f_00000-2-2 loss: 1.058998  [   96/  126]
train() client id: f_00000-3-0 loss: 1.098139  [   32/  126]
train() client id: f_00000-3-1 loss: 0.916738  [   64/  126]
train() client id: f_00000-3-2 loss: 0.941707  [   96/  126]
train() client id: f_00000-4-0 loss: 0.982719  [   32/  126]
train() client id: f_00000-4-1 loss: 1.149010  [   64/  126]
train() client id: f_00000-4-2 loss: 0.980620  [   96/  126]
train() client id: f_00000-5-0 loss: 0.928043  [   32/  126]
train() client id: f_00000-5-1 loss: 0.988463  [   64/  126]
train() client id: f_00000-5-2 loss: 0.867169  [   96/  126]
train() client id: f_00000-6-0 loss: 0.871050  [   32/  126]
train() client id: f_00000-6-1 loss: 1.007262  [   64/  126]
train() client id: f_00000-6-2 loss: 0.841166  [   96/  126]
train() client id: f_00000-7-0 loss: 0.933804  [   32/  126]
train() client id: f_00000-7-1 loss: 0.895218  [   64/  126]
train() client id: f_00000-7-2 loss: 0.915358  [   96/  126]
train() client id: f_00000-8-0 loss: 0.827285  [   32/  126]
train() client id: f_00000-8-1 loss: 0.879776  [   64/  126]
train() client id: f_00000-8-2 loss: 0.882157  [   96/  126]
train() client id: f_00000-9-0 loss: 0.896445  [   32/  126]
train() client id: f_00000-9-1 loss: 0.813542  [   64/  126]
train() client id: f_00000-9-2 loss: 0.826089  [   96/  126]
train() client id: f_00000-10-0 loss: 0.788195  [   32/  126]
train() client id: f_00000-10-1 loss: 0.891888  [   64/  126]
train() client id: f_00000-10-2 loss: 1.038995  [   96/  126]
train() client id: f_00001-0-0 loss: 0.450496  [   32/  265]
train() client id: f_00001-0-1 loss: 0.522102  [   64/  265]
train() client id: f_00001-0-2 loss: 0.538904  [   96/  265]
train() client id: f_00001-0-3 loss: 0.507918  [  128/  265]
train() client id: f_00001-0-4 loss: 0.568826  [  160/  265]
train() client id: f_00001-0-5 loss: 0.441065  [  192/  265]
train() client id: f_00001-0-6 loss: 0.448585  [  224/  265]
train() client id: f_00001-0-7 loss: 0.521796  [  256/  265]
train() client id: f_00001-1-0 loss: 0.463524  [   32/  265]
train() client id: f_00001-1-1 loss: 0.488763  [   64/  265]
train() client id: f_00001-1-2 loss: 0.456447  [   96/  265]
train() client id: f_00001-1-3 loss: 0.532486  [  128/  265]
train() client id: f_00001-1-4 loss: 0.595910  [  160/  265]
train() client id: f_00001-1-5 loss: 0.441917  [  192/  265]
train() client id: f_00001-1-6 loss: 0.486775  [  224/  265]
train() client id: f_00001-1-7 loss: 0.471562  [  256/  265]
train() client id: f_00001-2-0 loss: 0.505221  [   32/  265]
train() client id: f_00001-2-1 loss: 0.488559  [   64/  265]
train() client id: f_00001-2-2 loss: 0.424793  [   96/  265]
train() client id: f_00001-2-3 loss: 0.437202  [  128/  265]
train() client id: f_00001-2-4 loss: 0.644106  [  160/  265]
train() client id: f_00001-2-5 loss: 0.570319  [  192/  265]
train() client id: f_00001-2-6 loss: 0.417379  [  224/  265]
train() client id: f_00001-2-7 loss: 0.402354  [  256/  265]
train() client id: f_00001-3-0 loss: 0.626724  [   32/  265]
train() client id: f_00001-3-1 loss: 0.461006  [   64/  265]
train() client id: f_00001-3-2 loss: 0.413392  [   96/  265]
train() client id: f_00001-3-3 loss: 0.436472  [  128/  265]
train() client id: f_00001-3-4 loss: 0.398619  [  160/  265]
train() client id: f_00001-3-5 loss: 0.524857  [  192/  265]
train() client id: f_00001-3-6 loss: 0.472050  [  224/  265]
train() client id: f_00001-3-7 loss: 0.503003  [  256/  265]
train() client id: f_00001-4-0 loss: 0.462496  [   32/  265]
train() client id: f_00001-4-1 loss: 0.447641  [   64/  265]
train() client id: f_00001-4-2 loss: 0.464470  [   96/  265]
train() client id: f_00001-4-3 loss: 0.433057  [  128/  265]
train() client id: f_00001-4-4 loss: 0.448587  [  160/  265]
train() client id: f_00001-4-5 loss: 0.439439  [  192/  265]
train() client id: f_00001-4-6 loss: 0.596604  [  224/  265]
train() client id: f_00001-4-7 loss: 0.571015  [  256/  265]
train() client id: f_00001-5-0 loss: 0.408350  [   32/  265]
train() client id: f_00001-5-1 loss: 0.575861  [   64/  265]
train() client id: f_00001-5-2 loss: 0.544353  [   96/  265]
train() client id: f_00001-5-3 loss: 0.389230  [  128/  265]
train() client id: f_00001-5-4 loss: 0.613024  [  160/  265]
train() client id: f_00001-5-5 loss: 0.536372  [  192/  265]
train() client id: f_00001-5-6 loss: 0.379636  [  224/  265]
train() client id: f_00001-5-7 loss: 0.418303  [  256/  265]
train() client id: f_00001-6-0 loss: 0.446729  [   32/  265]
train() client id: f_00001-6-1 loss: 0.456079  [   64/  265]
train() client id: f_00001-6-2 loss: 0.458042  [   96/  265]
train() client id: f_00001-6-3 loss: 0.480137  [  128/  265]
train() client id: f_00001-6-4 loss: 0.500590  [  160/  265]
train() client id: f_00001-6-5 loss: 0.519428  [  192/  265]
train() client id: f_00001-6-6 loss: 0.533185  [  224/  265]
train() client id: f_00001-6-7 loss: 0.456142  [  256/  265]
train() client id: f_00001-7-0 loss: 0.377995  [   32/  265]
train() client id: f_00001-7-1 loss: 0.444768  [   64/  265]
train() client id: f_00001-7-2 loss: 0.466598  [   96/  265]
train() client id: f_00001-7-3 loss: 0.452769  [  128/  265]
train() client id: f_00001-7-4 loss: 0.555336  [  160/  265]
train() client id: f_00001-7-5 loss: 0.449799  [  192/  265]
train() client id: f_00001-7-6 loss: 0.593720  [  224/  265]
train() client id: f_00001-7-7 loss: 0.427947  [  256/  265]
train() client id: f_00001-8-0 loss: 0.531509  [   32/  265]
train() client id: f_00001-8-1 loss: 0.421499  [   64/  265]
train() client id: f_00001-8-2 loss: 0.382587  [   96/  265]
train() client id: f_00001-8-3 loss: 0.477355  [  128/  265]
train() client id: f_00001-8-4 loss: 0.512470  [  160/  265]
train() client id: f_00001-8-5 loss: 0.518869  [  192/  265]
train() client id: f_00001-8-6 loss: 0.441505  [  224/  265]
train() client id: f_00001-8-7 loss: 0.572141  [  256/  265]
train() client id: f_00001-9-0 loss: 0.475225  [   32/  265]
train() client id: f_00001-9-1 loss: 0.477360  [   64/  265]
train() client id: f_00001-9-2 loss: 0.543612  [   96/  265]
train() client id: f_00001-9-3 loss: 0.453274  [  128/  265]
train() client id: f_00001-9-4 loss: 0.462840  [  160/  265]
train() client id: f_00001-9-5 loss: 0.470232  [  192/  265]
train() client id: f_00001-9-6 loss: 0.480607  [  224/  265]
train() client id: f_00001-9-7 loss: 0.503215  [  256/  265]
train() client id: f_00001-10-0 loss: 0.548076  [   32/  265]
train() client id: f_00001-10-1 loss: 0.452417  [   64/  265]
train() client id: f_00001-10-2 loss: 0.449649  [   96/  265]
train() client id: f_00001-10-3 loss: 0.389050  [  128/  265]
train() client id: f_00001-10-4 loss: 0.526219  [  160/  265]
train() client id: f_00001-10-5 loss: 0.483352  [  192/  265]
train() client id: f_00001-10-6 loss: 0.458378  [  224/  265]
train() client id: f_00001-10-7 loss: 0.523208  [  256/  265]
train() client id: f_00002-0-0 loss: 1.164493  [   32/  124]
train() client id: f_00002-0-1 loss: 1.098963  [   64/  124]
train() client id: f_00002-0-2 loss: 1.142613  [   96/  124]
train() client id: f_00002-1-0 loss: 1.133282  [   32/  124]
train() client id: f_00002-1-1 loss: 1.163596  [   64/  124]
train() client id: f_00002-1-2 loss: 1.075971  [   96/  124]
train() client id: f_00002-2-0 loss: 1.169062  [   32/  124]
train() client id: f_00002-2-1 loss: 1.031534  [   64/  124]
train() client id: f_00002-2-2 loss: 1.202321  [   96/  124]
train() client id: f_00002-3-0 loss: 1.104072  [   32/  124]
train() client id: f_00002-3-1 loss: 1.213765  [   64/  124]
train() client id: f_00002-3-2 loss: 0.973028  [   96/  124]
train() client id: f_00002-4-0 loss: 1.008539  [   32/  124]
train() client id: f_00002-4-1 loss: 1.046534  [   64/  124]
train() client id: f_00002-4-2 loss: 1.062134  [   96/  124]
train() client id: f_00002-5-0 loss: 1.078506  [   32/  124]
train() client id: f_00002-5-1 loss: 1.196738  [   64/  124]
train() client id: f_00002-5-2 loss: 1.013459  [   96/  124]
train() client id: f_00002-6-0 loss: 0.906883  [   32/  124]
train() client id: f_00002-6-1 loss: 1.000218  [   64/  124]
train() client id: f_00002-6-2 loss: 1.123863  [   96/  124]
train() client id: f_00002-7-0 loss: 1.195373  [   32/  124]
train() client id: f_00002-7-1 loss: 0.839668  [   64/  124]
train() client id: f_00002-7-2 loss: 0.995491  [   96/  124]
train() client id: f_00002-8-0 loss: 1.007997  [   32/  124]
train() client id: f_00002-8-1 loss: 1.268787  [   64/  124]
train() client id: f_00002-8-2 loss: 0.897297  [   96/  124]
train() client id: f_00002-9-0 loss: 1.206491  [   32/  124]
train() client id: f_00002-9-1 loss: 1.008542  [   64/  124]
train() client id: f_00002-9-2 loss: 0.909910  [   96/  124]
train() client id: f_00002-10-0 loss: 1.000744  [   32/  124]
train() client id: f_00002-10-1 loss: 1.092978  [   64/  124]
train() client id: f_00002-10-2 loss: 0.921737  [   96/  124]
train() client id: f_00003-0-0 loss: 0.454439  [   32/   43]
train() client id: f_00003-1-0 loss: 0.588524  [   32/   43]
train() client id: f_00003-2-0 loss: 0.662922  [   32/   43]
train() client id: f_00003-3-0 loss: 0.686820  [   32/   43]
train() client id: f_00003-4-0 loss: 0.656372  [   32/   43]
train() client id: f_00003-5-0 loss: 0.558929  [   32/   43]
train() client id: f_00003-6-0 loss: 0.610292  [   32/   43]
train() client id: f_00003-7-0 loss: 0.594263  [   32/   43]
train() client id: f_00003-8-0 loss: 0.715006  [   32/   43]
train() client id: f_00003-9-0 loss: 0.539923  [   32/   43]
train() client id: f_00003-10-0 loss: 0.631620  [   32/   43]
train() client id: f_00004-0-0 loss: 0.840506  [   32/  306]
train() client id: f_00004-0-1 loss: 0.818781  [   64/  306]
train() client id: f_00004-0-2 loss: 0.733091  [   96/  306]
train() client id: f_00004-0-3 loss: 0.839454  [  128/  306]
train() client id: f_00004-0-4 loss: 0.760931  [  160/  306]
train() client id: f_00004-0-5 loss: 0.881034  [  192/  306]
train() client id: f_00004-0-6 loss: 0.992596  [  224/  306]
train() client id: f_00004-0-7 loss: 0.718183  [  256/  306]
train() client id: f_00004-0-8 loss: 0.740656  [  288/  306]
train() client id: f_00004-1-0 loss: 0.797103  [   32/  306]
train() client id: f_00004-1-1 loss: 0.891883  [   64/  306]
train() client id: f_00004-1-2 loss: 0.898890  [   96/  306]
train() client id: f_00004-1-3 loss: 0.762633  [  128/  306]
train() client id: f_00004-1-4 loss: 0.680153  [  160/  306]
train() client id: f_00004-1-5 loss: 0.903187  [  192/  306]
train() client id: f_00004-1-6 loss: 0.726808  [  224/  306]
train() client id: f_00004-1-7 loss: 0.825125  [  256/  306]
train() client id: f_00004-1-8 loss: 0.758404  [  288/  306]
train() client id: f_00004-2-0 loss: 0.767796  [   32/  306]
train() client id: f_00004-2-1 loss: 0.791660  [   64/  306]
train() client id: f_00004-2-2 loss: 0.802936  [   96/  306]
train() client id: f_00004-2-3 loss: 0.803001  [  128/  306]
train() client id: f_00004-2-4 loss: 0.949684  [  160/  306]
train() client id: f_00004-2-5 loss: 0.768232  [  192/  306]
train() client id: f_00004-2-6 loss: 0.802481  [  224/  306]
train() client id: f_00004-2-7 loss: 0.706693  [  256/  306]
train() client id: f_00004-2-8 loss: 0.848810  [  288/  306]
train() client id: f_00004-3-0 loss: 0.911954  [   32/  306]
train() client id: f_00004-3-1 loss: 0.773759  [   64/  306]
train() client id: f_00004-3-2 loss: 0.734251  [   96/  306]
train() client id: f_00004-3-3 loss: 0.803333  [  128/  306]
train() client id: f_00004-3-4 loss: 0.862606  [  160/  306]
train() client id: f_00004-3-5 loss: 0.880486  [  192/  306]
train() client id: f_00004-3-6 loss: 0.734071  [  224/  306]
train() client id: f_00004-3-7 loss: 0.676142  [  256/  306]
train() client id: f_00004-3-8 loss: 0.938085  [  288/  306]
train() client id: f_00004-4-0 loss: 0.687208  [   32/  306]
train() client id: f_00004-4-1 loss: 0.804061  [   64/  306]
train() client id: f_00004-4-2 loss: 0.797659  [   96/  306]
train() client id: f_00004-4-3 loss: 0.839442  [  128/  306]
train() client id: f_00004-4-4 loss: 0.736386  [  160/  306]
train() client id: f_00004-4-5 loss: 0.933799  [  192/  306]
train() client id: f_00004-4-6 loss: 0.878831  [  224/  306]
train() client id: f_00004-4-7 loss: 0.823618  [  256/  306]
train() client id: f_00004-4-8 loss: 0.841212  [  288/  306]
train() client id: f_00004-5-0 loss: 0.808438  [   32/  306]
train() client id: f_00004-5-1 loss: 0.752098  [   64/  306]
train() client id: f_00004-5-2 loss: 0.768490  [   96/  306]
train() client id: f_00004-5-3 loss: 0.821743  [  128/  306]
train() client id: f_00004-5-4 loss: 0.958834  [  160/  306]
train() client id: f_00004-5-5 loss: 0.889836  [  192/  306]
train() client id: f_00004-5-6 loss: 0.814216  [  224/  306]
train() client id: f_00004-5-7 loss: 0.744470  [  256/  306]
train() client id: f_00004-5-8 loss: 0.740587  [  288/  306]
train() client id: f_00004-6-0 loss: 0.798427  [   32/  306]
train() client id: f_00004-6-1 loss: 0.833351  [   64/  306]
train() client id: f_00004-6-2 loss: 0.696577  [   96/  306]
train() client id: f_00004-6-3 loss: 0.717948  [  128/  306]
train() client id: f_00004-6-4 loss: 0.756084  [  160/  306]
train() client id: f_00004-6-5 loss: 0.945127  [  192/  306]
train() client id: f_00004-6-6 loss: 0.777905  [  224/  306]
train() client id: f_00004-6-7 loss: 0.860827  [  256/  306]
train() client id: f_00004-6-8 loss: 0.853285  [  288/  306]
train() client id: f_00004-7-0 loss: 0.752839  [   32/  306]
train() client id: f_00004-7-1 loss: 0.787222  [   64/  306]
train() client id: f_00004-7-2 loss: 0.889814  [   96/  306]
train() client id: f_00004-7-3 loss: 0.862569  [  128/  306]
train() client id: f_00004-7-4 loss: 0.805254  [  160/  306]
train() client id: f_00004-7-5 loss: 0.778429  [  192/  306]
train() client id: f_00004-7-6 loss: 0.804790  [  224/  306]
train() client id: f_00004-7-7 loss: 0.712606  [  256/  306]
train() client id: f_00004-7-8 loss: 0.817148  [  288/  306]
train() client id: f_00004-8-0 loss: 0.935105  [   32/  306]
train() client id: f_00004-8-1 loss: 0.798982  [   64/  306]
train() client id: f_00004-8-2 loss: 0.678841  [   96/  306]
train() client id: f_00004-8-3 loss: 0.887325  [  128/  306]
train() client id: f_00004-8-4 loss: 0.822147  [  160/  306]
train() client id: f_00004-8-5 loss: 0.758303  [  192/  306]
train() client id: f_00004-8-6 loss: 0.856236  [  224/  306]
train() client id: f_00004-8-7 loss: 0.856937  [  256/  306]
train() client id: f_00004-8-8 loss: 0.825516  [  288/  306]
train() client id: f_00004-9-0 loss: 0.756873  [   32/  306]
train() client id: f_00004-9-1 loss: 0.794381  [   64/  306]
train() client id: f_00004-9-2 loss: 0.878056  [   96/  306]
train() client id: f_00004-9-3 loss: 0.817071  [  128/  306]
train() client id: f_00004-9-4 loss: 0.734415  [  160/  306]
train() client id: f_00004-9-5 loss: 0.799578  [  192/  306]
train() client id: f_00004-9-6 loss: 0.871266  [  224/  306]
train() client id: f_00004-9-7 loss: 0.890295  [  256/  306]
train() client id: f_00004-9-8 loss: 0.733665  [  288/  306]
train() client id: f_00004-10-0 loss: 0.778718  [   32/  306]
train() client id: f_00004-10-1 loss: 0.789513  [   64/  306]
train() client id: f_00004-10-2 loss: 0.704211  [   96/  306]
train() client id: f_00004-10-3 loss: 0.843399  [  128/  306]
train() client id: f_00004-10-4 loss: 0.837834  [  160/  306]
train() client id: f_00004-10-5 loss: 0.775299  [  192/  306]
train() client id: f_00004-10-6 loss: 0.934825  [  224/  306]
train() client id: f_00004-10-7 loss: 0.766685  [  256/  306]
train() client id: f_00004-10-8 loss: 0.845058  [  288/  306]
train() client id: f_00005-0-0 loss: 0.425446  [   32/  146]
train() client id: f_00005-0-1 loss: 0.320434  [   64/  146]
train() client id: f_00005-0-2 loss: 0.630687  [   96/  146]
train() client id: f_00005-0-3 loss: 0.574184  [  128/  146]
train() client id: f_00005-1-0 loss: 0.934385  [   32/  146]
train() client id: f_00005-1-1 loss: 0.474565  [   64/  146]
train() client id: f_00005-1-2 loss: 0.593891  [   96/  146]
train() client id: f_00005-1-3 loss: 0.319399  [  128/  146]
train() client id: f_00005-2-0 loss: 0.666259  [   32/  146]
train() client id: f_00005-2-1 loss: 0.471539  [   64/  146]
train() client id: f_00005-2-2 loss: 0.489395  [   96/  146]
train() client id: f_00005-2-3 loss: 0.532674  [  128/  146]
train() client id: f_00005-3-0 loss: 0.512482  [   32/  146]
train() client id: f_00005-3-1 loss: 0.617233  [   64/  146]
train() client id: f_00005-3-2 loss: 0.658515  [   96/  146]
train() client id: f_00005-3-3 loss: 0.446777  [  128/  146]
train() client id: f_00005-4-0 loss: 0.630727  [   32/  146]
train() client id: f_00005-4-1 loss: 0.360194  [   64/  146]
train() client id: f_00005-4-2 loss: 0.482065  [   96/  146]
train() client id: f_00005-4-3 loss: 0.676540  [  128/  146]
train() client id: f_00005-5-0 loss: 0.588825  [   32/  146]
train() client id: f_00005-5-1 loss: 0.631783  [   64/  146]
train() client id: f_00005-5-2 loss: 0.548255  [   96/  146]
train() client id: f_00005-5-3 loss: 0.473998  [  128/  146]
train() client id: f_00005-6-0 loss: 0.707590  [   32/  146]
train() client id: f_00005-6-1 loss: 0.498686  [   64/  146]
train() client id: f_00005-6-2 loss: 0.427563  [   96/  146]
train() client id: f_00005-6-3 loss: 0.475489  [  128/  146]
train() client id: f_00005-7-0 loss: 0.213648  [   32/  146]
train() client id: f_00005-7-1 loss: 0.618679  [   64/  146]
train() client id: f_00005-7-2 loss: 0.787794  [   96/  146]
train() client id: f_00005-7-3 loss: 0.706703  [  128/  146]
train() client id: f_00005-8-0 loss: 0.179113  [   32/  146]
train() client id: f_00005-8-1 loss: 0.595642  [   64/  146]
train() client id: f_00005-8-2 loss: 0.657878  [   96/  146]
train() client id: f_00005-8-3 loss: 0.685880  [  128/  146]
train() client id: f_00005-9-0 loss: 0.860043  [   32/  146]
train() client id: f_00005-9-1 loss: 0.352131  [   64/  146]
train() client id: f_00005-9-2 loss: 0.298001  [   96/  146]
train() client id: f_00005-9-3 loss: 0.655386  [  128/  146]
train() client id: f_00005-10-0 loss: 0.337434  [   32/  146]
train() client id: f_00005-10-1 loss: 0.455700  [   64/  146]
train() client id: f_00005-10-2 loss: 0.433135  [   96/  146]
train() client id: f_00005-10-3 loss: 0.785773  [  128/  146]
train() client id: f_00006-0-0 loss: 0.450779  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505381  [   32/   54]
train() client id: f_00006-2-0 loss: 0.459048  [   32/   54]
train() client id: f_00006-3-0 loss: 0.449192  [   32/   54]
train() client id: f_00006-4-0 loss: 0.460391  [   32/   54]
train() client id: f_00006-5-0 loss: 0.474143  [   32/   54]
train() client id: f_00006-6-0 loss: 0.456003  [   32/   54]
train() client id: f_00006-7-0 loss: 0.388498  [   32/   54]
train() client id: f_00006-8-0 loss: 0.428950  [   32/   54]
train() client id: f_00006-9-0 loss: 0.444081  [   32/   54]
train() client id: f_00006-10-0 loss: 0.438796  [   32/   54]
train() client id: f_00007-0-0 loss: 0.451749  [   32/  179]
train() client id: f_00007-0-1 loss: 0.426816  [   64/  179]
train() client id: f_00007-0-2 loss: 0.699007  [   96/  179]
train() client id: f_00007-0-3 loss: 0.665840  [  128/  179]
train() client id: f_00007-0-4 loss: 0.652795  [  160/  179]
train() client id: f_00007-1-0 loss: 0.421838  [   32/  179]
train() client id: f_00007-1-1 loss: 0.633118  [   64/  179]
train() client id: f_00007-1-2 loss: 0.630251  [   96/  179]
train() client id: f_00007-1-3 loss: 0.596325  [  128/  179]
train() client id: f_00007-1-4 loss: 0.529240  [  160/  179]
train() client id: f_00007-2-0 loss: 0.483897  [   32/  179]
train() client id: f_00007-2-1 loss: 0.649014  [   64/  179]
train() client id: f_00007-2-2 loss: 0.402890  [   96/  179]
train() client id: f_00007-2-3 loss: 0.423538  [  128/  179]
train() client id: f_00007-2-4 loss: 0.824147  [  160/  179]
train() client id: f_00007-3-0 loss: 0.582377  [   32/  179]
train() client id: f_00007-3-1 loss: 0.419420  [   64/  179]
train() client id: f_00007-3-2 loss: 0.587009  [   96/  179]
train() client id: f_00007-3-3 loss: 0.389360  [  128/  179]
train() client id: f_00007-3-4 loss: 0.390743  [  160/  179]
train() client id: f_00007-4-0 loss: 0.645581  [   32/  179]
train() client id: f_00007-4-1 loss: 0.485513  [   64/  179]
train() client id: f_00007-4-2 loss: 0.728934  [   96/  179]
train() client id: f_00007-4-3 loss: 0.442243  [  128/  179]
train() client id: f_00007-4-4 loss: 0.371664  [  160/  179]
train() client id: f_00007-5-0 loss: 0.437081  [   32/  179]
train() client id: f_00007-5-1 loss: 0.361217  [   64/  179]
train() client id: f_00007-5-2 loss: 0.557183  [   96/  179]
train() client id: f_00007-5-3 loss: 0.673777  [  128/  179]
train() client id: f_00007-5-4 loss: 0.482909  [  160/  179]
train() client id: f_00007-6-0 loss: 0.460704  [   32/  179]
train() client id: f_00007-6-1 loss: 0.614350  [   64/  179]
train() client id: f_00007-6-2 loss: 0.531320  [   96/  179]
train() client id: f_00007-6-3 loss: 0.653234  [  128/  179]
train() client id: f_00007-6-4 loss: 0.423030  [  160/  179]
train() client id: f_00007-7-0 loss: 0.499325  [   32/  179]
train() client id: f_00007-7-1 loss: 0.538004  [   64/  179]
train() client id: f_00007-7-2 loss: 0.444891  [   96/  179]
train() client id: f_00007-7-3 loss: 0.545771  [  128/  179]
train() client id: f_00007-7-4 loss: 0.561356  [  160/  179]
train() client id: f_00007-8-0 loss: 0.505501  [   32/  179]
train() client id: f_00007-8-1 loss: 0.729444  [   64/  179]
train() client id: f_00007-8-2 loss: 0.340528  [   96/  179]
train() client id: f_00007-8-3 loss: 0.477196  [  128/  179]
train() client id: f_00007-8-4 loss: 0.461407  [  160/  179]
train() client id: f_00007-9-0 loss: 0.554334  [   32/  179]
train() client id: f_00007-9-1 loss: 0.382584  [   64/  179]
train() client id: f_00007-9-2 loss: 0.582611  [   96/  179]
train() client id: f_00007-9-3 loss: 0.442539  [  128/  179]
train() client id: f_00007-9-4 loss: 0.479580  [  160/  179]
train() client id: f_00007-10-0 loss: 0.651315  [   32/  179]
train() client id: f_00007-10-1 loss: 0.645162  [   64/  179]
train() client id: f_00007-10-2 loss: 0.471252  [   96/  179]
train() client id: f_00007-10-3 loss: 0.374020  [  128/  179]
train() client id: f_00007-10-4 loss: 0.444498  [  160/  179]
train() client id: f_00008-0-0 loss: 0.803765  [   32/  130]
train() client id: f_00008-0-1 loss: 0.770131  [   64/  130]
train() client id: f_00008-0-2 loss: 0.866637  [   96/  130]
train() client id: f_00008-0-3 loss: 0.761450  [  128/  130]
train() client id: f_00008-1-0 loss: 0.756377  [   32/  130]
train() client id: f_00008-1-1 loss: 0.739281  [   64/  130]
train() client id: f_00008-1-2 loss: 0.830541  [   96/  130]
train() client id: f_00008-1-3 loss: 0.890697  [  128/  130]
train() client id: f_00008-2-0 loss: 0.838180  [   32/  130]
train() client id: f_00008-2-1 loss: 0.836104  [   64/  130]
train() client id: f_00008-2-2 loss: 0.792794  [   96/  130]
train() client id: f_00008-2-3 loss: 0.739086  [  128/  130]
train() client id: f_00008-3-0 loss: 0.721063  [   32/  130]
train() client id: f_00008-3-1 loss: 0.801766  [   64/  130]
train() client id: f_00008-3-2 loss: 0.926780  [   96/  130]
train() client id: f_00008-3-3 loss: 0.763834  [  128/  130]
train() client id: f_00008-4-0 loss: 0.863692  [   32/  130]
train() client id: f_00008-4-1 loss: 0.711815  [   64/  130]
train() client id: f_00008-4-2 loss: 0.787133  [   96/  130]
train() client id: f_00008-4-3 loss: 0.810855  [  128/  130]
train() client id: f_00008-5-0 loss: 0.820538  [   32/  130]
train() client id: f_00008-5-1 loss: 0.912698  [   64/  130]
train() client id: f_00008-5-2 loss: 0.725699  [   96/  130]
train() client id: f_00008-5-3 loss: 0.737177  [  128/  130]
train() client id: f_00008-6-0 loss: 0.793058  [   32/  130]
train() client id: f_00008-6-1 loss: 0.789989  [   64/  130]
train() client id: f_00008-6-2 loss: 0.837643  [   96/  130]
train() client id: f_00008-6-3 loss: 0.743673  [  128/  130]
train() client id: f_00008-7-0 loss: 0.868671  [   32/  130]
train() client id: f_00008-7-1 loss: 0.830352  [   64/  130]
train() client id: f_00008-7-2 loss: 0.791958  [   96/  130]
train() client id: f_00008-7-3 loss: 0.712750  [  128/  130]
train() client id: f_00008-8-0 loss: 0.680458  [   32/  130]
train() client id: f_00008-8-1 loss: 0.770254  [   64/  130]
train() client id: f_00008-8-2 loss: 0.810759  [   96/  130]
train() client id: f_00008-8-3 loss: 0.906539  [  128/  130]
train() client id: f_00008-9-0 loss: 0.793905  [   32/  130]
train() client id: f_00008-9-1 loss: 0.823040  [   64/  130]
train() client id: f_00008-9-2 loss: 0.780702  [   96/  130]
train() client id: f_00008-9-3 loss: 0.797950  [  128/  130]
train() client id: f_00008-10-0 loss: 0.846177  [   32/  130]
train() client id: f_00008-10-1 loss: 0.698423  [   64/  130]
train() client id: f_00008-10-2 loss: 0.750284  [   96/  130]
train() client id: f_00008-10-3 loss: 0.897404  [  128/  130]
train() client id: f_00009-0-0 loss: 0.910915  [   32/  118]
train() client id: f_00009-0-1 loss: 0.971754  [   64/  118]
train() client id: f_00009-0-2 loss: 0.786910  [   96/  118]
train() client id: f_00009-1-0 loss: 0.924101  [   32/  118]
train() client id: f_00009-1-1 loss: 0.751059  [   64/  118]
train() client id: f_00009-1-2 loss: 0.860054  [   96/  118]
train() client id: f_00009-2-0 loss: 0.782456  [   32/  118]
train() client id: f_00009-2-1 loss: 0.834348  [   64/  118]
train() client id: f_00009-2-2 loss: 0.925255  [   96/  118]
train() client id: f_00009-3-0 loss: 0.714974  [   32/  118]
train() client id: f_00009-3-1 loss: 0.913777  [   64/  118]
train() client id: f_00009-3-2 loss: 0.658407  [   96/  118]
train() client id: f_00009-4-0 loss: 0.890765  [   32/  118]
train() client id: f_00009-4-1 loss: 0.778078  [   64/  118]
train() client id: f_00009-4-2 loss: 0.747854  [   96/  118]
train() client id: f_00009-5-0 loss: 0.763382  [   32/  118]
train() client id: f_00009-5-1 loss: 0.735971  [   64/  118]
train() client id: f_00009-5-2 loss: 0.818867  [   96/  118]
train() client id: f_00009-6-0 loss: 0.720544  [   32/  118]
train() client id: f_00009-6-1 loss: 0.766043  [   64/  118]
train() client id: f_00009-6-2 loss: 0.708564  [   96/  118]
train() client id: f_00009-7-0 loss: 0.740902  [   32/  118]
train() client id: f_00009-7-1 loss: 0.750430  [   64/  118]
train() client id: f_00009-7-2 loss: 0.748863  [   96/  118]
train() client id: f_00009-8-0 loss: 0.661300  [   32/  118]
train() client id: f_00009-8-1 loss: 0.720756  [   64/  118]
train() client id: f_00009-8-2 loss: 0.753807  [   96/  118]
train() client id: f_00009-9-0 loss: 0.751525  [   32/  118]
train() client id: f_00009-9-1 loss: 0.714542  [   64/  118]
train() client id: f_00009-9-2 loss: 0.816970  [   96/  118]
train() client id: f_00009-10-0 loss: 0.905496  [   32/  118]
train() client id: f_00009-10-1 loss: 0.668573  [   64/  118]
train() client id: f_00009-10-2 loss: 0.653746  [   96/  118]
At round 55 accuracy: 0.6472148541114059
At round 55 training accuracy: 0.590878604963112
At round 55 training loss: 0.8240655733497717
update_location
xs = [  -3.9056584     4.20031788  295.00902392   18.81129433    0.97929623
    3.95640986 -257.44319194 -236.32485185  279.66397685 -222.06087855]
ys = [ 287.5879595   270.55583871    1.32061395 -257.45517586  249.35018685
  232.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [304.50301906 288.47548341 311.49970821 276.83394368 268.65679724
 253.41286859 276.19537943 256.61276636 297.52413334 243.57143847]
dists_bs = [204.81533719 203.08538164 501.07186281 473.89903665 191.04099818
 188.06348039 195.82521531 184.65804049 481.14846951 177.35619392]
uav_gains = [2.45442945e-12 3.35809681e-12 2.16122255e-12 4.27299009e-12
 5.07197201e-12 6.93588738e-12 4.33052174e-12 6.50389323e-12
 2.80382037e-12 8.39484877e-12]
bs_gains = [3.72802792e-11 3.81762994e-11 3.04490705e-12 3.55937522e-12
 4.53043251e-11 4.73414518e-11 4.22728929e-11 4.98268075e-11
 3.41124240e-12 5.57858785e-11]
Round 56
-------------------------------
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.61572451 7.33497615 3.57029297 1.30775167 8.45662183 4.06918315
 1.60870824 5.01890964 3.70473634 3.29930761]
obj_prev = 41.986212112103765
eta_min = 2.748442212366637e-26	eta_max = 0.9375089458427088
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 9.666975258494574	eta = 0.909090909090909
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 21.262798910283387	eta = 0.41331150066296846
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 15.102037364580543	eta = 0.5819187910708925
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 14.000878826985186	eta = 0.6276862641626415
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.93490538719279	eta = 0.6306579830804683
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.934640394529897	eta = 0.6306699761950073
eta = 0.6306699761950073
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.03803835 0.0800013  0.03743457 0.01298134 0.09237885 0.04407619
 0.01630215 0.05403861 0.03924592 0.03562322]
ene_total = [1.3587627  2.13524871 1.36925661 0.66156516 2.4302258  1.25152696
 0.7407784  1.61476761 1.33281256 1.03969587]
ti_comp = [0.93897812 1.03996482 0.92863396 0.97451355 1.04269211 1.04336226
 0.97521529 0.99392524 0.96170282 1.04576067]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [3.90152269e-06 2.95892377e-05 3.80197612e-06 1.43967031e-07
 4.53195465e-05 4.91611198e-06 2.84717213e-07 9.98355239e-06
 4.08490276e-06 2.58353803e-06]
ene_total = [0.43191848 0.18645457 0.45712329 0.34523224 0.18019189 0.17757424
 0.34352564 0.29816856 0.37654616 0.17167282]
optimize_network iter = 0 obj = 2.96840789621886
eta = 0.6306699761950073
freqs = [20255185.25779217 38463462.35602392 20155718.00296714  6660421.60236042
 44298241.69414774 21122186.81412296  8358231.69852659 27184444.26259059
 20404389.67271167 17032203.97092095]
eta_min = 0.6306699761950091	eta_max = 0.7038160577019408
af = 0.0020423891099591697	bf = 1.0953806736156944	zeta = 0.0022466280209550867	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.06674524e-07 6.11783813e-06 7.86092385e-07 2.97664645e-08
 9.37021941e-06 1.01644989e-06 5.88678167e-08 2.06418827e-06
 8.44589985e-07 5.34169471e-07]
ene_total = [1.75441871 0.75517231 1.85682433 1.40253966 0.72849401 0.72103243
 1.3955953  1.21056458 1.52944703 0.69724031]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 1 obj = 3.70059882926673
eta = 0.7038160577019408
freqs = [20185710.85684811 37218665.59464449 20155718.00296712  6563875.93252434
 42834368.86978544 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748624 16456368.51125106]
eta_min = 0.7038160577019519	eta_max = 0.703816057701941
af = 0.0019282538903956608	bf = 1.0953806736156944	zeta = 0.0021210792794352272	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.01150298e-07 5.72826139e-06 7.86092385e-07 2.89097636e-08
 8.76115832e-06 9.50052597e-07 5.71494239e-08 1.98200003e-06
 8.26731677e-07 4.98660964e-07]
ene_total = [1.75441816 0.75513375 1.85682433 1.40253957 0.72843371 0.72102585
 1.39559513 1.21055645 1.52944526 0.6972368 ]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 2 obj = 3.7005988292667333
eta = 0.703816057701941
freqs = [20185710.85684812 37218665.5946445  20155718.00296713  6563875.93252435
 42834368.86978545 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748625 16456368.51125106]
Done!
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.82371805e-06 2.01897137e-05 2.77064524e-06 1.01894765e-07
 3.08794005e-05 3.34853607e-06 2.01427698e-07 6.98571701e-06
 2.91388167e-06 1.75757030e-06]
ene_total = [0.01772335 0.00764205 0.01875771 0.01416708 0.00738001 0.00728546
 0.01409701 0.0122328  0.01545097 0.00704403]
At round 56 energy consumption: 0.12178045716881733
At round 56 eta: 0.703816057701941
At round 56 a_n: 9.000035425972609
At round 56 local rounds: 11.50132443724633
At round 56 global rounds: 30.386642017599986
gradient difference: 0.5383197069168091
train() client id: f_00000-0-0 loss: 0.897402  [   32/  126]
train() client id: f_00000-0-1 loss: 0.858756  [   64/  126]
train() client id: f_00000-0-2 loss: 0.905785  [   96/  126]
train() client id: f_00000-1-0 loss: 0.833623  [   32/  126]
train() client id: f_00000-1-1 loss: 0.757772  [   64/  126]
train() client id: f_00000-1-2 loss: 0.854157  [   96/  126]
train() client id: f_00000-2-0 loss: 0.786196  [   32/  126]
train() client id: f_00000-2-1 loss: 0.709963  [   64/  126]
train() client id: f_00000-2-2 loss: 0.807939  [   96/  126]
train() client id: f_00000-3-0 loss: 0.625080  [   32/  126]
train() client id: f_00000-3-1 loss: 0.657560  [   64/  126]
train() client id: f_00000-3-2 loss: 0.784618  [   96/  126]
train() client id: f_00000-4-0 loss: 0.728366  [   32/  126]
train() client id: f_00000-4-1 loss: 0.656457  [   64/  126]
train() client id: f_00000-4-2 loss: 0.720958  [   96/  126]
train() client id: f_00000-5-0 loss: 0.593816  [   32/  126]
train() client id: f_00000-5-1 loss: 0.704381  [   64/  126]
train() client id: f_00000-5-2 loss: 0.724306  [   96/  126]
train() client id: f_00000-6-0 loss: 0.665597  [   32/  126]
train() client id: f_00000-6-1 loss: 0.763602  [   64/  126]
train() client id: f_00000-6-2 loss: 0.615851  [   96/  126]
train() client id: f_00000-7-0 loss: 0.577972  [   32/  126]
train() client id: f_00000-7-1 loss: 0.628186  [   64/  126]
train() client id: f_00000-7-2 loss: 0.716571  [   96/  126]
train() client id: f_00000-8-0 loss: 0.747851  [   32/  126]
train() client id: f_00000-8-1 loss: 0.646036  [   64/  126]
train() client id: f_00000-8-2 loss: 0.600929  [   96/  126]
train() client id: f_00000-9-0 loss: 0.554888  [   32/  126]
train() client id: f_00000-9-1 loss: 0.680723  [   64/  126]
train() client id: f_00000-9-2 loss: 0.651612  [   96/  126]
train() client id: f_00000-10-0 loss: 0.652586  [   32/  126]
train() client id: f_00000-10-1 loss: 0.666452  [   64/  126]
train() client id: f_00000-10-2 loss: 0.569261  [   96/  126]
train() client id: f_00001-0-0 loss: 0.359767  [   32/  265]
train() client id: f_00001-0-1 loss: 0.490445  [   64/  265]
train() client id: f_00001-0-2 loss: 0.317032  [   96/  265]
train() client id: f_00001-0-3 loss: 0.362585  [  128/  265]
train() client id: f_00001-0-4 loss: 0.493973  [  160/  265]
train() client id: f_00001-0-5 loss: 0.424848  [  192/  265]
train() client id: f_00001-0-6 loss: 0.372915  [  224/  265]
train() client id: f_00001-0-7 loss: 0.571317  [  256/  265]
train() client id: f_00001-1-0 loss: 0.452904  [   32/  265]
train() client id: f_00001-1-1 loss: 0.509502  [   64/  265]
train() client id: f_00001-1-2 loss: 0.472025  [   96/  265]
train() client id: f_00001-1-3 loss: 0.377667  [  128/  265]
train() client id: f_00001-1-4 loss: 0.366029  [  160/  265]
train() client id: f_00001-1-5 loss: 0.309243  [  192/  265]
train() client id: f_00001-1-6 loss: 0.413002  [  224/  265]
train() client id: f_00001-1-7 loss: 0.399924  [  256/  265]
train() client id: f_00001-2-0 loss: 0.350654  [   32/  265]
train() client id: f_00001-2-1 loss: 0.348588  [   64/  265]
train() client id: f_00001-2-2 loss: 0.456444  [   96/  265]
train() client id: f_00001-2-3 loss: 0.330162  [  128/  265]
train() client id: f_00001-2-4 loss: 0.367653  [  160/  265]
train() client id: f_00001-2-5 loss: 0.468936  [  192/  265]
train() client id: f_00001-2-6 loss: 0.345677  [  224/  265]
train() client id: f_00001-2-7 loss: 0.520527  [  256/  265]
train() client id: f_00001-3-0 loss: 0.349645  [   32/  265]
train() client id: f_00001-3-1 loss: 0.351858  [   64/  265]
train() client id: f_00001-3-2 loss: 0.494510  [   96/  265]
train() client id: f_00001-3-3 loss: 0.385016  [  128/  265]
train() client id: f_00001-3-4 loss: 0.420434  [  160/  265]
train() client id: f_00001-3-5 loss: 0.512592  [  192/  265]
train() client id: f_00001-3-6 loss: 0.411020  [  224/  265]
train() client id: f_00001-3-7 loss: 0.307336  [  256/  265]
train() client id: f_00001-4-0 loss: 0.456132  [   32/  265]
train() client id: f_00001-4-1 loss: 0.457822  [   64/  265]
train() client id: f_00001-4-2 loss: 0.386306  [   96/  265]
train() client id: f_00001-4-3 loss: 0.298906  [  128/  265]
train() client id: f_00001-4-4 loss: 0.369326  [  160/  265]
train() client id: f_00001-4-5 loss: 0.305928  [  192/  265]
train() client id: f_00001-4-6 loss: 0.492261  [  224/  265]
train() client id: f_00001-4-7 loss: 0.424044  [  256/  265]
train() client id: f_00001-5-0 loss: 0.502358  [   32/  265]
train() client id: f_00001-5-1 loss: 0.396816  [   64/  265]
train() client id: f_00001-5-2 loss: 0.354331  [   96/  265]
train() client id: f_00001-5-3 loss: 0.379391  [  128/  265]
train() client id: f_00001-5-4 loss: 0.389559  [  160/  265]
train() client id: f_00001-5-5 loss: 0.306632  [  192/  265]
train() client id: f_00001-5-6 loss: 0.426915  [  224/  265]
train() client id: f_00001-5-7 loss: 0.402197  [  256/  265]
train() client id: f_00001-6-0 loss: 0.334927  [   32/  265]
train() client id: f_00001-6-1 loss: 0.299512  [   64/  265]
train() client id: f_00001-6-2 loss: 0.427376  [   96/  265]
train() client id: f_00001-6-3 loss: 0.497608  [  128/  265]
train() client id: f_00001-6-4 loss: 0.461914  [  160/  265]
train() client id: f_00001-6-5 loss: 0.340759  [  192/  265]
train() client id: f_00001-6-6 loss: 0.332157  [  224/  265]
train() client id: f_00001-6-7 loss: 0.442584  [  256/  265]
train() client id: f_00001-7-0 loss: 0.404641  [   32/  265]
train() client id: f_00001-7-1 loss: 0.440357  [   64/  265]
train() client id: f_00001-7-2 loss: 0.405567  [   96/  265]
train() client id: f_00001-7-3 loss: 0.361597  [  128/  265]
train() client id: f_00001-7-4 loss: 0.407315  [  160/  265]
train() client id: f_00001-7-5 loss: 0.372646  [  192/  265]
train() client id: f_00001-7-6 loss: 0.359946  [  224/  265]
train() client id: f_00001-7-7 loss: 0.331189  [  256/  265]
train() client id: f_00001-8-0 loss: 0.337015  [   32/  265]
train() client id: f_00001-8-1 loss: 0.353031  [   64/  265]
train() client id: f_00001-8-2 loss: 0.367768  [   96/  265]
train() client id: f_00001-8-3 loss: 0.397921  [  128/  265]
train() client id: f_00001-8-4 loss: 0.302456  [  160/  265]
train() client id: f_00001-8-5 loss: 0.438249  [  192/  265]
train() client id: f_00001-8-6 loss: 0.296997  [  224/  265]
train() client id: f_00001-8-7 loss: 0.543177  [  256/  265]
train() client id: f_00001-9-0 loss: 0.329023  [   32/  265]
train() client id: f_00001-9-1 loss: 0.295716  [   64/  265]
train() client id: f_00001-9-2 loss: 0.389459  [   96/  265]
train() client id: f_00001-9-3 loss: 0.416270  [  128/  265]
train() client id: f_00001-9-4 loss: 0.305531  [  160/  265]
train() client id: f_00001-9-5 loss: 0.305595  [  192/  265]
train() client id: f_00001-9-6 loss: 0.444692  [  224/  265]
train() client id: f_00001-9-7 loss: 0.556630  [  256/  265]
train() client id: f_00001-10-0 loss: 0.382175  [   32/  265]
train() client id: f_00001-10-1 loss: 0.450234  [   64/  265]
train() client id: f_00001-10-2 loss: 0.469227  [   96/  265]
train() client id: f_00001-10-3 loss: 0.410480  [  128/  265]
train() client id: f_00001-10-4 loss: 0.317024  [  160/  265]
train() client id: f_00001-10-5 loss: 0.324353  [  192/  265]
train() client id: f_00001-10-6 loss: 0.389099  [  224/  265]
train() client id: f_00001-10-7 loss: 0.358936  [  256/  265]
train() client id: f_00002-0-0 loss: 1.273970  [   32/  124]
train() client id: f_00002-0-1 loss: 1.154776  [   64/  124]
train() client id: f_00002-0-2 loss: 1.061968  [   96/  124]
train() client id: f_00002-1-0 loss: 1.319270  [   32/  124]
train() client id: f_00002-1-1 loss: 1.026232  [   64/  124]
train() client id: f_00002-1-2 loss: 1.016486  [   96/  124]
train() client id: f_00002-2-0 loss: 1.112550  [   32/  124]
train() client id: f_00002-2-1 loss: 1.112544  [   64/  124]
train() client id: f_00002-2-2 loss: 1.123575  [   96/  124]
train() client id: f_00002-3-0 loss: 1.038656  [   32/  124]
train() client id: f_00002-3-1 loss: 1.117311  [   64/  124]
train() client id: f_00002-3-2 loss: 1.073687  [   96/  124]
train() client id: f_00002-4-0 loss: 1.048262  [   32/  124]
train() client id: f_00002-4-1 loss: 0.918169  [   64/  124]
train() client id: f_00002-4-2 loss: 1.133397  [   96/  124]
train() client id: f_00002-5-0 loss: 1.192488  [   32/  124]
train() client id: f_00002-5-1 loss: 1.089288  [   64/  124]
train() client id: f_00002-5-2 loss: 0.984131  [   96/  124]
train() client id: f_00002-6-0 loss: 0.982024  [   32/  124]
train() client id: f_00002-6-1 loss: 0.979978  [   64/  124]
train() client id: f_00002-6-2 loss: 1.277439  [   96/  124]
train() client id: f_00002-7-0 loss: 0.969292  [   32/  124]
train() client id: f_00002-7-1 loss: 1.099450  [   64/  124]
train() client id: f_00002-7-2 loss: 0.911685  [   96/  124]
train() client id: f_00002-8-0 loss: 1.163976  [   32/  124]
train() client id: f_00002-8-1 loss: 1.019180  [   64/  124]
train() client id: f_00002-8-2 loss: 1.027683  [   96/  124]
train() client id: f_00002-9-0 loss: 0.961196  [   32/  124]
train() client id: f_00002-9-1 loss: 1.225465  [   64/  124]
train() client id: f_00002-9-2 loss: 0.866452  [   96/  124]
train() client id: f_00002-10-0 loss: 0.980431  [   32/  124]
train() client id: f_00002-10-1 loss: 1.016955  [   64/  124]
train() client id: f_00002-10-2 loss: 1.079933  [   96/  124]
train() client id: f_00003-0-0 loss: 0.581873  [   32/   43]
train() client id: f_00003-1-0 loss: 0.677682  [   32/   43]
train() client id: f_00003-2-0 loss: 0.846336  [   32/   43]
train() client id: f_00003-3-0 loss: 0.602140  [   32/   43]
train() client id: f_00003-4-0 loss: 0.720279  [   32/   43]
train() client id: f_00003-5-0 loss: 0.831702  [   32/   43]
train() client id: f_00003-6-0 loss: 0.627947  [   32/   43]
train() client id: f_00003-7-0 loss: 0.638343  [   32/   43]
train() client id: f_00003-8-0 loss: 0.482602  [   32/   43]
train() client id: f_00003-9-0 loss: 0.548113  [   32/   43]
train() client id: f_00003-10-0 loss: 0.644979  [   32/   43]
train() client id: f_00004-0-0 loss: 0.903943  [   32/  306]
train() client id: f_00004-0-1 loss: 0.890019  [   64/  306]
train() client id: f_00004-0-2 loss: 0.639482  [   96/  306]
train() client id: f_00004-0-3 loss: 0.834681  [  128/  306]
train() client id: f_00004-0-4 loss: 0.853905  [  160/  306]
train() client id: f_00004-0-5 loss: 0.893050  [  192/  306]
train() client id: f_00004-0-6 loss: 0.745680  [  224/  306]
train() client id: f_00004-0-7 loss: 0.809404  [  256/  306]
train() client id: f_00004-0-8 loss: 0.736279  [  288/  306]
train() client id: f_00004-1-0 loss: 0.676488  [   32/  306]
train() client id: f_00004-1-1 loss: 0.846748  [   64/  306]
train() client id: f_00004-1-2 loss: 0.726296  [   96/  306]
train() client id: f_00004-1-3 loss: 0.862668  [  128/  306]
train() client id: f_00004-1-4 loss: 0.847041  [  160/  306]
train() client id: f_00004-1-5 loss: 0.807701  [  192/  306]
train() client id: f_00004-1-6 loss: 0.807779  [  224/  306]
train() client id: f_00004-1-7 loss: 0.780647  [  256/  306]
train() client id: f_00004-1-8 loss: 0.910991  [  288/  306]
train() client id: f_00004-2-0 loss: 0.835197  [   32/  306]
train() client id: f_00004-2-1 loss: 0.832133  [   64/  306]
train() client id: f_00004-2-2 loss: 0.796115  [   96/  306]
train() client id: f_00004-2-3 loss: 0.904644  [  128/  306]
train() client id: f_00004-2-4 loss: 0.832429  [  160/  306]
train() client id: f_00004-2-5 loss: 0.822213  [  192/  306]
train() client id: f_00004-2-6 loss: 0.795882  [  224/  306]
train() client id: f_00004-2-7 loss: 0.826379  [  256/  306]
train() client id: f_00004-2-8 loss: 0.742520  [  288/  306]
train() client id: f_00004-3-0 loss: 0.922421  [   32/  306]
train() client id: f_00004-3-1 loss: 0.756493  [   64/  306]
train() client id: f_00004-3-2 loss: 0.786056  [   96/  306]
train() client id: f_00004-3-3 loss: 0.715532  [  128/  306]
train() client id: f_00004-3-4 loss: 0.955841  [  160/  306]
train() client id: f_00004-3-5 loss: 0.801663  [  192/  306]
train() client id: f_00004-3-6 loss: 0.820225  [  224/  306]
train() client id: f_00004-3-7 loss: 0.779808  [  256/  306]
train() client id: f_00004-3-8 loss: 0.829939  [  288/  306]
train() client id: f_00004-4-0 loss: 0.945377  [   32/  306]
train() client id: f_00004-4-1 loss: 0.825513  [   64/  306]
train() client id: f_00004-4-2 loss: 0.924537  [   96/  306]
train() client id: f_00004-4-3 loss: 0.722804  [  128/  306]
train() client id: f_00004-4-4 loss: 0.834448  [  160/  306]
train() client id: f_00004-4-5 loss: 0.766659  [  192/  306]
train() client id: f_00004-4-6 loss: 0.780868  [  224/  306]
train() client id: f_00004-4-7 loss: 0.726851  [  256/  306]
train() client id: f_00004-4-8 loss: 0.875491  [  288/  306]
train() client id: f_00004-5-0 loss: 0.756630  [   32/  306]
train() client id: f_00004-5-1 loss: 0.821672  [   64/  306]
train() client id: f_00004-5-2 loss: 0.809001  [   96/  306]
train() client id: f_00004-5-3 loss: 0.822365  [  128/  306]
train() client id: f_00004-5-4 loss: 0.929863  [  160/  306]
train() client id: f_00004-5-5 loss: 0.881421  [  192/  306]
train() client id: f_00004-5-6 loss: 0.688108  [  224/  306]
train() client id: f_00004-5-7 loss: 0.812833  [  256/  306]
train() client id: f_00004-5-8 loss: 0.800393  [  288/  306]
train() client id: f_00004-6-0 loss: 0.916068  [   32/  306]
train() client id: f_00004-6-1 loss: 0.841721  [   64/  306]
train() client id: f_00004-6-2 loss: 0.713253  [   96/  306]
train() client id: f_00004-6-3 loss: 0.821381  [  128/  306]
train() client id: f_00004-6-4 loss: 0.828705  [  160/  306]
train() client id: f_00004-6-5 loss: 0.932082  [  192/  306]
train() client id: f_00004-6-6 loss: 0.752237  [  224/  306]
train() client id: f_00004-6-7 loss: 0.848705  [  256/  306]
train() client id: f_00004-6-8 loss: 0.732164  [  288/  306]
train() client id: f_00004-7-0 loss: 0.736673  [   32/  306]
train() client id: f_00004-7-1 loss: 0.804073  [   64/  306]
train() client id: f_00004-7-2 loss: 0.893926  [   96/  306]
train() client id: f_00004-7-3 loss: 0.828735  [  128/  306]
train() client id: f_00004-7-4 loss: 0.999759  [  160/  306]
train() client id: f_00004-7-5 loss: 0.816056  [  192/  306]
train() client id: f_00004-7-6 loss: 0.731238  [  224/  306]
train() client id: f_00004-7-7 loss: 0.790871  [  256/  306]
train() client id: f_00004-7-8 loss: 0.720277  [  288/  306]
train() client id: f_00004-8-0 loss: 0.890884  [   32/  306]
train() client id: f_00004-8-1 loss: 0.805496  [   64/  306]
train() client id: f_00004-8-2 loss: 0.854980  [   96/  306]
train() client id: f_00004-8-3 loss: 0.805320  [  128/  306]
train() client id: f_00004-8-4 loss: 0.770723  [  160/  306]
train() client id: f_00004-8-5 loss: 0.790098  [  192/  306]
train() client id: f_00004-8-6 loss: 0.987959  [  224/  306]
train() client id: f_00004-8-7 loss: 0.689812  [  256/  306]
train() client id: f_00004-8-8 loss: 0.799580  [  288/  306]
train() client id: f_00004-9-0 loss: 0.800812  [   32/  306]
train() client id: f_00004-9-1 loss: 0.864636  [   64/  306]
train() client id: f_00004-9-2 loss: 0.767979  [   96/  306]
train() client id: f_00004-9-3 loss: 0.869991  [  128/  306]
train() client id: f_00004-9-4 loss: 0.821335  [  160/  306]
train() client id: f_00004-9-5 loss: 0.885202  [  192/  306]
train() client id: f_00004-9-6 loss: 0.814853  [  224/  306]
train() client id: f_00004-9-7 loss: 0.742687  [  256/  306]
train() client id: f_00004-9-8 loss: 0.863400  [  288/  306]
train() client id: f_00004-10-0 loss: 0.698089  [   32/  306]
train() client id: f_00004-10-1 loss: 0.855675  [   64/  306]
train() client id: f_00004-10-2 loss: 0.848944  [   96/  306]
train() client id: f_00004-10-3 loss: 0.773655  [  128/  306]
train() client id: f_00004-10-4 loss: 0.976114  [  160/  306]
train() client id: f_00004-10-5 loss: 0.864256  [  192/  306]
train() client id: f_00004-10-6 loss: 0.682842  [  224/  306]
train() client id: f_00004-10-7 loss: 0.792526  [  256/  306]
train() client id: f_00004-10-8 loss: 0.907390  [  288/  306]
train() client id: f_00005-0-0 loss: 0.446114  [   32/  146]
train() client id: f_00005-0-1 loss: 0.615251  [   64/  146]
train() client id: f_00005-0-2 loss: 0.282982  [   96/  146]
train() client id: f_00005-0-3 loss: 0.244331  [  128/  146]
train() client id: f_00005-1-0 loss: 0.146015  [   32/  146]
train() client id: f_00005-1-1 loss: 0.308656  [   64/  146]
train() client id: f_00005-1-2 loss: 0.398240  [   96/  146]
train() client id: f_00005-1-3 loss: 0.486015  [  128/  146]
train() client id: f_00005-2-0 loss: 0.619868  [   32/  146]
train() client id: f_00005-2-1 loss: 0.160818  [   64/  146]
train() client id: f_00005-2-2 loss: 0.374863  [   96/  146]
train() client id: f_00005-2-3 loss: 0.210861  [  128/  146]
train() client id: f_00005-3-0 loss: 0.283970  [   32/  146]
train() client id: f_00005-3-1 loss: 0.311311  [   64/  146]
train() client id: f_00005-3-2 loss: 0.382552  [   96/  146]
train() client id: f_00005-3-3 loss: 0.568065  [  128/  146]
train() client id: f_00005-4-0 loss: 0.611231  [   32/  146]
train() client id: f_00005-4-1 loss: 0.478149  [   64/  146]
train() client id: f_00005-4-2 loss: 0.200094  [   96/  146]
train() client id: f_00005-4-3 loss: 0.230824  [  128/  146]
train() client id: f_00005-5-0 loss: 0.331125  [   32/  146]
train() client id: f_00005-5-1 loss: 0.257641  [   64/  146]
train() client id: f_00005-5-2 loss: 0.308267  [   96/  146]
train() client id: f_00005-5-3 loss: 0.654373  [  128/  146]
train() client id: f_00005-6-0 loss: 0.325149  [   32/  146]
train() client id: f_00005-6-1 loss: 0.697011  [   64/  146]
train() client id: f_00005-6-2 loss: 0.480220  [   96/  146]
train() client id: f_00005-6-3 loss: 0.129995  [  128/  146]
train() client id: f_00005-7-0 loss: 0.439912  [   32/  146]
train() client id: f_00005-7-1 loss: 0.369544  [   64/  146]
train() client id: f_00005-7-2 loss: 0.342479  [   96/  146]
train() client id: f_00005-7-3 loss: 0.288656  [  128/  146]
train() client id: f_00005-8-0 loss: 0.575968  [   32/  146]
train() client id: f_00005-8-1 loss: 0.199671  [   64/  146]
train() client id: f_00005-8-2 loss: 0.324846  [   96/  146]
train() client id: f_00005-8-3 loss: 0.297662  [  128/  146]
train() client id: f_00005-9-0 loss: 0.551525  [   32/  146]
train() client id: f_00005-9-1 loss: 0.189990  [   64/  146]
train() client id: f_00005-9-2 loss: 0.168500  [   96/  146]
train() client id: f_00005-9-3 loss: 0.428098  [  128/  146]
train() client id: f_00005-10-0 loss: 0.429358  [   32/  146]
train() client id: f_00005-10-1 loss: 0.048837  [   64/  146]
train() client id: f_00005-10-2 loss: 0.255049  [   96/  146]
train() client id: f_00005-10-3 loss: 0.419661  [  128/  146]
train() client id: f_00006-0-0 loss: 0.558113  [   32/   54]
train() client id: f_00006-1-0 loss: 0.471464  [   32/   54]
train() client id: f_00006-2-0 loss: 0.559046  [   32/   54]
train() client id: f_00006-3-0 loss: 0.514838  [   32/   54]
train() client id: f_00006-4-0 loss: 0.461041  [   32/   54]
train() client id: f_00006-5-0 loss: 0.555210  [   32/   54]
train() client id: f_00006-6-0 loss: 0.536766  [   32/   54]
train() client id: f_00006-7-0 loss: 0.557877  [   32/   54]
train() client id: f_00006-8-0 loss: 0.447921  [   32/   54]
train() client id: f_00006-9-0 loss: 0.443616  [   32/   54]
train() client id: f_00006-10-0 loss: 0.549790  [   32/   54]
train() client id: f_00007-0-0 loss: 0.500828  [   32/  179]
train() client id: f_00007-0-1 loss: 0.688912  [   64/  179]
train() client id: f_00007-0-2 loss: 0.725438  [   96/  179]
train() client id: f_00007-0-3 loss: 0.587904  [  128/  179]
train() client id: f_00007-0-4 loss: 0.903777  [  160/  179]
train() client id: f_00007-1-0 loss: 0.802163  [   32/  179]
train() client id: f_00007-1-1 loss: 0.840841  [   64/  179]
train() client id: f_00007-1-2 loss: 0.574099  [   96/  179]
train() client id: f_00007-1-3 loss: 0.576648  [  128/  179]
train() client id: f_00007-1-4 loss: 0.586186  [  160/  179]
train() client id: f_00007-2-0 loss: 0.693932  [   32/  179]
train() client id: f_00007-2-1 loss: 0.619574  [   64/  179]
train() client id: f_00007-2-2 loss: 0.572268  [   96/  179]
train() client id: f_00007-2-3 loss: 0.565445  [  128/  179]
train() client id: f_00007-2-4 loss: 0.768812  [  160/  179]
train() client id: f_00007-3-0 loss: 0.615963  [   32/  179]
train() client id: f_00007-3-1 loss: 0.651979  [   64/  179]
train() client id: f_00007-3-2 loss: 0.591740  [   96/  179]
train() client id: f_00007-3-3 loss: 0.669839  [  128/  179]
train() client id: f_00007-3-4 loss: 0.601419  [  160/  179]
train() client id: f_00007-4-0 loss: 0.588604  [   32/  179]
train() client id: f_00007-4-1 loss: 0.791901  [   64/  179]
train() client id: f_00007-4-2 loss: 0.515837  [   96/  179]
train() client id: f_00007-4-3 loss: 0.704071  [  128/  179]
train() client id: f_00007-4-4 loss: 0.640236  [  160/  179]
train() client id: f_00007-5-0 loss: 0.585372  [   32/  179]
train() client id: f_00007-5-1 loss: 0.604133  [   64/  179]
train() client id: f_00007-5-2 loss: 0.659264  [   96/  179]
train() client id: f_00007-5-3 loss: 0.701318  [  128/  179]
train() client id: f_00007-5-4 loss: 0.709468  [  160/  179]
train() client id: f_00007-6-0 loss: 0.723135  [   32/  179]
train() client id: f_00007-6-1 loss: 0.497570  [   64/  179]
train() client id: f_00007-6-2 loss: 0.642717  [   96/  179]
train() client id: f_00007-6-3 loss: 0.501476  [  128/  179]
train() client id: f_00007-6-4 loss: 0.850533  [  160/  179]
train() client id: f_00007-7-0 loss: 0.811591  [   32/  179]
train() client id: f_00007-7-1 loss: 0.490430  [   64/  179]
train() client id: f_00007-7-2 loss: 0.573683  [   96/  179]
train() client id: f_00007-7-3 loss: 0.757638  [  128/  179]
train() client id: f_00007-7-4 loss: 0.550633  [  160/  179]
train() client id: f_00007-8-0 loss: 0.719957  [   32/  179]
train() client id: f_00007-8-1 loss: 0.779033  [   64/  179]
train() client id: f_00007-8-2 loss: 0.529108  [   96/  179]
train() client id: f_00007-8-3 loss: 0.711843  [  128/  179]
train() client id: f_00007-8-4 loss: 0.484483  [  160/  179]
train() client id: f_00007-9-0 loss: 0.759567  [   32/  179]
train() client id: f_00007-9-1 loss: 0.816990  [   64/  179]
train() client id: f_00007-9-2 loss: 0.568626  [   96/  179]
train() client id: f_00007-9-3 loss: 0.468664  [  128/  179]
train() client id: f_00007-9-4 loss: 0.598369  [  160/  179]
train() client id: f_00007-10-0 loss: 0.754966  [   32/  179]
train() client id: f_00007-10-1 loss: 0.618802  [   64/  179]
train() client id: f_00007-10-2 loss: 0.582661  [   96/  179]
train() client id: f_00007-10-3 loss: 0.462586  [  128/  179]
train() client id: f_00007-10-4 loss: 0.686189  [  160/  179]
train() client id: f_00008-0-0 loss: 0.693776  [   32/  130]
train() client id: f_00008-0-1 loss: 0.690164  [   64/  130]
train() client id: f_00008-0-2 loss: 0.638374  [   96/  130]
train() client id: f_00008-0-3 loss: 0.620969  [  128/  130]
train() client id: f_00008-1-0 loss: 0.724716  [   32/  130]
train() client id: f_00008-1-1 loss: 0.616110  [   64/  130]
train() client id: f_00008-1-2 loss: 0.681926  [   96/  130]
train() client id: f_00008-1-3 loss: 0.632471  [  128/  130]
train() client id: f_00008-2-0 loss: 0.754800  [   32/  130]
train() client id: f_00008-2-1 loss: 0.661998  [   64/  130]
train() client id: f_00008-2-2 loss: 0.535487  [   96/  130]
train() client id: f_00008-2-3 loss: 0.677014  [  128/  130]
train() client id: f_00008-3-0 loss: 0.603223  [   32/  130]
train() client id: f_00008-3-1 loss: 0.585328  [   64/  130]
train() client id: f_00008-3-2 loss: 0.777233  [   96/  130]
train() client id: f_00008-3-3 loss: 0.643852  [  128/  130]
train() client id: f_00008-4-0 loss: 0.726598  [   32/  130]
train() client id: f_00008-4-1 loss: 0.586713  [   64/  130]
train() client id: f_00008-4-2 loss: 0.653134  [   96/  130]
train() client id: f_00008-4-3 loss: 0.688483  [  128/  130]
train() client id: f_00008-5-0 loss: 0.627041  [   32/  130]
train() client id: f_00008-5-1 loss: 0.665543  [   64/  130]
train() client id: f_00008-5-2 loss: 0.731867  [   96/  130]
train() client id: f_00008-5-3 loss: 0.625596  [  128/  130]
train() client id: f_00008-6-0 loss: 0.699112  [   32/  130]
train() client id: f_00008-6-1 loss: 0.624202  [   64/  130]
train() client id: f_00008-6-2 loss: 0.773514  [   96/  130]
train() client id: f_00008-6-3 loss: 0.567657  [  128/  130]
train() client id: f_00008-7-0 loss: 0.569953  [   32/  130]
train() client id: f_00008-7-1 loss: 0.715369  [   64/  130]
train() client id: f_00008-7-2 loss: 0.588913  [   96/  130]
train() client id: f_00008-7-3 loss: 0.768725  [  128/  130]
train() client id: f_00008-8-0 loss: 0.678584  [   32/  130]
train() client id: f_00008-8-1 loss: 0.656892  [   64/  130]
train() client id: f_00008-8-2 loss: 0.602703  [   96/  130]
train() client id: f_00008-8-3 loss: 0.725898  [  128/  130]
train() client id: f_00008-9-0 loss: 0.659056  [   32/  130]
train() client id: f_00008-9-1 loss: 0.620330  [   64/  130]
train() client id: f_00008-9-2 loss: 0.661634  [   96/  130]
train() client id: f_00008-9-3 loss: 0.718240  [  128/  130]
train() client id: f_00008-10-0 loss: 0.600929  [   32/  130]
train() client id: f_00008-10-1 loss: 0.668283  [   64/  130]
train() client id: f_00008-10-2 loss: 0.750309  [   96/  130]
train() client id: f_00008-10-3 loss: 0.610163  [  128/  130]
train() client id: f_00009-0-0 loss: 1.108008  [   32/  118]
train() client id: f_00009-0-1 loss: 1.102628  [   64/  118]
train() client id: f_00009-0-2 loss: 1.166408  [   96/  118]
train() client id: f_00009-1-0 loss: 1.061203  [   32/  118]
train() client id: f_00009-1-1 loss: 0.900967  [   64/  118]
train() client id: f_00009-1-2 loss: 1.024025  [   96/  118]
train() client id: f_00009-2-0 loss: 1.134953  [   32/  118]
train() client id: f_00009-2-1 loss: 0.902574  [   64/  118]
train() client id: f_00009-2-2 loss: 1.017461  [   96/  118]
train() client id: f_00009-3-0 loss: 0.971448  [   32/  118]
train() client id: f_00009-3-1 loss: 0.999045  [   64/  118]
train() client id: f_00009-3-2 loss: 0.828201  [   96/  118]
train() client id: f_00009-4-0 loss: 0.823701  [   32/  118]
train() client id: f_00009-4-1 loss: 1.120256  [   64/  118]
train() client id: f_00009-4-2 loss: 0.909487  [   96/  118]
train() client id: f_00009-5-0 loss: 0.899710  [   32/  118]
train() client id: f_00009-5-1 loss: 0.962213  [   64/  118]
train() client id: f_00009-5-2 loss: 0.933981  [   96/  118]
train() client id: f_00009-6-0 loss: 0.931008  [   32/  118]
train() client id: f_00009-6-1 loss: 0.951546  [   64/  118]
train() client id: f_00009-6-2 loss: 0.781809  [   96/  118]
train() client id: f_00009-7-0 loss: 0.795769  [   32/  118]
train() client id: f_00009-7-1 loss: 0.990836  [   64/  118]
train() client id: f_00009-7-2 loss: 0.836027  [   96/  118]
train() client id: f_00009-8-0 loss: 1.026114  [   32/  118]
train() client id: f_00009-8-1 loss: 1.011544  [   64/  118]
train() client id: f_00009-8-2 loss: 0.839093  [   96/  118]
train() client id: f_00009-9-0 loss: 0.882881  [   32/  118]
train() client id: f_00009-9-1 loss: 0.914523  [   64/  118]
train() client id: f_00009-9-2 loss: 0.761628  [   96/  118]
train() client id: f_00009-10-0 loss: 0.968147  [   32/  118]
train() client id: f_00009-10-1 loss: 0.929364  [   64/  118]
train() client id: f_00009-10-2 loss: 0.872548  [   96/  118]
At round 56 accuracy: 0.6472148541114059
At round 56 training accuracy: 0.5928906773977196
At round 56 training loss: 0.8162558465218307
update_location
xs = [  -3.9056584     4.20031788  300.00902392   18.81129433    0.97929623
    3.95640986 -262.44319194 -241.32485185  284.66397685 -227.06087855]
ys = [ 292.5879595   275.55583871    1.32061395 -262.45517586  254.35018685
  237.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [309.22963669 293.17002391 316.23908432 281.48993611 273.30381733
 258.0139986  280.86174452 261.22473155 302.22880354 248.13837757]
dists_bs = [207.60588129 205.4853538  505.76486823 478.46614164 193.04187331
 189.66025984 197.98218826 186.37821878 485.87600216 178.74794631]
uav_gains = [2.25068246e-12 3.05507187e-12 1.99030200e-12 3.87733494e-12
 4.60120712e-12 6.32147895e-12 3.92831823e-12 5.91917228e-12
 2.56156197e-12 7.69388670e-12]
bs_gains = [3.58940992e-11 3.69409145e-11 2.96645552e-12 3.46505949e-12
 4.40017369e-11 4.62338788e-11 4.09959494e-11 4.85498225e-11
 3.31911903e-12 5.45781883e-11]
Round 57
-------------------------------
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.48417658 7.05628152 3.44074209 1.26260335 8.13515315 3.91462212
 1.55207962 4.8313157  3.56521466 3.17400612]
obj_prev = 40.416194911724524
eta_min = 2.86092118758865e-27	eta_max = 0.9374658932840896
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 9.299045348130402	eta = 0.909090909090909
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 20.758435476455084	eta = 0.4072405937720066
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 14.636258285641127	eta = 0.5775846137877274
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.545044928479468	eta = 0.6241158766062833
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.479247501397975	eta = 0.6271624278976032
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979113930322	eta = 0.6271749156783474
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979109435532	eta = 0.6271749158874893
eta = 0.6271749158874893
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.03849692 0.08096575 0.03788586 0.01313784 0.09349252 0.04460754
 0.01649868 0.05469007 0.03971905 0.03605267]
ene_total = [1.32155705 2.05839072 1.33209932 0.6463361  2.34272368 1.20578025
 0.72268136 1.56297838 1.28498919 1.00144307]
ti_comp = [0.9850295  1.0924086  0.97437897 1.022215   1.09523156 1.09599371
 1.02294619 1.0429956  1.01299828 1.09844051]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [3.67501858e-06 2.77980148e-05 3.57977419e-06 1.35633586e-07
 4.25793239e-05 4.61837109e-06 2.68239096e-07 9.39810980e-06
 3.81644508e-06 2.42738867e-06]
ene_total = [0.42769316 0.17890492 0.45242289 0.34126134 0.17269288 0.17004157
 0.3395665  0.29322113 0.36274925 0.16430889]
optimize_network iter = 0 obj = 2.90286252803159
eta = 0.6271749158874893
freqs = [19540999.15308214 37058362.31216387 19441028.73626838  6426161.89579366
 42681624.07611285 20350273.65967661  8064295.3890079  26217785.78934446
 19604695.49055661 16410844.32139915]
eta_min = 0.6271749158874926	eta_max = 0.7063960382409014
af = 0.0018241050152146168	bf = 1.0814089668242106	zeta = 0.002006515516736079	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.50791650e-07 5.67902365e-06 7.31333600e-07 2.77094010e-08
 8.69878619e-06 9.43514808e-07 5.48001783e-08 1.91999638e-06
 7.79684519e-07 4.95905832e-07]
ene_total = [1.75372365 0.73159623 1.85514936 1.39952837 0.7050001  0.6970034
 1.39256766 1.20180996 1.48737301 0.67365931]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 1 obj = 3.685249982672195
eta = 0.7063960382409014
freqs = [19468958.52365561 35736956.5886699  19441028.73626838  6324869.42089178
 41128502.02279741 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527822]
eta_min = 0.7063960382409034	eta_max = 0.7063960382409006
af = 0.0017119208968572406	bf = 1.0814089668242106	zeta = 0.0018831129865429649	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.45266057e-07 5.28124555e-06 7.31333600e-07 2.68427459e-08
 8.07723219e-06 8.75740514e-07 5.30618587e-08 1.83637587e-06
 7.59730875e-07 4.59683582e-07]
ene_total = [1.75372312 0.73155835 1.85514936 1.39952829 0.7049409  0.69699695
 1.39256749 1.20180199 1.48737111 0.67365586]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 2 obj = 3.6852499826721847
eta = 0.7063960382409006
freqs = [19468958.52365561 35736956.5886699  19441028.73626836  6324869.42089178
 41128502.02279742 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527823]
Done!
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.62674960e-06 1.86141707e-05 2.57764354e-06 9.46093966e-08
 2.84688484e-05 3.08661721e-06 1.87020749e-07 6.47245305e-06
 2.67773200e-06 1.62019141e-06]
ene_total = [0.01841708 0.00769516 0.01948208 0.014696   0.00742271 0.00732112
 0.01462297 0.01262431 0.01562025 0.00707497]
At round 57 energy consumption: 0.12497665052505343
At round 57 eta: 0.7063960382409006
At round 57 a_n: 8.65748957900329
At round 57 local rounds: 11.38151016928333
At round 57 global rounds: 29.486964437171718
gradient difference: 0.5355476140975952
train() client id: f_00000-0-0 loss: 1.061918  [   32/  126]
train() client id: f_00000-0-1 loss: 1.000936  [   64/  126]
train() client id: f_00000-0-2 loss: 0.949495  [   96/  126]
train() client id: f_00000-1-0 loss: 1.100787  [   32/  126]
train() client id: f_00000-1-1 loss: 0.962956  [   64/  126]
train() client id: f_00000-1-2 loss: 0.974273  [   96/  126]
train() client id: f_00000-2-0 loss: 1.007696  [   32/  126]
train() client id: f_00000-2-1 loss: 0.925588  [   64/  126]
train() client id: f_00000-2-2 loss: 0.943705  [   96/  126]
train() client id: f_00000-3-0 loss: 1.096085  [   32/  126]
train() client id: f_00000-3-1 loss: 0.810863  [   64/  126]
train() client id: f_00000-3-2 loss: 0.925884  [   96/  126]
train() client id: f_00000-4-0 loss: 1.011818  [   32/  126]
train() client id: f_00000-4-1 loss: 1.010291  [   64/  126]
train() client id: f_00000-4-2 loss: 0.782012  [   96/  126]
train() client id: f_00000-5-0 loss: 0.937590  [   32/  126]
train() client id: f_00000-5-1 loss: 0.923759  [   64/  126]
train() client id: f_00000-5-2 loss: 0.869099  [   96/  126]
train() client id: f_00000-6-0 loss: 0.903299  [   32/  126]
train() client id: f_00000-6-1 loss: 0.810977  [   64/  126]
train() client id: f_00000-6-2 loss: 0.880700  [   96/  126]
train() client id: f_00000-7-0 loss: 0.807838  [   32/  126]
train() client id: f_00000-7-1 loss: 0.913520  [   64/  126]
train() client id: f_00000-7-2 loss: 0.920260  [   96/  126]
train() client id: f_00000-8-0 loss: 0.800412  [   32/  126]
train() client id: f_00000-8-1 loss: 1.071351  [   64/  126]
train() client id: f_00000-8-2 loss: 0.849720  [   96/  126]
train() client id: f_00000-9-0 loss: 0.906029  [   32/  126]
train() client id: f_00000-9-1 loss: 0.963662  [   64/  126]
train() client id: f_00000-9-2 loss: 0.971313  [   96/  126]
train() client id: f_00000-10-0 loss: 1.045174  [   32/  126]
train() client id: f_00000-10-1 loss: 1.037358  [   64/  126]
train() client id: f_00000-10-2 loss: 0.894611  [   96/  126]
train() client id: f_00001-0-0 loss: 0.553833  [   32/  265]
train() client id: f_00001-0-1 loss: 0.497193  [   64/  265]
train() client id: f_00001-0-2 loss: 0.421487  [   96/  265]
train() client id: f_00001-0-3 loss: 0.682182  [  128/  265]
train() client id: f_00001-0-4 loss: 0.497021  [  160/  265]
train() client id: f_00001-0-5 loss: 0.441473  [  192/  265]
train() client id: f_00001-0-6 loss: 0.444158  [  224/  265]
train() client id: f_00001-0-7 loss: 0.482279  [  256/  265]
train() client id: f_00001-1-0 loss: 0.442224  [   32/  265]
train() client id: f_00001-1-1 loss: 0.593658  [   64/  265]
train() client id: f_00001-1-2 loss: 0.548417  [   96/  265]
train() client id: f_00001-1-3 loss: 0.525545  [  128/  265]
train() client id: f_00001-1-4 loss: 0.433950  [  160/  265]
train() client id: f_00001-1-5 loss: 0.466854  [  192/  265]
train() client id: f_00001-1-6 loss: 0.489326  [  224/  265]
train() client id: f_00001-1-7 loss: 0.458621  [  256/  265]
train() client id: f_00001-2-0 loss: 0.507264  [   32/  265]
train() client id: f_00001-2-1 loss: 0.433817  [   64/  265]
train() client id: f_00001-2-2 loss: 0.508893  [   96/  265]
train() client id: f_00001-2-3 loss: 0.511753  [  128/  265]
train() client id: f_00001-2-4 loss: 0.472171  [  160/  265]
train() client id: f_00001-2-5 loss: 0.621196  [  192/  265]
train() client id: f_00001-2-6 loss: 0.391333  [  224/  265]
train() client id: f_00001-2-7 loss: 0.403378  [  256/  265]
train() client id: f_00001-3-0 loss: 0.504209  [   32/  265]
train() client id: f_00001-3-1 loss: 0.392932  [   64/  265]
train() client id: f_00001-3-2 loss: 0.477977  [   96/  265]
train() client id: f_00001-3-3 loss: 0.470672  [  128/  265]
train() client id: f_00001-3-4 loss: 0.496929  [  160/  265]
train() client id: f_00001-3-5 loss: 0.568080  [  192/  265]
train() client id: f_00001-3-6 loss: 0.502524  [  224/  265]
train() client id: f_00001-3-7 loss: 0.463906  [  256/  265]
train() client id: f_00001-4-0 loss: 0.464184  [   32/  265]
train() client id: f_00001-4-1 loss: 0.385844  [   64/  265]
train() client id: f_00001-4-2 loss: 0.414520  [   96/  265]
train() client id: f_00001-4-3 loss: 0.512442  [  128/  265]
train() client id: f_00001-4-4 loss: 0.476336  [  160/  265]
train() client id: f_00001-4-5 loss: 0.756490  [  192/  265]
train() client id: f_00001-4-6 loss: 0.417089  [  224/  265]
train() client id: f_00001-4-7 loss: 0.429278  [  256/  265]
train() client id: f_00001-5-0 loss: 0.442097  [   32/  265]
train() client id: f_00001-5-1 loss: 0.453796  [   64/  265]
train() client id: f_00001-5-2 loss: 0.418639  [   96/  265]
train() client id: f_00001-5-3 loss: 0.459463  [  128/  265]
train() client id: f_00001-5-4 loss: 0.539326  [  160/  265]
train() client id: f_00001-5-5 loss: 0.472338  [  192/  265]
train() client id: f_00001-5-6 loss: 0.376383  [  224/  265]
train() client id: f_00001-5-7 loss: 0.593218  [  256/  265]
train() client id: f_00001-6-0 loss: 0.383978  [   32/  265]
train() client id: f_00001-6-1 loss: 0.433680  [   64/  265]
train() client id: f_00001-6-2 loss: 0.471004  [   96/  265]
train() client id: f_00001-6-3 loss: 0.527900  [  128/  265]
train() client id: f_00001-6-4 loss: 0.534133  [  160/  265]
train() client id: f_00001-6-5 loss: 0.398881  [  192/  265]
train() client id: f_00001-6-6 loss: 0.541284  [  224/  265]
train() client id: f_00001-6-7 loss: 0.465964  [  256/  265]
train() client id: f_00001-7-0 loss: 0.521738  [   32/  265]
train() client id: f_00001-7-1 loss: 0.375546  [   64/  265]
train() client id: f_00001-7-2 loss: 0.490944  [   96/  265]
train() client id: f_00001-7-3 loss: 0.426613  [  128/  265]
train() client id: f_00001-7-4 loss: 0.456873  [  160/  265]
train() client id: f_00001-7-5 loss: 0.544757  [  192/  265]
train() client id: f_00001-7-6 loss: 0.556688  [  224/  265]
train() client id: f_00001-7-7 loss: 0.439982  [  256/  265]
train() client id: f_00001-8-0 loss: 0.542202  [   32/  265]
train() client id: f_00001-8-1 loss: 0.397458  [   64/  265]
train() client id: f_00001-8-2 loss: 0.424993  [   96/  265]
train() client id: f_00001-8-3 loss: 0.402027  [  128/  265]
train() client id: f_00001-8-4 loss: 0.420718  [  160/  265]
train() client id: f_00001-8-5 loss: 0.535183  [  192/  265]
train() client id: f_00001-8-6 loss: 0.368946  [  224/  265]
train() client id: f_00001-8-7 loss: 0.542663  [  256/  265]
train() client id: f_00001-9-0 loss: 0.412750  [   32/  265]
train() client id: f_00001-9-1 loss: 0.478190  [   64/  265]
train() client id: f_00001-9-2 loss: 0.462383  [   96/  265]
train() client id: f_00001-9-3 loss: 0.370190  [  128/  265]
train() client id: f_00001-9-4 loss: 0.617678  [  160/  265]
train() client id: f_00001-9-5 loss: 0.392730  [  192/  265]
train() client id: f_00001-9-6 loss: 0.566442  [  224/  265]
train() client id: f_00001-9-7 loss: 0.416251  [  256/  265]
train() client id: f_00001-10-0 loss: 0.382733  [   32/  265]
train() client id: f_00001-10-1 loss: 0.492921  [   64/  265]
train() client id: f_00001-10-2 loss: 0.456334  [   96/  265]
train() client id: f_00001-10-3 loss: 0.614846  [  128/  265]
train() client id: f_00001-10-4 loss: 0.379773  [  160/  265]
train() client id: f_00001-10-5 loss: 0.466489  [  192/  265]
train() client id: f_00001-10-6 loss: 0.497270  [  224/  265]
train() client id: f_00001-10-7 loss: 0.515433  [  256/  265]
train() client id: f_00002-0-0 loss: 1.163612  [   32/  124]
train() client id: f_00002-0-1 loss: 0.980766  [   64/  124]
train() client id: f_00002-0-2 loss: 1.187016  [   96/  124]
train() client id: f_00002-1-0 loss: 1.140959  [   32/  124]
train() client id: f_00002-1-1 loss: 1.110798  [   64/  124]
train() client id: f_00002-1-2 loss: 0.867286  [   96/  124]
train() client id: f_00002-2-0 loss: 1.024034  [   32/  124]
train() client id: f_00002-2-1 loss: 0.983101  [   64/  124]
train() client id: f_00002-2-2 loss: 1.002954  [   96/  124]
train() client id: f_00002-3-0 loss: 1.185989  [   32/  124]
train() client id: f_00002-3-1 loss: 0.781637  [   64/  124]
train() client id: f_00002-3-2 loss: 1.069263  [   96/  124]
train() client id: f_00002-4-0 loss: 0.955199  [   32/  124]
train() client id: f_00002-4-1 loss: 0.869770  [   64/  124]
train() client id: f_00002-4-2 loss: 0.756127  [   96/  124]
train() client id: f_00002-5-0 loss: 1.112797  [   32/  124]
train() client id: f_00002-5-1 loss: 0.872817  [   64/  124]
train() client id: f_00002-5-2 loss: 0.838126  [   96/  124]
train() client id: f_00002-6-0 loss: 0.896295  [   32/  124]
train() client id: f_00002-6-1 loss: 0.747280  [   64/  124]
train() client id: f_00002-6-2 loss: 0.920944  [   96/  124]
train() client id: f_00002-7-0 loss: 1.032957  [   32/  124]
train() client id: f_00002-7-1 loss: 0.706241  [   64/  124]
train() client id: f_00002-7-2 loss: 0.796183  [   96/  124]
train() client id: f_00002-8-0 loss: 0.991680  [   32/  124]
train() client id: f_00002-8-1 loss: 0.820845  [   64/  124]
train() client id: f_00002-8-2 loss: 0.896577  [   96/  124]
train() client id: f_00002-9-0 loss: 1.013669  [   32/  124]
train() client id: f_00002-9-1 loss: 0.996289  [   64/  124]
train() client id: f_00002-9-2 loss: 0.737666  [   96/  124]
train() client id: f_00002-10-0 loss: 1.028352  [   32/  124]
train() client id: f_00002-10-1 loss: 0.866325  [   64/  124]
train() client id: f_00002-10-2 loss: 0.732364  [   96/  124]
train() client id: f_00003-0-0 loss: 0.425980  [   32/   43]
train() client id: f_00003-1-0 loss: 0.624857  [   32/   43]
train() client id: f_00003-2-0 loss: 0.587715  [   32/   43]
train() client id: f_00003-3-0 loss: 0.715673  [   32/   43]
train() client id: f_00003-4-0 loss: 0.587098  [   32/   43]
train() client id: f_00003-5-0 loss: 0.470174  [   32/   43]
train() client id: f_00003-6-0 loss: 0.578999  [   32/   43]
train() client id: f_00003-7-0 loss: 0.670631  [   32/   43]
train() client id: f_00003-8-0 loss: 0.735510  [   32/   43]
train() client id: f_00003-9-0 loss: 0.672264  [   32/   43]
train() client id: f_00003-10-0 loss: 0.695560  [   32/   43]
train() client id: f_00004-0-0 loss: 0.807643  [   32/  306]
train() client id: f_00004-0-1 loss: 0.766048  [   64/  306]
train() client id: f_00004-0-2 loss: 0.729187  [   96/  306]
train() client id: f_00004-0-3 loss: 0.776270  [  128/  306]
train() client id: f_00004-0-4 loss: 0.747413  [  160/  306]
train() client id: f_00004-0-5 loss: 0.655716  [  192/  306]
train() client id: f_00004-0-6 loss: 0.696719  [  224/  306]
train() client id: f_00004-0-7 loss: 0.653647  [  256/  306]
train() client id: f_00004-0-8 loss: 0.769269  [  288/  306]
train() client id: f_00004-1-0 loss: 0.790860  [   32/  306]
train() client id: f_00004-1-1 loss: 0.788035  [   64/  306]
train() client id: f_00004-1-2 loss: 0.722740  [   96/  306]
train() client id: f_00004-1-3 loss: 0.857564  [  128/  306]
train() client id: f_00004-1-4 loss: 0.660450  [  160/  306]
train() client id: f_00004-1-5 loss: 0.798264  [  192/  306]
train() client id: f_00004-1-6 loss: 0.631396  [  224/  306]
train() client id: f_00004-1-7 loss: 0.690802  [  256/  306]
train() client id: f_00004-1-8 loss: 0.807659  [  288/  306]
train() client id: f_00004-2-0 loss: 0.664695  [   32/  306]
train() client id: f_00004-2-1 loss: 0.707353  [   64/  306]
train() client id: f_00004-2-2 loss: 0.704233  [   96/  306]
train() client id: f_00004-2-3 loss: 0.752041  [  128/  306]
train() client id: f_00004-2-4 loss: 0.754943  [  160/  306]
train() client id: f_00004-2-5 loss: 0.648756  [  192/  306]
train() client id: f_00004-2-6 loss: 0.771414  [  224/  306]
train() client id: f_00004-2-7 loss: 0.984285  [  256/  306]
train() client id: f_00004-2-8 loss: 0.789290  [  288/  306]
train() client id: f_00004-3-0 loss: 0.681410  [   32/  306]
train() client id: f_00004-3-1 loss: 0.894854  [   64/  306]
train() client id: f_00004-3-2 loss: 0.720178  [   96/  306]
train() client id: f_00004-3-3 loss: 0.763947  [  128/  306]
train() client id: f_00004-3-4 loss: 0.786954  [  160/  306]
train() client id: f_00004-3-5 loss: 0.793815  [  192/  306]
train() client id: f_00004-3-6 loss: 0.567101  [  224/  306]
train() client id: f_00004-3-7 loss: 0.730432  [  256/  306]
train() client id: f_00004-3-8 loss: 0.824466  [  288/  306]
train() client id: f_00004-4-0 loss: 0.606297  [   32/  306]
train() client id: f_00004-4-1 loss: 0.828130  [   64/  306]
train() client id: f_00004-4-2 loss: 0.659049  [   96/  306]
train() client id: f_00004-4-3 loss: 0.789074  [  128/  306]
train() client id: f_00004-4-4 loss: 0.784318  [  160/  306]
train() client id: f_00004-4-5 loss: 0.784716  [  192/  306]
train() client id: f_00004-4-6 loss: 0.746775  [  224/  306]
train() client id: f_00004-4-7 loss: 0.842579  [  256/  306]
train() client id: f_00004-4-8 loss: 0.751805  [  288/  306]
train() client id: f_00004-5-0 loss: 0.900634  [   32/  306]
train() client id: f_00004-5-1 loss: 0.700998  [   64/  306]
train() client id: f_00004-5-2 loss: 0.780241  [   96/  306]
train() client id: f_00004-5-3 loss: 0.682886  [  128/  306]
train() client id: f_00004-5-4 loss: 0.685460  [  160/  306]
train() client id: f_00004-5-5 loss: 0.751450  [  192/  306]
train() client id: f_00004-5-6 loss: 0.796079  [  224/  306]
train() client id: f_00004-5-7 loss: 0.624655  [  256/  306]
train() client id: f_00004-5-8 loss: 0.797985  [  288/  306]
train() client id: f_00004-6-0 loss: 0.700627  [   32/  306]
train() client id: f_00004-6-1 loss: 0.823560  [   64/  306]
train() client id: f_00004-6-2 loss: 0.712900  [   96/  306]
train() client id: f_00004-6-3 loss: 0.659734  [  128/  306]
train() client id: f_00004-6-4 loss: 0.787593  [  160/  306]
train() client id: f_00004-6-5 loss: 0.834704  [  192/  306]
train() client id: f_00004-6-6 loss: 0.759214  [  224/  306]
train() client id: f_00004-6-7 loss: 0.731542  [  256/  306]
train() client id: f_00004-6-8 loss: 0.782341  [  288/  306]
train() client id: f_00004-7-0 loss: 0.782541  [   32/  306]
train() client id: f_00004-7-1 loss: 0.741536  [   64/  306]
train() client id: f_00004-7-2 loss: 0.690076  [   96/  306]
train() client id: f_00004-7-3 loss: 0.807520  [  128/  306]
train() client id: f_00004-7-4 loss: 0.887773  [  160/  306]
train() client id: f_00004-7-5 loss: 0.615230  [  192/  306]
train() client id: f_00004-7-6 loss: 0.840302  [  224/  306]
train() client id: f_00004-7-7 loss: 0.797446  [  256/  306]
train() client id: f_00004-7-8 loss: 0.695479  [  288/  306]
train() client id: f_00004-8-0 loss: 0.673373  [   32/  306]
train() client id: f_00004-8-1 loss: 0.856305  [   64/  306]
train() client id: f_00004-8-2 loss: 0.828321  [   96/  306]
train() client id: f_00004-8-3 loss: 0.737212  [  128/  306]
train() client id: f_00004-8-4 loss: 0.652778  [  160/  306]
train() client id: f_00004-8-5 loss: 0.847330  [  192/  306]
train() client id: f_00004-8-6 loss: 0.703308  [  224/  306]
train() client id: f_00004-8-7 loss: 0.869571  [  256/  306]
train() client id: f_00004-8-8 loss: 0.661813  [  288/  306]
train() client id: f_00004-9-0 loss: 0.755802  [   32/  306]
train() client id: f_00004-9-1 loss: 0.820381  [   64/  306]
train() client id: f_00004-9-2 loss: 0.677715  [   96/  306]
train() client id: f_00004-9-3 loss: 0.788534  [  128/  306]
train() client id: f_00004-9-4 loss: 0.745058  [  160/  306]
train() client id: f_00004-9-5 loss: 0.747169  [  192/  306]
train() client id: f_00004-9-6 loss: 0.724308  [  224/  306]
train() client id: f_00004-9-7 loss: 0.700211  [  256/  306]
train() client id: f_00004-9-8 loss: 0.799887  [  288/  306]
train() client id: f_00004-10-0 loss: 0.819248  [   32/  306]
train() client id: f_00004-10-1 loss: 0.754471  [   64/  306]
train() client id: f_00004-10-2 loss: 0.740091  [   96/  306]
train() client id: f_00004-10-3 loss: 0.914786  [  128/  306]
train() client id: f_00004-10-4 loss: 0.676308  [  160/  306]
train() client id: f_00004-10-5 loss: 0.697244  [  192/  306]
train() client id: f_00004-10-6 loss: 0.753971  [  224/  306]
train() client id: f_00004-10-7 loss: 0.792736  [  256/  306]
train() client id: f_00004-10-8 loss: 0.644587  [  288/  306]
train() client id: f_00005-0-0 loss: 0.270634  [   32/  146]
train() client id: f_00005-0-1 loss: 0.457251  [   64/  146]
train() client id: f_00005-0-2 loss: 0.540803  [   96/  146]
train() client id: f_00005-0-3 loss: 0.694336  [  128/  146]
train() client id: f_00005-1-0 loss: 0.490018  [   32/  146]
train() client id: f_00005-1-1 loss: 0.413014  [   64/  146]
train() client id: f_00005-1-2 loss: 0.479566  [   96/  146]
train() client id: f_00005-1-3 loss: 0.658356  [  128/  146]
train() client id: f_00005-2-0 loss: 0.367410  [   32/  146]
train() client id: f_00005-2-1 loss: 0.452497  [   64/  146]
train() client id: f_00005-2-2 loss: 0.619346  [   96/  146]
train() client id: f_00005-2-3 loss: 0.794369  [  128/  146]
train() client id: f_00005-3-0 loss: 0.592494  [   32/  146]
train() client id: f_00005-3-1 loss: 0.584129  [   64/  146]
train() client id: f_00005-3-2 loss: 0.552974  [   96/  146]
train() client id: f_00005-3-3 loss: 0.331096  [  128/  146]
train() client id: f_00005-4-0 loss: 0.600926  [   32/  146]
train() client id: f_00005-4-1 loss: 0.247953  [   64/  146]
train() client id: f_00005-4-2 loss: 0.640188  [   96/  146]
train() client id: f_00005-4-3 loss: 0.416516  [  128/  146]
train() client id: f_00005-5-0 loss: 0.793556  [   32/  146]
train() client id: f_00005-5-1 loss: 0.307599  [   64/  146]
train() client id: f_00005-5-2 loss: 0.350399  [   96/  146]
train() client id: f_00005-5-3 loss: 0.387219  [  128/  146]
train() client id: f_00005-6-0 loss: 0.554681  [   32/  146]
train() client id: f_00005-6-1 loss: 0.509081  [   64/  146]
train() client id: f_00005-6-2 loss: 0.338192  [   96/  146]
train() client id: f_00005-6-3 loss: 0.497319  [  128/  146]
train() client id: f_00005-7-0 loss: 0.645548  [   32/  146]
train() client id: f_00005-7-1 loss: 0.354682  [   64/  146]
train() client id: f_00005-7-2 loss: 0.727303  [   96/  146]
train() client id: f_00005-7-3 loss: 0.407640  [  128/  146]
train() client id: f_00005-8-0 loss: 0.628141  [   32/  146]
train() client id: f_00005-8-1 loss: 0.452641  [   64/  146]
train() client id: f_00005-8-2 loss: 0.752065  [   96/  146]
train() client id: f_00005-8-3 loss: 0.363453  [  128/  146]
train() client id: f_00005-9-0 loss: 0.449978  [   32/  146]
train() client id: f_00005-9-1 loss: 0.752589  [   64/  146]
train() client id: f_00005-9-2 loss: 0.745157  [   96/  146]
train() client id: f_00005-9-3 loss: 0.203920  [  128/  146]
train() client id: f_00005-10-0 loss: 0.499882  [   32/  146]
train() client id: f_00005-10-1 loss: 0.479571  [   64/  146]
train() client id: f_00005-10-2 loss: 0.691624  [   96/  146]
train() client id: f_00005-10-3 loss: 0.598149  [  128/  146]
train() client id: f_00006-0-0 loss: 0.482240  [   32/   54]
train() client id: f_00006-1-0 loss: 0.460282  [   32/   54]
train() client id: f_00006-2-0 loss: 0.430031  [   32/   54]
train() client id: f_00006-3-0 loss: 0.445595  [   32/   54]
train() client id: f_00006-4-0 loss: 0.496254  [   32/   54]
train() client id: f_00006-5-0 loss: 0.440545  [   32/   54]
train() client id: f_00006-6-0 loss: 0.519327  [   32/   54]
train() client id: f_00006-7-0 loss: 0.448100  [   32/   54]
train() client id: f_00006-8-0 loss: 0.459382  [   32/   54]
train() client id: f_00006-9-0 loss: 0.488075  [   32/   54]
train() client id: f_00006-10-0 loss: 0.501519  [   32/   54]
train() client id: f_00007-0-0 loss: 0.589579  [   32/  179]
train() client id: f_00007-0-1 loss: 0.557869  [   64/  179]
train() client id: f_00007-0-2 loss: 0.524466  [   96/  179]
train() client id: f_00007-0-3 loss: 0.398801  [  128/  179]
train() client id: f_00007-0-4 loss: 0.665186  [  160/  179]
train() client id: f_00007-1-0 loss: 0.649192  [   32/  179]
train() client id: f_00007-1-1 loss: 0.492609  [   64/  179]
train() client id: f_00007-1-2 loss: 0.515018  [   96/  179]
train() client id: f_00007-1-3 loss: 0.497072  [  128/  179]
train() client id: f_00007-1-4 loss: 0.578931  [  160/  179]
train() client id: f_00007-2-0 loss: 0.377940  [   32/  179]
train() client id: f_00007-2-1 loss: 0.519687  [   64/  179]
train() client id: f_00007-2-2 loss: 0.682788  [   96/  179]
train() client id: f_00007-2-3 loss: 0.419454  [  128/  179]
train() client id: f_00007-2-4 loss: 0.537754  [  160/  179]
train() client id: f_00007-3-0 loss: 0.522445  [   32/  179]
train() client id: f_00007-3-1 loss: 0.541802  [   64/  179]
train() client id: f_00007-3-2 loss: 0.510932  [   96/  179]
train() client id: f_00007-3-3 loss: 0.542814  [  128/  179]
train() client id: f_00007-3-4 loss: 0.552843  [  160/  179]
train() client id: f_00007-4-0 loss: 0.607152  [   32/  179]
train() client id: f_00007-4-1 loss: 0.738760  [   64/  179]
train() client id: f_00007-4-2 loss: 0.396949  [   96/  179]
train() client id: f_00007-4-3 loss: 0.551087  [  128/  179]
train() client id: f_00007-4-4 loss: 0.348137  [  160/  179]
train() client id: f_00007-5-0 loss: 0.723772  [   32/  179]
train() client id: f_00007-5-1 loss: 0.500273  [   64/  179]
train() client id: f_00007-5-2 loss: 0.483169  [   96/  179]
train() client id: f_00007-5-3 loss: 0.383663  [  128/  179]
train() client id: f_00007-5-4 loss: 0.510347  [  160/  179]
train() client id: f_00007-6-0 loss: 0.343236  [   32/  179]
train() client id: f_00007-6-1 loss: 0.750546  [   64/  179]
train() client id: f_00007-6-2 loss: 0.630600  [   96/  179]
train() client id: f_00007-6-3 loss: 0.477662  [  128/  179]
train() client id: f_00007-6-4 loss: 0.461262  [  160/  179]
train() client id: f_00007-7-0 loss: 0.531964  [   32/  179]
train() client id: f_00007-7-1 loss: 0.426381  [   64/  179]
train() client id: f_00007-7-2 loss: 0.459614  [   96/  179]
train() client id: f_00007-7-3 loss: 0.449837  [  128/  179]
train() client id: f_00007-7-4 loss: 0.664522  [  160/  179]
train() client id: f_00007-8-0 loss: 0.323706  [   32/  179]
train() client id: f_00007-8-1 loss: 0.398974  [   64/  179]
train() client id: f_00007-8-2 loss: 0.537425  [   96/  179]
train() client id: f_00007-8-3 loss: 0.657334  [  128/  179]
train() client id: f_00007-8-4 loss: 0.615637  [  160/  179]
train() client id: f_00007-9-0 loss: 0.423515  [   32/  179]
train() client id: f_00007-9-1 loss: 0.608986  [   64/  179]
train() client id: f_00007-9-2 loss: 0.608342  [   96/  179]
train() client id: f_00007-9-3 loss: 0.341145  [  128/  179]
train() client id: f_00007-9-4 loss: 0.400900  [  160/  179]
train() client id: f_00007-10-0 loss: 0.327966  [   32/  179]
train() client id: f_00007-10-1 loss: 0.347620  [   64/  179]
train() client id: f_00007-10-2 loss: 0.601429  [   96/  179]
train() client id: f_00007-10-3 loss: 0.698202  [  128/  179]
train() client id: f_00007-10-4 loss: 0.558376  [  160/  179]
train() client id: f_00008-0-0 loss: 0.862803  [   32/  130]
train() client id: f_00008-0-1 loss: 0.769556  [   64/  130]
train() client id: f_00008-0-2 loss: 0.781810  [   96/  130]
train() client id: f_00008-0-3 loss: 0.790273  [  128/  130]
train() client id: f_00008-1-0 loss: 0.753693  [   32/  130]
train() client id: f_00008-1-1 loss: 0.870952  [   64/  130]
train() client id: f_00008-1-2 loss: 0.747682  [   96/  130]
train() client id: f_00008-1-3 loss: 0.842366  [  128/  130]
train() client id: f_00008-2-0 loss: 0.838308  [   32/  130]
train() client id: f_00008-2-1 loss: 0.809163  [   64/  130]
train() client id: f_00008-2-2 loss: 0.828888  [   96/  130]
train() client id: f_00008-2-3 loss: 0.732669  [  128/  130]
train() client id: f_00008-3-0 loss: 0.811508  [   32/  130]
train() client id: f_00008-3-1 loss: 0.828158  [   64/  130]
train() client id: f_00008-3-2 loss: 0.749470  [   96/  130]
train() client id: f_00008-3-3 loss: 0.800417  [  128/  130]
train() client id: f_00008-4-0 loss: 0.745835  [   32/  130]
train() client id: f_00008-4-1 loss: 0.846826  [   64/  130]
train() client id: f_00008-4-2 loss: 0.815396  [   96/  130]
train() client id: f_00008-4-3 loss: 0.767661  [  128/  130]
train() client id: f_00008-5-0 loss: 0.763043  [   32/  130]
train() client id: f_00008-5-1 loss: 0.856978  [   64/  130]
train() client id: f_00008-5-2 loss: 0.797642  [   96/  130]
train() client id: f_00008-5-3 loss: 0.802933  [  128/  130]
train() client id: f_00008-6-0 loss: 0.850854  [   32/  130]
train() client id: f_00008-6-1 loss: 0.704691  [   64/  130]
train() client id: f_00008-6-2 loss: 0.845502  [   96/  130]
train() client id: f_00008-6-3 loss: 0.796028  [  128/  130]
train() client id: f_00008-7-0 loss: 0.766203  [   32/  130]
train() client id: f_00008-7-1 loss: 0.794668  [   64/  130]
train() client id: f_00008-7-2 loss: 0.817332  [   96/  130]
train() client id: f_00008-7-3 loss: 0.844349  [  128/  130]
train() client id: f_00008-8-0 loss: 0.625225  [   32/  130]
train() client id: f_00008-8-1 loss: 0.922180  [   64/  130]
train() client id: f_00008-8-2 loss: 0.847655  [   96/  130]
train() client id: f_00008-8-3 loss: 0.794023  [  128/  130]
train() client id: f_00008-9-0 loss: 0.796472  [   32/  130]
train() client id: f_00008-9-1 loss: 0.760592  [   64/  130]
train() client id: f_00008-9-2 loss: 0.780399  [   96/  130]
train() client id: f_00008-9-3 loss: 0.865541  [  128/  130]
train() client id: f_00008-10-0 loss: 0.837842  [   32/  130]
train() client id: f_00008-10-1 loss: 0.878694  [   64/  130]
train() client id: f_00008-10-2 loss: 0.666708  [   96/  130]
train() client id: f_00008-10-3 loss: 0.831907  [  128/  130]
train() client id: f_00009-0-0 loss: 1.013766  [   32/  118]
train() client id: f_00009-0-1 loss: 0.872187  [   64/  118]
train() client id: f_00009-0-2 loss: 0.997356  [   96/  118]
train() client id: f_00009-1-0 loss: 0.881231  [   32/  118]
train() client id: f_00009-1-1 loss: 0.932071  [   64/  118]
train() client id: f_00009-1-2 loss: 1.018868  [   96/  118]
train() client id: f_00009-2-0 loss: 0.973407  [   32/  118]
train() client id: f_00009-2-1 loss: 0.916924  [   64/  118]
train() client id: f_00009-2-2 loss: 0.913506  [   96/  118]
train() client id: f_00009-3-0 loss: 0.852325  [   32/  118]
train() client id: f_00009-3-1 loss: 0.908259  [   64/  118]
train() client id: f_00009-3-2 loss: 0.866786  [   96/  118]
train() client id: f_00009-4-0 loss: 0.987985  [   32/  118]
train() client id: f_00009-4-1 loss: 0.702123  [   64/  118]
train() client id: f_00009-4-2 loss: 1.032404  [   96/  118]
train() client id: f_00009-5-0 loss: 0.791155  [   32/  118]
train() client id: f_00009-5-1 loss: 0.984962  [   64/  118]
train() client id: f_00009-5-2 loss: 0.920395  [   96/  118]
train() client id: f_00009-6-0 loss: 0.947624  [   32/  118]
train() client id: f_00009-6-1 loss: 0.884608  [   64/  118]
train() client id: f_00009-6-2 loss: 0.831160  [   96/  118]
train() client id: f_00009-7-0 loss: 0.722664  [   32/  118]
train() client id: f_00009-7-1 loss: 0.839080  [   64/  118]
train() client id: f_00009-7-2 loss: 0.980735  [   96/  118]
train() client id: f_00009-8-0 loss: 0.699988  [   32/  118]
train() client id: f_00009-8-1 loss: 0.824311  [   64/  118]
train() client id: f_00009-8-2 loss: 0.980905  [   96/  118]
train() client id: f_00009-9-0 loss: 0.878080  [   32/  118]
train() client id: f_00009-9-1 loss: 0.989877  [   64/  118]
train() client id: f_00009-9-2 loss: 0.840604  [   96/  118]
train() client id: f_00009-10-0 loss: 0.903761  [   32/  118]
train() client id: f_00009-10-1 loss: 0.849426  [   64/  118]
train() client id: f_00009-10-2 loss: 0.865433  [   96/  118]
At round 57 accuracy: 0.6472148541114059
At round 57 training accuracy: 0.5915492957746479
At round 57 training loss: 0.8268435796514143
update_location
xs = [  -3.9056584     4.20031788  305.00902392   18.81129433    0.97929623
    3.95640986 -267.44319194 -246.32485185  289.66397685 -232.06087855]
ys = [ 297.5879595   280.55583871    1.32061395 -267.45517586  259.35018685
  242.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [313.96472382 297.87450596 320.98636839 286.15753684 277.96308826
 262.62971077 285.53940438 265.85072671 306.94281138 252.721711  ]
dists_bs = [210.47822106 207.97785702 510.4637031  483.04182061 195.15036951
 191.37438614 200.24080201 188.21553855 490.60893718 180.26768178]
uav_gains = [2.06976062e-12 2.78476917e-12 1.83844324e-12 3.52095540e-12
 4.17321580e-12 5.75002013e-12 3.56593276e-12 5.37807949e-12
 2.34619438e-12 7.03198726e-12]
bs_gains = [3.45393392e-11 3.57146344e-11 2.89062969e-12 3.37393595e-12
 4.26834807e-11 4.50836859e-11 3.97142964e-11 4.72344384e-11
 3.23024015e-12 5.32996109e-11]
Round 58
-------------------------------
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.3523265  6.77756785 3.31084036 1.21735079 7.81367428 3.76005882
 1.49535056 4.64371042 3.4255904  3.04870737]
obj_prev = 38.84517734952248
eta_min = 2.470941254922374e-28	eta_max = 0.9375517380484754
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 8.931115437766236	eta = 0.909090909090909
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 20.2431781023763	eta = 0.40108306173335817
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 14.165631330058979	eta = 0.5731615953668162
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.085726257080514	eta = 0.6204619975235665
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.020212302687447	eta = 0.623583983407007
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.019941071601606	eta = 0.6235969738929089
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.01994106692125	eta = 0.623596974117077
eta = 0.623596974117077
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.03896902 0.08195865 0.03835046 0.01329895 0.09463905 0.04515458
 0.01670101 0.05536075 0.04020613 0.0364948 ]
ene_total = [1.28342906 1.9813468  1.29389683 0.63079383 2.25502199 1.16000083
 0.7042715  1.51102723 1.23695895 0.96319404]
ti_comp = [1.03546301 1.14942413 1.02453888 1.07419893 1.1523401  1.15319241
 1.07495744 1.09635059 1.06887034 1.15568559]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [3.44960441e-06 2.60437692e-05 3.35842002e-06 1.27398049e-07
 3.98960382e-05 4.32694980e-06 2.51956506e-07 8.82240566e-06
 3.55555046e-06 2.27453939e-06]
ene_total = [0.42265881 0.17141173 0.4467888  0.33701571 0.16527621 0.16260767
 0.33534287 0.28827359 0.34886259 0.15705475]
optimize_network iter = 0 obj = 2.83529273119976
eta = 0.623596974117077
freqs = [18817195.6470524  35652050.01875965 18715963.43024579  6190171.56284024
 41063853.09945352 19578076.71969618  7768218.52091367 25247740.71552675
 18807768.58899782 15789240.96805261]
eta_min = 0.6235969741170779	eta_max = 0.7095637697055857
af = 0.0016217254572042322	bf = 1.0664469018281375	zeta = 0.0017838980029246557	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.96202700e-07 5.25618019e-06 6.77799772e-07 2.57116048e-08
 8.05185933e-06 8.73269445e-07 5.08501203e-08 1.78054696e-06
 7.17584839e-07 4.59049871e-07]
ene_total = [1.74973877 0.70781749 1.84965402 1.39538178 0.68140253 0.67295033
 1.38844638 1.19293349 1.4441827  0.65010868]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 1 obj = 3.6736658995998672
eta = 0.7095637697055857
freqs = [18742766.932176   34251295.00082695 18715963.43024578  6084298.06393133
 39418319.48246514 18789072.00779956  7633454.28294466 24640242.31966294
 18518666.61499257 15142443.926058  ]
eta_min = 0.7095637697056919	eta_max = 0.7095637697055788
af = 0.0015119964848909963	bf = 1.0664469018281375	zeta = 0.001663196133380096	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.90706132e-07 4.85126751e-06 6.77799772e-07 2.48396102e-08
 7.41947189e-06 8.04301492e-07 4.91011128e-08 1.69589257e-06
 6.95693805e-07 4.22210775e-07]
ene_total = [1.74973827 0.70778046 1.84965402 1.3953817  0.68134469 0.67294402
 1.38844622 1.19292575 1.44418069 0.65010531]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 2 obj = 3.67366589959978
eta = 0.7095637697055788
freqs = [18742766.93217598 34251295.00082704 18715963.43024575  6084298.06393133
 39418319.48246525 18789072.00779961  7633454.28294467 24640242.31966297
 18518666.61499257 15142443.92605804]
Done!
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.43444879e-06 1.70986789e-05 2.38895930e-06 8.75491851e-08
 2.61505198e-05 2.83482469e-06 1.73060784e-07 5.97730848e-06
 2.45202824e-06 1.48811552e-06]
ene_total = [0.01913201 0.00775057 0.02022438 0.01525607 0.00746802 0.00735947
 0.01518031 0.0130468  0.0157913  0.00710881]
At round 58 energy consumption: 0.12831774437155147
At round 58 eta: 0.7095637697055788
At round 58 a_n: 8.314943732033974
At round 58 local rounds: 11.234997825322132
At round 58 global rounds: 28.629154577598474
gradient difference: 0.46156564354896545
train() client id: f_00000-0-0 loss: 1.122540  [   32/  126]
train() client id: f_00000-0-1 loss: 0.893419  [   64/  126]
train() client id: f_00000-0-2 loss: 1.091531  [   96/  126]
train() client id: f_00000-1-0 loss: 0.939118  [   32/  126]
train() client id: f_00000-1-1 loss: 1.039583  [   64/  126]
train() client id: f_00000-1-2 loss: 1.086873  [   96/  126]
train() client id: f_00000-2-0 loss: 1.029852  [   32/  126]
train() client id: f_00000-2-1 loss: 1.080464  [   64/  126]
train() client id: f_00000-2-2 loss: 0.895344  [   96/  126]
train() client id: f_00000-3-0 loss: 0.905380  [   32/  126]
train() client id: f_00000-3-1 loss: 1.032229  [   64/  126]
train() client id: f_00000-3-2 loss: 0.927198  [   96/  126]
train() client id: f_00000-4-0 loss: 0.880337  [   32/  126]
train() client id: f_00000-4-1 loss: 0.964861  [   64/  126]
train() client id: f_00000-4-2 loss: 0.765579  [   96/  126]
train() client id: f_00000-5-0 loss: 0.831328  [   32/  126]
train() client id: f_00000-5-1 loss: 1.037620  [   64/  126]
train() client id: f_00000-5-2 loss: 0.847450  [   96/  126]
train() client id: f_00000-6-0 loss: 0.858865  [   32/  126]
train() client id: f_00000-6-1 loss: 0.816390  [   64/  126]
train() client id: f_00000-6-2 loss: 1.015369  [   96/  126]
train() client id: f_00000-7-0 loss: 0.830428  [   32/  126]
train() client id: f_00000-7-1 loss: 0.883360  [   64/  126]
train() client id: f_00000-7-2 loss: 0.893863  [   96/  126]
train() client id: f_00000-8-0 loss: 0.906983  [   32/  126]
train() client id: f_00000-8-1 loss: 0.849878  [   64/  126]
train() client id: f_00000-8-2 loss: 0.788448  [   96/  126]
train() client id: f_00000-9-0 loss: 0.969863  [   32/  126]
train() client id: f_00000-9-1 loss: 0.695715  [   64/  126]
train() client id: f_00000-9-2 loss: 0.857599  [   96/  126]
train() client id: f_00000-10-0 loss: 0.858090  [   32/  126]
train() client id: f_00000-10-1 loss: 0.970189  [   64/  126]
train() client id: f_00000-10-2 loss: 0.708422  [   96/  126]
train() client id: f_00001-0-0 loss: 0.380139  [   32/  265]
train() client id: f_00001-0-1 loss: 0.341122  [   64/  265]
train() client id: f_00001-0-2 loss: 0.488013  [   96/  265]
train() client id: f_00001-0-3 loss: 0.328156  [  128/  265]
train() client id: f_00001-0-4 loss: 0.348376  [  160/  265]
train() client id: f_00001-0-5 loss: 0.661980  [  192/  265]
train() client id: f_00001-0-6 loss: 0.388303  [  224/  265]
train() client id: f_00001-0-7 loss: 0.390276  [  256/  265]
train() client id: f_00001-1-0 loss: 0.421705  [   32/  265]
train() client id: f_00001-1-1 loss: 0.378140  [   64/  265]
train() client id: f_00001-1-2 loss: 0.347908  [   96/  265]
train() client id: f_00001-1-3 loss: 0.333680  [  128/  265]
train() client id: f_00001-1-4 loss: 0.337102  [  160/  265]
train() client id: f_00001-1-5 loss: 0.441148  [  192/  265]
train() client id: f_00001-1-6 loss: 0.527832  [  224/  265]
train() client id: f_00001-1-7 loss: 0.441767  [  256/  265]
train() client id: f_00001-2-0 loss: 0.392823  [   32/  265]
train() client id: f_00001-2-1 loss: 0.438469  [   64/  265]
train() client id: f_00001-2-2 loss: 0.486833  [   96/  265]
train() client id: f_00001-2-3 loss: 0.296563  [  128/  265]
train() client id: f_00001-2-4 loss: 0.372949  [  160/  265]
train() client id: f_00001-2-5 loss: 0.348789  [  192/  265]
train() client id: f_00001-2-6 loss: 0.430007  [  224/  265]
train() client id: f_00001-2-7 loss: 0.320922  [  256/  265]
train() client id: f_00001-3-0 loss: 0.292293  [   32/  265]
train() client id: f_00001-3-1 loss: 0.323438  [   64/  265]
train() client id: f_00001-3-2 loss: 0.373974  [   96/  265]
train() client id: f_00001-3-3 loss: 0.448827  [  128/  265]
train() client id: f_00001-3-4 loss: 0.271929  [  160/  265]
train() client id: f_00001-3-5 loss: 0.509399  [  192/  265]
train() client id: f_00001-3-6 loss: 0.524915  [  224/  265]
train() client id: f_00001-3-7 loss: 0.301707  [  256/  265]
train() client id: f_00001-4-0 loss: 0.432860  [   32/  265]
train() client id: f_00001-4-1 loss: 0.367625  [   64/  265]
train() client id: f_00001-4-2 loss: 0.392337  [   96/  265]
train() client id: f_00001-4-3 loss: 0.348313  [  128/  265]
train() client id: f_00001-4-4 loss: 0.301932  [  160/  265]
train() client id: f_00001-4-5 loss: 0.370129  [  192/  265]
train() client id: f_00001-4-6 loss: 0.431920  [  224/  265]
train() client id: f_00001-4-7 loss: 0.372892  [  256/  265]
train() client id: f_00001-5-0 loss: 0.458619  [   32/  265]
train() client id: f_00001-5-1 loss: 0.291782  [   64/  265]
train() client id: f_00001-5-2 loss: 0.304350  [   96/  265]
train() client id: f_00001-5-3 loss: 0.330253  [  128/  265]
train() client id: f_00001-5-4 loss: 0.368987  [  160/  265]
train() client id: f_00001-5-5 loss: 0.330188  [  192/  265]
train() client id: f_00001-5-6 loss: 0.341506  [  224/  265]
train() client id: f_00001-5-7 loss: 0.584019  [  256/  265]
train() client id: f_00001-6-0 loss: 0.415971  [   32/  265]
train() client id: f_00001-6-1 loss: 0.348207  [   64/  265]
train() client id: f_00001-6-2 loss: 0.357618  [   96/  265]
train() client id: f_00001-6-3 loss: 0.457247  [  128/  265]
train() client id: f_00001-6-4 loss: 0.410719  [  160/  265]
train() client id: f_00001-6-5 loss: 0.355207  [  192/  265]
train() client id: f_00001-6-6 loss: 0.409795  [  224/  265]
train() client id: f_00001-6-7 loss: 0.264664  [  256/  265]
train() client id: f_00001-7-0 loss: 0.441043  [   32/  265]
train() client id: f_00001-7-1 loss: 0.376628  [   64/  265]
train() client id: f_00001-7-2 loss: 0.460417  [   96/  265]
train() client id: f_00001-7-3 loss: 0.326452  [  128/  265]
train() client id: f_00001-7-4 loss: 0.386537  [  160/  265]
train() client id: f_00001-7-5 loss: 0.362699  [  192/  265]
train() client id: f_00001-7-6 loss: 0.276272  [  224/  265]
train() client id: f_00001-7-7 loss: 0.295757  [  256/  265]
train() client id: f_00001-8-0 loss: 0.326807  [   32/  265]
train() client id: f_00001-8-1 loss: 0.367939  [   64/  265]
train() client id: f_00001-8-2 loss: 0.273084  [   96/  265]
train() client id: f_00001-8-3 loss: 0.391613  [  128/  265]
train() client id: f_00001-8-4 loss: 0.489568  [  160/  265]
train() client id: f_00001-8-5 loss: 0.527966  [  192/  265]
train() client id: f_00001-8-6 loss: 0.333037  [  224/  265]
train() client id: f_00001-8-7 loss: 0.264514  [  256/  265]
train() client id: f_00001-9-0 loss: 0.284688  [   32/  265]
train() client id: f_00001-9-1 loss: 0.393877  [   64/  265]
train() client id: f_00001-9-2 loss: 0.318961  [   96/  265]
train() client id: f_00001-9-3 loss: 0.398668  [  128/  265]
train() client id: f_00001-9-4 loss: 0.308956  [  160/  265]
train() client id: f_00001-9-5 loss: 0.336915  [  192/  265]
train() client id: f_00001-9-6 loss: 0.503872  [  224/  265]
train() client id: f_00001-9-7 loss: 0.413108  [  256/  265]
train() client id: f_00001-10-0 loss: 0.406635  [   32/  265]
train() client id: f_00001-10-1 loss: 0.278214  [   64/  265]
train() client id: f_00001-10-2 loss: 0.403852  [   96/  265]
train() client id: f_00001-10-3 loss: 0.388173  [  128/  265]
train() client id: f_00001-10-4 loss: 0.298397  [  160/  265]
train() client id: f_00001-10-5 loss: 0.409069  [  192/  265]
train() client id: f_00001-10-6 loss: 0.425694  [  224/  265]
train() client id: f_00001-10-7 loss: 0.351522  [  256/  265]
train() client id: f_00002-0-0 loss: 1.061303  [   32/  124]
train() client id: f_00002-0-1 loss: 1.357050  [   64/  124]
train() client id: f_00002-0-2 loss: 1.343368  [   96/  124]
train() client id: f_00002-1-0 loss: 1.293255  [   32/  124]
train() client id: f_00002-1-1 loss: 1.224794  [   64/  124]
train() client id: f_00002-1-2 loss: 1.137388  [   96/  124]
train() client id: f_00002-2-0 loss: 1.264584  [   32/  124]
train() client id: f_00002-2-1 loss: 1.234897  [   64/  124]
train() client id: f_00002-2-2 loss: 0.899269  [   96/  124]
train() client id: f_00002-3-0 loss: 1.417551  [   32/  124]
train() client id: f_00002-3-1 loss: 0.964808  [   64/  124]
train() client id: f_00002-3-2 loss: 1.063210  [   96/  124]
train() client id: f_00002-4-0 loss: 1.246959  [   32/  124]
train() client id: f_00002-4-1 loss: 1.193432  [   64/  124]
train() client id: f_00002-4-2 loss: 1.107571  [   96/  124]
train() client id: f_00002-5-0 loss: 1.297281  [   32/  124]
train() client id: f_00002-5-1 loss: 1.129554  [   64/  124]
train() client id: f_00002-5-2 loss: 1.050811  [   96/  124]
train() client id: f_00002-6-0 loss: 1.079989  [   32/  124]
train() client id: f_00002-6-1 loss: 1.148146  [   64/  124]
train() client id: f_00002-6-2 loss: 1.101401  [   96/  124]
train() client id: f_00002-7-0 loss: 1.162573  [   32/  124]
train() client id: f_00002-7-1 loss: 0.973948  [   64/  124]
train() client id: f_00002-7-2 loss: 1.102678  [   96/  124]
train() client id: f_00002-8-0 loss: 0.942546  [   32/  124]
train() client id: f_00002-8-1 loss: 1.150309  [   64/  124]
train() client id: f_00002-8-2 loss: 1.262191  [   96/  124]
train() client id: f_00002-9-0 loss: 1.253706  [   32/  124]
train() client id: f_00002-9-1 loss: 0.948017  [   64/  124]
train() client id: f_00002-9-2 loss: 1.149905  [   96/  124]
train() client id: f_00002-10-0 loss: 1.055105  [   32/  124]
train() client id: f_00002-10-1 loss: 1.114290  [   64/  124]
train() client id: f_00002-10-2 loss: 1.034887  [   96/  124]
train() client id: f_00003-0-0 loss: 0.520406  [   32/   43]
train() client id: f_00003-1-0 loss: 0.525759  [   32/   43]
train() client id: f_00003-2-0 loss: 0.666298  [   32/   43]
train() client id: f_00003-3-0 loss: 0.510234  [   32/   43]
train() client id: f_00003-4-0 loss: 0.633144  [   32/   43]
train() client id: f_00003-5-0 loss: 0.549288  [   32/   43]
train() client id: f_00003-6-0 loss: 0.718725  [   32/   43]
train() client id: f_00003-7-0 loss: 0.613447  [   32/   43]
train() client id: f_00003-8-0 loss: 0.804337  [   32/   43]
train() client id: f_00003-9-0 loss: 0.426041  [   32/   43]
train() client id: f_00003-10-0 loss: 0.672190  [   32/   43]
train() client id: f_00004-0-0 loss: 1.036385  [   32/  306]
train() client id: f_00004-0-1 loss: 1.065196  [   64/  306]
train() client id: f_00004-0-2 loss: 0.929258  [   96/  306]
train() client id: f_00004-0-3 loss: 1.010323  [  128/  306]
train() client id: f_00004-0-4 loss: 0.929827  [  160/  306]
train() client id: f_00004-0-5 loss: 0.956565  [  192/  306]
train() client id: f_00004-0-6 loss: 0.999638  [  224/  306]
train() client id: f_00004-0-7 loss: 0.862028  [  256/  306]
train() client id: f_00004-0-8 loss: 0.990739  [  288/  306]
train() client id: f_00004-1-0 loss: 0.889128  [   32/  306]
train() client id: f_00004-1-1 loss: 1.129220  [   64/  306]
train() client id: f_00004-1-2 loss: 0.967448  [   96/  306]
train() client id: f_00004-1-3 loss: 0.946647  [  128/  306]
train() client id: f_00004-1-4 loss: 1.063474  [  160/  306]
train() client id: f_00004-1-5 loss: 0.939164  [  192/  306]
train() client id: f_00004-1-6 loss: 0.815655  [  224/  306]
train() client id: f_00004-1-7 loss: 1.058424  [  256/  306]
train() client id: f_00004-1-8 loss: 1.105263  [  288/  306]
train() client id: f_00004-2-0 loss: 1.083183  [   32/  306]
train() client id: f_00004-2-1 loss: 1.001922  [   64/  306]
train() client id: f_00004-2-2 loss: 1.043743  [   96/  306]
train() client id: f_00004-2-3 loss: 0.794775  [  128/  306]
train() client id: f_00004-2-4 loss: 0.967320  [  160/  306]
train() client id: f_00004-2-5 loss: 0.960321  [  192/  306]
train() client id: f_00004-2-6 loss: 1.026985  [  224/  306]
train() client id: f_00004-2-7 loss: 1.023254  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939255  [  288/  306]
train() client id: f_00004-3-0 loss: 1.039603  [   32/  306]
train() client id: f_00004-3-1 loss: 0.887502  [   64/  306]
train() client id: f_00004-3-2 loss: 0.966198  [   96/  306]
train() client id: f_00004-3-3 loss: 0.905540  [  128/  306]
train() client id: f_00004-3-4 loss: 0.967607  [  160/  306]
train() client id: f_00004-3-5 loss: 0.889216  [  192/  306]
train() client id: f_00004-3-6 loss: 1.014739  [  224/  306]
train() client id: f_00004-3-7 loss: 1.040480  [  256/  306]
train() client id: f_00004-3-8 loss: 0.897458  [  288/  306]
train() client id: f_00004-4-0 loss: 0.867832  [   32/  306]
train() client id: f_00004-4-1 loss: 0.939205  [   64/  306]
train() client id: f_00004-4-2 loss: 1.081604  [   96/  306]
train() client id: f_00004-4-3 loss: 1.085403  [  128/  306]
train() client id: f_00004-4-4 loss: 0.959063  [  160/  306]
train() client id: f_00004-4-5 loss: 1.112008  [  192/  306]
train() client id: f_00004-4-6 loss: 0.864896  [  224/  306]
train() client id: f_00004-4-7 loss: 0.924950  [  256/  306]
train() client id: f_00004-4-8 loss: 0.901598  [  288/  306]
train() client id: f_00004-5-0 loss: 0.870163  [   32/  306]
train() client id: f_00004-5-1 loss: 1.010000  [   64/  306]
train() client id: f_00004-5-2 loss: 0.972556  [   96/  306]
train() client id: f_00004-5-3 loss: 0.977647  [  128/  306]
train() client id: f_00004-5-4 loss: 1.044484  [  160/  306]
train() client id: f_00004-5-5 loss: 0.902829  [  192/  306]
train() client id: f_00004-5-6 loss: 0.902724  [  224/  306]
train() client id: f_00004-5-7 loss: 1.037526  [  256/  306]
train() client id: f_00004-5-8 loss: 0.966252  [  288/  306]
train() client id: f_00004-6-0 loss: 0.865290  [   32/  306]
train() client id: f_00004-6-1 loss: 1.078119  [   64/  306]
train() client id: f_00004-6-2 loss: 1.044775  [   96/  306]
train() client id: f_00004-6-3 loss: 0.968075  [  128/  306]
train() client id: f_00004-6-4 loss: 0.988379  [  160/  306]
train() client id: f_00004-6-5 loss: 0.943644  [  192/  306]
train() client id: f_00004-6-6 loss: 0.925951  [  224/  306]
train() client id: f_00004-6-7 loss: 0.947007  [  256/  306]
train() client id: f_00004-6-8 loss: 0.869524  [  288/  306]
train() client id: f_00004-7-0 loss: 0.990403  [   32/  306]
train() client id: f_00004-7-1 loss: 0.848204  [   64/  306]
train() client id: f_00004-7-2 loss: 0.928248  [   96/  306]
train() client id: f_00004-7-3 loss: 1.064730  [  128/  306]
train() client id: f_00004-7-4 loss: 0.908521  [  160/  306]
train() client id: f_00004-7-5 loss: 1.060240  [  192/  306]
train() client id: f_00004-7-6 loss: 0.974676  [  224/  306]
train() client id: f_00004-7-7 loss: 0.967479  [  256/  306]
train() client id: f_00004-7-8 loss: 1.004192  [  288/  306]
train() client id: f_00004-8-0 loss: 1.048965  [   32/  306]
train() client id: f_00004-8-1 loss: 0.924279  [   64/  306]
train() client id: f_00004-8-2 loss: 1.043149  [   96/  306]
train() client id: f_00004-8-3 loss: 0.970316  [  128/  306]
train() client id: f_00004-8-4 loss: 1.040113  [  160/  306]
train() client id: f_00004-8-5 loss: 0.854521  [  192/  306]
train() client id: f_00004-8-6 loss: 0.904750  [  224/  306]
train() client id: f_00004-8-7 loss: 0.884575  [  256/  306]
train() client id: f_00004-8-8 loss: 0.974011  [  288/  306]
train() client id: f_00004-9-0 loss: 1.014336  [   32/  306]
train() client id: f_00004-9-1 loss: 0.981685  [   64/  306]
train() client id: f_00004-9-2 loss: 0.863097  [   96/  306]
train() client id: f_00004-9-3 loss: 1.078487  [  128/  306]
train() client id: f_00004-9-4 loss: 0.950466  [  160/  306]
train() client id: f_00004-9-5 loss: 0.996301  [  192/  306]
train() client id: f_00004-9-6 loss: 0.941442  [  224/  306]
train() client id: f_00004-9-7 loss: 0.883701  [  256/  306]
train() client id: f_00004-9-8 loss: 0.811152  [  288/  306]
train() client id: f_00004-10-0 loss: 0.962270  [   32/  306]
train() client id: f_00004-10-1 loss: 0.954942  [   64/  306]
train() client id: f_00004-10-2 loss: 0.932102  [   96/  306]
train() client id: f_00004-10-3 loss: 0.920736  [  128/  306]
train() client id: f_00004-10-4 loss: 0.961661  [  160/  306]
train() client id: f_00004-10-5 loss: 0.937699  [  192/  306]
train() client id: f_00004-10-6 loss: 0.916939  [  224/  306]
train() client id: f_00004-10-7 loss: 0.886468  [  256/  306]
train() client id: f_00004-10-8 loss: 1.034237  [  288/  306]
train() client id: f_00005-0-0 loss: 0.559788  [   32/  146]
train() client id: f_00005-0-1 loss: 0.419506  [   64/  146]
train() client id: f_00005-0-2 loss: 0.478268  [   96/  146]
train() client id: f_00005-0-3 loss: 0.662095  [  128/  146]
train() client id: f_00005-1-0 loss: 0.582744  [   32/  146]
train() client id: f_00005-1-1 loss: 0.637931  [   64/  146]
train() client id: f_00005-1-2 loss: 0.457642  [   96/  146]
train() client id: f_00005-1-3 loss: 0.214710  [  128/  146]
train() client id: f_00005-2-0 loss: 0.742355  [   32/  146]
train() client id: f_00005-2-1 loss: 0.323047  [   64/  146]
train() client id: f_00005-2-2 loss: 0.771364  [   96/  146]
train() client id: f_00005-2-3 loss: 0.428119  [  128/  146]
train() client id: f_00005-3-0 loss: 0.582479  [   32/  146]
train() client id: f_00005-3-1 loss: 0.406003  [   64/  146]
train() client id: f_00005-3-2 loss: 0.603716  [   96/  146]
train() client id: f_00005-3-3 loss: 0.725882  [  128/  146]
train() client id: f_00005-4-0 loss: 0.342121  [   32/  146]
train() client id: f_00005-4-1 loss: 0.318531  [   64/  146]
train() client id: f_00005-4-2 loss: 0.600764  [   96/  146]
train() client id: f_00005-4-3 loss: 0.738790  [  128/  146]
train() client id: f_00005-5-0 loss: 0.438093  [   32/  146]
train() client id: f_00005-5-1 loss: 0.613195  [   64/  146]
train() client id: f_00005-5-2 loss: 0.347887  [   96/  146]
train() client id: f_00005-5-3 loss: 0.655845  [  128/  146]
train() client id: f_00005-6-0 loss: 0.465401  [   32/  146]
train() client id: f_00005-6-1 loss: 0.598191  [   64/  146]
train() client id: f_00005-6-2 loss: 0.612388  [   96/  146]
train() client id: f_00005-6-3 loss: 0.594384  [  128/  146]
train() client id: f_00005-7-0 loss: 0.482553  [   32/  146]
train() client id: f_00005-7-1 loss: 0.654953  [   64/  146]
train() client id: f_00005-7-2 loss: 0.525219  [   96/  146]
train() client id: f_00005-7-3 loss: 0.746921  [  128/  146]
train() client id: f_00005-8-0 loss: 0.454193  [   32/  146]
train() client id: f_00005-8-1 loss: 0.570856  [   64/  146]
train() client id: f_00005-8-2 loss: 0.465106  [   96/  146]
train() client id: f_00005-8-3 loss: 0.614072  [  128/  146]
train() client id: f_00005-9-0 loss: 0.546852  [   32/  146]
train() client id: f_00005-9-1 loss: 0.540786  [   64/  146]
train() client id: f_00005-9-2 loss: 0.958909  [   96/  146]
train() client id: f_00005-9-3 loss: 0.313940  [  128/  146]
train() client id: f_00005-10-0 loss: 0.445789  [   32/  146]
train() client id: f_00005-10-1 loss: 0.375442  [   64/  146]
train() client id: f_00005-10-2 loss: 0.365589  [   96/  146]
train() client id: f_00005-10-3 loss: 0.961036  [  128/  146]
train() client id: f_00006-0-0 loss: 0.491923  [   32/   54]
train() client id: f_00006-1-0 loss: 0.501661  [   32/   54]
train() client id: f_00006-2-0 loss: 0.484286  [   32/   54]
train() client id: f_00006-3-0 loss: 0.481557  [   32/   54]
train() client id: f_00006-4-0 loss: 0.457105  [   32/   54]
train() client id: f_00006-5-0 loss: 0.484829  [   32/   54]
train() client id: f_00006-6-0 loss: 0.384787  [   32/   54]
train() client id: f_00006-7-0 loss: 0.371327  [   32/   54]
train() client id: f_00006-8-0 loss: 0.492068  [   32/   54]
train() client id: f_00006-9-0 loss: 0.389031  [   32/   54]
train() client id: f_00006-10-0 loss: 0.473438  [   32/   54]
train() client id: f_00007-0-0 loss: 0.509509  [   32/  179]
train() client id: f_00007-0-1 loss: 0.414095  [   64/  179]
train() client id: f_00007-0-2 loss: 0.517466  [   96/  179]
train() client id: f_00007-0-3 loss: 0.806733  [  128/  179]
train() client id: f_00007-0-4 loss: 0.530922  [  160/  179]
train() client id: f_00007-1-0 loss: 0.394249  [   32/  179]
train() client id: f_00007-1-1 loss: 0.684598  [   64/  179]
train() client id: f_00007-1-2 loss: 0.473514  [   96/  179]
train() client id: f_00007-1-3 loss: 0.492149  [  128/  179]
train() client id: f_00007-1-4 loss: 0.620939  [  160/  179]
train() client id: f_00007-2-0 loss: 0.640333  [   32/  179]
train() client id: f_00007-2-1 loss: 0.370360  [   64/  179]
train() client id: f_00007-2-2 loss: 0.529546  [   96/  179]
train() client id: f_00007-2-3 loss: 0.466330  [  128/  179]
train() client id: f_00007-2-4 loss: 0.674036  [  160/  179]
train() client id: f_00007-3-0 loss: 0.513567  [   32/  179]
train() client id: f_00007-3-1 loss: 0.542889  [   64/  179]
train() client id: f_00007-3-2 loss: 0.353616  [   96/  179]
train() client id: f_00007-3-3 loss: 0.488574  [  128/  179]
train() client id: f_00007-3-4 loss: 0.591893  [  160/  179]
train() client id: f_00007-4-0 loss: 0.721156  [   32/  179]
train() client id: f_00007-4-1 loss: 0.330094  [   64/  179]
train() client id: f_00007-4-2 loss: 0.473082  [   96/  179]
train() client id: f_00007-4-3 loss: 0.546645  [  128/  179]
train() client id: f_00007-4-4 loss: 0.488207  [  160/  179]
train() client id: f_00007-5-0 loss: 0.429963  [   32/  179]
train() client id: f_00007-5-1 loss: 0.632199  [   64/  179]
train() client id: f_00007-5-2 loss: 0.582205  [   96/  179]
train() client id: f_00007-5-3 loss: 0.438858  [  128/  179]
train() client id: f_00007-5-4 loss: 0.480859  [  160/  179]
train() client id: f_00007-6-0 loss: 0.354448  [   32/  179]
train() client id: f_00007-6-1 loss: 0.535075  [   64/  179]
train() client id: f_00007-6-2 loss: 0.383513  [   96/  179]
train() client id: f_00007-6-3 loss: 0.489851  [  128/  179]
train() client id: f_00007-6-4 loss: 0.656175  [  160/  179]
train() client id: f_00007-7-0 loss: 0.573966  [   32/  179]
train() client id: f_00007-7-1 loss: 0.407547  [   64/  179]
train() client id: f_00007-7-2 loss: 0.690204  [   96/  179]
train() client id: f_00007-7-3 loss: 0.338129  [  128/  179]
train() client id: f_00007-7-4 loss: 0.390729  [  160/  179]
train() client id: f_00007-8-0 loss: 0.348201  [   32/  179]
train() client id: f_00007-8-1 loss: 0.452347  [   64/  179]
train() client id: f_00007-8-2 loss: 0.413507  [   96/  179]
train() client id: f_00007-8-3 loss: 0.411120  [  128/  179]
train() client id: f_00007-8-4 loss: 0.761760  [  160/  179]
train() client id: f_00007-9-0 loss: 0.494830  [   32/  179]
train() client id: f_00007-9-1 loss: 0.447408  [   64/  179]
train() client id: f_00007-9-2 loss: 0.524896  [   96/  179]
train() client id: f_00007-9-3 loss: 0.541027  [  128/  179]
train() client id: f_00007-9-4 loss: 0.321867  [  160/  179]
train() client id: f_00007-10-0 loss: 0.595989  [   32/  179]
train() client id: f_00007-10-1 loss: 0.503905  [   64/  179]
train() client id: f_00007-10-2 loss: 0.330326  [   96/  179]
train() client id: f_00007-10-3 loss: 0.557198  [  128/  179]
train() client id: f_00007-10-4 loss: 0.485559  [  160/  179]
train() client id: f_00008-0-0 loss: 0.688332  [   32/  130]
train() client id: f_00008-0-1 loss: 0.634115  [   64/  130]
train() client id: f_00008-0-2 loss: 0.718798  [   96/  130]
train() client id: f_00008-0-3 loss: 0.682002  [  128/  130]
train() client id: f_00008-1-0 loss: 0.693878  [   32/  130]
train() client id: f_00008-1-1 loss: 0.689890  [   64/  130]
train() client id: f_00008-1-2 loss: 0.581240  [   96/  130]
train() client id: f_00008-1-3 loss: 0.768376  [  128/  130]
train() client id: f_00008-2-0 loss: 0.708947  [   32/  130]
train() client id: f_00008-2-1 loss: 0.667687  [   64/  130]
train() client id: f_00008-2-2 loss: 0.684049  [   96/  130]
train() client id: f_00008-2-3 loss: 0.673011  [  128/  130]
train() client id: f_00008-3-0 loss: 0.732759  [   32/  130]
train() client id: f_00008-3-1 loss: 0.616160  [   64/  130]
train() client id: f_00008-3-2 loss: 0.772981  [   96/  130]
train() client id: f_00008-3-3 loss: 0.610084  [  128/  130]
train() client id: f_00008-4-0 loss: 0.712236  [   32/  130]
train() client id: f_00008-4-1 loss: 0.667301  [   64/  130]
train() client id: f_00008-4-2 loss: 0.651417  [   96/  130]
train() client id: f_00008-4-3 loss: 0.702117  [  128/  130]
train() client id: f_00008-5-0 loss: 0.674906  [   32/  130]
train() client id: f_00008-5-1 loss: 0.692834  [   64/  130]
train() client id: f_00008-5-2 loss: 0.695577  [   96/  130]
train() client id: f_00008-5-3 loss: 0.684428  [  128/  130]
train() client id: f_00008-6-0 loss: 0.669516  [   32/  130]
train() client id: f_00008-6-1 loss: 0.692342  [   64/  130]
train() client id: f_00008-6-2 loss: 0.674427  [   96/  130]
train() client id: f_00008-6-3 loss: 0.698459  [  128/  130]
train() client id: f_00008-7-0 loss: 0.680803  [   32/  130]
train() client id: f_00008-7-1 loss: 0.656182  [   64/  130]
train() client id: f_00008-7-2 loss: 0.702163  [   96/  130]
train() client id: f_00008-7-3 loss: 0.682092  [  128/  130]
train() client id: f_00008-8-0 loss: 0.721454  [   32/  130]
train() client id: f_00008-8-1 loss: 0.729298  [   64/  130]
train() client id: f_00008-8-2 loss: 0.712002  [   96/  130]
train() client id: f_00008-8-3 loss: 0.555906  [  128/  130]
train() client id: f_00008-9-0 loss: 0.674626  [   32/  130]
train() client id: f_00008-9-1 loss: 0.743195  [   64/  130]
train() client id: f_00008-9-2 loss: 0.726050  [   96/  130]
train() client id: f_00008-9-3 loss: 0.611657  [  128/  130]
train() client id: f_00008-10-0 loss: 0.706468  [   32/  130]
train() client id: f_00008-10-1 loss: 0.673344  [   64/  130]
train() client id: f_00008-10-2 loss: 0.631510  [   96/  130]
train() client id: f_00008-10-3 loss: 0.752991  [  128/  130]
train() client id: f_00009-0-0 loss: 1.056438  [   32/  118]
train() client id: f_00009-0-1 loss: 1.030152  [   64/  118]
train() client id: f_00009-0-2 loss: 1.024023  [   96/  118]
train() client id: f_00009-1-0 loss: 0.814742  [   32/  118]
train() client id: f_00009-1-1 loss: 1.167107  [   64/  118]
train() client id: f_00009-1-2 loss: 0.940411  [   96/  118]
train() client id: f_00009-2-0 loss: 0.937841  [   32/  118]
train() client id: f_00009-2-1 loss: 1.000921  [   64/  118]
train() client id: f_00009-2-2 loss: 0.919943  [   96/  118]
train() client id: f_00009-3-0 loss: 0.863291  [   32/  118]
train() client id: f_00009-3-1 loss: 0.838703  [   64/  118]
train() client id: f_00009-3-2 loss: 1.039339  [   96/  118]
train() client id: f_00009-4-0 loss: 0.963960  [   32/  118]
train() client id: f_00009-4-1 loss: 0.823216  [   64/  118]
train() client id: f_00009-4-2 loss: 0.778335  [   96/  118]
train() client id: f_00009-5-0 loss: 0.913824  [   32/  118]
train() client id: f_00009-5-1 loss: 0.967454  [   64/  118]
train() client id: f_00009-5-2 loss: 0.879007  [   96/  118]
train() client id: f_00009-6-0 loss: 0.815331  [   32/  118]
train() client id: f_00009-6-1 loss: 0.855242  [   64/  118]
train() client id: f_00009-6-2 loss: 0.780281  [   96/  118]
train() client id: f_00009-7-0 loss: 0.845462  [   32/  118]
train() client id: f_00009-7-1 loss: 0.837205  [   64/  118]
train() client id: f_00009-7-2 loss: 0.783798  [   96/  118]
train() client id: f_00009-8-0 loss: 0.984173  [   32/  118]
train() client id: f_00009-8-1 loss: 0.726252  [   64/  118]
train() client id: f_00009-8-2 loss: 0.741217  [   96/  118]
train() client id: f_00009-9-0 loss: 0.782120  [   32/  118]
train() client id: f_00009-9-1 loss: 0.835868  [   64/  118]
train() client id: f_00009-9-2 loss: 0.785101  [   96/  118]
train() client id: f_00009-10-0 loss: 0.860011  [   32/  118]
train() client id: f_00009-10-1 loss: 0.731393  [   64/  118]
train() client id: f_00009-10-2 loss: 0.851131  [   96/  118]
At round 58 accuracy: 0.6472148541114059
At round 58 training accuracy: 0.5922199865861838
At round 58 training loss: 0.8230364382670092
update_location
xs = [  -3.9056584     4.20031788  310.00902392   18.81129433    0.97929623
    3.95640986 -272.44319194 -251.32485185  294.66397685 -237.06087855]
ys = [ 302.5879595   285.55583871    1.32061395 -272.45517586  264.35018685
  247.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [318.70790295 302.58846589 325.74121466 290.83618696 282.63400416
 267.25924958 290.22781289 270.490032   311.66573316 257.32056272]
dists_bs = [213.4290541  210.55960534 515.16820789 487.62583218 197.36303755
 193.20273595 202.59765721 190.16660451 495.34711972 181.91219277]
uav_gains = [1.90906750e-12 2.54420374e-12 1.70337274e-12 3.20136059e-12
 3.78635653e-12 5.22235086e-12 3.24089386e-12 4.88091679e-12
 2.15488058e-12 6.41123405e-12]
bs_gains = [3.32188179e-11 3.45019724e-11 2.81732361e-12 3.28587708e-12
 4.13570723e-11 4.38992321e-11 3.84341882e-11 4.58900129e-11
 3.14446733e-12 5.19614211e-11]
Round 59
-------------------------------
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.22013439 6.49883235 3.18055184 1.17195349 7.49218211 3.60548997
 1.43848068 4.45606788 3.28586187 2.92340788]
obj_prev = 37.272962461679306
eta_min = 1.7285175423879787e-29	eta_max = 0.9377803249311158
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 8.56318552740206	eta = 0.909090909090909
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 19.714881852065698	eta = 0.3948648627079844
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 13.689416383079331	eta = 0.5686666179167633
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.622409124377576	eta = 0.6167375846489944
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557303683729339	eta = 0.6199351637809642
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030281762911	eta = 0.6199486615180114
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030276907934	eta = 0.6199486617577047
eta = 0.6199486617577047
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.0394532  0.08297697 0.03882696 0.01346419 0.09581492 0.04571561
 0.01690851 0.0560486  0.04070568 0.03694824]
ene_total = [1.24429988 1.90408729 1.25458552 0.61484934 2.16708445 1.11417022
 0.68545833 1.4588381  1.18872479 0.92493235]
ti_comp = [1.09084903 1.21156017 1.07968336 1.13102359 1.21456653 1.21550713
 1.13180693 1.15453068 1.12986699 1.21804473]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [3.22550318e-06 2.43255565e-05 3.13824884e-06 1.19255159e-07
 3.72680608e-05 4.04164983e-06 2.35858602e-07 8.25587810e-06
 3.30210249e-06 2.12488134e-06]
ene_total = [0.4167514  0.16397633 0.44017198 0.33241142 0.15794132 0.15527123
 0.33077065 0.2832709  0.3349044  0.14990784]
optimize_network iter = 0 obj = 2.7653774631169483
eta = 0.6199486617577047
freqs = [18083713.04857494 34243850.94632931 17980716.29606591  5952213.52469169
 39444079.62545728 18805161.01539245  7469699.35866398 24273324.57197437
 18013484.95120145 15167027.70060071]
eta_min = 0.6199486617577048	eta_max = 0.7133529451356779
af = 0.0014346255173217418	bf = 1.0502994844473539	zeta = 0.001578088069053916	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.42985407e-07 4.84915903e-06 6.25591760e-07 2.37728263e-08
 7.42917241e-06 8.05679522e-07 4.70170484e-08 1.64576156e-06
 6.58255038e-07 4.23582807e-07]
ene_total = [1.74202974 0.68380564 1.83994709 1.38965906 0.65766716 0.64883763
 1.38279153 1.18365265 1.39985765 0.62655025]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 1 obj = 3.6656508882711805
eta = 0.7133529451356779
freqs = [18007133.55773868 32762205.35417381 17980716.2960659   5842034.71544074
 37704373.23913116 17970837.74832523  7329456.31607533 23636668.595279
 17687093.31024747 14483474.75860257]
eta_min = 0.7133529451356788	eta_max = 0.7133529451356644
af = 0.0013279300383138625	bf = 1.0502994844473539	zeta = 0.0014607230421452488	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.37551209e-07 4.43861511e-06 6.25591760e-07 2.29008752e-08
 6.78828762e-06 7.35774715e-07 4.52681388e-08 1.56056161e-06
 6.34616915e-07 3.86262812e-07]
ene_total = [1.74202926 0.68376964 1.83994709 1.38965899 0.65761096 0.6488315
 1.38279138 1.18364518 1.39985558 0.62654698]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 2 obj = 3.6656508882710073
eta = 0.7133529451356644
freqs = [18007133.55773862 32762205.35417396 17980716.29606583  5842034.71544073
 37704373.23913135 17970837.74832532  7329456.31607533 23636668.59527903
 17687093.31024747 14483474.75860265]
Done!
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.24710003e-06 1.56442526e-05 2.20494800e-06 8.07159591e-08
 2.39258606e-05 2.59329661e-06 1.59551162e-07 5.50032375e-06
 2.23675788e-06 1.36141406e-06]
ene_total = [0.01986593 0.00780822 0.02098246 0.01584631 0.00751586 0.00740047
 0.01576806 0.01350102 0.01596413 0.00714548]
At round 59 energy consumption: 0.131797934436514
At round 59 eta: 0.7133529451356644
At round 59 a_n: 7.972397885064655
At round 59 local rounds: 11.060599497015481
At round 59 global rounds: 27.812593047006303
gradient difference: 0.4528314173221588
train() client id: f_00000-0-0 loss: 1.249460  [   32/  126]
train() client id: f_00000-0-1 loss: 1.110397  [   64/  126]
train() client id: f_00000-0-2 loss: 1.152751  [   96/  126]
train() client id: f_00000-1-0 loss: 0.984404  [   32/  126]
train() client id: f_00000-1-1 loss: 1.386296  [   64/  126]
train() client id: f_00000-1-2 loss: 1.153691  [   96/  126]
train() client id: f_00000-2-0 loss: 1.104158  [   32/  126]
train() client id: f_00000-2-1 loss: 1.259838  [   64/  126]
train() client id: f_00000-2-2 loss: 1.054727  [   96/  126]
train() client id: f_00000-3-0 loss: 0.945166  [   32/  126]
train() client id: f_00000-3-1 loss: 1.090476  [   64/  126]
train() client id: f_00000-3-2 loss: 1.100937  [   96/  126]
train() client id: f_00000-4-0 loss: 1.031223  [   32/  126]
train() client id: f_00000-4-1 loss: 1.039952  [   64/  126]
train() client id: f_00000-4-2 loss: 1.055406  [   96/  126]
train() client id: f_00000-5-0 loss: 0.888428  [   32/  126]
train() client id: f_00000-5-1 loss: 1.047976  [   64/  126]
train() client id: f_00000-5-2 loss: 0.887109  [   96/  126]
train() client id: f_00000-6-0 loss: 1.016184  [   32/  126]
train() client id: f_00000-6-1 loss: 0.994204  [   64/  126]
train() client id: f_00000-6-2 loss: 0.781984  [   96/  126]
train() client id: f_00000-7-0 loss: 0.896888  [   32/  126]
train() client id: f_00000-7-1 loss: 0.913756  [   64/  126]
train() client id: f_00000-7-2 loss: 0.971706  [   96/  126]
train() client id: f_00000-8-0 loss: 0.941432  [   32/  126]
train() client id: f_00000-8-1 loss: 1.080594  [   64/  126]
train() client id: f_00000-8-2 loss: 0.914619  [   96/  126]
train() client id: f_00000-9-0 loss: 0.982659  [   32/  126]
train() client id: f_00000-9-1 loss: 0.899921  [   64/  126]
train() client id: f_00000-9-2 loss: 1.026530  [   96/  126]
train() client id: f_00000-10-0 loss: 0.806443  [   32/  126]
train() client id: f_00000-10-1 loss: 1.079119  [   64/  126]
train() client id: f_00000-10-2 loss: 0.863349  [   96/  126]
train() client id: f_00001-0-0 loss: 0.402480  [   32/  265]
train() client id: f_00001-0-1 loss: 0.449362  [   64/  265]
train() client id: f_00001-0-2 loss: 0.496387  [   96/  265]
train() client id: f_00001-0-3 loss: 0.468006  [  128/  265]
train() client id: f_00001-0-4 loss: 0.398136  [  160/  265]
train() client id: f_00001-0-5 loss: 0.443513  [  192/  265]
train() client id: f_00001-0-6 loss: 0.488950  [  224/  265]
train() client id: f_00001-0-7 loss: 0.623117  [  256/  265]
train() client id: f_00001-1-0 loss: 0.549124  [   32/  265]
train() client id: f_00001-1-1 loss: 0.404024  [   64/  265]
train() client id: f_00001-1-2 loss: 0.609242  [   96/  265]
train() client id: f_00001-1-3 loss: 0.432849  [  128/  265]
train() client id: f_00001-1-4 loss: 0.399780  [  160/  265]
train() client id: f_00001-1-5 loss: 0.380609  [  192/  265]
train() client id: f_00001-1-6 loss: 0.393024  [  224/  265]
train() client id: f_00001-1-7 loss: 0.496730  [  256/  265]
train() client id: f_00001-2-0 loss: 0.443475  [   32/  265]
train() client id: f_00001-2-1 loss: 0.436291  [   64/  265]
train() client id: f_00001-2-2 loss: 0.439284  [   96/  265]
train() client id: f_00001-2-3 loss: 0.465798  [  128/  265]
train() client id: f_00001-2-4 loss: 0.534241  [  160/  265]
train() client id: f_00001-2-5 loss: 0.383576  [  192/  265]
train() client id: f_00001-2-6 loss: 0.444934  [  224/  265]
train() client id: f_00001-2-7 loss: 0.492107  [  256/  265]
train() client id: f_00001-3-0 loss: 0.344591  [   32/  265]
train() client id: f_00001-3-1 loss: 0.386084  [   64/  265]
train() client id: f_00001-3-2 loss: 0.577356  [   96/  265]
train() client id: f_00001-3-3 loss: 0.431049  [  128/  265]
train() client id: f_00001-3-4 loss: 0.491941  [  160/  265]
train() client id: f_00001-3-5 loss: 0.406440  [  192/  265]
train() client id: f_00001-3-6 loss: 0.385185  [  224/  265]
train() client id: f_00001-3-7 loss: 0.497503  [  256/  265]
train() client id: f_00001-4-0 loss: 0.544158  [   32/  265]
train() client id: f_00001-4-1 loss: 0.355036  [   64/  265]
train() client id: f_00001-4-2 loss: 0.470621  [   96/  265]
train() client id: f_00001-4-3 loss: 0.339703  [  128/  265]
train() client id: f_00001-4-4 loss: 0.343206  [  160/  265]
train() client id: f_00001-4-5 loss: 0.477744  [  192/  265]
train() client id: f_00001-4-6 loss: 0.373908  [  224/  265]
train() client id: f_00001-4-7 loss: 0.554761  [  256/  265]
train() client id: f_00001-5-0 loss: 0.491646  [   32/  265]
train() client id: f_00001-5-1 loss: 0.445657  [   64/  265]
train() client id: f_00001-5-2 loss: 0.463286  [   96/  265]
train() client id: f_00001-5-3 loss: 0.443465  [  128/  265]
train() client id: f_00001-5-4 loss: 0.343991  [  160/  265]
train() client id: f_00001-5-5 loss: 0.493925  [  192/  265]
train() client id: f_00001-5-6 loss: 0.470988  [  224/  265]
train() client id: f_00001-5-7 loss: 0.346526  [  256/  265]
train() client id: f_00001-6-0 loss: 0.487477  [   32/  265]
train() client id: f_00001-6-1 loss: 0.355026  [   64/  265]
train() client id: f_00001-6-2 loss: 0.632643  [   96/  265]
train() client id: f_00001-6-3 loss: 0.404080  [  128/  265]
train() client id: f_00001-6-4 loss: 0.398857  [  160/  265]
train() client id: f_00001-6-5 loss: 0.496143  [  192/  265]
train() client id: f_00001-6-6 loss: 0.400167  [  224/  265]
train() client id: f_00001-6-7 loss: 0.354495  [  256/  265]
train() client id: f_00001-7-0 loss: 0.611077  [   32/  265]
train() client id: f_00001-7-1 loss: 0.370888  [   64/  265]
train() client id: f_00001-7-2 loss: 0.338655  [   96/  265]
train() client id: f_00001-7-3 loss: 0.409093  [  128/  265]
train() client id: f_00001-7-4 loss: 0.489949  [  160/  265]
train() client id: f_00001-7-5 loss: 0.423799  [  192/  265]
train() client id: f_00001-7-6 loss: 0.499647  [  224/  265]
train() client id: f_00001-7-7 loss: 0.363917  [  256/  265]
train() client id: f_00001-8-0 loss: 0.388517  [   32/  265]
train() client id: f_00001-8-1 loss: 0.458666  [   64/  265]
train() client id: f_00001-8-2 loss: 0.411362  [   96/  265]
train() client id: f_00001-8-3 loss: 0.423959  [  128/  265]
train() client id: f_00001-8-4 loss: 0.445316  [  160/  265]
train() client id: f_00001-8-5 loss: 0.503984  [  192/  265]
train() client id: f_00001-8-6 loss: 0.373794  [  224/  265]
train() client id: f_00001-8-7 loss: 0.412907  [  256/  265]
train() client id: f_00001-9-0 loss: 0.426655  [   32/  265]
train() client id: f_00001-9-1 loss: 0.509514  [   64/  265]
train() client id: f_00001-9-2 loss: 0.422416  [   96/  265]
train() client id: f_00001-9-3 loss: 0.380456  [  128/  265]
train() client id: f_00001-9-4 loss: 0.458487  [  160/  265]
train() client id: f_00001-9-5 loss: 0.422812  [  192/  265]
train() client id: f_00001-9-6 loss: 0.457011  [  224/  265]
train() client id: f_00001-9-7 loss: 0.423673  [  256/  265]
train() client id: f_00001-10-0 loss: 0.409657  [   32/  265]
train() client id: f_00001-10-1 loss: 0.431854  [   64/  265]
train() client id: f_00001-10-2 loss: 0.339035  [   96/  265]
train() client id: f_00001-10-3 loss: 0.495820  [  128/  265]
train() client id: f_00001-10-4 loss: 0.587237  [  160/  265]
train() client id: f_00001-10-5 loss: 0.395087  [  192/  265]
train() client id: f_00001-10-6 loss: 0.382808  [  224/  265]
train() client id: f_00001-10-7 loss: 0.475561  [  256/  265]
train() client id: f_00002-0-0 loss: 1.104113  [   32/  124]
train() client id: f_00002-0-1 loss: 1.314599  [   64/  124]
train() client id: f_00002-0-2 loss: 1.175865  [   96/  124]
train() client id: f_00002-1-0 loss: 1.171901  [   32/  124]
train() client id: f_00002-1-1 loss: 1.108892  [   64/  124]
train() client id: f_00002-1-2 loss: 1.120579  [   96/  124]
train() client id: f_00002-2-0 loss: 1.109120  [   32/  124]
train() client id: f_00002-2-1 loss: 1.209347  [   64/  124]
train() client id: f_00002-2-2 loss: 1.079785  [   96/  124]
train() client id: f_00002-3-0 loss: 0.997895  [   32/  124]
train() client id: f_00002-3-1 loss: 1.270667  [   64/  124]
train() client id: f_00002-3-2 loss: 1.112575  [   96/  124]
train() client id: f_00002-4-0 loss: 0.985039  [   32/  124]
train() client id: f_00002-4-1 loss: 1.357456  [   64/  124]
train() client id: f_00002-4-2 loss: 1.024417  [   96/  124]
train() client id: f_00002-5-0 loss: 1.135759  [   32/  124]
train() client id: f_00002-5-1 loss: 1.007181  [   64/  124]
train() client id: f_00002-5-2 loss: 1.118381  [   96/  124]
train() client id: f_00002-6-0 loss: 1.042777  [   32/  124]
train() client id: f_00002-6-1 loss: 1.097788  [   64/  124]
train() client id: f_00002-6-2 loss: 1.072828  [   96/  124]
train() client id: f_00002-7-0 loss: 1.017613  [   32/  124]
train() client id: f_00002-7-1 loss: 1.308135  [   64/  124]
train() client id: f_00002-7-2 loss: 0.853940  [   96/  124]
train() client id: f_00002-8-0 loss: 0.994513  [   32/  124]
train() client id: f_00002-8-1 loss: 1.104332  [   64/  124]
train() client id: f_00002-8-2 loss: 1.004457  [   96/  124]
train() client id: f_00002-9-0 loss: 0.998081  [   32/  124]
train() client id: f_00002-9-1 loss: 0.993843  [   64/  124]
train() client id: f_00002-9-2 loss: 1.057214  [   96/  124]
train() client id: f_00002-10-0 loss: 0.921609  [   32/  124]
train() client id: f_00002-10-1 loss: 0.868110  [   64/  124]
train() client id: f_00002-10-2 loss: 1.139607  [   96/  124]
train() client id: f_00003-0-0 loss: 0.391399  [   32/   43]
train() client id: f_00003-1-0 loss: 0.706853  [   32/   43]
train() client id: f_00003-2-0 loss: 0.403688  [   32/   43]
train() client id: f_00003-3-0 loss: 0.532819  [   32/   43]
train() client id: f_00003-4-0 loss: 0.669165  [   32/   43]
train() client id: f_00003-5-0 loss: 0.690463  [   32/   43]
train() client id: f_00003-6-0 loss: 0.493413  [   32/   43]
train() client id: f_00003-7-0 loss: 0.336605  [   32/   43]
train() client id: f_00003-8-0 loss: 0.451982  [   32/   43]
train() client id: f_00003-9-0 loss: 0.413464  [   32/   43]
train() client id: f_00003-10-0 loss: 0.652048  [   32/   43]
train() client id: f_00004-0-0 loss: 0.829233  [   32/  306]
train() client id: f_00004-0-1 loss: 0.739530  [   64/  306]
train() client id: f_00004-0-2 loss: 0.751116  [   96/  306]
train() client id: f_00004-0-3 loss: 0.800762  [  128/  306]
train() client id: f_00004-0-4 loss: 0.836426  [  160/  306]
train() client id: f_00004-0-5 loss: 0.685015  [  192/  306]
train() client id: f_00004-0-6 loss: 0.567374  [  224/  306]
train() client id: f_00004-0-7 loss: 0.892069  [  256/  306]
train() client id: f_00004-0-8 loss: 0.739141  [  288/  306]
train() client id: f_00004-1-0 loss: 0.728768  [   32/  306]
train() client id: f_00004-1-1 loss: 0.875685  [   64/  306]
train() client id: f_00004-1-2 loss: 0.766381  [   96/  306]
train() client id: f_00004-1-3 loss: 0.621799  [  128/  306]
train() client id: f_00004-1-4 loss: 0.695893  [  160/  306]
train() client id: f_00004-1-5 loss: 0.902357  [  192/  306]
train() client id: f_00004-1-6 loss: 0.822454  [  224/  306]
train() client id: f_00004-1-7 loss: 0.829357  [  256/  306]
train() client id: f_00004-1-8 loss: 0.676632  [  288/  306]
train() client id: f_00004-2-0 loss: 0.773899  [   32/  306]
train() client id: f_00004-2-1 loss: 0.685296  [   64/  306]
train() client id: f_00004-2-2 loss: 0.865642  [   96/  306]
train() client id: f_00004-2-3 loss: 0.772268  [  128/  306]
train() client id: f_00004-2-4 loss: 0.800948  [  160/  306]
train() client id: f_00004-2-5 loss: 0.935435  [  192/  306]
train() client id: f_00004-2-6 loss: 0.591584  [  224/  306]
train() client id: f_00004-2-7 loss: 0.645550  [  256/  306]
train() client id: f_00004-2-8 loss: 0.877409  [  288/  306]
train() client id: f_00004-3-0 loss: 0.897593  [   32/  306]
train() client id: f_00004-3-1 loss: 0.741260  [   64/  306]
train() client id: f_00004-3-2 loss: 0.731674  [   96/  306]
train() client id: f_00004-3-3 loss: 0.731425  [  128/  306]
train() client id: f_00004-3-4 loss: 0.748454  [  160/  306]
train() client id: f_00004-3-5 loss: 0.792812  [  192/  306]
train() client id: f_00004-3-6 loss: 0.859631  [  224/  306]
train() client id: f_00004-3-7 loss: 0.719582  [  256/  306]
train() client id: f_00004-3-8 loss: 0.693263  [  288/  306]
train() client id: f_00004-4-0 loss: 0.798779  [   32/  306]
train() client id: f_00004-4-1 loss: 0.781121  [   64/  306]
train() client id: f_00004-4-2 loss: 0.596453  [   96/  306]
train() client id: f_00004-4-3 loss: 0.856313  [  128/  306]
train() client id: f_00004-4-4 loss: 0.778384  [  160/  306]
train() client id: f_00004-4-5 loss: 0.728364  [  192/  306]
train() client id: f_00004-4-6 loss: 0.713995  [  224/  306]
train() client id: f_00004-4-7 loss: 0.728828  [  256/  306]
train() client id: f_00004-4-8 loss: 0.923679  [  288/  306]
train() client id: f_00004-5-0 loss: 0.740681  [   32/  306]
train() client id: f_00004-5-1 loss: 0.730365  [   64/  306]
train() client id: f_00004-5-2 loss: 0.670116  [   96/  306]
train() client id: f_00004-5-3 loss: 0.746808  [  128/  306]
train() client id: f_00004-5-4 loss: 0.776121  [  160/  306]
train() client id: f_00004-5-5 loss: 0.886989  [  192/  306]
train() client id: f_00004-5-6 loss: 0.693536  [  224/  306]
train() client id: f_00004-5-7 loss: 0.910381  [  256/  306]
train() client id: f_00004-5-8 loss: 0.872759  [  288/  306]
train() client id: f_00004-6-0 loss: 0.700848  [   32/  306]
train() client id: f_00004-6-1 loss: 0.732178  [   64/  306]
train() client id: f_00004-6-2 loss: 0.748974  [   96/  306]
train() client id: f_00004-6-3 loss: 0.870031  [  128/  306]
train() client id: f_00004-6-4 loss: 0.755184  [  160/  306]
train() client id: f_00004-6-5 loss: 0.776594  [  192/  306]
train() client id: f_00004-6-6 loss: 0.823766  [  224/  306]
train() client id: f_00004-6-7 loss: 0.806161  [  256/  306]
train() client id: f_00004-6-8 loss: 0.865314  [  288/  306]
train() client id: f_00004-7-0 loss: 0.818486  [   32/  306]
train() client id: f_00004-7-1 loss: 0.857606  [   64/  306]
train() client id: f_00004-7-2 loss: 0.821156  [   96/  306]
train() client id: f_00004-7-3 loss: 0.788262  [  128/  306]
train() client id: f_00004-7-4 loss: 0.733495  [  160/  306]
train() client id: f_00004-7-5 loss: 0.754562  [  192/  306]
train() client id: f_00004-7-6 loss: 0.794631  [  224/  306]
train() client id: f_00004-7-7 loss: 0.732325  [  256/  306]
train() client id: f_00004-7-8 loss: 0.720886  [  288/  306]
train() client id: f_00004-8-0 loss: 0.814029  [   32/  306]
train() client id: f_00004-8-1 loss: 0.800861  [   64/  306]
train() client id: f_00004-8-2 loss: 0.784560  [   96/  306]
train() client id: f_00004-8-3 loss: 0.739795  [  128/  306]
train() client id: f_00004-8-4 loss: 0.860776  [  160/  306]
train() client id: f_00004-8-5 loss: 0.786133  [  192/  306]
train() client id: f_00004-8-6 loss: 0.659121  [  224/  306]
train() client id: f_00004-8-7 loss: 0.804835  [  256/  306]
train() client id: f_00004-8-8 loss: 0.733603  [  288/  306]
train() client id: f_00004-9-0 loss: 0.805306  [   32/  306]
train() client id: f_00004-9-1 loss: 0.736689  [   64/  306]
train() client id: f_00004-9-2 loss: 0.802735  [   96/  306]
train() client id: f_00004-9-3 loss: 0.721538  [  128/  306]
train() client id: f_00004-9-4 loss: 0.827502  [  160/  306]
train() client id: f_00004-9-5 loss: 0.748684  [  192/  306]
train() client id: f_00004-9-6 loss: 0.741799  [  224/  306]
train() client id: f_00004-9-7 loss: 0.837868  [  256/  306]
train() client id: f_00004-9-8 loss: 0.723366  [  288/  306]
train() client id: f_00004-10-0 loss: 0.802947  [   32/  306]
train() client id: f_00004-10-1 loss: 0.760899  [   64/  306]
train() client id: f_00004-10-2 loss: 0.813729  [   96/  306]
train() client id: f_00004-10-3 loss: 0.768196  [  128/  306]
train() client id: f_00004-10-4 loss: 0.850471  [  160/  306]
train() client id: f_00004-10-5 loss: 0.719370  [  192/  306]
train() client id: f_00004-10-6 loss: 0.861409  [  224/  306]
train() client id: f_00004-10-7 loss: 0.722502  [  256/  306]
train() client id: f_00004-10-8 loss: 0.808414  [  288/  306]
train() client id: f_00005-0-0 loss: 0.198516  [   32/  146]
train() client id: f_00005-0-1 loss: 0.014771  [   64/  146]
train() client id: f_00005-0-2 loss: 0.483294  [   96/  146]
train() client id: f_00005-0-3 loss: 0.244297  [  128/  146]
train() client id: f_00005-1-0 loss: 0.126541  [   32/  146]
train() client id: f_00005-1-1 loss: -0.022046  [   64/  146]
train() client id: f_00005-1-2 loss: 0.026569  [   96/  146]
train() client id: f_00005-1-3 loss: 0.345685  [  128/  146]
train() client id: f_00005-2-0 loss: 0.431837  [   32/  146]
train() client id: f_00005-2-1 loss: 0.256641  [   64/  146]
train() client id: f_00005-2-2 loss: 0.047953  [   96/  146]
train() client id: f_00005-2-3 loss: 0.183878  [  128/  146]
train() client id: f_00005-3-0 loss: 0.299355  [   32/  146]
train() client id: f_00005-3-1 loss: 0.213815  [   64/  146]
train() client id: f_00005-3-2 loss: 0.153087  [   96/  146]
train() client id: f_00005-3-3 loss: 0.332120  [  128/  146]
train() client id: f_00005-4-0 loss: 0.301371  [   32/  146]
train() client id: f_00005-4-1 loss: -0.059666  [   64/  146]
train() client id: f_00005-4-2 loss: 0.198661  [   96/  146]
train() client id: f_00005-4-3 loss: 0.125910  [  128/  146]
train() client id: f_00005-5-0 loss: 0.040833  [   32/  146]
train() client id: f_00005-5-1 loss: 0.218903  [   64/  146]
train() client id: f_00005-5-2 loss: 0.413160  [   96/  146]
train() client id: f_00005-5-3 loss: 0.364428  [  128/  146]
train() client id: f_00005-6-0 loss: 0.069460  [   32/  146]
train() client id: f_00005-6-1 loss: 0.372839  [   64/  146]
train() client id: f_00005-6-2 loss: 0.368603  [   96/  146]
train() client id: f_00005-6-3 loss: 0.171048  [  128/  146]
train() client id: f_00005-7-0 loss: 0.235896  [   32/  146]
train() client id: f_00005-7-1 loss: 0.049315  [   64/  146]
train() client id: f_00005-7-2 loss: 0.369906  [   96/  146]
train() client id: f_00005-7-3 loss: 0.078130  [  128/  146]
train() client id: f_00005-8-0 loss: 0.548456  [   32/  146]
train() client id: f_00005-8-1 loss: 0.130569  [   64/  146]
train() client id: f_00005-8-2 loss: 0.080081  [   96/  146]
train() client id: f_00005-8-3 loss: 0.103603  [  128/  146]
train() client id: f_00005-9-0 loss: 0.138406  [   32/  146]
train() client id: f_00005-9-1 loss: 0.222186  [   64/  146]
train() client id: f_00005-9-2 loss: 0.300604  [   96/  146]
train() client id: f_00005-9-3 loss: 0.072555  [  128/  146]
train() client id: f_00005-10-0 loss: 0.282804  [   32/  146]
train() client id: f_00005-10-1 loss: -0.011205  [   64/  146]
train() client id: f_00005-10-2 loss: 0.467371  [   96/  146]
train() client id: f_00005-10-3 loss: 0.229975  [  128/  146]
train() client id: f_00006-0-0 loss: 0.492664  [   32/   54]
train() client id: f_00006-1-0 loss: 0.432606  [   32/   54]
train() client id: f_00006-2-0 loss: 0.498647  [   32/   54]
train() client id: f_00006-3-0 loss: 0.501134  [   32/   54]
train() client id: f_00006-4-0 loss: 0.409823  [   32/   54]
train() client id: f_00006-5-0 loss: 0.503554  [   32/   54]
train() client id: f_00006-6-0 loss: 0.459301  [   32/   54]
train() client id: f_00006-7-0 loss: 0.430270  [   32/   54]
train() client id: f_00006-8-0 loss: 0.456709  [   32/   54]
train() client id: f_00006-9-0 loss: 0.482701  [   32/   54]
train() client id: f_00006-10-0 loss: 0.505924  [   32/   54]
train() client id: f_00007-0-0 loss: 0.514285  [   32/  179]
train() client id: f_00007-0-1 loss: 0.529765  [   64/  179]
train() client id: f_00007-0-2 loss: 0.475000  [   96/  179]
train() client id: f_00007-0-3 loss: 0.818940  [  128/  179]
train() client id: f_00007-0-4 loss: 0.566881  [  160/  179]
train() client id: f_00007-1-0 loss: 0.603032  [   32/  179]
train() client id: f_00007-1-1 loss: 0.571678  [   64/  179]
train() client id: f_00007-1-2 loss: 0.401617  [   96/  179]
train() client id: f_00007-1-3 loss: 0.685358  [  128/  179]
train() client id: f_00007-1-4 loss: 0.587916  [  160/  179]
train() client id: f_00007-2-0 loss: 0.645567  [   32/  179]
train() client id: f_00007-2-1 loss: 0.576502  [   64/  179]
train() client id: f_00007-2-2 loss: 0.595159  [   96/  179]
train() client id: f_00007-2-3 loss: 0.503552  [  128/  179]
train() client id: f_00007-2-4 loss: 0.564363  [  160/  179]
train() client id: f_00007-3-0 loss: 0.582317  [   32/  179]
train() client id: f_00007-3-1 loss: 0.429893  [   64/  179]
train() client id: f_00007-3-2 loss: 0.620052  [   96/  179]
train() client id: f_00007-3-3 loss: 0.473166  [  128/  179]
train() client id: f_00007-3-4 loss: 0.643185  [  160/  179]
train() client id: f_00007-4-0 loss: 0.536398  [   32/  179]
train() client id: f_00007-4-1 loss: 0.349571  [   64/  179]
train() client id: f_00007-4-2 loss: 0.358465  [   96/  179]
train() client id: f_00007-4-3 loss: 0.874936  [  128/  179]
train() client id: f_00007-4-4 loss: 0.545267  [  160/  179]
train() client id: f_00007-5-0 loss: 0.730480  [   32/  179]
train() client id: f_00007-5-1 loss: 0.405856  [   64/  179]
train() client id: f_00007-5-2 loss: 0.390611  [   96/  179]
train() client id: f_00007-5-3 loss: 0.492160  [  128/  179]
train() client id: f_00007-5-4 loss: 0.763176  [  160/  179]
train() client id: f_00007-6-0 loss: 0.448397  [   32/  179]
train() client id: f_00007-6-1 loss: 0.435305  [   64/  179]
train() client id: f_00007-6-2 loss: 0.498902  [   96/  179]
train() client id: f_00007-6-3 loss: 0.655610  [  128/  179]
train() client id: f_00007-6-4 loss: 0.618010  [  160/  179]
train() client id: f_00007-7-0 loss: 0.696153  [   32/  179]
train() client id: f_00007-7-1 loss: 0.361566  [   64/  179]
train() client id: f_00007-7-2 loss: 0.497343  [   96/  179]
train() client id: f_00007-7-3 loss: 0.449427  [  128/  179]
train() client id: f_00007-7-4 loss: 0.633248  [  160/  179]
train() client id: f_00007-8-0 loss: 0.530737  [   32/  179]
train() client id: f_00007-8-1 loss: 0.605013  [   64/  179]
train() client id: f_00007-8-2 loss: 0.543590  [   96/  179]
train() client id: f_00007-8-3 loss: 0.373976  [  128/  179]
train() client id: f_00007-8-4 loss: 0.646369  [  160/  179]
train() client id: f_00007-9-0 loss: 0.486418  [   32/  179]
train() client id: f_00007-9-1 loss: 0.361806  [   64/  179]
train() client id: f_00007-9-2 loss: 0.511399  [   96/  179]
train() client id: f_00007-9-3 loss: 0.552957  [  128/  179]
train() client id: f_00007-9-4 loss: 0.630037  [  160/  179]
train() client id: f_00007-10-0 loss: 0.370769  [   32/  179]
train() client id: f_00007-10-1 loss: 0.479507  [   64/  179]
train() client id: f_00007-10-2 loss: 0.588271  [   96/  179]
train() client id: f_00007-10-3 loss: 0.606684  [  128/  179]
train() client id: f_00007-10-4 loss: 0.548947  [  160/  179]
train() client id: f_00008-0-0 loss: 0.547233  [   32/  130]
train() client id: f_00008-0-1 loss: 0.626793  [   64/  130]
train() client id: f_00008-0-2 loss: 0.681752  [   96/  130]
train() client id: f_00008-0-3 loss: 0.729739  [  128/  130]
train() client id: f_00008-1-0 loss: 0.649344  [   32/  130]
train() client id: f_00008-1-1 loss: 0.719154  [   64/  130]
train() client id: f_00008-1-2 loss: 0.615502  [   96/  130]
train() client id: f_00008-1-3 loss: 0.623555  [  128/  130]
train() client id: f_00008-2-0 loss: 0.548350  [   32/  130]
train() client id: f_00008-2-1 loss: 0.638991  [   64/  130]
train() client id: f_00008-2-2 loss: 0.656097  [   96/  130]
train() client id: f_00008-2-3 loss: 0.766175  [  128/  130]
train() client id: f_00008-3-0 loss: 0.712546  [   32/  130]
train() client id: f_00008-3-1 loss: 0.583203  [   64/  130]
train() client id: f_00008-3-2 loss: 0.717485  [   96/  130]
train() client id: f_00008-3-3 loss: 0.602921  [  128/  130]
train() client id: f_00008-4-0 loss: 0.671155  [   32/  130]
train() client id: f_00008-4-1 loss: 0.703116  [   64/  130]
train() client id: f_00008-4-2 loss: 0.627214  [   96/  130]
train() client id: f_00008-4-3 loss: 0.616013  [  128/  130]
train() client id: f_00008-5-0 loss: 0.622496  [   32/  130]
train() client id: f_00008-5-1 loss: 0.662791  [   64/  130]
train() client id: f_00008-5-2 loss: 0.699973  [   96/  130]
train() client id: f_00008-5-3 loss: 0.609035  [  128/  130]
train() client id: f_00008-6-0 loss: 0.661802  [   32/  130]
train() client id: f_00008-6-1 loss: 0.574187  [   64/  130]
train() client id: f_00008-6-2 loss: 0.751001  [   96/  130]
train() client id: f_00008-6-3 loss: 0.632935  [  128/  130]
train() client id: f_00008-7-0 loss: 0.604210  [   32/  130]
train() client id: f_00008-7-1 loss: 0.693011  [   64/  130]
train() client id: f_00008-7-2 loss: 0.563461  [   96/  130]
train() client id: f_00008-7-3 loss: 0.721637  [  128/  130]
train() client id: f_00008-8-0 loss: 0.762732  [   32/  130]
train() client id: f_00008-8-1 loss: 0.658222  [   64/  130]
train() client id: f_00008-8-2 loss: 0.591162  [   96/  130]
train() client id: f_00008-8-3 loss: 0.593903  [  128/  130]
train() client id: f_00008-9-0 loss: 0.658813  [   32/  130]
train() client id: f_00008-9-1 loss: 0.589842  [   64/  130]
train() client id: f_00008-9-2 loss: 0.645564  [   96/  130]
train() client id: f_00008-9-3 loss: 0.721805  [  128/  130]
train() client id: f_00008-10-0 loss: 0.657295  [   32/  130]
train() client id: f_00008-10-1 loss: 0.602283  [   64/  130]
train() client id: f_00008-10-2 loss: 0.756195  [   96/  130]
train() client id: f_00008-10-3 loss: 0.617341  [  128/  130]
train() client id: f_00009-0-0 loss: 1.110856  [   32/  118]
train() client id: f_00009-0-1 loss: 1.065639  [   64/  118]
train() client id: f_00009-0-2 loss: 0.927043  [   96/  118]
train() client id: f_00009-1-0 loss: 0.971167  [   32/  118]
train() client id: f_00009-1-1 loss: 0.908808  [   64/  118]
train() client id: f_00009-1-2 loss: 1.058636  [   96/  118]
train() client id: f_00009-2-0 loss: 0.909244  [   32/  118]
train() client id: f_00009-2-1 loss: 0.989468  [   64/  118]
train() client id: f_00009-2-2 loss: 0.823546  [   96/  118]
train() client id: f_00009-3-0 loss: 0.891225  [   32/  118]
train() client id: f_00009-3-1 loss: 1.016163  [   64/  118]
train() client id: f_00009-3-2 loss: 0.762492  [   96/  118]
train() client id: f_00009-4-0 loss: 0.775756  [   32/  118]
train() client id: f_00009-4-1 loss: 0.942865  [   64/  118]
train() client id: f_00009-4-2 loss: 0.800183  [   96/  118]
train() client id: f_00009-5-0 loss: 0.770392  [   32/  118]
train() client id: f_00009-5-1 loss: 0.854393  [   64/  118]
train() client id: f_00009-5-2 loss: 0.757703  [   96/  118]
train() client id: f_00009-6-0 loss: 0.847993  [   32/  118]
train() client id: f_00009-6-1 loss: 0.993361  [   64/  118]
train() client id: f_00009-6-2 loss: 0.642595  [   96/  118]
train() client id: f_00009-7-0 loss: 0.848771  [   32/  118]
train() client id: f_00009-7-1 loss: 0.802572  [   64/  118]
train() client id: f_00009-7-2 loss: 0.774987  [   96/  118]
train() client id: f_00009-8-0 loss: 0.729211  [   32/  118]
train() client id: f_00009-8-1 loss: 0.918070  [   64/  118]
train() client id: f_00009-8-2 loss: 0.835205  [   96/  118]
train() client id: f_00009-9-0 loss: 0.786184  [   32/  118]
train() client id: f_00009-9-1 loss: 0.745078  [   64/  118]
train() client id: f_00009-9-2 loss: 0.763089  [   96/  118]
train() client id: f_00009-10-0 loss: 0.635433  [   32/  118]
train() client id: f_00009-10-1 loss: 1.058915  [   64/  118]
train() client id: f_00009-10-2 loss: 0.665714  [   96/  118]
At round 59 accuracy: 0.6472148541114059
At round 59 training accuracy: 0.5942320590207915
At round 59 training loss: 0.8183128034571474
update_location
xs = [  -3.9056584     4.20031788  315.00902392   18.81129433    0.97929623
    3.95640986 -277.44319194 -256.32485185  299.66397685 -242.06087855]
ys = [ 307.5879595   290.55583871    1.32061395 -277.45517586  269.35018685
  252.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [323.45881808 307.31146753 330.50329676 295.5253617  287.31599707
 271.90190877 294.92645743 275.14197413 316.3971697  261.93411534]
dists_bs = [216.45517025 213.22735703 519.87822868 492.21794357 199.67641438
 195.1420987  205.04936632 192.2279532  500.09040061 183.67812789]
uav_gains = [1.76622191e-12 2.33041781e-12 1.58304528e-12 2.91576877e-12
 3.43842860e-12 4.73853249e-12 2.95041862e-12 4.42719338e-12
 1.98494725e-12 5.83324094e-12]
bs_gains = [3.19347696e-11 3.33068759e-11 2.74643623e-12 3.20076101e-12
 4.00294047e-11 4.26885463e-11 3.71612640e-11 4.45253937e-11
 3.06166898e-12 5.05746914e-11]
Round 60
-------------------------------
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.0875637  6.22007234 3.04984448 1.12636964 7.17067363 3.45091238
 1.38142813 4.26835796 3.14602737 2.7981042 ]
obj_prev = 35.69935382138897
eta_min = 9.518668131305417e-31	eta_max = 0.9381641580347129
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 8.195255617037894	eta = 0.909090909090909
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 19.17140763760524	eta = 0.38861165126506014
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 13.206898476870052	eta = 0.5641167297661407
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.154598615650826	eta = 0.6129558543819038
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.090043572188366	eta = 0.6162287451356823
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768790589424	eta = 0.6162427510544748
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768785575135	eta = 0.6162427513100645
eta = 0.6162427513100645
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.03994795 0.08401752 0.03931386 0.01363303 0.09701646 0.0462889
 0.01712055 0.05675146 0.04121614 0.03741158]
ene_total = [1.20410292 1.8265834  1.21411492 0.59841362 2.07887597 1.06827008
 0.66615118 1.4063269  1.14028819 0.8866416 ]
ti_comp = [1.15185544 1.27946326 1.14047878 1.19334864 1.28255751 1.28358448
 1.19415401 1.21817826 1.19663411 1.28616459]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [3.00308521e-06 2.26429994e-05 2.91972948e-06 1.11204574e-07
 3.46946537e-05 3.76237107e-06 2.19944230e-07 7.69820333e-06
 3.05604171e-06 1.97835807e-06]
ene_total = [0.40991842 0.15659935 0.43253582 0.32736424 0.15068698 0.14803017
 0.32576518 0.27814891 0.32089064 0.14286493]
optimize_network iter = 0 obj = 2.6928046428038024
eta = 0.6162427513100645
freqs = [17340697.3279414  32833111.65400368 17235681.79847541  5712090.78528441
 37821484.27558679 18031106.89702021  7168485.47995044 23293576.56497024
 17221697.8440477  14543852.06878811]
eta_min = 0.6162427513100649	eta_max = 0.7177903732762834
af = 0.0012621994794922648	bf = 1.0327722521438556	zeta = 0.0013884194274414914	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.91233479e-07 4.45784864e-06 5.74822789e-07 2.18934405e-08
 6.83052241e-06 7.40718154e-07 4.33015993e-08 1.51558655e-06
 6.01659308e-07 3.89489955e-07]
ene_total = [1.73019841 0.65953123 1.82567951 1.3819049  0.63376085 0.62463054
 1.37514743 1.17363935 1.35437921 0.60294663]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 1 obj = 3.6609545155387906
eta = 0.7177903732762834
freqs = [17262263.50383285 31270462.24446457 17235681.79847541  5597996.92086191
 35987511.86898271 17151433.11384352  7023260.57451295 22629570.05842017
 16857767.37652736 13823527.31845897]
eta_min = 0.7177903732762844	eta_max = 0.7177903732762764
af = 0.001159170611255296	bf = 1.0327722521438556	zeta = 0.0012750876723808255	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.85897150e-07 4.04361532e-06 5.74822789e-07 2.10275717e-08
 6.18415585e-06 6.70207126e-07 4.15648943e-08 1.43041150e-06
 5.76499351e-07 3.51864226e-07]
ene_total = [1.73019796 0.65949647 1.82567951 1.38190483 0.6337066  0.62462463
 1.37514729 1.1736322  1.3543771  0.60294347]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 2 obj = 3.6609545155387
eta = 0.7177903732762764
freqs = [17262263.50383282 31270462.24446465 17235681.79847537  5597996.92086191
 35987511.8689828  17151433.11384356  7023260.57451294 22629570.05842018
 16857767.37652736 13823527.31845901]
Done!
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [1.87731017e-06 1.29564040e-05 1.84182611e-06 6.73757745e-08
 1.98150455e-05 2.14745310e-06 1.33180711e-07 4.58327210e-06
 1.84719809e-06 1.12743045e-06]
ene_total = [0.02061646 0.00786676 0.0217541  0.01646533 0.0075642  0.00744383
 0.01638486 0.01398689 0.01613857 0.0071848 ]
At round 60 energy consumption: 0.135405804121047
At round 60 eta: 0.7177903732762764
At round 60 a_n: 7.62985203809534
At round 60 local rounds: 10.857538900724403
At round 60 global rounds: 27.03611541063686
gradient difference: 0.5629528760910034
train() client id: f_00000-0-0 loss: 1.233205  [   32/  126]
train() client id: f_00000-0-1 loss: 1.226087  [   64/  126]
train() client id: f_00000-0-2 loss: 0.914127  [   96/  126]
train() client id: f_00000-1-0 loss: 0.948256  [   32/  126]
train() client id: f_00000-1-1 loss: 1.092697  [   64/  126]
train() client id: f_00000-1-2 loss: 1.003008  [   96/  126]
train() client id: f_00000-2-0 loss: 1.058153  [   32/  126]
train() client id: f_00000-2-1 loss: 1.048662  [   64/  126]
train() client id: f_00000-2-2 loss: 1.083275  [   96/  126]
train() client id: f_00000-3-0 loss: 0.770782  [   32/  126]
train() client id: f_00000-3-1 loss: 1.034447  [   64/  126]
train() client id: f_00000-3-2 loss: 1.037935  [   96/  126]
train() client id: f_00000-4-0 loss: 0.889469  [   32/  126]
train() client id: f_00000-4-1 loss: 0.977935  [   64/  126]
train() client id: f_00000-4-2 loss: 0.942231  [   96/  126]
train() client id: f_00000-5-0 loss: 0.995724  [   32/  126]
train() client id: f_00000-5-1 loss: 0.989398  [   64/  126]
train() client id: f_00000-5-2 loss: 0.785571  [   96/  126]
train() client id: f_00000-6-0 loss: 0.859035  [   32/  126]
train() client id: f_00000-6-1 loss: 0.980873  [   64/  126]
train() client id: f_00000-6-2 loss: 0.958652  [   96/  126]
train() client id: f_00000-7-0 loss: 0.924362  [   32/  126]
train() client id: f_00000-7-1 loss: 0.890289  [   64/  126]
train() client id: f_00000-7-2 loss: 0.828810  [   96/  126]
train() client id: f_00000-8-0 loss: 0.955221  [   32/  126]
train() client id: f_00000-8-1 loss: 0.872843  [   64/  126]
train() client id: f_00000-8-2 loss: 0.947581  [   96/  126]
train() client id: f_00000-9-0 loss: 1.004597  [   32/  126]
train() client id: f_00000-9-1 loss: 0.825467  [   64/  126]
train() client id: f_00000-9-2 loss: 1.006129  [   96/  126]
train() client id: f_00001-0-0 loss: 0.413483  [   32/  265]
train() client id: f_00001-0-1 loss: 0.423152  [   64/  265]
train() client id: f_00001-0-2 loss: 0.296001  [   96/  265]
train() client id: f_00001-0-3 loss: 0.478525  [  128/  265]
train() client id: f_00001-0-4 loss: 0.407604  [  160/  265]
train() client id: f_00001-0-5 loss: 0.379573  [  192/  265]
train() client id: f_00001-0-6 loss: 0.430980  [  224/  265]
train() client id: f_00001-0-7 loss: 0.407604  [  256/  265]
train() client id: f_00001-1-0 loss: 0.329418  [   32/  265]
train() client id: f_00001-1-1 loss: 0.474938  [   64/  265]
train() client id: f_00001-1-2 loss: 0.476992  [   96/  265]
train() client id: f_00001-1-3 loss: 0.381409  [  128/  265]
train() client id: f_00001-1-4 loss: 0.335701  [  160/  265]
train() client id: f_00001-1-5 loss: 0.318135  [  192/  265]
train() client id: f_00001-1-6 loss: 0.469001  [  224/  265]
train() client id: f_00001-1-7 loss: 0.378568  [  256/  265]
train() client id: f_00001-2-0 loss: 0.422633  [   32/  265]
train() client id: f_00001-2-1 loss: 0.358770  [   64/  265]
train() client id: f_00001-2-2 loss: 0.347385  [   96/  265]
train() client id: f_00001-2-3 loss: 0.370798  [  128/  265]
train() client id: f_00001-2-4 loss: 0.330647  [  160/  265]
train() client id: f_00001-2-5 loss: 0.440159  [  192/  265]
train() client id: f_00001-2-6 loss: 0.464926  [  224/  265]
train() client id: f_00001-2-7 loss: 0.391960  [  256/  265]
train() client id: f_00001-3-0 loss: 0.447852  [   32/  265]
train() client id: f_00001-3-1 loss: 0.295997  [   64/  265]
train() client id: f_00001-3-2 loss: 0.436374  [   96/  265]
train() client id: f_00001-3-3 loss: 0.322197  [  128/  265]
train() client id: f_00001-3-4 loss: 0.453818  [  160/  265]
train() client id: f_00001-3-5 loss: 0.327083  [  192/  265]
train() client id: f_00001-3-6 loss: 0.357425  [  224/  265]
train() client id: f_00001-3-7 loss: 0.329217  [  256/  265]
train() client id: f_00001-4-0 loss: 0.419001  [   32/  265]
train() client id: f_00001-4-1 loss: 0.389832  [   64/  265]
train() client id: f_00001-4-2 loss: 0.334141  [   96/  265]
train() client id: f_00001-4-3 loss: 0.321880  [  128/  265]
train() client id: f_00001-4-4 loss: 0.413780  [  160/  265]
train() client id: f_00001-4-5 loss: 0.303454  [  192/  265]
train() client id: f_00001-4-6 loss: 0.384789  [  224/  265]
train() client id: f_00001-4-7 loss: 0.516265  [  256/  265]
train() client id: f_00001-5-0 loss: 0.431151  [   32/  265]
train() client id: f_00001-5-1 loss: 0.325523  [   64/  265]
train() client id: f_00001-5-2 loss: 0.322656  [   96/  265]
train() client id: f_00001-5-3 loss: 0.351344  [  128/  265]
train() client id: f_00001-5-4 loss: 0.408864  [  160/  265]
train() client id: f_00001-5-5 loss: 0.284054  [  192/  265]
train() client id: f_00001-5-6 loss: 0.441247  [  224/  265]
train() client id: f_00001-5-7 loss: 0.463502  [  256/  265]
train() client id: f_00001-6-0 loss: 0.384084  [   32/  265]
train() client id: f_00001-6-1 loss: 0.292620  [   64/  265]
train() client id: f_00001-6-2 loss: 0.277426  [   96/  265]
train() client id: f_00001-6-3 loss: 0.340226  [  128/  265]
train() client id: f_00001-6-4 loss: 0.540878  [  160/  265]
train() client id: f_00001-6-5 loss: 0.325171  [  192/  265]
train() client id: f_00001-6-6 loss: 0.429933  [  224/  265]
train() client id: f_00001-6-7 loss: 0.426195  [  256/  265]
train() client id: f_00001-7-0 loss: 0.402082  [   32/  265]
train() client id: f_00001-7-1 loss: 0.359671  [   64/  265]
train() client id: f_00001-7-2 loss: 0.414624  [   96/  265]
train() client id: f_00001-7-3 loss: 0.372547  [  128/  265]
train() client id: f_00001-7-4 loss: 0.404566  [  160/  265]
train() client id: f_00001-7-5 loss: 0.294579  [  192/  265]
train() client id: f_00001-7-6 loss: 0.420230  [  224/  265]
train() client id: f_00001-7-7 loss: 0.358432  [  256/  265]
train() client id: f_00001-8-0 loss: 0.319895  [   32/  265]
train() client id: f_00001-8-1 loss: 0.440848  [   64/  265]
train() client id: f_00001-8-2 loss: 0.270400  [   96/  265]
train() client id: f_00001-8-3 loss: 0.615230  [  128/  265]
train() client id: f_00001-8-4 loss: 0.315823  [  160/  265]
train() client id: f_00001-8-5 loss: 0.351602  [  192/  265]
train() client id: f_00001-8-6 loss: 0.381977  [  224/  265]
train() client id: f_00001-8-7 loss: 0.324839  [  256/  265]
train() client id: f_00001-9-0 loss: 0.426514  [   32/  265]
train() client id: f_00001-9-1 loss: 0.312592  [   64/  265]
train() client id: f_00001-9-2 loss: 0.286112  [   96/  265]
train() client id: f_00001-9-3 loss: 0.391610  [  128/  265]
train() client id: f_00001-9-4 loss: 0.371721  [  160/  265]
train() client id: f_00001-9-5 loss: 0.385531  [  192/  265]
train() client id: f_00001-9-6 loss: 0.449284  [  224/  265]
train() client id: f_00001-9-7 loss: 0.381249  [  256/  265]
train() client id: f_00002-0-0 loss: 1.118818  [   32/  124]
train() client id: f_00002-0-1 loss: 0.854302  [   64/  124]
train() client id: f_00002-0-2 loss: 0.875574  [   96/  124]
train() client id: f_00002-1-0 loss: 1.004320  [   32/  124]
train() client id: f_00002-1-1 loss: 0.881426  [   64/  124]
train() client id: f_00002-1-2 loss: 0.888781  [   96/  124]
train() client id: f_00002-2-0 loss: 0.759200  [   32/  124]
train() client id: f_00002-2-1 loss: 1.014384  [   64/  124]
train() client id: f_00002-2-2 loss: 0.749144  [   96/  124]
train() client id: f_00002-3-0 loss: 0.886016  [   32/  124]
train() client id: f_00002-3-1 loss: 0.861963  [   64/  124]
train() client id: f_00002-3-2 loss: 1.013140  [   96/  124]
train() client id: f_00002-4-0 loss: 0.690349  [   32/  124]
train() client id: f_00002-4-1 loss: 0.986510  [   64/  124]
train() client id: f_00002-4-2 loss: 0.890034  [   96/  124]
train() client id: f_00002-5-0 loss: 0.818876  [   32/  124]
train() client id: f_00002-5-1 loss: 0.865473  [   64/  124]
train() client id: f_00002-5-2 loss: 0.920959  [   96/  124]
train() client id: f_00002-6-0 loss: 0.729190  [   32/  124]
train() client id: f_00002-6-1 loss: 0.831003  [   64/  124]
train() client id: f_00002-6-2 loss: 1.027766  [   96/  124]
train() client id: f_00002-7-0 loss: 0.726816  [   32/  124]
train() client id: f_00002-7-1 loss: 0.766021  [   64/  124]
train() client id: f_00002-7-2 loss: 0.877452  [   96/  124]
train() client id: f_00002-8-0 loss: 1.018307  [   32/  124]
train() client id: f_00002-8-1 loss: 0.772493  [   64/  124]
train() client id: f_00002-8-2 loss: 0.915828  [   96/  124]
train() client id: f_00002-9-0 loss: 0.919401  [   32/  124]
train() client id: f_00002-9-1 loss: 0.647965  [   64/  124]
train() client id: f_00002-9-2 loss: 0.922169  [   96/  124]
train() client id: f_00003-0-0 loss: 0.783376  [   32/   43]
train() client id: f_00003-1-0 loss: 0.678406  [   32/   43]
train() client id: f_00003-2-0 loss: 0.533842  [   32/   43]
train() client id: f_00003-3-0 loss: 0.722040  [   32/   43]
train() client id: f_00003-4-0 loss: 0.535390  [   32/   43]
train() client id: f_00003-5-0 loss: 0.790886  [   32/   43]
train() client id: f_00003-6-0 loss: 0.768073  [   32/   43]
train() client id: f_00003-7-0 loss: 0.501246  [   32/   43]
train() client id: f_00003-8-0 loss: 0.538348  [   32/   43]
train() client id: f_00003-9-0 loss: 0.647143  [   32/   43]
train() client id: f_00004-0-0 loss: 0.696715  [   32/  306]
train() client id: f_00004-0-1 loss: 0.809273  [   64/  306]
train() client id: f_00004-0-2 loss: 0.947545  [   96/  306]
train() client id: f_00004-0-3 loss: 0.770449  [  128/  306]
train() client id: f_00004-0-4 loss: 0.732148  [  160/  306]
train() client id: f_00004-0-5 loss: 0.976228  [  192/  306]
train() client id: f_00004-0-6 loss: 0.837346  [  224/  306]
train() client id: f_00004-0-7 loss: 0.940049  [  256/  306]
train() client id: f_00004-0-8 loss: 0.882548  [  288/  306]
train() client id: f_00004-1-0 loss: 0.848631  [   32/  306]
train() client id: f_00004-1-1 loss: 0.760268  [   64/  306]
train() client id: f_00004-1-2 loss: 0.827264  [   96/  306]
train() client id: f_00004-1-3 loss: 1.034113  [  128/  306]
train() client id: f_00004-1-4 loss: 0.908371  [  160/  306]
train() client id: f_00004-1-5 loss: 0.810807  [  192/  306]
train() client id: f_00004-1-6 loss: 0.745134  [  224/  306]
train() client id: f_00004-1-7 loss: 0.813219  [  256/  306]
train() client id: f_00004-1-8 loss: 0.804754  [  288/  306]
train() client id: f_00004-2-0 loss: 0.841040  [   32/  306]
train() client id: f_00004-2-1 loss: 0.811391  [   64/  306]
train() client id: f_00004-2-2 loss: 0.820160  [   96/  306]
train() client id: f_00004-2-3 loss: 0.845619  [  128/  306]
train() client id: f_00004-2-4 loss: 0.710799  [  160/  306]
train() client id: f_00004-2-5 loss: 0.910012  [  192/  306]
train() client id: f_00004-2-6 loss: 0.744756  [  224/  306]
train() client id: f_00004-2-7 loss: 0.996467  [  256/  306]
train() client id: f_00004-2-8 loss: 0.850536  [  288/  306]
train() client id: f_00004-3-0 loss: 0.861057  [   32/  306]
train() client id: f_00004-3-1 loss: 0.753614  [   64/  306]
train() client id: f_00004-3-2 loss: 0.924066  [   96/  306]
train() client id: f_00004-3-3 loss: 0.787584  [  128/  306]
train() client id: f_00004-3-4 loss: 0.888041  [  160/  306]
train() client id: f_00004-3-5 loss: 0.826415  [  192/  306]
train() client id: f_00004-3-6 loss: 0.875608  [  224/  306]
train() client id: f_00004-3-7 loss: 0.861871  [  256/  306]
train() client id: f_00004-3-8 loss: 0.705786  [  288/  306]
train() client id: f_00004-4-0 loss: 0.756315  [   32/  306]
train() client id: f_00004-4-1 loss: 0.795102  [   64/  306]
train() client id: f_00004-4-2 loss: 0.750678  [   96/  306]
train() client id: f_00004-4-3 loss: 0.778938  [  128/  306]
train() client id: f_00004-4-4 loss: 0.835210  [  160/  306]
train() client id: f_00004-4-5 loss: 0.878655  [  192/  306]
train() client id: f_00004-4-6 loss: 0.820864  [  224/  306]
train() client id: f_00004-4-7 loss: 0.929393  [  256/  306]
train() client id: f_00004-4-8 loss: 0.915458  [  288/  306]
train() client id: f_00004-5-0 loss: 0.903764  [   32/  306]
train() client id: f_00004-5-1 loss: 0.780067  [   64/  306]
train() client id: f_00004-5-2 loss: 0.769284  [   96/  306]
train() client id: f_00004-5-3 loss: 0.841921  [  128/  306]
train() client id: f_00004-5-4 loss: 0.895584  [  160/  306]
train() client id: f_00004-5-5 loss: 0.836800  [  192/  306]
train() client id: f_00004-5-6 loss: 0.788350  [  224/  306]
train() client id: f_00004-5-7 loss: 0.840063  [  256/  306]
train() client id: f_00004-5-8 loss: 0.767098  [  288/  306]
train() client id: f_00004-6-0 loss: 0.951649  [   32/  306]
train() client id: f_00004-6-1 loss: 0.850621  [   64/  306]
train() client id: f_00004-6-2 loss: 0.759181  [   96/  306]
train() client id: f_00004-6-3 loss: 0.779090  [  128/  306]
train() client id: f_00004-6-4 loss: 0.886086  [  160/  306]
train() client id: f_00004-6-5 loss: 0.763500  [  192/  306]
train() client id: f_00004-6-6 loss: 0.835706  [  224/  306]
train() client id: f_00004-6-7 loss: 0.885313  [  256/  306]
train() client id: f_00004-6-8 loss: 0.721532  [  288/  306]
train() client id: f_00004-7-0 loss: 0.777875  [   32/  306]
train() client id: f_00004-7-1 loss: 0.762071  [   64/  306]
train() client id: f_00004-7-2 loss: 0.835484  [   96/  306]
train() client id: f_00004-7-3 loss: 0.915646  [  128/  306]
train() client id: f_00004-7-4 loss: 0.950607  [  160/  306]
train() client id: f_00004-7-5 loss: 0.807158  [  192/  306]
train() client id: f_00004-7-6 loss: 0.814667  [  224/  306]
train() client id: f_00004-7-7 loss: 0.670747  [  256/  306]
train() client id: f_00004-7-8 loss: 0.854704  [  288/  306]
train() client id: f_00004-8-0 loss: 0.801716  [   32/  306]
train() client id: f_00004-8-1 loss: 0.882913  [   64/  306]
train() client id: f_00004-8-2 loss: 0.846238  [   96/  306]
train() client id: f_00004-8-3 loss: 0.837888  [  128/  306]
train() client id: f_00004-8-4 loss: 0.837256  [  160/  306]
train() client id: f_00004-8-5 loss: 0.825324  [  192/  306]
train() client id: f_00004-8-6 loss: 0.799406  [  224/  306]
train() client id: f_00004-8-7 loss: 0.833293  [  256/  306]
train() client id: f_00004-8-8 loss: 0.706883  [  288/  306]
train() client id: f_00004-9-0 loss: 0.864560  [   32/  306]
train() client id: f_00004-9-1 loss: 0.724643  [   64/  306]
train() client id: f_00004-9-2 loss: 0.919004  [   96/  306]
train() client id: f_00004-9-3 loss: 0.995816  [  128/  306]
train() client id: f_00004-9-4 loss: 0.715045  [  160/  306]
train() client id: f_00004-9-5 loss: 0.800057  [  192/  306]
train() client id: f_00004-9-6 loss: 0.788707  [  224/  306]
train() client id: f_00004-9-7 loss: 0.824782  [  256/  306]
train() client id: f_00004-9-8 loss: 0.729305  [  288/  306]
train() client id: f_00005-0-0 loss: 0.773650  [   32/  146]
train() client id: f_00005-0-1 loss: 0.521765  [   64/  146]
train() client id: f_00005-0-2 loss: 0.499624  [   96/  146]
train() client id: f_00005-0-3 loss: 0.532792  [  128/  146]
train() client id: f_00005-1-0 loss: 0.389134  [   32/  146]
train() client id: f_00005-1-1 loss: 0.580460  [   64/  146]
train() client id: f_00005-1-2 loss: 0.795066  [   96/  146]
train() client id: f_00005-1-3 loss: 0.626351  [  128/  146]
train() client id: f_00005-2-0 loss: 0.451505  [   32/  146]
train() client id: f_00005-2-1 loss: 0.577252  [   64/  146]
train() client id: f_00005-2-2 loss: 0.671372  [   96/  146]
train() client id: f_00005-2-3 loss: 0.424582  [  128/  146]
train() client id: f_00005-3-0 loss: 0.574749  [   32/  146]
train() client id: f_00005-3-1 loss: 0.259295  [   64/  146]
train() client id: f_00005-3-2 loss: 0.824982  [   96/  146]
train() client id: f_00005-3-3 loss: 0.612378  [  128/  146]
train() client id: f_00005-4-0 loss: 0.694763  [   32/  146]
train() client id: f_00005-4-1 loss: 0.657018  [   64/  146]
train() client id: f_00005-4-2 loss: 0.373999  [   96/  146]
train() client id: f_00005-4-3 loss: 0.636801  [  128/  146]
train() client id: f_00005-5-0 loss: 0.593787  [   32/  146]
train() client id: f_00005-5-1 loss: 0.506024  [   64/  146]
train() client id: f_00005-5-2 loss: 0.521706  [   96/  146]
train() client id: f_00005-5-3 loss: 0.664951  [  128/  146]
train() client id: f_00005-6-0 loss: 0.520174  [   32/  146]
train() client id: f_00005-6-1 loss: 0.513642  [   64/  146]
train() client id: f_00005-6-2 loss: 0.618291  [   96/  146]
train() client id: f_00005-6-3 loss: 0.471745  [  128/  146]
train() client id: f_00005-7-0 loss: 0.576592  [   32/  146]
train() client id: f_00005-7-1 loss: 0.401334  [   64/  146]
train() client id: f_00005-7-2 loss: 0.768417  [   96/  146]
train() client id: f_00005-7-3 loss: 0.545690  [  128/  146]
train() client id: f_00005-8-0 loss: 0.457182  [   32/  146]
train() client id: f_00005-8-1 loss: 0.672115  [   64/  146]
train() client id: f_00005-8-2 loss: 0.572928  [   96/  146]
train() client id: f_00005-8-3 loss: 0.618793  [  128/  146]
train() client id: f_00005-9-0 loss: 0.756134  [   32/  146]
train() client id: f_00005-9-1 loss: 0.490569  [   64/  146]
train() client id: f_00005-9-2 loss: 0.492124  [   96/  146]
train() client id: f_00005-9-3 loss: 0.450347  [  128/  146]
train() client id: f_00006-0-0 loss: 0.394431  [   32/   54]
train() client id: f_00006-1-0 loss: 0.434009  [   32/   54]
train() client id: f_00006-2-0 loss: 0.487214  [   32/   54]
train() client id: f_00006-3-0 loss: 0.412391  [   32/   54]
train() client id: f_00006-4-0 loss: 0.386177  [   32/   54]
train() client id: f_00006-5-0 loss: 0.385326  [   32/   54]
train() client id: f_00006-6-0 loss: 0.466179  [   32/   54]
train() client id: f_00006-7-0 loss: 0.499883  [   32/   54]
train() client id: f_00006-8-0 loss: 0.481671  [   32/   54]
train() client id: f_00006-9-0 loss: 0.453267  [   32/   54]
train() client id: f_00007-0-0 loss: 0.706896  [   32/  179]
train() client id: f_00007-0-1 loss: 0.484185  [   64/  179]
train() client id: f_00007-0-2 loss: 0.626139  [   96/  179]
train() client id: f_00007-0-3 loss: 0.585445  [  128/  179]
train() client id: f_00007-0-4 loss: 0.512207  [  160/  179]
train() client id: f_00007-1-0 loss: 0.667543  [   32/  179]
train() client id: f_00007-1-1 loss: 0.575813  [   64/  179]
train() client id: f_00007-1-2 loss: 0.600419  [   96/  179]
train() client id: f_00007-1-3 loss: 0.508594  [  128/  179]
train() client id: f_00007-1-4 loss: 0.543893  [  160/  179]
train() client id: f_00007-2-0 loss: 0.536428  [   32/  179]
train() client id: f_00007-2-1 loss: 0.588049  [   64/  179]
train() client id: f_00007-2-2 loss: 0.486505  [   96/  179]
train() client id: f_00007-2-3 loss: 0.616392  [  128/  179]
train() client id: f_00007-2-4 loss: 0.529949  [  160/  179]
train() client id: f_00007-3-0 loss: 0.463006  [   32/  179]
train() client id: f_00007-3-1 loss: 0.419307  [   64/  179]
train() client id: f_00007-3-2 loss: 0.468989  [   96/  179]
train() client id: f_00007-3-3 loss: 0.796533  [  128/  179]
train() client id: f_00007-3-4 loss: 0.496764  [  160/  179]
train() client id: f_00007-4-0 loss: 0.514663  [   32/  179]
train() client id: f_00007-4-1 loss: 0.467457  [   64/  179]
train() client id: f_00007-4-2 loss: 0.623231  [   96/  179]
train() client id: f_00007-4-3 loss: 0.524610  [  128/  179]
train() client id: f_00007-4-4 loss: 0.652310  [  160/  179]
train() client id: f_00007-5-0 loss: 0.583709  [   32/  179]
train() client id: f_00007-5-1 loss: 0.402763  [   64/  179]
train() client id: f_00007-5-2 loss: 0.567843  [   96/  179]
train() client id: f_00007-5-3 loss: 0.491252  [  128/  179]
train() client id: f_00007-5-4 loss: 0.379564  [  160/  179]
train() client id: f_00007-6-0 loss: 0.590012  [   32/  179]
train() client id: f_00007-6-1 loss: 0.481256  [   64/  179]
train() client id: f_00007-6-2 loss: 0.369196  [   96/  179]
train() client id: f_00007-6-3 loss: 0.817884  [  128/  179]
train() client id: f_00007-6-4 loss: 0.382830  [  160/  179]
train() client id: f_00007-7-0 loss: 0.523675  [   32/  179]
train() client id: f_00007-7-1 loss: 0.348415  [   64/  179]
train() client id: f_00007-7-2 loss: 0.666943  [   96/  179]
train() client id: f_00007-7-3 loss: 0.375683  [  128/  179]
train() client id: f_00007-7-4 loss: 0.457965  [  160/  179]
train() client id: f_00007-8-0 loss: 0.538138  [   32/  179]
train() client id: f_00007-8-1 loss: 0.463360  [   64/  179]
train() client id: f_00007-8-2 loss: 0.532820  [   96/  179]
train() client id: f_00007-8-3 loss: 0.585731  [  128/  179]
train() client id: f_00007-8-4 loss: 0.365450  [  160/  179]
train() client id: f_00007-9-0 loss: 0.473477  [   32/  179]
train() client id: f_00007-9-1 loss: 0.444224  [   64/  179]
train() client id: f_00007-9-2 loss: 0.703359  [   96/  179]
train() client id: f_00007-9-3 loss: 0.574808  [  128/  179]
train() client id: f_00007-9-4 loss: 0.453985  [  160/  179]
train() client id: f_00008-0-0 loss: 0.806186  [   32/  130]
train() client id: f_00008-0-1 loss: 0.694450  [   64/  130]
train() client id: f_00008-0-2 loss: 0.747679  [   96/  130]
train() client id: f_00008-0-3 loss: 0.774661  [  128/  130]
train() client id: f_00008-1-0 loss: 0.758527  [   32/  130]
train() client id: f_00008-1-1 loss: 0.842703  [   64/  130]
train() client id: f_00008-1-2 loss: 0.605968  [   96/  130]
train() client id: f_00008-1-3 loss: 0.811795  [  128/  130]
train() client id: f_00008-2-0 loss: 0.731016  [   32/  130]
train() client id: f_00008-2-1 loss: 0.801332  [   64/  130]
train() client id: f_00008-2-2 loss: 0.665902  [   96/  130]
train() client id: f_00008-2-3 loss: 0.834362  [  128/  130]
train() client id: f_00008-3-0 loss: 0.757979  [   32/  130]
train() client id: f_00008-3-1 loss: 0.789112  [   64/  130]
train() client id: f_00008-3-2 loss: 0.775062  [   96/  130]
train() client id: f_00008-3-3 loss: 0.688525  [  128/  130]
train() client id: f_00008-4-0 loss: 0.665251  [   32/  130]
train() client id: f_00008-4-1 loss: 0.690096  [   64/  130]
train() client id: f_00008-4-2 loss: 0.851394  [   96/  130]
train() client id: f_00008-4-3 loss: 0.778345  [  128/  130]
train() client id: f_00008-5-0 loss: 0.862141  [   32/  130]
train() client id: f_00008-5-1 loss: 0.743597  [   64/  130]
train() client id: f_00008-5-2 loss: 0.720473  [   96/  130]
train() client id: f_00008-5-3 loss: 0.702516  [  128/  130]
train() client id: f_00008-6-0 loss: 0.900753  [   32/  130]
train() client id: f_00008-6-1 loss: 0.650646  [   64/  130]
train() client id: f_00008-6-2 loss: 0.686970  [   96/  130]
train() client id: f_00008-6-3 loss: 0.794734  [  128/  130]
train() client id: f_00008-7-0 loss: 0.589532  [   32/  130]
train() client id: f_00008-7-1 loss: 0.872784  [   64/  130]
train() client id: f_00008-7-2 loss: 0.826878  [   96/  130]
train() client id: f_00008-7-3 loss: 0.712970  [  128/  130]
train() client id: f_00008-8-0 loss: 0.734959  [   32/  130]
train() client id: f_00008-8-1 loss: 0.719683  [   64/  130]
train() client id: f_00008-8-2 loss: 0.764641  [   96/  130]
train() client id: f_00008-8-3 loss: 0.789467  [  128/  130]
train() client id: f_00008-9-0 loss: 0.744708  [   32/  130]
train() client id: f_00008-9-1 loss: 0.835721  [   64/  130]
train() client id: f_00008-9-2 loss: 0.698806  [   96/  130]
train() client id: f_00008-9-3 loss: 0.753860  [  128/  130]
train() client id: f_00009-0-0 loss: 1.033406  [   32/  118]
train() client id: f_00009-0-1 loss: 1.029696  [   64/  118]
train() client id: f_00009-0-2 loss: 0.991498  [   96/  118]
train() client id: f_00009-1-0 loss: 0.867149  [   32/  118]
train() client id: f_00009-1-1 loss: 1.190564  [   64/  118]
train() client id: f_00009-1-2 loss: 1.017656  [   96/  118]
train() client id: f_00009-2-0 loss: 0.955724  [   32/  118]
train() client id: f_00009-2-1 loss: 1.002691  [   64/  118]
train() client id: f_00009-2-2 loss: 0.966615  [   96/  118]
train() client id: f_00009-3-0 loss: 0.907965  [   32/  118]
train() client id: f_00009-3-1 loss: 0.952245  [   64/  118]
train() client id: f_00009-3-2 loss: 1.037821  [   96/  118]
train() client id: f_00009-4-0 loss: 1.130601  [   32/  118]
train() client id: f_00009-4-1 loss: 0.963268  [   64/  118]
train() client id: f_00009-4-2 loss: 0.761130  [   96/  118]
train() client id: f_00009-5-0 loss: 0.965246  [   32/  118]
train() client id: f_00009-5-1 loss: 0.947097  [   64/  118]
train() client id: f_00009-5-2 loss: 0.821356  [   96/  118]
train() client id: f_00009-6-0 loss: 0.800020  [   32/  118]
train() client id: f_00009-6-1 loss: 0.860837  [   64/  118]
train() client id: f_00009-6-2 loss: 1.069233  [   96/  118]
train() client id: f_00009-7-0 loss: 0.912999  [   32/  118]
train() client id: f_00009-7-1 loss: 0.890571  [   64/  118]
train() client id: f_00009-7-2 loss: 0.838142  [   96/  118]
train() client id: f_00009-8-0 loss: 1.096702  [   32/  118]
train() client id: f_00009-8-1 loss: 0.913282  [   64/  118]
train() client id: f_00009-8-2 loss: 0.758441  [   96/  118]
train() client id: f_00009-9-0 loss: 0.819311  [   32/  118]
train() client id: f_00009-9-1 loss: 0.886896  [   64/  118]
train() client id: f_00009-9-2 loss: 1.026423  [   96/  118]
At round 60 accuracy: 0.6472148541114059
At round 60 training accuracy: 0.5855130784708249
At round 60 training loss: 0.8331878809254595
update_location
xs = [  -3.9056584     4.20031788  320.00902392   18.81129433    0.97929623
    3.95640986 -282.44319194 -261.32485185  304.66397685 -247.06087855]
ys = [ 312.5879595   295.55583871    1.32061395 -282.45517586  274.35018685
  257.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [328.2171333  312.04310033 335.27230636 300.22456789 292.0085342
 276.55702758 299.63485647 279.80592283 321.13674465 266.56160558]
dists_bs = [219.55345664 215.9779252  524.5936169  496.81793016 202.08704146
 197.18919898 207.59256862 194.39607637 504.83863615 185.5620205 ]
uav_gains = [1.63906976e-12 2.14056491e-12 1.47563907e-12 2.66124979e-12
 3.12682385e-12 4.29784027e-12 2.69155883e-12 4.01567151e-12
 1.83392238e-12 5.29895911e-12]
bs_gains = [3.06888987e-11 3.21327477e-11 2.67787117e-12 3.11847132e-12
 3.87067209e-11 4.14592405e-11 3.59005447e-11 4.31488375e-11
 2.98171993e-12 4.91501249e-11]
Round 61
-------------------------------
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.95458156 5.94128521 2.91869021 1.0805571  6.84914591 3.29632295
 1.32415061 4.08054691 3.00608516 2.67279298]
obj_prev = 34.12415860899734
eta_min = 3.98916673942854e-32	eta_max = 0.9387143479457988
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 7.8273257066737205	eta = 0.9090909090909091
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 18.61065565296208	eta = 0.3823481974584871
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 12.717399503431304	eta = 0.5595287496088128
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.681826444107466	eta = 0.60912997436458
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617979436161402	eta = 0.6124774692131586
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704178505617	eta = 0.6124919806097139
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704173351733	eta = 0.6124919808814296
eta = 0.6124919808814296
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.04045173 0.08507706 0.03980964 0.01380496 0.09823992 0.04687264
 0.01733646 0.05746714 0.04173591 0.03788337]
ene_total = [1.16278399 1.74880761 1.17244705 0.58140006 1.9903631  1.02228243
 0.6462616  1.35340359 1.09164915 0.84830559]
ti_comp = [1.21927116 1.35390191 1.20771185 1.26195871 1.35708161 1.35819303
 1.2627831  1.28806187 1.26993947 1.36081379]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [2.78285633e-06 2.09963099e-05 2.70344885e-06 1.03251135e-07
 3.21760040e-05 3.48911535e-06 2.04222466e-07 7.14933098e-06
 2.81736924e-06 1.83496748e-06]
ene_total = [0.40211875 0.14928058 0.4238552  0.32179204 0.14351122 0.14088165
 0.32024363 0.27283609 0.3068348  0.13592205]
optimize_network iter = 0 obj = 2.617276020137961
eta = 0.6124919808814296
freqs = [16588488.88317586 31419209.64704492 16481431.96089219  5469654.50966602
 36195288.80600585 17255515.63488949  6864384.21294392 22307602.23984509
 16432245.4685148  13919379.90342778]
eta_min = 0.6124919808814304	eta_max = 0.7228965206792303
af = 0.0011038579011319113	bf = 1.0136743134601105	zeta = 0.0012142436912451024	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.41052682e-07 4.08217617e-06 5.25614002e-07 2.00744476e-08
 6.25577148e-06 6.78366036e-07 3.97056477e-08 1.38999799e-06
 5.47762803e-07 3.56760810e-07]
ene_total = [1.71388696 0.63496578 1.80654562 1.37166028 0.60965148 0.60029516
 1.36505352 1.16252622 1.30772847 0.57926126]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 1 obj = 3.659284534384905
eta = 0.7228965206792303
freqs = [16508552.89543737 29777055.50547944 16481431.96089219  5352148.75441332
 34268839.33358462 16331361.10328626  6714818.53144301 21618818.75231183
 16031043.09450184 13163002.59895364]
eta_min = 0.7228965206792295	eta_max = 0.7228965206792275
af = 0.0010051578008740583	bf = 1.0136743134601105	zeta = 0.0011056735809614642	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.35850837e-07 3.66661011e-06 5.25614002e-07 1.92211852e-08
 5.60758124e-06 6.07649264e-07 3.79942326e-08 1.30548627e-06
 5.21341470e-07 3.19041588e-07]
ene_total = [1.71388654 0.63493247 1.80654562 1.37166021 0.60959952 0.60028949
 1.36505338 1.16251944 1.30772635 0.57925823]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 2 obj = 3.6592845343848683
eta = 0.7228965206792275
freqs = [16508552.89543735 29777055.50547947 16481431.96089218  5352148.75441332
 34268839.33358465 16331361.10328627  6714818.531443   21618818.75231183
 16031043.09450183 13163002.59895365]
Done!
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.71695361e-06 1.17484177e-05 1.68415312e-06 6.15878170e-08
 1.79676062e-05 1.94700750e-06 1.21739727e-07 4.18299125e-06
 1.67046323e-06 1.02226136e-06]
ene_total = [0.0213819  0.00792886 0.0225378  0.01711149 0.00761711 0.00748995
 0.01702911 0.0145053  0.01631503 0.00722695]
At round 61 energy consumption: 0.13914349848735766
At round 61 eta: 0.7228965206792275
At round 61 a_n: 7.287306191126021
At round 61 local rounds: 10.625424748609518
At round 61 global rounds: 26.298140351714242
gradient difference: 0.5265921950340271
train() client id: f_00000-0-0 loss: 1.320044  [   32/  126]
train() client id: f_00000-0-1 loss: 0.822128  [   64/  126]
train() client id: f_00000-0-2 loss: 0.840157  [   96/  126]
train() client id: f_00000-1-0 loss: 0.915405  [   32/  126]
train() client id: f_00000-1-1 loss: 0.906893  [   64/  126]
train() client id: f_00000-1-2 loss: 0.902472  [   96/  126]
train() client id: f_00000-2-0 loss: 0.988126  [   32/  126]
train() client id: f_00000-2-1 loss: 0.797746  [   64/  126]
train() client id: f_00000-2-2 loss: 0.755736  [   96/  126]
train() client id: f_00000-3-0 loss: 0.705383  [   32/  126]
train() client id: f_00000-3-1 loss: 0.671777  [   64/  126]
train() client id: f_00000-3-2 loss: 0.876547  [   96/  126]
train() client id: f_00000-4-0 loss: 0.779951  [   32/  126]
train() client id: f_00000-4-1 loss: 0.814946  [   64/  126]
train() client id: f_00000-4-2 loss: 0.669638  [   96/  126]
train() client id: f_00000-5-0 loss: 0.858160  [   32/  126]
train() client id: f_00000-5-1 loss: 0.723603  [   64/  126]
train() client id: f_00000-5-2 loss: 0.582358  [   96/  126]
train() client id: f_00000-6-0 loss: 0.703812  [   32/  126]
train() client id: f_00000-6-1 loss: 0.704483  [   64/  126]
train() client id: f_00000-6-2 loss: 0.749145  [   96/  126]
train() client id: f_00000-7-0 loss: 0.669317  [   32/  126]
train() client id: f_00000-7-1 loss: 0.744344  [   64/  126]
train() client id: f_00000-7-2 loss: 0.636801  [   96/  126]
train() client id: f_00000-8-0 loss: 0.724223  [   32/  126]
train() client id: f_00000-8-1 loss: 0.645289  [   64/  126]
train() client id: f_00000-8-2 loss: 0.720923  [   96/  126]
train() client id: f_00000-9-0 loss: 0.757733  [   32/  126]
train() client id: f_00000-9-1 loss: 0.709260  [   64/  126]
train() client id: f_00000-9-2 loss: 0.596060  [   96/  126]
train() client id: f_00001-0-0 loss: 0.532234  [   32/  265]
train() client id: f_00001-0-1 loss: 0.454593  [   64/  265]
train() client id: f_00001-0-2 loss: 0.412834  [   96/  265]
train() client id: f_00001-0-3 loss: 0.428515  [  128/  265]
train() client id: f_00001-0-4 loss: 0.356181  [  160/  265]
train() client id: f_00001-0-5 loss: 0.390335  [  192/  265]
train() client id: f_00001-0-6 loss: 0.504267  [  224/  265]
train() client id: f_00001-0-7 loss: 0.576901  [  256/  265]
train() client id: f_00001-1-0 loss: 0.436359  [   32/  265]
train() client id: f_00001-1-1 loss: 0.427857  [   64/  265]
train() client id: f_00001-1-2 loss: 0.557346  [   96/  265]
train() client id: f_00001-1-3 loss: 0.502687  [  128/  265]
train() client id: f_00001-1-4 loss: 0.333482  [  160/  265]
train() client id: f_00001-1-5 loss: 0.408338  [  192/  265]
train() client id: f_00001-1-6 loss: 0.418659  [  224/  265]
train() client id: f_00001-1-7 loss: 0.497302  [  256/  265]
train() client id: f_00001-2-0 loss: 0.371450  [   32/  265]
train() client id: f_00001-2-1 loss: 0.390477  [   64/  265]
train() client id: f_00001-2-2 loss: 0.460092  [   96/  265]
train() client id: f_00001-2-3 loss: 0.407383  [  128/  265]
train() client id: f_00001-2-4 loss: 0.430673  [  160/  265]
train() client id: f_00001-2-5 loss: 0.510012  [  192/  265]
train() client id: f_00001-2-6 loss: 0.348561  [  224/  265]
train() client id: f_00001-2-7 loss: 0.551438  [  256/  265]
train() client id: f_00001-3-0 loss: 0.420623  [   32/  265]
train() client id: f_00001-3-1 loss: 0.470355  [   64/  265]
train() client id: f_00001-3-2 loss: 0.552235  [   96/  265]
train() client id: f_00001-3-3 loss: 0.330983  [  128/  265]
train() client id: f_00001-3-4 loss: 0.455807  [  160/  265]
train() client id: f_00001-3-5 loss: 0.485934  [  192/  265]
train() client id: f_00001-3-6 loss: 0.393834  [  224/  265]
train() client id: f_00001-3-7 loss: 0.336117  [  256/  265]
train() client id: f_00001-4-0 loss: 0.387826  [   32/  265]
train() client id: f_00001-4-1 loss: 0.428762  [   64/  265]
train() client id: f_00001-4-2 loss: 0.527979  [   96/  265]
train() client id: f_00001-4-3 loss: 0.334850  [  128/  265]
train() client id: f_00001-4-4 loss: 0.340067  [  160/  265]
train() client id: f_00001-4-5 loss: 0.557551  [  192/  265]
train() client id: f_00001-4-6 loss: 0.552696  [  224/  265]
train() client id: f_00001-4-7 loss: 0.316699  [  256/  265]
train() client id: f_00001-5-0 loss: 0.496619  [   32/  265]
train() client id: f_00001-5-1 loss: 0.380286  [   64/  265]
train() client id: f_00001-5-2 loss: 0.393828  [   96/  265]
train() client id: f_00001-5-3 loss: 0.496807  [  128/  265]
train() client id: f_00001-5-4 loss: 0.371961  [  160/  265]
train() client id: f_00001-5-5 loss: 0.366832  [  192/  265]
train() client id: f_00001-5-6 loss: 0.480703  [  224/  265]
train() client id: f_00001-5-7 loss: 0.445757  [  256/  265]
train() client id: f_00001-6-0 loss: 0.325716  [   32/  265]
train() client id: f_00001-6-1 loss: 0.509234  [   64/  265]
train() client id: f_00001-6-2 loss: 0.493080  [   96/  265]
train() client id: f_00001-6-3 loss: 0.472499  [  128/  265]
train() client id: f_00001-6-4 loss: 0.426289  [  160/  265]
train() client id: f_00001-6-5 loss: 0.440818  [  192/  265]
train() client id: f_00001-6-6 loss: 0.323258  [  224/  265]
train() client id: f_00001-6-7 loss: 0.426244  [  256/  265]
train() client id: f_00001-7-0 loss: 0.483379  [   32/  265]
train() client id: f_00001-7-1 loss: 0.491417  [   64/  265]
train() client id: f_00001-7-2 loss: 0.421956  [   96/  265]
train() client id: f_00001-7-3 loss: 0.382300  [  128/  265]
train() client id: f_00001-7-4 loss: 0.330313  [  160/  265]
train() client id: f_00001-7-5 loss: 0.467478  [  192/  265]
train() client id: f_00001-7-6 loss: 0.335821  [  224/  265]
train() client id: f_00001-7-7 loss: 0.426935  [  256/  265]
train() client id: f_00001-8-0 loss: 0.363843  [   32/  265]
train() client id: f_00001-8-1 loss: 0.492380  [   64/  265]
train() client id: f_00001-8-2 loss: 0.513067  [   96/  265]
train() client id: f_00001-8-3 loss: 0.418939  [  128/  265]
train() client id: f_00001-8-4 loss: 0.378370  [  160/  265]
train() client id: f_00001-8-5 loss: 0.331610  [  192/  265]
train() client id: f_00001-8-6 loss: 0.458377  [  224/  265]
train() client id: f_00001-8-7 loss: 0.368791  [  256/  265]
train() client id: f_00001-9-0 loss: 0.406761  [   32/  265]
train() client id: f_00001-9-1 loss: 0.442065  [   64/  265]
train() client id: f_00001-9-2 loss: 0.352321  [   96/  265]
train() client id: f_00001-9-3 loss: 0.393940  [  128/  265]
train() client id: f_00001-9-4 loss: 0.526060  [  160/  265]
train() client id: f_00001-9-5 loss: 0.392820  [  192/  265]
train() client id: f_00001-9-6 loss: 0.400174  [  224/  265]
train() client id: f_00001-9-7 loss: 0.391964  [  256/  265]
train() client id: f_00002-0-0 loss: 0.880745  [   32/  124]
train() client id: f_00002-0-1 loss: 1.070278  [   64/  124]
train() client id: f_00002-0-2 loss: 1.116268  [   96/  124]
train() client id: f_00002-1-0 loss: 1.107996  [   32/  124]
train() client id: f_00002-1-1 loss: 0.934185  [   64/  124]
train() client id: f_00002-1-2 loss: 1.019859  [   96/  124]
train() client id: f_00002-2-0 loss: 0.721647  [   32/  124]
train() client id: f_00002-2-1 loss: 0.974380  [   64/  124]
train() client id: f_00002-2-2 loss: 0.979206  [   96/  124]
train() client id: f_00002-3-0 loss: 1.117684  [   32/  124]
train() client id: f_00002-3-1 loss: 0.849078  [   64/  124]
train() client id: f_00002-3-2 loss: 0.906499  [   96/  124]
train() client id: f_00002-4-0 loss: 0.862893  [   32/  124]
train() client id: f_00002-4-1 loss: 0.966601  [   64/  124]
train() client id: f_00002-4-2 loss: 1.004728  [   96/  124]
train() client id: f_00002-5-0 loss: 0.758934  [   32/  124]
train() client id: f_00002-5-1 loss: 1.014432  [   64/  124]
train() client id: f_00002-5-2 loss: 1.147231  [   96/  124]
train() client id: f_00002-6-0 loss: 0.846888  [   32/  124]
train() client id: f_00002-6-1 loss: 1.033726  [   64/  124]
train() client id: f_00002-6-2 loss: 0.769063  [   96/  124]
train() client id: f_00002-7-0 loss: 0.804302  [   32/  124]
train() client id: f_00002-7-1 loss: 0.781644  [   64/  124]
train() client id: f_00002-7-2 loss: 0.984554  [   96/  124]
train() client id: f_00002-8-0 loss: 0.739192  [   32/  124]
train() client id: f_00002-8-1 loss: 0.833343  [   64/  124]
train() client id: f_00002-8-2 loss: 0.894149  [   96/  124]
train() client id: f_00002-9-0 loss: 0.888765  [   32/  124]
train() client id: f_00002-9-1 loss: 1.040624  [   64/  124]
train() client id: f_00002-9-2 loss: 0.885457  [   96/  124]
train() client id: f_00003-0-0 loss: 0.707384  [   32/   43]
train() client id: f_00003-1-0 loss: 0.564967  [   32/   43]
train() client id: f_00003-2-0 loss: 0.502397  [   32/   43]
train() client id: f_00003-3-0 loss: 0.409734  [   32/   43]
train() client id: f_00003-4-0 loss: 0.625510  [   32/   43]
train() client id: f_00003-5-0 loss: 0.486461  [   32/   43]
train() client id: f_00003-6-0 loss: 0.684969  [   32/   43]
train() client id: f_00003-7-0 loss: 0.618096  [   32/   43]
train() client id: f_00003-8-0 loss: 0.713593  [   32/   43]
train() client id: f_00003-9-0 loss: 0.511574  [   32/   43]
train() client id: f_00004-0-0 loss: 0.919272  [   32/  306]
train() client id: f_00004-0-1 loss: 0.828617  [   64/  306]
train() client id: f_00004-0-2 loss: 0.852750  [   96/  306]
train() client id: f_00004-0-3 loss: 1.018228  [  128/  306]
train() client id: f_00004-0-4 loss: 0.887879  [  160/  306]
train() client id: f_00004-0-5 loss: 0.747403  [  192/  306]
train() client id: f_00004-0-6 loss: 0.823160  [  224/  306]
train() client id: f_00004-0-7 loss: 1.014334  [  256/  306]
train() client id: f_00004-0-8 loss: 0.874851  [  288/  306]
train() client id: f_00004-1-0 loss: 0.912766  [   32/  306]
train() client id: f_00004-1-1 loss: 0.789134  [   64/  306]
train() client id: f_00004-1-2 loss: 0.894390  [   96/  306]
train() client id: f_00004-1-3 loss: 0.887635  [  128/  306]
train() client id: f_00004-1-4 loss: 0.953044  [  160/  306]
train() client id: f_00004-1-5 loss: 0.847220  [  192/  306]
train() client id: f_00004-1-6 loss: 0.818132  [  224/  306]
train() client id: f_00004-1-7 loss: 0.870530  [  256/  306]
train() client id: f_00004-1-8 loss: 0.938729  [  288/  306]
train() client id: f_00004-2-0 loss: 0.940864  [   32/  306]
train() client id: f_00004-2-1 loss: 0.990109  [   64/  306]
train() client id: f_00004-2-2 loss: 0.856859  [   96/  306]
train() client id: f_00004-2-3 loss: 0.831634  [  128/  306]
train() client id: f_00004-2-4 loss: 0.855300  [  160/  306]
train() client id: f_00004-2-5 loss: 0.930901  [  192/  306]
train() client id: f_00004-2-6 loss: 0.886997  [  224/  306]
train() client id: f_00004-2-7 loss: 0.900249  [  256/  306]
train() client id: f_00004-2-8 loss: 0.862482  [  288/  306]
train() client id: f_00004-3-0 loss: 0.886576  [   32/  306]
train() client id: f_00004-3-1 loss: 0.931767  [   64/  306]
train() client id: f_00004-3-2 loss: 0.770577  [   96/  306]
train() client id: f_00004-3-3 loss: 0.859836  [  128/  306]
train() client id: f_00004-3-4 loss: 1.012992  [  160/  306]
train() client id: f_00004-3-5 loss: 0.951331  [  192/  306]
train() client id: f_00004-3-6 loss: 0.998046  [  224/  306]
train() client id: f_00004-3-7 loss: 0.812713  [  256/  306]
train() client id: f_00004-3-8 loss: 0.818706  [  288/  306]
train() client id: f_00004-4-0 loss: 0.936988  [   32/  306]
train() client id: f_00004-4-1 loss: 0.876368  [   64/  306]
train() client id: f_00004-4-2 loss: 0.811369  [   96/  306]
train() client id: f_00004-4-3 loss: 0.931958  [  128/  306]
train() client id: f_00004-4-4 loss: 0.855897  [  160/  306]
train() client id: f_00004-4-5 loss: 0.926423  [  192/  306]
train() client id: f_00004-4-6 loss: 0.928272  [  224/  306]
train() client id: f_00004-4-7 loss: 0.894662  [  256/  306]
train() client id: f_00004-4-8 loss: 0.778098  [  288/  306]
train() client id: f_00004-5-0 loss: 0.734078  [   32/  306]
train() client id: f_00004-5-1 loss: 0.955106  [   64/  306]
train() client id: f_00004-5-2 loss: 0.914368  [   96/  306]
train() client id: f_00004-5-3 loss: 0.952983  [  128/  306]
train() client id: f_00004-5-4 loss: 0.921253  [  160/  306]
train() client id: f_00004-5-5 loss: 0.922064  [  192/  306]
train() client id: f_00004-5-6 loss: 0.855084  [  224/  306]
train() client id: f_00004-5-7 loss: 0.911284  [  256/  306]
train() client id: f_00004-5-8 loss: 0.917774  [  288/  306]
train() client id: f_00004-6-0 loss: 0.857209  [   32/  306]
train() client id: f_00004-6-1 loss: 1.011897  [   64/  306]
train() client id: f_00004-6-2 loss: 0.781640  [   96/  306]
train() client id: f_00004-6-3 loss: 0.902767  [  128/  306]
train() client id: f_00004-6-4 loss: 0.786441  [  160/  306]
train() client id: f_00004-6-5 loss: 0.832887  [  192/  306]
train() client id: f_00004-6-6 loss: 0.971460  [  224/  306]
train() client id: f_00004-6-7 loss: 0.919067  [  256/  306]
train() client id: f_00004-6-8 loss: 0.972399  [  288/  306]
train() client id: f_00004-7-0 loss: 0.978766  [   32/  306]
train() client id: f_00004-7-1 loss: 0.955408  [   64/  306]
train() client id: f_00004-7-2 loss: 0.883148  [   96/  306]
train() client id: f_00004-7-3 loss: 0.860026  [  128/  306]
train() client id: f_00004-7-4 loss: 0.855365  [  160/  306]
train() client id: f_00004-7-5 loss: 0.832795  [  192/  306]
train() client id: f_00004-7-6 loss: 0.749646  [  224/  306]
train() client id: f_00004-7-7 loss: 0.993182  [  256/  306]
train() client id: f_00004-7-8 loss: 0.918619  [  288/  306]
train() client id: f_00004-8-0 loss: 0.944382  [   32/  306]
train() client id: f_00004-8-1 loss: 0.895379  [   64/  306]
train() client id: f_00004-8-2 loss: 0.896819  [   96/  306]
train() client id: f_00004-8-3 loss: 0.943931  [  128/  306]
train() client id: f_00004-8-4 loss: 0.981139  [  160/  306]
train() client id: f_00004-8-5 loss: 0.767920  [  192/  306]
train() client id: f_00004-8-6 loss: 0.817784  [  224/  306]
train() client id: f_00004-8-7 loss: 0.871478  [  256/  306]
train() client id: f_00004-8-8 loss: 0.932009  [  288/  306]
train() client id: f_00004-9-0 loss: 0.934229  [   32/  306]
train() client id: f_00004-9-1 loss: 0.904764  [   64/  306]
train() client id: f_00004-9-2 loss: 0.985953  [   96/  306]
train() client id: f_00004-9-3 loss: 0.883352  [  128/  306]
train() client id: f_00004-9-4 loss: 0.926619  [  160/  306]
train() client id: f_00004-9-5 loss: 0.794842  [  192/  306]
train() client id: f_00004-9-6 loss: 0.785743  [  224/  306]
train() client id: f_00004-9-7 loss: 0.949338  [  256/  306]
train() client id: f_00004-9-8 loss: 0.841222  [  288/  306]
train() client id: f_00005-0-0 loss: 0.828391  [   32/  146]
train() client id: f_00005-0-1 loss: 0.795574  [   64/  146]
train() client id: f_00005-0-2 loss: 0.756382  [   96/  146]
train() client id: f_00005-0-3 loss: 0.764759  [  128/  146]
train() client id: f_00005-1-0 loss: 0.786759  [   32/  146]
train() client id: f_00005-1-1 loss: 0.784288  [   64/  146]
train() client id: f_00005-1-2 loss: 0.663991  [   96/  146]
train() client id: f_00005-1-3 loss: 0.708117  [  128/  146]
train() client id: f_00005-2-0 loss: 0.721982  [   32/  146]
train() client id: f_00005-2-1 loss: 0.730393  [   64/  146]
train() client id: f_00005-2-2 loss: 0.710207  [   96/  146]
train() client id: f_00005-2-3 loss: 0.788032  [  128/  146]
train() client id: f_00005-3-0 loss: 0.574518  [   32/  146]
train() client id: f_00005-3-1 loss: 0.687613  [   64/  146]
train() client id: f_00005-3-2 loss: 0.619524  [   96/  146]
train() client id: f_00005-3-3 loss: 0.979495  [  128/  146]
train() client id: f_00005-4-0 loss: 0.899441  [   32/  146]
train() client id: f_00005-4-1 loss: 0.506748  [   64/  146]
train() client id: f_00005-4-2 loss: 0.804470  [   96/  146]
train() client id: f_00005-4-3 loss: 0.894589  [  128/  146]
train() client id: f_00005-5-0 loss: 0.556616  [   32/  146]
train() client id: f_00005-5-1 loss: 0.739501  [   64/  146]
train() client id: f_00005-5-2 loss: 0.659180  [   96/  146]
train() client id: f_00005-5-3 loss: 1.028627  [  128/  146]
train() client id: f_00005-6-0 loss: 0.600309  [   32/  146]
train() client id: f_00005-6-1 loss: 0.618190  [   64/  146]
train() client id: f_00005-6-2 loss: 0.711340  [   96/  146]
train() client id: f_00005-6-3 loss: 0.853360  [  128/  146]
train() client id: f_00005-7-0 loss: 0.656031  [   32/  146]
train() client id: f_00005-7-1 loss: 0.731236  [   64/  146]
train() client id: f_00005-7-2 loss: 0.788858  [   96/  146]
train() client id: f_00005-7-3 loss: 0.708746  [  128/  146]
train() client id: f_00005-8-0 loss: 0.784767  [   32/  146]
train() client id: f_00005-8-1 loss: 0.597871  [   64/  146]
train() client id: f_00005-8-2 loss: 0.662996  [   96/  146]
train() client id: f_00005-8-3 loss: 0.830531  [  128/  146]
train() client id: f_00005-9-0 loss: 0.555488  [   32/  146]
train() client id: f_00005-9-1 loss: 0.427504  [   64/  146]
train() client id: f_00005-9-2 loss: 0.869542  [   96/  146]
train() client id: f_00005-9-3 loss: 0.994728  [  128/  146]
train() client id: f_00006-0-0 loss: 0.469877  [   32/   54]
train() client id: f_00006-1-0 loss: 0.540323  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469195  [   32/   54]
train() client id: f_00006-3-0 loss: 0.551429  [   32/   54]
train() client id: f_00006-4-0 loss: 0.515955  [   32/   54]
train() client id: f_00006-5-0 loss: 0.484014  [   32/   54]
train() client id: f_00006-6-0 loss: 0.492281  [   32/   54]
train() client id: f_00006-7-0 loss: 0.507619  [   32/   54]
train() client id: f_00006-8-0 loss: 0.439876  [   32/   54]
train() client id: f_00006-9-0 loss: 0.501954  [   32/   54]
train() client id: f_00007-0-0 loss: 0.678065  [   32/  179]
train() client id: f_00007-0-1 loss: 0.532438  [   64/  179]
train() client id: f_00007-0-2 loss: 0.778658  [   96/  179]
train() client id: f_00007-0-3 loss: 0.641809  [  128/  179]
train() client id: f_00007-0-4 loss: 0.593561  [  160/  179]
train() client id: f_00007-1-0 loss: 0.537014  [   32/  179]
train() client id: f_00007-1-1 loss: 0.669762  [   64/  179]
train() client id: f_00007-1-2 loss: 0.606031  [   96/  179]
train() client id: f_00007-1-3 loss: 0.467769  [  128/  179]
train() client id: f_00007-1-4 loss: 0.740733  [  160/  179]
train() client id: f_00007-2-0 loss: 0.478381  [   32/  179]
train() client id: f_00007-2-1 loss: 0.547849  [   64/  179]
train() client id: f_00007-2-2 loss: 0.756495  [   96/  179]
train() client id: f_00007-2-3 loss: 0.670118  [  128/  179]
train() client id: f_00007-2-4 loss: 0.651262  [  160/  179]
train() client id: f_00007-3-0 loss: 0.660515  [   32/  179]
train() client id: f_00007-3-1 loss: 0.588970  [   64/  179]
train() client id: f_00007-3-2 loss: 0.838627  [   96/  179]
train() client id: f_00007-3-3 loss: 0.441058  [  128/  179]
train() client id: f_00007-3-4 loss: 0.523915  [  160/  179]
train() client id: f_00007-4-0 loss: 0.405110  [   32/  179]
train() client id: f_00007-4-1 loss: 0.779958  [   64/  179]
train() client id: f_00007-4-2 loss: 0.485075  [   96/  179]
train() client id: f_00007-4-3 loss: 0.513453  [  128/  179]
train() client id: f_00007-4-4 loss: 0.505721  [  160/  179]
train() client id: f_00007-5-0 loss: 0.821451  [   32/  179]
train() client id: f_00007-5-1 loss: 0.649197  [   64/  179]
train() client id: f_00007-5-2 loss: 0.453743  [   96/  179]
train() client id: f_00007-5-3 loss: 0.521075  [  128/  179]
train() client id: f_00007-5-4 loss: 0.503719  [  160/  179]
train() client id: f_00007-6-0 loss: 0.837768  [   32/  179]
train() client id: f_00007-6-1 loss: 0.516632  [   64/  179]
train() client id: f_00007-6-2 loss: 0.434390  [   96/  179]
train() client id: f_00007-6-3 loss: 0.594437  [  128/  179]
train() client id: f_00007-6-4 loss: 0.645476  [  160/  179]
train() client id: f_00007-7-0 loss: 0.485483  [   32/  179]
train() client id: f_00007-7-1 loss: 0.659366  [   64/  179]
train() client id: f_00007-7-2 loss: 0.726196  [   96/  179]
train() client id: f_00007-7-3 loss: 0.555043  [  128/  179]
train() client id: f_00007-7-4 loss: 0.607342  [  160/  179]
train() client id: f_00007-8-0 loss: 0.534072  [   32/  179]
train() client id: f_00007-8-1 loss: 0.817358  [   64/  179]
train() client id: f_00007-8-2 loss: 0.696172  [   96/  179]
train() client id: f_00007-8-3 loss: 0.411661  [  128/  179]
train() client id: f_00007-8-4 loss: 0.527892  [  160/  179]
train() client id: f_00007-9-0 loss: 0.433496  [   32/  179]
train() client id: f_00007-9-1 loss: 0.783488  [   64/  179]
train() client id: f_00007-9-2 loss: 0.486272  [   96/  179]
train() client id: f_00007-9-3 loss: 0.647882  [  128/  179]
train() client id: f_00007-9-4 loss: 0.532228  [  160/  179]
train() client id: f_00008-0-0 loss: 0.769576  [   32/  130]
train() client id: f_00008-0-1 loss: 0.734476  [   64/  130]
train() client id: f_00008-0-2 loss: 0.599486  [   96/  130]
train() client id: f_00008-0-3 loss: 0.742195  [  128/  130]
train() client id: f_00008-1-0 loss: 0.714434  [   32/  130]
train() client id: f_00008-1-1 loss: 0.766474  [   64/  130]
train() client id: f_00008-1-2 loss: 0.733005  [   96/  130]
train() client id: f_00008-1-3 loss: 0.651671  [  128/  130]
train() client id: f_00008-2-0 loss: 0.739554  [   32/  130]
train() client id: f_00008-2-1 loss: 0.671762  [   64/  130]
train() client id: f_00008-2-2 loss: 0.736628  [   96/  130]
train() client id: f_00008-2-3 loss: 0.724913  [  128/  130]
train() client id: f_00008-3-0 loss: 0.722259  [   32/  130]
train() client id: f_00008-3-1 loss: 0.635710  [   64/  130]
train() client id: f_00008-3-2 loss: 0.758928  [   96/  130]
train() client id: f_00008-3-3 loss: 0.755121  [  128/  130]
train() client id: f_00008-4-0 loss: 0.688702  [   32/  130]
train() client id: f_00008-4-1 loss: 0.696803  [   64/  130]
train() client id: f_00008-4-2 loss: 0.702263  [   96/  130]
train() client id: f_00008-4-3 loss: 0.787968  [  128/  130]
train() client id: f_00008-5-0 loss: 0.774292  [   32/  130]
train() client id: f_00008-5-1 loss: 0.658819  [   64/  130]
train() client id: f_00008-5-2 loss: 0.799345  [   96/  130]
train() client id: f_00008-5-3 loss: 0.604918  [  128/  130]
train() client id: f_00008-6-0 loss: 0.685158  [   32/  130]
train() client id: f_00008-6-1 loss: 0.742583  [   64/  130]
train() client id: f_00008-6-2 loss: 0.679493  [   96/  130]
train() client id: f_00008-6-3 loss: 0.758790  [  128/  130]
train() client id: f_00008-7-0 loss: 0.600009  [   32/  130]
train() client id: f_00008-7-1 loss: 0.815810  [   64/  130]
train() client id: f_00008-7-2 loss: 0.720825  [   96/  130]
train() client id: f_00008-7-3 loss: 0.712163  [  128/  130]
train() client id: f_00008-8-0 loss: 0.660426  [   32/  130]
train() client id: f_00008-8-1 loss: 0.651921  [   64/  130]
train() client id: f_00008-8-2 loss: 0.718208  [   96/  130]
train() client id: f_00008-8-3 loss: 0.832308  [  128/  130]
train() client id: f_00008-9-0 loss: 0.696500  [   32/  130]
train() client id: f_00008-9-1 loss: 0.744854  [   64/  130]
train() client id: f_00008-9-2 loss: 0.637630  [   96/  130]
train() client id: f_00008-9-3 loss: 0.789770  [  128/  130]
train() client id: f_00009-0-0 loss: 0.920536  [   32/  118]
train() client id: f_00009-0-1 loss: 0.784799  [   64/  118]
train() client id: f_00009-0-2 loss: 0.993626  [   96/  118]
train() client id: f_00009-1-0 loss: 0.794759  [   32/  118]
train() client id: f_00009-1-1 loss: 0.900501  [   64/  118]
train() client id: f_00009-1-2 loss: 0.809730  [   96/  118]
train() client id: f_00009-2-0 loss: 0.751480  [   32/  118]
train() client id: f_00009-2-1 loss: 0.797666  [   64/  118]
train() client id: f_00009-2-2 loss: 0.777389  [   96/  118]
train() client id: f_00009-3-0 loss: 0.723462  [   32/  118]
train() client id: f_00009-3-1 loss: 0.747506  [   64/  118]
train() client id: f_00009-3-2 loss: 0.864474  [   96/  118]
train() client id: f_00009-4-0 loss: 0.813681  [   32/  118]
train() client id: f_00009-4-1 loss: 0.684877  [   64/  118]
train() client id: f_00009-4-2 loss: 0.904883  [   96/  118]
train() client id: f_00009-5-0 loss: 0.799695  [   32/  118]
train() client id: f_00009-5-1 loss: 0.789206  [   64/  118]
train() client id: f_00009-5-2 loss: 0.682499  [   96/  118]
train() client id: f_00009-6-0 loss: 0.944434  [   32/  118]
train() client id: f_00009-6-1 loss: 0.785242  [   64/  118]
train() client id: f_00009-6-2 loss: 0.581785  [   96/  118]
train() client id: f_00009-7-0 loss: 0.628031  [   32/  118]
train() client id: f_00009-7-1 loss: 0.868657  [   64/  118]
train() client id: f_00009-7-2 loss: 0.790348  [   96/  118]
train() client id: f_00009-8-0 loss: 0.773112  [   32/  118]
train() client id: f_00009-8-1 loss: 0.717954  [   64/  118]
train() client id: f_00009-8-2 loss: 0.664089  [   96/  118]
train() client id: f_00009-9-0 loss: 0.771349  [   32/  118]
train() client id: f_00009-9-1 loss: 0.761342  [   64/  118]
train() client id: f_00009-9-2 loss: 0.578787  [   96/  118]
At round 61 accuracy: 0.6472148541114059
At round 61 training accuracy: 0.5888665325285044
At round 61 training loss: 0.8318413919068185
update_location
xs = [  -3.9056584     4.20031788  325.00902392   18.81129433    0.97929623
    3.95640986 -287.44319194 -266.32485185  309.66397685 -252.06087855]
ys = [ 317.5879595   300.55583871    1.32061395 -287.45517586  279.35018685
  262.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [332.98253135 316.78297753 340.04795198 304.93334177 296.71111525
 281.22398726 304.3525573  284.48128755 325.88410291 271.20231996]
dists_bs = [222.7209014  218.80818669 529.31422911 501.42557522 204.59148124
 199.34071762 210.22394361 196.66744272 509.59168784 187.56031626]
uav_gains = [1.52568368e-12 1.97196846e-12 1.37954414e-12 2.43484695e-12
 2.84867986e-12 3.89882946e-12 2.46132727e-12 3.64447161e-12
 1.69955459e-12 4.80856961e-12]
bs_gains = [2.94824353e-11 3.09824743e-11 2.61153645e-12 3.03889670e-12
 3.73946054e-11 4.02184414e-11 3.46564429e-11 4.17679507e-11
 2.90450111e-12 4.76979163e-11]
Round 62
-------------------------------
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.82115899 5.66246847 2.78706489 1.03447434 6.52759612 3.14171865
 1.26660636 3.89259813 2.86603349 2.54747095]
obj_prev = 32.54719040436046
eta_min = 1.2218214947720352e-33	eta_max = 0.9394406220974982
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 7.459395796309552	eta = 0.9090909090909091
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 18.0305957827678	eta = 0.37609788314465736
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 12.220288063290287	eta = 0.5549189078534793
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.203657475325056	eta = 0.605272779953424
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140690513111194	eta = 0.6086937697223753
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140415786273087	eta = 0.6087087803393881
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.14041578100363	eta = 0.6087087806273096
eta = 0.6087087806273096
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.040963   0.08615235 0.0403128  0.01397944 0.09948158 0.04746507
 0.01755557 0.05819347 0.04226342 0.03836218]
ene_total = [1.12030085 1.67073395 1.1295553  0.56372656 1.90151444 0.97618988
 0.62570552 1.29997444 1.04280632 0.80990852]
ti_comp = [1.2940368  1.43579767 1.28232047 1.33779388 1.43906047 1.44025443
 1.33863419 1.36510734 1.35070396 1.44291406]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [2.56544140e-06 1.93862966e-05 2.49009332e-06 9.54048993e-08
 2.97132334e-05 3.22198749e-06 1.88712718e-07 6.60950921e-06
 2.58614832e-06 1.69476237e-06]
ene_total = [0.39332206 0.14201894 0.41411524 0.31561678 0.13641131 0.13382205
 0.31412702 0.26725561 0.29274776 0.12907455]
optimize_network iter = 0 obj = 2.5385113123879783
eta = 0.6087087806273096
freqs = [15827603.24540961 30001562.15823117 15718689.94891933  5224809.85098799
 34564766.11963843 16478014.31267369  6557270.26566169 21314613.85120587
 15644958.54527248 13293299.23888385]
eta_min = 0.6087087806273109	eta_max = 0.7286862650795968
af = 0.0009590243179496368	bf = 0.9928211124535382	zeta = 0.0010549267497446007	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.92556679e-07 3.72210798e-06 4.78090085e-07 1.83174406e-08
 5.70484736e-06 6.18611464e-07 3.62322484e-08 1.26900498e-06
 4.96532343e-07 3.25389044e-07]
ene_total = [1.6927798  0.61008177 1.78228252 1.35847294 0.58530795 0.57579846
 1.35205494 1.14991453 1.25988629 0.5554585 ]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 1 obj = 3.660319648720293
eta = 0.7286862650795968
freqs = [15746566.89412167 28283149.87566576 15718689.94891934  5104500.91249742
 32549667.43739824 15511225.99197442  6404139.63058966 20604377.17176979
 15207342.34336579 12502384.68143362]
eta_min = 0.7286862650796062	eta_max = 0.7286862650795745
af = 0.0008653145558439159	bf = 0.9928211124535382	zeta = 0.0009518460114283075	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.87525871e-07 3.30793358e-06 4.78090085e-07 1.74835808e-08
 5.05906076e-06 5.48151319e-07 3.45597587e-08 1.18584348e-06
 4.69143123e-07 2.87821399e-07]
ene_total = [1.69277942 0.61005013 1.78228252 1.35847287 0.58525862 0.57579307
 1.35205481 1.14990818 1.25988419 0.55545563]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 2 obj = 3.6603196487199923
eta = 0.7286862650795745
freqs = [15746566.89412155 28283149.87566597 15718689.9489192   5104500.91249741
 32549667.4373985  15511225.99197455  6404139.63058965 20604377.17176981
 15207342.34336578 12502384.68143372]
Done!
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.56211252e-06 1.05991595e-05 1.53187873e-06 5.60202486e-08
 1.62100570e-05 1.75636636e-06 1.10735112e-07 3.79963621e-06
 1.50321120e-06 9.22226774e-07]
ene_total = [0.02216009 0.00799304 0.02333169 0.01778287 0.00767237 0.00753852
 0.0176989  0.01505527 0.01649331 0.00727172]
At round 62 energy consumption: 0.1429977884216537
At round 62 eta: 0.7286862650795745
At round 62 a_n: 6.944760344156705
At round 62 local rounds: 10.364211052331338
At round 62 global rounds: 25.59678869996595
gradient difference: 0.5309145450592041
train() client id: f_00000-0-0 loss: 1.095740  [   32/  126]
train() client id: f_00000-0-1 loss: 0.971786  [   64/  126]
train() client id: f_00000-0-2 loss: 1.280807  [   96/  126]
train() client id: f_00000-1-0 loss: 1.113219  [   32/  126]
train() client id: f_00000-1-1 loss: 0.849420  [   64/  126]
train() client id: f_00000-1-2 loss: 1.051591  [   96/  126]
train() client id: f_00000-2-0 loss: 0.894151  [   32/  126]
train() client id: f_00000-2-1 loss: 1.107111  [   64/  126]
train() client id: f_00000-2-2 loss: 1.050890  [   96/  126]
train() client id: f_00000-3-0 loss: 0.977141  [   32/  126]
train() client id: f_00000-3-1 loss: 1.038474  [   64/  126]
train() client id: f_00000-3-2 loss: 0.854764  [   96/  126]
train() client id: f_00000-4-0 loss: 0.906147  [   32/  126]
train() client id: f_00000-4-1 loss: 1.042283  [   64/  126]
train() client id: f_00000-4-2 loss: 0.910258  [   96/  126]
train() client id: f_00000-5-0 loss: 0.898018  [   32/  126]
train() client id: f_00000-5-1 loss: 0.893489  [   64/  126]
train() client id: f_00000-5-2 loss: 0.842988  [   96/  126]
train() client id: f_00000-6-0 loss: 0.873226  [   32/  126]
train() client id: f_00000-6-1 loss: 0.932758  [   64/  126]
train() client id: f_00000-6-2 loss: 0.930863  [   96/  126]
train() client id: f_00000-7-0 loss: 0.777107  [   32/  126]
train() client id: f_00000-7-1 loss: 0.990372  [   64/  126]
train() client id: f_00000-7-2 loss: 0.873648  [   96/  126]
train() client id: f_00000-8-0 loss: 0.870867  [   32/  126]
train() client id: f_00000-8-1 loss: 0.957762  [   64/  126]
train() client id: f_00000-8-2 loss: 0.969395  [   96/  126]
train() client id: f_00000-9-0 loss: 0.879708  [   32/  126]
train() client id: f_00000-9-1 loss: 0.936274  [   64/  126]
train() client id: f_00000-9-2 loss: 0.925517  [   96/  126]
train() client id: f_00001-0-0 loss: 0.495702  [   32/  265]
train() client id: f_00001-0-1 loss: 0.424848  [   64/  265]
train() client id: f_00001-0-2 loss: 0.501304  [   96/  265]
train() client id: f_00001-0-3 loss: 0.463675  [  128/  265]
train() client id: f_00001-0-4 loss: 0.528162  [  160/  265]
train() client id: f_00001-0-5 loss: 0.488112  [  192/  265]
train() client id: f_00001-0-6 loss: 0.498085  [  224/  265]
train() client id: f_00001-0-7 loss: 0.577628  [  256/  265]
train() client id: f_00001-1-0 loss: 0.497929  [   32/  265]
train() client id: f_00001-1-1 loss: 0.426391  [   64/  265]
train() client id: f_00001-1-2 loss: 0.509443  [   96/  265]
train() client id: f_00001-1-3 loss: 0.433321  [  128/  265]
train() client id: f_00001-1-4 loss: 0.502771  [  160/  265]
train() client id: f_00001-1-5 loss: 0.483372  [  192/  265]
train() client id: f_00001-1-6 loss: 0.569676  [  224/  265]
train() client id: f_00001-1-7 loss: 0.460346  [  256/  265]
train() client id: f_00001-2-0 loss: 0.408544  [   32/  265]
train() client id: f_00001-2-1 loss: 0.539337  [   64/  265]
train() client id: f_00001-2-2 loss: 0.497070  [   96/  265]
train() client id: f_00001-2-3 loss: 0.544917  [  128/  265]
train() client id: f_00001-2-4 loss: 0.466475  [  160/  265]
train() client id: f_00001-2-5 loss: 0.410248  [  192/  265]
train() client id: f_00001-2-6 loss: 0.547889  [  224/  265]
train() client id: f_00001-2-7 loss: 0.465874  [  256/  265]
train() client id: f_00001-3-0 loss: 0.393825  [   32/  265]
train() client id: f_00001-3-1 loss: 0.452178  [   64/  265]
train() client id: f_00001-3-2 loss: 0.422893  [   96/  265]
train() client id: f_00001-3-3 loss: 0.530373  [  128/  265]
train() client id: f_00001-3-4 loss: 0.474872  [  160/  265]
train() client id: f_00001-3-5 loss: 0.660583  [  192/  265]
train() client id: f_00001-3-6 loss: 0.414089  [  224/  265]
train() client id: f_00001-3-7 loss: 0.500366  [  256/  265]
train() client id: f_00001-4-0 loss: 0.391064  [   32/  265]
train() client id: f_00001-4-1 loss: 0.444967  [   64/  265]
train() client id: f_00001-4-2 loss: 0.497389  [   96/  265]
train() client id: f_00001-4-3 loss: 0.530024  [  128/  265]
train() client id: f_00001-4-4 loss: 0.371477  [  160/  265]
train() client id: f_00001-4-5 loss: 0.530942  [  192/  265]
train() client id: f_00001-4-6 loss: 0.559839  [  224/  265]
train() client id: f_00001-4-7 loss: 0.510487  [  256/  265]
train() client id: f_00001-5-0 loss: 0.423497  [   32/  265]
train() client id: f_00001-5-1 loss: 0.555935  [   64/  265]
train() client id: f_00001-5-2 loss: 0.417232  [   96/  265]
train() client id: f_00001-5-3 loss: 0.563101  [  128/  265]
train() client id: f_00001-5-4 loss: 0.444285  [  160/  265]
train() client id: f_00001-5-5 loss: 0.519923  [  192/  265]
train() client id: f_00001-5-6 loss: 0.385572  [  224/  265]
train() client id: f_00001-5-7 loss: 0.505391  [  256/  265]
train() client id: f_00001-6-0 loss: 0.386263  [   32/  265]
train() client id: f_00001-6-1 loss: 0.525778  [   64/  265]
train() client id: f_00001-6-2 loss: 0.424821  [   96/  265]
train() client id: f_00001-6-3 loss: 0.448581  [  128/  265]
train() client id: f_00001-6-4 loss: 0.595226  [  160/  265]
train() client id: f_00001-6-5 loss: 0.467649  [  192/  265]
train() client id: f_00001-6-6 loss: 0.453285  [  224/  265]
train() client id: f_00001-6-7 loss: 0.509548  [  256/  265]
train() client id: f_00001-7-0 loss: 0.520156  [   32/  265]
train() client id: f_00001-7-1 loss: 0.455271  [   64/  265]
train() client id: f_00001-7-2 loss: 0.511433  [   96/  265]
train() client id: f_00001-7-3 loss: 0.437687  [  128/  265]
train() client id: f_00001-7-4 loss: 0.472185  [  160/  265]
train() client id: f_00001-7-5 loss: 0.385937  [  192/  265]
train() client id: f_00001-7-6 loss: 0.491242  [  224/  265]
train() client id: f_00001-7-7 loss: 0.538133  [  256/  265]
train() client id: f_00001-8-0 loss: 0.374095  [   32/  265]
train() client id: f_00001-8-1 loss: 0.457588  [   64/  265]
train() client id: f_00001-8-2 loss: 0.551889  [   96/  265]
train() client id: f_00001-8-3 loss: 0.373509  [  128/  265]
train() client id: f_00001-8-4 loss: 0.705720  [  160/  265]
train() client id: f_00001-8-5 loss: 0.409919  [  192/  265]
train() client id: f_00001-8-6 loss: 0.500979  [  224/  265]
train() client id: f_00001-8-7 loss: 0.433971  [  256/  265]
train() client id: f_00001-9-0 loss: 0.506719  [   32/  265]
train() client id: f_00001-9-1 loss: 0.524734  [   64/  265]
train() client id: f_00001-9-2 loss: 0.389239  [   96/  265]
train() client id: f_00001-9-3 loss: 0.503967  [  128/  265]
train() client id: f_00001-9-4 loss: 0.377184  [  160/  265]
train() client id: f_00001-9-5 loss: 0.469190  [  192/  265]
train() client id: f_00001-9-6 loss: 0.563563  [  224/  265]
train() client id: f_00001-9-7 loss: 0.424270  [  256/  265]
train() client id: f_00002-0-0 loss: 1.339932  [   32/  124]
train() client id: f_00002-0-1 loss: 1.081612  [   64/  124]
train() client id: f_00002-0-2 loss: 1.071661  [   96/  124]
train() client id: f_00002-1-0 loss: 1.221011  [   32/  124]
train() client id: f_00002-1-1 loss: 1.109369  [   64/  124]
train() client id: f_00002-1-2 loss: 0.998258  [   96/  124]
train() client id: f_00002-2-0 loss: 1.043625  [   32/  124]
train() client id: f_00002-2-1 loss: 1.063842  [   64/  124]
train() client id: f_00002-2-2 loss: 1.157649  [   96/  124]
train() client id: f_00002-3-0 loss: 1.093377  [   32/  124]
train() client id: f_00002-3-1 loss: 1.046640  [   64/  124]
train() client id: f_00002-3-2 loss: 1.005402  [   96/  124]
train() client id: f_00002-4-0 loss: 1.183693  [   32/  124]
train() client id: f_00002-4-1 loss: 1.018726  [   64/  124]
train() client id: f_00002-4-2 loss: 0.842860  [   96/  124]
train() client id: f_00002-5-0 loss: 0.932235  [   32/  124]
train() client id: f_00002-5-1 loss: 0.971976  [   64/  124]
train() client id: f_00002-5-2 loss: 1.047142  [   96/  124]
train() client id: f_00002-6-0 loss: 0.971672  [   32/  124]
train() client id: f_00002-6-1 loss: 0.963195  [   64/  124]
train() client id: f_00002-6-2 loss: 1.162036  [   96/  124]
train() client id: f_00002-7-0 loss: 0.997644  [   32/  124]
train() client id: f_00002-7-1 loss: 0.919986  [   64/  124]
train() client id: f_00002-7-2 loss: 1.175041  [   96/  124]
train() client id: f_00002-8-0 loss: 0.858359  [   32/  124]
train() client id: f_00002-8-1 loss: 0.917827  [   64/  124]
train() client id: f_00002-8-2 loss: 1.134386  [   96/  124]
train() client id: f_00002-9-0 loss: 0.968209  [   32/  124]
train() client id: f_00002-9-1 loss: 1.129354  [   64/  124]
train() client id: f_00002-9-2 loss: 0.931169  [   96/  124]
train() client id: f_00003-0-0 loss: 0.770908  [   32/   43]
train() client id: f_00003-1-0 loss: 0.671884  [   32/   43]
train() client id: f_00003-2-0 loss: 0.727032  [   32/   43]
train() client id: f_00003-3-0 loss: 0.540556  [   32/   43]
train() client id: f_00003-4-0 loss: 0.849677  [   32/   43]
train() client id: f_00003-5-0 loss: 0.720454  [   32/   43]
train() client id: f_00003-6-0 loss: 0.836546  [   32/   43]
train() client id: f_00003-7-0 loss: 0.531836  [   32/   43]
train() client id: f_00003-8-0 loss: 0.804752  [   32/   43]
train() client id: f_00003-9-0 loss: 0.517069  [   32/   43]
train() client id: f_00004-0-0 loss: 1.032688  [   32/  306]
train() client id: f_00004-0-1 loss: 1.099088  [   64/  306]
train() client id: f_00004-0-2 loss: 0.843166  [   96/  306]
train() client id: f_00004-0-3 loss: 0.880168  [  128/  306]
train() client id: f_00004-0-4 loss: 0.714386  [  160/  306]
train() client id: f_00004-0-5 loss: 0.890602  [  192/  306]
train() client id: f_00004-0-6 loss: 0.986785  [  224/  306]
train() client id: f_00004-0-7 loss: 0.884354  [  256/  306]
train() client id: f_00004-0-8 loss: 0.769501  [  288/  306]
train() client id: f_00004-1-0 loss: 0.972308  [   32/  306]
train() client id: f_00004-1-1 loss: 0.905782  [   64/  306]
train() client id: f_00004-1-2 loss: 0.931613  [   96/  306]
train() client id: f_00004-1-3 loss: 0.836444  [  128/  306]
train() client id: f_00004-1-4 loss: 0.958258  [  160/  306]
train() client id: f_00004-1-5 loss: 0.665718  [  192/  306]
train() client id: f_00004-1-6 loss: 0.821560  [  224/  306]
train() client id: f_00004-1-7 loss: 0.996418  [  256/  306]
train() client id: f_00004-1-8 loss: 0.880617  [  288/  306]
train() client id: f_00004-2-0 loss: 0.859846  [   32/  306]
train() client id: f_00004-2-1 loss: 0.793381  [   64/  306]
train() client id: f_00004-2-2 loss: 1.005739  [   96/  306]
train() client id: f_00004-2-3 loss: 0.807605  [  128/  306]
train() client id: f_00004-2-4 loss: 0.929163  [  160/  306]
train() client id: f_00004-2-5 loss: 0.909736  [  192/  306]
train() client id: f_00004-2-6 loss: 0.812500  [  224/  306]
train() client id: f_00004-2-7 loss: 0.915082  [  256/  306]
train() client id: f_00004-2-8 loss: 0.975633  [  288/  306]
train() client id: f_00004-3-0 loss: 1.020398  [   32/  306]
train() client id: f_00004-3-1 loss: 0.891432  [   64/  306]
train() client id: f_00004-3-2 loss: 0.785801  [   96/  306]
train() client id: f_00004-3-3 loss: 1.072823  [  128/  306]
train() client id: f_00004-3-4 loss: 0.870094  [  160/  306]
train() client id: f_00004-3-5 loss: 0.859704  [  192/  306]
train() client id: f_00004-3-6 loss: 0.862431  [  224/  306]
train() client id: f_00004-3-7 loss: 0.870491  [  256/  306]
train() client id: f_00004-3-8 loss: 0.874585  [  288/  306]
train() client id: f_00004-4-0 loss: 0.828427  [   32/  306]
train() client id: f_00004-4-1 loss: 0.949583  [   64/  306]
train() client id: f_00004-4-2 loss: 0.881981  [   96/  306]
train() client id: f_00004-4-3 loss: 0.844324  [  128/  306]
train() client id: f_00004-4-4 loss: 0.846907  [  160/  306]
train() client id: f_00004-4-5 loss: 0.844282  [  192/  306]
train() client id: f_00004-4-6 loss: 1.028351  [  224/  306]
train() client id: f_00004-4-7 loss: 0.970587  [  256/  306]
train() client id: f_00004-4-8 loss: 0.945015  [  288/  306]
train() client id: f_00004-5-0 loss: 0.976953  [   32/  306]
train() client id: f_00004-5-1 loss: 0.860459  [   64/  306]
train() client id: f_00004-5-2 loss: 0.916900  [   96/  306]
train() client id: f_00004-5-3 loss: 0.820323  [  128/  306]
train() client id: f_00004-5-4 loss: 0.837683  [  160/  306]
train() client id: f_00004-5-5 loss: 0.803345  [  192/  306]
train() client id: f_00004-5-6 loss: 1.036978  [  224/  306]
train() client id: f_00004-5-7 loss: 0.930955  [  256/  306]
train() client id: f_00004-5-8 loss: 0.901557  [  288/  306]
train() client id: f_00004-6-0 loss: 0.896801  [   32/  306]
train() client id: f_00004-6-1 loss: 0.947918  [   64/  306]
train() client id: f_00004-6-2 loss: 0.929013  [   96/  306]
train() client id: f_00004-6-3 loss: 1.040260  [  128/  306]
train() client id: f_00004-6-4 loss: 0.818553  [  160/  306]
train() client id: f_00004-6-5 loss: 0.859536  [  192/  306]
train() client id: f_00004-6-6 loss: 0.773627  [  224/  306]
train() client id: f_00004-6-7 loss: 0.697963  [  256/  306]
train() client id: f_00004-6-8 loss: 1.063932  [  288/  306]
train() client id: f_00004-7-0 loss: 1.000049  [   32/  306]
train() client id: f_00004-7-1 loss: 0.840569  [   64/  306]
train() client id: f_00004-7-2 loss: 0.897545  [   96/  306]
train() client id: f_00004-7-3 loss: 1.074079  [  128/  306]
train() client id: f_00004-7-4 loss: 0.793946  [  160/  306]
train() client id: f_00004-7-5 loss: 0.785402  [  192/  306]
train() client id: f_00004-7-6 loss: 0.962790  [  224/  306]
train() client id: f_00004-7-7 loss: 0.869142  [  256/  306]
train() client id: f_00004-7-8 loss: 0.910096  [  288/  306]
train() client id: f_00004-8-0 loss: 0.854251  [   32/  306]
train() client id: f_00004-8-1 loss: 0.947039  [   64/  306]
train() client id: f_00004-8-2 loss: 0.857819  [   96/  306]
train() client id: f_00004-8-3 loss: 0.861885  [  128/  306]
train() client id: f_00004-8-4 loss: 0.944699  [  160/  306]
train() client id: f_00004-8-5 loss: 1.002471  [  192/  306]
train() client id: f_00004-8-6 loss: 0.857392  [  224/  306]
train() client id: f_00004-8-7 loss: 0.837365  [  256/  306]
train() client id: f_00004-8-8 loss: 0.863824  [  288/  306]
train() client id: f_00004-9-0 loss: 0.774746  [   32/  306]
train() client id: f_00004-9-1 loss: 0.953706  [   64/  306]
train() client id: f_00004-9-2 loss: 0.808700  [   96/  306]
train() client id: f_00004-9-3 loss: 0.789168  [  128/  306]
train() client id: f_00004-9-4 loss: 0.972905  [  160/  306]
train() client id: f_00004-9-5 loss: 1.085368  [  192/  306]
train() client id: f_00004-9-6 loss: 0.851002  [  224/  306]
train() client id: f_00004-9-7 loss: 1.008612  [  256/  306]
train() client id: f_00004-9-8 loss: 0.902597  [  288/  306]
train() client id: f_00005-0-0 loss: 0.342461  [   32/  146]
train() client id: f_00005-0-1 loss: 0.271286  [   64/  146]
train() client id: f_00005-0-2 loss: 0.442436  [   96/  146]
train() client id: f_00005-0-3 loss: 0.479640  [  128/  146]
train() client id: f_00005-1-0 loss: 0.424314  [   32/  146]
train() client id: f_00005-1-1 loss: 0.555776  [   64/  146]
train() client id: f_00005-1-2 loss: 0.326799  [   96/  146]
train() client id: f_00005-1-3 loss: 0.254784  [  128/  146]
train() client id: f_00005-2-0 loss: 0.416996  [   32/  146]
train() client id: f_00005-2-1 loss: 0.516563  [   64/  146]
train() client id: f_00005-2-2 loss: 0.326494  [   96/  146]
train() client id: f_00005-2-3 loss: 0.360769  [  128/  146]
train() client id: f_00005-3-0 loss: 0.484670  [   32/  146]
train() client id: f_00005-3-1 loss: 0.411761  [   64/  146]
train() client id: f_00005-3-2 loss: 0.475842  [   96/  146]
train() client id: f_00005-3-3 loss: 0.097139  [  128/  146]
train() client id: f_00005-4-0 loss: 0.359264  [   32/  146]
train() client id: f_00005-4-1 loss: 0.039905  [   64/  146]
train() client id: f_00005-4-2 loss: 0.344292  [   96/  146]
train() client id: f_00005-4-3 loss: 0.635025  [  128/  146]
train() client id: f_00005-5-0 loss: 0.552936  [   32/  146]
train() client id: f_00005-5-1 loss: 0.220920  [   64/  146]
train() client id: f_00005-5-2 loss: 0.112725  [   96/  146]
train() client id: f_00005-5-3 loss: 0.513336  [  128/  146]
train() client id: f_00005-6-0 loss: 0.500930  [   32/  146]
train() client id: f_00005-6-1 loss: 0.404853  [   64/  146]
train() client id: f_00005-6-2 loss: 0.203751  [   96/  146]
train() client id: f_00005-6-3 loss: 0.282716  [  128/  146]
train() client id: f_00005-7-0 loss: 0.281913  [   32/  146]
train() client id: f_00005-7-1 loss: 0.410526  [   64/  146]
train() client id: f_00005-7-2 loss: 0.542357  [   96/  146]
train() client id: f_00005-7-3 loss: 0.334339  [  128/  146]
train() client id: f_00005-8-0 loss: 0.182423  [   32/  146]
train() client id: f_00005-8-1 loss: 0.643453  [   64/  146]
train() client id: f_00005-8-2 loss: 0.320119  [   96/  146]
train() client id: f_00005-8-3 loss: 0.468136  [  128/  146]
train() client id: f_00005-9-0 loss: 0.351146  [   32/  146]
train() client id: f_00005-9-1 loss: 0.106031  [   64/  146]
train() client id: f_00005-9-2 loss: 0.424154  [   96/  146]
train() client id: f_00005-9-3 loss: 0.498135  [  128/  146]
train() client id: f_00006-0-0 loss: 0.523583  [   32/   54]
train() client id: f_00006-1-0 loss: 0.525284  [   32/   54]
train() client id: f_00006-2-0 loss: 0.411684  [   32/   54]
train() client id: f_00006-3-0 loss: 0.477274  [   32/   54]
train() client id: f_00006-4-0 loss: 0.528353  [   32/   54]
train() client id: f_00006-5-0 loss: 0.534331  [   32/   54]
train() client id: f_00006-6-0 loss: 0.514367  [   32/   54]
train() client id: f_00006-7-0 loss: 0.488927  [   32/   54]
train() client id: f_00006-8-0 loss: 0.537405  [   32/   54]
train() client id: f_00006-9-0 loss: 0.463543  [   32/   54]
train() client id: f_00007-0-0 loss: 0.505078  [   32/  179]
train() client id: f_00007-0-1 loss: 0.637914  [   64/  179]
train() client id: f_00007-0-2 loss: 0.536330  [   96/  179]
train() client id: f_00007-0-3 loss: 0.695147  [  128/  179]
train() client id: f_00007-0-4 loss: 0.534125  [  160/  179]
train() client id: f_00007-1-0 loss: 0.550034  [   32/  179]
train() client id: f_00007-1-1 loss: 0.625049  [   64/  179]
train() client id: f_00007-1-2 loss: 0.523123  [   96/  179]
train() client id: f_00007-1-3 loss: 0.771764  [  128/  179]
train() client id: f_00007-1-4 loss: 0.515904  [  160/  179]
train() client id: f_00007-2-0 loss: 0.430339  [   32/  179]
train() client id: f_00007-2-1 loss: 0.753966  [   64/  179]
train() client id: f_00007-2-2 loss: 0.542291  [   96/  179]
train() client id: f_00007-2-3 loss: 0.675326  [  128/  179]
train() client id: f_00007-2-4 loss: 0.635646  [  160/  179]
train() client id: f_00007-3-0 loss: 0.534969  [   32/  179]
train() client id: f_00007-3-1 loss: 0.563007  [   64/  179]
train() client id: f_00007-3-2 loss: 0.838712  [   96/  179]
train() client id: f_00007-3-3 loss: 0.462930  [  128/  179]
train() client id: f_00007-3-4 loss: 0.605704  [  160/  179]
train() client id: f_00007-4-0 loss: 0.485586  [   32/  179]
train() client id: f_00007-4-1 loss: 0.613745  [   64/  179]
train() client id: f_00007-4-2 loss: 0.604142  [   96/  179]
train() client id: f_00007-4-3 loss: 0.822184  [  128/  179]
train() client id: f_00007-4-4 loss: 0.384756  [  160/  179]
train() client id: f_00007-5-0 loss: 0.469575  [   32/  179]
train() client id: f_00007-5-1 loss: 0.551494  [   64/  179]
train() client id: f_00007-5-2 loss: 0.545278  [   96/  179]
train() client id: f_00007-5-3 loss: 0.764154  [  128/  179]
train() client id: f_00007-5-4 loss: 0.653773  [  160/  179]
train() client id: f_00007-6-0 loss: 0.540430  [   32/  179]
train() client id: f_00007-6-1 loss: 0.489861  [   64/  179]
train() client id: f_00007-6-2 loss: 0.636788  [   96/  179]
train() client id: f_00007-6-3 loss: 0.593773  [  128/  179]
train() client id: f_00007-6-4 loss: 0.407592  [  160/  179]
train() client id: f_00007-7-0 loss: 0.467226  [   32/  179]
train() client id: f_00007-7-1 loss: 0.679071  [   64/  179]
train() client id: f_00007-7-2 loss: 0.423440  [   96/  179]
train() client id: f_00007-7-3 loss: 0.488150  [  128/  179]
train() client id: f_00007-7-4 loss: 0.696860  [  160/  179]
train() client id: f_00007-8-0 loss: 0.626944  [   32/  179]
train() client id: f_00007-8-1 loss: 0.493791  [   64/  179]
train() client id: f_00007-8-2 loss: 0.664091  [   96/  179]
train() client id: f_00007-8-3 loss: 0.594927  [  128/  179]
train() client id: f_00007-8-4 loss: 0.532400  [  160/  179]
train() client id: f_00007-9-0 loss: 0.571910  [   32/  179]
train() client id: f_00007-9-1 loss: 0.586818  [   64/  179]
train() client id: f_00007-9-2 loss: 0.467541  [   96/  179]
train() client id: f_00007-9-3 loss: 0.504796  [  128/  179]
train() client id: f_00007-9-4 loss: 0.637845  [  160/  179]
train() client id: f_00008-0-0 loss: 0.673487  [   32/  130]
train() client id: f_00008-0-1 loss: 0.570295  [   64/  130]
train() client id: f_00008-0-2 loss: 0.709896  [   96/  130]
train() client id: f_00008-0-3 loss: 0.699423  [  128/  130]
train() client id: f_00008-1-0 loss: 0.686806  [   32/  130]
train() client id: f_00008-1-1 loss: 0.674023  [   64/  130]
train() client id: f_00008-1-2 loss: 0.594745  [   96/  130]
train() client id: f_00008-1-3 loss: 0.695626  [  128/  130]
train() client id: f_00008-2-0 loss: 0.591784  [   32/  130]
train() client id: f_00008-2-1 loss: 0.660392  [   64/  130]
train() client id: f_00008-2-2 loss: 0.690121  [   96/  130]
train() client id: f_00008-2-3 loss: 0.727582  [  128/  130]
train() client id: f_00008-3-0 loss: 0.629373  [   32/  130]
train() client id: f_00008-3-1 loss: 0.693917  [   64/  130]
train() client id: f_00008-3-2 loss: 0.716529  [   96/  130]
train() client id: f_00008-3-3 loss: 0.639168  [  128/  130]
train() client id: f_00008-4-0 loss: 0.660058  [   32/  130]
train() client id: f_00008-4-1 loss: 0.681468  [   64/  130]
train() client id: f_00008-4-2 loss: 0.651110  [   96/  130]
train() client id: f_00008-4-3 loss: 0.674991  [  128/  130]
train() client id: f_00008-5-0 loss: 0.611108  [   32/  130]
train() client id: f_00008-5-1 loss: 0.569925  [   64/  130]
train() client id: f_00008-5-2 loss: 0.800091  [   96/  130]
train() client id: f_00008-5-3 loss: 0.694447  [  128/  130]
train() client id: f_00008-6-0 loss: 0.536660  [   32/  130]
train() client id: f_00008-6-1 loss: 0.640954  [   64/  130]
train() client id: f_00008-6-2 loss: 0.807221  [   96/  130]
train() client id: f_00008-6-3 loss: 0.697821  [  128/  130]
train() client id: f_00008-7-0 loss: 0.689161  [   32/  130]
train() client id: f_00008-7-1 loss: 0.600441  [   64/  130]
train() client id: f_00008-7-2 loss: 0.721973  [   96/  130]
train() client id: f_00008-7-3 loss: 0.631834  [  128/  130]
train() client id: f_00008-8-0 loss: 0.586808  [   32/  130]
train() client id: f_00008-8-1 loss: 0.745869  [   64/  130]
train() client id: f_00008-8-2 loss: 0.742054  [   96/  130]
train() client id: f_00008-8-3 loss: 0.620762  [  128/  130]
train() client id: f_00008-9-0 loss: 0.768466  [   32/  130]
train() client id: f_00008-9-1 loss: 0.556362  [   64/  130]
train() client id: f_00008-9-2 loss: 0.756069  [   96/  130]
train() client id: f_00008-9-3 loss: 0.619133  [  128/  130]
train() client id: f_00009-0-0 loss: 0.969067  [   32/  118]
train() client id: f_00009-0-1 loss: 1.017479  [   64/  118]
train() client id: f_00009-0-2 loss: 1.193814  [   96/  118]
train() client id: f_00009-1-0 loss: 1.138798  [   32/  118]
train() client id: f_00009-1-1 loss: 1.016651  [   64/  118]
train() client id: f_00009-1-2 loss: 0.931701  [   96/  118]
train() client id: f_00009-2-0 loss: 1.124294  [   32/  118]
train() client id: f_00009-2-1 loss: 1.016431  [   64/  118]
train() client id: f_00009-2-2 loss: 1.062555  [   96/  118]
train() client id: f_00009-3-0 loss: 0.963502  [   32/  118]
train() client id: f_00009-3-1 loss: 0.976185  [   64/  118]
train() client id: f_00009-3-2 loss: 1.011791  [   96/  118]
train() client id: f_00009-4-0 loss: 1.144056  [   32/  118]
train() client id: f_00009-4-1 loss: 0.919205  [   64/  118]
train() client id: f_00009-4-2 loss: 1.063023  [   96/  118]
train() client id: f_00009-5-0 loss: 0.998257  [   32/  118]
train() client id: f_00009-5-1 loss: 1.105014  [   64/  118]
train() client id: f_00009-5-2 loss: 0.770044  [   96/  118]
train() client id: f_00009-6-0 loss: 0.996972  [   32/  118]
train() client id: f_00009-6-1 loss: 0.847358  [   64/  118]
train() client id: f_00009-6-2 loss: 1.072291  [   96/  118]
train() client id: f_00009-7-0 loss: 0.964802  [   32/  118]
train() client id: f_00009-7-1 loss: 0.818942  [   64/  118]
train() client id: f_00009-7-2 loss: 1.083160  [   96/  118]
train() client id: f_00009-8-0 loss: 0.910256  [   32/  118]
train() client id: f_00009-8-1 loss: 0.854993  [   64/  118]
train() client id: f_00009-8-2 loss: 1.002486  [   96/  118]
train() client id: f_00009-9-0 loss: 1.042251  [   32/  118]
train() client id: f_00009-9-1 loss: 1.034299  [   64/  118]
train() client id: f_00009-9-2 loss: 0.862054  [   96/  118]
At round 62 accuracy: 0.6472148541114059
At round 62 training accuracy: 0.590878604963112
At round 62 training loss: 0.8260926122689091
update_location
xs = [  -3.9056584     4.20031788  330.00902392   18.81129433    0.97929623
    3.95640986 -292.44319194 -271.32485185  314.66397685 -257.06087855]
ys = [ 322.5879595   305.55583871    1.32061395 -292.45517586  284.35018685
  267.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [337.75471245 321.53073452 344.82995793 309.65124686 301.42327014
 285.90220796 309.07913397 289.16751458 330.63890923 275.85559109]
dists_bs = [225.95459613 221.71508958 534.03992676 506.04066956 207.18633175
 201.59331142 212.94022257 199.03851774 514.34942217 189.66939928]
uav_gains = [1.42435425e-12 1.82215782e-12 1.29334638e-12 2.23367367e-12
 2.60101956e-12 3.53945382e-12 2.25679855e-12 3.31121347e-12
 1.57981843e-12 4.36146341e-12]
bs_gains = [2.83161877e-11 2.98584584e-11 2.54734430e-12 2.96193078e-12
 3.60979885e-11 3.89727411e-11 3.34327819e-11 4.03896532e-11
 2.82989921e-12 4.62276411e-11]
Round 63
-------------------------------
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.68727091 5.38361967 2.65494812 0.98808132 6.20602153 2.98709652
 1.20875503 3.70447299 2.72587058 2.42213493]
obj_prev = 30.968271608412685
eta_min = 2.604478585360512e-35	eta_max = 0.9403513878285256
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 7.091465885945381	eta = 0.9090909090909091
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 17.42929397319813	eta = 0.36988229006606876
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 11.714987188776163	eta = 0.5503025368408222
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.719694765497792	eta = 0.601396523881516
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657792616593055	eta = 0.6048895302207599
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519519314839	eta = 0.6049050304207853
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519513957864	eta = 0.6049050307248393
eta = 0.6049050307248393
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.04148026 0.08724024 0.04082185 0.01415596 0.10073778 0.04806443
 0.01777726 0.05892831 0.0427971  0.0388466 ]
ene_total = [1.07662235 1.59233833 1.08542302 0.54531731 1.81230098 0.92997575
 0.60440513 1.2459445  0.99375704 0.77143509]
ti_comp = [1.37728495 1.52626582 1.36543421 1.42198973 1.52960948 1.53088406
 1.42284286 1.45043831 1.44004221 1.53358086]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [2.35156460e-06 1.78143569e-05 2.28042807e-06 8.76809665e-08
 2.73083858e-05 2.96119390e-06 1.73444402e-07 6.07929826e-06
 2.36250370e-06 1.55784967e-06]
ene_total = [0.38350781 0.13481247 0.40330974 0.30876628 0.12938372 0.12684698
 0.3073421  0.26132755 0.2786378  0.12231706]
optimize_network iter = 0 obj = 2.4562515021459133
eta = 0.6049050307248393
freqs = [15058707.65852771 28579633.50514324 14948302.02718542  4977519.32111269
 32929248.53672925 15698259.84497274  6247090.26876788 20313966.31628257
 14859667.46433555 12665323.49976712]
eta_min = 0.6049050307248401	eta_max = 0.735169781083726
af = 0.0008271317221307269	bf = 0.9700368260492027	zeta = 0.0009098448943437997	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.45862869e-07 3.37764920e-06 4.32375195e-07 1.66245432e-08
 5.17774221e-06 5.61450199e-07 3.28855174e-08 1.15265103e-06
 4.47936952e-07 2.95372418e-07]
ene_total = [1.66660366 0.58485258 1.75266791 1.34190688 0.5607002  0.55110835
 1.3357123  1.1353834  1.21083327 0.53150371]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 1 obj = 3.6637216068534983
eta = 0.735169781083726
freqs = [14977014.44528698 26790042.71364152 14948302.02718542  4855109.38680831
 30831466.09028835 14691709.46586062  6091289.81349057 19586300.25463695
 14387143.5153923  11842221.20609098]
eta_min = 0.7351697810837274	eta_max = 0.735169781083714
af = 0.0007390421746511915	bf = 0.9700368260492027	zeta = 0.0008129463921163107	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.41038396e-07 2.96789166e-06 4.32375195e-07 1.58169176e-08
 4.53905178e-06 4.91759633e-07 3.12656630e-08 1.07155188e-06
 4.19901921e-07 2.58228221e-07]
ene_total = [1.66660331 0.58482282 1.75266791 1.34190682 0.56065381 0.55110329
 1.33571218 1.13537751 1.21083123 0.53150101]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 2 obj = 3.6637216068533323
eta = 0.735169781083714
freqs = [14977014.44528692 26790042.71364162 14948302.02718534  4855109.38680831
 30831466.09028849 14691709.46586069  6091289.81349056 19586300.25463695
 14387143.51539229 11842221.20609103]
Done!
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.41315906e-06 9.50960965e-06 1.38540075e-06 5.06799874e-08
 1.45438633e-05 1.57567819e-06 1.00180291e-07 3.43342726e-06
 1.34543434e-06 8.27405398e-07]
ene_total = [0.02294924 0.00805925 0.02413429 0.0184774  0.00772992 0.00758949
 0.01839214 0.01563593 0.01667345 0.00731906]
At round 63 energy consumption: 0.14696016345683638
At round 63 eta: 0.735169781083714
At round 63 a_n: 6.602214497187386
At round 63 local rounds: 10.074148866251232
At round 63 global rounds: 24.929989199134315
gradient difference: 0.5242912769317627
train() client id: f_00000-0-0 loss: 1.109361  [   32/  126]
train() client id: f_00000-0-1 loss: 1.277893  [   64/  126]
train() client id: f_00000-0-2 loss: 1.116739  [   96/  126]
train() client id: f_00000-1-0 loss: 1.253052  [   32/  126]
train() client id: f_00000-1-1 loss: 0.878732  [   64/  126]
train() client id: f_00000-1-2 loss: 1.237187  [   96/  126]
train() client id: f_00000-2-0 loss: 1.070164  [   32/  126]
train() client id: f_00000-2-1 loss: 1.045219  [   64/  126]
train() client id: f_00000-2-2 loss: 0.925553  [   96/  126]
train() client id: f_00000-3-0 loss: 0.965521  [   32/  126]
train() client id: f_00000-3-1 loss: 0.973675  [   64/  126]
train() client id: f_00000-3-2 loss: 0.948653  [   96/  126]
train() client id: f_00000-4-0 loss: 1.033606  [   32/  126]
train() client id: f_00000-4-1 loss: 0.960988  [   64/  126]
train() client id: f_00000-4-2 loss: 0.785825  [   96/  126]
train() client id: f_00000-5-0 loss: 0.943978  [   32/  126]
train() client id: f_00000-5-1 loss: 0.765989  [   64/  126]
train() client id: f_00000-5-2 loss: 1.011858  [   96/  126]
train() client id: f_00000-6-0 loss: 0.762257  [   32/  126]
train() client id: f_00000-6-1 loss: 0.935304  [   64/  126]
train() client id: f_00000-6-2 loss: 0.953603  [   96/  126]
train() client id: f_00000-7-0 loss: 0.961427  [   32/  126]
train() client id: f_00000-7-1 loss: 0.813672  [   64/  126]
train() client id: f_00000-7-2 loss: 0.779482  [   96/  126]
train() client id: f_00000-8-0 loss: 0.896639  [   32/  126]
train() client id: f_00000-8-1 loss: 0.739374  [   64/  126]
train() client id: f_00000-8-2 loss: 0.927915  [   96/  126]
train() client id: f_00000-9-0 loss: 0.774305  [   32/  126]
train() client id: f_00000-9-1 loss: 0.899842  [   64/  126]
train() client id: f_00000-9-2 loss: 0.829503  [   96/  126]
train() client id: f_00001-0-0 loss: 0.517691  [   32/  265]
train() client id: f_00001-0-1 loss: 0.508722  [   64/  265]
train() client id: f_00001-0-2 loss: 0.458106  [   96/  265]
train() client id: f_00001-0-3 loss: 0.473096  [  128/  265]
train() client id: f_00001-0-4 loss: 0.636142  [  160/  265]
train() client id: f_00001-0-5 loss: 0.508174  [  192/  265]
train() client id: f_00001-0-6 loss: 0.493151  [  224/  265]
train() client id: f_00001-0-7 loss: 0.441645  [  256/  265]
train() client id: f_00001-1-0 loss: 0.489494  [   32/  265]
train() client id: f_00001-1-1 loss: 0.476401  [   64/  265]
train() client id: f_00001-1-2 loss: 0.427473  [   96/  265]
train() client id: f_00001-1-3 loss: 0.541120  [  128/  265]
train() client id: f_00001-1-4 loss: 0.548577  [  160/  265]
train() client id: f_00001-1-5 loss: 0.505644  [  192/  265]
train() client id: f_00001-1-6 loss: 0.490554  [  224/  265]
train() client id: f_00001-1-7 loss: 0.540624  [  256/  265]
train() client id: f_00001-2-0 loss: 0.420807  [   32/  265]
train() client id: f_00001-2-1 loss: 0.475680  [   64/  265]
train() client id: f_00001-2-2 loss: 0.436920  [   96/  265]
train() client id: f_00001-2-3 loss: 0.447505  [  128/  265]
train() client id: f_00001-2-4 loss: 0.479563  [  160/  265]
train() client id: f_00001-2-5 loss: 0.562808  [  192/  265]
train() client id: f_00001-2-6 loss: 0.543076  [  224/  265]
train() client id: f_00001-2-7 loss: 0.625622  [  256/  265]
train() client id: f_00001-3-0 loss: 0.493357  [   32/  265]
train() client id: f_00001-3-1 loss: 0.520235  [   64/  265]
train() client id: f_00001-3-2 loss: 0.507992  [   96/  265]
train() client id: f_00001-3-3 loss: 0.613315  [  128/  265]
train() client id: f_00001-3-4 loss: 0.392201  [  160/  265]
train() client id: f_00001-3-5 loss: 0.551660  [  192/  265]
train() client id: f_00001-3-6 loss: 0.410986  [  224/  265]
train() client id: f_00001-3-7 loss: 0.427615  [  256/  265]
train() client id: f_00001-4-0 loss: 0.421410  [   32/  265]
train() client id: f_00001-4-1 loss: 0.621310  [   64/  265]
train() client id: f_00001-4-2 loss: 0.542291  [   96/  265]
train() client id: f_00001-4-3 loss: 0.485096  [  128/  265]
train() client id: f_00001-4-4 loss: 0.469661  [  160/  265]
train() client id: f_00001-4-5 loss: 0.418849  [  192/  265]
train() client id: f_00001-4-6 loss: 0.596506  [  224/  265]
train() client id: f_00001-4-7 loss: 0.396115  [  256/  265]
train() client id: f_00001-5-0 loss: 0.481072  [   32/  265]
train() client id: f_00001-5-1 loss: 0.471870  [   64/  265]
train() client id: f_00001-5-2 loss: 0.470498  [   96/  265]
train() client id: f_00001-5-3 loss: 0.439782  [  128/  265]
train() client id: f_00001-5-4 loss: 0.612646  [  160/  265]
train() client id: f_00001-5-5 loss: 0.427004  [  192/  265]
train() client id: f_00001-5-6 loss: 0.462519  [  224/  265]
train() client id: f_00001-5-7 loss: 0.534713  [  256/  265]
train() client id: f_00001-6-0 loss: 0.397774  [   32/  265]
train() client id: f_00001-6-1 loss: 0.509998  [   64/  265]
train() client id: f_00001-6-2 loss: 0.416510  [   96/  265]
train() client id: f_00001-6-3 loss: 0.516382  [  128/  265]
train() client id: f_00001-6-4 loss: 0.488986  [  160/  265]
train() client id: f_00001-6-5 loss: 0.588684  [  192/  265]
train() client id: f_00001-6-6 loss: 0.420748  [  224/  265]
train() client id: f_00001-6-7 loss: 0.504101  [  256/  265]
train() client id: f_00001-7-0 loss: 0.512972  [   32/  265]
train() client id: f_00001-7-1 loss: 0.573007  [   64/  265]
train() client id: f_00001-7-2 loss: 0.580269  [   96/  265]
train() client id: f_00001-7-3 loss: 0.435596  [  128/  265]
train() client id: f_00001-7-4 loss: 0.522260  [  160/  265]
train() client id: f_00001-7-5 loss: 0.393312  [  192/  265]
train() client id: f_00001-7-6 loss: 0.490619  [  224/  265]
train() client id: f_00001-7-7 loss: 0.399221  [  256/  265]
train() client id: f_00001-8-0 loss: 0.593267  [   32/  265]
train() client id: f_00001-8-1 loss: 0.396618  [   64/  265]
train() client id: f_00001-8-2 loss: 0.501877  [   96/  265]
train() client id: f_00001-8-3 loss: 0.550019  [  128/  265]
train() client id: f_00001-8-4 loss: 0.401675  [  160/  265]
train() client id: f_00001-8-5 loss: 0.509059  [  192/  265]
train() client id: f_00001-8-6 loss: 0.385267  [  224/  265]
train() client id: f_00001-8-7 loss: 0.487492  [  256/  265]
train() client id: f_00001-9-0 loss: 0.474572  [   32/  265]
train() client id: f_00001-9-1 loss: 0.525628  [   64/  265]
train() client id: f_00001-9-2 loss: 0.528087  [   96/  265]
train() client id: f_00001-9-3 loss: 0.458036  [  128/  265]
train() client id: f_00001-9-4 loss: 0.474934  [  160/  265]
train() client id: f_00001-9-5 loss: 0.494330  [  192/  265]
train() client id: f_00001-9-6 loss: 0.410814  [  224/  265]
train() client id: f_00001-9-7 loss: 0.539597  [  256/  265]
train() client id: f_00002-0-0 loss: 1.216411  [   32/  124]
train() client id: f_00002-0-1 loss: 1.148703  [   64/  124]
train() client id: f_00002-0-2 loss: 0.975216  [   96/  124]
train() client id: f_00002-1-0 loss: 1.015132  [   32/  124]
train() client id: f_00002-1-1 loss: 0.818458  [   64/  124]
train() client id: f_00002-1-2 loss: 1.211164  [   96/  124]
train() client id: f_00002-2-0 loss: 0.964012  [   32/  124]
train() client id: f_00002-2-1 loss: 1.029914  [   64/  124]
train() client id: f_00002-2-2 loss: 0.985583  [   96/  124]
train() client id: f_00002-3-0 loss: 1.157683  [   32/  124]
train() client id: f_00002-3-1 loss: 1.171195  [   64/  124]
train() client id: f_00002-3-2 loss: 0.758993  [   96/  124]
train() client id: f_00002-4-0 loss: 0.860834  [   32/  124]
train() client id: f_00002-4-1 loss: 1.182752  [   64/  124]
train() client id: f_00002-4-2 loss: 0.890820  [   96/  124]
train() client id: f_00002-5-0 loss: 0.974163  [   32/  124]
train() client id: f_00002-5-1 loss: 1.201757  [   64/  124]
train() client id: f_00002-5-2 loss: 0.864614  [   96/  124]
train() client id: f_00002-6-0 loss: 0.862504  [   32/  124]
train() client id: f_00002-6-1 loss: 1.161019  [   64/  124]
train() client id: f_00002-6-2 loss: 1.043800  [   96/  124]
train() client id: f_00002-7-0 loss: 0.808677  [   32/  124]
train() client id: f_00002-7-1 loss: 0.807075  [   64/  124]
train() client id: f_00002-7-2 loss: 1.296888  [   96/  124]
train() client id: f_00002-8-0 loss: 1.260003  [   32/  124]
train() client id: f_00002-8-1 loss: 0.896499  [   64/  124]
train() client id: f_00002-8-2 loss: 0.849001  [   96/  124]
train() client id: f_00002-9-0 loss: 1.093928  [   32/  124]
train() client id: f_00002-9-1 loss: 1.030770  [   64/  124]
train() client id: f_00002-9-2 loss: 0.947397  [   96/  124]
train() client id: f_00003-0-0 loss: 1.083781  [   32/   43]
train() client id: f_00003-1-0 loss: 0.821663  [   32/   43]
train() client id: f_00003-2-0 loss: 0.871899  [   32/   43]
train() client id: f_00003-3-0 loss: 0.867144  [   32/   43]
train() client id: f_00003-4-0 loss: 1.041669  [   32/   43]
train() client id: f_00003-5-0 loss: 0.825630  [   32/   43]
train() client id: f_00003-6-0 loss: 0.932398  [   32/   43]
train() client id: f_00003-7-0 loss: 0.987524  [   32/   43]
train() client id: f_00003-8-0 loss: 0.768421  [   32/   43]
train() client id: f_00003-9-0 loss: 0.971324  [   32/   43]
train() client id: f_00004-0-0 loss: 0.772104  [   32/  306]
train() client id: f_00004-0-1 loss: 0.837211  [   64/  306]
train() client id: f_00004-0-2 loss: 0.828556  [   96/  306]
train() client id: f_00004-0-3 loss: 0.954737  [  128/  306]
train() client id: f_00004-0-4 loss: 0.891743  [  160/  306]
train() client id: f_00004-0-5 loss: 0.769173  [  192/  306]
train() client id: f_00004-0-6 loss: 0.922191  [  224/  306]
train() client id: f_00004-0-7 loss: 0.750562  [  256/  306]
train() client id: f_00004-0-8 loss: 0.958831  [  288/  306]
train() client id: f_00004-1-0 loss: 0.713688  [   32/  306]
train() client id: f_00004-1-1 loss: 0.830253  [   64/  306]
train() client id: f_00004-1-2 loss: 0.959982  [   96/  306]
train() client id: f_00004-1-3 loss: 0.871634  [  128/  306]
train() client id: f_00004-1-4 loss: 0.745646  [  160/  306]
train() client id: f_00004-1-5 loss: 0.827536  [  192/  306]
train() client id: f_00004-1-6 loss: 0.917805  [  224/  306]
train() client id: f_00004-1-7 loss: 0.861891  [  256/  306]
train() client id: f_00004-1-8 loss: 0.841096  [  288/  306]
train() client id: f_00004-2-0 loss: 0.923937  [   32/  306]
train() client id: f_00004-2-1 loss: 0.763183  [   64/  306]
train() client id: f_00004-2-2 loss: 0.944311  [   96/  306]
train() client id: f_00004-2-3 loss: 0.855895  [  128/  306]
train() client id: f_00004-2-4 loss: 0.720299  [  160/  306]
train() client id: f_00004-2-5 loss: 0.867976  [  192/  306]
train() client id: f_00004-2-6 loss: 0.845125  [  224/  306]
train() client id: f_00004-2-7 loss: 0.815526  [  256/  306]
train() client id: f_00004-2-8 loss: 0.886750  [  288/  306]
train() client id: f_00004-3-0 loss: 0.813037  [   32/  306]
train() client id: f_00004-3-1 loss: 0.875002  [   64/  306]
train() client id: f_00004-3-2 loss: 0.774806  [   96/  306]
train() client id: f_00004-3-3 loss: 0.836107  [  128/  306]
train() client id: f_00004-3-4 loss: 0.991413  [  160/  306]
train() client id: f_00004-3-5 loss: 0.867067  [  192/  306]
train() client id: f_00004-3-6 loss: 0.755580  [  224/  306]
train() client id: f_00004-3-7 loss: 0.810967  [  256/  306]
train() client id: f_00004-3-8 loss: 0.859793  [  288/  306]
train() client id: f_00004-4-0 loss: 0.875982  [   32/  306]
train() client id: f_00004-4-1 loss: 0.774798  [   64/  306]
train() client id: f_00004-4-2 loss: 0.771587  [   96/  306]
train() client id: f_00004-4-3 loss: 0.840098  [  128/  306]
train() client id: f_00004-4-4 loss: 0.801757  [  160/  306]
train() client id: f_00004-4-5 loss: 0.675308  [  192/  306]
train() client id: f_00004-4-6 loss: 0.921157  [  224/  306]
train() client id: f_00004-4-7 loss: 1.045906  [  256/  306]
train() client id: f_00004-4-8 loss: 0.858927  [  288/  306]
train() client id: f_00004-5-0 loss: 0.873201  [   32/  306]
train() client id: f_00004-5-1 loss: 0.794711  [   64/  306]
train() client id: f_00004-5-2 loss: 0.873658  [   96/  306]
train() client id: f_00004-5-3 loss: 0.854785  [  128/  306]
train() client id: f_00004-5-4 loss: 0.837232  [  160/  306]
train() client id: f_00004-5-5 loss: 0.855754  [  192/  306]
train() client id: f_00004-5-6 loss: 0.720013  [  224/  306]
train() client id: f_00004-5-7 loss: 0.984895  [  256/  306]
train() client id: f_00004-5-8 loss: 0.825435  [  288/  306]
train() client id: f_00004-6-0 loss: 0.768357  [   32/  306]
train() client id: f_00004-6-1 loss: 0.678403  [   64/  306]
train() client id: f_00004-6-2 loss: 1.006854  [   96/  306]
train() client id: f_00004-6-3 loss: 0.913853  [  128/  306]
train() client id: f_00004-6-4 loss: 0.830800  [  160/  306]
train() client id: f_00004-6-5 loss: 0.899276  [  192/  306]
train() client id: f_00004-6-6 loss: 0.831730  [  224/  306]
train() client id: f_00004-6-7 loss: 0.829112  [  256/  306]
train() client id: f_00004-6-8 loss: 0.845176  [  288/  306]
train() client id: f_00004-7-0 loss: 0.916310  [   32/  306]
train() client id: f_00004-7-1 loss: 0.834712  [   64/  306]
train() client id: f_00004-7-2 loss: 0.783614  [   96/  306]
train() client id: f_00004-7-3 loss: 0.843389  [  128/  306]
train() client id: f_00004-7-4 loss: 0.800742  [  160/  306]
train() client id: f_00004-7-5 loss: 0.812576  [  192/  306]
train() client id: f_00004-7-6 loss: 0.946545  [  224/  306]
train() client id: f_00004-7-7 loss: 0.825190  [  256/  306]
train() client id: f_00004-7-8 loss: 0.839033  [  288/  306]
train() client id: f_00004-8-0 loss: 0.736776  [   32/  306]
train() client id: f_00004-8-1 loss: 0.879399  [   64/  306]
train() client id: f_00004-8-2 loss: 0.789420  [   96/  306]
train() client id: f_00004-8-3 loss: 0.942049  [  128/  306]
train() client id: f_00004-8-4 loss: 0.777533  [  160/  306]
train() client id: f_00004-8-5 loss: 0.995490  [  192/  306]
train() client id: f_00004-8-6 loss: 0.852758  [  224/  306]
train() client id: f_00004-8-7 loss: 0.811112  [  256/  306]
train() client id: f_00004-8-8 loss: 0.763824  [  288/  306]
train() client id: f_00004-9-0 loss: 0.965303  [   32/  306]
train() client id: f_00004-9-1 loss: 0.907858  [   64/  306]
train() client id: f_00004-9-2 loss: 0.774946  [   96/  306]
train() client id: f_00004-9-3 loss: 0.753336  [  128/  306]
train() client id: f_00004-9-4 loss: 0.705403  [  160/  306]
train() client id: f_00004-9-5 loss: 0.825915  [  192/  306]
train() client id: f_00004-9-6 loss: 0.878941  [  224/  306]
train() client id: f_00004-9-7 loss: 0.853166  [  256/  306]
train() client id: f_00004-9-8 loss: 0.937130  [  288/  306]
train() client id: f_00005-0-0 loss: 0.547746  [   32/  146]
train() client id: f_00005-0-1 loss: 0.173025  [   64/  146]
train() client id: f_00005-0-2 loss: 0.829766  [   96/  146]
train() client id: f_00005-0-3 loss: 0.508320  [  128/  146]
train() client id: f_00005-1-0 loss: 0.472304  [   32/  146]
train() client id: f_00005-1-1 loss: 0.716868  [   64/  146]
train() client id: f_00005-1-2 loss: 0.494981  [   96/  146]
train() client id: f_00005-1-3 loss: 0.471567  [  128/  146]
train() client id: f_00005-2-0 loss: 0.419554  [   32/  146]
train() client id: f_00005-2-1 loss: 0.432803  [   64/  146]
train() client id: f_00005-2-2 loss: 0.755657  [   96/  146]
train() client id: f_00005-2-3 loss: 0.380882  [  128/  146]
train() client id: f_00005-3-0 loss: 0.372803  [   32/  146]
train() client id: f_00005-3-1 loss: 0.778736  [   64/  146]
train() client id: f_00005-3-2 loss: 0.270586  [   96/  146]
train() client id: f_00005-3-3 loss: 0.286098  [  128/  146]
train() client id: f_00005-4-0 loss: 0.276499  [   32/  146]
train() client id: f_00005-4-1 loss: 0.467310  [   64/  146]
train() client id: f_00005-4-2 loss: 0.698674  [   96/  146]
train() client id: f_00005-4-3 loss: 0.509482  [  128/  146]
train() client id: f_00005-5-0 loss: 0.533397  [   32/  146]
train() client id: f_00005-5-1 loss: 0.314232  [   64/  146]
train() client id: f_00005-5-2 loss: 0.657734  [   96/  146]
train() client id: f_00005-5-3 loss: 0.602942  [  128/  146]
train() client id: f_00005-6-0 loss: 0.411164  [   32/  146]
train() client id: f_00005-6-1 loss: 0.656047  [   64/  146]
train() client id: f_00005-6-2 loss: 0.624461  [   96/  146]
train() client id: f_00005-6-3 loss: 0.255477  [  128/  146]
train() client id: f_00005-7-0 loss: 0.227761  [   32/  146]
train() client id: f_00005-7-1 loss: 0.682667  [   64/  146]
train() client id: f_00005-7-2 loss: 0.692950  [   96/  146]
train() client id: f_00005-7-3 loss: 0.358885  [  128/  146]
train() client id: f_00005-8-0 loss: 0.522479  [   32/  146]
train() client id: f_00005-8-1 loss: 0.332183  [   64/  146]
train() client id: f_00005-8-2 loss: 0.577071  [   96/  146]
train() client id: f_00005-8-3 loss: 0.460469  [  128/  146]
train() client id: f_00005-9-0 loss: 0.459287  [   32/  146]
train() client id: f_00005-9-1 loss: 0.456726  [   64/  146]
train() client id: f_00005-9-2 loss: 0.579118  [   96/  146]
train() client id: f_00005-9-3 loss: 0.556368  [  128/  146]
train() client id: f_00006-0-0 loss: 0.517738  [   32/   54]
train() client id: f_00006-1-0 loss: 0.535019  [   32/   54]
train() client id: f_00006-2-0 loss: 0.582365  [   32/   54]
train() client id: f_00006-3-0 loss: 0.544021  [   32/   54]
train() client id: f_00006-4-0 loss: 0.531704  [   32/   54]
train() client id: f_00006-5-0 loss: 0.573906  [   32/   54]
train() client id: f_00006-6-0 loss: 0.493273  [   32/   54]
train() client id: f_00006-7-0 loss: 0.574655  [   32/   54]
train() client id: f_00006-8-0 loss: 0.515308  [   32/   54]
train() client id: f_00006-9-0 loss: 0.518206  [   32/   54]
train() client id: f_00007-0-0 loss: 0.449128  [   32/  179]
train() client id: f_00007-0-1 loss: 0.389471  [   64/  179]
train() client id: f_00007-0-2 loss: 0.396825  [   96/  179]
train() client id: f_00007-0-3 loss: 0.547718  [  128/  179]
train() client id: f_00007-0-4 loss: 0.761710  [  160/  179]
train() client id: f_00007-1-0 loss: 0.609942  [   32/  179]
train() client id: f_00007-1-1 loss: 0.624074  [   64/  179]
train() client id: f_00007-1-2 loss: 0.485153  [   96/  179]
train() client id: f_00007-1-3 loss: 0.445946  [  128/  179]
train() client id: f_00007-1-4 loss: 0.596843  [  160/  179]
train() client id: f_00007-2-0 loss: 0.491959  [   32/  179]
train() client id: f_00007-2-1 loss: 0.611246  [   64/  179]
train() client id: f_00007-2-2 loss: 0.344403  [   96/  179]
train() client id: f_00007-2-3 loss: 0.592400  [  128/  179]
train() client id: f_00007-2-4 loss: 0.666752  [  160/  179]
train() client id: f_00007-3-0 loss: 0.487044  [   32/  179]
train() client id: f_00007-3-1 loss: 0.527046  [   64/  179]
train() client id: f_00007-3-2 loss: 0.498308  [   96/  179]
train() client id: f_00007-3-3 loss: 0.461327  [  128/  179]
train() client id: f_00007-3-4 loss: 0.439953  [  160/  179]
train() client id: f_00007-4-0 loss: 0.696816  [   32/  179]
train() client id: f_00007-4-1 loss: 0.423511  [   64/  179]
train() client id: f_00007-4-2 loss: 0.408123  [   96/  179]
train() client id: f_00007-4-3 loss: 0.475611  [  128/  179]
train() client id: f_00007-4-4 loss: 0.498940  [  160/  179]
train() client id: f_00007-5-0 loss: 0.487686  [   32/  179]
train() client id: f_00007-5-1 loss: 0.465359  [   64/  179]
train() client id: f_00007-5-2 loss: 0.469749  [   96/  179]
train() client id: f_00007-5-3 loss: 0.586415  [  128/  179]
train() client id: f_00007-5-4 loss: 0.438103  [  160/  179]
train() client id: f_00007-6-0 loss: 0.549586  [   32/  179]
train() client id: f_00007-6-1 loss: 0.569723  [   64/  179]
train() client id: f_00007-6-2 loss: 0.413716  [   96/  179]
train() client id: f_00007-6-3 loss: 0.664887  [  128/  179]
train() client id: f_00007-6-4 loss: 0.447424  [  160/  179]
train() client id: f_00007-7-0 loss: 0.515859  [   32/  179]
train() client id: f_00007-7-1 loss: 0.501790  [   64/  179]
train() client id: f_00007-7-2 loss: 0.515339  [   96/  179]
train() client id: f_00007-7-3 loss: 0.494216  [  128/  179]
train() client id: f_00007-7-4 loss: 0.602202  [  160/  179]
train() client id: f_00007-8-0 loss: 0.651633  [   32/  179]
train() client id: f_00007-8-1 loss: 0.441866  [   64/  179]
train() client id: f_00007-8-2 loss: 0.444598  [   96/  179]
train() client id: f_00007-8-3 loss: 0.455466  [  128/  179]
train() client id: f_00007-8-4 loss: 0.471583  [  160/  179]
train() client id: f_00007-9-0 loss: 0.520360  [   32/  179]
train() client id: f_00007-9-1 loss: 0.637389  [   64/  179]
train() client id: f_00007-9-2 loss: 0.449029  [   96/  179]
train() client id: f_00007-9-3 loss: 0.440328  [  128/  179]
train() client id: f_00007-9-4 loss: 0.521641  [  160/  179]
train() client id: f_00008-0-0 loss: 0.667046  [   32/  130]
train() client id: f_00008-0-1 loss: 0.837427  [   64/  130]
train() client id: f_00008-0-2 loss: 0.826463  [   96/  130]
train() client id: f_00008-0-3 loss: 0.688102  [  128/  130]
train() client id: f_00008-1-0 loss: 0.735286  [   32/  130]
train() client id: f_00008-1-1 loss: 0.806619  [   64/  130]
train() client id: f_00008-1-2 loss: 0.707870  [   96/  130]
train() client id: f_00008-1-3 loss: 0.845637  [  128/  130]
train() client id: f_00008-2-0 loss: 0.768391  [   32/  130]
train() client id: f_00008-2-1 loss: 0.736775  [   64/  130]
train() client id: f_00008-2-2 loss: 0.871245  [   96/  130]
train() client id: f_00008-2-3 loss: 0.707811  [  128/  130]
train() client id: f_00008-3-0 loss: 0.739840  [   32/  130]
train() client id: f_00008-3-1 loss: 0.793907  [   64/  130]
train() client id: f_00008-3-2 loss: 0.705663  [   96/  130]
train() client id: f_00008-3-3 loss: 0.849190  [  128/  130]
train() client id: f_00008-4-0 loss: 0.820973  [   32/  130]
train() client id: f_00008-4-1 loss: 0.763943  [   64/  130]
train() client id: f_00008-4-2 loss: 0.723064  [   96/  130]
train() client id: f_00008-4-3 loss: 0.771846  [  128/  130]
train() client id: f_00008-5-0 loss: 0.816390  [   32/  130]
train() client id: f_00008-5-1 loss: 0.749504  [   64/  130]
train() client id: f_00008-5-2 loss: 0.718750  [   96/  130]
train() client id: f_00008-5-3 loss: 0.782808  [  128/  130]
train() client id: f_00008-6-0 loss: 0.720104  [   32/  130]
train() client id: f_00008-6-1 loss: 0.755231  [   64/  130]
train() client id: f_00008-6-2 loss: 0.804454  [   96/  130]
train() client id: f_00008-6-3 loss: 0.789500  [  128/  130]
train() client id: f_00008-7-0 loss: 0.677875  [   32/  130]
train() client id: f_00008-7-1 loss: 0.844826  [   64/  130]
train() client id: f_00008-7-2 loss: 0.782767  [   96/  130]
train() client id: f_00008-7-3 loss: 0.755289  [  128/  130]
train() client id: f_00008-8-0 loss: 0.828833  [   32/  130]
train() client id: f_00008-8-1 loss: 0.809721  [   64/  130]
train() client id: f_00008-8-2 loss: 0.775563  [   96/  130]
train() client id: f_00008-8-3 loss: 0.612352  [  128/  130]
train() client id: f_00008-9-0 loss: 0.696198  [   32/  130]
train() client id: f_00008-9-1 loss: 0.837834  [   64/  130]
train() client id: f_00008-9-2 loss: 0.745672  [   96/  130]
train() client id: f_00008-9-3 loss: 0.705707  [  128/  130]
train() client id: f_00009-0-0 loss: 1.061836  [   32/  118]
train() client id: f_00009-0-1 loss: 0.969378  [   64/  118]
train() client id: f_00009-0-2 loss: 0.961558  [   96/  118]
train() client id: f_00009-1-0 loss: 0.926174  [   32/  118]
train() client id: f_00009-1-1 loss: 1.035962  [   64/  118]
train() client id: f_00009-1-2 loss: 1.043840  [   96/  118]
train() client id: f_00009-2-0 loss: 0.867996  [   32/  118]
train() client id: f_00009-2-1 loss: 1.142593  [   64/  118]
train() client id: f_00009-2-2 loss: 0.839262  [   96/  118]
train() client id: f_00009-3-0 loss: 0.966464  [   32/  118]
train() client id: f_00009-3-1 loss: 1.065740  [   64/  118]
train() client id: f_00009-3-2 loss: 0.872804  [   96/  118]
train() client id: f_00009-4-0 loss: 1.085789  [   32/  118]
train() client id: f_00009-4-1 loss: 0.787157  [   64/  118]
train() client id: f_00009-4-2 loss: 0.971328  [   96/  118]
train() client id: f_00009-5-0 loss: 0.972199  [   32/  118]
train() client id: f_00009-5-1 loss: 0.967452  [   64/  118]
train() client id: f_00009-5-2 loss: 0.829159  [   96/  118]
train() client id: f_00009-6-0 loss: 0.888599  [   32/  118]
train() client id: f_00009-6-1 loss: 0.867041  [   64/  118]
train() client id: f_00009-6-2 loss: 0.920240  [   96/  118]
train() client id: f_00009-7-0 loss: 1.067449  [   32/  118]
train() client id: f_00009-7-1 loss: 0.777715  [   64/  118]
train() client id: f_00009-7-2 loss: 0.871613  [   96/  118]
train() client id: f_00009-8-0 loss: 0.910326  [   32/  118]
train() client id: f_00009-8-1 loss: 0.774687  [   64/  118]
train() client id: f_00009-8-2 loss: 0.979971  [   96/  118]
train() client id: f_00009-9-0 loss: 0.972604  [   32/  118]
train() client id: f_00009-9-1 loss: 0.859875  [   64/  118]
train() client id: f_00009-9-2 loss: 0.859889  [   96/  118]
At round 63 accuracy: 0.6472148541114059
At round 63 training accuracy: 0.5915492957746479
At round 63 training loss: 0.8229022369204305
update_location
xs = [  -3.9056584     4.20031788  335.00902392   18.81129433    0.97929623
    3.95640986 -297.44319194 -276.32485185  319.66397685 -262.06087855]
ys = [ 327.5879595   310.55583871    1.32061395 -297.45517586  289.35018685
  272.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [342.53339308 326.28602732 349.61806322 314.37787206 306.14455678
 290.59114581 313.81418542 293.86408424 335.40084685 280.5207941 ]
dists_bs = [229.25173742 224.69565936 538.77057605 510.6630112  209.86823946
 203.94363122 215.73819853 201.50578171 519.11171038 191.88561647]
uav_gains = [1.33357588e-12 1.68888635e-12 1.21580973e-12 2.05498399e-12
 2.38086830e-12 3.21721303e-12 2.07518341e-12 3.01317070e-12
 1.47290941e-12 3.95629760e-12]
bs_gains = [2.71905946e-11 2.87626551e-11 2.48521091e-12 2.88747186e-12
 3.48211629e-11 3.77281641e-11 3.22328236e-11 3.90201606e-11
 2.75780638e-12 4.47481726e-11]
Round 64
-------------------------------
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.55289596 5.10473648 2.52232289 0.94134025 5.8844195  2.83245373
 1.15055847 3.5161317  2.58559463 2.29678183]
obj_prev = 29.387235422694697
eta_min = 3.6403482068472025e-37	eta_max = 0.941453835955468
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 6.723535975581212	eta = 0.909090909090909
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 16.80493378903063	eta = 0.36372088751318643
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 11.200979803524296	eta = 0.5456938178232738
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.229583039555175	eta = 0.5975126658351408
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168941452928603	eta = 0.601075879986136
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671162537121	eta = 0.6010918570034193
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671157124352	eta = 0.6010918573233796
eta = 0.6010918573233796
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.04200208 0.08833771 0.04133538 0.01433404 0.10200505 0.04866908
 0.01800089 0.05966962 0.04333548 0.03933528]
ene_total = [1.03172726 1.51359867 1.04004204 0.52610415 1.72269629 0.88362426
 0.58229031 1.19121994 0.94449752 0.7328707 ]
ti_comp = [1.47039382 1.62666926 1.45842809 1.51593046 1.63009164 1.63144497
 1.51679335 1.54542978 1.53931646 1.63417731]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [2.14202768e-06 1.62824548e-05 2.07527568e-06 8.00990907e-08
 2.49643934e-05 2.70703886e-06 1.58456196e-07 5.55957146e-06
 2.14661920e-06 1.42438847e-06]
ene_total = [0.37266401 0.1276584  0.3914396  0.30117553 0.12242423 0.11995133
 0.29982271 0.25497116 0.26451064 0.11564362]
optimize_network iter = 0 obj = 2.3702612355707346
eta = 0.6010918573233796
freqs = [14282595.45433634 27152940.78877741 14171209.70092077  4727803.67422734
 31288134.07679844 14915942.00582812  5933864.17259814 19305186.95139418
 14076208.69920771 12035193.86722137]
eta_min = 0.6010918573233806	eta_max = 0.7423534842131878
af = 0.0007076189579367038	bf = 0.9451563237723394	zeta = 0.0007783808537303743	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [4.01088465e-07 3.04884240e-06 3.88589348e-07 1.49983222e-08
 4.67451020e-06 5.06885170e-07 2.96704627e-08 1.04101362e-06
 4.01948215e-07 2.66712607e-07]
ene_total = [1.63512592 0.5592525  1.71751674 1.3215506  0.53579916 0.52619371
 1.31561006 1.11849968 1.16054979 0.50736327]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 1 obj = 3.6691462360745932
eta = 0.7423534842131878
freqs = [14200721.70636506 25299122.79358197 14171209.70092077  4604073.01677737
 29115814.90637482 13873547.37129865  5776388.64214124 18564736.76689159
 13570968.78990086 11183104.37494726]
eta_min = 0.7423534842131893	eta_max = 0.7423534842131851
af = 0.0006257175094953287	bf = 0.9451563237723394	zeta = 0.0006882892604448616	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [3.96503235e-07 2.64674534e-06 3.88589348e-07 1.42235570e-08
 4.04794575e-06 4.38513790e-07 2.81165434e-08 9.62688933e-07
 3.73611652e-07 2.30283129e-07]
ene_total = [1.63512561 0.55922482 1.71751674 1.32155055 0.53575602 0.526189
 1.31560995 1.11849429 1.16054784 0.50736076]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 2 obj = 3.669146236074556
eta = 0.7423534842131851
freqs = [14200721.70636504 25299122.79358199 14171209.70092075  4604073.01677736
 29115814.90637484 13873547.37129866  5776388.64214123 18564736.76689158
 13570968.78990086 11183104.37494727]
Done!
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.14341502e-06 7.63254395e-06 1.12059337e-06 4.10171399e-08
 1.16732514e-05 1.26456283e-06 8.10809978e-08 2.77615133e-06
 1.07740148e-06 6.64078283e-07]
ene_total = [0.02374766 0.0081266  0.02494421 0.01919289 0.0077884  0.00764266
 0.01910664 0.01624569 0.01685533 0.00736883]
At round 64 energy consumption: 0.15101890456706119
At round 64 eta: 0.7423534842131851
At round 64 a_n: 6.259668650218071
At round 64 local rounds: 9.755733857653984
At round 64 global rounds: 24.295568799375985
gradient difference: 0.6207090020179749
train() client id: f_00000-0-0 loss: 1.076373  [   32/  126]
train() client id: f_00000-0-1 loss: 0.992093  [   64/  126]
train() client id: f_00000-0-2 loss: 0.952514  [   96/  126]
train() client id: f_00000-1-0 loss: 0.818894  [   32/  126]
train() client id: f_00000-1-1 loss: 1.212722  [   64/  126]
train() client id: f_00000-1-2 loss: 0.993760  [   96/  126]
train() client id: f_00000-2-0 loss: 0.972996  [   32/  126]
train() client id: f_00000-2-1 loss: 0.965051  [   64/  126]
train() client id: f_00000-2-2 loss: 0.973455  [   96/  126]
train() client id: f_00000-3-0 loss: 0.846668  [   32/  126]
train() client id: f_00000-3-1 loss: 1.047317  [   64/  126]
train() client id: f_00000-3-2 loss: 0.851885  [   96/  126]
train() client id: f_00000-4-0 loss: 1.049612  [   32/  126]
train() client id: f_00000-4-1 loss: 0.802435  [   64/  126]
train() client id: f_00000-4-2 loss: 0.767048  [   96/  126]
train() client id: f_00000-5-0 loss: 0.886477  [   32/  126]
train() client id: f_00000-5-1 loss: 0.848040  [   64/  126]
train() client id: f_00000-5-2 loss: 0.886102  [   96/  126]
train() client id: f_00000-6-0 loss: 0.823891  [   32/  126]
train() client id: f_00000-6-1 loss: 0.970689  [   64/  126]
train() client id: f_00000-6-2 loss: 0.799387  [   96/  126]
train() client id: f_00000-7-0 loss: 0.808189  [   32/  126]
train() client id: f_00000-7-1 loss: 0.856499  [   64/  126]
train() client id: f_00000-7-2 loss: 0.828246  [   96/  126]
train() client id: f_00000-8-0 loss: 0.996472  [   32/  126]
train() client id: f_00000-8-1 loss: 0.804196  [   64/  126]
train() client id: f_00000-8-2 loss: 0.767442  [   96/  126]
train() client id: f_00001-0-0 loss: 0.363132  [   32/  265]
train() client id: f_00001-0-1 loss: 0.448552  [   64/  265]
train() client id: f_00001-0-2 loss: 0.430611  [   96/  265]
train() client id: f_00001-0-3 loss: 0.460174  [  128/  265]
train() client id: f_00001-0-4 loss: 0.464098  [  160/  265]
train() client id: f_00001-0-5 loss: 0.386605  [  192/  265]
train() client id: f_00001-0-6 loss: 0.444722  [  224/  265]
train() client id: f_00001-0-7 loss: 0.450020  [  256/  265]
train() client id: f_00001-1-0 loss: 0.462264  [   32/  265]
train() client id: f_00001-1-1 loss: 0.566387  [   64/  265]
train() client id: f_00001-1-2 loss: 0.457896  [   96/  265]
train() client id: f_00001-1-3 loss: 0.420003  [  128/  265]
train() client id: f_00001-1-4 loss: 0.358391  [  160/  265]
train() client id: f_00001-1-5 loss: 0.328171  [  192/  265]
train() client id: f_00001-1-6 loss: 0.381252  [  224/  265]
train() client id: f_00001-1-7 loss: 0.390166  [  256/  265]
train() client id: f_00001-2-0 loss: 0.487281  [   32/  265]
train() client id: f_00001-2-1 loss: 0.448187  [   64/  265]
train() client id: f_00001-2-2 loss: 0.382140  [   96/  265]
train() client id: f_00001-2-3 loss: 0.459854  [  128/  265]
train() client id: f_00001-2-4 loss: 0.382021  [  160/  265]
train() client id: f_00001-2-5 loss: 0.330484  [  192/  265]
train() client id: f_00001-2-6 loss: 0.434258  [  224/  265]
train() client id: f_00001-2-7 loss: 0.393701  [  256/  265]
train() client id: f_00001-3-0 loss: 0.338956  [   32/  265]
train() client id: f_00001-3-1 loss: 0.397124  [   64/  265]
train() client id: f_00001-3-2 loss: 0.323800  [   96/  265]
train() client id: f_00001-3-3 loss: 0.356292  [  128/  265]
train() client id: f_00001-3-4 loss: 0.456027  [  160/  265]
train() client id: f_00001-3-5 loss: 0.442832  [  192/  265]
train() client id: f_00001-3-6 loss: 0.507629  [  224/  265]
train() client id: f_00001-3-7 loss: 0.443568  [  256/  265]
train() client id: f_00001-4-0 loss: 0.377713  [   32/  265]
train() client id: f_00001-4-1 loss: 0.507604  [   64/  265]
train() client id: f_00001-4-2 loss: 0.379621  [   96/  265]
train() client id: f_00001-4-3 loss: 0.315577  [  128/  265]
train() client id: f_00001-4-4 loss: 0.411939  [  160/  265]
train() client id: f_00001-4-5 loss: 0.461910  [  192/  265]
train() client id: f_00001-4-6 loss: 0.448508  [  224/  265]
train() client id: f_00001-4-7 loss: 0.323167  [  256/  265]
train() client id: f_00001-5-0 loss: 0.388361  [   32/  265]
train() client id: f_00001-5-1 loss: 0.465272  [   64/  265]
train() client id: f_00001-5-2 loss: 0.313798  [   96/  265]
train() client id: f_00001-5-3 loss: 0.358250  [  128/  265]
train() client id: f_00001-5-4 loss: 0.355218  [  160/  265]
train() client id: f_00001-5-5 loss: 0.342839  [  192/  265]
train() client id: f_00001-5-6 loss: 0.600235  [  224/  265]
train() client id: f_00001-5-7 loss: 0.393960  [  256/  265]
train() client id: f_00001-6-0 loss: 0.418213  [   32/  265]
train() client id: f_00001-6-1 loss: 0.351960  [   64/  265]
train() client id: f_00001-6-2 loss: 0.483675  [   96/  265]
train() client id: f_00001-6-3 loss: 0.332258  [  128/  265]
train() client id: f_00001-6-4 loss: 0.347665  [  160/  265]
train() client id: f_00001-6-5 loss: 0.459946  [  192/  265]
train() client id: f_00001-6-6 loss: 0.500075  [  224/  265]
train() client id: f_00001-6-7 loss: 0.316264  [  256/  265]
train() client id: f_00001-7-0 loss: 0.310824  [   32/  265]
train() client id: f_00001-7-1 loss: 0.314571  [   64/  265]
train() client id: f_00001-7-2 loss: 0.426675  [   96/  265]
train() client id: f_00001-7-3 loss: 0.482333  [  128/  265]
train() client id: f_00001-7-4 loss: 0.342887  [  160/  265]
train() client id: f_00001-7-5 loss: 0.474457  [  192/  265]
train() client id: f_00001-7-6 loss: 0.405693  [  224/  265]
train() client id: f_00001-7-7 loss: 0.380707  [  256/  265]
train() client id: f_00001-8-0 loss: 0.547689  [   32/  265]
train() client id: f_00001-8-1 loss: 0.361759  [   64/  265]
train() client id: f_00001-8-2 loss: 0.379557  [   96/  265]
train() client id: f_00001-8-3 loss: 0.366762  [  128/  265]
train() client id: f_00001-8-4 loss: 0.445277  [  160/  265]
train() client id: f_00001-8-5 loss: 0.365203  [  192/  265]
train() client id: f_00001-8-6 loss: 0.307591  [  224/  265]
train() client id: f_00001-8-7 loss: 0.398568  [  256/  265]
train() client id: f_00002-0-0 loss: 1.116530  [   32/  124]
train() client id: f_00002-0-1 loss: 1.249414  [   64/  124]
train() client id: f_00002-0-2 loss: 1.125383  [   96/  124]
train() client id: f_00002-1-0 loss: 1.027383  [   32/  124]
train() client id: f_00002-1-1 loss: 0.986779  [   64/  124]
train() client id: f_00002-1-2 loss: 1.251207  [   96/  124]
train() client id: f_00002-2-0 loss: 1.249315  [   32/  124]
train() client id: f_00002-2-1 loss: 1.094779  [   64/  124]
train() client id: f_00002-2-2 loss: 1.040343  [   96/  124]
train() client id: f_00002-3-0 loss: 0.981961  [   32/  124]
train() client id: f_00002-3-1 loss: 1.057585  [   64/  124]
train() client id: f_00002-3-2 loss: 1.044150  [   96/  124]
train() client id: f_00002-4-0 loss: 0.922328  [   32/  124]
train() client id: f_00002-4-1 loss: 0.927768  [   64/  124]
train() client id: f_00002-4-2 loss: 1.094064  [   96/  124]
train() client id: f_00002-5-0 loss: 0.999886  [   32/  124]
train() client id: f_00002-5-1 loss: 1.054563  [   64/  124]
train() client id: f_00002-5-2 loss: 1.001639  [   96/  124]
train() client id: f_00002-6-0 loss: 0.980471  [   32/  124]
train() client id: f_00002-6-1 loss: 0.962132  [   64/  124]
train() client id: f_00002-6-2 loss: 1.090783  [   96/  124]
train() client id: f_00002-7-0 loss: 0.908447  [   32/  124]
train() client id: f_00002-7-1 loss: 0.875852  [   64/  124]
train() client id: f_00002-7-2 loss: 1.159804  [   96/  124]
train() client id: f_00002-8-0 loss: 1.040938  [   32/  124]
train() client id: f_00002-8-1 loss: 0.827623  [   64/  124]
train() client id: f_00002-8-2 loss: 1.351324  [   96/  124]
train() client id: f_00003-0-0 loss: 0.591370  [   32/   43]
train() client id: f_00003-1-0 loss: 0.621488  [   32/   43]
train() client id: f_00003-2-0 loss: 0.263329  [   32/   43]
train() client id: f_00003-3-0 loss: 0.697832  [   32/   43]
train() client id: f_00003-4-0 loss: 0.392345  [   32/   43]
train() client id: f_00003-5-0 loss: 0.426500  [   32/   43]
train() client id: f_00003-6-0 loss: 0.381582  [   32/   43]
train() client id: f_00003-7-0 loss: 0.397998  [   32/   43]
train() client id: f_00003-8-0 loss: 0.599726  [   32/   43]
train() client id: f_00004-0-0 loss: 0.890462  [   32/  306]
train() client id: f_00004-0-1 loss: 0.600927  [   64/  306]
train() client id: f_00004-0-2 loss: 0.720547  [   96/  306]
train() client id: f_00004-0-3 loss: 0.756296  [  128/  306]
train() client id: f_00004-0-4 loss: 0.699219  [  160/  306]
train() client id: f_00004-0-5 loss: 0.774988  [  192/  306]
train() client id: f_00004-0-6 loss: 0.661100  [  224/  306]
train() client id: f_00004-0-7 loss: 0.699412  [  256/  306]
train() client id: f_00004-0-8 loss: 0.747339  [  288/  306]
train() client id: f_00004-1-0 loss: 0.902226  [   32/  306]
train() client id: f_00004-1-1 loss: 0.873508  [   64/  306]
train() client id: f_00004-1-2 loss: 0.608471  [   96/  306]
train() client id: f_00004-1-3 loss: 0.810680  [  128/  306]
train() client id: f_00004-1-4 loss: 0.874311  [  160/  306]
train() client id: f_00004-1-5 loss: 0.581793  [  192/  306]
train() client id: f_00004-1-6 loss: 0.732852  [  224/  306]
train() client id: f_00004-1-7 loss: 0.622278  [  256/  306]
train() client id: f_00004-1-8 loss: 0.571870  [  288/  306]
train() client id: f_00004-2-0 loss: 0.876843  [   32/  306]
train() client id: f_00004-2-1 loss: 0.647139  [   64/  306]
train() client id: f_00004-2-2 loss: 0.817306  [   96/  306]
train() client id: f_00004-2-3 loss: 0.612826  [  128/  306]
train() client id: f_00004-2-4 loss: 0.810299  [  160/  306]
train() client id: f_00004-2-5 loss: 0.710315  [  192/  306]
train() client id: f_00004-2-6 loss: 0.649726  [  224/  306]
train() client id: f_00004-2-7 loss: 0.697542  [  256/  306]
train() client id: f_00004-2-8 loss: 0.719039  [  288/  306]
train() client id: f_00004-3-0 loss: 0.777054  [   32/  306]
train() client id: f_00004-3-1 loss: 0.621009  [   64/  306]
train() client id: f_00004-3-2 loss: 0.838798  [   96/  306]
train() client id: f_00004-3-3 loss: 0.751646  [  128/  306]
train() client id: f_00004-3-4 loss: 0.831231  [  160/  306]
train() client id: f_00004-3-5 loss: 0.640470  [  192/  306]
train() client id: f_00004-3-6 loss: 0.706619  [  224/  306]
train() client id: f_00004-3-7 loss: 0.654044  [  256/  306]
train() client id: f_00004-3-8 loss: 0.748607  [  288/  306]
train() client id: f_00004-4-0 loss: 0.832192  [   32/  306]
train() client id: f_00004-4-1 loss: 0.612863  [   64/  306]
train() client id: f_00004-4-2 loss: 0.807695  [   96/  306]
train() client id: f_00004-4-3 loss: 0.808827  [  128/  306]
train() client id: f_00004-4-4 loss: 0.738973  [  160/  306]
train() client id: f_00004-4-5 loss: 0.642896  [  192/  306]
train() client id: f_00004-4-6 loss: 0.703332  [  224/  306]
train() client id: f_00004-4-7 loss: 0.682622  [  256/  306]
train() client id: f_00004-4-8 loss: 0.750872  [  288/  306]
train() client id: f_00004-5-0 loss: 0.716507  [   32/  306]
train() client id: f_00004-5-1 loss: 0.678969  [   64/  306]
train() client id: f_00004-5-2 loss: 0.748289  [   96/  306]
train() client id: f_00004-5-3 loss: 0.717566  [  128/  306]
train() client id: f_00004-5-4 loss: 0.855058  [  160/  306]
train() client id: f_00004-5-5 loss: 0.661063  [  192/  306]
train() client id: f_00004-5-6 loss: 0.730888  [  224/  306]
train() client id: f_00004-5-7 loss: 0.689839  [  256/  306]
train() client id: f_00004-5-8 loss: 0.830806  [  288/  306]
train() client id: f_00004-6-0 loss: 0.628124  [   32/  306]
train() client id: f_00004-6-1 loss: 0.764642  [   64/  306]
train() client id: f_00004-6-2 loss: 0.783266  [   96/  306]
train() client id: f_00004-6-3 loss: 0.729492  [  128/  306]
train() client id: f_00004-6-4 loss: 0.820548  [  160/  306]
train() client id: f_00004-6-5 loss: 0.751992  [  192/  306]
train() client id: f_00004-6-6 loss: 0.736693  [  224/  306]
train() client id: f_00004-6-7 loss: 0.735723  [  256/  306]
train() client id: f_00004-6-8 loss: 0.661424  [  288/  306]
train() client id: f_00004-7-0 loss: 0.801394  [   32/  306]
train() client id: f_00004-7-1 loss: 0.655105  [   64/  306]
train() client id: f_00004-7-2 loss: 0.802950  [   96/  306]
train() client id: f_00004-7-3 loss: 0.815150  [  128/  306]
train() client id: f_00004-7-4 loss: 0.683267  [  160/  306]
train() client id: f_00004-7-5 loss: 0.750769  [  192/  306]
train() client id: f_00004-7-6 loss: 0.712108  [  224/  306]
train() client id: f_00004-7-7 loss: 0.720100  [  256/  306]
train() client id: f_00004-7-8 loss: 0.699003  [  288/  306]
train() client id: f_00004-8-0 loss: 0.840115  [   32/  306]
train() client id: f_00004-8-1 loss: 0.824371  [   64/  306]
train() client id: f_00004-8-2 loss: 0.793522  [   96/  306]
train() client id: f_00004-8-3 loss: 0.680931  [  128/  306]
train() client id: f_00004-8-4 loss: 0.583042  [  160/  306]
train() client id: f_00004-8-5 loss: 0.767539  [  192/  306]
train() client id: f_00004-8-6 loss: 0.666112  [  224/  306]
train() client id: f_00004-8-7 loss: 0.745387  [  256/  306]
train() client id: f_00004-8-8 loss: 0.748287  [  288/  306]
train() client id: f_00005-0-0 loss: 0.516452  [   32/  146]
train() client id: f_00005-0-1 loss: 0.570755  [   64/  146]
train() client id: f_00005-0-2 loss: 0.454136  [   96/  146]
train() client id: f_00005-0-3 loss: 0.722465  [  128/  146]
train() client id: f_00005-1-0 loss: 0.349551  [   32/  146]
train() client id: f_00005-1-1 loss: 0.754358  [   64/  146]
train() client id: f_00005-1-2 loss: 0.715711  [   96/  146]
train() client id: f_00005-1-3 loss: 0.523568  [  128/  146]
train() client id: f_00005-2-0 loss: 0.610124  [   32/  146]
train() client id: f_00005-2-1 loss: 0.717834  [   64/  146]
train() client id: f_00005-2-2 loss: 0.646366  [   96/  146]
train() client id: f_00005-2-3 loss: 0.586028  [  128/  146]
train() client id: f_00005-3-0 loss: 0.401918  [   32/  146]
train() client id: f_00005-3-1 loss: 0.758730  [   64/  146]
train() client id: f_00005-3-2 loss: 0.712052  [   96/  146]
train() client id: f_00005-3-3 loss: 0.335434  [  128/  146]
train() client id: f_00005-4-0 loss: 0.377224  [   32/  146]
train() client id: f_00005-4-1 loss: 0.433101  [   64/  146]
train() client id: f_00005-4-2 loss: 0.544107  [   96/  146]
train() client id: f_00005-4-3 loss: 0.697875  [  128/  146]
train() client id: f_00005-5-0 loss: 0.651036  [   32/  146]
train() client id: f_00005-5-1 loss: 0.996551  [   64/  146]
train() client id: f_00005-5-2 loss: 0.362136  [   96/  146]
train() client id: f_00005-5-3 loss: 0.489266  [  128/  146]
train() client id: f_00005-6-0 loss: 0.484065  [   32/  146]
train() client id: f_00005-6-1 loss: 0.570192  [   64/  146]
train() client id: f_00005-6-2 loss: 0.531552  [   96/  146]
train() client id: f_00005-6-3 loss: 0.738945  [  128/  146]
train() client id: f_00005-7-0 loss: 0.618732  [   32/  146]
train() client id: f_00005-7-1 loss: 0.433793  [   64/  146]
train() client id: f_00005-7-2 loss: 0.401983  [   96/  146]
train() client id: f_00005-7-3 loss: 0.962629  [  128/  146]
train() client id: f_00005-8-0 loss: 0.254912  [   32/  146]
train() client id: f_00005-8-1 loss: 0.559340  [   64/  146]
train() client id: f_00005-8-2 loss: 0.734379  [   96/  146]
train() client id: f_00005-8-3 loss: 0.693207  [  128/  146]
train() client id: f_00006-0-0 loss: 0.541376  [   32/   54]
train() client id: f_00006-1-0 loss: 0.573138  [   32/   54]
train() client id: f_00006-2-0 loss: 0.553780  [   32/   54]
train() client id: f_00006-3-0 loss: 0.471636  [   32/   54]
train() client id: f_00006-4-0 loss: 0.551253  [   32/   54]
train() client id: f_00006-5-0 loss: 0.559993  [   32/   54]
train() client id: f_00006-6-0 loss: 0.526336  [   32/   54]
train() client id: f_00006-7-0 loss: 0.567665  [   32/   54]
train() client id: f_00006-8-0 loss: 0.545294  [   32/   54]
train() client id: f_00007-0-0 loss: 0.655103  [   32/  179]
train() client id: f_00007-0-1 loss: 0.537397  [   64/  179]
train() client id: f_00007-0-2 loss: 0.543714  [   96/  179]
train() client id: f_00007-0-3 loss: 0.452924  [  128/  179]
train() client id: f_00007-0-4 loss: 0.499910  [  160/  179]
train() client id: f_00007-1-0 loss: 0.432397  [   32/  179]
train() client id: f_00007-1-1 loss: 0.690113  [   64/  179]
train() client id: f_00007-1-2 loss: 0.677822  [   96/  179]
train() client id: f_00007-1-3 loss: 0.639501  [  128/  179]
train() client id: f_00007-1-4 loss: 0.398641  [  160/  179]
train() client id: f_00007-2-0 loss: 0.546256  [   32/  179]
train() client id: f_00007-2-1 loss: 0.735972  [   64/  179]
train() client id: f_00007-2-2 loss: 0.386002  [   96/  179]
train() client id: f_00007-2-3 loss: 0.577470  [  128/  179]
train() client id: f_00007-2-4 loss: 0.378000  [  160/  179]
train() client id: f_00007-3-0 loss: 0.644726  [   32/  179]
train() client id: f_00007-3-1 loss: 0.469201  [   64/  179]
train() client id: f_00007-3-2 loss: 0.427890  [   96/  179]
train() client id: f_00007-3-3 loss: 0.651591  [  128/  179]
train() client id: f_00007-3-4 loss: 0.572271  [  160/  179]
train() client id: f_00007-4-0 loss: 0.502754  [   32/  179]
train() client id: f_00007-4-1 loss: 0.643099  [   64/  179]
train() client id: f_00007-4-2 loss: 0.441040  [   96/  179]
train() client id: f_00007-4-3 loss: 0.366691  [  128/  179]
train() client id: f_00007-4-4 loss: 0.706453  [  160/  179]
train() client id: f_00007-5-0 loss: 0.597215  [   32/  179]
train() client id: f_00007-5-1 loss: 0.662188  [   64/  179]
train() client id: f_00007-5-2 loss: 0.399076  [   96/  179]
train() client id: f_00007-5-3 loss: 0.392567  [  128/  179]
train() client id: f_00007-5-4 loss: 0.683322  [  160/  179]
train() client id: f_00007-6-0 loss: 0.508363  [   32/  179]
train() client id: f_00007-6-1 loss: 0.570003  [   64/  179]
train() client id: f_00007-6-2 loss: 0.486268  [   96/  179]
train() client id: f_00007-6-3 loss: 0.670092  [  128/  179]
train() client id: f_00007-6-4 loss: 0.508734  [  160/  179]
train() client id: f_00007-7-0 loss: 0.562782  [   32/  179]
train() client id: f_00007-7-1 loss: 0.363884  [   64/  179]
train() client id: f_00007-7-2 loss: 0.555691  [   96/  179]
train() client id: f_00007-7-3 loss: 0.399635  [  128/  179]
train() client id: f_00007-7-4 loss: 0.454205  [  160/  179]
train() client id: f_00007-8-0 loss: 0.686830  [   32/  179]
train() client id: f_00007-8-1 loss: 0.619678  [   64/  179]
train() client id: f_00007-8-2 loss: 0.551833  [   96/  179]
train() client id: f_00007-8-3 loss: 0.443365  [  128/  179]
train() client id: f_00007-8-4 loss: 0.411490  [  160/  179]
train() client id: f_00008-0-0 loss: 0.854985  [   32/  130]
train() client id: f_00008-0-1 loss: 0.642312  [   64/  130]
train() client id: f_00008-0-2 loss: 0.739691  [   96/  130]
train() client id: f_00008-0-3 loss: 0.855656  [  128/  130]
train() client id: f_00008-1-0 loss: 0.809028  [   32/  130]
train() client id: f_00008-1-1 loss: 0.652656  [   64/  130]
train() client id: f_00008-1-2 loss: 0.789912  [   96/  130]
train() client id: f_00008-1-3 loss: 0.825153  [  128/  130]
train() client id: f_00008-2-0 loss: 0.661814  [   32/  130]
train() client id: f_00008-2-1 loss: 0.695450  [   64/  130]
train() client id: f_00008-2-2 loss: 0.935632  [   96/  130]
train() client id: f_00008-2-3 loss: 0.829723  [  128/  130]
train() client id: f_00008-3-0 loss: 0.736840  [   32/  130]
train() client id: f_00008-3-1 loss: 0.720590  [   64/  130]
train() client id: f_00008-3-2 loss: 0.836306  [   96/  130]
train() client id: f_00008-3-3 loss: 0.771715  [  128/  130]
train() client id: f_00008-4-0 loss: 0.698554  [   32/  130]
train() client id: f_00008-4-1 loss: 0.854495  [   64/  130]
train() client id: f_00008-4-2 loss: 0.753038  [   96/  130]
train() client id: f_00008-4-3 loss: 0.796613  [  128/  130]
train() client id: f_00008-5-0 loss: 0.637112  [   32/  130]
train() client id: f_00008-5-1 loss: 0.836814  [   64/  130]
train() client id: f_00008-5-2 loss: 0.851520  [   96/  130]
train() client id: f_00008-5-3 loss: 0.802765  [  128/  130]
train() client id: f_00008-6-0 loss: 0.739741  [   32/  130]
train() client id: f_00008-6-1 loss: 0.776521  [   64/  130]
train() client id: f_00008-6-2 loss: 0.835667  [   96/  130]
train() client id: f_00008-6-3 loss: 0.772521  [  128/  130]
train() client id: f_00008-7-0 loss: 0.692737  [   32/  130]
train() client id: f_00008-7-1 loss: 0.718018  [   64/  130]
train() client id: f_00008-7-2 loss: 0.904284  [   96/  130]
train() client id: f_00008-7-3 loss: 0.763579  [  128/  130]
train() client id: f_00008-8-0 loss: 0.744717  [   32/  130]
train() client id: f_00008-8-1 loss: 0.873270  [   64/  130]
train() client id: f_00008-8-2 loss: 0.692029  [   96/  130]
train() client id: f_00008-8-3 loss: 0.801367  [  128/  130]
train() client id: f_00009-0-0 loss: 0.755600  [   32/  118]
train() client id: f_00009-0-1 loss: 1.103412  [   64/  118]
train() client id: f_00009-0-2 loss: 1.025665  [   96/  118]
train() client id: f_00009-1-0 loss: 1.007716  [   32/  118]
train() client id: f_00009-1-1 loss: 0.922490  [   64/  118]
train() client id: f_00009-1-2 loss: 0.820566  [   96/  118]
train() client id: f_00009-2-0 loss: 0.922727  [   32/  118]
train() client id: f_00009-2-1 loss: 0.965513  [   64/  118]
train() client id: f_00009-2-2 loss: 0.850356  [   96/  118]
train() client id: f_00009-3-0 loss: 0.896871  [   32/  118]
train() client id: f_00009-3-1 loss: 0.815286  [   64/  118]
train() client id: f_00009-3-2 loss: 0.965486  [   96/  118]
train() client id: f_00009-4-0 loss: 0.830376  [   32/  118]
train() client id: f_00009-4-1 loss: 0.897038  [   64/  118]
train() client id: f_00009-4-2 loss: 0.859248  [   96/  118]
train() client id: f_00009-5-0 loss: 0.981985  [   32/  118]
train() client id: f_00009-5-1 loss: 0.957594  [   64/  118]
train() client id: f_00009-5-2 loss: 0.709153  [   96/  118]
train() client id: f_00009-6-0 loss: 0.831353  [   32/  118]
train() client id: f_00009-6-1 loss: 1.027550  [   64/  118]
train() client id: f_00009-6-2 loss: 0.813317  [   96/  118]
train() client id: f_00009-7-0 loss: 0.923778  [   32/  118]
train() client id: f_00009-7-1 loss: 0.725324  [   64/  118]
train() client id: f_00009-7-2 loss: 0.967086  [   96/  118]
train() client id: f_00009-8-0 loss: 0.911751  [   32/  118]
train() client id: f_00009-8-1 loss: 0.792536  [   64/  118]
train() client id: f_00009-8-2 loss: 0.786080  [   96/  118]
At round 64 accuracy: 0.6472148541114059
At round 64 training accuracy: 0.5881958417169685
At round 64 training loss: 0.8332774164416221
update_location
xs = [  -3.9056584     4.20031788  340.00902392   18.81129433    0.97929623
    3.95640986 -302.44319194 -281.32485185  324.66397685 -267.06087855]
ys = [ 332.5879595   315.55583871    1.32061395 -302.45517586  294.35018685
  277.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [347.31830497 331.04853121 354.41202063 319.11282989 310.87455913
 295.29029028 318.55733376 298.57050846 340.16961628 285.19734345]
dists_bs = [232.60962728 227.74700376 543.50604767 515.29240511 212.63391028
 206.38833839 218.61473469 204.06574573 523.87842829 194.20530012]
uav_gains = [1.25202968e-12 1.57013546e-12 1.14585755e-12 1.89621938e-12
 2.18534457e-12 2.92930714e-12 1.91387845e-12 2.74742002e-12
 1.37723251e-12 3.59110754e-12]
bs_gains = [2.61057735e-11 2.76966092e-11 2.42505626e-12 2.81542271e-12
 3.35678104e-11 3.64901503e-11 3.10593012e-11 3.76649830e-11
 2.68811993e-12 4.32676259e-11]
Round 65
-------------------------------
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.41801623 4.82581662 2.38917526 0.89421619 5.56278746 2.67778749
 1.09198131 3.3275343  2.44520381 2.17140866]
obj_prev = 27.80392733965525
eta_min = 3.100306852884529e-39	eta_max = 0.942754072368237
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 6.3556060652170405	eta = 0.9090909090909091
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 16.15583264303265	eta = 0.3576308212219315
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 10.677811898063974	eta = 0.5411055889361986
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.733010612728588	eta = 0.5936317061131898
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673834452832171	eta = 0.597263031926322
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568210715082	eta = 0.5972794701806058
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568205281397	eta = 0.5972794705161003
eta = 0.5972794705161003
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.04252711 0.08944194 0.04185208 0.01451322 0.10328012 0.04927744
 0.0182259  0.0604155  0.04387718 0.03982698]
ene_total = [0.98560292 1.43449508 0.99341101 0.50602748 1.63267669 0.83712064
 0.55929955 1.13571028 0.89502304 0.69420151]
ti_comp = [1.57505947 1.73869087 1.56299493 1.62132055 1.74218994 1.74362015
 1.62219032 1.65177999 1.65020898 1.7463865 ]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.93768734e-06 1.47930885e-05 1.87549493e-06 7.26831216e-08
 2.26850246e-05 2.45991889e-06 1.43794955e-07 5.05150367e-06
 1.93873362e-06 1.29458708e-06]
ene_total = [0.36078585 0.12055314 0.37851121 0.29278761 0.11552795 0.1131294
 0.29151072 0.24810709 0.25036961 0.10904772]
optimize_network iter = 0 obj = 2.280330304694935
eta = 0.5972794705161003
freqs = [13500159.94622198 25721057.81080511 13388423.45249819  4475740.44373556
 29640890.63331935 14130785.41729783  5617683.65648641 18287997.60708139
 13294430.26761132 11402680.79036755]
eta_min = 0.5972794705161009	eta_max = 0.7502409697515174
af = 0.0005999271736932453	bf = 0.9180266431472464	zeta = 0.0006599198910625699	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.58346965e-07 2.73576559e-06 3.46845386e-07 1.34416814e-08
 4.19526387e-06 4.54926060e-07 2.65927760e-08 9.34201802e-07
 3.58540459e-07 2.39414968e-07]
ene_total = [1.59815174 0.53325669 1.67667707 1.29702355 0.51057679 0.50102436
 1.29136322 1.09882803 1.10901597 0.48300462]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 1 obj = 3.676253139763781
eta = 0.7502409697515174
freqs = [13418605.79030925 23811832.48313907 13388423.4524982   4351530.1809025
 27404358.81401067 13057508.37108007  5459605.33203652 17539930.50999551
 12759371.1105045  10525653.53083322]
eta_min = 0.7502409697515208	eta_max = 0.7502409697515149
af = 0.00052469220506583	bf = 0.9180266431472464	zeta = 0.000577161425572413	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.54030510e-07 2.34469776e-06 3.46845386e-07 1.27059696e-08
 3.58604780e-06 3.88444377e-07 2.51172226e-08 8.59338214e-07
 3.30260967e-07 2.04002507e-07]
ene_total = [1.59815146 0.53323124 1.67667707 1.2970235  0.51053714 0.50102003
 1.29136313 1.09882316 1.10901413 0.48300231]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 2 obj = 3.6762531397637437
eta = 0.7502409697515149
freqs = [13418605.79030923 23811832.48313908 13388423.45249818  4351530.1809025
 27404358.81401069 13057508.37108008  5459605.33203651 17539930.5099955
 12759371.11050449 10525653.53083322]
Done!
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.02093442e-06 6.76151514e-06 1.00021434e-06 3.66408019e-08
 1.03412546e-05 1.12017531e-06 7.24317156e-08 2.47811400e-06
 9.52389074e-07 5.88291620e-07]
ene_total = [0.02455424 0.00819684 0.02576067 0.01992715 0.00785051 0.00769827
 0.0198402  0.01688364 0.01703922 0.0074211 ]
At round 65 energy consumption: 0.15517184374296747
At round 65 eta: 0.7502409697515149
At round 65 a_n: 5.917122803248752
At round 65 local rounds: 9.409653576790653
At round 65 global rounds: 23.691326785509258
gradient difference: 0.528747022151947
train() client id: f_00000-0-0 loss: 1.293813  [   32/  126]
train() client id: f_00000-0-1 loss: 1.084196  [   64/  126]
train() client id: f_00000-0-2 loss: 1.130933  [   96/  126]
train() client id: f_00000-1-0 loss: 1.191459  [   32/  126]
train() client id: f_00000-1-1 loss: 1.096212  [   64/  126]
train() client id: f_00000-1-2 loss: 1.237310  [   96/  126]
train() client id: f_00000-2-0 loss: 1.103878  [   32/  126]
train() client id: f_00000-2-1 loss: 1.126476  [   64/  126]
train() client id: f_00000-2-2 loss: 1.082498  [   96/  126]
train() client id: f_00000-3-0 loss: 1.079040  [   32/  126]
train() client id: f_00000-3-1 loss: 0.968655  [   64/  126]
train() client id: f_00000-3-2 loss: 1.035620  [   96/  126]
train() client id: f_00000-4-0 loss: 1.006271  [   32/  126]
train() client id: f_00000-4-1 loss: 1.054720  [   64/  126]
train() client id: f_00000-4-2 loss: 0.890564  [   96/  126]
train() client id: f_00000-5-0 loss: 0.894600  [   32/  126]
train() client id: f_00000-5-1 loss: 1.072057  [   64/  126]
train() client id: f_00000-5-2 loss: 1.025669  [   96/  126]
train() client id: f_00000-6-0 loss: 0.983810  [   32/  126]
train() client id: f_00000-6-1 loss: 0.972118  [   64/  126]
train() client id: f_00000-6-2 loss: 0.919246  [   96/  126]
train() client id: f_00000-7-0 loss: 0.896288  [   32/  126]
train() client id: f_00000-7-1 loss: 0.893039  [   64/  126]
train() client id: f_00000-7-2 loss: 1.131618  [   96/  126]
train() client id: f_00000-8-0 loss: 0.894251  [   32/  126]
train() client id: f_00000-8-1 loss: 1.004026  [   64/  126]
train() client id: f_00000-8-2 loss: 0.940862  [   96/  126]
train() client id: f_00001-0-0 loss: 0.539354  [   32/  265]
train() client id: f_00001-0-1 loss: 0.435200  [   64/  265]
train() client id: f_00001-0-2 loss: 0.400956  [   96/  265]
train() client id: f_00001-0-3 loss: 0.538556  [  128/  265]
train() client id: f_00001-0-4 loss: 0.569870  [  160/  265]
train() client id: f_00001-0-5 loss: 0.461215  [  192/  265]
train() client id: f_00001-0-6 loss: 0.384489  [  224/  265]
train() client id: f_00001-0-7 loss: 0.409879  [  256/  265]
train() client id: f_00001-1-0 loss: 0.356316  [   32/  265]
train() client id: f_00001-1-1 loss: 0.476731  [   64/  265]
train() client id: f_00001-1-2 loss: 0.410910  [   96/  265]
train() client id: f_00001-1-3 loss: 0.393739  [  128/  265]
train() client id: f_00001-1-4 loss: 0.512608  [  160/  265]
train() client id: f_00001-1-5 loss: 0.454952  [  192/  265]
train() client id: f_00001-1-6 loss: 0.445910  [  224/  265]
train() client id: f_00001-1-7 loss: 0.607447  [  256/  265]
train() client id: f_00001-2-0 loss: 0.453266  [   32/  265]
train() client id: f_00001-2-1 loss: 0.354557  [   64/  265]
train() client id: f_00001-2-2 loss: 0.541651  [   96/  265]
train() client id: f_00001-2-3 loss: 0.432634  [  128/  265]
train() client id: f_00001-2-4 loss: 0.352629  [  160/  265]
train() client id: f_00001-2-5 loss: 0.610229  [  192/  265]
train() client id: f_00001-2-6 loss: 0.383432  [  224/  265]
train() client id: f_00001-2-7 loss: 0.468028  [  256/  265]
train() client id: f_00001-3-0 loss: 0.418213  [   32/  265]
train() client id: f_00001-3-1 loss: 0.486425  [   64/  265]
train() client id: f_00001-3-2 loss: 0.471819  [   96/  265]
train() client id: f_00001-3-3 loss: 0.422125  [  128/  265]
train() client id: f_00001-3-4 loss: 0.370417  [  160/  265]
train() client id: f_00001-3-5 loss: 0.474038  [  192/  265]
train() client id: f_00001-3-6 loss: 0.593808  [  224/  265]
train() client id: f_00001-3-7 loss: 0.438628  [  256/  265]
train() client id: f_00001-4-0 loss: 0.347454  [   32/  265]
train() client id: f_00001-4-1 loss: 0.489373  [   64/  265]
train() client id: f_00001-4-2 loss: 0.474035  [   96/  265]
train() client id: f_00001-4-3 loss: 0.414645  [  128/  265]
train() client id: f_00001-4-4 loss: 0.540453  [  160/  265]
train() client id: f_00001-4-5 loss: 0.588381  [  192/  265]
train() client id: f_00001-4-6 loss: 0.453809  [  224/  265]
train() client id: f_00001-4-7 loss: 0.355045  [  256/  265]
train() client id: f_00001-5-0 loss: 0.388066  [   32/  265]
train() client id: f_00001-5-1 loss: 0.478446  [   64/  265]
train() client id: f_00001-5-2 loss: 0.375992  [   96/  265]
train() client id: f_00001-5-3 loss: 0.536576  [  128/  265]
train() client id: f_00001-5-4 loss: 0.429569  [  160/  265]
train() client id: f_00001-5-5 loss: 0.500758  [  192/  265]
train() client id: f_00001-5-6 loss: 0.396787  [  224/  265]
train() client id: f_00001-5-7 loss: 0.473530  [  256/  265]
train() client id: f_00001-6-0 loss: 0.566733  [   32/  265]
train() client id: f_00001-6-1 loss: 0.423436  [   64/  265]
train() client id: f_00001-6-2 loss: 0.413178  [   96/  265]
train() client id: f_00001-6-3 loss: 0.434397  [  128/  265]
train() client id: f_00001-6-4 loss: 0.400190  [  160/  265]
train() client id: f_00001-6-5 loss: 0.369238  [  192/  265]
train() client id: f_00001-6-6 loss: 0.444336  [  224/  265]
train() client id: f_00001-6-7 loss: 0.543497  [  256/  265]
train() client id: f_00001-7-0 loss: 0.384691  [   32/  265]
train() client id: f_00001-7-1 loss: 0.420299  [   64/  265]
train() client id: f_00001-7-2 loss: 0.607254  [   96/  265]
train() client id: f_00001-7-3 loss: 0.416186  [  128/  265]
train() client id: f_00001-7-4 loss: 0.437641  [  160/  265]
train() client id: f_00001-7-5 loss: 0.343002  [  192/  265]
train() client id: f_00001-7-6 loss: 0.589278  [  224/  265]
train() client id: f_00001-7-7 loss: 0.355883  [  256/  265]
train() client id: f_00001-8-0 loss: 0.403668  [   32/  265]
train() client id: f_00001-8-1 loss: 0.425216  [   64/  265]
train() client id: f_00001-8-2 loss: 0.459650  [   96/  265]
train() client id: f_00001-8-3 loss: 0.390671  [  128/  265]
train() client id: f_00001-8-4 loss: 0.371410  [  160/  265]
train() client id: f_00001-8-5 loss: 0.527444  [  192/  265]
train() client id: f_00001-8-6 loss: 0.438428  [  224/  265]
train() client id: f_00001-8-7 loss: 0.473174  [  256/  265]
train() client id: f_00002-0-0 loss: 0.703871  [   32/  124]
train() client id: f_00002-0-1 loss: 0.802270  [   64/  124]
train() client id: f_00002-0-2 loss: 0.737895  [   96/  124]
train() client id: f_00002-1-0 loss: 0.671691  [   32/  124]
train() client id: f_00002-1-1 loss: 0.819546  [   64/  124]
train() client id: f_00002-1-2 loss: 0.724136  [   96/  124]
train() client id: f_00002-2-0 loss: 0.608104  [   32/  124]
train() client id: f_00002-2-1 loss: 0.698898  [   64/  124]
train() client id: f_00002-2-2 loss: 0.877004  [   96/  124]
train() client id: f_00002-3-0 loss: 0.796156  [   32/  124]
train() client id: f_00002-3-1 loss: 0.601225  [   64/  124]
train() client id: f_00002-3-2 loss: 0.529390  [   96/  124]
train() client id: f_00002-4-0 loss: 0.751580  [   32/  124]
train() client id: f_00002-4-1 loss: 0.644070  [   64/  124]
train() client id: f_00002-4-2 loss: 0.621988  [   96/  124]
train() client id: f_00002-5-0 loss: 0.622962  [   32/  124]
train() client id: f_00002-5-1 loss: 0.708246  [   64/  124]
train() client id: f_00002-5-2 loss: 0.555883  [   96/  124]
train() client id: f_00002-6-0 loss: 0.676398  [   32/  124]
train() client id: f_00002-6-1 loss: 0.709730  [   64/  124]
train() client id: f_00002-6-2 loss: 0.613251  [   96/  124]
train() client id: f_00002-7-0 loss: 0.568714  [   32/  124]
train() client id: f_00002-7-1 loss: 0.639544  [   64/  124]
train() client id: f_00002-7-2 loss: 0.588697  [   96/  124]
train() client id: f_00002-8-0 loss: 0.782401  [   32/  124]
train() client id: f_00002-8-1 loss: 0.412568  [   64/  124]
train() client id: f_00002-8-2 loss: 0.643041  [   96/  124]
train() client id: f_00003-0-0 loss: 0.644740  [   32/   43]
train() client id: f_00003-1-0 loss: 0.647714  [   32/   43]
train() client id: f_00003-2-0 loss: 0.667728  [   32/   43]
train() client id: f_00003-3-0 loss: 0.459506  [   32/   43]
train() client id: f_00003-4-0 loss: 0.580057  [   32/   43]
train() client id: f_00003-5-0 loss: 0.624156  [   32/   43]
train() client id: f_00003-6-0 loss: 0.538330  [   32/   43]
train() client id: f_00003-7-0 loss: 0.534644  [   32/   43]
train() client id: f_00003-8-0 loss: 0.575714  [   32/   43]
train() client id: f_00004-0-0 loss: 0.723759  [   32/  306]
train() client id: f_00004-0-1 loss: 0.813197  [   64/  306]
train() client id: f_00004-0-2 loss: 0.728138  [   96/  306]
train() client id: f_00004-0-3 loss: 0.906418  [  128/  306]
train() client id: f_00004-0-4 loss: 0.881238  [  160/  306]
train() client id: f_00004-0-5 loss: 0.891121  [  192/  306]
train() client id: f_00004-0-6 loss: 1.043963  [  224/  306]
train() client id: f_00004-0-7 loss: 0.734010  [  256/  306]
train() client id: f_00004-0-8 loss: 0.874474  [  288/  306]
train() client id: f_00004-1-0 loss: 0.928843  [   32/  306]
train() client id: f_00004-1-1 loss: 0.787232  [   64/  306]
train() client id: f_00004-1-2 loss: 0.793987  [   96/  306]
train() client id: f_00004-1-3 loss: 0.728234  [  128/  306]
train() client id: f_00004-1-4 loss: 0.786816  [  160/  306]
train() client id: f_00004-1-5 loss: 0.713359  [  192/  306]
train() client id: f_00004-1-6 loss: 0.966403  [  224/  306]
train() client id: f_00004-1-7 loss: 0.884165  [  256/  306]
train() client id: f_00004-1-8 loss: 0.901513  [  288/  306]
train() client id: f_00004-2-0 loss: 0.813275  [   32/  306]
train() client id: f_00004-2-1 loss: 0.876227  [   64/  306]
train() client id: f_00004-2-2 loss: 0.873745  [   96/  306]
train() client id: f_00004-2-3 loss: 0.828360  [  128/  306]
train() client id: f_00004-2-4 loss: 0.774710  [  160/  306]
train() client id: f_00004-2-5 loss: 0.750484  [  192/  306]
train() client id: f_00004-2-6 loss: 0.851590  [  224/  306]
train() client id: f_00004-2-7 loss: 0.791995  [  256/  306]
train() client id: f_00004-2-8 loss: 0.987196  [  288/  306]
train() client id: f_00004-3-0 loss: 0.956186  [   32/  306]
train() client id: f_00004-3-1 loss: 0.787676  [   64/  306]
train() client id: f_00004-3-2 loss: 0.799610  [   96/  306]
train() client id: f_00004-3-3 loss: 0.801647  [  128/  306]
train() client id: f_00004-3-4 loss: 0.687537  [  160/  306]
train() client id: f_00004-3-5 loss: 0.820748  [  192/  306]
train() client id: f_00004-3-6 loss: 0.865243  [  224/  306]
train() client id: f_00004-3-7 loss: 0.758674  [  256/  306]
train() client id: f_00004-3-8 loss: 0.968687  [  288/  306]
train() client id: f_00004-4-0 loss: 0.777295  [   32/  306]
train() client id: f_00004-4-1 loss: 0.777764  [   64/  306]
train() client id: f_00004-4-2 loss: 1.070194  [   96/  306]
train() client id: f_00004-4-3 loss: 0.809166  [  128/  306]
train() client id: f_00004-4-4 loss: 0.917323  [  160/  306]
train() client id: f_00004-4-5 loss: 0.705905  [  192/  306]
train() client id: f_00004-4-6 loss: 0.877679  [  224/  306]
train() client id: f_00004-4-7 loss: 0.752517  [  256/  306]
train() client id: f_00004-4-8 loss: 0.727624  [  288/  306]
train() client id: f_00004-5-0 loss: 0.918074  [   32/  306]
train() client id: f_00004-5-1 loss: 0.777194  [   64/  306]
train() client id: f_00004-5-2 loss: 0.825641  [   96/  306]
train() client id: f_00004-5-3 loss: 0.833863  [  128/  306]
train() client id: f_00004-5-4 loss: 0.847789  [  160/  306]
train() client id: f_00004-5-5 loss: 0.933398  [  192/  306]
train() client id: f_00004-5-6 loss: 0.834356  [  224/  306]
train() client id: f_00004-5-7 loss: 0.755023  [  256/  306]
train() client id: f_00004-5-8 loss: 0.664905  [  288/  306]
train() client id: f_00004-6-0 loss: 0.757139  [   32/  306]
train() client id: f_00004-6-1 loss: 0.940686  [   64/  306]
train() client id: f_00004-6-2 loss: 0.853355  [   96/  306]
train() client id: f_00004-6-3 loss: 0.706327  [  128/  306]
train() client id: f_00004-6-4 loss: 0.845910  [  160/  306]
train() client id: f_00004-6-5 loss: 0.869363  [  192/  306]
train() client id: f_00004-6-6 loss: 0.811366  [  224/  306]
train() client id: f_00004-6-7 loss: 0.863754  [  256/  306]
train() client id: f_00004-6-8 loss: 0.801892  [  288/  306]
train() client id: f_00004-7-0 loss: 0.735931  [   32/  306]
train() client id: f_00004-7-1 loss: 0.970623  [   64/  306]
train() client id: f_00004-7-2 loss: 0.714945  [   96/  306]
train() client id: f_00004-7-3 loss: 0.760886  [  128/  306]
train() client id: f_00004-7-4 loss: 0.777812  [  160/  306]
train() client id: f_00004-7-5 loss: 0.836763  [  192/  306]
train() client id: f_00004-7-6 loss: 0.825474  [  224/  306]
train() client id: f_00004-7-7 loss: 0.844649  [  256/  306]
train() client id: f_00004-7-8 loss: 0.935994  [  288/  306]
train() client id: f_00004-8-0 loss: 0.803438  [   32/  306]
train() client id: f_00004-8-1 loss: 0.938844  [   64/  306]
train() client id: f_00004-8-2 loss: 0.828501  [   96/  306]
train() client id: f_00004-8-3 loss: 0.807535  [  128/  306]
train() client id: f_00004-8-4 loss: 0.756727  [  160/  306]
train() client id: f_00004-8-5 loss: 0.787393  [  192/  306]
train() client id: f_00004-8-6 loss: 0.797443  [  224/  306]
train() client id: f_00004-8-7 loss: 0.778683  [  256/  306]
train() client id: f_00004-8-8 loss: 0.756750  [  288/  306]
train() client id: f_00005-0-0 loss: 0.623682  [   32/  146]
train() client id: f_00005-0-1 loss: 0.511052  [   64/  146]
train() client id: f_00005-0-2 loss: 0.663462  [   96/  146]
train() client id: f_00005-0-3 loss: 0.507107  [  128/  146]
train() client id: f_00005-1-0 loss: 0.949210  [   32/  146]
train() client id: f_00005-1-1 loss: 0.305793  [   64/  146]
train() client id: f_00005-1-2 loss: 0.280199  [   96/  146]
train() client id: f_00005-1-3 loss: 0.512765  [  128/  146]
train() client id: f_00005-2-0 loss: 0.448938  [   32/  146]
train() client id: f_00005-2-1 loss: 0.514452  [   64/  146]
train() client id: f_00005-2-2 loss: 0.820273  [   96/  146]
train() client id: f_00005-2-3 loss: 0.365008  [  128/  146]
train() client id: f_00005-3-0 loss: 0.663655  [   32/  146]
train() client id: f_00005-3-1 loss: 0.369464  [   64/  146]
train() client id: f_00005-3-2 loss: 0.604823  [   96/  146]
train() client id: f_00005-3-3 loss: 0.532636  [  128/  146]
train() client id: f_00005-4-0 loss: 0.242483  [   32/  146]
train() client id: f_00005-4-1 loss: 0.634068  [   64/  146]
train() client id: f_00005-4-2 loss: 0.864303  [   96/  146]
train() client id: f_00005-4-3 loss: 0.480965  [  128/  146]
train() client id: f_00005-5-0 loss: 0.320799  [   32/  146]
train() client id: f_00005-5-1 loss: 0.829956  [   64/  146]
train() client id: f_00005-5-2 loss: 0.449756  [   96/  146]
train() client id: f_00005-5-3 loss: 0.577225  [  128/  146]
train() client id: f_00005-6-0 loss: 0.502477  [   32/  146]
train() client id: f_00005-6-1 loss: 0.709546  [   64/  146]
train() client id: f_00005-6-2 loss: 0.406477  [   96/  146]
train() client id: f_00005-6-3 loss: 0.510343  [  128/  146]
train() client id: f_00005-7-0 loss: 0.318644  [   32/  146]
train() client id: f_00005-7-1 loss: 0.466981  [   64/  146]
train() client id: f_00005-7-2 loss: 0.364929  [   96/  146]
train() client id: f_00005-7-3 loss: 0.793362  [  128/  146]
train() client id: f_00005-8-0 loss: 0.715160  [   32/  146]
train() client id: f_00005-8-1 loss: 0.727020  [   64/  146]
train() client id: f_00005-8-2 loss: 0.412178  [   96/  146]
train() client id: f_00005-8-3 loss: 0.338823  [  128/  146]
train() client id: f_00006-0-0 loss: 0.543782  [   32/   54]
train() client id: f_00006-1-0 loss: 0.500167  [   32/   54]
train() client id: f_00006-2-0 loss: 0.526861  [   32/   54]
train() client id: f_00006-3-0 loss: 0.445900  [   32/   54]
train() client id: f_00006-4-0 loss: 0.489203  [   32/   54]
train() client id: f_00006-5-0 loss: 0.487711  [   32/   54]
train() client id: f_00006-6-0 loss: 0.465849  [   32/   54]
train() client id: f_00006-7-0 loss: 0.464931  [   32/   54]
train() client id: f_00006-8-0 loss: 0.497064  [   32/   54]
train() client id: f_00007-0-0 loss: 0.763757  [   32/  179]
train() client id: f_00007-0-1 loss: 0.648990  [   64/  179]
train() client id: f_00007-0-2 loss: 0.885383  [   96/  179]
train() client id: f_00007-0-3 loss: 0.727223  [  128/  179]
train() client id: f_00007-0-4 loss: 0.695268  [  160/  179]
train() client id: f_00007-1-0 loss: 0.723969  [   32/  179]
train() client id: f_00007-1-1 loss: 0.808299  [   64/  179]
train() client id: f_00007-1-2 loss: 0.664083  [   96/  179]
train() client id: f_00007-1-3 loss: 0.662050  [  128/  179]
train() client id: f_00007-1-4 loss: 0.775828  [  160/  179]
train() client id: f_00007-2-0 loss: 0.675146  [   32/  179]
train() client id: f_00007-2-1 loss: 0.957973  [   64/  179]
train() client id: f_00007-2-2 loss: 0.566609  [   96/  179]
train() client id: f_00007-2-3 loss: 0.862845  [  128/  179]
train() client id: f_00007-2-4 loss: 0.659832  [  160/  179]
train() client id: f_00007-3-0 loss: 0.596076  [   32/  179]
train() client id: f_00007-3-1 loss: 0.699166  [   64/  179]
train() client id: f_00007-3-2 loss: 0.849010  [   96/  179]
train() client id: f_00007-3-3 loss: 0.880845  [  128/  179]
train() client id: f_00007-3-4 loss: 0.723812  [  160/  179]
train() client id: f_00007-4-0 loss: 0.575982  [   32/  179]
train() client id: f_00007-4-1 loss: 0.668469  [   64/  179]
train() client id: f_00007-4-2 loss: 0.641346  [   96/  179]
train() client id: f_00007-4-3 loss: 0.753271  [  128/  179]
train() client id: f_00007-4-4 loss: 0.796446  [  160/  179]
train() client id: f_00007-5-0 loss: 0.714278  [   32/  179]
train() client id: f_00007-5-1 loss: 0.555229  [   64/  179]
train() client id: f_00007-5-2 loss: 0.716249  [   96/  179]
train() client id: f_00007-5-3 loss: 0.921416  [  128/  179]
train() client id: f_00007-5-4 loss: 0.773295  [  160/  179]
train() client id: f_00007-6-0 loss: 0.867960  [   32/  179]
train() client id: f_00007-6-1 loss: 0.614752  [   64/  179]
train() client id: f_00007-6-2 loss: 0.759908  [   96/  179]
train() client id: f_00007-6-3 loss: 0.650248  [  128/  179]
train() client id: f_00007-6-4 loss: 0.738806  [  160/  179]
train() client id: f_00007-7-0 loss: 0.711453  [   32/  179]
train() client id: f_00007-7-1 loss: 0.688311  [   64/  179]
train() client id: f_00007-7-2 loss: 0.838725  [   96/  179]
train() client id: f_00007-7-3 loss: 0.736154  [  128/  179]
train() client id: f_00007-7-4 loss: 0.583590  [  160/  179]
train() client id: f_00007-8-0 loss: 0.900218  [   32/  179]
train() client id: f_00007-8-1 loss: 0.754590  [   64/  179]
train() client id: f_00007-8-2 loss: 0.831948  [   96/  179]
train() client id: f_00007-8-3 loss: 0.557270  [  128/  179]
train() client id: f_00007-8-4 loss: 0.706079  [  160/  179]
train() client id: f_00008-0-0 loss: 0.710973  [   32/  130]
train() client id: f_00008-0-1 loss: 0.653396  [   64/  130]
train() client id: f_00008-0-2 loss: 0.697763  [   96/  130]
train() client id: f_00008-0-3 loss: 0.793319  [  128/  130]
train() client id: f_00008-1-0 loss: 0.700368  [   32/  130]
train() client id: f_00008-1-1 loss: 0.695760  [   64/  130]
train() client id: f_00008-1-2 loss: 0.725325  [   96/  130]
train() client id: f_00008-1-3 loss: 0.725721  [  128/  130]
train() client id: f_00008-2-0 loss: 0.765056  [   32/  130]
train() client id: f_00008-2-1 loss: 0.726183  [   64/  130]
train() client id: f_00008-2-2 loss: 0.618548  [   96/  130]
train() client id: f_00008-2-3 loss: 0.775106  [  128/  130]
train() client id: f_00008-3-0 loss: 0.796667  [   32/  130]
train() client id: f_00008-3-1 loss: 0.691535  [   64/  130]
train() client id: f_00008-3-2 loss: 0.624196  [   96/  130]
train() client id: f_00008-3-3 loss: 0.736995  [  128/  130]
train() client id: f_00008-4-0 loss: 0.707263  [   32/  130]
train() client id: f_00008-4-1 loss: 0.763097  [   64/  130]
train() client id: f_00008-4-2 loss: 0.689920  [   96/  130]
train() client id: f_00008-4-3 loss: 0.713035  [  128/  130]
train() client id: f_00008-5-0 loss: 0.730091  [   32/  130]
train() client id: f_00008-5-1 loss: 0.785595  [   64/  130]
train() client id: f_00008-5-2 loss: 0.715029  [   96/  130]
train() client id: f_00008-5-3 loss: 0.645724  [  128/  130]
train() client id: f_00008-6-0 loss: 0.624830  [   32/  130]
train() client id: f_00008-6-1 loss: 0.727503  [   64/  130]
train() client id: f_00008-6-2 loss: 0.706278  [   96/  130]
train() client id: f_00008-6-3 loss: 0.784424  [  128/  130]
train() client id: f_00008-7-0 loss: 0.781304  [   32/  130]
train() client id: f_00008-7-1 loss: 0.717593  [   64/  130]
train() client id: f_00008-7-2 loss: 0.653355  [   96/  130]
train() client id: f_00008-7-3 loss: 0.741227  [  128/  130]
train() client id: f_00008-8-0 loss: 0.634340  [   32/  130]
train() client id: f_00008-8-1 loss: 0.737120  [   64/  130]
train() client id: f_00008-8-2 loss: 0.814636  [   96/  130]
train() client id: f_00008-8-3 loss: 0.700459  [  128/  130]
train() client id: f_00009-0-0 loss: 0.879402  [   32/  118]
train() client id: f_00009-0-1 loss: 0.865810  [   64/  118]
train() client id: f_00009-0-2 loss: 1.094782  [   96/  118]
train() client id: f_00009-1-0 loss: 1.040760  [   32/  118]
train() client id: f_00009-1-1 loss: 0.828307  [   64/  118]
train() client id: f_00009-1-2 loss: 0.947487  [   96/  118]
train() client id: f_00009-2-0 loss: 0.879917  [   32/  118]
train() client id: f_00009-2-1 loss: 0.796854  [   64/  118]
train() client id: f_00009-2-2 loss: 1.106832  [   96/  118]
train() client id: f_00009-3-0 loss: 1.048391  [   32/  118]
train() client id: f_00009-3-1 loss: 0.804141  [   64/  118]
train() client id: f_00009-3-2 loss: 0.998810  [   96/  118]
train() client id: f_00009-4-0 loss: 0.828663  [   32/  118]
train() client id: f_00009-4-1 loss: 0.875290  [   64/  118]
train() client id: f_00009-4-2 loss: 0.977879  [   96/  118]
train() client id: f_00009-5-0 loss: 0.843917  [   32/  118]
train() client id: f_00009-5-1 loss: 0.778715  [   64/  118]
train() client id: f_00009-5-2 loss: 1.048145  [   96/  118]
train() client id: f_00009-6-0 loss: 0.887072  [   32/  118]
train() client id: f_00009-6-1 loss: 0.844071  [   64/  118]
train() client id: f_00009-6-2 loss: 0.842106  [   96/  118]
train() client id: f_00009-7-0 loss: 0.845149  [   32/  118]
train() client id: f_00009-7-1 loss: 0.761489  [   64/  118]
train() client id: f_00009-7-2 loss: 0.949361  [   96/  118]
train() client id: f_00009-8-0 loss: 0.741158  [   32/  118]
train() client id: f_00009-8-1 loss: 0.946339  [   64/  118]
train() client id: f_00009-8-2 loss: 0.822569  [   96/  118]
At round 65 accuracy: 0.6472148541114059
At round 65 training accuracy: 0.5848423876592891
At round 65 training loss: 0.8355255469833074
update_location
xs = [  -3.9056584     4.20031788  345.00902392   18.81129433    0.97929623
    3.95640986 -307.44319194 -286.32485185  329.66397685 -272.06087855]
ys = [ 337.5879595   320.55583871    1.32061395 -307.45517586  299.35018685
  282.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [352.10919409 335.81793937 359.21159587 323.85575486 315.61288533
 299.99916173 323.30822262 303.28632848 344.94493416 289.88469   ]
dists_bs = [236.02567297 230.86631653 548.24621667 519.92866292 215.48011896
 208.92411955 221.5667713  206.71496584 528.64945606 196.62478831]
uav_gains = [1.17856502e-12 1.46410888e-12 1.08255453e-12 1.75503525e-12
 2.01172458e-12 2.67278145e-12 1.77049497e-12 2.51097274e-12
 1.29138642e-12 3.26345136e-12]
bs_gains = [2.50615657e-11 2.66614938e-11 2.36680394e-12 2.74569035e-12
 3.23410344e-11 3.52635523e-11 2.99144570e-11 3.63289386e-11
 2.62074203e-12 4.17933260e-11]
Round 66
-------------------------------
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.28261697 4.54685792 2.2554939  0.84667744 5.24112296 2.52309512
 1.03299146 3.13864151 2.30469629 2.04601254]
obj_prev = 26.218206122007363
eta_min = 1.468265776943342e-41	eta_max = 0.944257265961695
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 5.987676154852869	eta = 0.9090909090909091
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 15.480452461148737	eta = 0.3516267998379439
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 10.14509351030873	eta = 0.5365492149900848
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.229709829328627	eta = 0.5897630651031099
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.172211188857027	eta = 0.5934601642807859
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195028532138	eta = 0.5934770457345999
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195027990423	eta = 0.5934770460851201
eta = 0.5934770460851201
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.04305412 0.09055032 0.04237072 0.01469307 0.10456    0.0498881
 0.01845176 0.06116418 0.04442091 0.04032052]
ene_total = [0.9382438  1.35500986 0.94553393 0.48503672 1.54222126 0.79045116
 0.53538055 1.0793304  0.84532808 0.65541452]
ti_comp = [1.69339463 1.86443235 1.68124436 1.74028295 1.86800619 1.86951147
 1.74115692 1.77160865 1.77482088 1.87231038]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [1.73943290e-06 1.33492469e-05 1.68196045e-06 6.54603068e-08
 2.04748185e-05 2.22031557e-06 1.29514340e-07 4.55654761e-06
 1.73913543e-06 1.16869917e-06]
ene_total = [0.34787422 0.11349245 0.36453489 0.28355414 0.10868941 0.10637493
 0.28235657 0.24065936 0.23621585 0.10252241]
optimize_network iter = 0 obj = 2.186274239551174
eta = 0.5934770460851201
freqs = [12712369.24332196 24283617.19415748 12600999.04046834  4221460.42224169
 27987058.04437446 13342550.5059045   5298707.90102926 17262328.31372014
 12514196.1057277  10767584.65528894]
eta_min = 0.5934770460851205	eta_max = 0.7588338965075888
af = 0.0005034964568992424	bf = 0.8885079592597985	zeta = 0.0005538461025891666	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.17745128e-07 2.43852935e-06 3.07246539e-07 1.19577441e-08
 3.74016947e-06 4.05588772e-07 2.36586018e-08 8.32352201e-07
 3.17690788e-07 2.13488239e-07]
ene_total = [1.55552023 0.5068411  1.6300255  1.26798066 0.48500598 0.47557109
 1.26262219 1.07594073 1.05621167 0.45839627]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 1 obj = 3.6847138773635306
eta = 0.7588338965075888
freqs = [12631650.06371988 22329634.8077465  12600999.04046835  4097654.84679903
 25698769.41757561 12244375.3475877   5141153.96101874 16512221.31849963
 11952921.87158274  9870499.99002256]
eta_min = 0.7588338965075906	eta_max = 0.7588338965075868
af = 0.00043529367837054833	bf = 0.8885079592597985	zeta = 0.0004788230462076032	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.13722794e-07 2.06188515e-06 3.07246539e-07 1.12666437e-08
 3.15356247e-06 3.41571386e-07 2.22725702e-08 7.61586768e-07
 2.89832359e-07 1.79397204e-07]
ene_total = [1.55551998 0.506818   1.6300255  1.26798062 0.48497001 0.47556716
 1.2626221  1.0759364  1.05620996 0.45839418]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 2 obj = 3.6847138773635004
eta = 0.7588338965075868
freqs = [12631650.06371987 22329634.80774652 12600999.04046834  4097654.84679903
 25698769.41757564 12244375.34758771  5141153.96101873 16512221.31849963
 11952921.87158274  9870499.99002257]
Done!
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.04697167e-07 5.94595514e-06 8.86021287e-07 3.24901501e-08
 9.09407634e-06 9.85005461e-07 6.42284576e-08 2.19622356e-06
 8.35803196e-07 5.17336149e-07]
ene_total = [0.0253677  0.00826897 0.02658271 0.020678   0.00791473 0.0077561
 0.02059063 0.01754759 0.01722501 0.00747574]
At round 66 energy consumption: 0.15940718136832308
At round 66 eta: 0.7588338965075868
At round 66 a_n: 5.574576956279433
At round 66 local rounds: 9.036737525084137
At round 66 global rounds: 23.115093189100694
gradient difference: 0.63105309009552
train() client id: f_00000-0-0 loss: 1.367816  [   32/  126]
train() client id: f_00000-0-1 loss: 0.807549  [   64/  126]
train() client id: f_00000-0-2 loss: 0.910327  [   96/  126]
train() client id: f_00000-1-0 loss: 0.908560  [   32/  126]
train() client id: f_00000-1-1 loss: 1.037407  [   64/  126]
train() client id: f_00000-1-2 loss: 0.962636  [   96/  126]
train() client id: f_00000-2-0 loss: 0.972197  [   32/  126]
train() client id: f_00000-2-1 loss: 0.928474  [   64/  126]
train() client id: f_00000-2-2 loss: 0.914494  [   96/  126]
train() client id: f_00000-3-0 loss: 0.972576  [   32/  126]
train() client id: f_00000-3-1 loss: 0.935334  [   64/  126]
train() client id: f_00000-3-2 loss: 0.895389  [   96/  126]
train() client id: f_00000-4-0 loss: 0.819464  [   32/  126]
train() client id: f_00000-4-1 loss: 0.894758  [   64/  126]
train() client id: f_00000-4-2 loss: 0.824779  [   96/  126]
train() client id: f_00000-5-0 loss: 0.787753  [   32/  126]
train() client id: f_00000-5-1 loss: 0.971574  [   64/  126]
train() client id: f_00000-5-2 loss: 0.756486  [   96/  126]
train() client id: f_00000-6-0 loss: 0.964550  [   32/  126]
train() client id: f_00000-6-1 loss: 0.875173  [   64/  126]
train() client id: f_00000-6-2 loss: 0.779264  [   96/  126]
train() client id: f_00000-7-0 loss: 0.737893  [   32/  126]
train() client id: f_00000-7-1 loss: 1.060921  [   64/  126]
train() client id: f_00000-7-2 loss: 0.834634  [   96/  126]
train() client id: f_00000-8-0 loss: 0.879711  [   32/  126]
train() client id: f_00000-8-1 loss: 0.787672  [   64/  126]
train() client id: f_00000-8-2 loss: 0.817278  [   96/  126]
train() client id: f_00001-0-0 loss: 0.360441  [   32/  265]
train() client id: f_00001-0-1 loss: 0.428510  [   64/  265]
train() client id: f_00001-0-2 loss: 0.399090  [   96/  265]
train() client id: f_00001-0-3 loss: 0.387486  [  128/  265]
train() client id: f_00001-0-4 loss: 0.343945  [  160/  265]
train() client id: f_00001-0-5 loss: 0.500467  [  192/  265]
train() client id: f_00001-0-6 loss: 0.436381  [  224/  265]
train() client id: f_00001-0-7 loss: 0.413410  [  256/  265]
train() client id: f_00001-1-0 loss: 0.320207  [   32/  265]
train() client id: f_00001-1-1 loss: 0.479369  [   64/  265]
train() client id: f_00001-1-2 loss: 0.528830  [   96/  265]
train() client id: f_00001-1-3 loss: 0.470069  [  128/  265]
train() client id: f_00001-1-4 loss: 0.399895  [  160/  265]
train() client id: f_00001-1-5 loss: 0.288757  [  192/  265]
train() client id: f_00001-1-6 loss: 0.315421  [  224/  265]
train() client id: f_00001-1-7 loss: 0.417589  [  256/  265]
train() client id: f_00001-2-0 loss: 0.453534  [   32/  265]
train() client id: f_00001-2-1 loss: 0.461420  [   64/  265]
train() client id: f_00001-2-2 loss: 0.368937  [   96/  265]
train() client id: f_00001-2-3 loss: 0.402270  [  128/  265]
train() client id: f_00001-2-4 loss: 0.319444  [  160/  265]
train() client id: f_00001-2-5 loss: 0.486147  [  192/  265]
train() client id: f_00001-2-6 loss: 0.327858  [  224/  265]
train() client id: f_00001-2-7 loss: 0.308790  [  256/  265]
train() client id: f_00001-3-0 loss: 0.387937  [   32/  265]
train() client id: f_00001-3-1 loss: 0.436571  [   64/  265]
train() client id: f_00001-3-2 loss: 0.376516  [   96/  265]
train() client id: f_00001-3-3 loss: 0.309835  [  128/  265]
train() client id: f_00001-3-4 loss: 0.281944  [  160/  265]
train() client id: f_00001-3-5 loss: 0.375375  [  192/  265]
train() client id: f_00001-3-6 loss: 0.491281  [  224/  265]
train() client id: f_00001-3-7 loss: 0.487382  [  256/  265]
train() client id: f_00001-4-0 loss: 0.370931  [   32/  265]
train() client id: f_00001-4-1 loss: 0.356612  [   64/  265]
train() client id: f_00001-4-2 loss: 0.440541  [   96/  265]
train() client id: f_00001-4-3 loss: 0.381183  [  128/  265]
train() client id: f_00001-4-4 loss: 0.366214  [  160/  265]
train() client id: f_00001-4-5 loss: 0.390570  [  192/  265]
train() client id: f_00001-4-6 loss: 0.426920  [  224/  265]
train() client id: f_00001-4-7 loss: 0.379126  [  256/  265]
train() client id: f_00001-5-0 loss: 0.358779  [   32/  265]
train() client id: f_00001-5-1 loss: 0.319729  [   64/  265]
train() client id: f_00001-5-2 loss: 0.402162  [   96/  265]
train() client id: f_00001-5-3 loss: 0.463441  [  128/  265]
train() client id: f_00001-5-4 loss: 0.327590  [  160/  265]
train() client id: f_00001-5-5 loss: 0.446669  [  192/  265]
train() client id: f_00001-5-6 loss: 0.344065  [  224/  265]
train() client id: f_00001-5-7 loss: 0.349780  [  256/  265]
train() client id: f_00001-6-0 loss: 0.364273  [   32/  265]
train() client id: f_00001-6-1 loss: 0.305741  [   64/  265]
train() client id: f_00001-6-2 loss: 0.411718  [   96/  265]
train() client id: f_00001-6-3 loss: 0.374116  [  128/  265]
train() client id: f_00001-6-4 loss: 0.388397  [  160/  265]
train() client id: f_00001-6-5 loss: 0.343186  [  192/  265]
train() client id: f_00001-6-6 loss: 0.353352  [  224/  265]
train() client id: f_00001-6-7 loss: 0.470745  [  256/  265]
train() client id: f_00001-7-0 loss: 0.389765  [   32/  265]
train() client id: f_00001-7-1 loss: 0.278250  [   64/  265]
train() client id: f_00001-7-2 loss: 0.465499  [   96/  265]
train() client id: f_00001-7-3 loss: 0.422960  [  128/  265]
train() client id: f_00001-7-4 loss: 0.350355  [  160/  265]
train() client id: f_00001-7-5 loss: 0.430393  [  192/  265]
train() client id: f_00001-7-6 loss: 0.416971  [  224/  265]
train() client id: f_00001-7-7 loss: 0.332101  [  256/  265]
train() client id: f_00001-8-0 loss: 0.487117  [   32/  265]
train() client id: f_00001-8-1 loss: 0.301719  [   64/  265]
train() client id: f_00001-8-2 loss: 0.318923  [   96/  265]
train() client id: f_00001-8-3 loss: 0.279760  [  128/  265]
train() client id: f_00001-8-4 loss: 0.411422  [  160/  265]
train() client id: f_00001-8-5 loss: 0.478568  [  192/  265]
train() client id: f_00001-8-6 loss: 0.393668  [  224/  265]
train() client id: f_00001-8-7 loss: 0.406818  [  256/  265]
train() client id: f_00002-0-0 loss: 0.869246  [   32/  124]
train() client id: f_00002-0-1 loss: 0.972836  [   64/  124]
train() client id: f_00002-0-2 loss: 1.140994  [   96/  124]
train() client id: f_00002-1-0 loss: 1.014437  [   32/  124]
train() client id: f_00002-1-1 loss: 0.923141  [   64/  124]
train() client id: f_00002-1-2 loss: 1.004107  [   96/  124]
train() client id: f_00002-2-0 loss: 0.884486  [   32/  124]
train() client id: f_00002-2-1 loss: 1.059473  [   64/  124]
train() client id: f_00002-2-2 loss: 0.854149  [   96/  124]
train() client id: f_00002-3-0 loss: 1.068925  [   32/  124]
train() client id: f_00002-3-1 loss: 1.080100  [   64/  124]
train() client id: f_00002-3-2 loss: 0.865306  [   96/  124]
train() client id: f_00002-4-0 loss: 0.916685  [   32/  124]
train() client id: f_00002-4-1 loss: 0.709819  [   64/  124]
train() client id: f_00002-4-2 loss: 0.989454  [   96/  124]
train() client id: f_00002-5-0 loss: 0.920374  [   32/  124]
train() client id: f_00002-5-1 loss: 0.926520  [   64/  124]
train() client id: f_00002-5-2 loss: 0.851616  [   96/  124]
train() client id: f_00002-6-0 loss: 0.848018  [   32/  124]
train() client id: f_00002-6-1 loss: 1.061315  [   64/  124]
train() client id: f_00002-6-2 loss: 0.816834  [   96/  124]
train() client id: f_00002-7-0 loss: 1.025189  [   32/  124]
train() client id: f_00002-7-1 loss: 0.964268  [   64/  124]
train() client id: f_00002-7-2 loss: 0.736108  [   96/  124]
train() client id: f_00002-8-0 loss: 0.926668  [   32/  124]
train() client id: f_00002-8-1 loss: 0.819339  [   64/  124]
train() client id: f_00002-8-2 loss: 1.194989  [   96/  124]
train() client id: f_00003-0-0 loss: 0.263575  [   32/   43]
train() client id: f_00003-1-0 loss: 0.600141  [   32/   43]
train() client id: f_00003-2-0 loss: 0.859137  [   32/   43]
train() client id: f_00003-3-0 loss: 0.796255  [   32/   43]
train() client id: f_00003-4-0 loss: 0.653694  [   32/   43]
train() client id: f_00003-5-0 loss: 0.495776  [   32/   43]
train() client id: f_00003-6-0 loss: 0.720435  [   32/   43]
train() client id: f_00003-7-0 loss: 0.699478  [   32/   43]
train() client id: f_00003-8-0 loss: 0.672096  [   32/   43]
train() client id: f_00004-0-0 loss: 0.875030  [   32/  306]
train() client id: f_00004-0-1 loss: 0.893446  [   64/  306]
train() client id: f_00004-0-2 loss: 0.849855  [   96/  306]
train() client id: f_00004-0-3 loss: 0.996149  [  128/  306]
train() client id: f_00004-0-4 loss: 0.814438  [  160/  306]
train() client id: f_00004-0-5 loss: 0.928794  [  192/  306]
train() client id: f_00004-0-6 loss: 0.838249  [  224/  306]
train() client id: f_00004-0-7 loss: 1.269777  [  256/  306]
train() client id: f_00004-0-8 loss: 1.141266  [  288/  306]
train() client id: f_00004-1-0 loss: 0.942389  [   32/  306]
train() client id: f_00004-1-1 loss: 0.959252  [   64/  306]
train() client id: f_00004-1-2 loss: 0.849366  [   96/  306]
train() client id: f_00004-1-3 loss: 0.987555  [  128/  306]
train() client id: f_00004-1-4 loss: 1.009925  [  160/  306]
train() client id: f_00004-1-5 loss: 0.959465  [  192/  306]
train() client id: f_00004-1-6 loss: 0.940907  [  224/  306]
train() client id: f_00004-1-7 loss: 0.916213  [  256/  306]
train() client id: f_00004-1-8 loss: 0.962479  [  288/  306]
train() client id: f_00004-2-0 loss: 0.797165  [   32/  306]
train() client id: f_00004-2-1 loss: 0.944832  [   64/  306]
train() client id: f_00004-2-2 loss: 1.007975  [   96/  306]
train() client id: f_00004-2-3 loss: 0.937073  [  128/  306]
train() client id: f_00004-2-4 loss: 0.972221  [  160/  306]
train() client id: f_00004-2-5 loss: 0.870466  [  192/  306]
train() client id: f_00004-2-6 loss: 0.932135  [  224/  306]
train() client id: f_00004-2-7 loss: 0.911814  [  256/  306]
train() client id: f_00004-2-8 loss: 1.114009  [  288/  306]
train() client id: f_00004-3-0 loss: 0.791778  [   32/  306]
train() client id: f_00004-3-1 loss: 0.921906  [   64/  306]
train() client id: f_00004-3-2 loss: 0.962787  [   96/  306]
train() client id: f_00004-3-3 loss: 1.212769  [  128/  306]
train() client id: f_00004-3-4 loss: 0.876851  [  160/  306]
train() client id: f_00004-3-5 loss: 0.860216  [  192/  306]
train() client id: f_00004-3-6 loss: 0.953792  [  224/  306]
train() client id: f_00004-3-7 loss: 0.845436  [  256/  306]
train() client id: f_00004-3-8 loss: 1.092516  [  288/  306]
train() client id: f_00004-4-0 loss: 0.957723  [   32/  306]
train() client id: f_00004-4-1 loss: 1.008051  [   64/  306]
train() client id: f_00004-4-2 loss: 0.876767  [   96/  306]
train() client id: f_00004-4-3 loss: 1.028854  [  128/  306]
train() client id: f_00004-4-4 loss: 0.954675  [  160/  306]
train() client id: f_00004-4-5 loss: 0.969745  [  192/  306]
train() client id: f_00004-4-6 loss: 0.863761  [  224/  306]
train() client id: f_00004-4-7 loss: 0.975238  [  256/  306]
train() client id: f_00004-4-8 loss: 0.857368  [  288/  306]
train() client id: f_00004-5-0 loss: 0.854795  [   32/  306]
train() client id: f_00004-5-1 loss: 0.913431  [   64/  306]
train() client id: f_00004-5-2 loss: 1.107376  [   96/  306]
train() client id: f_00004-5-3 loss: 0.975133  [  128/  306]
train() client id: f_00004-5-4 loss: 0.996818  [  160/  306]
train() client id: f_00004-5-5 loss: 0.899823  [  192/  306]
train() client id: f_00004-5-6 loss: 0.900959  [  224/  306]
train() client id: f_00004-5-7 loss: 0.807048  [  256/  306]
train() client id: f_00004-5-8 loss: 0.999696  [  288/  306]
train() client id: f_00004-6-0 loss: 0.982852  [   32/  306]
train() client id: f_00004-6-1 loss: 1.118157  [   64/  306]
train() client id: f_00004-6-2 loss: 0.961339  [   96/  306]
train() client id: f_00004-6-3 loss: 0.885181  [  128/  306]
train() client id: f_00004-6-4 loss: 0.959325  [  160/  306]
train() client id: f_00004-6-5 loss: 0.995174  [  192/  306]
train() client id: f_00004-6-6 loss: 0.884118  [  224/  306]
train() client id: f_00004-6-7 loss: 0.848520  [  256/  306]
train() client id: f_00004-6-8 loss: 0.891199  [  288/  306]
train() client id: f_00004-7-0 loss: 1.060527  [   32/  306]
train() client id: f_00004-7-1 loss: 1.064310  [   64/  306]
train() client id: f_00004-7-2 loss: 0.949449  [   96/  306]
train() client id: f_00004-7-3 loss: 0.885348  [  128/  306]
train() client id: f_00004-7-4 loss: 0.831603  [  160/  306]
train() client id: f_00004-7-5 loss: 0.884875  [  192/  306]
train() client id: f_00004-7-6 loss: 0.913507  [  224/  306]
train() client id: f_00004-7-7 loss: 0.813170  [  256/  306]
train() client id: f_00004-7-8 loss: 1.009758  [  288/  306]
train() client id: f_00004-8-0 loss: 0.889977  [   32/  306]
train() client id: f_00004-8-1 loss: 1.008935  [   64/  306]
train() client id: f_00004-8-2 loss: 1.003419  [   96/  306]
train() client id: f_00004-8-3 loss: 0.975945  [  128/  306]
train() client id: f_00004-8-4 loss: 0.926874  [  160/  306]
train() client id: f_00004-8-5 loss: 1.003990  [  192/  306]
train() client id: f_00004-8-6 loss: 0.794361  [  224/  306]
train() client id: f_00004-8-7 loss: 0.891735  [  256/  306]
train() client id: f_00004-8-8 loss: 0.870521  [  288/  306]
train() client id: f_00005-0-0 loss: 0.728718  [   32/  146]
train() client id: f_00005-0-1 loss: 0.871652  [   64/  146]
train() client id: f_00005-0-2 loss: 0.623353  [   96/  146]
train() client id: f_00005-0-3 loss: 0.941276  [  128/  146]
train() client id: f_00005-1-0 loss: 0.648257  [   32/  146]
train() client id: f_00005-1-1 loss: 0.872770  [   64/  146]
train() client id: f_00005-1-2 loss: 0.982729  [   96/  146]
train() client id: f_00005-1-3 loss: 0.842401  [  128/  146]
train() client id: f_00005-2-0 loss: 0.812644  [   32/  146]
train() client id: f_00005-2-1 loss: 0.668546  [   64/  146]
train() client id: f_00005-2-2 loss: 0.725553  [   96/  146]
train() client id: f_00005-2-3 loss: 0.898512  [  128/  146]
train() client id: f_00005-3-0 loss: 0.650970  [   32/  146]
train() client id: f_00005-3-1 loss: 0.900578  [   64/  146]
train() client id: f_00005-3-2 loss: 0.687680  [   96/  146]
train() client id: f_00005-3-3 loss: 0.974663  [  128/  146]
train() client id: f_00005-4-0 loss: 0.802776  [   32/  146]
train() client id: f_00005-4-1 loss: 0.685845  [   64/  146]
train() client id: f_00005-4-2 loss: 0.782984  [   96/  146]
train() client id: f_00005-4-3 loss: 0.841939  [  128/  146]
train() client id: f_00005-5-0 loss: 0.755798  [   32/  146]
train() client id: f_00005-5-1 loss: 0.834261  [   64/  146]
train() client id: f_00005-5-2 loss: 0.536472  [   96/  146]
train() client id: f_00005-5-3 loss: 0.929053  [  128/  146]
train() client id: f_00005-6-0 loss: 0.746998  [   32/  146]
train() client id: f_00005-6-1 loss: 0.672389  [   64/  146]
train() client id: f_00005-6-2 loss: 0.936933  [   96/  146]
train() client id: f_00005-6-3 loss: 0.912417  [  128/  146]
train() client id: f_00005-7-0 loss: 0.890262  [   32/  146]
train() client id: f_00005-7-1 loss: 0.872418  [   64/  146]
train() client id: f_00005-7-2 loss: 0.720004  [   96/  146]
train() client id: f_00005-7-3 loss: 0.681334  [  128/  146]
train() client id: f_00005-8-0 loss: 0.679099  [   32/  146]
train() client id: f_00005-8-1 loss: 1.089542  [   64/  146]
train() client id: f_00005-8-2 loss: 0.685928  [   96/  146]
train() client id: f_00005-8-3 loss: 0.666899  [  128/  146]
train() client id: f_00006-0-0 loss: 0.481153  [   32/   54]
train() client id: f_00006-1-0 loss: 0.469265  [   32/   54]
train() client id: f_00006-2-0 loss: 0.414172  [   32/   54]
train() client id: f_00006-3-0 loss: 0.521671  [   32/   54]
train() client id: f_00006-4-0 loss: 0.459896  [   32/   54]
train() client id: f_00006-5-0 loss: 0.467956  [   32/   54]
train() client id: f_00006-6-0 loss: 0.483845  [   32/   54]
train() client id: f_00006-7-0 loss: 0.473188  [   32/   54]
train() client id: f_00006-8-0 loss: 0.435781  [   32/   54]
train() client id: f_00007-0-0 loss: 0.510405  [   32/  179]
train() client id: f_00007-0-1 loss: 0.658189  [   64/  179]
train() client id: f_00007-0-2 loss: 0.702098  [   96/  179]
train() client id: f_00007-0-3 loss: 0.683890  [  128/  179]
train() client id: f_00007-0-4 loss: 0.798288  [  160/  179]
train() client id: f_00007-1-0 loss: 0.492402  [   32/  179]
train() client id: f_00007-1-1 loss: 0.672224  [   64/  179]
train() client id: f_00007-1-2 loss: 0.686522  [   96/  179]
train() client id: f_00007-1-3 loss: 0.644598  [  128/  179]
train() client id: f_00007-1-4 loss: 0.738323  [  160/  179]
train() client id: f_00007-2-0 loss: 0.609165  [   32/  179]
train() client id: f_00007-2-1 loss: 0.536653  [   64/  179]
train() client id: f_00007-2-2 loss: 0.608336  [   96/  179]
train() client id: f_00007-2-3 loss: 0.583162  [  128/  179]
train() client id: f_00007-2-4 loss: 0.852298  [  160/  179]
train() client id: f_00007-3-0 loss: 0.747717  [   32/  179]
train() client id: f_00007-3-1 loss: 0.677459  [   64/  179]
train() client id: f_00007-3-2 loss: 0.710994  [   96/  179]
train() client id: f_00007-3-3 loss: 0.574267  [  128/  179]
train() client id: f_00007-3-4 loss: 0.562677  [  160/  179]
train() client id: f_00007-4-0 loss: 0.981367  [   32/  179]
train() client id: f_00007-4-1 loss: 0.684831  [   64/  179]
train() client id: f_00007-4-2 loss: 0.612801  [   96/  179]
train() client id: f_00007-4-3 loss: 0.456825  [  128/  179]
train() client id: f_00007-4-4 loss: 0.491912  [  160/  179]
train() client id: f_00007-5-0 loss: 0.769387  [   32/  179]
train() client id: f_00007-5-1 loss: 0.622322  [   64/  179]
train() client id: f_00007-5-2 loss: 0.792736  [   96/  179]
train() client id: f_00007-5-3 loss: 0.570713  [  128/  179]
train() client id: f_00007-5-4 loss: 0.515532  [  160/  179]
train() client id: f_00007-6-0 loss: 0.551949  [   32/  179]
train() client id: f_00007-6-1 loss: 0.723466  [   64/  179]
train() client id: f_00007-6-2 loss: 0.519048  [   96/  179]
train() client id: f_00007-6-3 loss: 0.793800  [  128/  179]
train() client id: f_00007-6-4 loss: 0.669016  [  160/  179]
train() client id: f_00007-7-0 loss: 0.552961  [   32/  179]
train() client id: f_00007-7-1 loss: 0.552334  [   64/  179]
train() client id: f_00007-7-2 loss: 0.591121  [   96/  179]
train() client id: f_00007-7-3 loss: 0.790050  [  128/  179]
train() client id: f_00007-7-4 loss: 0.694658  [  160/  179]
train() client id: f_00007-8-0 loss: 0.499668  [   32/  179]
train() client id: f_00007-8-1 loss: 0.482028  [   64/  179]
train() client id: f_00007-8-2 loss: 0.746751  [   96/  179]
train() client id: f_00007-8-3 loss: 0.863738  [  128/  179]
train() client id: f_00007-8-4 loss: 0.603260  [  160/  179]
train() client id: f_00008-0-0 loss: 0.830398  [   32/  130]
train() client id: f_00008-0-1 loss: 0.786820  [   64/  130]
train() client id: f_00008-0-2 loss: 0.745442  [   96/  130]
train() client id: f_00008-0-3 loss: 0.685042  [  128/  130]
train() client id: f_00008-1-0 loss: 0.765070  [   32/  130]
train() client id: f_00008-1-1 loss: 0.721645  [   64/  130]
train() client id: f_00008-1-2 loss: 0.866671  [   96/  130]
train() client id: f_00008-1-3 loss: 0.709907  [  128/  130]
train() client id: f_00008-2-0 loss: 0.838856  [   32/  130]
train() client id: f_00008-2-1 loss: 0.886505  [   64/  130]
train() client id: f_00008-2-2 loss: 0.668363  [   96/  130]
train() client id: f_00008-2-3 loss: 0.674539  [  128/  130]
train() client id: f_00008-3-0 loss: 0.769287  [   32/  130]
train() client id: f_00008-3-1 loss: 0.764110  [   64/  130]
train() client id: f_00008-3-2 loss: 0.795251  [   96/  130]
train() client id: f_00008-3-3 loss: 0.726345  [  128/  130]
train() client id: f_00008-4-0 loss: 0.707359  [   32/  130]
train() client id: f_00008-4-1 loss: 0.826360  [   64/  130]
train() client id: f_00008-4-2 loss: 0.747857  [   96/  130]
train() client id: f_00008-4-3 loss: 0.775605  [  128/  130]
train() client id: f_00008-5-0 loss: 0.827578  [   32/  130]
train() client id: f_00008-5-1 loss: 0.722758  [   64/  130]
train() client id: f_00008-5-2 loss: 0.722532  [   96/  130]
train() client id: f_00008-5-3 loss: 0.764537  [  128/  130]
train() client id: f_00008-6-0 loss: 0.797335  [   32/  130]
train() client id: f_00008-6-1 loss: 0.765385  [   64/  130]
train() client id: f_00008-6-2 loss: 0.721581  [   96/  130]
train() client id: f_00008-6-3 loss: 0.775864  [  128/  130]
train() client id: f_00008-7-0 loss: 0.689418  [   32/  130]
train() client id: f_00008-7-1 loss: 0.890381  [   64/  130]
train() client id: f_00008-7-2 loss: 0.720564  [   96/  130]
train() client id: f_00008-7-3 loss: 0.758147  [  128/  130]
train() client id: f_00008-8-0 loss: 0.729622  [   32/  130]
train() client id: f_00008-8-1 loss: 0.871233  [   64/  130]
train() client id: f_00008-8-2 loss: 0.681535  [   96/  130]
train() client id: f_00008-8-3 loss: 0.741816  [  128/  130]
train() client id: f_00009-0-0 loss: 0.619875  [   32/  118]
train() client id: f_00009-0-1 loss: 0.958960  [   64/  118]
train() client id: f_00009-0-2 loss: 1.037970  [   96/  118]
train() client id: f_00009-1-0 loss: 1.006945  [   32/  118]
train() client id: f_00009-1-1 loss: 0.985815  [   64/  118]
train() client id: f_00009-1-2 loss: 0.782972  [   96/  118]
train() client id: f_00009-2-0 loss: 0.955447  [   32/  118]
train() client id: f_00009-2-1 loss: 0.650013  [   64/  118]
train() client id: f_00009-2-2 loss: 0.850255  [   96/  118]
train() client id: f_00009-3-0 loss: 0.661554  [   32/  118]
train() client id: f_00009-3-1 loss: 0.665923  [   64/  118]
train() client id: f_00009-3-2 loss: 0.965558  [   96/  118]
train() client id: f_00009-4-0 loss: 0.864890  [   32/  118]
train() client id: f_00009-4-1 loss: 0.818877  [   64/  118]
train() client id: f_00009-4-2 loss: 0.699015  [   96/  118]
train() client id: f_00009-5-0 loss: 0.887100  [   32/  118]
train() client id: f_00009-5-1 loss: 0.789341  [   64/  118]
train() client id: f_00009-5-2 loss: 0.755614  [   96/  118]
train() client id: f_00009-6-0 loss: 0.914202  [   32/  118]
train() client id: f_00009-6-1 loss: 0.773548  [   64/  118]
train() client id: f_00009-6-2 loss: 0.758253  [   96/  118]
train() client id: f_00009-7-0 loss: 0.877977  [   32/  118]
train() client id: f_00009-7-1 loss: 0.861073  [   64/  118]
train() client id: f_00009-7-2 loss: 0.718328  [   96/  118]
train() client id: f_00009-8-0 loss: 0.919922  [   32/  118]
train() client id: f_00009-8-1 loss: 0.620045  [   64/  118]
train() client id: f_00009-8-2 loss: 0.919889  [   96/  118]
At round 66 accuracy: 0.6445623342175066
At round 66 training accuracy: 0.5915492957746479
At round 66 training loss: 0.8167756926436572
update_location
xs = [  -3.9056584     4.20031788  350.00902392   18.81129433    0.97929623
    3.95640986 -312.44319194 -291.32485185  334.66397685 -277.06087855]
ys = [ 342.5879595   325.55583871    1.32061395 -312.45517586  304.35018685
  287.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [356.90581973 340.59396176 364.01656672 328.606302   320.35916602
 304.71730923 328.06651571 308.01111272 349.72653227 294.58231834]
dists_bs = [239.49738599 234.05088014 552.99096225 524.57160263 218.40371686
 211.54769968 224.59133123 209.45005519 533.42467806 199.14044332]
uav_gains = [1.11218118e-12 1.36922033e-12 1.02508951e-12 1.62931179e-12
 1.85748367e-12 2.44465100e-12 1.64287124e-12 2.30088160e-12
 1.21414591e-12 2.97056458e-12]
bs_gains = [2.40575783e-11 2.56581480e-11 2.31038093e-12 2.67818585e-12
 3.11433994e-11 3.40526430e-11 2.88000814e-11 3.50161782e-11
 2.55557949e-12 4.03317968e-11]
Round 67
-------------------------------
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.14668611 4.26785824 2.12126966 0.79869584 4.91942362 2.36837405
 0.97356032 2.94941557 2.16407019 1.92059068]
obj_prev = 24.629944275071566
eta_min = 3.445448803028318e-44	eta_max = 0.9459678027745403
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 5.6197462444887005	eta = 0.9090909090909091
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 14.777404826319295	eta = 0.34572107093955495
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 9.602497695143875	eta = 0.5320345168992939
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.719456153128892	eta = 0.585915008062652
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663852506476184	eta = 0.5896753457475884
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663598265580141	eta = 0.5896926502882287
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.66359826021894	eta = 0.5896926506531418
eta = 0.5896926506531418
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.04358199 0.09166053 0.04289021 0.01487322 0.10584197 0.05049976
 0.01867799 0.06191409 0.04496554 0.04081488]
ene_total = [0.88965002 1.27512756 0.89641868 0.46309031 1.45131187 0.74360323
 0.51049018 1.02200221 0.79540655 0.61649765]
ti_comp = [1.82806619 2.00655166 1.81584028 1.87549598 2.01019846 2.01177704
 1.87637173 1.90759375 1.91580954 2.01460717]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [1.54816501e-06 1.19543613e-05 1.49554405e-06 5.84605015e-08
 1.83390085e-05 1.98878720e-06 1.15673257e-07 4.07639963e-06
 1.54815642e-06 1.04701937e-06]
ene_total = [0.33393439 0.10647147 0.34952357 0.27343533 0.10190267 0.09968125
 0.27231935 0.23255711 0.22204852 0.09606041]
optimize_network iter = 0 obj = 2.0879340736516
eta = 0.5896926506531418
freqs = [11920243.00940963 22840310.78845544 11810016.9072595   3965142.48672103
 26326248.17150466 12551033.48678927  4977157.22105609 16228322.10506473
 11735389.30408466 10129735.66511689]
eta_min = 0.589692650653142	eta_max = 0.7681327803953306
af = 0.00041776276143721885	bf = 0.8564740523773019	zeta = 0.00045953903758094077	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.79380519e-07 2.15727370e-06 2.69884586e-07 1.05497316e-08
 3.30944163e-06 3.58894819e-07 2.08742959e-08 7.35623554e-07
 2.79378970e-07 1.88944211e-07]
ene_total = [1.50710005 0.47998245 1.57746246 1.23411502 0.45906057 0.44980564
 1.22907549 1.04942654 1.00211652 0.43350782]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 1 obj = 3.694218537284266
eta = 0.7681327803953306
freqs = [11840881.82931954 20853986.15252445 11810016.9072595   3842652.168879
 24000712.97449805 11434929.97175277  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340382]
eta_min = 0.7681327803953335	eta_max = 0.7681327803953281
af = 0.0003568274848523184	bf = 0.8564740523773019	zeta = 0.00039251023333755023	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.75672850e-07 1.79837142e-06 2.69884586e-07 9.90799918e-09
 2.75058500e-06 2.97903243e-07 1.95873319e-08 6.69522327e-07
 2.52301484e-07 1.56472000e-07]
ene_total = [1.50709984 0.47996179 1.57746246 1.23411499 0.4590284  0.44980213
 1.22907542 1.04942273 1.00211496 0.43350595]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 2 obj = 3.6942185372842253
eta = 0.7681327803953281
freqs = [11840881.82931953 20853986.15252446 11810016.90725948  3842652.168879
 24000712.97449807 11434929.97175278  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340384]
Done!
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.06640672e-07 4.60982063e-06 6.91803437e-07 2.53974782e-08
 7.05065889e-06 7.63624521e-07 5.02088085e-08 1.71620712e-06
 6.46732134e-07 4.01089477e-07]
ene_total = [0.02618703 0.00834238 0.0274096  0.02144336 0.00798014 0.007816
 0.02135582 0.01823528 0.01741263 0.00753262]
At round 67 energy consumption: 0.16371485787111706
At round 67 eta: 0.7681327803953281
At round 67 a_n: 5.2320311093101175
At round 67 local rounds: 8.637912231455728
At round 67 global rounds: 22.564772710133866
gradient difference: 0.5718132853507996
train() client id: f_00000-0-0 loss: 1.067247  [   32/  126]
train() client id: f_00000-0-1 loss: 1.105416  [   64/  126]
train() client id: f_00000-0-2 loss: 1.095401  [   96/  126]
train() client id: f_00000-1-0 loss: 0.924093  [   32/  126]
train() client id: f_00000-1-1 loss: 1.088941  [   64/  126]
train() client id: f_00000-1-2 loss: 1.084684  [   96/  126]
train() client id: f_00000-2-0 loss: 0.839291  [   32/  126]
train() client id: f_00000-2-1 loss: 0.963322  [   64/  126]
train() client id: f_00000-2-2 loss: 0.970489  [   96/  126]
train() client id: f_00000-3-0 loss: 1.010230  [   32/  126]
train() client id: f_00000-3-1 loss: 0.871210  [   64/  126]
train() client id: f_00000-3-2 loss: 0.888141  [   96/  126]
train() client id: f_00000-4-0 loss: 0.930488  [   32/  126]
train() client id: f_00000-4-1 loss: 0.911913  [   64/  126]
train() client id: f_00000-4-2 loss: 0.913905  [   96/  126]
train() client id: f_00000-5-0 loss: 0.734910  [   32/  126]
train() client id: f_00000-5-1 loss: 1.006412  [   64/  126]
train() client id: f_00000-5-2 loss: 0.983286  [   96/  126]
train() client id: f_00000-6-0 loss: 0.965926  [   32/  126]
train() client id: f_00000-6-1 loss: 0.857616  [   64/  126]
train() client id: f_00000-6-2 loss: 0.881284  [   96/  126]
train() client id: f_00000-7-0 loss: 0.885258  [   32/  126]
train() client id: f_00000-7-1 loss: 0.932513  [   64/  126]
train() client id: f_00000-7-2 loss: 0.785377  [   96/  126]
train() client id: f_00001-0-0 loss: 0.569335  [   32/  265]
train() client id: f_00001-0-1 loss: 0.497681  [   64/  265]
train() client id: f_00001-0-2 loss: 0.489902  [   96/  265]
train() client id: f_00001-0-3 loss: 0.373027  [  128/  265]
train() client id: f_00001-0-4 loss: 0.438595  [  160/  265]
train() client id: f_00001-0-5 loss: 0.465603  [  192/  265]
train() client id: f_00001-0-6 loss: 0.415189  [  224/  265]
train() client id: f_00001-0-7 loss: 0.443290  [  256/  265]
train() client id: f_00001-1-0 loss: 0.484102  [   32/  265]
train() client id: f_00001-1-1 loss: 0.419699  [   64/  265]
train() client id: f_00001-1-2 loss: 0.440134  [   96/  265]
train() client id: f_00001-1-3 loss: 0.392667  [  128/  265]
train() client id: f_00001-1-4 loss: 0.373249  [  160/  265]
train() client id: f_00001-1-5 loss: 0.437782  [  192/  265]
train() client id: f_00001-1-6 loss: 0.402584  [  224/  265]
train() client id: f_00001-1-7 loss: 0.663114  [  256/  265]
train() client id: f_00001-2-0 loss: 0.397393  [   32/  265]
train() client id: f_00001-2-1 loss: 0.480150  [   64/  265]
train() client id: f_00001-2-2 loss: 0.473328  [   96/  265]
train() client id: f_00001-2-3 loss: 0.395037  [  128/  265]
train() client id: f_00001-2-4 loss: 0.461390  [  160/  265]
train() client id: f_00001-2-5 loss: 0.474363  [  192/  265]
train() client id: f_00001-2-6 loss: 0.445052  [  224/  265]
train() client id: f_00001-2-7 loss: 0.476638  [  256/  265]
train() client id: f_00001-3-0 loss: 0.470843  [   32/  265]
train() client id: f_00001-3-1 loss: 0.434897  [   64/  265]
train() client id: f_00001-3-2 loss: 0.422324  [   96/  265]
train() client id: f_00001-3-3 loss: 0.457369  [  128/  265]
train() client id: f_00001-3-4 loss: 0.393622  [  160/  265]
train() client id: f_00001-3-5 loss: 0.517518  [  192/  265]
train() client id: f_00001-3-6 loss: 0.369871  [  224/  265]
train() client id: f_00001-3-7 loss: 0.515121  [  256/  265]
train() client id: f_00001-4-0 loss: 0.504320  [   32/  265]
train() client id: f_00001-4-1 loss: 0.419856  [   64/  265]
train() client id: f_00001-4-2 loss: 0.436795  [   96/  265]
train() client id: f_00001-4-3 loss: 0.365985  [  128/  265]
train() client id: f_00001-4-4 loss: 0.542579  [  160/  265]
train() client id: f_00001-4-5 loss: 0.427228  [  192/  265]
train() client id: f_00001-4-6 loss: 0.504678  [  224/  265]
train() client id: f_00001-4-7 loss: 0.445979  [  256/  265]
train() client id: f_00001-5-0 loss: 0.403401  [   32/  265]
train() client id: f_00001-5-1 loss: 0.558516  [   64/  265]
train() client id: f_00001-5-2 loss: 0.363531  [   96/  265]
train() client id: f_00001-5-3 loss: 0.533794  [  128/  265]
train() client id: f_00001-5-4 loss: 0.445685  [  160/  265]
train() client id: f_00001-5-5 loss: 0.442310  [  192/  265]
train() client id: f_00001-5-6 loss: 0.437783  [  224/  265]
train() client id: f_00001-5-7 loss: 0.460606  [  256/  265]
train() client id: f_00001-6-0 loss: 0.399453  [   32/  265]
train() client id: f_00001-6-1 loss: 0.524218  [   64/  265]
train() client id: f_00001-6-2 loss: 0.587287  [   96/  265]
train() client id: f_00001-6-3 loss: 0.352692  [  128/  265]
train() client id: f_00001-6-4 loss: 0.454798  [  160/  265]
train() client id: f_00001-6-5 loss: 0.350641  [  192/  265]
train() client id: f_00001-6-6 loss: 0.496840  [  224/  265]
train() client id: f_00001-6-7 loss: 0.451530  [  256/  265]
train() client id: f_00001-7-0 loss: 0.374989  [   32/  265]
train() client id: f_00001-7-1 loss: 0.548220  [   64/  265]
train() client id: f_00001-7-2 loss: 0.537383  [   96/  265]
train() client id: f_00001-7-3 loss: 0.388439  [  128/  265]
train() client id: f_00001-7-4 loss: 0.399391  [  160/  265]
train() client id: f_00001-7-5 loss: 0.525077  [  192/  265]
train() client id: f_00001-7-6 loss: 0.468253  [  224/  265]
train() client id: f_00001-7-7 loss: 0.410628  [  256/  265]
train() client id: f_00002-0-0 loss: 0.781679  [   32/  124]
train() client id: f_00002-0-1 loss: 0.839160  [   64/  124]
train() client id: f_00002-0-2 loss: 0.617417  [   96/  124]
train() client id: f_00002-1-0 loss: 0.799027  [   32/  124]
train() client id: f_00002-1-1 loss: 0.639334  [   64/  124]
train() client id: f_00002-1-2 loss: 0.651905  [   96/  124]
train() client id: f_00002-2-0 loss: 0.728257  [   32/  124]
train() client id: f_00002-2-1 loss: 0.637309  [   64/  124]
train() client id: f_00002-2-2 loss: 0.836983  [   96/  124]
train() client id: f_00002-3-0 loss: 0.721956  [   32/  124]
train() client id: f_00002-3-1 loss: 0.486952  [   64/  124]
train() client id: f_00002-3-2 loss: 0.856885  [   96/  124]
train() client id: f_00002-4-0 loss: 0.730698  [   32/  124]
train() client id: f_00002-4-1 loss: 0.827024  [   64/  124]
train() client id: f_00002-4-2 loss: 0.525544  [   96/  124]
train() client id: f_00002-5-0 loss: 0.549890  [   32/  124]
train() client id: f_00002-5-1 loss: 0.581125  [   64/  124]
train() client id: f_00002-5-2 loss: 0.831714  [   96/  124]
train() client id: f_00002-6-0 loss: 0.576926  [   32/  124]
train() client id: f_00002-6-1 loss: 0.732362  [   64/  124]
train() client id: f_00002-6-2 loss: 0.833573  [   96/  124]
train() client id: f_00002-7-0 loss: 0.674872  [   32/  124]
train() client id: f_00002-7-1 loss: 0.542791  [   64/  124]
train() client id: f_00002-7-2 loss: 0.596691  [   96/  124]
train() client id: f_00003-0-0 loss: 0.562387  [   32/   43]
train() client id: f_00003-1-0 loss: 0.463779  [   32/   43]
train() client id: f_00003-2-0 loss: 0.665709  [   32/   43]
train() client id: f_00003-3-0 loss: 0.534773  [   32/   43]
train() client id: f_00003-4-0 loss: 0.716693  [   32/   43]
train() client id: f_00003-5-0 loss: 0.501974  [   32/   43]
train() client id: f_00003-6-0 loss: 0.603940  [   32/   43]
train() client id: f_00003-7-0 loss: 0.647726  [   32/   43]
train() client id: f_00004-0-0 loss: 0.837516  [   32/  306]
train() client id: f_00004-0-1 loss: 0.877511  [   64/  306]
train() client id: f_00004-0-2 loss: 0.916822  [   96/  306]
train() client id: f_00004-0-3 loss: 0.818536  [  128/  306]
train() client id: f_00004-0-4 loss: 0.889024  [  160/  306]
train() client id: f_00004-0-5 loss: 0.893339  [  192/  306]
train() client id: f_00004-0-6 loss: 0.943579  [  224/  306]
train() client id: f_00004-0-7 loss: 0.943198  [  256/  306]
train() client id: f_00004-0-8 loss: 0.700196  [  288/  306]
train() client id: f_00004-1-0 loss: 0.888126  [   32/  306]
train() client id: f_00004-1-1 loss: 0.751491  [   64/  306]
train() client id: f_00004-1-2 loss: 0.694692  [   96/  306]
train() client id: f_00004-1-3 loss: 0.945341  [  128/  306]
train() client id: f_00004-1-4 loss: 0.854009  [  160/  306]
train() client id: f_00004-1-5 loss: 0.777873  [  192/  306]
train() client id: f_00004-1-6 loss: 0.913177  [  224/  306]
train() client id: f_00004-1-7 loss: 0.977763  [  256/  306]
train() client id: f_00004-1-8 loss: 0.965783  [  288/  306]
train() client id: f_00004-2-0 loss: 0.879898  [   32/  306]
train() client id: f_00004-2-1 loss: 0.955534  [   64/  306]
train() client id: f_00004-2-2 loss: 0.880460  [   96/  306]
train() client id: f_00004-2-3 loss: 0.871236  [  128/  306]
train() client id: f_00004-2-4 loss: 0.807438  [  160/  306]
train() client id: f_00004-2-5 loss: 0.680831  [  192/  306]
train() client id: f_00004-2-6 loss: 0.729875  [  224/  306]
train() client id: f_00004-2-7 loss: 0.966907  [  256/  306]
train() client id: f_00004-2-8 loss: 0.908583  [  288/  306]
train() client id: f_00004-3-0 loss: 0.865906  [   32/  306]
train() client id: f_00004-3-1 loss: 0.878289  [   64/  306]
train() client id: f_00004-3-2 loss: 0.773993  [   96/  306]
train() client id: f_00004-3-3 loss: 0.823345  [  128/  306]
train() client id: f_00004-3-4 loss: 0.812647  [  160/  306]
train() client id: f_00004-3-5 loss: 1.004068  [  192/  306]
train() client id: f_00004-3-6 loss: 0.860640  [  224/  306]
train() client id: f_00004-3-7 loss: 0.968423  [  256/  306]
train() client id: f_00004-3-8 loss: 0.853496  [  288/  306]
train() client id: f_00004-4-0 loss: 0.904956  [   32/  306]
train() client id: f_00004-4-1 loss: 1.029357  [   64/  306]
train() client id: f_00004-4-2 loss: 0.724211  [   96/  306]
train() client id: f_00004-4-3 loss: 0.819996  [  128/  306]
train() client id: f_00004-4-4 loss: 0.875966  [  160/  306]
train() client id: f_00004-4-5 loss: 0.841867  [  192/  306]
train() client id: f_00004-4-6 loss: 0.822818  [  224/  306]
train() client id: f_00004-4-7 loss: 0.958907  [  256/  306]
train() client id: f_00004-4-8 loss: 0.830053  [  288/  306]
train() client id: f_00004-5-0 loss: 0.892959  [   32/  306]
train() client id: f_00004-5-1 loss: 0.793689  [   64/  306]
train() client id: f_00004-5-2 loss: 0.876516  [   96/  306]
train() client id: f_00004-5-3 loss: 0.795989  [  128/  306]
train() client id: f_00004-5-4 loss: 0.872805  [  160/  306]
train() client id: f_00004-5-5 loss: 0.963580  [  192/  306]
train() client id: f_00004-5-6 loss: 0.916912  [  224/  306]
train() client id: f_00004-5-7 loss: 0.925341  [  256/  306]
train() client id: f_00004-5-8 loss: 0.835386  [  288/  306]
train() client id: f_00004-6-0 loss: 0.881403  [   32/  306]
train() client id: f_00004-6-1 loss: 0.944532  [   64/  306]
train() client id: f_00004-6-2 loss: 0.931088  [   96/  306]
train() client id: f_00004-6-3 loss: 0.880499  [  128/  306]
train() client id: f_00004-6-4 loss: 0.852092  [  160/  306]
train() client id: f_00004-6-5 loss: 0.850834  [  192/  306]
train() client id: f_00004-6-6 loss: 0.848057  [  224/  306]
train() client id: f_00004-6-7 loss: 0.915495  [  256/  306]
train() client id: f_00004-6-8 loss: 0.806819  [  288/  306]
train() client id: f_00004-7-0 loss: 0.855364  [   32/  306]
train() client id: f_00004-7-1 loss: 0.904812  [   64/  306]
train() client id: f_00004-7-2 loss: 0.861539  [   96/  306]
train() client id: f_00004-7-3 loss: 1.040004  [  128/  306]
train() client id: f_00004-7-4 loss: 0.780910  [  160/  306]
train() client id: f_00004-7-5 loss: 0.909874  [  192/  306]
train() client id: f_00004-7-6 loss: 0.855365  [  224/  306]
train() client id: f_00004-7-7 loss: 0.845811  [  256/  306]
train() client id: f_00004-7-8 loss: 0.837918  [  288/  306]
train() client id: f_00005-0-0 loss: 0.670195  [   32/  146]
train() client id: f_00005-0-1 loss: 0.793944  [   64/  146]
train() client id: f_00005-0-2 loss: 1.064797  [   96/  146]
train() client id: f_00005-0-3 loss: 0.655042  [  128/  146]
train() client id: f_00005-1-0 loss: 0.792569  [   32/  146]
train() client id: f_00005-1-1 loss: 0.884150  [   64/  146]
train() client id: f_00005-1-2 loss: 0.855927  [   96/  146]
train() client id: f_00005-1-3 loss: 0.642476  [  128/  146]
train() client id: f_00005-2-0 loss: 0.787952  [   32/  146]
train() client id: f_00005-2-1 loss: 0.777169  [   64/  146]
train() client id: f_00005-2-2 loss: 0.613537  [   96/  146]
train() client id: f_00005-2-3 loss: 0.620244  [  128/  146]
train() client id: f_00005-3-0 loss: 0.777794  [   32/  146]
train() client id: f_00005-3-1 loss: 0.677108  [   64/  146]
train() client id: f_00005-3-2 loss: 0.859829  [   96/  146]
train() client id: f_00005-3-3 loss: 0.766551  [  128/  146]
train() client id: f_00005-4-0 loss: 0.807393  [   32/  146]
train() client id: f_00005-4-1 loss: 0.972249  [   64/  146]
train() client id: f_00005-4-2 loss: 0.658751  [   96/  146]
train() client id: f_00005-4-3 loss: 0.637934  [  128/  146]
train() client id: f_00005-5-0 loss: 0.671338  [   32/  146]
train() client id: f_00005-5-1 loss: 0.984848  [   64/  146]
train() client id: f_00005-5-2 loss: 0.679398  [   96/  146]
train() client id: f_00005-5-3 loss: 0.830223  [  128/  146]
train() client id: f_00005-6-0 loss: 0.768561  [   32/  146]
train() client id: f_00005-6-1 loss: 0.543050  [   64/  146]
train() client id: f_00005-6-2 loss: 0.863999  [   96/  146]
train() client id: f_00005-6-3 loss: 0.901009  [  128/  146]
train() client id: f_00005-7-0 loss: 0.878247  [   32/  146]
train() client id: f_00005-7-1 loss: 0.886824  [   64/  146]
train() client id: f_00005-7-2 loss: 0.769795  [   96/  146]
train() client id: f_00005-7-3 loss: 0.596053  [  128/  146]
train() client id: f_00006-0-0 loss: 0.547944  [   32/   54]
train() client id: f_00006-1-0 loss: 0.590690  [   32/   54]
train() client id: f_00006-2-0 loss: 0.543290  [   32/   54]
train() client id: f_00006-3-0 loss: 0.605871  [   32/   54]
train() client id: f_00006-4-0 loss: 0.541372  [   32/   54]
train() client id: f_00006-5-0 loss: 0.606013  [   32/   54]
train() client id: f_00006-6-0 loss: 0.546160  [   32/   54]
train() client id: f_00006-7-0 loss: 0.544381  [   32/   54]
train() client id: f_00007-0-0 loss: 0.472819  [   32/  179]
train() client id: f_00007-0-1 loss: 0.551895  [   64/  179]
train() client id: f_00007-0-2 loss: 0.503498  [   96/  179]
train() client id: f_00007-0-3 loss: 0.994540  [  128/  179]
train() client id: f_00007-0-4 loss: 0.662815  [  160/  179]
train() client id: f_00007-1-0 loss: 0.680527  [   32/  179]
train() client id: f_00007-1-1 loss: 0.670662  [   64/  179]
train() client id: f_00007-1-2 loss: 0.500830  [   96/  179]
train() client id: f_00007-1-3 loss: 0.575938  [  128/  179]
train() client id: f_00007-1-4 loss: 0.688498  [  160/  179]
train() client id: f_00007-2-0 loss: 0.700355  [   32/  179]
train() client id: f_00007-2-1 loss: 0.626727  [   64/  179]
train() client id: f_00007-2-2 loss: 0.639027  [   96/  179]
train() client id: f_00007-2-3 loss: 0.550764  [  128/  179]
train() client id: f_00007-2-4 loss: 0.425000  [  160/  179]
train() client id: f_00007-3-0 loss: 0.536866  [   32/  179]
train() client id: f_00007-3-1 loss: 0.464950  [   64/  179]
train() client id: f_00007-3-2 loss: 0.737798  [   96/  179]
train() client id: f_00007-3-3 loss: 0.459358  [  128/  179]
train() client id: f_00007-3-4 loss: 0.805611  [  160/  179]
train() client id: f_00007-4-0 loss: 0.762402  [   32/  179]
train() client id: f_00007-4-1 loss: 0.416268  [   64/  179]
train() client id: f_00007-4-2 loss: 0.838320  [   96/  179]
train() client id: f_00007-4-3 loss: 0.487317  [  128/  179]
train() client id: f_00007-4-4 loss: 0.491330  [  160/  179]
train() client id: f_00007-5-0 loss: 0.616499  [   32/  179]
train() client id: f_00007-5-1 loss: 0.438070  [   64/  179]
train() client id: f_00007-5-2 loss: 0.482683  [   96/  179]
train() client id: f_00007-5-3 loss: 0.889504  [  128/  179]
train() client id: f_00007-5-4 loss: 0.496429  [  160/  179]
train() client id: f_00007-6-0 loss: 0.745346  [   32/  179]
train() client id: f_00007-6-1 loss: 0.672059  [   64/  179]
train() client id: f_00007-6-2 loss: 0.514340  [   96/  179]
train() client id: f_00007-6-3 loss: 0.466468  [  128/  179]
train() client id: f_00007-6-4 loss: 0.591332  [  160/  179]
train() client id: f_00007-7-0 loss: 0.447368  [   32/  179]
train() client id: f_00007-7-1 loss: 0.718430  [   64/  179]
train() client id: f_00007-7-2 loss: 0.496351  [   96/  179]
train() client id: f_00007-7-3 loss: 0.565957  [  128/  179]
train() client id: f_00007-7-4 loss: 0.660500  [  160/  179]
train() client id: f_00008-0-0 loss: 0.755109  [   32/  130]
train() client id: f_00008-0-1 loss: 0.600372  [   64/  130]
train() client id: f_00008-0-2 loss: 0.797436  [   96/  130]
train() client id: f_00008-0-3 loss: 0.925907  [  128/  130]
train() client id: f_00008-1-0 loss: 0.774025  [   32/  130]
train() client id: f_00008-1-1 loss: 0.838314  [   64/  130]
train() client id: f_00008-1-2 loss: 0.788917  [   96/  130]
train() client id: f_00008-1-3 loss: 0.698617  [  128/  130]
train() client id: f_00008-2-0 loss: 0.911738  [   32/  130]
train() client id: f_00008-2-1 loss: 0.738036  [   64/  130]
train() client id: f_00008-2-2 loss: 0.707264  [   96/  130]
train() client id: f_00008-2-3 loss: 0.743324  [  128/  130]
train() client id: f_00008-3-0 loss: 0.849944  [   32/  130]
train() client id: f_00008-3-1 loss: 0.805448  [   64/  130]
train() client id: f_00008-3-2 loss: 0.717087  [   96/  130]
train() client id: f_00008-3-3 loss: 0.739091  [  128/  130]
train() client id: f_00008-4-0 loss: 0.786961  [   32/  130]
train() client id: f_00008-4-1 loss: 0.736025  [   64/  130]
train() client id: f_00008-4-2 loss: 0.775636  [   96/  130]
train() client id: f_00008-4-3 loss: 0.803014  [  128/  130]
train() client id: f_00008-5-0 loss: 0.839227  [   32/  130]
train() client id: f_00008-5-1 loss: 0.756028  [   64/  130]
train() client id: f_00008-5-2 loss: 0.814123  [   96/  130]
train() client id: f_00008-5-3 loss: 0.657030  [  128/  130]
train() client id: f_00008-6-0 loss: 0.743924  [   32/  130]
train() client id: f_00008-6-1 loss: 0.867540  [   64/  130]
train() client id: f_00008-6-2 loss: 0.777086  [   96/  130]
train() client id: f_00008-6-3 loss: 0.720511  [  128/  130]
train() client id: f_00008-7-0 loss: 0.722258  [   32/  130]
train() client id: f_00008-7-1 loss: 0.796883  [   64/  130]
train() client id: f_00008-7-2 loss: 0.816939  [   96/  130]
train() client id: f_00008-7-3 loss: 0.766387  [  128/  130]
train() client id: f_00009-0-0 loss: 0.969759  [   32/  118]
train() client id: f_00009-0-1 loss: 0.912998  [   64/  118]
train() client id: f_00009-0-2 loss: 0.625773  [   96/  118]
train() client id: f_00009-1-0 loss: 0.745584  [   32/  118]
train() client id: f_00009-1-1 loss: 0.905418  [   64/  118]
train() client id: f_00009-1-2 loss: 0.749646  [   96/  118]
train() client id: f_00009-2-0 loss: 0.781132  [   32/  118]
train() client id: f_00009-2-1 loss: 0.668280  [   64/  118]
train() client id: f_00009-2-2 loss: 0.794003  [   96/  118]
train() client id: f_00009-3-0 loss: 0.787755  [   32/  118]
train() client id: f_00009-3-1 loss: 0.704246  [   64/  118]
train() client id: f_00009-3-2 loss: 0.892870  [   96/  118]
train() client id: f_00009-4-0 loss: 0.729334  [   32/  118]
train() client id: f_00009-4-1 loss: 0.818342  [   64/  118]
train() client id: f_00009-4-2 loss: 0.789696  [   96/  118]
train() client id: f_00009-5-0 loss: 0.781095  [   32/  118]
train() client id: f_00009-5-1 loss: 0.751032  [   64/  118]
train() client id: f_00009-5-2 loss: 0.744004  [   96/  118]
train() client id: f_00009-6-0 loss: 0.744846  [   32/  118]
train() client id: f_00009-6-1 loss: 0.596239  [   64/  118]
train() client id: f_00009-6-2 loss: 0.784620  [   96/  118]
train() client id: f_00009-7-0 loss: 0.864209  [   32/  118]
train() client id: f_00009-7-1 loss: 0.804436  [   64/  118]
train() client id: f_00009-7-2 loss: 0.632068  [   96/  118]
At round 67 accuracy: 0.6472148541114059
At round 67 training accuracy: 0.5888665325285044
At round 67 training loss: 0.8303487225381746
update_location
xs = [  -3.9056584     4.20031788  355.00902392   18.81129433    0.97929623
    3.95640986 -317.44319194 -296.32485185  339.66397685 -282.06087855]
ys = [ 347.5879595   330.55583871    1.32061395 -317.45517586  309.35018685
  292.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [361.70795368 345.37632399 368.8267223  333.36414546 325.11305283
 309.44430849 332.83189548 312.74445491 354.51415648 299.28974434]
dists_bs = [243.02238063 237.29806759 557.74016761 529.22104837 221.40163822
 214.25585347 227.68552432 212.26769452 538.20398264 201.74866778]
uav_gains = [1.05200963e-12 1.28407742e-12 9.72759726e-13 1.51715294e-12
 1.72031827e-12 2.24199922e-12 1.52907249e-12 2.11432101e-12
 1.14444349e-12 2.70950730e-12]
bs_gains = [2.30932218e-11 2.46871139e-11 2.25571746e-12 2.61282415e-12
 2.99769719e-11 3.28611333e-11 2.77175544e-11 3.37302187e-11
 2.49254349e-12 3.88887688e-11]
Round 68
-------------------------------
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.01021389 3.98881553 1.98649518 0.75024676 4.59768714 2.21362175
 0.91366287 2.75982094 2.02332361 1.79514037]
obj_prev = 23.039028040351884
eta_min = nan	eta_max = nan
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 5.251816334124527	eta = 0.9090909090909091
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 14.045450908523552	eta = 0.3399234753417846
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 9.049757744866326	eta = 0.5275697560275713
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.202066092537866	eta = 0.5820946127112314
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148578542585753	eta = 0.5859155017793716
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332307464996	eta = 0.5859332076078638
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332302200442	eta = 0.5859332079864292
eta = 0.5859332079864292
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.04410974 0.09277049 0.04340959 0.01505332 0.10712366 0.05111129
 0.01890417 0.06266384 0.04551005 0.04130912]
ene_total = [0.83982608 1.19483484 0.84607581 0.44015542 1.35993302 0.69656538
 0.48459427 0.96365584 0.74525191 0.57743974]
ti_comp = [1.98249026 2.16845803 1.97019612 2.03038779 2.17217608 2.17382626
 2.03126319 2.06316582 2.07658364 2.17668634]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [1.36477616e-06 1.06122506e-05 1.31709789e-06 5.17153298e-08
 1.62834394e-05 1.76595985e-06 1.02334201e-07 3.61295660e-06
 1.36616489e-06 9.29878567e-07]
ene_total = [0.31897471 0.09948487 0.33349161 0.26239966 0.0951614  0.09304137
 0.26136655 0.22373596 0.20786507 0.08965418]
optimize_network iter = 0 obj = 1.9851753783046104
eta = 0.5859332079864292
freqs = [11124831.81396149 21390888.52316821 11016564.88463251  3707007.23591558
 24658143.18962516 11756065.47830876  4653305.14511856 15186331.26526243
 10957914.22550441  9488993.01846851]
eta_min = 0.5859332079864303	eta_max = 0.7781376768963073
af = 0.0003421552139497174	bf = 0.8218123015887785	zeta = 0.00037637073534468917	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.43339611e-07 1.89216446e-06 2.34838575e-07 9.22084412e-09
 2.90333750e-06 3.14870668e-07 1.82461896e-08 6.44190219e-07
 2.43587222e-07 1.65797363e-07]
ene_total = [1.45278485 0.4526582  1.5189076  1.19515856 0.43271528 0.42370069
 1.19045077 1.01889828 0.94670988 0.40830993]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 1 obj = 3.7044806936267416
eta = 0.7781376768963073
freqs = [11047352.84864209 19386314.72442644 11016564.8846325   3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166095
 10357782.55225607  8569597.11605592]
eta_min = 0.7781376768963095	eta_max = 0.7781376768963063
af = 0.0002885807068502459	bf = 0.8218123015887785	zeta = 0.0003174387775352705	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.39961934e-07 1.55414576e-06 2.34838575e-07 8.63230839e-09
 2.37709741e-06 2.57436357e-07 1.70659737e-08 5.83230415e-07
 2.17636778e-07 1.35225369e-07]
ene_total = [1.45278467 0.45264002 1.5189076  1.19515853 0.43268697 0.4236976
 1.19045071 1.018895   0.94670848 0.40830829]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 2 obj = 3.7044806936267247
eta = 0.7781376768963063
freqs = [11047352.84864208 19386314.72442644 11016564.88463249  3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166094
 10357782.55225607  8569597.11605592]
Done!
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.15101785e-07 3.98378951e-06 6.01968921e-07 2.21274609e-08
 6.09328669e-06 6.59894509e-07 4.37457339e-08 1.49501241e-06
 5.57875029e-07 3.46627335e-07]
ene_total = [0.02701167 0.00841826 0.02824107 0.02222133 0.00804857 0.00787812
 0.02213381 0.018945   0.01760228 0.00759179]
At round 68 energy consumption: 0.16809188719852586
At round 68 eta: 0.7781376768963063
At round 68 a_n: 4.8894852623407985
At round 68 local rounds: 8.214162669625793
At round 68 global rounds: 22.038375844715002
gradient difference: 0.5705627202987671
train() client id: f_00000-0-0 loss: 0.966063  [   32/  126]
train() client id: f_00000-0-1 loss: 1.176851  [   64/  126]
train() client id: f_00000-0-2 loss: 1.106763  [   96/  126]
train() client id: f_00000-1-0 loss: 1.038137  [   32/  126]
train() client id: f_00000-1-1 loss: 0.991240  [   64/  126]
train() client id: f_00000-1-2 loss: 1.229737  [   96/  126]
train() client id: f_00000-2-0 loss: 1.018805  [   32/  126]
train() client id: f_00000-2-1 loss: 1.073901  [   64/  126]
train() client id: f_00000-2-2 loss: 0.914349  [   96/  126]
train() client id: f_00000-3-0 loss: 0.922352  [   32/  126]
train() client id: f_00000-3-1 loss: 0.966670  [   64/  126]
train() client id: f_00000-3-2 loss: 0.980033  [   96/  126]
train() client id: f_00000-4-0 loss: 1.056599  [   32/  126]
train() client id: f_00000-4-1 loss: 0.884011  [   64/  126]
train() client id: f_00000-4-2 loss: 0.907463  [   96/  126]
train() client id: f_00000-5-0 loss: 0.942875  [   32/  126]
train() client id: f_00000-5-1 loss: 0.898396  [   64/  126]
train() client id: f_00000-5-2 loss: 0.885855  [   96/  126]
train() client id: f_00000-6-0 loss: 0.924112  [   32/  126]
train() client id: f_00000-6-1 loss: 0.884379  [   64/  126]
train() client id: f_00000-6-2 loss: 0.909072  [   96/  126]
train() client id: f_00000-7-0 loss: 1.030821  [   32/  126]
train() client id: f_00000-7-1 loss: 0.809586  [   64/  126]
train() client id: f_00000-7-2 loss: 0.874749  [   96/  126]
train() client id: f_00001-0-0 loss: 0.451813  [   32/  265]
train() client id: f_00001-0-1 loss: 0.484510  [   64/  265]
train() client id: f_00001-0-2 loss: 0.506801  [   96/  265]
train() client id: f_00001-0-3 loss: 0.477431  [  128/  265]
train() client id: f_00001-0-4 loss: 0.492869  [  160/  265]
train() client id: f_00001-0-5 loss: 0.615585  [  192/  265]
train() client id: f_00001-0-6 loss: 0.582543  [  224/  265]
train() client id: f_00001-0-7 loss: 0.575585  [  256/  265]
train() client id: f_00001-1-0 loss: 0.529323  [   32/  265]
train() client id: f_00001-1-1 loss: 0.495891  [   64/  265]
train() client id: f_00001-1-2 loss: 0.611898  [   96/  265]
train() client id: f_00001-1-3 loss: 0.555049  [  128/  265]
train() client id: f_00001-1-4 loss: 0.498382  [  160/  265]
train() client id: f_00001-1-5 loss: 0.495731  [  192/  265]
train() client id: f_00001-1-6 loss: 0.419492  [  224/  265]
train() client id: f_00001-1-7 loss: 0.539535  [  256/  265]
train() client id: f_00001-2-0 loss: 0.532355  [   32/  265]
train() client id: f_00001-2-1 loss: 0.567154  [   64/  265]
train() client id: f_00001-2-2 loss: 0.459164  [   96/  265]
train() client id: f_00001-2-3 loss: 0.559300  [  128/  265]
train() client id: f_00001-2-4 loss: 0.522876  [  160/  265]
train() client id: f_00001-2-5 loss: 0.435920  [  192/  265]
train() client id: f_00001-2-6 loss: 0.503204  [  224/  265]
train() client id: f_00001-2-7 loss: 0.472245  [  256/  265]
train() client id: f_00001-3-0 loss: 0.460255  [   32/  265]
train() client id: f_00001-3-1 loss: 0.636948  [   64/  265]
train() client id: f_00001-3-2 loss: 0.605915  [   96/  265]
train() client id: f_00001-3-3 loss: 0.474859  [  128/  265]
train() client id: f_00001-3-4 loss: 0.580576  [  160/  265]
train() client id: f_00001-3-5 loss: 0.394025  [  192/  265]
train() client id: f_00001-3-6 loss: 0.475720  [  224/  265]
train() client id: f_00001-3-7 loss: 0.459486  [  256/  265]
train() client id: f_00001-4-0 loss: 0.530073  [   32/  265]
train() client id: f_00001-4-1 loss: 0.481633  [   64/  265]
train() client id: f_00001-4-2 loss: 0.450918  [   96/  265]
train() client id: f_00001-4-3 loss: 0.463493  [  128/  265]
train() client id: f_00001-4-4 loss: 0.615881  [  160/  265]
train() client id: f_00001-4-5 loss: 0.463530  [  192/  265]
train() client id: f_00001-4-6 loss: 0.568094  [  224/  265]
train() client id: f_00001-4-7 loss: 0.417251  [  256/  265]
train() client id: f_00001-5-0 loss: 0.481092  [   32/  265]
train() client id: f_00001-5-1 loss: 0.500705  [   64/  265]
train() client id: f_00001-5-2 loss: 0.485691  [   96/  265]
train() client id: f_00001-5-3 loss: 0.426103  [  128/  265]
train() client id: f_00001-5-4 loss: 0.418263  [  160/  265]
train() client id: f_00001-5-5 loss: 0.556515  [  192/  265]
train() client id: f_00001-5-6 loss: 0.524353  [  224/  265]
train() client id: f_00001-5-7 loss: 0.546674  [  256/  265]
train() client id: f_00001-6-0 loss: 0.602268  [   32/  265]
train() client id: f_00001-6-1 loss: 0.609025  [   64/  265]
train() client id: f_00001-6-2 loss: 0.426325  [   96/  265]
train() client id: f_00001-6-3 loss: 0.462263  [  128/  265]
train() client id: f_00001-6-4 loss: 0.447866  [  160/  265]
train() client id: f_00001-6-5 loss: 0.437071  [  192/  265]
train() client id: f_00001-6-6 loss: 0.495191  [  224/  265]
train() client id: f_00001-6-7 loss: 0.516037  [  256/  265]
train() client id: f_00001-7-0 loss: 0.556202  [   32/  265]
train() client id: f_00001-7-1 loss: 0.429917  [   64/  265]
train() client id: f_00001-7-2 loss: 0.608542  [   96/  265]
train() client id: f_00001-7-3 loss: 0.487255  [  128/  265]
train() client id: f_00001-7-4 loss: 0.465406  [  160/  265]
train() client id: f_00001-7-5 loss: 0.409076  [  192/  265]
train() client id: f_00001-7-6 loss: 0.448460  [  224/  265]
train() client id: f_00001-7-7 loss: 0.567907  [  256/  265]
train() client id: f_00002-0-0 loss: 0.974791  [   32/  124]
train() client id: f_00002-0-1 loss: 1.209559  [   64/  124]
train() client id: f_00002-0-2 loss: 0.961503  [   96/  124]
train() client id: f_00002-1-0 loss: 0.978368  [   32/  124]
train() client id: f_00002-1-1 loss: 1.009318  [   64/  124]
train() client id: f_00002-1-2 loss: 1.287056  [   96/  124]
train() client id: f_00002-2-0 loss: 1.139462  [   32/  124]
train() client id: f_00002-2-1 loss: 0.915173  [   64/  124]
train() client id: f_00002-2-2 loss: 1.199061  [   96/  124]
train() client id: f_00002-3-0 loss: 1.146883  [   32/  124]
train() client id: f_00002-3-1 loss: 0.967024  [   64/  124]
train() client id: f_00002-3-2 loss: 1.125926  [   96/  124]
train() client id: f_00002-4-0 loss: 1.136100  [   32/  124]
train() client id: f_00002-4-1 loss: 0.975556  [   64/  124]
train() client id: f_00002-4-2 loss: 1.018962  [   96/  124]
train() client id: f_00002-5-0 loss: 0.988684  [   32/  124]
train() client id: f_00002-5-1 loss: 1.243668  [   64/  124]
train() client id: f_00002-5-2 loss: 0.853233  [   96/  124]
train() client id: f_00002-6-0 loss: 1.193720  [   32/  124]
train() client id: f_00002-6-1 loss: 0.990176  [   64/  124]
train() client id: f_00002-6-2 loss: 0.917035  [   96/  124]
train() client id: f_00002-7-0 loss: 0.923399  [   32/  124]
train() client id: f_00002-7-1 loss: 1.057742  [   64/  124]
train() client id: f_00002-7-2 loss: 1.117344  [   96/  124]
train() client id: f_00003-0-0 loss: 0.508042  [   32/   43]
train() client id: f_00003-1-0 loss: 0.533188  [   32/   43]
train() client id: f_00003-2-0 loss: 0.432220  [   32/   43]
train() client id: f_00003-3-0 loss: 0.606614  [   32/   43]
train() client id: f_00003-4-0 loss: 0.452778  [   32/   43]
train() client id: f_00003-5-0 loss: 0.350070  [   32/   43]
train() client id: f_00003-6-0 loss: 0.576791  [   32/   43]
train() client id: f_00003-7-0 loss: 0.420575  [   32/   43]
train() client id: f_00004-0-0 loss: 0.908985  [   32/  306]
train() client id: f_00004-0-1 loss: 0.851560  [   64/  306]
train() client id: f_00004-0-2 loss: 1.034855  [   96/  306]
train() client id: f_00004-0-3 loss: 0.846707  [  128/  306]
train() client id: f_00004-0-4 loss: 0.887676  [  160/  306]
train() client id: f_00004-0-5 loss: 0.784810  [  192/  306]
train() client id: f_00004-0-6 loss: 0.742306  [  224/  306]
train() client id: f_00004-0-7 loss: 0.861499  [  256/  306]
train() client id: f_00004-0-8 loss: 0.729330  [  288/  306]
train() client id: f_00004-1-0 loss: 0.778904  [   32/  306]
train() client id: f_00004-1-1 loss: 0.722097  [   64/  306]
train() client id: f_00004-1-2 loss: 0.901955  [   96/  306]
train() client id: f_00004-1-3 loss: 0.801005  [  128/  306]
train() client id: f_00004-1-4 loss: 0.812848  [  160/  306]
train() client id: f_00004-1-5 loss: 0.734420  [  192/  306]
train() client id: f_00004-1-6 loss: 0.938159  [  224/  306]
train() client id: f_00004-1-7 loss: 0.926100  [  256/  306]
train() client id: f_00004-1-8 loss: 0.851825  [  288/  306]
train() client id: f_00004-2-0 loss: 0.827976  [   32/  306]
train() client id: f_00004-2-1 loss: 0.885432  [   64/  306]
train() client id: f_00004-2-2 loss: 0.794775  [   96/  306]
train() client id: f_00004-2-3 loss: 0.751086  [  128/  306]
train() client id: f_00004-2-4 loss: 0.912706  [  160/  306]
train() client id: f_00004-2-5 loss: 0.806188  [  192/  306]
train() client id: f_00004-2-6 loss: 0.756090  [  224/  306]
train() client id: f_00004-2-7 loss: 0.712035  [  256/  306]
train() client id: f_00004-2-8 loss: 1.034612  [  288/  306]
train() client id: f_00004-3-0 loss: 0.811535  [   32/  306]
train() client id: f_00004-3-1 loss: 0.720424  [   64/  306]
train() client id: f_00004-3-2 loss: 0.882082  [   96/  306]
train() client id: f_00004-3-3 loss: 0.743749  [  128/  306]
train() client id: f_00004-3-4 loss: 0.931944  [  160/  306]
train() client id: f_00004-3-5 loss: 0.776605  [  192/  306]
train() client id: f_00004-3-6 loss: 0.861247  [  224/  306]
train() client id: f_00004-3-7 loss: 0.726239  [  256/  306]
train() client id: f_00004-3-8 loss: 0.984425  [  288/  306]
train() client id: f_00004-4-0 loss: 0.825390  [   32/  306]
train() client id: f_00004-4-1 loss: 0.688865  [   64/  306]
train() client id: f_00004-4-2 loss: 0.811285  [   96/  306]
train() client id: f_00004-4-3 loss: 0.879045  [  128/  306]
train() client id: f_00004-4-4 loss: 0.894522  [  160/  306]
train() client id: f_00004-4-5 loss: 0.852365  [  192/  306]
train() client id: f_00004-4-6 loss: 0.892612  [  224/  306]
train() client id: f_00004-4-7 loss: 0.773041  [  256/  306]
train() client id: f_00004-4-8 loss: 0.842196  [  288/  306]
train() client id: f_00004-5-0 loss: 0.756244  [   32/  306]
train() client id: f_00004-5-1 loss: 0.830926  [   64/  306]
train() client id: f_00004-5-2 loss: 0.787701  [   96/  306]
train() client id: f_00004-5-3 loss: 0.960755  [  128/  306]
train() client id: f_00004-5-4 loss: 0.764341  [  160/  306]
train() client id: f_00004-5-5 loss: 0.704999  [  192/  306]
train() client id: f_00004-5-6 loss: 0.833959  [  224/  306]
train() client id: f_00004-5-7 loss: 0.897978  [  256/  306]
train() client id: f_00004-5-8 loss: 0.968972  [  288/  306]
train() client id: f_00004-6-0 loss: 0.840383  [   32/  306]
train() client id: f_00004-6-1 loss: 0.658515  [   64/  306]
train() client id: f_00004-6-2 loss: 0.821680  [   96/  306]
train() client id: f_00004-6-3 loss: 0.847015  [  128/  306]
train() client id: f_00004-6-4 loss: 0.818561  [  160/  306]
train() client id: f_00004-6-5 loss: 0.925567  [  192/  306]
train() client id: f_00004-6-6 loss: 0.779331  [  224/  306]
train() client id: f_00004-6-7 loss: 0.862684  [  256/  306]
train() client id: f_00004-6-8 loss: 0.812645  [  288/  306]
train() client id: f_00004-7-0 loss: 0.874840  [   32/  306]
train() client id: f_00004-7-1 loss: 0.885989  [   64/  306]
train() client id: f_00004-7-2 loss: 0.863823  [   96/  306]
train() client id: f_00004-7-3 loss: 0.811786  [  128/  306]
train() client id: f_00004-7-4 loss: 0.783389  [  160/  306]
train() client id: f_00004-7-5 loss: 0.886200  [  192/  306]
train() client id: f_00004-7-6 loss: 0.764507  [  224/  306]
train() client id: f_00004-7-7 loss: 0.798569  [  256/  306]
train() client id: f_00004-7-8 loss: 0.833054  [  288/  306]
train() client id: f_00005-0-0 loss: 0.729476  [   32/  146]
train() client id: f_00005-0-1 loss: 1.037047  [   64/  146]
train() client id: f_00005-0-2 loss: 0.581571  [   96/  146]
train() client id: f_00005-0-3 loss: 0.610978  [  128/  146]
train() client id: f_00005-1-0 loss: 0.701465  [   32/  146]
train() client id: f_00005-1-1 loss: 0.609543  [   64/  146]
train() client id: f_00005-1-2 loss: 0.975778  [   96/  146]
train() client id: f_00005-1-3 loss: 0.546505  [  128/  146]
train() client id: f_00005-2-0 loss: 0.664021  [   32/  146]
train() client id: f_00005-2-1 loss: 0.745448  [   64/  146]
train() client id: f_00005-2-2 loss: 0.865096  [   96/  146]
train() client id: f_00005-2-3 loss: 0.922730  [  128/  146]
train() client id: f_00005-3-0 loss: 0.702931  [   32/  146]
train() client id: f_00005-3-1 loss: 0.404873  [   64/  146]
train() client id: f_00005-3-2 loss: 0.748265  [   96/  146]
train() client id: f_00005-3-3 loss: 0.986407  [  128/  146]
train() client id: f_00005-4-0 loss: 0.625600  [   32/  146]
train() client id: f_00005-4-1 loss: 0.601223  [   64/  146]
train() client id: f_00005-4-2 loss: 0.671229  [   96/  146]
train() client id: f_00005-4-3 loss: 1.301547  [  128/  146]
train() client id: f_00005-5-0 loss: 0.706383  [   32/  146]
train() client id: f_00005-5-1 loss: 0.939516  [   64/  146]
train() client id: f_00005-5-2 loss: 0.850194  [   96/  146]
train() client id: f_00005-5-3 loss: 0.655220  [  128/  146]
train() client id: f_00005-6-0 loss: 0.759614  [   32/  146]
train() client id: f_00005-6-1 loss: 0.733904  [   64/  146]
train() client id: f_00005-6-2 loss: 0.648736  [   96/  146]
train() client id: f_00005-6-3 loss: 1.007650  [  128/  146]
train() client id: f_00005-7-0 loss: 0.706573  [   32/  146]
train() client id: f_00005-7-1 loss: 0.744210  [   64/  146]
train() client id: f_00005-7-2 loss: 0.668265  [   96/  146]
train() client id: f_00005-7-3 loss: 0.767050  [  128/  146]
train() client id: f_00006-0-0 loss: 0.527518  [   32/   54]
train() client id: f_00006-1-0 loss: 0.494822  [   32/   54]
train() client id: f_00006-2-0 loss: 0.492533  [   32/   54]
train() client id: f_00006-3-0 loss: 0.536624  [   32/   54]
train() client id: f_00006-4-0 loss: 0.487948  [   32/   54]
train() client id: f_00006-5-0 loss: 0.537058  [   32/   54]
train() client id: f_00006-6-0 loss: 0.571779  [   32/   54]
train() client id: f_00006-7-0 loss: 0.543957  [   32/   54]
train() client id: f_00007-0-0 loss: 0.685660  [   32/  179]
train() client id: f_00007-0-1 loss: 0.577827  [   64/  179]
train() client id: f_00007-0-2 loss: 0.665376  [   96/  179]
train() client id: f_00007-0-3 loss: 0.504920  [  128/  179]
train() client id: f_00007-0-4 loss: 0.502081  [  160/  179]
train() client id: f_00007-1-0 loss: 0.454958  [   32/  179]
train() client id: f_00007-1-1 loss: 0.634299  [   64/  179]
train() client id: f_00007-1-2 loss: 0.573968  [   96/  179]
train() client id: f_00007-1-3 loss: 0.723625  [  128/  179]
train() client id: f_00007-1-4 loss: 0.525510  [  160/  179]
train() client id: f_00007-2-0 loss: 0.448989  [   32/  179]
train() client id: f_00007-2-1 loss: 0.537024  [   64/  179]
train() client id: f_00007-2-2 loss: 0.594365  [   96/  179]
train() client id: f_00007-2-3 loss: 0.769592  [  128/  179]
train() client id: f_00007-2-4 loss: 0.619653  [  160/  179]
train() client id: f_00007-3-0 loss: 0.513830  [   32/  179]
train() client id: f_00007-3-1 loss: 0.576019  [   64/  179]
train() client id: f_00007-3-2 loss: 0.751225  [   96/  179]
train() client id: f_00007-3-3 loss: 0.584572  [  128/  179]
train() client id: f_00007-3-4 loss: 0.509981  [  160/  179]
train() client id: f_00007-4-0 loss: 0.634203  [   32/  179]
train() client id: f_00007-4-1 loss: 0.500058  [   64/  179]
train() client id: f_00007-4-2 loss: 0.506721  [   96/  179]
train() client id: f_00007-4-3 loss: 0.615785  [  128/  179]
train() client id: f_00007-4-4 loss: 0.715316  [  160/  179]
train() client id: f_00007-5-0 loss: 0.673309  [   32/  179]
train() client id: f_00007-5-1 loss: 0.518600  [   64/  179]
train() client id: f_00007-5-2 loss: 0.618947  [   96/  179]
train() client id: f_00007-5-3 loss: 0.496269  [  128/  179]
train() client id: f_00007-5-4 loss: 0.484143  [  160/  179]
train() client id: f_00007-6-0 loss: 0.404310  [   32/  179]
train() client id: f_00007-6-1 loss: 0.435146  [   64/  179]
train() client id: f_00007-6-2 loss: 0.735652  [   96/  179]
train() client id: f_00007-6-3 loss: 0.614408  [  128/  179]
train() client id: f_00007-6-4 loss: 0.481773  [  160/  179]
train() client id: f_00007-7-0 loss: 0.413985  [   32/  179]
train() client id: f_00007-7-1 loss: 0.759619  [   64/  179]
train() client id: f_00007-7-2 loss: 0.604163  [   96/  179]
train() client id: f_00007-7-3 loss: 0.455203  [  128/  179]
train() client id: f_00007-7-4 loss: 0.510605  [  160/  179]
train() client id: f_00008-0-0 loss: 0.720647  [   32/  130]
train() client id: f_00008-0-1 loss: 0.763920  [   64/  130]
train() client id: f_00008-0-2 loss: 0.668522  [   96/  130]
train() client id: f_00008-0-3 loss: 0.677995  [  128/  130]
train() client id: f_00008-1-0 loss: 0.682198  [   32/  130]
train() client id: f_00008-1-1 loss: 0.620471  [   64/  130]
train() client id: f_00008-1-2 loss: 0.852591  [   96/  130]
train() client id: f_00008-1-3 loss: 0.752835  [  128/  130]
train() client id: f_00008-2-0 loss: 0.778448  [   32/  130]
train() client id: f_00008-2-1 loss: 0.661070  [   64/  130]
train() client id: f_00008-2-2 loss: 0.662724  [   96/  130]
train() client id: f_00008-2-3 loss: 0.810663  [  128/  130]
train() client id: f_00008-3-0 loss: 0.768437  [   32/  130]
train() client id: f_00008-3-1 loss: 0.698847  [   64/  130]
train() client id: f_00008-3-2 loss: 0.717290  [   96/  130]
train() client id: f_00008-3-3 loss: 0.699422  [  128/  130]
train() client id: f_00008-4-0 loss: 0.753150  [   32/  130]
train() client id: f_00008-4-1 loss: 0.727162  [   64/  130]
train() client id: f_00008-4-2 loss: 0.693521  [   96/  130]
train() client id: f_00008-4-3 loss: 0.747127  [  128/  130]
train() client id: f_00008-5-0 loss: 0.761376  [   32/  130]
train() client id: f_00008-5-1 loss: 0.689278  [   64/  130]
train() client id: f_00008-5-2 loss: 0.805262  [   96/  130]
train() client id: f_00008-5-3 loss: 0.624676  [  128/  130]
train() client id: f_00008-6-0 loss: 0.820498  [   32/  130]
train() client id: f_00008-6-1 loss: 0.762550  [   64/  130]
train() client id: f_00008-6-2 loss: 0.675163  [   96/  130]
train() client id: f_00008-6-3 loss: 0.666210  [  128/  130]
train() client id: f_00008-7-0 loss: 0.611048  [   32/  130]
train() client id: f_00008-7-1 loss: 0.720762  [   64/  130]
train() client id: f_00008-7-2 loss: 0.675565  [   96/  130]
train() client id: f_00008-7-3 loss: 0.914479  [  128/  130]
train() client id: f_00009-0-0 loss: 1.179907  [   32/  118]
train() client id: f_00009-0-1 loss: 1.070407  [   64/  118]
train() client id: f_00009-0-2 loss: 0.997524  [   96/  118]
train() client id: f_00009-1-0 loss: 1.142875  [   32/  118]
train() client id: f_00009-1-1 loss: 1.073887  [   64/  118]
train() client id: f_00009-1-2 loss: 0.960204  [   96/  118]
train() client id: f_00009-2-0 loss: 1.046476  [   32/  118]
train() client id: f_00009-2-1 loss: 1.084740  [   64/  118]
train() client id: f_00009-2-2 loss: 1.003099  [   96/  118]
train() client id: f_00009-3-0 loss: 1.149336  [   32/  118]
train() client id: f_00009-3-1 loss: 0.983554  [   64/  118]
train() client id: f_00009-3-2 loss: 0.932618  [   96/  118]
train() client id: f_00009-4-0 loss: 0.931564  [   32/  118]
train() client id: f_00009-4-1 loss: 1.056596  [   64/  118]
train() client id: f_00009-4-2 loss: 0.983630  [   96/  118]
train() client id: f_00009-5-0 loss: 0.872878  [   32/  118]
train() client id: f_00009-5-1 loss: 1.091194  [   64/  118]
train() client id: f_00009-5-2 loss: 1.028949  [   96/  118]
train() client id: f_00009-6-0 loss: 1.228024  [   32/  118]
train() client id: f_00009-6-1 loss: 0.758356  [   64/  118]
train() client id: f_00009-6-2 loss: 0.854812  [   96/  118]
train() client id: f_00009-7-0 loss: 0.775571  [   32/  118]
train() client id: f_00009-7-1 loss: 0.868396  [   64/  118]
train() client id: f_00009-7-2 loss: 1.229850  [   96/  118]
At round 68 accuracy: 0.6445623342175066
At round 68 training accuracy: 0.5902079141515761
At round 68 training loss: 0.8170302562112446
update_location
xs = [  -3.9056584     4.20031788  360.00902392   18.81129433    0.97929623
    3.95640986 -322.44319194 -301.32485185  344.66397685 -287.06087855]
ys = [ 352.5879595   335.55583871    1.32061395 -322.45517586  314.35018685
  297.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [366.51537942 350.16476631 373.64186238 338.12897722 329.87421692
 314.17975995 337.60406184 317.48597228 359.3075659  304.00651284]
dists_bs = [246.59837202 240.60534339 562.4937198  533.87683018 224.47090518
 217.04541519 230.84655055 215.16464081 542.987262   204.44591885]
uav_gains = [9.97297579e-13 1.20746354e-12 9.24956572e-13 1.41687733e-12
 1.59815306e-12 2.06205032e-12 1.42738234e-12 1.94864202e-12
 1.08135149e-12 2.47729216e-12]
bs_gains = [2.21677447e-11 2.37486723e-11 2.20274684e-12 2.54952386e-12
 2.88433648e-11 3.16921979e-11 2.66678863e-11 3.24739829e-11
 2.43154935e-12 3.74692015e-11]
Round 69
-------------------------------
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.8731924  3.7097278  1.85116443 0.70130907 4.2759113  2.0588358
 0.85327763 2.56982483 1.88245466 1.66965903]
obj_prev = 21.445356959883735
eta_min = 1.2228941532952692e-50	eta_max = 0.9500254409851863
af = 4.439896748873053	bf = 0.784423253509249	zeta = 4.883886423760359	eta = 0.9090909090909091
af = 4.439896748873053	bf = 0.784423253509249	zeta = 13.283496720526747	eta = 0.3342415662294824
af = 4.439896748873053	bf = 0.784423253509249	zeta = 8.486662978303045	eta = 0.5231616667498248
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.677394178566005	eta = 0.5783077754778437
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626245838238497	eta = 0.5821864181995182
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626008957353865	eta = 0.5822045022110287
eta = 0.5822045022110287
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.04463654 0.09387843 0.04392802 0.0152331  0.10840302 0.0517217
 0.01912994 0.06341222 0.04605357 0.04180247]
ene_total = [0.78877956 1.11412037 0.79451743 0.41620728 1.26807169 0.64932725
 0.45766691 0.90423052 0.69485736 0.53823058]
ti_comp = [2.16111529 2.35459485 2.14875789 2.20941898 2.35838257 2.36010268
 2.21029221 2.24279005 2.26158603 2.36299155]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [1.19013336e-06 9.32706721e-06 1.14743992e-06 4.52573450e-08
 1.43144819e-05 1.55251797e-06 8.95615827e-08 3.16826659e-06
 1.19355847e-06 8.17638912e-07]
ene_total = [0.30300549 0.09252695 0.3164538  0.25042331 0.088459   0.08644808
 0.24947344 0.214139   0.19366152 0.08329608]
optimize_network iter = 0 obj = 1.8778866839980972
eta = 0.5822045022110287
freqs = [10327199.39136077 19935155.93888518 10221724.11669465  3447309.92299876
 22982492.36545908 10957510.88478662  4327469.55320673 14136905.79686903
 10181697.58740329  8845243.50076053]
eta_min = 0.5822045022110292	eta_max = 0.7888487440198617
af = 0.000276093861788328	bf = 0.784423253509249	zeta = 0.00030370324796716086	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.09696425e-07 1.64338949e-06 2.02174023e-07 7.97415130e-09
 2.52215070e-06 2.73547049e-07 1.57803692e-08 5.58235069e-07
 2.10299915e-07 1.44064491e-07]
ene_total = [1.3924886  0.42484648 1.45429544 1.1508812  0.40594569 0.39722982
 1.14651401 0.98399884 0.88997085 0.38277432]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 1 obj = 3.715240812383447
eta = 0.7888487440198617
freqs = [10252122.85547994 17928004.42999087 10221724.11669466  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117568
  9570237.57402436  7925070.10389261]
eta_min = 0.7888487440198628	eta_max = 0.7888487440198605
af = 0.00022982602808494754	bf = 0.784423253509249	zeta = 0.0002528086308934423	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.06658611e-07 1.32912288e-06 2.02174023e-07 7.44162632e-09
 2.03296858e-06 2.20155092e-07 1.47125432e-08 5.02791340e-07
 1.85799332e-07 1.15649455e-07]
ene_total = [1.39248845 0.42483077 1.45429544 1.15088117 0.40592122 0.39722715
 1.14651395 0.98399607 0.88996963 0.38277289]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 2 obj = 3.715240812383425
eta = 0.7888487440198605
freqs = [10252122.85547993 17928004.42999088 10221724.11669465  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117567
  9570237.57402436  7925070.10389262]
Done!
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [4.63517562e-07 2.98110875e-06 4.53459015e-07 1.66909304e-08
 4.55977439e-06 4.93789014e-07 3.29989742e-08 1.12771791e-06
 4.16731985e-07 2.59391820e-07]
ene_total = [0.02784093 0.00849549 0.02907666 0.02301012 0.0081183  0.00794222
 0.02292281 0.01967412 0.01779381 0.0076531 ]
At round 69 energy consumption: 0.17252756466419225
At round 69 eta: 0.7888487440198605
At round 69 a_n: 4.546939415371483
At round 69 local rounds: 7.766500565656417
At round 69 global rounds: 21.53403916195109
gradient difference: 0.6513196229934692
train() client id: f_00000-0-0 loss: 0.967968  [   32/  126]
train() client id: f_00000-0-1 loss: 0.780897  [   64/  126]
train() client id: f_00000-0-2 loss: 0.960976  [   96/  126]
train() client id: f_00000-1-0 loss: 0.925591  [   32/  126]
train() client id: f_00000-1-1 loss: 1.035736  [   64/  126]
train() client id: f_00000-1-2 loss: 0.786643  [   96/  126]
train() client id: f_00000-2-0 loss: 0.747252  [   32/  126]
train() client id: f_00000-2-1 loss: 0.943234  [   64/  126]
train() client id: f_00000-2-2 loss: 0.783302  [   96/  126]
train() client id: f_00000-3-0 loss: 0.890386  [   32/  126]
train() client id: f_00000-3-1 loss: 0.857649  [   64/  126]
train() client id: f_00000-3-2 loss: 0.847663  [   96/  126]
train() client id: f_00000-4-0 loss: 0.746228  [   32/  126]
train() client id: f_00000-4-1 loss: 0.930652  [   64/  126]
train() client id: f_00000-4-2 loss: 0.927910  [   96/  126]
train() client id: f_00000-5-0 loss: 0.911031  [   32/  126]
train() client id: f_00000-5-1 loss: 0.896344  [   64/  126]
train() client id: f_00000-5-2 loss: 0.855233  [   96/  126]
train() client id: f_00000-6-0 loss: 0.769641  [   32/  126]
train() client id: f_00000-6-1 loss: 0.993477  [   64/  126]
train() client id: f_00000-6-2 loss: 0.938437  [   96/  126]
train() client id: f_00001-0-0 loss: 0.341494  [   32/  265]
train() client id: f_00001-0-1 loss: 0.417889  [   64/  265]
train() client id: f_00001-0-2 loss: 0.420880  [   96/  265]
train() client id: f_00001-0-3 loss: 0.373504  [  128/  265]
train() client id: f_00001-0-4 loss: 0.447975  [  160/  265]
train() client id: f_00001-0-5 loss: 0.496266  [  192/  265]
train() client id: f_00001-0-6 loss: 0.420657  [  224/  265]
train() client id: f_00001-0-7 loss: 0.461398  [  256/  265]
train() client id: f_00001-1-0 loss: 0.340884  [   32/  265]
train() client id: f_00001-1-1 loss: 0.420916  [   64/  265]
train() client id: f_00001-1-2 loss: 0.357150  [   96/  265]
train() client id: f_00001-1-3 loss: 0.499315  [  128/  265]
train() client id: f_00001-1-4 loss: 0.423170  [  160/  265]
train() client id: f_00001-1-5 loss: 0.341745  [  192/  265]
train() client id: f_00001-1-6 loss: 0.329581  [  224/  265]
train() client id: f_00001-1-7 loss: 0.585351  [  256/  265]
train() client id: f_00001-2-0 loss: 0.467131  [   32/  265]
train() client id: f_00001-2-1 loss: 0.470363  [   64/  265]
train() client id: f_00001-2-2 loss: 0.399423  [   96/  265]
train() client id: f_00001-2-3 loss: 0.311911  [  128/  265]
train() client id: f_00001-2-4 loss: 0.401283  [  160/  265]
train() client id: f_00001-2-5 loss: 0.445243  [  192/  265]
train() client id: f_00001-2-6 loss: 0.398952  [  224/  265]
train() client id: f_00001-2-7 loss: 0.414796  [  256/  265]
train() client id: f_00001-3-0 loss: 0.424592  [   32/  265]
train() client id: f_00001-3-1 loss: 0.444852  [   64/  265]
train() client id: f_00001-3-2 loss: 0.482325  [   96/  265]
train() client id: f_00001-3-3 loss: 0.444244  [  128/  265]
train() client id: f_00001-3-4 loss: 0.430513  [  160/  265]
train() client id: f_00001-3-5 loss: 0.385027  [  192/  265]
train() client id: f_00001-3-6 loss: 0.400161  [  224/  265]
train() client id: f_00001-3-7 loss: 0.301825  [  256/  265]
train() client id: f_00001-4-0 loss: 0.438149  [   32/  265]
train() client id: f_00001-4-1 loss: 0.348964  [   64/  265]
train() client id: f_00001-4-2 loss: 0.472441  [   96/  265]
train() client id: f_00001-4-3 loss: 0.458280  [  128/  265]
train() client id: f_00001-4-4 loss: 0.427194  [  160/  265]
train() client id: f_00001-4-5 loss: 0.396846  [  192/  265]
train() client id: f_00001-4-6 loss: 0.316944  [  224/  265]
train() client id: f_00001-4-7 loss: 0.417820  [  256/  265]
train() client id: f_00001-5-0 loss: 0.296449  [   32/  265]
train() client id: f_00001-5-1 loss: 0.532696  [   64/  265]
train() client id: f_00001-5-2 loss: 0.447865  [   96/  265]
train() client id: f_00001-5-3 loss: 0.454302  [  128/  265]
train() client id: f_00001-5-4 loss: 0.397985  [  160/  265]
train() client id: f_00001-5-5 loss: 0.372144  [  192/  265]
train() client id: f_00001-5-6 loss: 0.476957  [  224/  265]
train() client id: f_00001-5-7 loss: 0.304376  [  256/  265]
train() client id: f_00001-6-0 loss: 0.439024  [   32/  265]
train() client id: f_00001-6-1 loss: 0.457284  [   64/  265]
train() client id: f_00001-6-2 loss: 0.369898  [   96/  265]
train() client id: f_00001-6-3 loss: 0.384576  [  128/  265]
train() client id: f_00001-6-4 loss: 0.521105  [  160/  265]
train() client id: f_00001-6-5 loss: 0.312300  [  192/  265]
train() client id: f_00001-6-6 loss: 0.365334  [  224/  265]
train() client id: f_00001-6-7 loss: 0.441345  [  256/  265]
train() client id: f_00002-0-0 loss: 1.051804  [   32/  124]
train() client id: f_00002-0-1 loss: 0.970899  [   64/  124]
train() client id: f_00002-0-2 loss: 1.032908  [   96/  124]
train() client id: f_00002-1-0 loss: 0.632086  [   32/  124]
train() client id: f_00002-1-1 loss: 1.081415  [   64/  124]
train() client id: f_00002-1-2 loss: 1.128132  [   96/  124]
train() client id: f_00002-2-0 loss: 0.853582  [   32/  124]
train() client id: f_00002-2-1 loss: 0.875625  [   64/  124]
train() client id: f_00002-2-2 loss: 1.100684  [   96/  124]
train() client id: f_00002-3-0 loss: 1.000341  [   32/  124]
train() client id: f_00002-3-1 loss: 0.978816  [   64/  124]
train() client id: f_00002-3-2 loss: 0.762043  [   96/  124]
train() client id: f_00002-4-0 loss: 1.027665  [   32/  124]
train() client id: f_00002-4-1 loss: 0.817406  [   64/  124]
train() client id: f_00002-4-2 loss: 0.844295  [   96/  124]
train() client id: f_00002-5-0 loss: 0.913137  [   32/  124]
train() client id: f_00002-5-1 loss: 0.734826  [   64/  124]
train() client id: f_00002-5-2 loss: 1.077720  [   96/  124]
train() client id: f_00002-6-0 loss: 0.883142  [   32/  124]
train() client id: f_00002-6-1 loss: 0.968035  [   64/  124]
train() client id: f_00002-6-2 loss: 0.722884  [   96/  124]
train() client id: f_00003-0-0 loss: 0.813356  [   32/   43]
train() client id: f_00003-1-0 loss: 0.748304  [   32/   43]
train() client id: f_00003-2-0 loss: 0.752361  [   32/   43]
train() client id: f_00003-3-0 loss: 0.867197  [   32/   43]
train() client id: f_00003-4-0 loss: 0.875833  [   32/   43]
train() client id: f_00003-5-0 loss: 0.692221  [   32/   43]
train() client id: f_00003-6-0 loss: 0.770841  [   32/   43]
train() client id: f_00004-0-0 loss: 0.797893  [   32/  306]
train() client id: f_00004-0-1 loss: 0.915940  [   64/  306]
train() client id: f_00004-0-2 loss: 0.966796  [   96/  306]
train() client id: f_00004-0-3 loss: 1.032599  [  128/  306]
train() client id: f_00004-0-4 loss: 0.772549  [  160/  306]
train() client id: f_00004-0-5 loss: 0.861152  [  192/  306]
train() client id: f_00004-0-6 loss: 0.940666  [  224/  306]
train() client id: f_00004-0-7 loss: 0.897292  [  256/  306]
train() client id: f_00004-0-8 loss: 0.793552  [  288/  306]
train() client id: f_00004-1-0 loss: 0.932474  [   32/  306]
train() client id: f_00004-1-1 loss: 0.782602  [   64/  306]
train() client id: f_00004-1-2 loss: 1.016967  [   96/  306]
train() client id: f_00004-1-3 loss: 0.830510  [  128/  306]
train() client id: f_00004-1-4 loss: 0.972246  [  160/  306]
train() client id: f_00004-1-5 loss: 0.747352  [  192/  306]
train() client id: f_00004-1-6 loss: 0.873134  [  224/  306]
train() client id: f_00004-1-7 loss: 0.859820  [  256/  306]
train() client id: f_00004-1-8 loss: 0.734588  [  288/  306]
train() client id: f_00004-2-0 loss: 1.014027  [   32/  306]
train() client id: f_00004-2-1 loss: 0.753069  [   64/  306]
train() client id: f_00004-2-2 loss: 0.905423  [   96/  306]
train() client id: f_00004-2-3 loss: 0.853004  [  128/  306]
train() client id: f_00004-2-4 loss: 0.835417  [  160/  306]
train() client id: f_00004-2-5 loss: 0.791988  [  192/  306]
train() client id: f_00004-2-6 loss: 0.846540  [  224/  306]
train() client id: f_00004-2-7 loss: 0.964381  [  256/  306]
train() client id: f_00004-2-8 loss: 0.943879  [  288/  306]
train() client id: f_00004-3-0 loss: 0.773111  [   32/  306]
train() client id: f_00004-3-1 loss: 0.852338  [   64/  306]
train() client id: f_00004-3-2 loss: 0.723341  [   96/  306]
train() client id: f_00004-3-3 loss: 0.839291  [  128/  306]
train() client id: f_00004-3-4 loss: 1.037773  [  160/  306]
train() client id: f_00004-3-5 loss: 0.904183  [  192/  306]
train() client id: f_00004-3-6 loss: 0.884620  [  224/  306]
train() client id: f_00004-3-7 loss: 0.922470  [  256/  306]
train() client id: f_00004-3-8 loss: 0.818983  [  288/  306]
train() client id: f_00004-4-0 loss: 0.840216  [   32/  306]
train() client id: f_00004-4-1 loss: 0.862895  [   64/  306]
train() client id: f_00004-4-2 loss: 0.693103  [   96/  306]
train() client id: f_00004-4-3 loss: 0.971941  [  128/  306]
train() client id: f_00004-4-4 loss: 0.940914  [  160/  306]
train() client id: f_00004-4-5 loss: 0.990177  [  192/  306]
train() client id: f_00004-4-6 loss: 0.892343  [  224/  306]
train() client id: f_00004-4-7 loss: 0.952889  [  256/  306]
train() client id: f_00004-4-8 loss: 0.861707  [  288/  306]
train() client id: f_00004-5-0 loss: 0.857100  [   32/  306]
train() client id: f_00004-5-1 loss: 0.801757  [   64/  306]
train() client id: f_00004-5-2 loss: 0.873870  [   96/  306]
train() client id: f_00004-5-3 loss: 0.741043  [  128/  306]
train() client id: f_00004-5-4 loss: 0.975386  [  160/  306]
train() client id: f_00004-5-5 loss: 1.011964  [  192/  306]
train() client id: f_00004-5-6 loss: 0.728305  [  224/  306]
train() client id: f_00004-5-7 loss: 0.766770  [  256/  306]
train() client id: f_00004-5-8 loss: 1.023848  [  288/  306]
train() client id: f_00004-6-0 loss: 0.761946  [   32/  306]
train() client id: f_00004-6-1 loss: 0.813961  [   64/  306]
train() client id: f_00004-6-2 loss: 0.790650  [   96/  306]
train() client id: f_00004-6-3 loss: 0.942053  [  128/  306]
train() client id: f_00004-6-4 loss: 1.002786  [  160/  306]
train() client id: f_00004-6-5 loss: 0.842400  [  192/  306]
train() client id: f_00004-6-6 loss: 0.881362  [  224/  306]
train() client id: f_00004-6-7 loss: 0.864191  [  256/  306]
train() client id: f_00004-6-8 loss: 0.897217  [  288/  306]
train() client id: f_00005-0-0 loss: 0.480348  [   32/  146]
train() client id: f_00005-0-1 loss: 0.597009  [   64/  146]
train() client id: f_00005-0-2 loss: 0.557834  [   96/  146]
train() client id: f_00005-0-3 loss: 0.897615  [  128/  146]
train() client id: f_00005-1-0 loss: 0.468890  [   32/  146]
train() client id: f_00005-1-1 loss: 0.691614  [   64/  146]
train() client id: f_00005-1-2 loss: 0.927899  [   96/  146]
train() client id: f_00005-1-3 loss: 0.419598  [  128/  146]
train() client id: f_00005-2-0 loss: 0.423648  [   32/  146]
train() client id: f_00005-2-1 loss: 0.780374  [   64/  146]
train() client id: f_00005-2-2 loss: 0.708215  [   96/  146]
train() client id: f_00005-2-3 loss: 0.770402  [  128/  146]
train() client id: f_00005-3-0 loss: 0.778142  [   32/  146]
train() client id: f_00005-3-1 loss: 0.816343  [   64/  146]
train() client id: f_00005-3-2 loss: 0.681950  [   96/  146]
train() client id: f_00005-3-3 loss: 0.440812  [  128/  146]
train() client id: f_00005-4-0 loss: 0.736172  [   32/  146]
train() client id: f_00005-4-1 loss: 0.602998  [   64/  146]
train() client id: f_00005-4-2 loss: 0.938187  [   96/  146]
train() client id: f_00005-4-3 loss: 0.355210  [  128/  146]
train() client id: f_00005-5-0 loss: 0.715099  [   32/  146]
train() client id: f_00005-5-1 loss: 0.813413  [   64/  146]
train() client id: f_00005-5-2 loss: 0.506326  [   96/  146]
train() client id: f_00005-5-3 loss: 0.558513  [  128/  146]
train() client id: f_00005-6-0 loss: 0.398380  [   32/  146]
train() client id: f_00005-6-1 loss: 0.770681  [   64/  146]
train() client id: f_00005-6-2 loss: 0.569914  [   96/  146]
train() client id: f_00005-6-3 loss: 0.652970  [  128/  146]
train() client id: f_00006-0-0 loss: 0.449278  [   32/   54]
train() client id: f_00006-1-0 loss: 0.435634  [   32/   54]
train() client id: f_00006-2-0 loss: 0.544568  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532288  [   32/   54]
train() client id: f_00006-4-0 loss: 0.493618  [   32/   54]
train() client id: f_00006-5-0 loss: 0.438298  [   32/   54]
train() client id: f_00006-6-0 loss: 0.486031  [   32/   54]
train() client id: f_00007-0-0 loss: 0.702686  [   32/  179]
train() client id: f_00007-0-1 loss: 0.658092  [   64/  179]
train() client id: f_00007-0-2 loss: 0.501842  [   96/  179]
train() client id: f_00007-0-3 loss: 0.550117  [  128/  179]
train() client id: f_00007-0-4 loss: 0.765226  [  160/  179]
train() client id: f_00007-1-0 loss: 0.790523  [   32/  179]
train() client id: f_00007-1-1 loss: 0.649530  [   64/  179]
train() client id: f_00007-1-2 loss: 0.560343  [   96/  179]
train() client id: f_00007-1-3 loss: 0.617531  [  128/  179]
train() client id: f_00007-1-4 loss: 0.651297  [  160/  179]
train() client id: f_00007-2-0 loss: 0.507929  [   32/  179]
train() client id: f_00007-2-1 loss: 0.905949  [   64/  179]
train() client id: f_00007-2-2 loss: 0.658455  [   96/  179]
train() client id: f_00007-2-3 loss: 0.477622  [  128/  179]
train() client id: f_00007-2-4 loss: 0.536959  [  160/  179]
train() client id: f_00007-3-0 loss: 0.636957  [   32/  179]
train() client id: f_00007-3-1 loss: 0.587244  [   64/  179]
train() client id: f_00007-3-2 loss: 0.578577  [   96/  179]
train() client id: f_00007-3-3 loss: 0.571359  [  128/  179]
train() client id: f_00007-3-4 loss: 0.803351  [  160/  179]
train() client id: f_00007-4-0 loss: 0.734197  [   32/  179]
train() client id: f_00007-4-1 loss: 0.730444  [   64/  179]
train() client id: f_00007-4-2 loss: 0.652185  [   96/  179]
train() client id: f_00007-4-3 loss: 0.586431  [  128/  179]
train() client id: f_00007-4-4 loss: 0.480110  [  160/  179]
train() client id: f_00007-5-0 loss: 0.650402  [   32/  179]
train() client id: f_00007-5-1 loss: 0.620300  [   64/  179]
train() client id: f_00007-5-2 loss: 0.743535  [   96/  179]
train() client id: f_00007-5-3 loss: 0.536096  [  128/  179]
train() client id: f_00007-5-4 loss: 0.653411  [  160/  179]
train() client id: f_00007-6-0 loss: 0.552144  [   32/  179]
train() client id: f_00007-6-1 loss: 0.589912  [   64/  179]
train() client id: f_00007-6-2 loss: 0.560536  [   96/  179]
train() client id: f_00007-6-3 loss: 0.582706  [  128/  179]
train() client id: f_00007-6-4 loss: 0.798976  [  160/  179]
train() client id: f_00008-0-0 loss: 0.601768  [   32/  130]
train() client id: f_00008-0-1 loss: 0.589966  [   64/  130]
train() client id: f_00008-0-2 loss: 0.616901  [   96/  130]
train() client id: f_00008-0-3 loss: 0.537712  [  128/  130]
train() client id: f_00008-1-0 loss: 0.583502  [   32/  130]
train() client id: f_00008-1-1 loss: 0.626740  [   64/  130]
train() client id: f_00008-1-2 loss: 0.609923  [   96/  130]
train() client id: f_00008-1-3 loss: 0.523936  [  128/  130]
train() client id: f_00008-2-0 loss: 0.623098  [   32/  130]
train() client id: f_00008-2-1 loss: 0.620884  [   64/  130]
train() client id: f_00008-2-2 loss: 0.474153  [   96/  130]
train() client id: f_00008-2-3 loss: 0.632033  [  128/  130]
train() client id: f_00008-3-0 loss: 0.539870  [   32/  130]
train() client id: f_00008-3-1 loss: 0.712434  [   64/  130]
train() client id: f_00008-3-2 loss: 0.563486  [   96/  130]
train() client id: f_00008-3-3 loss: 0.543182  [  128/  130]
train() client id: f_00008-4-0 loss: 0.440720  [   32/  130]
train() client id: f_00008-4-1 loss: 0.580597  [   64/  130]
train() client id: f_00008-4-2 loss: 0.575252  [   96/  130]
train() client id: f_00008-4-3 loss: 0.744787  [  128/  130]
train() client id: f_00008-5-0 loss: 0.618606  [   32/  130]
train() client id: f_00008-5-1 loss: 0.484559  [   64/  130]
train() client id: f_00008-5-2 loss: 0.644892  [   96/  130]
train() client id: f_00008-5-3 loss: 0.601783  [  128/  130]
train() client id: f_00008-6-0 loss: 0.603035  [   32/  130]
train() client id: f_00008-6-1 loss: 0.646092  [   64/  130]
train() client id: f_00008-6-2 loss: 0.571458  [   96/  130]
train() client id: f_00008-6-3 loss: 0.521297  [  128/  130]
train() client id: f_00009-0-0 loss: 0.655465  [   32/  118]
train() client id: f_00009-0-1 loss: 0.670276  [   64/  118]
train() client id: f_00009-0-2 loss: 0.787255  [   96/  118]
train() client id: f_00009-1-0 loss: 0.700507  [   32/  118]
train() client id: f_00009-1-1 loss: 0.540046  [   64/  118]
train() client id: f_00009-1-2 loss: 0.587886  [   96/  118]
train() client id: f_00009-2-0 loss: 0.569535  [   32/  118]
train() client id: f_00009-2-1 loss: 0.518742  [   64/  118]
train() client id: f_00009-2-2 loss: 0.614261  [   96/  118]
train() client id: f_00009-3-0 loss: 0.723738  [   32/  118]
train() client id: f_00009-3-1 loss: 0.409470  [   64/  118]
train() client id: f_00009-3-2 loss: 0.601479  [   96/  118]
train() client id: f_00009-4-0 loss: 0.707301  [   32/  118]
train() client id: f_00009-4-1 loss: 0.599445  [   64/  118]
train() client id: f_00009-4-2 loss: 0.440086  [   96/  118]
train() client id: f_00009-5-0 loss: 0.360401  [   32/  118]
train() client id: f_00009-5-1 loss: 0.711257  [   64/  118]
train() client id: f_00009-5-2 loss: 0.551481  [   96/  118]
train() client id: f_00009-6-0 loss: 0.712856  [   32/  118]
train() client id: f_00009-6-1 loss: 0.476970  [   64/  118]
train() client id: f_00009-6-2 loss: 0.471237  [   96/  118]
At round 69 accuracy: 0.6472148541114059
At round 69 training accuracy: 0.5861837692823608
At round 69 training loss: 0.8268754680740726
update_location
xs = [  -3.9056584     4.20031788  365.00902392   18.81129433    0.97929623
    3.95640986 -327.44319194 -306.32485185  349.66397685 -292.06087855]
ys = [ 357.5879595   340.55583871    1.32061395 -327.45517586  319.35018685
  302.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [371.32789142 354.95904264 378.4617967  342.90050597 334.64234768
 318.92328712 342.382731   322.23530396 364.10653205 308.73219566]
dists_bs = [250.22317375 243.97026388 567.25150952 538.53878371 227.60863152
 219.91328691 234.07170231 218.13773441 547.77441202 207.22872031]
uav_gains = [9.47393004e-13 1.13831941e-12 8.81152900e-13 1.32700416e-12
 1.48913730e-12 1.90221719e-12 1.33628878e-12 1.80140553e-12
 1.02406515e-12 2.27098709e-12]
bs_gains = [2.12802637e-11 2.28428757e-11 2.15140531e-12 2.48820711e-12
 2.77437810e-11 3.05485056e-11 2.56517576e-11 3.12498438e-11
 2.37251633e-12 3.60773175e-11]
Round 70
-------------------------------
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.73561523 3.43059311 1.71527241 0.65186493 3.95409396 1.90401388
 0.79238643 2.37939759 1.74146138 1.54414415]
obj_prev = 19.848843077614966
eta_min = 1.1829730818327539e-54	eta_max = 0.9523787254678187
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 4.515956513396188	eta = 0.9090909090909091
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 12.490584420921488	eta = 0.32868077856324013
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.913053448586266	eta = 0.518815529157309
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.145329232998529	eta = 0.5745592509885677
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096743771047384	eta = 0.5784927770574487
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096517585671171	eta = 0.5785112152005008
eta = 0.5785112152005008
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.04516167 0.09498287 0.04444481 0.01541231 0.10967833 0.05233018
 0.019355   0.06415824 0.04659537 0.04229426]
ene_total = [0.7365201  1.0329747  0.74175636 0.39122836 1.17571715 0.60187953
 0.42968967 0.84367489 0.64421599 0.49886084]
ti_comp = [2.36984321 2.57086067 2.35742536 2.41850329 2.57471655 2.57650502
 2.41937284 2.45238669 2.47671475 2.57942159]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.02506317e-06 8.10324115e-06 9.87341466e-07 3.91192234e-08
 1.24389480e-05 1.34919525e-06 7.74201308e-08 2.74447586e-06
 1.03075701e-06 7.10688982e-07]
ene_total = [0.28603811 0.08559175 0.29842467 0.23748935 0.08178872 0.07989408
 0.23662235 0.2037173  0.17943272 0.0769784 ]
optimize_network iter = 0 obj = 1.7659774351917215
eta = 0.5785112152005008
freqs = [ 9528407.84474552 18472970.62942295  9426557.92578857  3186333.13230265
 21299107.60121082 10155265.18397391  4000003.44569053 13080775.34726803
  9406688.61857887  8198399.6009876 ]
eta_min = 0.578511215200502	eta_max = 0.8002666857305597
af = 0.00021898789766085596	bf = 0.7442198316471329	zeta = 0.00024088668742694157	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.78511656e-07 1.41115497e-06 1.71942534e-07 6.81249461e-09
 2.16620522e-06 2.34958278e-07 1.34824820e-08 4.77942182e-07
 1.79503221e-07 1.23764340e-07]
ene_total = [1.32614126 0.39652609 1.38357144 1.10108858 0.3787282  0.37036751
 1.09706736 0.9444053  0.83187829 0.35687372]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 1 obj = 3.726268206640733
eta = 0.8002666857305597
freqs = [ 9456245.97160665 16480383.53188894  9426557.92578857  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052094
  8790118.03381017  7285272.29487025]
eta_min = 0.800266685730563	eta_max = 0.8002666857305585
af = 0.00017982620690873804	bf = 0.7442198316471329	zeta = 0.00019780882759961186	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.75818035e-07 1.12314508e-06 1.71942534e-07 6.33773081e-09
 1.71795736e-06 1.86032089e-07 1.25305500e-08 4.28277123e-07
 1.56742989e-07 9.77302347e-08]
ene_total = [1.32614114 0.39651277 1.38357144 1.10108855 0.37870747 0.37036524
 1.09706732 0.944403   0.83187723 0.35687251]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 2 obj = 3.7262682066407105
eta = 0.8002666857305585
freqs = [ 9456245.97160664 16480383.53188894  9426557.92578855  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052093
  8790118.03381016  7285272.29487025]
Done!
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [3.94344793e-07 2.51911821e-06 3.85652375e-07 1.42149873e-08
 3.85323121e-06 4.17254039e-07 2.81049502e-08 9.60588899e-07
 3.51561096e-07 2.19200545e-07]
ene_total = [0.02867458 0.00857495 0.02991635 0.02380819 0.0081907  0.00800842
 0.02372125 0.02042079 0.01798738 0.00771656]
At round 70 energy consumption: 0.1770191732645484
At round 70 eta: 0.8002666857305585
At round 70 a_n: 4.204393568402164
At round 70 local rounds: 7.295939587071601
At round 70 global rounds: 21.05003656390746
gradient difference: 0.6199457049369812
train() client id: f_00000-0-0 loss: 0.764279  [   32/  126]
train() client id: f_00000-0-1 loss: 1.141735  [   64/  126]
train() client id: f_00000-0-2 loss: 1.248252  [   96/  126]
train() client id: f_00000-1-0 loss: 1.064838  [   32/  126]
train() client id: f_00000-1-1 loss: 0.857215  [   64/  126]
train() client id: f_00000-1-2 loss: 0.778553  [   96/  126]
train() client id: f_00000-2-0 loss: 0.903921  [   32/  126]
train() client id: f_00000-2-1 loss: 0.894445  [   64/  126]
train() client id: f_00000-2-2 loss: 0.930451  [   96/  126]
train() client id: f_00000-3-0 loss: 0.995771  [   32/  126]
train() client id: f_00000-3-1 loss: 0.839567  [   64/  126]
train() client id: f_00000-3-2 loss: 0.850523  [   96/  126]
train() client id: f_00000-4-0 loss: 0.850898  [   32/  126]
train() client id: f_00000-4-1 loss: 0.947248  [   64/  126]
train() client id: f_00000-4-2 loss: 0.803138  [   96/  126]
train() client id: f_00000-5-0 loss: 0.886591  [   32/  126]
train() client id: f_00000-5-1 loss: 0.769153  [   64/  126]
train() client id: f_00000-5-2 loss: 0.723808  [   96/  126]
train() client id: f_00000-6-0 loss: 0.788630  [   32/  126]
train() client id: f_00000-6-1 loss: 0.843144  [   64/  126]
train() client id: f_00000-6-2 loss: 0.803963  [   96/  126]
train() client id: f_00001-0-0 loss: 0.429115  [   32/  265]
train() client id: f_00001-0-1 loss: 0.377952  [   64/  265]
train() client id: f_00001-0-2 loss: 0.463901  [   96/  265]
train() client id: f_00001-0-3 loss: 0.606208  [  128/  265]
train() client id: f_00001-0-4 loss: 0.392585  [  160/  265]
train() client id: f_00001-0-5 loss: 0.488881  [  192/  265]
train() client id: f_00001-0-6 loss: 0.580088  [  224/  265]
train() client id: f_00001-0-7 loss: 0.479807  [  256/  265]
train() client id: f_00001-1-0 loss: 0.492595  [   32/  265]
train() client id: f_00001-1-1 loss: 0.500377  [   64/  265]
train() client id: f_00001-1-2 loss: 0.368440  [   96/  265]
train() client id: f_00001-1-3 loss: 0.404875  [  128/  265]
train() client id: f_00001-1-4 loss: 0.435356  [  160/  265]
train() client id: f_00001-1-5 loss: 0.627059  [  192/  265]
train() client id: f_00001-1-6 loss: 0.468776  [  224/  265]
train() client id: f_00001-1-7 loss: 0.480227  [  256/  265]
train() client id: f_00001-2-0 loss: 0.420823  [   32/  265]
train() client id: f_00001-2-1 loss: 0.460610  [   64/  265]
train() client id: f_00001-2-2 loss: 0.554652  [   96/  265]
train() client id: f_00001-2-3 loss: 0.413435  [  128/  265]
train() client id: f_00001-2-4 loss: 0.413813  [  160/  265]
train() client id: f_00001-2-5 loss: 0.361437  [  192/  265]
train() client id: f_00001-2-6 loss: 0.555071  [  224/  265]
train() client id: f_00001-2-7 loss: 0.586348  [  256/  265]
train() client id: f_00001-3-0 loss: 0.369050  [   32/  265]
train() client id: f_00001-3-1 loss: 0.560476  [   64/  265]
train() client id: f_00001-3-2 loss: 0.385840  [   96/  265]
train() client id: f_00001-3-3 loss: 0.552967  [  128/  265]
train() client id: f_00001-3-4 loss: 0.530663  [  160/  265]
train() client id: f_00001-3-5 loss: 0.514950  [  192/  265]
train() client id: f_00001-3-6 loss: 0.366813  [  224/  265]
train() client id: f_00001-3-7 loss: 0.464935  [  256/  265]
train() client id: f_00001-4-0 loss: 0.353475  [   32/  265]
train() client id: f_00001-4-1 loss: 0.393287  [   64/  265]
train() client id: f_00001-4-2 loss: 0.470997  [   96/  265]
train() client id: f_00001-4-3 loss: 0.508091  [  128/  265]
train() client id: f_00001-4-4 loss: 0.457805  [  160/  265]
train() client id: f_00001-4-5 loss: 0.447572  [  192/  265]
train() client id: f_00001-4-6 loss: 0.605012  [  224/  265]
train() client id: f_00001-4-7 loss: 0.487815  [  256/  265]
train() client id: f_00001-5-0 loss: 0.408017  [   32/  265]
train() client id: f_00001-5-1 loss: 0.449592  [   64/  265]
train() client id: f_00001-5-2 loss: 0.382199  [   96/  265]
train() client id: f_00001-5-3 loss: 0.369088  [  128/  265]
train() client id: f_00001-5-4 loss: 0.541208  [  160/  265]
train() client id: f_00001-5-5 loss: 0.464772  [  192/  265]
train() client id: f_00001-5-6 loss: 0.561720  [  224/  265]
train() client id: f_00001-5-7 loss: 0.532369  [  256/  265]
train() client id: f_00001-6-0 loss: 0.540031  [   32/  265]
train() client id: f_00001-6-1 loss: 0.417133  [   64/  265]
train() client id: f_00001-6-2 loss: 0.378107  [   96/  265]
train() client id: f_00001-6-3 loss: 0.437899  [  128/  265]
train() client id: f_00001-6-4 loss: 0.475151  [  160/  265]
train() client id: f_00001-6-5 loss: 0.408907  [  192/  265]
train() client id: f_00001-6-6 loss: 0.368051  [  224/  265]
train() client id: f_00001-6-7 loss: 0.580405  [  256/  265]
train() client id: f_00002-0-0 loss: 1.075620  [   32/  124]
train() client id: f_00002-0-1 loss: 1.154231  [   64/  124]
train() client id: f_00002-0-2 loss: 1.250989  [   96/  124]
train() client id: f_00002-1-0 loss: 1.240900  [   32/  124]
train() client id: f_00002-1-1 loss: 1.117289  [   64/  124]
train() client id: f_00002-1-2 loss: 1.053529  [   96/  124]
train() client id: f_00002-2-0 loss: 1.108900  [   32/  124]
train() client id: f_00002-2-1 loss: 1.143212  [   64/  124]
train() client id: f_00002-2-2 loss: 1.207665  [   96/  124]
train() client id: f_00002-3-0 loss: 1.082812  [   32/  124]
train() client id: f_00002-3-1 loss: 1.237109  [   64/  124]
train() client id: f_00002-3-2 loss: 1.093902  [   96/  124]
train() client id: f_00002-4-0 loss: 1.108942  [   32/  124]
train() client id: f_00002-4-1 loss: 1.397248  [   64/  124]
train() client id: f_00002-4-2 loss: 0.956262  [   96/  124]
train() client id: f_00002-5-0 loss: 0.991323  [   32/  124]
train() client id: f_00002-5-1 loss: 1.092539  [   64/  124]
train() client id: f_00002-5-2 loss: 1.087113  [   96/  124]
train() client id: f_00002-6-0 loss: 1.299886  [   32/  124]
train() client id: f_00002-6-1 loss: 0.919376  [   64/  124]
train() client id: f_00002-6-2 loss: 1.340585  [   96/  124]
train() client id: f_00003-0-0 loss: 0.545755  [   32/   43]
train() client id: f_00003-1-0 loss: 0.663730  [   32/   43]
train() client id: f_00003-2-0 loss: 0.666224  [   32/   43]
train() client id: f_00003-3-0 loss: 0.683334  [   32/   43]
train() client id: f_00003-4-0 loss: 0.673221  [   32/   43]
train() client id: f_00003-5-0 loss: 0.651651  [   32/   43]
train() client id: f_00003-6-0 loss: 0.559008  [   32/   43]
train() client id: f_00004-0-0 loss: 0.766284  [   32/  306]
train() client id: f_00004-0-1 loss: 0.851845  [   64/  306]
train() client id: f_00004-0-2 loss: 0.711435  [   96/  306]
train() client id: f_00004-0-3 loss: 0.879231  [  128/  306]
train() client id: f_00004-0-4 loss: 0.827276  [  160/  306]
train() client id: f_00004-0-5 loss: 0.944555  [  192/  306]
train() client id: f_00004-0-6 loss: 0.772273  [  224/  306]
train() client id: f_00004-0-7 loss: 0.755424  [  256/  306]
train() client id: f_00004-0-8 loss: 0.681126  [  288/  306]
train() client id: f_00004-1-0 loss: 0.709231  [   32/  306]
train() client id: f_00004-1-1 loss: 0.699083  [   64/  306]
train() client id: f_00004-1-2 loss: 0.675456  [   96/  306]
train() client id: f_00004-1-3 loss: 0.826814  [  128/  306]
train() client id: f_00004-1-4 loss: 0.670555  [  160/  306]
train() client id: f_00004-1-5 loss: 1.072849  [  192/  306]
train() client id: f_00004-1-6 loss: 0.837622  [  224/  306]
train() client id: f_00004-1-7 loss: 0.838116  [  256/  306]
train() client id: f_00004-1-8 loss: 0.853984  [  288/  306]
train() client id: f_00004-2-0 loss: 0.794157  [   32/  306]
train() client id: f_00004-2-1 loss: 0.674644  [   64/  306]
train() client id: f_00004-2-2 loss: 0.727079  [   96/  306]
train() client id: f_00004-2-3 loss: 0.745362  [  128/  306]
train() client id: f_00004-2-4 loss: 0.864433  [  160/  306]
train() client id: f_00004-2-5 loss: 0.755590  [  192/  306]
train() client id: f_00004-2-6 loss: 0.827374  [  224/  306]
train() client id: f_00004-2-7 loss: 0.935191  [  256/  306]
train() client id: f_00004-2-8 loss: 0.751132  [  288/  306]
train() client id: f_00004-3-0 loss: 0.857325  [   32/  306]
train() client id: f_00004-3-1 loss: 0.766562  [   64/  306]
train() client id: f_00004-3-2 loss: 0.930866  [   96/  306]
train() client id: f_00004-3-3 loss: 0.762909  [  128/  306]
train() client id: f_00004-3-4 loss: 0.655139  [  160/  306]
train() client id: f_00004-3-5 loss: 0.993824  [  192/  306]
train() client id: f_00004-3-6 loss: 0.645150  [  224/  306]
train() client id: f_00004-3-7 loss: 0.638407  [  256/  306]
train() client id: f_00004-3-8 loss: 0.876131  [  288/  306]
train() client id: f_00004-4-0 loss: 0.791802  [   32/  306]
train() client id: f_00004-4-1 loss: 0.798911  [   64/  306]
train() client id: f_00004-4-2 loss: 0.823095  [   96/  306]
train() client id: f_00004-4-3 loss: 0.824130  [  128/  306]
train() client id: f_00004-4-4 loss: 0.809843  [  160/  306]
train() client id: f_00004-4-5 loss: 0.744338  [  192/  306]
train() client id: f_00004-4-6 loss: 0.625510  [  224/  306]
train() client id: f_00004-4-7 loss: 0.927134  [  256/  306]
train() client id: f_00004-4-8 loss: 0.825245  [  288/  306]
train() client id: f_00004-5-0 loss: 0.806901  [   32/  306]
train() client id: f_00004-5-1 loss: 0.535912  [   64/  306]
train() client id: f_00004-5-2 loss: 0.908178  [   96/  306]
train() client id: f_00004-5-3 loss: 0.670584  [  128/  306]
train() client id: f_00004-5-4 loss: 0.809531  [  160/  306]
train() client id: f_00004-5-5 loss: 0.741242  [  192/  306]
train() client id: f_00004-5-6 loss: 0.877424  [  224/  306]
train() client id: f_00004-5-7 loss: 0.860957  [  256/  306]
train() client id: f_00004-5-8 loss: 0.939064  [  288/  306]
train() client id: f_00004-6-0 loss: 0.774910  [   32/  306]
train() client id: f_00004-6-1 loss: 0.856560  [   64/  306]
train() client id: f_00004-6-2 loss: 0.782276  [   96/  306]
train() client id: f_00004-6-3 loss: 0.879172  [  128/  306]
train() client id: f_00004-6-4 loss: 0.751935  [  160/  306]
train() client id: f_00004-6-5 loss: 0.742041  [  192/  306]
train() client id: f_00004-6-6 loss: 0.811283  [  224/  306]
train() client id: f_00004-6-7 loss: 0.690477  [  256/  306]
train() client id: f_00004-6-8 loss: 0.834819  [  288/  306]
train() client id: f_00005-0-0 loss: 0.868698  [   32/  146]
train() client id: f_00005-0-1 loss: 0.623327  [   64/  146]
train() client id: f_00005-0-2 loss: 0.603883  [   96/  146]
train() client id: f_00005-0-3 loss: 0.528141  [  128/  146]
train() client id: f_00005-1-0 loss: 0.800495  [   32/  146]
train() client id: f_00005-1-1 loss: 0.758400  [   64/  146]
train() client id: f_00005-1-2 loss: 0.673222  [   96/  146]
train() client id: f_00005-1-3 loss: 0.491045  [  128/  146]
train() client id: f_00005-2-0 loss: 0.590204  [   32/  146]
train() client id: f_00005-2-1 loss: 0.637477  [   64/  146]
train() client id: f_00005-2-2 loss: 0.615545  [   96/  146]
train() client id: f_00005-2-3 loss: 0.643373  [  128/  146]
train() client id: f_00005-3-0 loss: 0.472568  [   32/  146]
train() client id: f_00005-3-1 loss: 0.686052  [   64/  146]
train() client id: f_00005-3-2 loss: 0.776298  [   96/  146]
train() client id: f_00005-3-3 loss: 0.538433  [  128/  146]
train() client id: f_00005-4-0 loss: 0.563704  [   32/  146]
train() client id: f_00005-4-1 loss: 0.595681  [   64/  146]
train() client id: f_00005-4-2 loss: 0.576338  [   96/  146]
train() client id: f_00005-4-3 loss: 0.609151  [  128/  146]
train() client id: f_00005-5-0 loss: 0.514021  [   32/  146]
train() client id: f_00005-5-1 loss: 0.768238  [   64/  146]
train() client id: f_00005-5-2 loss: 0.417894  [   96/  146]
train() client id: f_00005-5-3 loss: 0.770246  [  128/  146]
train() client id: f_00005-6-0 loss: 0.680384  [   32/  146]
train() client id: f_00005-6-1 loss: 0.521912  [   64/  146]
train() client id: f_00005-6-2 loss: 0.799683  [   96/  146]
train() client id: f_00005-6-3 loss: 0.716565  [  128/  146]
train() client id: f_00006-0-0 loss: 0.417933  [   32/   54]
train() client id: f_00006-1-0 loss: 0.471100  [   32/   54]
train() client id: f_00006-2-0 loss: 0.470971  [   32/   54]
train() client id: f_00006-3-0 loss: 0.448435  [   32/   54]
train() client id: f_00006-4-0 loss: 0.471308  [   32/   54]
train() client id: f_00006-5-0 loss: 0.437545  [   32/   54]
train() client id: f_00006-6-0 loss: 0.511473  [   32/   54]
train() client id: f_00007-0-0 loss: 0.417638  [   32/  179]
train() client id: f_00007-0-1 loss: 0.565240  [   64/  179]
train() client id: f_00007-0-2 loss: 0.427127  [   96/  179]
train() client id: f_00007-0-3 loss: 0.325815  [  128/  179]
train() client id: f_00007-0-4 loss: 0.425903  [  160/  179]
train() client id: f_00007-1-0 loss: 0.386982  [   32/  179]
train() client id: f_00007-1-1 loss: 0.357765  [   64/  179]
train() client id: f_00007-1-2 loss: 0.341776  [   96/  179]
train() client id: f_00007-1-3 loss: 0.331506  [  128/  179]
train() client id: f_00007-1-4 loss: 0.376435  [  160/  179]
train() client id: f_00007-2-0 loss: 0.203276  [   32/  179]
train() client id: f_00007-2-1 loss: 0.534133  [   64/  179]
train() client id: f_00007-2-2 loss: 0.331319  [   96/  179]
train() client id: f_00007-2-3 loss: 0.598333  [  128/  179]
train() client id: f_00007-2-4 loss: 0.315958  [  160/  179]
train() client id: f_00007-3-0 loss: 0.446426  [   32/  179]
train() client id: f_00007-3-1 loss: 0.363126  [   64/  179]
train() client id: f_00007-3-2 loss: 0.524542  [   96/  179]
train() client id: f_00007-3-3 loss: 0.507663  [  128/  179]
train() client id: f_00007-3-4 loss: 0.224110  [  160/  179]
train() client id: f_00007-4-0 loss: 0.400985  [   32/  179]
train() client id: f_00007-4-1 loss: 0.534178  [   64/  179]
train() client id: f_00007-4-2 loss: 0.364635  [   96/  179]
train() client id: f_00007-4-3 loss: 0.337984  [  128/  179]
train() client id: f_00007-4-4 loss: 0.400099  [  160/  179]
train() client id: f_00007-5-0 loss: 0.230049  [   32/  179]
train() client id: f_00007-5-1 loss: 0.325354  [   64/  179]
train() client id: f_00007-5-2 loss: 0.334229  [   96/  179]
train() client id: f_00007-5-3 loss: 0.425256  [  128/  179]
train() client id: f_00007-5-4 loss: 0.619155  [  160/  179]
train() client id: f_00007-6-0 loss: 0.291819  [   32/  179]
train() client id: f_00007-6-1 loss: 0.201177  [   64/  179]
train() client id: f_00007-6-2 loss: 0.686204  [   96/  179]
train() client id: f_00007-6-3 loss: 0.388272  [  128/  179]
train() client id: f_00007-6-4 loss: 0.320730  [  160/  179]
train() client id: f_00008-0-0 loss: 0.741663  [   32/  130]
train() client id: f_00008-0-1 loss: 0.828900  [   64/  130]
train() client id: f_00008-0-2 loss: 0.735143  [   96/  130]
train() client id: f_00008-0-3 loss: 0.931861  [  128/  130]
train() client id: f_00008-1-0 loss: 0.918801  [   32/  130]
train() client id: f_00008-1-1 loss: 0.774714  [   64/  130]
train() client id: f_00008-1-2 loss: 0.861264  [   96/  130]
train() client id: f_00008-1-3 loss: 0.664984  [  128/  130]
train() client id: f_00008-2-0 loss: 0.912876  [   32/  130]
train() client id: f_00008-2-1 loss: 0.705235  [   64/  130]
train() client id: f_00008-2-2 loss: 0.784040  [   96/  130]
train() client id: f_00008-2-3 loss: 0.838213  [  128/  130]
train() client id: f_00008-3-0 loss: 0.802986  [   32/  130]
train() client id: f_00008-3-1 loss: 0.778474  [   64/  130]
train() client id: f_00008-3-2 loss: 0.759171  [   96/  130]
train() client id: f_00008-3-3 loss: 0.896474  [  128/  130]
train() client id: f_00008-4-0 loss: 0.751379  [   32/  130]
train() client id: f_00008-4-1 loss: 0.786139  [   64/  130]
train() client id: f_00008-4-2 loss: 0.860769  [   96/  130]
train() client id: f_00008-4-3 loss: 0.842084  [  128/  130]
train() client id: f_00008-5-0 loss: 0.751296  [   32/  130]
train() client id: f_00008-5-1 loss: 0.782871  [   64/  130]
train() client id: f_00008-5-2 loss: 0.874517  [   96/  130]
train() client id: f_00008-5-3 loss: 0.825953  [  128/  130]
train() client id: f_00008-6-0 loss: 0.742690  [   32/  130]
train() client id: f_00008-6-1 loss: 0.720545  [   64/  130]
train() client id: f_00008-6-2 loss: 0.903086  [   96/  130]
train() client id: f_00008-6-3 loss: 0.826134  [  128/  130]
train() client id: f_00009-0-0 loss: 1.264627  [   32/  118]
train() client id: f_00009-0-1 loss: 0.828266  [   64/  118]
train() client id: f_00009-0-2 loss: 0.872523  [   96/  118]
train() client id: f_00009-1-0 loss: 0.956408  [   32/  118]
train() client id: f_00009-1-1 loss: 0.887060  [   64/  118]
train() client id: f_00009-1-2 loss: 0.863486  [   96/  118]
train() client id: f_00009-2-0 loss: 0.784360  [   32/  118]
train() client id: f_00009-2-1 loss: 0.909740  [   64/  118]
train() client id: f_00009-2-2 loss: 0.865749  [   96/  118]
train() client id: f_00009-3-0 loss: 0.954417  [   32/  118]
train() client id: f_00009-3-1 loss: 0.764718  [   64/  118]
train() client id: f_00009-3-2 loss: 0.877254  [   96/  118]
train() client id: f_00009-4-0 loss: 0.940940  [   32/  118]
train() client id: f_00009-4-1 loss: 0.701896  [   64/  118]
train() client id: f_00009-4-2 loss: 0.910087  [   96/  118]
train() client id: f_00009-5-0 loss: 0.861645  [   32/  118]
train() client id: f_00009-5-1 loss: 0.984448  [   64/  118]
train() client id: f_00009-5-2 loss: 0.842413  [   96/  118]
train() client id: f_00009-6-0 loss: 0.879750  [   32/  118]
train() client id: f_00009-6-1 loss: 1.010996  [   64/  118]
train() client id: f_00009-6-2 loss: 0.714509  [   96/  118]
At round 70 accuracy: 0.6472148541114059
At round 70 training accuracy: 0.5895372233400402
At round 70 training loss: 0.8316048809869987
update_location
xs = [  -3.9056584     4.20031788  370.00902392   18.81129433    0.97929623
    3.95640986 -332.44319194 -311.32485185  354.66397685 -297.06087855]
ys = [ 362.5879595   345.55583871    1.32061395 -332.45517586  324.35018685
  307.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [376.14529445 359.75891974 383.2863444  347.67845598 339.41715149
 323.67453495 347.16763445 326.99210944 368.91083807 313.46638962]
dists_bs = [253.89469525 247.39047687 572.01343104 543.20675007 230.81202527
 222.85644543 237.35836564 221.18390469 552.5653321  210.09367269]
uav_gains = [9.01731222e-13 1.07572493e-12 8.40891865e-13 1.24623615e-12
 1.39163386e-12 1.76012882e-12 1.25446710e-12 1.67039793e-12
 9.71887012e-13 2.08779148e-12]
bs_gains = [2.04297913e-11 2.19695797e-11 2.10163190e-12 2.42879938e-12
 2.66790571e-11 2.94322543e-11 2.46695578e-11 3.00596713e-11
 2.31536738e-12 3.47166460e-11]
Round 71
-------------------------------
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.59747704 3.1514096  1.57881481 0.60189948 3.63223305 1.7491537
 0.73097414 2.18851291 1.6003418  1.41859333]
obj_prev = 18.24940985510058
eta_min = 2.2153155362897003e-59	eta_max = 0.954951968660827
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 4.14802660303202	eta = 0.909090909090909
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 11.665880506041221	eta = 0.323244634087488
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 7.328813927254386	eta = 0.5145352730897301
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.605790167817666	eta = 0.5708527185521312
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.559990536518958	eta = 0.5748382188192438
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.55977636966569	eta = 0.5748569864243457
eta = 0.5748569864243457
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.04568455 0.09608258 0.0449594  0.01559076 0.11094819 0.05293607
 0.01957909 0.06490107 0.04713485 0.04278394]
ene_total = [0.68305847 0.95139002 0.68780539 0.3652074  1.08286069 0.55421391
 0.40065058 0.78194702 0.59332082 0.45932206]
ti_comp = [2.6166745  2.82525406 2.60419719 2.66565241 2.82917673 2.83103203
 2.66651706 2.69997525 2.72996789 2.83397532]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [8.70339434e-07 6.94542833e-06 8.37517236e-07 3.33330274e-08
 1.06640102e-05 1.15676577e-06 6.59734165e-08 2.34377520e-06
 8.78195905e-07 6.09439114e-07]
ene_total = [0.26808419 0.07867313 0.27941784 0.2235868  0.07514369 0.07337205
 0.22280168 0.19243014 0.16517258 0.0706935 ]
optimize_network iter = 0 obj = 1.6493755938248962
eta = 0.5748569864243457
freqs = [ 8729505.68014144 17004237.96072038  8632103.28335727  2924379.60099508
 19607858.17168588  9349252.32733673  3671285.85880158 12018826.22316111
  8632858.47902665  7548397.32371458]
eta_min = 0.5748569864243458	eta_max = 0.8123930873707683
af = 0.00017023437323749214	bf = 0.7011262629418353	zeta = 0.00018725781056124136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.49832217e-07 1.19568169e-06 1.44181752e-07 5.73840643e-09
 1.83584959e-06 1.99141592e-07 1.13575726e-08 4.03489744e-07
 1.51184738e-07 1.04917243e-07]
ene_total = [1.25368474 0.36767638 1.30668851 1.04561886 0.35103995 0.34308906
 1.04194602 0.8998312  0.77241077 0.33058187]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 1 obj = 3.7373617634764136
eta = 0.8123930873707683
freqs = [ 8660759.82156696 15044717.2754607   8632103.28335727  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144383
  8017957.73562188  6650755.80408876]
eta_min = 0.8123930873707704	eta_max = 0.8123930873707671
af = 0.0001378387179910577	bf = 0.7011262629418353	zeta = 0.0001516225897901635	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.47481618e-07 9.35985870e-07 1.44181752e-07 5.32207417e-09
 1.43171779e-06 1.55028805e-07 1.05228684e-08 3.59748455e-07
 1.30414606e-07 8.14478020e-08]
ene_total = [1.25368464 0.36766535 1.30668851 1.04561884 0.35102278 0.34308719
 1.04194599 0.89982934 0.77240989 0.33058088]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 2 obj = 3.7373617634763896
eta = 0.8123930873707671
freqs = [ 8660759.82156695 15044717.2754607   8632103.28335726  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144382
  8017957.73562187  6650755.80408876]
Done!
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [2.83533115e-07 1.79943097e-06 2.77189130e-07 1.02316770e-08
 2.75247459e-06 2.98042568e-07 2.02301936e-08 6.91615688e-07
 2.50721820e-07 1.56583237e-07]
ene_total = [0.02951226 0.00865582 0.03075999 0.0246142  0.00826451 0.00807652
 0.02452774 0.02118259 0.01818289 0.00778205]
At round 71 energy consumption: 0.18155856217199728
At round 71 eta: 0.8123930873707671
At round 71 a_n: 3.8618477214328486
At round 71 local rounds: 6.803476774877765
At round 71 global rounds: 20.584783723108377
gradient difference: 0.6552057266235352
train() client id: f_00000-0-0 loss: 1.010460  [   32/  126]
train() client id: f_00000-0-1 loss: 1.128339  [   64/  126]
train() client id: f_00000-0-2 loss: 0.918140  [   96/  126]
train() client id: f_00000-1-0 loss: 0.880452  [   32/  126]
train() client id: f_00000-1-1 loss: 1.001238  [   64/  126]
train() client id: f_00000-1-2 loss: 0.989877  [   96/  126]
train() client id: f_00000-2-0 loss: 0.932642  [   32/  126]
train() client id: f_00000-2-1 loss: 0.749951  [   64/  126]
train() client id: f_00000-2-2 loss: 0.957666  [   96/  126]
train() client id: f_00000-3-0 loss: 0.972565  [   32/  126]
train() client id: f_00000-3-1 loss: 0.895381  [   64/  126]
train() client id: f_00000-3-2 loss: 0.986933  [   96/  126]
train() client id: f_00000-4-0 loss: 0.931340  [   32/  126]
train() client id: f_00000-4-1 loss: 0.864203  [   64/  126]
train() client id: f_00000-4-2 loss: 1.005491  [   96/  126]
train() client id: f_00000-5-0 loss: 0.914917  [   32/  126]
train() client id: f_00000-5-1 loss: 0.951151  [   64/  126]
train() client id: f_00000-5-2 loss: 0.845145  [   96/  126]
train() client id: f_00001-0-0 loss: 0.353375  [   32/  265]
train() client id: f_00001-0-1 loss: 0.383114  [   64/  265]
train() client id: f_00001-0-2 loss: 0.299662  [   96/  265]
train() client id: f_00001-0-3 loss: 0.307898  [  128/  265]
train() client id: f_00001-0-4 loss: 0.346421  [  160/  265]
train() client id: f_00001-0-5 loss: 0.482194  [  192/  265]
train() client id: f_00001-0-6 loss: 0.398390  [  224/  265]
train() client id: f_00001-0-7 loss: 0.413453  [  256/  265]
train() client id: f_00001-1-0 loss: 0.276383  [   32/  265]
train() client id: f_00001-1-1 loss: 0.432539  [   64/  265]
train() client id: f_00001-1-2 loss: 0.617977  [   96/  265]
train() client id: f_00001-1-3 loss: 0.345975  [  128/  265]
train() client id: f_00001-1-4 loss: 0.295954  [  160/  265]
train() client id: f_00001-1-5 loss: 0.342134  [  192/  265]
train() client id: f_00001-1-6 loss: 0.315473  [  224/  265]
train() client id: f_00001-1-7 loss: 0.289317  [  256/  265]
train() client id: f_00001-2-0 loss: 0.325019  [   32/  265]
train() client id: f_00001-2-1 loss: 0.387721  [   64/  265]
train() client id: f_00001-2-2 loss: 0.354295  [   96/  265]
train() client id: f_00001-2-3 loss: 0.443700  [  128/  265]
train() client id: f_00001-2-4 loss: 0.430472  [  160/  265]
train() client id: f_00001-2-5 loss: 0.237223  [  192/  265]
train() client id: f_00001-2-6 loss: 0.407681  [  224/  265]
train() client id: f_00001-2-7 loss: 0.266833  [  256/  265]
train() client id: f_00001-3-0 loss: 0.246551  [   32/  265]
train() client id: f_00001-3-1 loss: 0.332573  [   64/  265]
train() client id: f_00001-3-2 loss: 0.422200  [   96/  265]
train() client id: f_00001-3-3 loss: 0.314225  [  128/  265]
train() client id: f_00001-3-4 loss: 0.402719  [  160/  265]
train() client id: f_00001-3-5 loss: 0.295758  [  192/  265]
train() client id: f_00001-3-6 loss: 0.305943  [  224/  265]
train() client id: f_00001-3-7 loss: 0.510788  [  256/  265]
train() client id: f_00001-4-0 loss: 0.488011  [   32/  265]
train() client id: f_00001-4-1 loss: 0.368969  [   64/  265]
train() client id: f_00001-4-2 loss: 0.373382  [   96/  265]
train() client id: f_00001-4-3 loss: 0.372395  [  128/  265]
train() client id: f_00001-4-4 loss: 0.317801  [  160/  265]
train() client id: f_00001-4-5 loss: 0.243548  [  192/  265]
train() client id: f_00001-4-6 loss: 0.269582  [  224/  265]
train() client id: f_00001-4-7 loss: 0.340668  [  256/  265]
train() client id: f_00001-5-0 loss: 0.285577  [   32/  265]
train() client id: f_00001-5-1 loss: 0.331668  [   64/  265]
train() client id: f_00001-5-2 loss: 0.388678  [   96/  265]
train() client id: f_00001-5-3 loss: 0.365254  [  128/  265]
train() client id: f_00001-5-4 loss: 0.246258  [  160/  265]
train() client id: f_00001-5-5 loss: 0.331113  [  192/  265]
train() client id: f_00001-5-6 loss: 0.352053  [  224/  265]
train() client id: f_00001-5-7 loss: 0.444629  [  256/  265]
train() client id: f_00002-0-0 loss: 1.111359  [   32/  124]
train() client id: f_00002-0-1 loss: 0.772758  [   64/  124]
train() client id: f_00002-0-2 loss: 0.715499  [   96/  124]
train() client id: f_00002-1-0 loss: 1.081942  [   32/  124]
train() client id: f_00002-1-1 loss: 0.746116  [   64/  124]
train() client id: f_00002-1-2 loss: 0.577730  [   96/  124]
train() client id: f_00002-2-0 loss: 0.812372  [   32/  124]
train() client id: f_00002-2-1 loss: 0.927417  [   64/  124]
train() client id: f_00002-2-2 loss: 0.619861  [   96/  124]
train() client id: f_00002-3-0 loss: 0.771746  [   32/  124]
train() client id: f_00002-3-1 loss: 0.903176  [   64/  124]
train() client id: f_00002-3-2 loss: 0.577770  [   96/  124]
train() client id: f_00002-4-0 loss: 0.767358  [   32/  124]
train() client id: f_00002-4-1 loss: 0.739659  [   64/  124]
train() client id: f_00002-4-2 loss: 0.747163  [   96/  124]
train() client id: f_00002-5-0 loss: 0.585911  [   32/  124]
train() client id: f_00002-5-1 loss: 0.862053  [   64/  124]
train() client id: f_00002-5-2 loss: 0.808491  [   96/  124]
train() client id: f_00003-0-0 loss: 0.706356  [   32/   43]
train() client id: f_00003-1-0 loss: 0.822415  [   32/   43]
train() client id: f_00003-2-0 loss: 0.724271  [   32/   43]
train() client id: f_00003-3-0 loss: 0.759630  [   32/   43]
train() client id: f_00003-4-0 loss: 0.622566  [   32/   43]
train() client id: f_00003-5-0 loss: 0.560725  [   32/   43]
train() client id: f_00004-0-0 loss: 0.826820  [   32/  306]
train() client id: f_00004-0-1 loss: 0.733002  [   64/  306]
train() client id: f_00004-0-2 loss: 0.738466  [   96/  306]
train() client id: f_00004-0-3 loss: 0.799009  [  128/  306]
train() client id: f_00004-0-4 loss: 0.772518  [  160/  306]
train() client id: f_00004-0-5 loss: 0.752517  [  192/  306]
train() client id: f_00004-0-6 loss: 0.810722  [  224/  306]
train() client id: f_00004-0-7 loss: 0.778687  [  256/  306]
train() client id: f_00004-0-8 loss: 0.850145  [  288/  306]
train() client id: f_00004-1-0 loss: 0.806085  [   32/  306]
train() client id: f_00004-1-1 loss: 0.981351  [   64/  306]
train() client id: f_00004-1-2 loss: 0.657910  [   96/  306]
train() client id: f_00004-1-3 loss: 0.850842  [  128/  306]
train() client id: f_00004-1-4 loss: 0.808359  [  160/  306]
train() client id: f_00004-1-5 loss: 0.814954  [  192/  306]
train() client id: f_00004-1-6 loss: 0.676910  [  224/  306]
train() client id: f_00004-1-7 loss: 0.705981  [  256/  306]
train() client id: f_00004-1-8 loss: 0.800567  [  288/  306]
train() client id: f_00004-2-0 loss: 0.712545  [   32/  306]
train() client id: f_00004-2-1 loss: 0.806373  [   64/  306]
train() client id: f_00004-2-2 loss: 0.869828  [   96/  306]
train() client id: f_00004-2-3 loss: 0.801670  [  128/  306]
train() client id: f_00004-2-4 loss: 0.704961  [  160/  306]
train() client id: f_00004-2-5 loss: 0.848397  [  192/  306]
train() client id: f_00004-2-6 loss: 0.820406  [  224/  306]
train() client id: f_00004-2-7 loss: 0.900077  [  256/  306]
train() client id: f_00004-2-8 loss: 0.702365  [  288/  306]
train() client id: f_00004-3-0 loss: 0.737278  [   32/  306]
train() client id: f_00004-3-1 loss: 0.770210  [   64/  306]
train() client id: f_00004-3-2 loss: 0.732883  [   96/  306]
train() client id: f_00004-3-3 loss: 0.805227  [  128/  306]
train() client id: f_00004-3-4 loss: 0.744369  [  160/  306]
train() client id: f_00004-3-5 loss: 0.856030  [  192/  306]
train() client id: f_00004-3-6 loss: 0.823552  [  224/  306]
train() client id: f_00004-3-7 loss: 0.847273  [  256/  306]
train() client id: f_00004-3-8 loss: 0.861051  [  288/  306]
train() client id: f_00004-4-0 loss: 0.890738  [   32/  306]
train() client id: f_00004-4-1 loss: 0.785524  [   64/  306]
train() client id: f_00004-4-2 loss: 0.753788  [   96/  306]
train() client id: f_00004-4-3 loss: 0.770220  [  128/  306]
train() client id: f_00004-4-4 loss: 0.774452  [  160/  306]
train() client id: f_00004-4-5 loss: 0.777107  [  192/  306]
train() client id: f_00004-4-6 loss: 0.704427  [  224/  306]
train() client id: f_00004-4-7 loss: 0.867800  [  256/  306]
train() client id: f_00004-4-8 loss: 0.818359  [  288/  306]
train() client id: f_00004-5-0 loss: 0.857671  [   32/  306]
train() client id: f_00004-5-1 loss: 0.702841  [   64/  306]
train() client id: f_00004-5-2 loss: 0.881543  [   96/  306]
train() client id: f_00004-5-3 loss: 0.842232  [  128/  306]
train() client id: f_00004-5-4 loss: 0.718254  [  160/  306]
train() client id: f_00004-5-5 loss: 0.878189  [  192/  306]
train() client id: f_00004-5-6 loss: 0.703847  [  224/  306]
train() client id: f_00004-5-7 loss: 0.661810  [  256/  306]
train() client id: f_00004-5-8 loss: 0.934793  [  288/  306]
train() client id: f_00005-0-0 loss: 0.372745  [   32/  146]
train() client id: f_00005-0-1 loss: 0.879530  [   64/  146]
train() client id: f_00005-0-2 loss: 0.219598  [   96/  146]
train() client id: f_00005-0-3 loss: 0.761145  [  128/  146]
train() client id: f_00005-1-0 loss: 0.386756  [   32/  146]
train() client id: f_00005-1-1 loss: 0.462375  [   64/  146]
train() client id: f_00005-1-2 loss: 0.552966  [   96/  146]
train() client id: f_00005-1-3 loss: 0.709977  [  128/  146]
train() client id: f_00005-2-0 loss: 0.304186  [   32/  146]
train() client id: f_00005-2-1 loss: 0.989851  [   64/  146]
train() client id: f_00005-2-2 loss: 0.399119  [   96/  146]
train() client id: f_00005-2-3 loss: 0.639865  [  128/  146]
train() client id: f_00005-3-0 loss: 0.427756  [   32/  146]
train() client id: f_00005-3-1 loss: 0.555026  [   64/  146]
train() client id: f_00005-3-2 loss: 0.712856  [   96/  146]
train() client id: f_00005-3-3 loss: 0.455983  [  128/  146]
train() client id: f_00005-4-0 loss: 0.749224  [   32/  146]
train() client id: f_00005-4-1 loss: 0.442618  [   64/  146]
train() client id: f_00005-4-2 loss: 0.637664  [   96/  146]
train() client id: f_00005-4-3 loss: 0.432005  [  128/  146]
train() client id: f_00005-5-0 loss: 0.763772  [   32/  146]
train() client id: f_00005-5-1 loss: 0.433326  [   64/  146]
train() client id: f_00005-5-2 loss: 0.351673  [   96/  146]
train() client id: f_00005-5-3 loss: 0.437814  [  128/  146]
train() client id: f_00006-0-0 loss: 0.473982  [   32/   54]
train() client id: f_00006-1-0 loss: 0.426096  [   32/   54]
train() client id: f_00006-2-0 loss: 0.468403  [   32/   54]
train() client id: f_00006-3-0 loss: 0.442449  [   32/   54]
train() client id: f_00006-4-0 loss: 0.461282  [   32/   54]
train() client id: f_00006-5-0 loss: 0.498801  [   32/   54]
train() client id: f_00007-0-0 loss: 0.691236  [   32/  179]
train() client id: f_00007-0-1 loss: 0.558417  [   64/  179]
train() client id: f_00007-0-2 loss: 0.626652  [   96/  179]
train() client id: f_00007-0-3 loss: 0.755169  [  128/  179]
train() client id: f_00007-0-4 loss: 0.827726  [  160/  179]
train() client id: f_00007-1-0 loss: 0.638172  [   32/  179]
train() client id: f_00007-1-1 loss: 0.731003  [   64/  179]
train() client id: f_00007-1-2 loss: 0.742038  [   96/  179]
train() client id: f_00007-1-3 loss: 0.585157  [  128/  179]
train() client id: f_00007-1-4 loss: 0.615665  [  160/  179]
train() client id: f_00007-2-0 loss: 0.864155  [   32/  179]
train() client id: f_00007-2-1 loss: 0.610205  [   64/  179]
train() client id: f_00007-2-2 loss: 0.524083  [   96/  179]
train() client id: f_00007-2-3 loss: 0.669966  [  128/  179]
train() client id: f_00007-2-4 loss: 0.641266  [  160/  179]
train() client id: f_00007-3-0 loss: 0.759317  [   32/  179]
train() client id: f_00007-3-1 loss: 0.629621  [   64/  179]
train() client id: f_00007-3-2 loss: 0.679692  [   96/  179]
train() client id: f_00007-3-3 loss: 0.618623  [  128/  179]
train() client id: f_00007-3-4 loss: 0.579668  [  160/  179]
train() client id: f_00007-4-0 loss: 0.510062  [   32/  179]
train() client id: f_00007-4-1 loss: 0.502770  [   64/  179]
train() client id: f_00007-4-2 loss: 0.822701  [   96/  179]
train() client id: f_00007-4-3 loss: 0.669945  [  128/  179]
train() client id: f_00007-4-4 loss: 0.756260  [  160/  179]
train() client id: f_00007-5-0 loss: 0.633454  [   32/  179]
train() client id: f_00007-5-1 loss: 0.493263  [   64/  179]
train() client id: f_00007-5-2 loss: 0.846665  [   96/  179]
train() client id: f_00007-5-3 loss: 0.816994  [  128/  179]
train() client id: f_00007-5-4 loss: 0.478961  [  160/  179]
train() client id: f_00008-0-0 loss: 0.758609  [   32/  130]
train() client id: f_00008-0-1 loss: 0.687615  [   64/  130]
train() client id: f_00008-0-2 loss: 0.719579  [   96/  130]
train() client id: f_00008-0-3 loss: 0.654237  [  128/  130]
train() client id: f_00008-1-0 loss: 0.656916  [   32/  130]
train() client id: f_00008-1-1 loss: 0.716256  [   64/  130]
train() client id: f_00008-1-2 loss: 0.666842  [   96/  130]
train() client id: f_00008-1-3 loss: 0.760713  [  128/  130]
train() client id: f_00008-2-0 loss: 0.690289  [   32/  130]
train() client id: f_00008-2-1 loss: 0.702547  [   64/  130]
train() client id: f_00008-2-2 loss: 0.691618  [   96/  130]
train() client id: f_00008-2-3 loss: 0.718484  [  128/  130]
train() client id: f_00008-3-0 loss: 0.670059  [   32/  130]
train() client id: f_00008-3-1 loss: 0.788214  [   64/  130]
train() client id: f_00008-3-2 loss: 0.755276  [   96/  130]
train() client id: f_00008-3-3 loss: 0.609774  [  128/  130]
train() client id: f_00008-4-0 loss: 0.650082  [   32/  130]
train() client id: f_00008-4-1 loss: 0.771054  [   64/  130]
train() client id: f_00008-4-2 loss: 0.675548  [   96/  130]
train() client id: f_00008-4-3 loss: 0.694117  [  128/  130]
train() client id: f_00008-5-0 loss: 0.784223  [   32/  130]
train() client id: f_00008-5-1 loss: 0.598840  [   64/  130]
train() client id: f_00008-5-2 loss: 0.709346  [   96/  130]
train() client id: f_00008-5-3 loss: 0.720571  [  128/  130]
train() client id: f_00009-0-0 loss: 1.069343  [   32/  118]
train() client id: f_00009-0-1 loss: 0.973846  [   64/  118]
train() client id: f_00009-0-2 loss: 0.918117  [   96/  118]
train() client id: f_00009-1-0 loss: 0.858781  [   32/  118]
train() client id: f_00009-1-1 loss: 1.068791  [   64/  118]
train() client id: f_00009-1-2 loss: 0.932594  [   96/  118]
train() client id: f_00009-2-0 loss: 0.872093  [   32/  118]
train() client id: f_00009-2-1 loss: 1.024702  [   64/  118]
train() client id: f_00009-2-2 loss: 0.899282  [   96/  118]
train() client id: f_00009-3-0 loss: 0.902211  [   32/  118]
train() client id: f_00009-3-1 loss: 1.094915  [   64/  118]
train() client id: f_00009-3-2 loss: 0.778720  [   96/  118]
train() client id: f_00009-4-0 loss: 0.901648  [   32/  118]
train() client id: f_00009-4-1 loss: 1.022814  [   64/  118]
train() client id: f_00009-4-2 loss: 0.910096  [   96/  118]
train() client id: f_00009-5-0 loss: 0.875837  [   32/  118]
train() client id: f_00009-5-1 loss: 0.969706  [   64/  118]
train() client id: f_00009-5-2 loss: 0.973070  [   96/  118]
At round 71 accuracy: 0.6472148541114059
At round 71 training accuracy: 0.5888665325285044
At round 71 training loss: 0.8180691816848003
update_location
xs = [  -3.9056584     4.20031788  375.00902392   18.81129433    0.97929623
    3.95640986 -337.44319194 -316.32485185  359.66397685 -302.06087855]
ys = [ 367.5879595   350.55583871    1.32061395 -337.45517586  329.35018685
  312.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [380.96740298 364.56417641 388.11533343 352.46256611 344.19835066
 328.43316837 351.95851791 331.75606724 373.72027804 318.20871485]
dists_bs = [257.61093895 250.86372084 576.77938202 547.88057556 234.07839046
 225.87194774 240.70402087 224.30017435 557.35992501 213.03746171]
uav_gains = [8.59823011e-13 1.01888229e-12 8.03777175e-13 1.17344140e-12
 1.30420376e-12 1.63364151e-12 1.18076150e-12 1.55363346e-12
 9.24212893e-13 1.92508765e-12]
bs_gains = [1.96152589e-11 2.11284714e-11 2.05336828e-12 2.37122932e-12
 2.56497054e-11 2.83452084e-11 2.37214220e-11 2.89048802e-11
 2.26002902e-12 3.33900711e-11]
Round 72
-------------------------------
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.45877326 2.87217542 1.44178771 0.55140047 3.31032656 1.59425308
 0.66902829 1.99714791 1.45909395 1.29300424]
obj_prev = 16.646990883599326
eta_min = 4.971520837874105e-65	eta_max = 0.9577477121135591
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 3.7800966926678474	eta = 0.9090909090909091
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 10.808661784694127	eta = 0.3179349680138224
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.733867507222867	eta = 0.5103236045412168
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.058721545799979	eta = 0.5671908690326205
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015928897442357	eta = 0.5712254245973457
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015728044345185	eta = 0.5712444966689002
eta = 0.5712444966689002
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.04620474 0.09717663 0.04547133 0.01576828 0.11221151 0.05353882
 0.01980203 0.06564007 0.04767156 0.04327111]
ene_total = [0.62840586 0.86936    0.63267678 0.33813836 0.98949538 0.50632297
 0.37054313 0.71901402 0.54216497 0.41960659]
ti_comp = [2.91272976 3.128895   2.90019242 2.96199725 3.13288317 3.13480384
 2.96285607 2.99669538 3.03246495 3.13777295]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [7.26673296e-07 5.85846172e-06 6.98617374e-07 2.79295552e-08
 8.99712661e-06 9.76035873e-07 5.52825571e-08 1.96834785e-06
 7.36319715e-07 5.14317079e-07]
ene_total = [0.24915505 0.07176489 0.2594457  0.20870969 0.06851708 0.06687473
 0.20800497 0.18024477 0.15087435 0.06443384]
optimize_network iter = 0 obj = 1.52802506642978
eta = 0.5712444966689002
freqs = [ 7931518.36924844 15528906.23802998  7839364.44612375  2661765.47781446
 17908664.85509679  8539421.8514933   3341713.30591816 10952075.17012927
  7860199.06137994  6895193.77630764]
eta_min = 0.5712444966689012	eta_max = 0.8252306516812821
af = 0.0001292173845627228	bf = 0.6550768027395515	zeta = 0.00014213912301899509	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.23691134e-07 9.97201600e-07 1.18915578e-07 4.75404610e-09
 1.53145134e-06 1.66136535e-07 9.40995383e-09 3.35043518e-07
 1.25333105e-07 8.75447921e-08]
ene_total = [1.17506927 0.33827729 1.22360405 0.98433881 0.32285882 0.31537062
 0.98101428 0.85002707 0.71154662 0.30387348]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 1 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
eta_min = 0.8252306516812807	eta_max = 0.8252306516812821
af = 0.00010312038831994186	bf = 0.6550768027395515	zeta = 0.00011343242715193606	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.21677013e-07 7.67354369e-07 1.18915578e-07 4.39574606e-09
 1.17380585e-06 1.27096235e-07 8.69166314e-09 2.97251805e-07
 1.06754438e-07 6.67767395e-08]
ene_total = [1.17506919 0.33826839 1.22360405 0.98433879 0.32284498 0.31536911
 0.98101425 0.85002561 0.7115459  0.30387267]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 2 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
Done!
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.33923815e-07 1.47523724e-06 2.28614962e-07 8.45081303e-09
 2.25663941e-06 2.44342258e-07 1.67097050e-08 5.71466002e-07
 2.05235193e-07 1.28378148e-07]
ene_total = [0.03035399 0.00873871 0.03160772 0.02542702 0.00834068 0.0081466
 0.02534115 0.02195777 0.01838045 0.00784957]
At round 72 energy consumption: 0.18614365042570813
At round 72 eta: 0.8252306516812821
At round 72 a_n: 3.5193018744635296
At round 72 local rounds: 6.290079613751617
At round 72 global rounds: 20.136836970093626
gradient difference: 0.6809030175209045
train() client id: f_00000-0-0 loss: 1.266497  [   32/  126]
train() client id: f_00000-0-1 loss: 1.118920  [   64/  126]
train() client id: f_00000-0-2 loss: 1.086297  [   96/  126]
train() client id: f_00000-1-0 loss: 1.167614  [   32/  126]
train() client id: f_00000-1-1 loss: 1.194143  [   64/  126]
train() client id: f_00000-1-2 loss: 0.935363  [   96/  126]
train() client id: f_00000-2-0 loss: 0.927248  [   32/  126]
train() client id: f_00000-2-1 loss: 1.108599  [   64/  126]
train() client id: f_00000-2-2 loss: 0.970114  [   96/  126]
train() client id: f_00000-3-0 loss: 0.951459  [   32/  126]
train() client id: f_00000-3-1 loss: 1.065629  [   64/  126]
train() client id: f_00000-3-2 loss: 0.880185  [   96/  126]
train() client id: f_00000-4-0 loss: 1.028316  [   32/  126]
train() client id: f_00000-4-1 loss: 0.834334  [   64/  126]
train() client id: f_00000-4-2 loss: 0.955889  [   96/  126]
train() client id: f_00000-5-0 loss: 0.765568  [   32/  126]
train() client id: f_00000-5-1 loss: 0.974370  [   64/  126]
train() client id: f_00000-5-2 loss: 0.920204  [   96/  126]
train() client id: f_00001-0-0 loss: 0.416491  [   32/  265]
train() client id: f_00001-0-1 loss: 0.491289  [   64/  265]
train() client id: f_00001-0-2 loss: 0.493659  [   96/  265]
train() client id: f_00001-0-3 loss: 0.332233  [  128/  265]
train() client id: f_00001-0-4 loss: 0.427985  [  160/  265]
train() client id: f_00001-0-5 loss: 0.440646  [  192/  265]
train() client id: f_00001-0-6 loss: 0.314460  [  224/  265]
train() client id: f_00001-0-7 loss: 0.319928  [  256/  265]
train() client id: f_00001-1-0 loss: 0.472187  [   32/  265]
train() client id: f_00001-1-1 loss: 0.411311  [   64/  265]
train() client id: f_00001-1-2 loss: 0.356430  [   96/  265]
train() client id: f_00001-1-3 loss: 0.345111  [  128/  265]
train() client id: f_00001-1-4 loss: 0.501766  [  160/  265]
train() client id: f_00001-1-5 loss: 0.336888  [  192/  265]
train() client id: f_00001-1-6 loss: 0.450488  [  224/  265]
train() client id: f_00001-1-7 loss: 0.324055  [  256/  265]
train() client id: f_00001-2-0 loss: 0.309132  [   32/  265]
train() client id: f_00001-2-1 loss: 0.461184  [   64/  265]
train() client id: f_00001-2-2 loss: 0.383521  [   96/  265]
train() client id: f_00001-2-3 loss: 0.434613  [  128/  265]
train() client id: f_00001-2-4 loss: 0.388784  [  160/  265]
train() client id: f_00001-2-5 loss: 0.388442  [  192/  265]
train() client id: f_00001-2-6 loss: 0.403435  [  224/  265]
train() client id: f_00001-2-7 loss: 0.427996  [  256/  265]
train() client id: f_00001-3-0 loss: 0.361327  [   32/  265]
train() client id: f_00001-3-1 loss: 0.357741  [   64/  265]
train() client id: f_00001-3-2 loss: 0.550883  [   96/  265]
train() client id: f_00001-3-3 loss: 0.349709  [  128/  265]
train() client id: f_00001-3-4 loss: 0.310439  [  160/  265]
train() client id: f_00001-3-5 loss: 0.502798  [  192/  265]
train() client id: f_00001-3-6 loss: 0.300041  [  224/  265]
train() client id: f_00001-3-7 loss: 0.425046  [  256/  265]
train() client id: f_00001-4-0 loss: 0.371408  [   32/  265]
train() client id: f_00001-4-1 loss: 0.292448  [   64/  265]
train() client id: f_00001-4-2 loss: 0.355893  [   96/  265]
train() client id: f_00001-4-3 loss: 0.541845  [  128/  265]
train() client id: f_00001-4-4 loss: 0.443374  [  160/  265]
train() client id: f_00001-4-5 loss: 0.357118  [  192/  265]
train() client id: f_00001-4-6 loss: 0.402927  [  224/  265]
train() client id: f_00001-4-7 loss: 0.377803  [  256/  265]
train() client id: f_00001-5-0 loss: 0.302040  [   32/  265]
train() client id: f_00001-5-1 loss: 0.358117  [   64/  265]
train() client id: f_00001-5-2 loss: 0.295914  [   96/  265]
train() client id: f_00001-5-3 loss: 0.328613  [  128/  265]
train() client id: f_00001-5-4 loss: 0.354827  [  160/  265]
train() client id: f_00001-5-5 loss: 0.533887  [  192/  265]
train() client id: f_00001-5-6 loss: 0.373152  [  224/  265]
train() client id: f_00001-5-7 loss: 0.399632  [  256/  265]
train() client id: f_00002-0-0 loss: 0.859985  [   32/  124]
train() client id: f_00002-0-1 loss: 0.935947  [   64/  124]
train() client id: f_00002-0-2 loss: 0.823686  [   96/  124]
train() client id: f_00002-1-0 loss: 0.825045  [   32/  124]
train() client id: f_00002-1-1 loss: 0.928729  [   64/  124]
train() client id: f_00002-1-2 loss: 0.969146  [   96/  124]
train() client id: f_00002-2-0 loss: 0.630183  [   32/  124]
train() client id: f_00002-2-1 loss: 0.889207  [   64/  124]
train() client id: f_00002-2-2 loss: 0.841156  [   96/  124]
train() client id: f_00002-3-0 loss: 0.811389  [   32/  124]
train() client id: f_00002-3-1 loss: 0.762238  [   64/  124]
train() client id: f_00002-3-2 loss: 0.931319  [   96/  124]
train() client id: f_00002-4-0 loss: 0.832451  [   32/  124]
train() client id: f_00002-4-1 loss: 0.951061  [   64/  124]
train() client id: f_00002-4-2 loss: 0.865622  [   96/  124]
train() client id: f_00002-5-0 loss: 0.727261  [   32/  124]
train() client id: f_00002-5-1 loss: 0.930992  [   64/  124]
train() client id: f_00002-5-2 loss: 1.082425  [   96/  124]
train() client id: f_00003-0-0 loss: 0.781720  [   32/   43]
train() client id: f_00003-1-0 loss: 0.783035  [   32/   43]
train() client id: f_00003-2-0 loss: 0.664254  [   32/   43]
train() client id: f_00003-3-0 loss: 0.713161  [   32/   43]
train() client id: f_00003-4-0 loss: 0.979547  [   32/   43]
train() client id: f_00003-5-0 loss: 0.716747  [   32/   43]
train() client id: f_00004-0-0 loss: 0.587426  [   32/  306]
train() client id: f_00004-0-1 loss: 0.638063  [   64/  306]
train() client id: f_00004-0-2 loss: 0.615261  [   96/  306]
train() client id: f_00004-0-3 loss: 0.686804  [  128/  306]
train() client id: f_00004-0-4 loss: 0.664439  [  160/  306]
train() client id: f_00004-0-5 loss: 0.633370  [  192/  306]
train() client id: f_00004-0-6 loss: 0.793903  [  224/  306]
train() client id: f_00004-0-7 loss: 0.785352  [  256/  306]
train() client id: f_00004-0-8 loss: 0.807331  [  288/  306]
train() client id: f_00004-1-0 loss: 0.579958  [   32/  306]
train() client id: f_00004-1-1 loss: 0.740565  [   64/  306]
train() client id: f_00004-1-2 loss: 0.650948  [   96/  306]
train() client id: f_00004-1-3 loss: 0.795199  [  128/  306]
train() client id: f_00004-1-4 loss: 0.714719  [  160/  306]
train() client id: f_00004-1-5 loss: 0.653220  [  192/  306]
train() client id: f_00004-1-6 loss: 0.556558  [  224/  306]
train() client id: f_00004-1-7 loss: 0.655032  [  256/  306]
train() client id: f_00004-1-8 loss: 0.722931  [  288/  306]
train() client id: f_00004-2-0 loss: 0.661779  [   32/  306]
train() client id: f_00004-2-1 loss: 0.720189  [   64/  306]
train() client id: f_00004-2-2 loss: 0.589014  [   96/  306]
train() client id: f_00004-2-3 loss: 0.696652  [  128/  306]
train() client id: f_00004-2-4 loss: 0.792834  [  160/  306]
train() client id: f_00004-2-5 loss: 0.664654  [  192/  306]
train() client id: f_00004-2-6 loss: 0.764115  [  224/  306]
train() client id: f_00004-2-7 loss: 0.646339  [  256/  306]
train() client id: f_00004-2-8 loss: 0.636160  [  288/  306]
train() client id: f_00004-3-0 loss: 0.663195  [   32/  306]
train() client id: f_00004-3-1 loss: 0.704026  [   64/  306]
train() client id: f_00004-3-2 loss: 0.846860  [   96/  306]
train() client id: f_00004-3-3 loss: 0.768483  [  128/  306]
train() client id: f_00004-3-4 loss: 0.624741  [  160/  306]
train() client id: f_00004-3-5 loss: 0.580560  [  192/  306]
train() client id: f_00004-3-6 loss: 0.726068  [  224/  306]
train() client id: f_00004-3-7 loss: 0.664231  [  256/  306]
train() client id: f_00004-3-8 loss: 0.540316  [  288/  306]
train() client id: f_00004-4-0 loss: 0.583533  [   32/  306]
train() client id: f_00004-4-1 loss: 0.537081  [   64/  306]
train() client id: f_00004-4-2 loss: 0.710684  [   96/  306]
train() client id: f_00004-4-3 loss: 0.823841  [  128/  306]
train() client id: f_00004-4-4 loss: 0.665079  [  160/  306]
train() client id: f_00004-4-5 loss: 0.597706  [  192/  306]
train() client id: f_00004-4-6 loss: 0.779082  [  224/  306]
train() client id: f_00004-4-7 loss: 0.840111  [  256/  306]
train() client id: f_00004-4-8 loss: 0.702295  [  288/  306]
train() client id: f_00004-5-0 loss: 0.776343  [   32/  306]
train() client id: f_00004-5-1 loss: 0.675924  [   64/  306]
train() client id: f_00004-5-2 loss: 0.736433  [   96/  306]
train() client id: f_00004-5-3 loss: 0.719910  [  128/  306]
train() client id: f_00004-5-4 loss: 0.813792  [  160/  306]
train() client id: f_00004-5-5 loss: 0.630303  [  192/  306]
train() client id: f_00004-5-6 loss: 0.658493  [  224/  306]
train() client id: f_00004-5-7 loss: 0.711884  [  256/  306]
train() client id: f_00004-5-8 loss: 0.653688  [  288/  306]
train() client id: f_00005-0-0 loss: 0.762257  [   32/  146]
train() client id: f_00005-0-1 loss: 0.643804  [   64/  146]
train() client id: f_00005-0-2 loss: 0.876841  [   96/  146]
train() client id: f_00005-0-3 loss: 0.616579  [  128/  146]
train() client id: f_00005-1-0 loss: 0.817308  [   32/  146]
train() client id: f_00005-1-1 loss: 0.827278  [   64/  146]
train() client id: f_00005-1-2 loss: 0.685849  [   96/  146]
train() client id: f_00005-1-3 loss: 0.876403  [  128/  146]
train() client id: f_00005-2-0 loss: 0.688853  [   32/  146]
train() client id: f_00005-2-1 loss: 0.666624  [   64/  146]
train() client id: f_00005-2-2 loss: 0.772451  [   96/  146]
train() client id: f_00005-2-3 loss: 0.717886  [  128/  146]
train() client id: f_00005-3-0 loss: 0.914142  [   32/  146]
train() client id: f_00005-3-1 loss: 0.681646  [   64/  146]
train() client id: f_00005-3-2 loss: 0.771207  [   96/  146]
train() client id: f_00005-3-3 loss: 0.631816  [  128/  146]
train() client id: f_00005-4-0 loss: 0.899101  [   32/  146]
train() client id: f_00005-4-1 loss: 0.745319  [   64/  146]
train() client id: f_00005-4-2 loss: 0.690345  [   96/  146]
train() client id: f_00005-4-3 loss: 0.586657  [  128/  146]
train() client id: f_00005-5-0 loss: 0.845603  [   32/  146]
train() client id: f_00005-5-1 loss: 0.681612  [   64/  146]
train() client id: f_00005-5-2 loss: 0.955857  [   96/  146]
train() client id: f_00005-5-3 loss: 0.673924  [  128/  146]
train() client id: f_00006-0-0 loss: 0.459747  [   32/   54]
train() client id: f_00006-1-0 loss: 0.516574  [   32/   54]
train() client id: f_00006-2-0 loss: 0.464025  [   32/   54]
train() client id: f_00006-3-0 loss: 0.396336  [   32/   54]
train() client id: f_00006-4-0 loss: 0.444985  [   32/   54]
train() client id: f_00006-5-0 loss: 0.486132  [   32/   54]
train() client id: f_00007-0-0 loss: 0.918484  [   32/  179]
train() client id: f_00007-0-1 loss: 0.729899  [   64/  179]
train() client id: f_00007-0-2 loss: 0.739041  [   96/  179]
train() client id: f_00007-0-3 loss: 0.774894  [  128/  179]
train() client id: f_00007-0-4 loss: 0.769158  [  160/  179]
train() client id: f_00007-1-0 loss: 0.816350  [   32/  179]
train() client id: f_00007-1-1 loss: 0.617278  [   64/  179]
train() client id: f_00007-1-2 loss: 0.962602  [   96/  179]
train() client id: f_00007-1-3 loss: 0.817615  [  128/  179]
train() client id: f_00007-1-4 loss: 0.795595  [  160/  179]
train() client id: f_00007-2-0 loss: 0.696614  [   32/  179]
train() client id: f_00007-2-1 loss: 0.632838  [   64/  179]
train() client id: f_00007-2-2 loss: 0.771070  [   96/  179]
train() client id: f_00007-2-3 loss: 0.886266  [  128/  179]
train() client id: f_00007-2-4 loss: 1.034659  [  160/  179]
train() client id: f_00007-3-0 loss: 0.864462  [   32/  179]
train() client id: f_00007-3-1 loss: 0.627531  [   64/  179]
train() client id: f_00007-3-2 loss: 0.761040  [   96/  179]
train() client id: f_00007-3-3 loss: 0.740385  [  128/  179]
train() client id: f_00007-3-4 loss: 1.019808  [  160/  179]
train() client id: f_00007-4-0 loss: 0.878355  [   32/  179]
train() client id: f_00007-4-1 loss: 0.669156  [   64/  179]
train() client id: f_00007-4-2 loss: 0.953009  [   96/  179]
train() client id: f_00007-4-3 loss: 0.856068  [  128/  179]
train() client id: f_00007-4-4 loss: 0.667908  [  160/  179]
train() client id: f_00007-5-0 loss: 0.861850  [   32/  179]
train() client id: f_00007-5-1 loss: 0.641666  [   64/  179]
train() client id: f_00007-5-2 loss: 0.796190  [   96/  179]
train() client id: f_00007-5-3 loss: 0.704370  [  128/  179]
train() client id: f_00007-5-4 loss: 0.984541  [  160/  179]
train() client id: f_00008-0-0 loss: 0.774783  [   32/  130]
train() client id: f_00008-0-1 loss: 0.701123  [   64/  130]
train() client id: f_00008-0-2 loss: 0.735152  [   96/  130]
train() client id: f_00008-0-3 loss: 0.749127  [  128/  130]
train() client id: f_00008-1-0 loss: 0.661779  [   32/  130]
train() client id: f_00008-1-1 loss: 0.918496  [   64/  130]
train() client id: f_00008-1-2 loss: 0.735003  [   96/  130]
train() client id: f_00008-1-3 loss: 0.645772  [  128/  130]
train() client id: f_00008-2-0 loss: 0.782498  [   32/  130]
train() client id: f_00008-2-1 loss: 0.641544  [   64/  130]
train() client id: f_00008-2-2 loss: 0.728573  [   96/  130]
train() client id: f_00008-2-3 loss: 0.841378  [  128/  130]
train() client id: f_00008-3-0 loss: 0.733825  [   32/  130]
train() client id: f_00008-3-1 loss: 0.799633  [   64/  130]
train() client id: f_00008-3-2 loss: 0.767387  [   96/  130]
train() client id: f_00008-3-3 loss: 0.663304  [  128/  130]
train() client id: f_00008-4-0 loss: 0.725274  [   32/  130]
train() client id: f_00008-4-1 loss: 0.672583  [   64/  130]
train() client id: f_00008-4-2 loss: 0.775670  [   96/  130]
train() client id: f_00008-4-3 loss: 0.777012  [  128/  130]
train() client id: f_00008-5-0 loss: 0.750408  [   32/  130]
train() client id: f_00008-5-1 loss: 0.729727  [   64/  130]
train() client id: f_00008-5-2 loss: 0.732834  [   96/  130]
train() client id: f_00008-5-3 loss: 0.732865  [  128/  130]
train() client id: f_00009-0-0 loss: 0.818406  [   32/  118]
train() client id: f_00009-0-1 loss: 0.905375  [   64/  118]
train() client id: f_00009-0-2 loss: 0.933973  [   96/  118]
train() client id: f_00009-1-0 loss: 0.732853  [   32/  118]
train() client id: f_00009-1-1 loss: 0.835437  [   64/  118]
train() client id: f_00009-1-2 loss: 0.866399  [   96/  118]
train() client id: f_00009-2-0 loss: 0.949975  [   32/  118]
train() client id: f_00009-2-1 loss: 0.662592  [   64/  118]
train() client id: f_00009-2-2 loss: 0.800949  [   96/  118]
train() client id: f_00009-3-0 loss: 0.716226  [   32/  118]
train() client id: f_00009-3-1 loss: 0.885466  [   64/  118]
train() client id: f_00009-3-2 loss: 0.778484  [   96/  118]
train() client id: f_00009-4-0 loss: 0.800577  [   32/  118]
train() client id: f_00009-4-1 loss: 0.748529  [   64/  118]
train() client id: f_00009-4-2 loss: 0.833251  [   96/  118]
train() client id: f_00009-5-0 loss: 0.731618  [   32/  118]
train() client id: f_00009-5-1 loss: 0.907485  [   64/  118]
train() client id: f_00009-5-2 loss: 0.766144  [   96/  118]
At round 72 accuracy: 0.6472148541114059
At round 72 training accuracy: 0.5888665325285044
At round 72 training loss: 0.8189534933525938
update_location
xs = [  -3.9056584     4.20031788  380.00902392   18.81129433    0.97929623
    3.95640986 -342.44319194 -321.32485185  364.66397685 -307.06087855]
ys = [ 372.5879595   355.55583871    1.32061395 -342.45517586  334.35018685
  317.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [385.79404056 369.37460269 392.94860005 357.25258889 348.98568232
 333.19887093 356.75514046 336.52687362 378.53465625 322.95881315]
dists_bs = [261.36999725 254.38782364 581.5492634  552.56011151 237.40512789
 228.95693543 244.1062424  227.48366256 562.15809678 216.05686492]
uav_gains = [8.21244196e-13 9.67100320e-13 7.69464620e-13 1.10763527e-12
 1.22558846e-12 1.52083801e-12 1.11416668e-12 1.44934733e-12
 8.80519312e-13 1.78047100e-12]
bs_gains = [1.88355381e-11 2.03190964e-11 2.00655865e-12 2.31542863e-12
 2.46559544e-11 2.72887370e-11 2.28072648e-11 2.77864773e-11
 2.20643109e-12 3.20998859e-11]
Round 73
-------------------------------
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.3194998  2.59288881 1.30418738 0.50035792 2.98837254 1.43930991
 0.6065387  1.80528297 1.31771582 1.16737463]
obj_prev = 15.041528473465261
eta_min = 6.730086322824428e-72	eta_max = 0.960768447983313
af = 3.101969802094253	bf = 0.606014338290774	zeta = 3.4121667823036788	eta = 0.9090909090909091
af = 3.101969802094253	bf = 0.606014338290774	zeta = 9.918300016635595	eta = 0.3127521648761819
af = 3.101969802094253	bf = 0.606014338290774	zeta = 6.128169132936568	eta = 0.5061821458912337
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.504089108696306	eta = 0.5635755055624424
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464521898505789	eta = 0.5676562121459247
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464335618649047	eta = 0.5676755636142928
eta = 0.5676755636142928
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.0467219  0.09826431 0.04598028 0.01594477 0.11346746 0.05413807
 0.02002367 0.06637476 0.04820513 0.04375543]
ene_total = [0.57257327 0.78687954 0.57638183 0.31001941 0.89561579 0.45820012
 0.33936518 0.65485131 0.49074166 0.3797075 ]
ti_comp = [3.27393193 3.49770683 3.26133269 3.32347001 3.50175933 3.50374398
 3.32432233 3.35848843 3.40012884 3.50673809]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [5.94705746e-07 4.84730784e-06 5.71221622e-07 2.29377929e-08
 7.44597339e-06 8.07836791e-07 4.54051187e-08 1.62032140e-06
 6.05576618e-07 4.25764209e-07]
ene_total = [0.22926124 0.06486083 0.2385191  0.19285604 0.06190211 0.060395
 0.19222992 0.16713595 0.13653078 0.05819209]
optimize_network iter = 0 obj = 1.401883055865988
eta = 0.5676755636142928
freqs = [ 7135441.14428652 14046961.66554712  7049308.38877608  2398814.24536511
 16201493.85610097  7725745.89318307  3011692.04068683  9881641.69424418
  7088721.37557428  6238764.68717198]
eta_min = 0.5676755636142934	eta_max = 0.8387833537635655
af = 9.530769728321942e-05	bf = 0.606014338290774	zeta = 0.00010483846701154136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.00107717e-07 8.15954657e-07 9.61545994e-08 3.86115336e-09
 1.25339196e-06 1.35984388e-07 7.64311227e-09 2.72751152e-07
 1.01937628e-07 7.16695331e-08]
ene_total = [1.09025033 0.30830925 1.13427738 0.91713951 0.29416335 0.28718908
 0.91416128 0.79477943 0.64926391 0.27672413]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 1 obj = 3.7590876976936305
eta = 0.8387833537635655
freqs = [ 7074978.59491438 12213971.70314103  7049308.38877607  2303188.06336389
 14058067.6277585   6696837.19701407  2890083.78088009  9285155.45638792
  6499537.15984087  5399630.45377395]
eta_min = 0.8387833537635758	eta_max = 0.8387833537635625
af = 7.4931901688053e-05	bf = 0.606014338290774	zeta = 8.242509185685831e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [9.84183688e-08 6.16900397e-07 9.61545994e-08 3.55944769e-09
 9.43687147e-07 1.02175717e-07 7.03833579e-09 2.40816781e-07
 8.56965960e-08 5.36865431e-08]
ene_total = [1.09025027 0.30830229 1.13427738 0.9171395  0.29415253 0.2871879
 0.91416126 0.79477832 0.64926334 0.2767235 ]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 2 obj = 3.759087697693561
eta = 0.8387833537635625
freqs = [ 7074978.59491436 12213971.70314105  7049308.38877605  2303188.06336388
 14058067.62775852  6696837.19701408  2890083.78088009  9285155.45638791
  6499537.15984087  5399630.45377396]
Done!
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.57674265e-07 9.88324819e-07 1.54047521e-07 5.70252590e-09
 1.51186388e-06 1.63693844e-07 1.12759888e-08 3.85808151e-07
 1.37292946e-07 8.60102267e-08]
ene_total = [0.0311997  0.00882304 0.03245962 0.02624574 0.00841832 0.00821851
 0.02616052 0.02274428 0.01857999 0.00791902]
At round 73 energy consumption: 0.19076875112517003
At round 73 eta: 0.8387833537635625
At round 73 a_n: 3.176756027494214
At round 73 local rounds: 5.756677690695127
At round 73 global rounds: 19.70488843214887
gradient difference: 0.6471970677375793
train() client id: f_00000-0-0 loss: 1.047846  [   32/  126]
train() client id: f_00000-0-1 loss: 1.123499  [   64/  126]
train() client id: f_00000-0-2 loss: 1.038557  [   96/  126]
train() client id: f_00000-1-0 loss: 0.933027  [   32/  126]
train() client id: f_00000-1-1 loss: 0.927717  [   64/  126]
train() client id: f_00000-1-2 loss: 0.901294  [   96/  126]
train() client id: f_00000-2-0 loss: 0.868452  [   32/  126]
train() client id: f_00000-2-1 loss: 0.881738  [   64/  126]
train() client id: f_00000-2-2 loss: 0.971580  [   96/  126]
train() client id: f_00000-3-0 loss: 1.056893  [   32/  126]
train() client id: f_00000-3-1 loss: 0.878337  [   64/  126]
train() client id: f_00000-3-2 loss: 0.785432  [   96/  126]
train() client id: f_00000-4-0 loss: 0.786023  [   32/  126]
train() client id: f_00000-4-1 loss: 0.951295  [   64/  126]
train() client id: f_00000-4-2 loss: 0.974192  [   96/  126]
train() client id: f_00001-0-0 loss: 0.556207  [   32/  265]
train() client id: f_00001-0-1 loss: 0.408896  [   64/  265]
train() client id: f_00001-0-2 loss: 0.484452  [   96/  265]
train() client id: f_00001-0-3 loss: 0.569029  [  128/  265]
train() client id: f_00001-0-4 loss: 0.444104  [  160/  265]
train() client id: f_00001-0-5 loss: 0.429762  [  192/  265]
train() client id: f_00001-0-6 loss: 0.455564  [  224/  265]
train() client id: f_00001-0-7 loss: 0.492061  [  256/  265]
train() client id: f_00001-1-0 loss: 0.558617  [   32/  265]
train() client id: f_00001-1-1 loss: 0.482633  [   64/  265]
train() client id: f_00001-1-2 loss: 0.408696  [   96/  265]
train() client id: f_00001-1-3 loss: 0.547915  [  128/  265]
train() client id: f_00001-1-4 loss: 0.494025  [  160/  265]
train() client id: f_00001-1-5 loss: 0.460578  [  192/  265]
train() client id: f_00001-1-6 loss: 0.390889  [  224/  265]
train() client id: f_00001-1-7 loss: 0.470648  [  256/  265]
train() client id: f_00001-2-0 loss: 0.562412  [   32/  265]
train() client id: f_00001-2-1 loss: 0.410260  [   64/  265]
train() client id: f_00001-2-2 loss: 0.455333  [   96/  265]
train() client id: f_00001-2-3 loss: 0.459572  [  128/  265]
train() client id: f_00001-2-4 loss: 0.428112  [  160/  265]
train() client id: f_00001-2-5 loss: 0.504694  [  192/  265]
train() client id: f_00001-2-6 loss: 0.507134  [  224/  265]
train() client id: f_00001-2-7 loss: 0.383356  [  256/  265]
train() client id: f_00001-3-0 loss: 0.641003  [   32/  265]
train() client id: f_00001-3-1 loss: 0.417288  [   64/  265]
train() client id: f_00001-3-2 loss: 0.389950  [   96/  265]
train() client id: f_00001-3-3 loss: 0.528455  [  128/  265]
train() client id: f_00001-3-4 loss: 0.451884  [  160/  265]
train() client id: f_00001-3-5 loss: 0.434603  [  192/  265]
train() client id: f_00001-3-6 loss: 0.552305  [  224/  265]
train() client id: f_00001-3-7 loss: 0.371679  [  256/  265]
train() client id: f_00001-4-0 loss: 0.550647  [   32/  265]
train() client id: f_00001-4-1 loss: 0.378058  [   64/  265]
train() client id: f_00001-4-2 loss: 0.472310  [   96/  265]
train() client id: f_00001-4-3 loss: 0.527951  [  128/  265]
train() client id: f_00001-4-4 loss: 0.411953  [  160/  265]
train() client id: f_00001-4-5 loss: 0.523697  [  192/  265]
train() client id: f_00001-4-6 loss: 0.403463  [  224/  265]
train() client id: f_00001-4-7 loss: 0.515722  [  256/  265]
train() client id: f_00002-0-0 loss: 0.982784  [   32/  124]
train() client id: f_00002-0-1 loss: 0.776487  [   64/  124]
train() client id: f_00002-0-2 loss: 0.624603  [   96/  124]
train() client id: f_00002-1-0 loss: 0.738818  [   32/  124]
train() client id: f_00002-1-1 loss: 0.591504  [   64/  124]
train() client id: f_00002-1-2 loss: 0.703975  [   96/  124]
train() client id: f_00002-2-0 loss: 0.621106  [   32/  124]
train() client id: f_00002-2-1 loss: 0.655097  [   64/  124]
train() client id: f_00002-2-2 loss: 0.884430  [   96/  124]
train() client id: f_00002-3-0 loss: 0.832926  [   32/  124]
train() client id: f_00002-3-1 loss: 0.739798  [   64/  124]
train() client id: f_00002-3-2 loss: 0.472823  [   96/  124]
train() client id: f_00002-4-0 loss: 0.820404  [   32/  124]
train() client id: f_00002-4-1 loss: 0.690528  [   64/  124]
train() client id: f_00002-4-2 loss: 0.650087  [   96/  124]
train() client id: f_00003-0-0 loss: 0.872555  [   32/   43]
train() client id: f_00003-1-0 loss: 0.586755  [   32/   43]
train() client id: f_00003-2-0 loss: 0.574130  [   32/   43]
train() client id: f_00003-3-0 loss: 0.587908  [   32/   43]
train() client id: f_00003-4-0 loss: 0.566139  [   32/   43]
train() client id: f_00004-0-0 loss: 0.884264  [   32/  306]
train() client id: f_00004-0-1 loss: 0.969873  [   64/  306]
train() client id: f_00004-0-2 loss: 0.909157  [   96/  306]
train() client id: f_00004-0-3 loss: 0.951070  [  128/  306]
train() client id: f_00004-0-4 loss: 0.753154  [  160/  306]
train() client id: f_00004-0-5 loss: 0.868215  [  192/  306]
train() client id: f_00004-0-6 loss: 0.890082  [  224/  306]
train() client id: f_00004-0-7 loss: 0.945924  [  256/  306]
train() client id: f_00004-0-8 loss: 0.873928  [  288/  306]
train() client id: f_00004-1-0 loss: 0.895202  [   32/  306]
train() client id: f_00004-1-1 loss: 0.830601  [   64/  306]
train() client id: f_00004-1-2 loss: 0.906695  [   96/  306]
train() client id: f_00004-1-3 loss: 0.882611  [  128/  306]
train() client id: f_00004-1-4 loss: 0.957522  [  160/  306]
train() client id: f_00004-1-5 loss: 0.896765  [  192/  306]
train() client id: f_00004-1-6 loss: 0.840639  [  224/  306]
train() client id: f_00004-1-7 loss: 0.915306  [  256/  306]
train() client id: f_00004-1-8 loss: 0.927593  [  288/  306]
train() client id: f_00004-2-0 loss: 0.885430  [   32/  306]
train() client id: f_00004-2-1 loss: 0.803655  [   64/  306]
train() client id: f_00004-2-2 loss: 1.079600  [   96/  306]
train() client id: f_00004-2-3 loss: 0.777221  [  128/  306]
train() client id: f_00004-2-4 loss: 1.037936  [  160/  306]
train() client id: f_00004-2-5 loss: 1.035100  [  192/  306]
train() client id: f_00004-2-6 loss: 0.905933  [  224/  306]
train() client id: f_00004-2-7 loss: 0.816600  [  256/  306]
train() client id: f_00004-2-8 loss: 0.772544  [  288/  306]
train() client id: f_00004-3-0 loss: 0.928765  [   32/  306]
train() client id: f_00004-3-1 loss: 0.877756  [   64/  306]
train() client id: f_00004-3-2 loss: 0.902300  [   96/  306]
train() client id: f_00004-3-3 loss: 0.855802  [  128/  306]
train() client id: f_00004-3-4 loss: 0.926291  [  160/  306]
train() client id: f_00004-3-5 loss: 0.932080  [  192/  306]
train() client id: f_00004-3-6 loss: 0.944627  [  224/  306]
train() client id: f_00004-3-7 loss: 0.951913  [  256/  306]
train() client id: f_00004-3-8 loss: 0.773279  [  288/  306]
train() client id: f_00004-4-0 loss: 0.853783  [   32/  306]
train() client id: f_00004-4-1 loss: 0.863072  [   64/  306]
train() client id: f_00004-4-2 loss: 0.958952  [   96/  306]
train() client id: f_00004-4-3 loss: 0.865373  [  128/  306]
train() client id: f_00004-4-4 loss: 0.935814  [  160/  306]
train() client id: f_00004-4-5 loss: 0.888630  [  192/  306]
train() client id: f_00004-4-6 loss: 0.969620  [  224/  306]
train() client id: f_00004-4-7 loss: 0.891456  [  256/  306]
train() client id: f_00004-4-8 loss: 0.931369  [  288/  306]
train() client id: f_00005-0-0 loss: 0.667652  [   32/  146]
train() client id: f_00005-0-1 loss: 0.519890  [   64/  146]
train() client id: f_00005-0-2 loss: 1.052625  [   96/  146]
train() client id: f_00005-0-3 loss: 0.517859  [  128/  146]
train() client id: f_00005-1-0 loss: 0.573492  [   32/  146]
train() client id: f_00005-1-1 loss: 0.673775  [   64/  146]
train() client id: f_00005-1-2 loss: 0.917356  [   96/  146]
train() client id: f_00005-1-3 loss: 0.539720  [  128/  146]
train() client id: f_00005-2-0 loss: 1.052113  [   32/  146]
train() client id: f_00005-2-1 loss: 0.675535  [   64/  146]
train() client id: f_00005-2-2 loss: 0.661898  [   96/  146]
train() client id: f_00005-2-3 loss: 0.602955  [  128/  146]
train() client id: f_00005-3-0 loss: 0.636521  [   32/  146]
train() client id: f_00005-3-1 loss: 0.936386  [   64/  146]
train() client id: f_00005-3-2 loss: 0.473676  [   96/  146]
train() client id: f_00005-3-3 loss: 1.004100  [  128/  146]
train() client id: f_00005-4-0 loss: 0.761919  [   32/  146]
train() client id: f_00005-4-1 loss: 0.871821  [   64/  146]
train() client id: f_00005-4-2 loss: 0.757941  [   96/  146]
train() client id: f_00005-4-3 loss: 0.550120  [  128/  146]
train() client id: f_00006-0-0 loss: 0.491819  [   32/   54]
train() client id: f_00006-1-0 loss: 0.478154  [   32/   54]
train() client id: f_00006-2-0 loss: 0.549360  [   32/   54]
train() client id: f_00006-3-0 loss: 0.537980  [   32/   54]
train() client id: f_00006-4-0 loss: 0.518127  [   32/   54]
train() client id: f_00007-0-0 loss: 0.395486  [   32/  179]
train() client id: f_00007-0-1 loss: 0.579727  [   64/  179]
train() client id: f_00007-0-2 loss: 0.374098  [   96/  179]
train() client id: f_00007-0-3 loss: 0.528750  [  128/  179]
train() client id: f_00007-0-4 loss: 0.406353  [  160/  179]
train() client id: f_00007-1-0 loss: 0.250849  [   32/  179]
train() client id: f_00007-1-1 loss: 0.378566  [   64/  179]
train() client id: f_00007-1-2 loss: 0.635866  [   96/  179]
train() client id: f_00007-1-3 loss: 0.360016  [  128/  179]
train() client id: f_00007-1-4 loss: 0.559561  [  160/  179]
train() client id: f_00007-2-0 loss: 0.371659  [   32/  179]
train() client id: f_00007-2-1 loss: 0.371708  [   64/  179]
train() client id: f_00007-2-2 loss: 0.394375  [   96/  179]
train() client id: f_00007-2-3 loss: 0.327770  [  128/  179]
train() client id: f_00007-2-4 loss: 0.691943  [  160/  179]
train() client id: f_00007-3-0 loss: 0.462202  [   32/  179]
train() client id: f_00007-3-1 loss: 0.472464  [   64/  179]
train() client id: f_00007-3-2 loss: 0.349250  [   96/  179]
train() client id: f_00007-3-3 loss: 0.203812  [  128/  179]
train() client id: f_00007-3-4 loss: 0.446043  [  160/  179]
train() client id: f_00007-4-0 loss: 0.350197  [   32/  179]
train() client id: f_00007-4-1 loss: 0.409762  [   64/  179]
train() client id: f_00007-4-2 loss: 0.295457  [   96/  179]
train() client id: f_00007-4-3 loss: 0.489367  [  128/  179]
train() client id: f_00007-4-4 loss: 0.526771  [  160/  179]
train() client id: f_00008-0-0 loss: 0.825198  [   32/  130]
train() client id: f_00008-0-1 loss: 0.642782  [   64/  130]
train() client id: f_00008-0-2 loss: 0.694016  [   96/  130]
train() client id: f_00008-0-3 loss: 0.698487  [  128/  130]
train() client id: f_00008-1-0 loss: 0.811982  [   32/  130]
train() client id: f_00008-1-1 loss: 0.722828  [   64/  130]
train() client id: f_00008-1-2 loss: 0.643716  [   96/  130]
train() client id: f_00008-1-3 loss: 0.722603  [  128/  130]
train() client id: f_00008-2-0 loss: 0.722261  [   32/  130]
train() client id: f_00008-2-1 loss: 0.744104  [   64/  130]
train() client id: f_00008-2-2 loss: 0.569479  [   96/  130]
train() client id: f_00008-2-3 loss: 0.847129  [  128/  130]
train() client id: f_00008-3-0 loss: 0.751288  [   32/  130]
train() client id: f_00008-3-1 loss: 0.826352  [   64/  130]
train() client id: f_00008-3-2 loss: 0.702705  [   96/  130]
train() client id: f_00008-3-3 loss: 0.634135  [  128/  130]
train() client id: f_00008-4-0 loss: 0.861735  [   32/  130]
train() client id: f_00008-4-1 loss: 0.682546  [   64/  130]
train() client id: f_00008-4-2 loss: 0.666143  [   96/  130]
train() client id: f_00008-4-3 loss: 0.660232  [  128/  130]
train() client id: f_00009-0-0 loss: 0.903058  [   32/  118]
train() client id: f_00009-0-1 loss: 0.981336  [   64/  118]
train() client id: f_00009-0-2 loss: 0.812970  [   96/  118]
train() client id: f_00009-1-0 loss: 0.898215  [   32/  118]
train() client id: f_00009-1-1 loss: 0.886628  [   64/  118]
train() client id: f_00009-1-2 loss: 1.023543  [   96/  118]
train() client id: f_00009-2-0 loss: 0.954714  [   32/  118]
train() client id: f_00009-2-1 loss: 0.806610  [   64/  118]
train() client id: f_00009-2-2 loss: 0.832695  [   96/  118]
train() client id: f_00009-3-0 loss: 0.938952  [   32/  118]
train() client id: f_00009-3-1 loss: 0.865714  [   64/  118]
train() client id: f_00009-3-2 loss: 0.822448  [   96/  118]
train() client id: f_00009-4-0 loss: 0.819393  [   32/  118]
train() client id: f_00009-4-1 loss: 0.913243  [   64/  118]
train() client id: f_00009-4-2 loss: 0.957239  [   96/  118]
At round 73 accuracy: 0.6445623342175066
At round 73 training accuracy: 0.5902079141515761
At round 73 training loss: 0.8239360788956589
update_location
xs = [  -3.9056584     4.20031788  385.00902392   18.81129433    0.97929623
    3.95640986 -347.44319194 -326.32485185  369.66397685 -312.06087855]
ys = [ 377.5879595   360.55583871    1.32061395 -347.45517586  339.35018685
  322.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [390.6250393  374.18999919 397.78598834 362.04828963 353.77889752
 337.97134361 361.5572737  341.30424139 383.35378667 327.71634652]
dists_bs = [265.17004933 257.96070089 586.32297926 557.24521406 240.78973528
 232.10863791 247.56269812 230.73158702 566.95975654 219.14875693]
uav_gains = [7.85626588e-13 9.19780429e-13 7.37654771e-13 1.04796295e-12
 1.15469134e-12 1.42001853e-12 1.05381008e-12 1.35598266e-12
 8.40352469e-13 1.65176343e-12]
bs_gains = [1.80894587e-11 1.95408815e-11 1.96114959e-12 2.26133192e-12
 2.36977859e-11 2.62638529e-11 2.19268126e-11 2.67051078e-11
 2.15450662e-12 3.08478466e-11]
Round 74
-------------------------------
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.17965282 2.31354803 1.16601007 0.44876366 2.6663691  1.28432213
 0.54349708 1.6129016  1.17620535 1.04170236]
obj_prev = 13.43297219536657
eta_min = 1.985153234470665e-80	eta_max = 0.9640166902686983
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 3.044236871939507	eta = 0.9090909090909091
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 8.994246026442418	eta = 0.3076953929504865
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 5.5116993178884695	eta = 0.5021115822515108
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.941875446838901	eta = 0.5600076519876258
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905748712654075	eta = 0.5641316397354368
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905578223300182	eta = 0.5641512456685993
eta = 0.5641512456685993
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.04723579 0.09934512 0.04648602 0.01612015 0.11471549 0.05473354
 0.02024391 0.06710481 0.04873534 0.04423669]
ene_total = [0.51557111 0.70394459 0.51893062 0.28085198 0.80121777 0.40983951
 0.30711799 0.58944182 0.4390443  0.33961853]
ti_comp = [3.7239081  3.95531799 3.71124405 3.77370601 3.95943373 3.96148105
 3.77455137 3.80899893 3.85658756 3.96449942]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [4.75002260e-07 3.91702877e-06 4.55835322e-07 1.83844731e-08
 6.01838646e-06 6.53018300e-07 3.63942310e-08 1.30172529e-06
 4.86413493e-07 3.44232011e-07]
ene_total = [0.20841223 0.05795483 0.2166472  0.17602698 0.05529214 0.05392593
 0.17547739 0.15308527 0.12213433 0.05196115]
optimize_network iter = 0 obj = 1.2709174591621142
eta = 0.5641512456685993
freqs = [ 6342233.67091708 12558423.2934157   6262861.66135149  2135851.43012001
 14486350.74522172  6908216.2161498   2681631.30754287  8808720.47508642
  6318453.65727492  5579101.94228267]
eta_min = 0.5641512456686001	eta_max = 0.8530565293483942
af = 6.786276083478417e-05	bf = 0.5538889445570291	zeta = 7.464903691826259e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.90879612e-08 6.52185990e-07 7.58966627e-08 3.06101806e-09
 1.00206242e-06 1.08727664e-07 6.05964597e-09 2.16737493e-07
 8.09879336e-08 5.73146914e-08]
ene_total = [0.99918599 0.27775315 1.03866771 0.84393208 0.2649327  0.25852208
 0.84129665 0.73390845 0.58554042 0.2491103 ]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 1 obj = 3.769458367807797
eta = 0.8530565293483942
freqs = [ 6286609.35895016 10821081.10112869  6262861.66135149  2047669.00897199
 12455063.70509777  5933111.71394756  2569503.70041527  8257350.63294458
  5754226.36383451  4783979.0224429 ]
eta_min = 0.8530565293483974	eta_max = 0.8530565293483936
af = 5.2542085376783516e-05	bf = 0.5538889445570291	zeta = 5.7796293914461874e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.77067690e-08 4.84219790e-07 7.58966627e-08 2.81347673e-09
 7.40744934e-07 8.01997961e-08 5.56349384e-09 1.90453887e-07
 6.71695741e-08 4.21420732e-08]
ene_total = [0.99918595 0.27774791 1.03866771 0.84393207 0.26492456 0.25852119
 0.84129664 0.73390763 0.58553999 0.24910983]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 2 obj = 3.7694583678077827
eta = 0.8530565293483936
freqs = [ 6286609.35895015 10821081.10112868  6262861.66135148  2047669.00897199
 12455063.70509776  5933111.71394756  2569503.70041527  8257350.63294457
  5754226.3638345   4783979.0224429 ]
Done!
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.24492590e-07 7.75759650e-07 1.21592652e-07 4.50741950e-09
 1.18673388e-06 1.28486623e-07 8.91317151e-09 3.05122682e-07
 1.07611143e-07 6.75150431e-08]
ene_total = [0.03204957 0.00890923 0.03331597 0.02706966 0.00849807 0.00829228
 0.02698513 0.02354067 0.01878161 0.00799038]
At round 74 energy consumption: 0.1954325657499382
At round 74 eta: 0.8530565293483936
At round 74 a_n: 2.834210180524895
At round 74 local rounds: 5.2041580597357475
At round 74 global rounds: 19.287758537054202
gradient difference: 0.9280350804328918
train() client id: f_00000-0-0 loss: 0.869940  [   32/  126]
train() client id: f_00000-0-1 loss: 0.777654  [   64/  126]
train() client id: f_00000-0-2 loss: 0.665368  [   96/  126]
train() client id: f_00000-1-0 loss: 0.734995  [   32/  126]
train() client id: f_00000-1-1 loss: 1.003204  [   64/  126]
train() client id: f_00000-1-2 loss: 0.729137  [   96/  126]
train() client id: f_00000-2-0 loss: 0.880958  [   32/  126]
train() client id: f_00000-2-1 loss: 0.652005  [   64/  126]
train() client id: f_00000-2-2 loss: 0.859966  [   96/  126]
train() client id: f_00000-3-0 loss: 0.616704  [   32/  126]
train() client id: f_00000-3-1 loss: 0.749509  [   64/  126]
train() client id: f_00000-3-2 loss: 0.923513  [   96/  126]
train() client id: f_00000-4-0 loss: 0.892598  [   32/  126]
train() client id: f_00000-4-1 loss: 0.707575  [   64/  126]
train() client id: f_00000-4-2 loss: 0.789159  [   96/  126]
train() client id: f_00001-0-0 loss: 0.471308  [   32/  265]
train() client id: f_00001-0-1 loss: 0.476134  [   64/  265]
train() client id: f_00001-0-2 loss: 0.407634  [   96/  265]
train() client id: f_00001-0-3 loss: 0.456674  [  128/  265]
train() client id: f_00001-0-4 loss: 0.417887  [  160/  265]
train() client id: f_00001-0-5 loss: 0.544393  [  192/  265]
train() client id: f_00001-0-6 loss: 0.376594  [  224/  265]
train() client id: f_00001-0-7 loss: 0.617517  [  256/  265]
train() client id: f_00001-1-0 loss: 0.439920  [   32/  265]
train() client id: f_00001-1-1 loss: 0.422586  [   64/  265]
train() client id: f_00001-1-2 loss: 0.418999  [   96/  265]
train() client id: f_00001-1-3 loss: 0.488406  [  128/  265]
train() client id: f_00001-1-4 loss: 0.481156  [  160/  265]
train() client id: f_00001-1-5 loss: 0.572024  [  192/  265]
train() client id: f_00001-1-6 loss: 0.524006  [  224/  265]
train() client id: f_00001-1-7 loss: 0.401061  [  256/  265]
train() client id: f_00001-2-0 loss: 0.379786  [   32/  265]
train() client id: f_00001-2-1 loss: 0.388254  [   64/  265]
train() client id: f_00001-2-2 loss: 0.390669  [   96/  265]
train() client id: f_00001-2-3 loss: 0.552391  [  128/  265]
train() client id: f_00001-2-4 loss: 0.560830  [  160/  265]
train() client id: f_00001-2-5 loss: 0.492724  [  192/  265]
train() client id: f_00001-2-6 loss: 0.504118  [  224/  265]
train() client id: f_00001-2-7 loss: 0.476185  [  256/  265]
train() client id: f_00001-3-0 loss: 0.526975  [   32/  265]
train() client id: f_00001-3-1 loss: 0.442344  [   64/  265]
train() client id: f_00001-3-2 loss: 0.439390  [   96/  265]
train() client id: f_00001-3-3 loss: 0.414841  [  128/  265]
train() client id: f_00001-3-4 loss: 0.527473  [  160/  265]
train() client id: f_00001-3-5 loss: 0.375407  [  192/  265]
train() client id: f_00001-3-6 loss: 0.524145  [  224/  265]
train() client id: f_00001-3-7 loss: 0.497801  [  256/  265]
train() client id: f_00001-4-0 loss: 0.569153  [   32/  265]
train() client id: f_00001-4-1 loss: 0.548757  [   64/  265]
train() client id: f_00001-4-2 loss: 0.361154  [   96/  265]
train() client id: f_00001-4-3 loss: 0.471551  [  128/  265]
train() client id: f_00001-4-4 loss: 0.461035  [  160/  265]
train() client id: f_00001-4-5 loss: 0.409429  [  192/  265]
train() client id: f_00001-4-6 loss: 0.446374  [  224/  265]
train() client id: f_00001-4-7 loss: 0.409020  [  256/  265]
train() client id: f_00002-0-0 loss: 0.958565  [   32/  124]
train() client id: f_00002-0-1 loss: 1.154996  [   64/  124]
train() client id: f_00002-0-2 loss: 0.720014  [   96/  124]
train() client id: f_00002-1-0 loss: 0.978682  [   32/  124]
train() client id: f_00002-1-1 loss: 0.979199  [   64/  124]
train() client id: f_00002-1-2 loss: 0.973940  [   96/  124]
train() client id: f_00002-2-0 loss: 0.813984  [   32/  124]
train() client id: f_00002-2-1 loss: 0.984248  [   64/  124]
train() client id: f_00002-2-2 loss: 0.888666  [   96/  124]
train() client id: f_00002-3-0 loss: 1.038373  [   32/  124]
train() client id: f_00002-3-1 loss: 0.886050  [   64/  124]
train() client id: f_00002-3-2 loss: 1.025335  [   96/  124]
train() client id: f_00002-4-0 loss: 1.088771  [   32/  124]
train() client id: f_00002-4-1 loss: 0.994582  [   64/  124]
train() client id: f_00002-4-2 loss: 0.947940  [   96/  124]
train() client id: f_00003-0-0 loss: 0.640842  [   32/   43]
train() client id: f_00003-1-0 loss: 0.563216  [   32/   43]
train() client id: f_00003-2-0 loss: 0.717025  [   32/   43]
train() client id: f_00003-3-0 loss: 0.767255  [   32/   43]
train() client id: f_00003-4-0 loss: 0.550498  [   32/   43]
train() client id: f_00004-0-0 loss: 0.843118  [   32/  306]
train() client id: f_00004-0-1 loss: 0.784436  [   64/  306]
train() client id: f_00004-0-2 loss: 0.899505  [   96/  306]
train() client id: f_00004-0-3 loss: 0.637800  [  128/  306]
train() client id: f_00004-0-4 loss: 0.834213  [  160/  306]
train() client id: f_00004-0-5 loss: 0.709739  [  192/  306]
train() client id: f_00004-0-6 loss: 0.935096  [  224/  306]
train() client id: f_00004-0-7 loss: 0.836939  [  256/  306]
train() client id: f_00004-0-8 loss: 0.594049  [  288/  306]
train() client id: f_00004-1-0 loss: 0.744594  [   32/  306]
train() client id: f_00004-1-1 loss: 0.812120  [   64/  306]
train() client id: f_00004-1-2 loss: 0.852726  [   96/  306]
train() client id: f_00004-1-3 loss: 0.795267  [  128/  306]
train() client id: f_00004-1-4 loss: 0.869324  [  160/  306]
train() client id: f_00004-1-5 loss: 0.689825  [  192/  306]
train() client id: f_00004-1-6 loss: 0.896731  [  224/  306]
train() client id: f_00004-1-7 loss: 0.676403  [  256/  306]
train() client id: f_00004-1-8 loss: 0.749754  [  288/  306]
train() client id: f_00004-2-0 loss: 0.866545  [   32/  306]
train() client id: f_00004-2-1 loss: 0.802300  [   64/  306]
train() client id: f_00004-2-2 loss: 0.682495  [   96/  306]
train() client id: f_00004-2-3 loss: 0.754172  [  128/  306]
train() client id: f_00004-2-4 loss: 0.884560  [  160/  306]
train() client id: f_00004-2-5 loss: 0.688554  [  192/  306]
train() client id: f_00004-2-6 loss: 0.958840  [  224/  306]
train() client id: f_00004-2-7 loss: 0.686181  [  256/  306]
train() client id: f_00004-2-8 loss: 0.691747  [  288/  306]
train() client id: f_00004-3-0 loss: 0.994650  [   32/  306]
train() client id: f_00004-3-1 loss: 0.800496  [   64/  306]
train() client id: f_00004-3-2 loss: 0.675893  [   96/  306]
train() client id: f_00004-3-3 loss: 0.861327  [  128/  306]
train() client id: f_00004-3-4 loss: 0.855868  [  160/  306]
train() client id: f_00004-3-5 loss: 0.672200  [  192/  306]
train() client id: f_00004-3-6 loss: 0.704826  [  224/  306]
train() client id: f_00004-3-7 loss: 0.772756  [  256/  306]
train() client id: f_00004-3-8 loss: 0.853102  [  288/  306]
train() client id: f_00004-4-0 loss: 0.744953  [   32/  306]
train() client id: f_00004-4-1 loss: 0.746398  [   64/  306]
train() client id: f_00004-4-2 loss: 0.750844  [   96/  306]
train() client id: f_00004-4-3 loss: 0.738478  [  128/  306]
train() client id: f_00004-4-4 loss: 0.719311  [  160/  306]
train() client id: f_00004-4-5 loss: 0.888673  [  192/  306]
train() client id: f_00004-4-6 loss: 0.846401  [  224/  306]
train() client id: f_00004-4-7 loss: 0.837734  [  256/  306]
train() client id: f_00004-4-8 loss: 0.914827  [  288/  306]
train() client id: f_00005-0-0 loss: 0.914584  [   32/  146]
train() client id: f_00005-0-1 loss: 0.688062  [   64/  146]
train() client id: f_00005-0-2 loss: 0.647869  [   96/  146]
train() client id: f_00005-0-3 loss: 1.072263  [  128/  146]
train() client id: f_00005-1-0 loss: 0.915320  [   32/  146]
train() client id: f_00005-1-1 loss: 0.685987  [   64/  146]
train() client id: f_00005-1-2 loss: 0.755492  [   96/  146]
train() client id: f_00005-1-3 loss: 0.884692  [  128/  146]
train() client id: f_00005-2-0 loss: 0.719988  [   32/  146]
train() client id: f_00005-2-1 loss: 0.599809  [   64/  146]
train() client id: f_00005-2-2 loss: 1.021268  [   96/  146]
train() client id: f_00005-2-3 loss: 0.800177  [  128/  146]
train() client id: f_00005-3-0 loss: 0.927368  [   32/  146]
train() client id: f_00005-3-1 loss: 0.728970  [   64/  146]
train() client id: f_00005-3-2 loss: 0.869400  [   96/  146]
train() client id: f_00005-3-3 loss: 0.558244  [  128/  146]
train() client id: f_00005-4-0 loss: 1.103886  [   32/  146]
train() client id: f_00005-4-1 loss: 0.780944  [   64/  146]
train() client id: f_00005-4-2 loss: 0.712753  [   96/  146]
train() client id: f_00005-4-3 loss: 0.669025  [  128/  146]
train() client id: f_00006-0-0 loss: 0.486785  [   32/   54]
train() client id: f_00006-1-0 loss: 0.531435  [   32/   54]
train() client id: f_00006-2-0 loss: 0.440045  [   32/   54]
train() client id: f_00006-3-0 loss: 0.546715  [   32/   54]
train() client id: f_00006-4-0 loss: 0.441361  [   32/   54]
train() client id: f_00007-0-0 loss: 0.392964  [   32/  179]
train() client id: f_00007-0-1 loss: 0.581000  [   64/  179]
train() client id: f_00007-0-2 loss: 0.506595  [   96/  179]
train() client id: f_00007-0-3 loss: 0.587434  [  128/  179]
train() client id: f_00007-0-4 loss: 0.556916  [  160/  179]
train() client id: f_00007-1-0 loss: 0.397481  [   32/  179]
train() client id: f_00007-1-1 loss: 0.415850  [   64/  179]
train() client id: f_00007-1-2 loss: 0.621013  [   96/  179]
train() client id: f_00007-1-3 loss: 0.503935  [  128/  179]
train() client id: f_00007-1-4 loss: 0.505689  [  160/  179]
train() client id: f_00007-2-0 loss: 0.445451  [   32/  179]
train() client id: f_00007-2-1 loss: 0.666429  [   64/  179]
train() client id: f_00007-2-2 loss: 0.490024  [   96/  179]
train() client id: f_00007-2-3 loss: 0.389379  [  128/  179]
train() client id: f_00007-2-4 loss: 0.390712  [  160/  179]
train() client id: f_00007-3-0 loss: 0.503635  [   32/  179]
train() client id: f_00007-3-1 loss: 0.719121  [   64/  179]
train() client id: f_00007-3-2 loss: 0.580059  [   96/  179]
train() client id: f_00007-3-3 loss: 0.321465  [  128/  179]
train() client id: f_00007-3-4 loss: 0.378186  [  160/  179]
train() client id: f_00007-4-0 loss: 0.758779  [   32/  179]
train() client id: f_00007-4-1 loss: 0.367356  [   64/  179]
train() client id: f_00007-4-2 loss: 0.440211  [   96/  179]
train() client id: f_00007-4-3 loss: 0.544864  [  128/  179]
train() client id: f_00007-4-4 loss: 0.329223  [  160/  179]
train() client id: f_00008-0-0 loss: 0.813031  [   32/  130]
train() client id: f_00008-0-1 loss: 0.751253  [   64/  130]
train() client id: f_00008-0-2 loss: 0.832777  [   96/  130]
train() client id: f_00008-0-3 loss: 0.774068  [  128/  130]
train() client id: f_00008-1-0 loss: 0.773872  [   32/  130]
train() client id: f_00008-1-1 loss: 0.803624  [   64/  130]
train() client id: f_00008-1-2 loss: 0.765080  [   96/  130]
train() client id: f_00008-1-3 loss: 0.848394  [  128/  130]
train() client id: f_00008-2-0 loss: 0.739798  [   32/  130]
train() client id: f_00008-2-1 loss: 0.934693  [   64/  130]
train() client id: f_00008-2-2 loss: 0.766881  [   96/  130]
train() client id: f_00008-2-3 loss: 0.744352  [  128/  130]
train() client id: f_00008-3-0 loss: 0.776730  [   32/  130]
train() client id: f_00008-3-1 loss: 0.861232  [   64/  130]
train() client id: f_00008-3-2 loss: 0.847323  [   96/  130]
train() client id: f_00008-3-3 loss: 0.673718  [  128/  130]
train() client id: f_00008-4-0 loss: 0.708171  [   32/  130]
train() client id: f_00008-4-1 loss: 0.825456  [   64/  130]
train() client id: f_00008-4-2 loss: 0.718912  [   96/  130]
train() client id: f_00008-4-3 loss: 0.908484  [  128/  130]
train() client id: f_00009-0-0 loss: 0.818339  [   32/  118]
train() client id: f_00009-0-1 loss: 0.659600  [   64/  118]
train() client id: f_00009-0-2 loss: 0.674085  [   96/  118]
train() client id: f_00009-1-0 loss: 0.742148  [   32/  118]
train() client id: f_00009-1-1 loss: 0.651039  [   64/  118]
train() client id: f_00009-1-2 loss: 0.575736  [   96/  118]
train() client id: f_00009-2-0 loss: 0.661032  [   32/  118]
train() client id: f_00009-2-1 loss: 0.696921  [   64/  118]
train() client id: f_00009-2-2 loss: 0.710039  [   96/  118]
train() client id: f_00009-3-0 loss: 0.581062  [   32/  118]
train() client id: f_00009-3-1 loss: 0.693385  [   64/  118]
train() client id: f_00009-3-2 loss: 0.729770  [   96/  118]
train() client id: f_00009-4-0 loss: 0.715751  [   32/  118]
train() client id: f_00009-4-1 loss: 0.837369  [   64/  118]
train() client id: f_00009-4-2 loss: 0.578118  [   96/  118]
At round 74 accuracy: 0.6445623342175066
At round 74 training accuracy: 0.5875251509054326
At round 74 training loss: 0.825416049081728
update_location
xs = [  -3.9056584     4.20031788  390.00902392   18.81129433    0.97929623
    3.95640986 -352.44319194 -331.32485185  374.66397685 -317.06087855]
ys = [ 382.5879595   365.55583871    1.32061395 -352.45517586  344.35018685
  327.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [395.46023937 379.01017649 402.62734974 366.84944567 358.57776033
 342.75030358 366.36470093 346.08789882 388.17749229 332.48099579]
dists_bs = [269.00935793 261.58035399 591.10043668 561.93574397 244.22980671
 235.32437464 251.07114812 234.04126509 571.76481643 222.31011324]
uav_gains = [7.52650140e-13 8.76404134e-13 7.08086688e-13 9.93683329e-13
 1.09055942e-12 1.32968634e-12 9.98935416e-13 1.27217390e-12
 8.03318596e-13 1.53701405e-12]
bs_gains = [1.73758236e-11 1.87931569e-11 1.91709000e-12 2.20887654e-12
 2.27749704e-11 2.52712502e-11 2.10796318e-11 2.56610996e-11
 2.10419163e-12 2.96352278e-11]
Round 75
-------------------------------
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.03922851 2.03415137 1.02725189 0.39661097 2.34431442 1.12928775
 0.47989662 1.41999007 1.0345605  0.91598534]
obj_prev = 11.821277437641127
eta_min = 2.632614097881737e-91	eta_max = 0.9674950324086027
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 2.6763069615753383	eta = 0.9090909090909091
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 8.03601399311375	eta = 0.3027628287842397
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.8844582536009025	eta = 0.4981118073659859
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.372075946270912	eta = 0.5564876636646818
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3396007499684774	eta = 0.5606521126909028
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3394472210647415	eta = 0.5606719484671788
eta = 0.5606719484671788
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.04774628 0.10041876 0.0469884  0.01629436 0.11595525 0.05532505
 0.02046269 0.06783003 0.04926203 0.04471477]
ene_total = [0.45740881 0.62055196 0.46033181 0.25063986 0.70629813 0.3612359
 0.2738053  0.5227749  0.38706651 0.29933403]
ti_comp = [4.2992849  4.53835717 4.28655232 4.34933896 4.54253515 4.54464388
 4.35017711 4.38486951 4.43846938 4.54768586]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [3.68049423e-07 3.07275006e-06 3.52887078e-07 1.42937379e-08
 4.72231164e-06 5.12443289e-07 2.82979099e-08 1.01445480e-06
 3.79271804e-07 2.70179295e-07]
ene_total = [0.18661622 0.05104089 0.19383747 0.15822588 0.04868069 0.04746084
 0.1577506  0.13808028 0.10767732 0.0457342 ]
optimize_network iter = 0 obj = 1.1351043989055445
eta = 0.5606719484671788
freqs = [ 5552816.27091243 11063338.14405196  5480908.36118572  1873200.15342277
 12763274.63448044  6086841.3557449   2351937.65383736  7734555.17253756
  5549439.33843933  4916211.22498621]
eta_min = 0.5606719484671795	eta_max = 0.8680569114774389
af = 4.622705191075305e-05	bf = 0.4986564559927847	zeta = 5.084975710182836e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [6.06251059e-08 5.06143430e-07 5.81275641e-08 2.35446469e-09
 7.77859235e-07 8.44096653e-08 4.66123208e-09 1.67101009e-07
 6.24736566e-08 4.45039370e-08]
ene_total = [0.90183473 0.24659032 0.93673252 0.76464342 0.23514662 0.22934794
 0.76234626 0.66726472 0.52035367 0.22100929]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 1 obj = 3.7793676563367
eta = 0.8680569114774389
freqs = [ 5502474.32221998  9444523.43544286  5480908.36118572  1793229.33817215
 10870816.12174629  5178356.71698614  2250264.85743831  7233513.5913764
  5018771.87691086  4175523.98380345]
eta_min = 0.8680569114774714	eta_max = 0.8680569114774374
af = 3.523192231617816e-05	bf = 0.4986564559927847	zeta = 3.8755114547795976e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [5.95308319e-08 3.68859801e-07 5.81275641e-08 2.15772197e-09
 5.64288349e-07 6.10930969e-08 4.26693859e-09 1.46152750e-07
 5.10967904e-08 3.21040135e-08]
ene_total = [0.9018347  0.24658655 0.93673252 0.76464342 0.23514077 0.2293473
 0.76234625 0.66726414 0.52035336 0.22100895]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 2 obj = 3.779367656336656
eta = 0.8680569114774374
freqs = [ 5502474.32221997  9444523.43544286  5480908.3611857   1793229.33817215
 10870816.12174629  5178356.71698613  2250264.85743831  7233513.59137638
  5018771.87691085  4175523.98380344]
Done!
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [7.62986036e-08 4.72754821e-07 7.45000840e-08 2.76547745e-09
 7.23228816e-07 7.83009046e-08 5.46878728e-09 1.87318914e-07
 6.54889850e-08 4.11466013e-08]
ene_total = [0.03290372 0.00899689 0.03417697 0.02789824 0.00857934 0.00836782
 0.02781442 0.02434537 0.01898526 0.00806358]
At round 75 energy consumption: 0.20013159600162664
At round 75 eta: 0.8680569114774374
At round 75 a_n: 2.4916643335555797
At round 75 local rounds: 4.633363424333758
At round 75 global rounds: 18.884386908447258
gradient difference: 0.8874387741088867
train() client id: f_00000-0-0 loss: 1.013844  [   32/  126]
train() client id: f_00000-0-1 loss: 0.996878  [   64/  126]
train() client id: f_00000-0-2 loss: 1.063639  [   96/  126]
train() client id: f_00000-1-0 loss: 0.991448  [   32/  126]
train() client id: f_00000-1-1 loss: 1.202363  [   64/  126]
train() client id: f_00000-1-2 loss: 0.974683  [   96/  126]
train() client id: f_00000-2-0 loss: 1.021919  [   32/  126]
train() client id: f_00000-2-1 loss: 0.943781  [   64/  126]
train() client id: f_00000-2-2 loss: 0.976940  [   96/  126]
train() client id: f_00000-3-0 loss: 1.097047  [   32/  126]
train() client id: f_00000-3-1 loss: 1.008049  [   64/  126]
train() client id: f_00000-3-2 loss: 0.812632  [   96/  126]
train() client id: f_00001-0-0 loss: 0.423060  [   32/  265]
train() client id: f_00001-0-1 loss: 0.557300  [   64/  265]
train() client id: f_00001-0-2 loss: 0.600889  [   96/  265]
train() client id: f_00001-0-3 loss: 0.565496  [  128/  265]
train() client id: f_00001-0-4 loss: 0.552039  [  160/  265]
train() client id: f_00001-0-5 loss: 0.500814  [  192/  265]
train() client id: f_00001-0-6 loss: 0.579618  [  224/  265]
train() client id: f_00001-0-7 loss: 0.596507  [  256/  265]
train() client id: f_00001-1-0 loss: 0.512079  [   32/  265]
train() client id: f_00001-1-1 loss: 0.544476  [   64/  265]
train() client id: f_00001-1-2 loss: 0.513409  [   96/  265]
train() client id: f_00001-1-3 loss: 0.541153  [  128/  265]
train() client id: f_00001-1-4 loss: 0.561329  [  160/  265]
train() client id: f_00001-1-5 loss: 0.589603  [  192/  265]
train() client id: f_00001-1-6 loss: 0.519580  [  224/  265]
train() client id: f_00001-1-7 loss: 0.521557  [  256/  265]
train() client id: f_00001-2-0 loss: 0.631305  [   32/  265]
train() client id: f_00001-2-1 loss: 0.522729  [   64/  265]
train() client id: f_00001-2-2 loss: 0.446946  [   96/  265]
train() client id: f_00001-2-3 loss: 0.570027  [  128/  265]
train() client id: f_00001-2-4 loss: 0.554627  [  160/  265]
train() client id: f_00001-2-5 loss: 0.507517  [  192/  265]
train() client id: f_00001-2-6 loss: 0.568301  [  224/  265]
train() client id: f_00001-2-7 loss: 0.507879  [  256/  265]
train() client id: f_00001-3-0 loss: 0.450500  [   32/  265]
train() client id: f_00001-3-1 loss: 0.609912  [   64/  265]
train() client id: f_00001-3-2 loss: 0.631047  [   96/  265]
train() client id: f_00001-3-3 loss: 0.535670  [  128/  265]
train() client id: f_00001-3-4 loss: 0.499911  [  160/  265]
train() client id: f_00001-3-5 loss: 0.556639  [  192/  265]
train() client id: f_00001-3-6 loss: 0.498444  [  224/  265]
train() client id: f_00001-3-7 loss: 0.524026  [  256/  265]
train() client id: f_00002-0-0 loss: 0.917741  [   32/  124]
train() client id: f_00002-0-1 loss: 0.969173  [   64/  124]
train() client id: f_00002-0-2 loss: 0.967496  [   96/  124]
train() client id: f_00002-1-0 loss: 1.227974  [   32/  124]
train() client id: f_00002-1-1 loss: 0.808201  [   64/  124]
train() client id: f_00002-1-2 loss: 0.745084  [   96/  124]
train() client id: f_00002-2-0 loss: 0.942071  [   32/  124]
train() client id: f_00002-2-1 loss: 0.884287  [   64/  124]
train() client id: f_00002-2-2 loss: 0.913416  [   96/  124]
train() client id: f_00002-3-0 loss: 0.997835  [   32/  124]
train() client id: f_00002-3-1 loss: 0.958134  [   64/  124]
train() client id: f_00002-3-2 loss: 0.945224  [   96/  124]
train() client id: f_00003-0-0 loss: 0.892809  [   32/   43]
train() client id: f_00003-1-0 loss: 0.798287  [   32/   43]
train() client id: f_00003-2-0 loss: 0.740127  [   32/   43]
train() client id: f_00003-3-0 loss: 0.827162  [   32/   43]
train() client id: f_00004-0-0 loss: 0.582110  [   32/  306]
train() client id: f_00004-0-1 loss: 0.600380  [   64/  306]
train() client id: f_00004-0-2 loss: 0.692302  [   96/  306]
train() client id: f_00004-0-3 loss: 0.750972  [  128/  306]
train() client id: f_00004-0-4 loss: 0.793470  [  160/  306]
train() client id: f_00004-0-5 loss: 0.825245  [  192/  306]
train() client id: f_00004-0-6 loss: 0.770740  [  224/  306]
train() client id: f_00004-0-7 loss: 0.567850  [  256/  306]
train() client id: f_00004-0-8 loss: 0.841978  [  288/  306]
train() client id: f_00004-1-0 loss: 0.792478  [   32/  306]
train() client id: f_00004-1-1 loss: 0.837152  [   64/  306]
train() client id: f_00004-1-2 loss: 0.748662  [   96/  306]
train() client id: f_00004-1-3 loss: 0.751753  [  128/  306]
train() client id: f_00004-1-4 loss: 0.622593  [  160/  306]
train() client id: f_00004-1-5 loss: 0.583368  [  192/  306]
train() client id: f_00004-1-6 loss: 0.622536  [  224/  306]
train() client id: f_00004-1-7 loss: 0.747693  [  256/  306]
train() client id: f_00004-1-8 loss: 0.643968  [  288/  306]
train() client id: f_00004-2-0 loss: 0.717959  [   32/  306]
train() client id: f_00004-2-1 loss: 0.754806  [   64/  306]
train() client id: f_00004-2-2 loss: 0.706407  [   96/  306]
train() client id: f_00004-2-3 loss: 0.533830  [  128/  306]
train() client id: f_00004-2-4 loss: 0.776295  [  160/  306]
train() client id: f_00004-2-5 loss: 0.768551  [  192/  306]
train() client id: f_00004-2-6 loss: 0.625099  [  224/  306]
train() client id: f_00004-2-7 loss: 0.722595  [  256/  306]
train() client id: f_00004-2-8 loss: 0.780781  [  288/  306]
train() client id: f_00004-3-0 loss: 0.639621  [   32/  306]
train() client id: f_00004-3-1 loss: 0.645214  [   64/  306]
train() client id: f_00004-3-2 loss: 0.743342  [   96/  306]
train() client id: f_00004-3-3 loss: 0.597277  [  128/  306]
train() client id: f_00004-3-4 loss: 0.788221  [  160/  306]
train() client id: f_00004-3-5 loss: 0.783376  [  192/  306]
train() client id: f_00004-3-6 loss: 0.820048  [  224/  306]
train() client id: f_00004-3-7 loss: 0.709030  [  256/  306]
train() client id: f_00004-3-8 loss: 0.791341  [  288/  306]
train() client id: f_00005-0-0 loss: 0.846584  [   32/  146]
train() client id: f_00005-0-1 loss: 0.635924  [   64/  146]
train() client id: f_00005-0-2 loss: 0.741218  [   96/  146]
train() client id: f_00005-0-3 loss: 0.701158  [  128/  146]
train() client id: f_00005-1-0 loss: 0.875272  [   32/  146]
train() client id: f_00005-1-1 loss: 0.645678  [   64/  146]
train() client id: f_00005-1-2 loss: 0.661340  [   96/  146]
train() client id: f_00005-1-3 loss: 0.696744  [  128/  146]
train() client id: f_00005-2-0 loss: 0.756685  [   32/  146]
train() client id: f_00005-2-1 loss: 0.772077  [   64/  146]
train() client id: f_00005-2-2 loss: 0.666635  [   96/  146]
train() client id: f_00005-2-3 loss: 0.718998  [  128/  146]
train() client id: f_00005-3-0 loss: 0.736370  [   32/  146]
train() client id: f_00005-3-1 loss: 0.874884  [   64/  146]
train() client id: f_00005-3-2 loss: 0.804800  [   96/  146]
train() client id: f_00005-3-3 loss: 0.540330  [  128/  146]
train() client id: f_00006-0-0 loss: 0.551224  [   32/   54]
train() client id: f_00006-1-0 loss: 0.533249  [   32/   54]
train() client id: f_00006-2-0 loss: 0.625703  [   32/   54]
train() client id: f_00006-3-0 loss: 0.571586  [   32/   54]
train() client id: f_00007-0-0 loss: 0.788487  [   32/  179]
train() client id: f_00007-0-1 loss: 0.737058  [   64/  179]
train() client id: f_00007-0-2 loss: 0.858193  [   96/  179]
train() client id: f_00007-0-3 loss: 0.627894  [  128/  179]
train() client id: f_00007-0-4 loss: 0.922718  [  160/  179]
train() client id: f_00007-1-0 loss: 0.703828  [   32/  179]
train() client id: f_00007-1-1 loss: 1.010056  [   64/  179]
train() client id: f_00007-1-2 loss: 0.676643  [   96/  179]
train() client id: f_00007-1-3 loss: 0.762981  [  128/  179]
train() client id: f_00007-1-4 loss: 0.808104  [  160/  179]
train() client id: f_00007-2-0 loss: 0.854276  [   32/  179]
train() client id: f_00007-2-1 loss: 0.691661  [   64/  179]
train() client id: f_00007-2-2 loss: 0.980283  [   96/  179]
train() client id: f_00007-2-3 loss: 0.618130  [  128/  179]
train() client id: f_00007-2-4 loss: 0.784687  [  160/  179]
train() client id: f_00007-3-0 loss: 0.679813  [   32/  179]
train() client id: f_00007-3-1 loss: 0.707511  [   64/  179]
train() client id: f_00007-3-2 loss: 1.021209  [   96/  179]
train() client id: f_00007-3-3 loss: 0.678067  [  128/  179]
train() client id: f_00007-3-4 loss: 0.722896  [  160/  179]
train() client id: f_00008-0-0 loss: 0.695222  [   32/  130]
train() client id: f_00008-0-1 loss: 0.645724  [   64/  130]
train() client id: f_00008-0-2 loss: 0.636027  [   96/  130]
train() client id: f_00008-0-3 loss: 0.618239  [  128/  130]
train() client id: f_00008-1-0 loss: 0.584346  [   32/  130]
train() client id: f_00008-1-1 loss: 0.587626  [   64/  130]
train() client id: f_00008-1-2 loss: 0.681580  [   96/  130]
train() client id: f_00008-1-3 loss: 0.738784  [  128/  130]
train() client id: f_00008-2-0 loss: 0.648465  [   32/  130]
train() client id: f_00008-2-1 loss: 0.600581  [   64/  130]
train() client id: f_00008-2-2 loss: 0.683732  [   96/  130]
train() client id: f_00008-2-3 loss: 0.628426  [  128/  130]
train() client id: f_00008-3-0 loss: 0.788008  [   32/  130]
train() client id: f_00008-3-1 loss: 0.614436  [   64/  130]
train() client id: f_00008-3-2 loss: 0.563583  [   96/  130]
train() client id: f_00008-3-3 loss: 0.629298  [  128/  130]
train() client id: f_00009-0-0 loss: 0.989549  [   32/  118]
train() client id: f_00009-0-1 loss: 0.792265  [   64/  118]
train() client id: f_00009-0-2 loss: 0.804329  [   96/  118]
train() client id: f_00009-1-0 loss: 0.809805  [   32/  118]
train() client id: f_00009-1-1 loss: 1.010872  [   64/  118]
train() client id: f_00009-1-2 loss: 0.855386  [   96/  118]
train() client id: f_00009-2-0 loss: 0.919186  [   32/  118]
train() client id: f_00009-2-1 loss: 0.871974  [   64/  118]
train() client id: f_00009-2-2 loss: 0.633522  [   96/  118]
train() client id: f_00009-3-0 loss: 0.701696  [   32/  118]
train() client id: f_00009-3-1 loss: 0.929657  [   64/  118]
train() client id: f_00009-3-2 loss: 0.935131  [   96/  118]
At round 75 accuracy: 0.6445623342175066
At round 75 training accuracy: 0.5855130784708249
At round 75 training loss: 0.8246667582762407
update_location
xs = [  -3.9056584     4.20031788  395.00902392   18.81129433    0.97929623
    3.95640986 -357.44319194 -336.32485185  379.66397685 -322.06087855]
ys = [ 387.5879595   370.55583871    1.32061395 -357.45517586  349.35018685
  332.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [400.29948853 383.83495447 407.47254263 371.65584557 363.38204698
 347.53548324 371.17721644 350.87758866 393.00560465 337.25245937]
dists_bs = [272.88626614 265.24486796 595.88154568 566.63156646 247.72303154
 238.60155659 254.62944319 237.41011412 576.57319143 225.53801284]
uav_gains = [7.22036186e-13 8.36522048e-13 6.80532523e-13 9.44154307e-13
 1.03236608e-12 1.24853056e-12 9.48887642e-13 1.19672866e-12
 7.69075581e-13 1.43449114e-12]
bs_gains = [1.66934225e-11 1.80751751e-11 1.87433090e-12 2.15800250e-12
 2.18870991e-11 2.43113414e-11 2.02651559e-11 2.46545045e-11
 2.05542502e-12 2.84628764e-11]
Round 76
-------------------------------
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.89822293 1.75469717 0.88790864 0.34389424 2.02220671 0.97420485
 0.41573163 1.22653712 0.89277918 0.79022156]
obj_prev = 10.206404031521155
eta_min = 1.1840901563596855e-105	eta_max = 0.9712061927430241
af = 2.098524592010151	bf = 0.440277105517973	zeta = 2.308377051211166	eta = 0.9090909090909091
af = 2.098524592010151	bf = 0.440277105517973	zeta = 7.043166478768837	eta = 0.29795186559000353
af = 2.098524592010151	bf = 0.440277105517973	zeta = 4.246460455912228	eta = 0.49418206381468455
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.794695110359663	eta = 0.5530153361415251
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.766078122653466	eta = 0.557217488237231
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.765942672970741	eta = 0.5572375296819754
eta = 0.5572375296819754
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.0482533  0.10148511 0.04748737 0.0164674  0.11718658 0.05591255
 0.02067998 0.06855032 0.04978515 0.0451896 ]
ene_total = [0.39809468 0.53669908 0.4005926  0.21938841 0.6108545  0.31238457
 0.23943255 0.45484532 0.3348021  0.25884885]
ti_comp = [5.06004778 5.30681242 5.04724229 5.11036031 5.31105174 5.31322071
 5.11119117 5.14610005 5.20576198 5.31628572]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [2.74253247e-07 2.31963433e-06 2.62727860e-07 1.06868990e-08
 3.56576398e-06 3.86983308e-07 2.11585747e-08 7.60242285e-07
 2.84584225e-07 2.04069817e-07]
ene_total = [0.16388001 0.0441132  0.1700956  0.13945761 0.04206153 0.04099331
 0.13905437 0.12211358 0.09315205 0.0395047 ]
optimize_network iter = 0 obj = 0.9944259543834252
eta = 0.5572375296819754
freqs = [ 4768067.39444     9561776.65897993  4704288.96039803  1611177.51564226
 11032332.74977597  5261643.95832219  2023010.29856296  6660414.59689484
  4781734.99264716  4250109.82064523]
eta_min = 0.5572375296819757	eta_max = 0.8837926289249317
af = 2.9732682841315033e-05	bf = 0.440277105517973	zeta = 3.2705951125446536e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.47003242e-08 3.78075401e-07 4.28218103e-08 1.74184938e-09
 5.81181107e-07 6.30741094e-08 3.44861970e-09 1.23911301e-07
 4.63841622e-08 3.32611812e-08]
ene_total = [0.79815367 0.21480247 0.82842621 0.67921232 0.20478538 0.19964562
 0.67724819 0.59472538 0.45368094 0.19239915]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 1 obj = 3.7887429688072136
eta = 0.8837926289249317
freqs = [4723436.47190019 8085223.78253195 4704288.96039802 1540154.03017325
 9306387.77331266 4433073.47420022 1932725.03801711 6214669.51457857
 4293583.32001691 3574670.85257318]
eta_min = 0.8837926289249345	eta_max = 0.8837926289249313
af = 2.229825334963228e-05	bf = 0.440277105517973	zeta = 2.452807868459551e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.38674166e-08 2.70324445e-07 4.28218103e-08 1.59166668e-09
 4.13560520e-07 4.47731935e-08 3.14767049e-09 1.07880876e-07
 3.73971494e-08 2.35293258e-08]
ene_total = [0.79815365 0.21479992 0.82842621 0.67921232 0.20478142 0.19964519
 0.67724818 0.594725   0.45368072 0.19239892]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 2 obj = 3.788742968807199
eta = 0.8837926289249313
freqs = [4723436.47190018 8085223.78253194 4704288.96039801 1540154.03017325
 9306387.77331265 4433073.47420021 1932725.03801711 6214669.51457855
 4293583.3200169  3574670.85257317]
Done!
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.62233473e-08 3.46465471e-07 5.48832299e-08 2.03998402e-09
 5.30046183e-07 5.73842500e-08 4.03426016e-09 1.38267179e-07
 4.79306301e-08 3.01567212e-08]
ene_total = [0.03376242 0.00908625 0.03504297 0.02873112 0.0086625  0.00844513
 0.02864803 0.02515728 0.019191   0.0081386 ]
At round 76 energy consumption: 0.20486531204581118
At round 76 eta: 0.8837926289249313
At round 76 a_n: 2.1491184865862607
At round 76 local rounds: 4.045092362532633
At round 76 global rounds: 18.49382243745926
gradient difference: 0.7592489123344421
train() client id: f_00000-0-0 loss: 1.273869  [   32/  126]
train() client id: f_00000-0-1 loss: 1.073568  [   64/  126]
train() client id: f_00000-0-2 loss: 0.922419  [   96/  126]
train() client id: f_00000-1-0 loss: 1.014968  [   32/  126]
train() client id: f_00000-1-1 loss: 1.113469  [   64/  126]
train() client id: f_00000-1-2 loss: 1.095983  [   96/  126]
train() client id: f_00000-2-0 loss: 1.101366  [   32/  126]
train() client id: f_00000-2-1 loss: 1.073371  [   64/  126]
train() client id: f_00000-2-2 loss: 1.100302  [   96/  126]
train() client id: f_00000-3-0 loss: 1.023068  [   32/  126]
train() client id: f_00000-3-1 loss: 0.867742  [   64/  126]
train() client id: f_00000-3-2 loss: 1.083621  [   96/  126]
train() client id: f_00001-0-0 loss: 0.534723  [   32/  265]
train() client id: f_00001-0-1 loss: 0.560025  [   64/  265]
train() client id: f_00001-0-2 loss: 0.578465  [   96/  265]
train() client id: f_00001-0-3 loss: 0.542856  [  128/  265]
train() client id: f_00001-0-4 loss: 0.493258  [  160/  265]
train() client id: f_00001-0-5 loss: 0.540625  [  192/  265]
train() client id: f_00001-0-6 loss: 0.513448  [  224/  265]
train() client id: f_00001-0-7 loss: 0.498036  [  256/  265]
train() client id: f_00001-1-0 loss: 0.506697  [   32/  265]
train() client id: f_00001-1-1 loss: 0.434555  [   64/  265]
train() client id: f_00001-1-2 loss: 0.551445  [   96/  265]
train() client id: f_00001-1-3 loss: 0.619811  [  128/  265]
train() client id: f_00001-1-4 loss: 0.437265  [  160/  265]
train() client id: f_00001-1-5 loss: 0.521603  [  192/  265]
train() client id: f_00001-1-6 loss: 0.544194  [  224/  265]
train() client id: f_00001-1-7 loss: 0.595135  [  256/  265]
train() client id: f_00001-2-0 loss: 0.668746  [   32/  265]
train() client id: f_00001-2-1 loss: 0.534823  [   64/  265]
train() client id: f_00001-2-2 loss: 0.464518  [   96/  265]
train() client id: f_00001-2-3 loss: 0.526833  [  128/  265]
train() client id: f_00001-2-4 loss: 0.436624  [  160/  265]
train() client id: f_00001-2-5 loss: 0.521061  [  192/  265]
train() client id: f_00001-2-6 loss: 0.435962  [  224/  265]
train() client id: f_00001-2-7 loss: 0.593279  [  256/  265]
train() client id: f_00001-3-0 loss: 0.612190  [   32/  265]
train() client id: f_00001-3-1 loss: 0.512720  [   64/  265]
train() client id: f_00001-3-2 loss: 0.555826  [   96/  265]
train() client id: f_00001-3-3 loss: 0.434790  [  128/  265]
train() client id: f_00001-3-4 loss: 0.550173  [  160/  265]
train() client id: f_00001-3-5 loss: 0.421545  [  192/  265]
train() client id: f_00001-3-6 loss: 0.498574  [  224/  265]
train() client id: f_00001-3-7 loss: 0.550178  [  256/  265]
train() client id: f_00002-0-0 loss: 1.124234  [   32/  124]
train() client id: f_00002-0-1 loss: 1.034003  [   64/  124]
train() client id: f_00002-0-2 loss: 0.781987  [   96/  124]
train() client id: f_00002-1-0 loss: 1.044507  [   32/  124]
train() client id: f_00002-1-1 loss: 0.990450  [   64/  124]
train() client id: f_00002-1-2 loss: 0.624388  [   96/  124]
train() client id: f_00002-2-0 loss: 0.871143  [   32/  124]
train() client id: f_00002-2-1 loss: 0.923041  [   64/  124]
train() client id: f_00002-2-2 loss: 1.013967  [   96/  124]
train() client id: f_00002-3-0 loss: 0.955400  [   32/  124]
train() client id: f_00002-3-1 loss: 1.208281  [   64/  124]
train() client id: f_00002-3-2 loss: 0.870035  [   96/  124]
train() client id: f_00003-0-0 loss: 0.870695  [   32/   43]
train() client id: f_00003-1-0 loss: 0.512318  [   32/   43]
train() client id: f_00003-2-0 loss: 0.824104  [   32/   43]
train() client id: f_00003-3-0 loss: 0.623723  [   32/   43]
train() client id: f_00004-0-0 loss: 0.721153  [   32/  306]
train() client id: f_00004-0-1 loss: 1.015013  [   64/  306]
train() client id: f_00004-0-2 loss: 0.825179  [   96/  306]
train() client id: f_00004-0-3 loss: 0.942049  [  128/  306]
train() client id: f_00004-0-4 loss: 0.780761  [  160/  306]
train() client id: f_00004-0-5 loss: 0.730721  [  192/  306]
train() client id: f_00004-0-6 loss: 0.890610  [  224/  306]
train() client id: f_00004-0-7 loss: 0.688844  [  256/  306]
train() client id: f_00004-0-8 loss: 0.745028  [  288/  306]
train() client id: f_00004-1-0 loss: 0.716703  [   32/  306]
train() client id: f_00004-1-1 loss: 0.697084  [   64/  306]
train() client id: f_00004-1-2 loss: 0.840187  [   96/  306]
train() client id: f_00004-1-3 loss: 0.643174  [  128/  306]
train() client id: f_00004-1-4 loss: 0.836932  [  160/  306]
train() client id: f_00004-1-5 loss: 0.769375  [  192/  306]
train() client id: f_00004-1-6 loss: 0.979715  [  224/  306]
train() client id: f_00004-1-7 loss: 0.894136  [  256/  306]
train() client id: f_00004-1-8 loss: 0.791606  [  288/  306]
train() client id: f_00004-2-0 loss: 0.814966  [   32/  306]
train() client id: f_00004-2-1 loss: 0.765244  [   64/  306]
train() client id: f_00004-2-2 loss: 0.708407  [   96/  306]
train() client id: f_00004-2-3 loss: 0.770139  [  128/  306]
train() client id: f_00004-2-4 loss: 0.729427  [  160/  306]
train() client id: f_00004-2-5 loss: 0.899721  [  192/  306]
train() client id: f_00004-2-6 loss: 0.772875  [  224/  306]
train() client id: f_00004-2-7 loss: 0.826974  [  256/  306]
train() client id: f_00004-2-8 loss: 0.919822  [  288/  306]
train() client id: f_00004-3-0 loss: 0.741024  [   32/  306]
train() client id: f_00004-3-1 loss: 0.755865  [   64/  306]
train() client id: f_00004-3-2 loss: 0.785430  [   96/  306]
train() client id: f_00004-3-3 loss: 0.800263  [  128/  306]
train() client id: f_00004-3-4 loss: 0.765691  [  160/  306]
train() client id: f_00004-3-5 loss: 0.831111  [  192/  306]
train() client id: f_00004-3-6 loss: 0.828012  [  224/  306]
train() client id: f_00004-3-7 loss: 0.986898  [  256/  306]
train() client id: f_00004-3-8 loss: 0.799654  [  288/  306]
train() client id: f_00005-0-0 loss: 0.823841  [   32/  146]
train() client id: f_00005-0-1 loss: 0.818614  [   64/  146]
train() client id: f_00005-0-2 loss: 0.950996  [   96/  146]
train() client id: f_00005-0-3 loss: 0.842336  [  128/  146]
train() client id: f_00005-1-0 loss: 0.913832  [   32/  146]
train() client id: f_00005-1-1 loss: 0.894178  [   64/  146]
train() client id: f_00005-1-2 loss: 1.020809  [   96/  146]
train() client id: f_00005-1-3 loss: 0.625763  [  128/  146]
train() client id: f_00005-2-0 loss: 0.775666  [   32/  146]
train() client id: f_00005-2-1 loss: 1.040880  [   64/  146]
train() client id: f_00005-2-2 loss: 0.892493  [   96/  146]
train() client id: f_00005-2-3 loss: 0.787136  [  128/  146]
train() client id: f_00005-3-0 loss: 0.686952  [   32/  146]
train() client id: f_00005-3-1 loss: 0.974935  [   64/  146]
train() client id: f_00005-3-2 loss: 0.874175  [   96/  146]
train() client id: f_00005-3-3 loss: 0.867854  [  128/  146]
train() client id: f_00006-0-0 loss: 0.471762  [   32/   54]
train() client id: f_00006-1-0 loss: 0.526650  [   32/   54]
train() client id: f_00006-2-0 loss: 0.488184  [   32/   54]
train() client id: f_00006-3-0 loss: 0.521958  [   32/   54]
train() client id: f_00007-0-0 loss: 0.554152  [   32/  179]
train() client id: f_00007-0-1 loss: 0.694793  [   64/  179]
train() client id: f_00007-0-2 loss: 0.582626  [   96/  179]
train() client id: f_00007-0-3 loss: 0.783069  [  128/  179]
train() client id: f_00007-0-4 loss: 0.857710  [  160/  179]
train() client id: f_00007-1-0 loss: 0.572405  [   32/  179]
train() client id: f_00007-1-1 loss: 0.632771  [   64/  179]
train() client id: f_00007-1-2 loss: 0.903593  [   96/  179]
train() client id: f_00007-1-3 loss: 0.736986  [  128/  179]
train() client id: f_00007-1-4 loss: 0.745141  [  160/  179]
train() client id: f_00007-2-0 loss: 0.905122  [   32/  179]
train() client id: f_00007-2-1 loss: 0.623762  [   64/  179]
train() client id: f_00007-2-2 loss: 0.849384  [   96/  179]
train() client id: f_00007-2-3 loss: 0.557277  [  128/  179]
train() client id: f_00007-2-4 loss: 0.764548  [  160/  179]
train() client id: f_00007-3-0 loss: 0.936168  [   32/  179]
train() client id: f_00007-3-1 loss: 0.672037  [   64/  179]
train() client id: f_00007-3-2 loss: 0.559048  [   96/  179]
train() client id: f_00007-3-3 loss: 0.745353  [  128/  179]
train() client id: f_00007-3-4 loss: 0.688710  [  160/  179]
train() client id: f_00008-0-0 loss: 0.629810  [   32/  130]
train() client id: f_00008-0-1 loss: 0.748751  [   64/  130]
train() client id: f_00008-0-2 loss: 0.695105  [   96/  130]
train() client id: f_00008-0-3 loss: 0.760064  [  128/  130]
train() client id: f_00008-1-0 loss: 0.672576  [   32/  130]
train() client id: f_00008-1-1 loss: 0.572844  [   64/  130]
train() client id: f_00008-1-2 loss: 0.757642  [   96/  130]
train() client id: f_00008-1-3 loss: 0.777460  [  128/  130]
train() client id: f_00008-2-0 loss: 0.619930  [   32/  130]
train() client id: f_00008-2-1 loss: 0.658707  [   64/  130]
train() client id: f_00008-2-2 loss: 0.879578  [   96/  130]
train() client id: f_00008-2-3 loss: 0.654897  [  128/  130]
train() client id: f_00008-3-0 loss: 0.599784  [   32/  130]
train() client id: f_00008-3-1 loss: 0.695602  [   64/  130]
train() client id: f_00008-3-2 loss: 0.764982  [   96/  130]
train() client id: f_00008-3-3 loss: 0.736485  [  128/  130]
train() client id: f_00009-0-0 loss: 0.925501  [   32/  118]
train() client id: f_00009-0-1 loss: 0.673226  [   64/  118]
train() client id: f_00009-0-2 loss: 0.745734  [   96/  118]
train() client id: f_00009-1-0 loss: 0.778262  [   32/  118]
train() client id: f_00009-1-1 loss: 0.798074  [   64/  118]
train() client id: f_00009-1-2 loss: 0.660722  [   96/  118]
train() client id: f_00009-2-0 loss: 0.873758  [   32/  118]
train() client id: f_00009-2-1 loss: 0.757942  [   64/  118]
train() client id: f_00009-2-2 loss: 0.656301  [   96/  118]
train() client id: f_00009-3-0 loss: 0.863274  [   32/  118]
train() client id: f_00009-3-1 loss: 0.663206  [   64/  118]
train() client id: f_00009-3-2 loss: 0.513918  [   96/  118]
At round 76 accuracy: 0.6445623342175066
At round 76 training accuracy: 0.5955734406438632
At round 76 training loss: 0.8116910397550042
update_location
xs = [  -3.9056584     4.20031788  400.00902392   18.81129433    0.97929623
    3.95640986 -362.44319194 -341.32485185  384.66397685 -327.06087855]
ys = [ 392.5879595   375.55583871    1.32061395 -362.45517586  354.35018685
  337.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [405.14264168 388.66416179 412.32143194 376.46728849 368.19154517
 352.32662917 375.99462486 355.67306722 397.83796332 342.03045206]
dists_bs = [276.79919408 268.95240911 600.66621906 571.33255103 251.26719289
 241.93768684 258.23552284 240.83565102 581.38479929 228.82963973]
uav_gains = [6.93541635e-13 7.99744288e-13 6.54792901e-13 8.98819590e-13
 9.79395095e-13 1.17540781e-12 9.03099454e-13 1.12860915e-12
 7.37325739e-13 1.34266847e-12]
bs_gains = [1.60410424e-11 1.73861274e-11 1.83282543e-12 2.10865230e-12
 2.10336130e-11 2.33842916e-11 1.94827084e-11 2.36851376e-11
 2.00814839e-12 2.73312636e-11]
Round 77
-------------------------------
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.7566319  1.47518382 0.74797579 0.2906086  1.70004424 0.81907155
 0.35099723 1.03253353 0.75085927 0.66440907]
obj_prev = 8.588314982517188
eta_min = nan	eta_max = nan
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 1.9404471408469977	eta = 0.9090909090909091
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 6.015300613417023	eta = 0.2932593013524188
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.5977300389560636	eta = 0.49032107362544536
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.2097433156161213	eta = 0.5495900082517465
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1851865241926465	eta = 0.5538271752429276
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1850702185308974	eta = 0.5538473987330524
eta = 0.5538473987330524
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.04875685 0.10254417 0.04798293 0.01663924 0.1184095  0.05649604
 0.02089579 0.06926569 0.05030469 0.04566118]
ene_total = [0.33763567 0.45238391 0.33971862 0.18710392 0.51488509 0.26328129
 0.20400615 0.38565213 0.28224513 0.2181583 ]
ti_comp = [6.11168561 6.36617567 6.09880235 6.16226391 6.37047552 6.37270362
 6.16308753 6.19819213 6.2639569  6.37579113]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [1.93938935e-07 1.66286030e-06 1.85631348e-07 7.58228597e-09
 2.55679530e-06 2.77515029e-07 1.50127409e-08 5.40635308e-07
 2.02771968e-07 1.46370394e-07]
ene_total = [0.14020895 0.03716612 0.14542564 0.1197279  0.03542864 0.0345172
 0.11939442 0.10518191 0.0785509  0.03326647]
optimize_network iter = 0 obj = 0.848868136074443
eta = 0.5538473987330524
freqs = [3988822.08036756 8053828.5629115  3933799.78027688 1350091.76188237
 9293615.50887199 4432658.36750978 1695237.49554636 5587571.86903341
 4015408.34280281 3580824.62742287]
eta_min = 0.5538473987330529	eta_max = 0.9002731777032497
af = 1.770021082250513e-05	bf = 0.3787142685879881	zeta = 1.9470231904755644e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.12834926e-08 2.68229162e-07 2.99434299e-08 1.22306739e-09
 4.12426142e-07 4.47648089e-08 2.42164354e-09 8.72076598e-08
 3.27083129e-08 2.36104067e-08]
ene_total = [0.68809723 0.18237166 0.71369919 0.58758591 0.17382975 0.1693947
 0.58594921 0.51618999 0.38549919 0.16325868]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 1 obj = 3.7975304096863765
eta = 0.9002731777032497
freqs = [3950315.32823697 6744042.83153091 3933799.78027688 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677077 5201849.38696053
 3579043.95899813 2981797.2351148 ]
eta_min = 0.8902239180004755	eta_max = 0.9002731777032479
af = 1.3057149041253039e-05	bf = 0.3787142685879881	zeta = 1.4362863945378344e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.06824073e-08 1.88079677e-07 2.99434299e-08 1.11439939e-09
 2.87746454e-07 3.11514510e-08 2.20390950e-09 7.55829668e-08
 2.59856015e-08 1.63716925e-08]
ene_total = [0.68809721 0.18237007 0.71369919 0.58758591 0.17382727 0.16939443
 0.58594921 0.51618975 0.38549905 0.16325854]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 2 obj = 3.7975304096863085
eta = 0.9002731777032479
freqs = [3950315.32823696 6744042.83153091 3933799.78027686 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677076 5201849.38696052
 3579043.95899813 2981797.2351148 ]
Done!
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [2.94934334e-08 1.80791402e-07 2.87830921e-08 1.07121530e-09
 2.76595992e-07 2.99443012e-08 2.11850581e-09 7.26540515e-08
 2.49786335e-08 1.57372731e-08]
ene_total = [0.03462595 0.00917709 0.03591427 0.02956809 0.0087472  0.00852415
 0.02948573 0.02597534 0.01939881 0.00821538]
At round 77 energy consumption: 0.20963201619321536
At round 77 eta: 0.9002731777032479
At round 77 a_n: 1.8065726396169453
At round 77 local rounds: 3.4401009464594003
At round 77 global rounds: 18.115213119307235
gradient difference: 0.9201159477233887
train() client id: f_00000-0-0 loss: 0.924606  [   32/  126]
train() client id: f_00000-0-1 loss: 0.643025  [   64/  126]
train() client id: f_00000-0-2 loss: 0.738587  [   96/  126]
train() client id: f_00000-1-0 loss: 0.896724  [   32/  126]
train() client id: f_00000-1-1 loss: 0.670881  [   64/  126]
train() client id: f_00000-1-2 loss: 1.022540  [   96/  126]
train() client id: f_00000-2-0 loss: 0.965452  [   32/  126]
train() client id: f_00000-2-1 loss: 0.732280  [   64/  126]
train() client id: f_00000-2-2 loss: 0.842507  [   96/  126]
train() client id: f_00001-0-0 loss: 0.593187  [   32/  265]
train() client id: f_00001-0-1 loss: 0.575576  [   64/  265]
train() client id: f_00001-0-2 loss: 0.482053  [   96/  265]
train() client id: f_00001-0-3 loss: 0.422284  [  128/  265]
train() client id: f_00001-0-4 loss: 0.536497  [  160/  265]
train() client id: f_00001-0-5 loss: 0.513368  [  192/  265]
train() client id: f_00001-0-6 loss: 0.545697  [  224/  265]
train() client id: f_00001-0-7 loss: 0.416657  [  256/  265]
train() client id: f_00001-1-0 loss: 0.426853  [   32/  265]
train() client id: f_00001-1-1 loss: 0.500666  [   64/  265]
train() client id: f_00001-1-2 loss: 0.471606  [   96/  265]
train() client id: f_00001-1-3 loss: 0.589366  [  128/  265]
train() client id: f_00001-1-4 loss: 0.517065  [  160/  265]
train() client id: f_00001-1-5 loss: 0.456660  [  192/  265]
train() client id: f_00001-1-6 loss: 0.583562  [  224/  265]
train() client id: f_00001-1-7 loss: 0.485203  [  256/  265]
train() client id: f_00001-2-0 loss: 0.612458  [   32/  265]
train() client id: f_00001-2-1 loss: 0.435439  [   64/  265]
train() client id: f_00001-2-2 loss: 0.423103  [   96/  265]
train() client id: f_00001-2-3 loss: 0.510875  [  128/  265]
train() client id: f_00001-2-4 loss: 0.681698  [  160/  265]
train() client id: f_00001-2-5 loss: 0.536573  [  192/  265]
train() client id: f_00001-2-6 loss: 0.428711  [  224/  265]
train() client id: f_00001-2-7 loss: 0.437548  [  256/  265]
train() client id: f_00002-0-0 loss: 1.083513  [   32/  124]
train() client id: f_00002-0-1 loss: 1.079100  [   64/  124]
train() client id: f_00002-0-2 loss: 1.067717  [   96/  124]
train() client id: f_00002-1-0 loss: 1.024042  [   32/  124]
train() client id: f_00002-1-1 loss: 1.013078  [   64/  124]
train() client id: f_00002-1-2 loss: 0.934139  [   96/  124]
train() client id: f_00002-2-0 loss: 1.026089  [   32/  124]
train() client id: f_00002-2-1 loss: 0.898736  [   64/  124]
train() client id: f_00002-2-2 loss: 0.999718  [   96/  124]
train() client id: f_00003-0-0 loss: 0.546692  [   32/   43]
train() client id: f_00003-1-0 loss: 0.598866  [   32/   43]
train() client id: f_00003-2-0 loss: 0.683868  [   32/   43]
train() client id: f_00004-0-0 loss: 0.819876  [   32/  306]
train() client id: f_00004-0-1 loss: 1.005398  [   64/  306]
train() client id: f_00004-0-2 loss: 0.736100  [   96/  306]
train() client id: f_00004-0-3 loss: 0.895094  [  128/  306]
train() client id: f_00004-0-4 loss: 0.827516  [  160/  306]
train() client id: f_00004-0-5 loss: 0.858492  [  192/  306]
train() client id: f_00004-0-6 loss: 0.715486  [  224/  306]
train() client id: f_00004-0-7 loss: 0.990016  [  256/  306]
train() client id: f_00004-0-8 loss: 0.898318  [  288/  306]
train() client id: f_00004-1-0 loss: 0.796981  [   32/  306]
train() client id: f_00004-1-1 loss: 0.896062  [   64/  306]
train() client id: f_00004-1-2 loss: 0.722401  [   96/  306]
train() client id: f_00004-1-3 loss: 0.909529  [  128/  306]
train() client id: f_00004-1-4 loss: 0.892222  [  160/  306]
train() client id: f_00004-1-5 loss: 0.984656  [  192/  306]
train() client id: f_00004-1-6 loss: 0.765454  [  224/  306]
train() client id: f_00004-1-7 loss: 0.889514  [  256/  306]
train() client id: f_00004-1-8 loss: 0.910914  [  288/  306]
train() client id: f_00004-2-0 loss: 0.790471  [   32/  306]
train() client id: f_00004-2-1 loss: 0.828357  [   64/  306]
train() client id: f_00004-2-2 loss: 0.872247  [   96/  306]
train() client id: f_00004-2-3 loss: 0.789151  [  128/  306]
train() client id: f_00004-2-4 loss: 0.838689  [  160/  306]
train() client id: f_00004-2-5 loss: 0.899360  [  192/  306]
train() client id: f_00004-2-6 loss: 0.847644  [  224/  306]
train() client id: f_00004-2-7 loss: 0.937747  [  256/  306]
train() client id: f_00004-2-8 loss: 0.805185  [  288/  306]
train() client id: f_00005-0-0 loss: 0.669930  [   32/  146]
train() client id: f_00005-0-1 loss: 0.639499  [   64/  146]
train() client id: f_00005-0-2 loss: 0.564858  [   96/  146]
train() client id: f_00005-0-3 loss: 0.797567  [  128/  146]
train() client id: f_00005-1-0 loss: 0.764081  [   32/  146]
train() client id: f_00005-1-1 loss: 0.608202  [   64/  146]
train() client id: f_00005-1-2 loss: 0.813125  [   96/  146]
train() client id: f_00005-1-3 loss: 0.645734  [  128/  146]
train() client id: f_00005-2-0 loss: 0.635260  [   32/  146]
train() client id: f_00005-2-1 loss: 0.503762  [   64/  146]
train() client id: f_00005-2-2 loss: 0.629804  [   96/  146]
train() client id: f_00005-2-3 loss: 0.892281  [  128/  146]
train() client id: f_00006-0-0 loss: 0.430957  [   32/   54]
train() client id: f_00006-1-0 loss: 0.423743  [   32/   54]
train() client id: f_00006-2-0 loss: 0.412356  [   32/   54]
train() client id: f_00007-0-0 loss: 0.552943  [   32/  179]
train() client id: f_00007-0-1 loss: 0.610713  [   64/  179]
train() client id: f_00007-0-2 loss: 0.721270  [   96/  179]
train() client id: f_00007-0-3 loss: 0.762375  [  128/  179]
train() client id: f_00007-0-4 loss: 0.876695  [  160/  179]
train() client id: f_00007-1-0 loss: 0.699374  [   32/  179]
train() client id: f_00007-1-1 loss: 0.639732  [   64/  179]
train() client id: f_00007-1-2 loss: 0.774863  [   96/  179]
train() client id: f_00007-1-3 loss: 0.807537  [  128/  179]
train() client id: f_00007-1-4 loss: 0.736822  [  160/  179]
train() client id: f_00007-2-0 loss: 0.801778  [   32/  179]
train() client id: f_00007-2-1 loss: 0.701069  [   64/  179]
train() client id: f_00007-2-2 loss: 0.696888  [   96/  179]
train() client id: f_00007-2-3 loss: 0.916750  [  128/  179]
train() client id: f_00007-2-4 loss: 0.640394  [  160/  179]
train() client id: f_00008-0-0 loss: 0.763973  [   32/  130]
train() client id: f_00008-0-1 loss: 0.707501  [   64/  130]
train() client id: f_00008-0-2 loss: 0.679012  [   96/  130]
train() client id: f_00008-0-3 loss: 0.685958  [  128/  130]
train() client id: f_00008-1-0 loss: 0.681421  [   32/  130]
train() client id: f_00008-1-1 loss: 0.722826  [   64/  130]
train() client id: f_00008-1-2 loss: 0.736305  [   96/  130]
train() client id: f_00008-1-3 loss: 0.695099  [  128/  130]
train() client id: f_00008-2-0 loss: 0.679193  [   32/  130]
train() client id: f_00008-2-1 loss: 0.651358  [   64/  130]
train() client id: f_00008-2-2 loss: 0.732485  [   96/  130]
train() client id: f_00008-2-3 loss: 0.720167  [  128/  130]
train() client id: f_00009-0-0 loss: 1.036112  [   32/  118]
train() client id: f_00009-0-1 loss: 0.876522  [   64/  118]
train() client id: f_00009-0-2 loss: 1.121840  [   96/  118]
train() client id: f_00009-1-0 loss: 1.168173  [   32/  118]
train() client id: f_00009-1-1 loss: 0.920798  [   64/  118]
train() client id: f_00009-1-2 loss: 0.879477  [   96/  118]
train() client id: f_00009-2-0 loss: 1.028944  [   32/  118]
train() client id: f_00009-2-1 loss: 0.883832  [   64/  118]
train() client id: f_00009-2-2 loss: 1.032772  [   96/  118]
At round 77 accuracy: 0.6445623342175066
At round 77 training accuracy: 0.5868544600938967
At round 77 training loss: 0.8215948424542941
update_location
xs = [  -3.9056584     4.20031788  405.00902392   18.81129433    0.97929623
    3.95640986 -367.44319194 -346.32485185  389.66397685 -332.06087855]
ys = [ 397.5879595   380.55583871    1.32061395 -367.45517586  359.35018685
  342.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [409.98956048 393.49763537 417.17388877 381.28358352 373.00605331
 357.12350123 380.8167405  360.47410346 402.67441541 346.81470401]
dists_bs = [280.74663567 272.7012225  605.45437232 576.0385713  254.86016576
 245.33036058 261.88741317 244.31549137 586.1995604  232.18228357]
uav_gains = [6.66953995e-13 7.65732155e-13 6.30692965e-13 8.57197052e-13
 9.31026172e-13 1.10932372e-12 8.61079374e-13 1.06691429e-12
 7.07809580e-13 1.26020837e-12]
bs_gains = [1.54174772e-11 1.67251587e-11 1.79252867e-12 2.06077086e-12
 2.02138293e-11 2.24900517e-11 1.87315249e-11 2.27526125e-11
 1.96230594e-12 2.62405338e-11]
Round 78
-------------------------------
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.61445089 1.1956097  0.60744834 0.2367497  1.37782529 0.66388603
 0.28568902 0.83797174 0.60879864 0.53854598]
obj_prev = 6.96697533321086
eta_min = nan	eta_max = nan
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 1.572517230482826	eta = 0.909090909090909
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 4.952035710772682	eta = 0.2886815043580717
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.9382966582345253	eta = 0.48652715668257346
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.6172340285407683	eta = 0.5462106571408896
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.596934549245399	eta = 0.5504802264023727
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.5968383965379944	eta = 0.5505006089430082
eta = 0.5505006089430082
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.049257   0.10359608 0.04847515 0.01680993 0.11962415 0.05707558
 0.02111014 0.06997622 0.05082072 0.04612957]
ene_total = [0.27603738 0.36760475 0.27771398 0.15379303 0.4183885  0.21392216
 0.16753295 0.31519772 0.22938986 0.17725807]
ti_comp = [7.65842292 7.92067487 7.64545672 7.70927839 7.9250345  7.92732068
 7.71009494 7.74538131 7.81728173 7.93043026]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.27351857e-07 1.10760673e-06 1.21795299e-07 4.99516971e-09
 1.70346924e-06 1.84917516e-07 9.89086254e-09 3.56981594e-07
 1.34242758e-07 9.75494579e-08]
ene_total = [0.11560694 0.03019425 0.11983004 0.09904284 0.02877625 0.02802669
 0.0987769  0.0872852  0.06386639 0.02701362]
optimize_network iter = 0 obj = 0.6984191265921221
eta = 0.5505006089430082
freqs = [3215871.18363491 6539599.20133967 3170192.94839821 1090240.14955988
 7547232.16704463 3599928.48687781 1368993.7919406  4517286.87955715
 3250536.39445521 2908390.39625094]
eta_min = 0.5505006089430086	eta_max = 0.917509374892285
af = 9.439588067529733e-06	bf = 0.3139333373586834	zeta = 1.0383546874282708e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [2.03340174e-08 1.76849361e-07 1.94468128e-08 7.97568798e-10
 2.71989541e-07 2.95254116e-08 1.57925432e-09 5.69985401e-08
 2.14342739e-08 1.55755277e-08]
ene_total = [0.57161603 0.14928029 0.59249717 0.48971657 0.14226095 0.1385753
 0.48840159 0.43157626 0.31578514 0.13356732]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 1 obj = 3.4532738111991463
eta = 0.909090909090909
freqs = [3106837.34109527 5412726.23484139 3089168.32072196 1019314.72951421
 6232384.57566306 2969191.46464435 1279276.24484534 4129943.16102444
 2848016.20722417 2394904.72079761]
eta_min = 0.9009430532547689	eta_max = 0.9090909090909057
af = 6.772388892494084e-06	bf = 0.3139333373586834	zeta = 7.449627781743493e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 2 obj = 3.4532738111990233
eta = 0.9090909090909057
freqs = [3106837.34109525 5412726.23484139 3089168.32072194 1019314.7295142
 6232384.57566307 2969191.46464435 1279276.24484533 4129943.16102442
 2848016.20722416 2394904.72079761]
Done!
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.82431083e-08 1.16457981e-07 1.77499073e-08 6.70156624e-10
 1.78287614e-07 1.93072545e-08 1.32560365e-09 4.57965159e-08
 1.58168328e-08 1.01519565e-08]
ene_total = [0.03549463 0.00926953 0.03679125 0.03040907 0.00883363 0.00860486
 0.03032741 0.02679882 0.01960875 0.00829389]
At round 78 energy consumption: 0.21443183922587233
At round 78 eta: 0.9090909090909057
At round 78 a_n: 1.4640267926476263
At round 78 local rounds: 3.120939520577876
At round 78 global rounds: 16.104294719123292
gradient difference: 0.9515635371208191
train() client id: f_00000-0-0 loss: 0.809510  [   32/  126]
train() client id: f_00000-0-1 loss: 0.820539  [   64/  126]
train() client id: f_00000-0-2 loss: 1.089568  [   96/  126]
train() client id: f_00000-1-0 loss: 0.772819  [   32/  126]
train() client id: f_00000-1-1 loss: 1.067955  [   64/  126]
train() client id: f_00000-1-2 loss: 0.945636  [   96/  126]
train() client id: f_00000-2-0 loss: 0.864473  [   32/  126]
train() client id: f_00000-2-1 loss: 0.818521  [   64/  126]
train() client id: f_00000-2-2 loss: 1.007492  [   96/  126]
train() client id: f_00001-0-0 loss: 0.655914  [   32/  265]
train() client id: f_00001-0-1 loss: 0.530276  [   64/  265]
train() client id: f_00001-0-2 loss: 0.630168  [   96/  265]
train() client id: f_00001-0-3 loss: 0.704081  [  128/  265]
train() client id: f_00001-0-4 loss: 0.537520  [  160/  265]
train() client id: f_00001-0-5 loss: 0.574025  [  192/  265]
train() client id: f_00001-0-6 loss: 0.515306  [  224/  265]
train() client id: f_00001-0-7 loss: 0.561150  [  256/  265]
train() client id: f_00001-1-0 loss: 0.500949  [   32/  265]
train() client id: f_00001-1-1 loss: 0.735478  [   64/  265]
train() client id: f_00001-1-2 loss: 0.621030  [   96/  265]
train() client id: f_00001-1-3 loss: 0.537489  [  128/  265]
train() client id: f_00001-1-4 loss: 0.471511  [  160/  265]
train() client id: f_00001-1-5 loss: 0.636237  [  192/  265]
train() client id: f_00001-1-6 loss: 0.486474  [  224/  265]
train() client id: f_00001-1-7 loss: 0.673904  [  256/  265]
train() client id: f_00001-2-0 loss: 0.686821  [   32/  265]
train() client id: f_00001-2-1 loss: 0.511505  [   64/  265]
train() client id: f_00001-2-2 loss: 0.642748  [   96/  265]
train() client id: f_00001-2-3 loss: 0.566243  [  128/  265]
train() client id: f_00001-2-4 loss: 0.468807  [  160/  265]
train() client id: f_00001-2-5 loss: 0.522787  [  192/  265]
train() client id: f_00001-2-6 loss: 0.590088  [  224/  265]
train() client id: f_00001-2-7 loss: 0.676298  [  256/  265]
train() client id: f_00002-0-0 loss: 0.837199  [   32/  124]
train() client id: f_00002-0-1 loss: 1.089260  [   64/  124]
train() client id: f_00002-0-2 loss: 0.825861  [   96/  124]
train() client id: f_00002-1-0 loss: 0.905783  [   32/  124]
train() client id: f_00002-1-1 loss: 0.932966  [   64/  124]
train() client id: f_00002-1-2 loss: 0.790806  [   96/  124]
train() client id: f_00002-2-0 loss: 0.874078  [   32/  124]
train() client id: f_00002-2-1 loss: 0.913856  [   64/  124]
train() client id: f_00002-2-2 loss: 0.881559  [   96/  124]
train() client id: f_00003-0-0 loss: 0.423183  [   32/   43]
train() client id: f_00003-1-0 loss: 0.770478  [   32/   43]
train() client id: f_00003-2-0 loss: 0.497361  [   32/   43]
train() client id: f_00004-0-0 loss: 0.960030  [   32/  306]
train() client id: f_00004-0-1 loss: 0.894030  [   64/  306]
train() client id: f_00004-0-2 loss: 0.892058  [   96/  306]
train() client id: f_00004-0-3 loss: 0.899170  [  128/  306]
train() client id: f_00004-0-4 loss: 1.086838  [  160/  306]
train() client id: f_00004-0-5 loss: 0.800344  [  192/  306]
train() client id: f_00004-0-6 loss: 0.979324  [  224/  306]
train() client id: f_00004-0-7 loss: 0.891940  [  256/  306]
train() client id: f_00004-0-8 loss: 0.857524  [  288/  306]
train() client id: f_00004-1-0 loss: 0.759759  [   32/  306]
train() client id: f_00004-1-1 loss: 0.943336  [   64/  306]
train() client id: f_00004-1-2 loss: 0.937177  [   96/  306]
train() client id: f_00004-1-3 loss: 0.885150  [  128/  306]
train() client id: f_00004-1-4 loss: 1.017842  [  160/  306]
train() client id: f_00004-1-5 loss: 0.850492  [  192/  306]
train() client id: f_00004-1-6 loss: 0.853986  [  224/  306]
train() client id: f_00004-1-7 loss: 0.958339  [  256/  306]
train() client id: f_00004-1-8 loss: 1.042869  [  288/  306]
train() client id: f_00004-2-0 loss: 0.944369  [   32/  306]
train() client id: f_00004-2-1 loss: 0.849006  [   64/  306]
train() client id: f_00004-2-2 loss: 1.093828  [   96/  306]
train() client id: f_00004-2-3 loss: 0.837198  [  128/  306]
train() client id: f_00004-2-4 loss: 0.930883  [  160/  306]
train() client id: f_00004-2-5 loss: 0.758466  [  192/  306]
train() client id: f_00004-2-6 loss: 0.888643  [  224/  306]
train() client id: f_00004-2-7 loss: 1.031051  [  256/  306]
train() client id: f_00004-2-8 loss: 0.901909  [  288/  306]
train() client id: f_00005-0-0 loss: 0.309072  [   32/  146]
train() client id: f_00005-0-1 loss: 0.654865  [   64/  146]
train() client id: f_00005-0-2 loss: 0.207403  [   96/  146]
train() client id: f_00005-0-3 loss: 0.427894  [  128/  146]
train() client id: f_00005-1-0 loss: 0.401654  [   32/  146]
train() client id: f_00005-1-1 loss: 0.621420  [   64/  146]
train() client id: f_00005-1-2 loss: 0.641466  [   96/  146]
train() client id: f_00005-1-3 loss: 0.241467  [  128/  146]
train() client id: f_00005-2-0 loss: 0.396230  [   32/  146]
train() client id: f_00005-2-1 loss: 0.408959  [   64/  146]
train() client id: f_00005-2-2 loss: 0.421046  [   96/  146]
train() client id: f_00005-2-3 loss: 0.617954  [  128/  146]
train() client id: f_00006-0-0 loss: 0.468542  [   32/   54]
train() client id: f_00006-1-0 loss: 0.557601  [   32/   54]
train() client id: f_00006-2-0 loss: 0.568723  [   32/   54]
train() client id: f_00007-0-0 loss: 0.733395  [   32/  179]
train() client id: f_00007-0-1 loss: 0.767195  [   64/  179]
train() client id: f_00007-0-2 loss: 0.770058  [   96/  179]
train() client id: f_00007-0-3 loss: 0.897585  [  128/  179]
train() client id: f_00007-0-4 loss: 0.789671  [  160/  179]
train() client id: f_00007-1-0 loss: 1.002713  [   32/  179]
train() client id: f_00007-1-1 loss: 0.729039  [   64/  179]
train() client id: f_00007-1-2 loss: 0.675461  [   96/  179]
train() client id: f_00007-1-3 loss: 0.740292  [  128/  179]
train() client id: f_00007-1-4 loss: 0.704908  [  160/  179]
train() client id: f_00007-2-0 loss: 0.726749  [   32/  179]
train() client id: f_00007-2-1 loss: 0.703029  [   64/  179]
train() client id: f_00007-2-2 loss: 0.787773  [   96/  179]
train() client id: f_00007-2-3 loss: 0.805416  [  128/  179]
train() client id: f_00007-2-4 loss: 0.826803  [  160/  179]
train() client id: f_00008-0-0 loss: 0.744263  [   32/  130]
train() client id: f_00008-0-1 loss: 0.611336  [   64/  130]
train() client id: f_00008-0-2 loss: 0.700525  [   96/  130]
train() client id: f_00008-0-3 loss: 0.749567  [  128/  130]
train() client id: f_00008-1-0 loss: 0.632820  [   32/  130]
train() client id: f_00008-1-1 loss: 0.830761  [   64/  130]
train() client id: f_00008-1-2 loss: 0.665946  [   96/  130]
train() client id: f_00008-1-3 loss: 0.717047  [  128/  130]
train() client id: f_00008-2-0 loss: 0.759985  [   32/  130]
train() client id: f_00008-2-1 loss: 0.794546  [   64/  130]
train() client id: f_00008-2-2 loss: 0.580065  [   96/  130]
train() client id: f_00008-2-3 loss: 0.697066  [  128/  130]
train() client id: f_00009-0-0 loss: 0.594730  [   32/  118]
train() client id: f_00009-0-1 loss: 0.864767  [   64/  118]
train() client id: f_00009-0-2 loss: 0.649352  [   96/  118]
train() client id: f_00009-1-0 loss: 0.767452  [   32/  118]
train() client id: f_00009-1-1 loss: 0.631997  [   64/  118]
train() client id: f_00009-1-2 loss: 0.628374  [   96/  118]
train() client id: f_00009-2-0 loss: 0.834503  [   32/  118]
train() client id: f_00009-2-1 loss: 0.693300  [   64/  118]
train() client id: f_00009-2-2 loss: 0.553347  [   96/  118]
At round 78 accuracy: 0.6445623342175066
At round 78 training accuracy: 0.5928906773977196
At round 78 training loss: 0.8160827767541151
update_location
xs = [  -3.9056584     4.20031788  410.00902392   18.81129433    0.97929623
    3.95640986 -372.44319194 -351.32485185  394.66397685 -337.06087855]
ys = [ 402.5879595   385.55583871    1.32061395 -372.45517586  364.35018685
  347.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [414.84011293 398.33521992 422.02979008 386.10454908 377.82537987
 361.92587174 385.64338677 365.28047824 407.51481519 351.60495973]
dists_bs = [284.72715542 276.48962935 610.24592354 580.74950485 258.49991481
 248.77726449 265.58322443 247.84734786 591.0173977  235.59333944]
uav_gains = [6.42087105e-13 7.34190954e-13 6.08078996e-13 8.18868515e-13
 8.86722082e-13 1.04941536e-12 8.22401288e-13 1.01086288e-12
 6.80300468e-13 1.18594342e-12]
bs_gains = [1.48215354e-11 1.60913809e-11 1.75339758e-12 2.01430537e-12
 1.94269648e-11 2.16283883e-11 1.80107706e-11 2.18563739e-11
 1.91784428e-12 2.51905507e-11]
Round 79
-------------------------------
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.47167495 0.91597325 0.46632083 0.18231339 1.05554823 0.50864652
 0.2198029  0.64284551 0.46659513 0.41263047]
obj_prev = 5.342351171013073
eta_min = 2.6352711032412936e-201	eta_max = 0.9959613407486563
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 1.2045873201186579	eta = 0.909090909090909
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 3.853002462740199	eta = 0.28421455540603213
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.268192123899988	eta = 0.48279833546161194
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0171814832442236	eta = 0.5428759836545998
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0013314548574064	eta = 0.5471754212767701
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0012564070127876	eta = 0.5471959405544855
eta = 0.5471959405544855
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.04975385 0.10464104 0.04896411 0.01697949 0.12083078 0.05765129
 0.02132308 0.07068206 0.05133334 0.04659488]
ene_total = [0.21330398 0.28236012 0.21458127 0.11946231 0.32136361 0.16430363
 0.13001973 0.24348682 0.17623076 0.13614417]
ti_comp = [10.15400802 10.42406192 10.14095348 10.20515528 10.42848067 10.43082397
 10.20596501 10.24142523 10.31948805 10.43395522]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [7.46595073e-08 6.59040563e-07 7.13437549e-08 2.93774855e-09
 1.01384295e-06 1.10070217e-07 5.81730210e-09 2.10419888e-07
 7.93893490e-08 5.80759835e-08]
ene_total = [0.09007648 0.02319242 0.09330975 0.07740842 0.02209888 0.02151627
 0.07720788 0.06842579 0.04909128 0.02074061]
optimize_network iter = 0 obj = 0.5430677938747402
eta = 0.5471959405544855
freqs = [2449961.18756652 5019206.37355413 2414176.71155869  831907.42340227
 5793307.05286581 2763505.9294842  1044638.0630291  3450792.08881117
 2487203.73595765 2232848.20767479]
eta_min = 0.5471959405544858	eta_max = 0.9355133020148048
af = 4.251199729612188e-06	bf = 0.24590073836681936	zeta = 4.6763197025734075e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [1.18016929e-08 1.04176878e-07 1.12775602e-08 4.64380326e-10
 1.60261749e-07 1.73991893e-08 9.19561561e-10 3.32618174e-08
 1.25493558e-08 9.18027656e-09]
ene_total = [0.44865618 0.11551099 0.46476064 0.3855593  0.11006058 0.10716806
 0.3845604  0.34081604 0.24451523 0.10330516]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 1 obj = 2.7049111966856447
eta = 0.909090909090909
freqs = [2249723.37868762 4098369.93259797 2230673.32764145  745935.59638761
 4722117.45608519 2250428.94068191  936334.90519755 3043809.99590222
 2120401.76543593 1816031.36876833]
eta_min = 0.9090909090910358	eta_max = 0.909090909090905
af = 2.9326740829202082e-06	bf = 0.24590073836681936	zeta = 3.2259414912122293e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 2 obj = 2.7049111966855257
eta = 0.909090909090905
freqs = [2249723.37868761 4098369.93259797 2230673.32764143  745935.59638761
 4722117.4560852  2250428.94068191  936334.90519755 3043809.99590221
 2120401.76543593 1816031.36876833]
Done!
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.56577248e-09 6.67666285e-08 9.25518050e-09 3.58890845e-10
 1.02349625e-07 1.10911026e-08 7.10145674e-10 2.48759324e-08
 8.76740211e-09 5.83740678e-09]
ene_total = [0.03636881 0.00936347 0.03767426 0.03125407 0.00892163 0.00868721
 0.0311731  0.0276271  0.0198208  0.00837408]
At round 79 energy consumption: 0.2192645430104472
At round 79 eta: 0.909090909090905
At round 79 a_n: 1.1214809456783108
At round 79 local rounds: 3.120939520577902
At round 79 global rounds: 12.336290402460858
gradient difference: 0.7681617140769958
train() client id: f_00000-0-0 loss: 1.073436  [   32/  126]
train() client id: f_00000-0-1 loss: 0.724372  [   64/  126]
train() client id: f_00000-0-2 loss: 0.822948  [   96/  126]
train() client id: f_00000-1-0 loss: 0.950919  [   32/  126]
train() client id: f_00000-1-1 loss: 0.767703  [   64/  126]
train() client id: f_00000-1-2 loss: 0.830088  [   96/  126]
train() client id: f_00000-2-0 loss: 0.736335  [   32/  126]
train() client id: f_00000-2-1 loss: 0.886629  [   64/  126]
train() client id: f_00000-2-2 loss: 0.801759  [   96/  126]
train() client id: f_00001-0-0 loss: 0.470468  [   32/  265]
train() client id: f_00001-0-1 loss: 0.576321  [   64/  265]
train() client id: f_00001-0-2 loss: 0.593302  [   96/  265]
train() client id: f_00001-0-3 loss: 0.458753  [  128/  265]
train() client id: f_00001-0-4 loss: 0.409455  [  160/  265]
train() client id: f_00001-0-5 loss: 0.393035  [  192/  265]
train() client id: f_00001-0-6 loss: 0.472130  [  224/  265]
train() client id: f_00001-0-7 loss: 0.573217  [  256/  265]
train() client id: f_00001-1-0 loss: 0.559871  [   32/  265]
train() client id: f_00001-1-1 loss: 0.528945  [   64/  265]
train() client id: f_00001-1-2 loss: 0.495675  [   96/  265]
train() client id: f_00001-1-3 loss: 0.409834  [  128/  265]
train() client id: f_00001-1-4 loss: 0.469277  [  160/  265]
train() client id: f_00001-1-5 loss: 0.532113  [  192/  265]
train() client id: f_00001-1-6 loss: 0.466010  [  224/  265]
train() client id: f_00001-1-7 loss: 0.510038  [  256/  265]
train() client id: f_00001-2-0 loss: 0.462051  [   32/  265]
train() client id: f_00001-2-1 loss: 0.473849  [   64/  265]
train() client id: f_00001-2-2 loss: 0.493579  [   96/  265]
train() client id: f_00001-2-3 loss: 0.584011  [  128/  265]
train() client id: f_00001-2-4 loss: 0.440548  [  160/  265]
train() client id: f_00001-2-5 loss: 0.492950  [  192/  265]
train() client id: f_00001-2-6 loss: 0.447264  [  224/  265]
train() client id: f_00001-2-7 loss: 0.545051  [  256/  265]
train() client id: f_00002-0-0 loss: 0.986766  [   32/  124]
train() client id: f_00002-0-1 loss: 1.081800  [   64/  124]
train() client id: f_00002-0-2 loss: 0.762428  [   96/  124]
train() client id: f_00002-1-0 loss: 0.786668  [   32/  124]
train() client id: f_00002-1-1 loss: 1.003930  [   64/  124]
train() client id: f_00002-1-2 loss: 0.878920  [   96/  124]
train() client id: f_00002-2-0 loss: 0.922312  [   32/  124]
train() client id: f_00002-2-1 loss: 1.060412  [   64/  124]
train() client id: f_00002-2-2 loss: 0.764978  [   96/  124]
train() client id: f_00003-0-0 loss: 0.584364  [   32/   43]
train() client id: f_00003-1-0 loss: 0.678869  [   32/   43]
train() client id: f_00003-2-0 loss: 0.622116  [   32/   43]
train() client id: f_00004-0-0 loss: 0.883978  [   32/  306]
train() client id: f_00004-0-1 loss: 0.780619  [   64/  306]
train() client id: f_00004-0-2 loss: 0.872033  [   96/  306]
train() client id: f_00004-0-3 loss: 0.950650  [  128/  306]
train() client id: f_00004-0-4 loss: 0.744892  [  160/  306]
train() client id: f_00004-0-5 loss: 0.745983  [  192/  306]
train() client id: f_00004-0-6 loss: 0.876184  [  224/  306]
train() client id: f_00004-0-7 loss: 0.835453  [  256/  306]
train() client id: f_00004-0-8 loss: 0.819426  [  288/  306]
train() client id: f_00004-1-0 loss: 0.653173  [   32/  306]
train() client id: f_00004-1-1 loss: 0.823174  [   64/  306]
train() client id: f_00004-1-2 loss: 0.876353  [   96/  306]
train() client id: f_00004-1-3 loss: 0.927810  [  128/  306]
train() client id: f_00004-1-4 loss: 0.765255  [  160/  306]
train() client id: f_00004-1-5 loss: 0.970553  [  192/  306]
train() client id: f_00004-1-6 loss: 0.818171  [  224/  306]
train() client id: f_00004-1-7 loss: 0.835790  [  256/  306]
train() client id: f_00004-1-8 loss: 0.863027  [  288/  306]
train() client id: f_00004-2-0 loss: 0.840597  [   32/  306]
train() client id: f_00004-2-1 loss: 0.761925  [   64/  306]
train() client id: f_00004-2-2 loss: 0.966042  [   96/  306]
train() client id: f_00004-2-3 loss: 0.734926  [  128/  306]
train() client id: f_00004-2-4 loss: 0.844994  [  160/  306]
train() client id: f_00004-2-5 loss: 0.902280  [  192/  306]
train() client id: f_00004-2-6 loss: 0.766387  [  224/  306]
train() client id: f_00004-2-7 loss: 0.811908  [  256/  306]
train() client id: f_00004-2-8 loss: 0.766296  [  288/  306]
train() client id: f_00005-0-0 loss: 0.580834  [   32/  146]
train() client id: f_00005-0-1 loss: 0.384831  [   64/  146]
train() client id: f_00005-0-2 loss: 0.711429  [   96/  146]
train() client id: f_00005-0-3 loss: 0.432814  [  128/  146]
train() client id: f_00005-1-0 loss: 0.489349  [   32/  146]
train() client id: f_00005-1-1 loss: 0.552775  [   64/  146]
train() client id: f_00005-1-2 loss: 0.622630  [   96/  146]
train() client id: f_00005-1-3 loss: 0.525752  [  128/  146]
train() client id: f_00005-2-0 loss: 0.480928  [   32/  146]
train() client id: f_00005-2-1 loss: 0.455861  [   64/  146]
train() client id: f_00005-2-2 loss: 0.495667  [   96/  146]
train() client id: f_00005-2-3 loss: 0.546712  [  128/  146]
train() client id: f_00006-0-0 loss: 0.380012  [   32/   54]
train() client id: f_00006-1-0 loss: 0.481525  [   32/   54]
train() client id: f_00006-2-0 loss: 0.498150  [   32/   54]
train() client id: f_00007-0-0 loss: 0.858929  [   32/  179]
train() client id: f_00007-0-1 loss: 0.671128  [   64/  179]
train() client id: f_00007-0-2 loss: 0.588554  [   96/  179]
train() client id: f_00007-0-3 loss: 0.540939  [  128/  179]
train() client id: f_00007-0-4 loss: 0.603139  [  160/  179]
train() client id: f_00007-1-0 loss: 0.696224  [   32/  179]
train() client id: f_00007-1-1 loss: 0.648989  [   64/  179]
train() client id: f_00007-1-2 loss: 0.733016  [   96/  179]
train() client id: f_00007-1-3 loss: 0.641301  [  128/  179]
train() client id: f_00007-1-4 loss: 0.461564  [  160/  179]
train() client id: f_00007-2-0 loss: 0.586524  [   32/  179]
train() client id: f_00007-2-1 loss: 0.582359  [   64/  179]
train() client id: f_00007-2-2 loss: 0.492049  [   96/  179]
train() client id: f_00007-2-3 loss: 0.779867  [  128/  179]
train() client id: f_00007-2-4 loss: 0.627597  [  160/  179]
train() client id: f_00008-0-0 loss: 0.733231  [   32/  130]
train() client id: f_00008-0-1 loss: 0.724726  [   64/  130]
train() client id: f_00008-0-2 loss: 0.713151  [   96/  130]
train() client id: f_00008-0-3 loss: 0.817951  [  128/  130]
train() client id: f_00008-1-0 loss: 0.698021  [   32/  130]
train() client id: f_00008-1-1 loss: 0.745980  [   64/  130]
train() client id: f_00008-1-2 loss: 0.640132  [   96/  130]
train() client id: f_00008-1-3 loss: 0.897864  [  128/  130]
train() client id: f_00008-2-0 loss: 0.764917  [   32/  130]
train() client id: f_00008-2-1 loss: 0.722951  [   64/  130]
train() client id: f_00008-2-2 loss: 0.805484  [   96/  130]
train() client id: f_00008-2-3 loss: 0.693315  [  128/  130]
train() client id: f_00009-0-0 loss: 0.956427  [   32/  118]
train() client id: f_00009-0-1 loss: 0.947038  [   64/  118]
train() client id: f_00009-0-2 loss: 0.960746  [   96/  118]
train() client id: f_00009-1-0 loss: 0.846679  [   32/  118]
train() client id: f_00009-1-1 loss: 1.029159  [   64/  118]
train() client id: f_00009-1-2 loss: 0.920355  [   96/  118]
train() client id: f_00009-2-0 loss: 0.903373  [   32/  118]
train() client id: f_00009-2-1 loss: 0.993949  [   64/  118]
train() client id: f_00009-2-2 loss: 0.938188  [   96/  118]
At round 79 accuracy: 0.6445623342175066
At round 79 training accuracy: 0.5868544600938967
At round 79 training loss: 0.8263409711711198
update_location
xs = [  -3.9056584     4.20031788  415.00902392   18.81129433    0.97929623
    3.95640986 -377.44319194 -356.32485185  399.66397685 -342.06087855]
ys = [ 407.5879595   390.55583871    1.32061395 -377.45517586  369.35018685
  352.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [419.69417305 403.17676746 426.88901831 390.93001238 382.6493428
 366.73352471 390.47439568 370.09198357 412.35902362 356.40097712]
dists_bs = [288.73938531 280.31602439 615.04079331 585.46523308 262.18449197
 252.27617572 269.32114847 251.42902847 595.83823656 239.06030698]
uav_gains = [6.18777499e-13 7.04863808e-13 5.86815526e-13 7.83470858e-13
 8.46017284e-13 9.94934833e-13 7.86695341e-13 9.59778197e-13
 6.54600040e-13 1.11885798e-12]
bs_gains = [1.42520458e-11 1.54838835e-11 1.71539091e-12 1.96920521e-12
 1.86721568e-11 2.07989120e-11 1.73195580e-11 2.09957270e-11
 1.87471239e-12 2.41809390e-11]
Round 80
-------------------------------
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.32829867 0.63627291 0.3245873  0.12729558 0.73321141 0.35335129
 0.15333479 0.44714953 0.32424655 0.28666076]
obj_prev = 3.7144087846743608
eta_min = 2.74115185868049e-289	eta_max = 0.9972962516128097
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 0.8366574097544858	eta = 0.9090909090909091
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 2.7178337538132036	eta = 0.2798543671643672
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.5874476546409413	eta = 0.4791324255686323
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.4095987989406396	eta = 0.539584487304448
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.398385343651186	eta = 0.5439113393776205
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.39833229498431	eta = 0.5439319737944585
eta = 0.5439319737944585
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.05024753 0.10567934 0.04944995 0.01714797 0.12202973 0.05822333
 0.02153466 0.0713834  0.05184269 0.04705721]
ene_total = [0.14943821 0.1966487  0.15032166 0.08411788 0.22380943 0.11442236
 0.09147288 0.17052575 0.12276251 0.0948129 ]
ti_comp = [14.85036606 15.12826574 14.83721764 14.90182223 15.13274303 15.13514254
 14.90262547 14.93825677 15.02250443 15.13829514]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [3.59542268e-08 3.22308656e-07 3.43299174e-08 1.41918577e-09
 4.95954233e-07 5.38515457e-08 2.81040133e-09 1.01876001e-07
 3.85885102e-08 2.84187337e-08]
ene_total = [0.06361869 0.01615569 0.06586436 0.05483025 0.01539129 0.01498072
 0.05469306 0.04860763 0.03421856 0.01444223]
optimize_network iter = 0 obj = 0.38280246883118135
eta = 0.5439319737944585
freqs = [1691794.45393543 3492777.65574166 1666416.00768811  575364.79907189
 4031976.3853505  1923448.44978197  722512.19806255 2389281.50597988
 1725501.02801081 1554244.18120876]
eta_min = 0.5439319737944591	eta_max = 0.9542982433752891
af = 1.4269447035111872e-06	bf = 0.17458309747545425	zeta = 1.569639173862306e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [5.62757246e-09 5.04479022e-08 5.37333478e-09 2.22131623e-10
 7.76269894e-08 8.42886922e-09 4.39885336e-10 1.59456795e-08
 6.03989174e-09 4.44811354e-09]
ene_total = [0.3191587  0.08104669 0.33042464 0.27506954 0.07721065 0.07515409
 0.2743813  0.24385151 0.17166562 0.07245282]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 1 obj = 1.9204150651233343
eta = 0.9090909090909091
freqs = [1477274.39950027 2813265.88968522 1461042.34829177  494588.56272364
 3243585.97051685 1546333.09614372  620927.40517717 2031588.97688467
 1431594.99214584 1248440.38657298]
eta_min = 0.9090909090909186	eta_max = 0.9090909090909067
af = 9.470450164325744e-07	bf = 0.17458309747545425	zeta = 1.0417495180758319e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 2 obj = 1.9204150651232854
eta = 0.9090909090909067
freqs = [1477274.39950027 2813265.88968522 1461042.34829176  494588.56272364
 3243585.97051684 1546333.09614371  620927.40517717 2031588.97688466
 1431594.99214584 1248440.38657297]
Done!
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.12462194e-09 3.14599982e-08 3.97043922e-09 1.57778511e-10
 4.82907013e-08 5.23660984e-09 3.12296182e-10 1.10819449e-08
 3.99645523e-09 2.75872802e-09]
ene_total = [0.03724884 0.0094589  0.03856368 0.03210322 0.00901118 0.00877119
 0.03202289 0.02845977 0.020035   0.00845593]
At round 80 energy consumption: 0.22413059127783166
At round 80 eta: 0.9090909090909067
At round 80 a_n: 0.7789350987089918
At round 80 local rounds: 3.120939520577836
At round 80 global rounds: 8.568286085798688
gradient difference: 0.8866745829582214
train() client id: f_00000-0-0 loss: 1.184516  [   32/  126]
train() client id: f_00000-0-1 loss: 0.824650  [   64/  126]
train() client id: f_00000-0-2 loss: 0.966789  [   96/  126]
train() client id: f_00000-1-0 loss: 0.839433  [   32/  126]
train() client id: f_00000-1-1 loss: 1.039876  [   64/  126]
train() client id: f_00000-1-2 loss: 0.994988  [   96/  126]
train() client id: f_00000-2-0 loss: 0.928290  [   32/  126]
train() client id: f_00000-2-1 loss: 0.851333  [   64/  126]
train() client id: f_00000-2-2 loss: 0.947350  [   96/  126]
train() client id: f_00001-0-0 loss: 0.505665  [   32/  265]
train() client id: f_00001-0-1 loss: 0.566065  [   64/  265]
train() client id: f_00001-0-2 loss: 0.453558  [   96/  265]
train() client id: f_00001-0-3 loss: 0.613966  [  128/  265]
train() client id: f_00001-0-4 loss: 0.502425  [  160/  265]
train() client id: f_00001-0-5 loss: 0.548209  [  192/  265]
train() client id: f_00001-0-6 loss: 0.505334  [  224/  265]
train() client id: f_00001-0-7 loss: 0.495918  [  256/  265]
train() client id: f_00001-1-0 loss: 0.424660  [   32/  265]
train() client id: f_00001-1-1 loss: 0.481900  [   64/  265]
train() client id: f_00001-1-2 loss: 0.480895  [   96/  265]
train() client id: f_00001-1-3 loss: 0.531262  [  128/  265]
train() client id: f_00001-1-4 loss: 0.571449  [  160/  265]
train() client id: f_00001-1-5 loss: 0.529682  [  192/  265]
train() client id: f_00001-1-6 loss: 0.557118  [  224/  265]
train() client id: f_00001-1-7 loss: 0.576185  [  256/  265]
train() client id: f_00001-2-0 loss: 0.625002  [   32/  265]
train() client id: f_00001-2-1 loss: 0.465676  [   64/  265]
train() client id: f_00001-2-2 loss: 0.505992  [   96/  265]
train() client id: f_00001-2-3 loss: 0.626505  [  128/  265]
train() client id: f_00001-2-4 loss: 0.501219  [  160/  265]
train() client id: f_00001-2-5 loss: 0.587594  [  192/  265]
train() client id: f_00001-2-6 loss: 0.422794  [  224/  265]
train() client id: f_00001-2-7 loss: 0.434491  [  256/  265]
train() client id: f_00002-0-0 loss: 1.104406  [   32/  124]
train() client id: f_00002-0-1 loss: 1.160809  [   64/  124]
train() client id: f_00002-0-2 loss: 1.025502  [   96/  124]
train() client id: f_00002-1-0 loss: 0.931708  [   32/  124]
train() client id: f_00002-1-1 loss: 1.107579  [   64/  124]
train() client id: f_00002-1-2 loss: 1.238157  [   96/  124]
train() client id: f_00002-2-0 loss: 1.228511  [   32/  124]
train() client id: f_00002-2-1 loss: 1.121170  [   64/  124]
train() client id: f_00002-2-2 loss: 0.921194  [   96/  124]
train() client id: f_00003-0-0 loss: 0.694430  [   32/   43]
train() client id: f_00003-1-0 loss: 0.581598  [   32/   43]
train() client id: f_00003-2-0 loss: 0.519574  [   32/   43]
train() client id: f_00004-0-0 loss: 1.044938  [   32/  306]
train() client id: f_00004-0-1 loss: 0.972322  [   64/  306]
train() client id: f_00004-0-2 loss: 0.844596  [   96/  306]
train() client id: f_00004-0-3 loss: 0.857757  [  128/  306]
train() client id: f_00004-0-4 loss: 0.833688  [  160/  306]
train() client id: f_00004-0-5 loss: 0.889702  [  192/  306]
train() client id: f_00004-0-6 loss: 1.138964  [  224/  306]
train() client id: f_00004-0-7 loss: 0.875483  [  256/  306]
train() client id: f_00004-0-8 loss: 1.170707  [  288/  306]
train() client id: f_00004-1-0 loss: 0.915577  [   32/  306]
train() client id: f_00004-1-1 loss: 0.823996  [   64/  306]
train() client id: f_00004-1-2 loss: 0.852375  [   96/  306]
train() client id: f_00004-1-3 loss: 0.995914  [  128/  306]
train() client id: f_00004-1-4 loss: 1.044056  [  160/  306]
train() client id: f_00004-1-5 loss: 0.904305  [  192/  306]
train() client id: f_00004-1-6 loss: 0.966738  [  224/  306]
train() client id: f_00004-1-7 loss: 0.913928  [  256/  306]
train() client id: f_00004-1-8 loss: 1.031053  [  288/  306]
train() client id: f_00004-2-0 loss: 0.931578  [   32/  306]
train() client id: f_00004-2-1 loss: 0.930246  [   64/  306]
train() client id: f_00004-2-2 loss: 1.015143  [   96/  306]
train() client id: f_00004-2-3 loss: 0.906092  [  128/  306]
train() client id: f_00004-2-4 loss: 0.960200  [  160/  306]
train() client id: f_00004-2-5 loss: 0.859828  [  192/  306]
train() client id: f_00004-2-6 loss: 0.957025  [  224/  306]
train() client id: f_00004-2-7 loss: 1.058823  [  256/  306]
train() client id: f_00004-2-8 loss: 0.759022  [  288/  306]
train() client id: f_00005-0-0 loss: 0.676802  [   32/  146]
train() client id: f_00005-0-1 loss: 0.557066  [   64/  146]
train() client id: f_00005-0-2 loss: 0.579716  [   96/  146]
train() client id: f_00005-0-3 loss: 0.531397  [  128/  146]
train() client id: f_00005-1-0 loss: 0.774817  [   32/  146]
train() client id: f_00005-1-1 loss: 0.690051  [   64/  146]
train() client id: f_00005-1-2 loss: 0.574280  [   96/  146]
train() client id: f_00005-1-3 loss: 0.251996  [  128/  146]
train() client id: f_00005-2-0 loss: 0.603543  [   32/  146]
train() client id: f_00005-2-1 loss: 0.436901  [   64/  146]
train() client id: f_00005-2-2 loss: 0.592107  [   96/  146]
train() client id: f_00005-2-3 loss: 0.541962  [  128/  146]
train() client id: f_00006-0-0 loss: 0.390458  [   32/   54]
train() client id: f_00006-1-0 loss: 0.420945  [   32/   54]
train() client id: f_00006-2-0 loss: 0.427590  [   32/   54]
train() client id: f_00007-0-0 loss: 0.487212  [   32/  179]
train() client id: f_00007-0-1 loss: 0.640793  [   64/  179]
train() client id: f_00007-0-2 loss: 0.643140  [   96/  179]
train() client id: f_00007-0-3 loss: 0.571916  [  128/  179]
train() client id: f_00007-0-4 loss: 0.750237  [  160/  179]
train() client id: f_00007-1-0 loss: 0.622702  [   32/  179]
train() client id: f_00007-1-1 loss: 0.798536  [   64/  179]
train() client id: f_00007-1-2 loss: 0.556027  [   96/  179]
train() client id: f_00007-1-3 loss: 0.532387  [  128/  179]
train() client id: f_00007-1-4 loss: 0.690912  [  160/  179]
train() client id: f_00007-2-0 loss: 0.605325  [   32/  179]
train() client id: f_00007-2-1 loss: 0.672749  [   64/  179]
train() client id: f_00007-2-2 loss: 0.632036  [   96/  179]
train() client id: f_00007-2-3 loss: 0.538927  [  128/  179]
train() client id: f_00007-2-4 loss: 0.649211  [  160/  179]
train() client id: f_00008-0-0 loss: 0.881706  [   32/  130]
train() client id: f_00008-0-1 loss: 0.695598  [   64/  130]
train() client id: f_00008-0-2 loss: 0.927306  [   96/  130]
train() client id: f_00008-0-3 loss: 0.656940  [  128/  130]
train() client id: f_00008-1-0 loss: 0.733393  [   32/  130]
train() client id: f_00008-1-1 loss: 0.813734  [   64/  130]
train() client id: f_00008-1-2 loss: 0.765151  [   96/  130]
train() client id: f_00008-1-3 loss: 0.860832  [  128/  130]
train() client id: f_00008-2-0 loss: 0.660475  [   32/  130]
train() client id: f_00008-2-1 loss: 0.842194  [   64/  130]
train() client id: f_00008-2-2 loss: 0.782173  [   96/  130]
train() client id: f_00008-2-3 loss: 0.889988  [  128/  130]
train() client id: f_00009-0-0 loss: 0.562440  [   32/  118]
train() client id: f_00009-0-1 loss: 0.787032  [   64/  118]
train() client id: f_00009-0-2 loss: 0.892759  [   96/  118]
train() client id: f_00009-1-0 loss: 0.671258  [   32/  118]
train() client id: f_00009-1-1 loss: 0.801618  [   64/  118]
train() client id: f_00009-1-2 loss: 0.681469  [   96/  118]
train() client id: f_00009-2-0 loss: 0.591051  [   32/  118]
train() client id: f_00009-2-1 loss: 0.842722  [   64/  118]
train() client id: f_00009-2-2 loss: 0.792614  [   96/  118]
At round 80 accuracy: 0.6445623342175066
At round 80 training accuracy: 0.5922199865861838
At round 80 training loss: 0.8125000454881846
update_location
xs = [  -3.9056584     4.20031788  420.00902392   18.81129433    0.97929623
    3.95640986 -382.44319194 -361.32485185  404.66397685 -347.06087855]
ys = [ 412.5879595   395.55583871    1.32061395 -382.45517586  374.35018685
  357.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [424.55162052 408.02213691 431.75146113 395.7598089  387.47776893
 371.54625506 395.30960727 374.90842191 417.20690806 361.20252668]
dists_bs = [292.78202169 284.1788731  619.83890462 590.18564105 265.91203375
 255.82496036 273.09945612 255.05843424 600.66200473 242.58078894]
uav_gains = [5.96881287e-13 6.77526367e-13 5.66782867e-13 7.50688319e-13
 8.08508001e-13 9.45234363e-13 7.53640053e-13 9.13074203e-13
 6.30534284e-13 1.05807042e-12]
bs_gains = [1.37078628e-11 1.49017434e-11 1.67846913e-12 1.92542185e-12
 1.79484814e-11 2.00011019e-11 1.66569601e-11 2.01698633e-11
 1.83286144e-12 2.32111230e-11]
Round 81
-------------------------------
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.18431614 0.35650715 0.18224127 0.07169206 0.41081325 0.19799863
 0.08628049 0.25087914 0.1817507  0.16063512]
obj_prev = 2.0831139657376503
eta_min = 0.0	eta_max = inf
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.4687274993903175	eta = 0.909090909090909
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 1.5461570538938256	eta = 0.2755967820109389
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.8960917211906734	eta = 0.47552711230325256
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7944965020417399	eta = 0.5363345306638816
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7881017357421507	eta = 0.5406864230991465
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7880715228657817	eta = 0.54070715179138
eta = 0.54070715179138
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.05073821 0.10671132 0.04993285 0.01731542 0.12322138 0.0587919
 0.02174495 0.07208048 0.05234895 0.04751674]
ene_total = [0.08444145 0.1104692  0.08493497 0.04776518 0.12572503 0.06427522
 0.05189808 0.09632167 0.06897994 0.05326078]
ti_comp = [26.93025964 27.2160528  26.91701177 26.98204373 27.22058811 27.22304297
 26.98284085 27.01864488 27.10909704 27.22621665]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.12565281e-08 1.02532430e-07 1.07395299e-08 4.45686613e-10
 1.57813146e-07 1.71379604e-08 8.82632352e-10 3.20631268e-08
 1.22003941e-08 9.04576573e-09]
ene_total = [0.03623342 0.00907936 0.03749214 0.03131323 0.00864849 0.00841512
 0.03123749 0.02783567 0.01924149 0.00811357]
optimize_network iter = 0 obj = 0.21760996983692812
eta = 0.54070715179138
freqs = [ 942029.79379861 1960448.1887988   927533.22358696  320869.35975109
 2263385.64084244 1079818.64242409  402940.3153214  1333902.54699206
  965523.69021721  872628.40406785]
eta_min = 0.5407071517913803	eta_max = 0.9738786232505544
af = 2.5132315156122307e-07	bf = 0.0999465483120546	zeta = 2.764554667173454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.74483827e-09 1.58932227e-08 1.66470005e-09 6.90844506e-11
 2.44621090e-08 2.65650021e-09 1.36814006e-10 4.97000232e-09
 1.89114391e-09 1.40215510e-09]
ene_total = [0.1830592  0.0458705  0.18941855 0.15820137 0.04369346 0.04251496
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 1 obj = 1.0994121997748156
eta = 0.909090909090909
freqs = [ 782640.11306705 1557441.83511423  772253.41470056  264366.82466173
 1796869.89992426  856934.27544998  331943.42290144 1092625.69941991
  779732.80435494  692177.58267536]
eta_min = 0.9090909090909166	eta_max = 0.9090909090908771
af = 1.605687647826223e-07	bf = 0.0999465483120546	zeta = 1.7662564126088454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 2 obj = 1.09941219977443
eta = 0.9090909090908771
freqs = [ 782640.11306704 1557441.83511425  772253.41470055  264366.82466173
 1796869.89992429  856934.27544999  331943.42290144 1092625.69941991
  779732.80435495  692177.58267537]
Done!
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.15767328e-09 9.64185965e-09 1.10925813e-09 4.50789326e-11
 1.48199232e-08 1.60819661e-09 8.92510132e-11 3.20543602e-09
 1.18556733e-09 8.48024357e-10]
ene_total = [0.03813509 0.00955579 0.03945988 0.03295668 0.00910226 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
At round 81 energy consumption: 0.22903076238567763
At round 81 eta: 0.9090909090908771
At round 81 a_n: 0.43638925173967635
At round 81 local rounds: 3.120939520578907
At round 81 global rounds: 4.80028176913475
gradient difference: 0.8854110836982727
train() client id: f_00000-0-0 loss: 0.898419  [   32/  126]
train() client id: f_00000-0-1 loss: 0.933701  [   64/  126]
train() client id: f_00000-0-2 loss: 0.915904  [   96/  126]
train() client id: f_00000-1-0 loss: 0.966255  [   32/  126]
train() client id: f_00000-1-1 loss: 0.904605  [   64/  126]
train() client id: f_00000-1-2 loss: 0.931359  [   96/  126]
train() client id: f_00000-2-0 loss: 0.924660  [   32/  126]
train() client id: f_00000-2-1 loss: 0.828818  [   64/  126]
train() client id: f_00000-2-2 loss: 1.080708  [   96/  126]
train() client id: f_00001-0-0 loss: 0.738098  [   32/  265]
train() client id: f_00001-0-1 loss: 0.630835  [   64/  265]
train() client id: f_00001-0-2 loss: 0.577253  [   96/  265]
train() client id: f_00001-0-3 loss: 0.548100  [  128/  265]
train() client id: f_00001-0-4 loss: 0.555047  [  160/  265]
train() client id: f_00001-0-5 loss: 0.540176  [  192/  265]
train() client id: f_00001-0-6 loss: 0.518770  [  224/  265]
train() client id: f_00001-0-7 loss: 0.707837  [  256/  265]
train() client id: f_00001-1-0 loss: 0.532155  [   32/  265]
train() client id: f_00001-1-1 loss: 0.602073  [   64/  265]
train() client id: f_00001-1-2 loss: 0.568559  [   96/  265]
train() client id: f_00001-1-3 loss: 0.691308  [  128/  265]
train() client id: f_00001-1-4 loss: 0.602656  [  160/  265]
train() client id: f_00001-1-5 loss: 0.669912  [  192/  265]
train() client id: f_00001-1-6 loss: 0.579301  [  224/  265]
train() client id: f_00001-1-7 loss: 0.626354  [  256/  265]
train() client id: f_00001-2-0 loss: 0.536876  [   32/  265]
train() client id: f_00001-2-1 loss: 0.594181  [   64/  265]
train() client id: f_00001-2-2 loss: 0.739794  [   96/  265]
train() client id: f_00001-2-3 loss: 0.698742  [  128/  265]
train() client id: f_00001-2-4 loss: 0.520953  [  160/  265]
train() client id: f_00001-2-5 loss: 0.618213  [  192/  265]
train() client id: f_00001-2-6 loss: 0.571400  [  224/  265]
train() client id: f_00001-2-7 loss: 0.593691  [  256/  265]
train() client id: f_00002-0-0 loss: 0.993186  [   32/  124]
train() client id: f_00002-0-1 loss: 1.235615  [   64/  124]
train() client id: f_00002-0-2 loss: 1.086788  [   96/  124]
train() client id: f_00002-1-0 loss: 1.150974  [   32/  124]
train() client id: f_00002-1-1 loss: 1.063745  [   64/  124]
train() client id: f_00002-1-2 loss: 1.126552  [   96/  124]
train() client id: f_00002-2-0 loss: 1.106624  [   32/  124]
train() client id: f_00002-2-1 loss: 1.167119  [   64/  124]
train() client id: f_00002-2-2 loss: 1.068834  [   96/  124]
train() client id: f_00003-0-0 loss: 0.719376  [   32/   43]
train() client id: f_00003-1-0 loss: 0.792417  [   32/   43]
train() client id: f_00003-2-0 loss: 0.633412  [   32/   43]
train() client id: f_00004-0-0 loss: 1.102196  [   32/  306]
train() client id: f_00004-0-1 loss: 1.111872  [   64/  306]
train() client id: f_00004-0-2 loss: 0.985116  [   96/  306]
train() client id: f_00004-0-3 loss: 0.907979  [  128/  306]
train() client id: f_00004-0-4 loss: 0.983744  [  160/  306]
train() client id: f_00004-0-5 loss: 0.997116  [  192/  306]
train() client id: f_00004-0-6 loss: 1.019963  [  224/  306]
train() client id: f_00004-0-7 loss: 0.968667  [  256/  306]
train() client id: f_00004-0-8 loss: 1.160015  [  288/  306]
train() client id: f_00004-1-0 loss: 1.172218  [   32/  306]
train() client id: f_00004-1-1 loss: 0.986552  [   64/  306]
train() client id: f_00004-1-2 loss: 0.988580  [   96/  306]
train() client id: f_00004-1-3 loss: 0.960254  [  128/  306]
train() client id: f_00004-1-4 loss: 1.062949  [  160/  306]
train() client id: f_00004-1-5 loss: 0.891282  [  192/  306]
train() client id: f_00004-1-6 loss: 1.025272  [  224/  306]
train() client id: f_00004-1-7 loss: 1.035440  [  256/  306]
train() client id: f_00004-1-8 loss: 0.996957  [  288/  306]
train() client id: f_00004-2-0 loss: 0.991267  [   32/  306]
train() client id: f_00004-2-1 loss: 0.991701  [   64/  306]
train() client id: f_00004-2-2 loss: 1.018546  [   96/  306]
train() client id: f_00004-2-3 loss: 1.129553  [  128/  306]
train() client id: f_00004-2-4 loss: 0.903323  [  160/  306]
train() client id: f_00004-2-5 loss: 1.075718  [  192/  306]
train() client id: f_00004-2-6 loss: 1.009364  [  224/  306]
train() client id: f_00004-2-7 loss: 1.016985  [  256/  306]
train() client id: f_00004-2-8 loss: 1.055853  [  288/  306]
train() client id: f_00005-0-0 loss: 0.672222  [   32/  146]
train() client id: f_00005-0-1 loss: 0.636862  [   64/  146]
train() client id: f_00005-0-2 loss: 0.608555  [   96/  146]
train() client id: f_00005-0-3 loss: 0.575897  [  128/  146]
train() client id: f_00005-1-0 loss: 0.482359  [   32/  146]
train() client id: f_00005-1-1 loss: 0.599001  [   64/  146]
train() client id: f_00005-1-2 loss: 0.658067  [   96/  146]
train() client id: f_00005-1-3 loss: 0.763172  [  128/  146]
train() client id: f_00005-2-0 loss: 0.719468  [   32/  146]
train() client id: f_00005-2-1 loss: 0.578332  [   64/  146]
train() client id: f_00005-2-2 loss: 0.610023  [   96/  146]
train() client id: f_00005-2-3 loss: 0.765271  [  128/  146]
train() client id: f_00006-0-0 loss: 0.512549  [   32/   54]
train() client id: f_00006-1-0 loss: 0.525837  [   32/   54]
train() client id: f_00006-2-0 loss: 0.450430  [   32/   54]
train() client id: f_00007-0-0 loss: 0.716846  [   32/  179]
train() client id: f_00007-0-1 loss: 0.531461  [   64/  179]
train() client id: f_00007-0-2 loss: 0.665156  [   96/  179]
train() client id: f_00007-0-3 loss: 0.464358  [  128/  179]
train() client id: f_00007-0-4 loss: 0.738682  [  160/  179]
train() client id: f_00007-1-0 loss: 0.499711  [   32/  179]
train() client id: f_00007-1-1 loss: 0.728469  [   64/  179]
train() client id: f_00007-1-2 loss: 0.512113  [   96/  179]
train() client id: f_00007-1-3 loss: 0.763053  [  128/  179]
train() client id: f_00007-1-4 loss: 0.652264  [  160/  179]
train() client id: f_00007-2-0 loss: 0.510229  [   32/  179]
train() client id: f_00007-2-1 loss: 0.645271  [   64/  179]
train() client id: f_00007-2-2 loss: 0.507055  [   96/  179]
train() client id: f_00007-2-3 loss: 0.875785  [  128/  179]
train() client id: f_00007-2-4 loss: 0.575396  [  160/  179]
train() client id: f_00008-0-0 loss: 0.571809  [   32/  130]
train() client id: f_00008-0-1 loss: 0.743524  [   64/  130]
train() client id: f_00008-0-2 loss: 0.688146  [   96/  130]
train() client id: f_00008-0-3 loss: 0.639660  [  128/  130]
train() client id: f_00008-1-0 loss: 0.672135  [   32/  130]
train() client id: f_00008-1-1 loss: 0.674816  [   64/  130]
train() client id: f_00008-1-2 loss: 0.638787  [   96/  130]
train() client id: f_00008-1-3 loss: 0.632803  [  128/  130]
train() client id: f_00008-2-0 loss: 0.645798  [   32/  130]
train() client id: f_00008-2-1 loss: 0.541222  [   64/  130]
train() client id: f_00008-2-2 loss: 0.700205  [   96/  130]
train() client id: f_00008-2-3 loss: 0.707481  [  128/  130]
train() client id: f_00009-0-0 loss: 0.672493  [   32/  118]
train() client id: f_00009-0-1 loss: 0.782962  [   64/  118]
train() client id: f_00009-0-2 loss: 0.737759  [   96/  118]
train() client id: f_00009-1-0 loss: 0.693075  [   32/  118]
train() client id: f_00009-1-1 loss: 0.655556  [   64/  118]
train() client id: f_00009-1-2 loss: 0.698259  [   96/  118]
train() client id: f_00009-2-0 loss: 0.585985  [   32/  118]
train() client id: f_00009-2-1 loss: 0.618596  [   64/  118]
train() client id: f_00009-2-2 loss: 0.778553  [   96/  118]
At round 81 accuracy: 0.6445623342175066
At round 81 training accuracy: 0.5922199865861838
At round 81 training loss: 0.8223233467079957
update_location
xs = [  -3.9056584     4.20031788  425.00902392   18.81129433    0.97929623
    3.95640986 -387.44319194 -366.32485185  409.66397685 -352.06087855]
ys = [ 417.5879595   400.55583871    1.32061395 -387.45517586  379.35018685
  362.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [429.4123404  412.87119371 436.61701116 400.5937819  392.31049346
 376.36386803 400.1488692  379.72960556 422.05834182 366.00939068]
dists_bs = [296.85382231 288.07670906 624.64018276 594.91061737 269.68075861
 259.42157167 276.91649437 258.73355676 605.48863217 246.15248922]
uav_gains = [5.76271498e-13 6.51982263e-13 5.47875000e-13 7.20245856e-13
 7.73843588e-13 8.99752934e-13 7.22955533e-13 8.70243248e-13
 6.07950194e-13 1.00281652e-12]
bs_gains = [1.31878706e-11 1.43440329e-11 1.64259431e-12 1.88290873e-12
 1.72549694e-11 1.92343292e-11 1.60220239e-11 1.93778844e-11
 1.79224469e-12 2.22803616e-11]
Round 82
-------------------------------
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.03972093 0.07667447 0.03927574 0.01549837 0.08835219 0.04258691
 0.01863559 0.05403005 0.03910533 0.03455187]
obj_prev = 0.44843144567355053
eta_min = 0.0	eta_max = inf
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.10079758902614538	eta = 0.9090909090909091
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.3375882886973583	eta = 0.2714376502678347
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.19414841544868844	eta = 0.47198001400206313
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17188140883090305	eta = 0.5331243935293785
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17048248758463935	eta = 0.5374990307814271
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17047588964991173	eta = 0.5375198336265012
eta = 0.5375198336265012
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.05122607 0.10773737 0.05041296 0.01748191 0.12440617 0.05935719
 0.02195403 0.07277355 0.0528523  0.04797362]
ene_total = [0.01831375 0.02382031 0.0184197  0.01040877 0.02710943 0.01385921
 0.01130017 0.020882   0.01487805 0.0114845 ]
ti_comp = [127.24870803 127.54244623 127.23535515 127.30084038 127.54703912
 127.54954855 127.30163179 127.33761381 127.43428871 127.5527431 ]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [5.18855480e-10 4.80473758e-09 4.94641987e-10 2.06055571e-11
 7.39716068e-09 8.03419283e-10 4.08088986e-11 1.48554980e-09
 5.68197076e-10 4.24138750e-10]
ene_total = [0.00791929 0.00195895 0.00819024 0.00686146 0.00186576 0.00181484
 0.0068454  0.00611528 0.00415361 0.00175002]
optimize_network iter = 0 obj = 0.04747485012945179
eta = 0.5375198336265012
freqs = [201283.26753271 422358.89209453 198109.08366522  68663.77799187
 487687.42285783 232682.88526082  86228.39443656 285750.39151138
 207370.78109251 188054.06037245]
eta_min = 0.537519833626502	eta_max = 0.9942699445819028
af = 2.5030539272411267e-09	bf = 0.021956175032555886	zeta = 2.7533593199652398e-09	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [7.96601739e-11 7.37674065e-10 7.59426628e-11 3.16358278e-12
 1.13569024e-09 1.23349415e-10 6.26541318e-12 2.28077296e-10
 8.72356169e-11 6.51182610e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 1 obj = 0.24151792697303298
eta = 0.909090909090909
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775164]
eta_min = 0.909090909090918	eta_max = 0.9684934797724077
af = 1.5403473503468153e-09	bf = 0.021956175032555886	zeta = 1.694382085381497e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 2 obj = 0.24151792697305713
eta = 0.909090909090918
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775163]
Done!
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.79066540e-11 4.35264361e-10 4.57108141e-11 1.89608676e-12
 6.69915714e-10 7.27490395e-11 3.75496892e-12 1.36371128e-10
 5.18337331e-11 3.83975697e-11]
ene_total = [0.03902796 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
At round 82 energy consumption: 0.23396609732358067
At round 82 eta: 0.909090909090918
At round 82 a_n: 0.09384340477035735
At round 82 local rounds: 3.1209395205774326
At round 82 global rounds: 1.0322774524740326
gradient difference: 0.9109660387039185
train() client id: f_00000-0-0 loss: 0.726019  [   32/  126]
train() client id: f_00000-0-1 loss: 0.722463  [   64/  126]
train() client id: f_00000-0-2 loss: 0.715388  [   96/  126]
train() client id: f_00000-1-0 loss: 0.769011  [   32/  126]
train() client id: f_00000-1-1 loss: 0.633761  [   64/  126]
train() client id: f_00000-1-2 loss: 0.862053  [   96/  126]
train() client id: f_00000-2-0 loss: 0.699002  [   32/  126]
train() client id: f_00000-2-1 loss: 0.746195  [   64/  126]
train() client id: f_00000-2-2 loss: 0.650895  [   96/  126]
train() client id: f_00001-0-0 loss: 0.504616  [   32/  265]
train() client id: f_00001-0-1 loss: 0.518568  [   64/  265]
train() client id: f_00001-0-2 loss: 0.520771  [   96/  265]
train() client id: f_00001-0-3 loss: 0.536604  [  128/  265]
train() client id: f_00001-0-4 loss: 0.446628  [  160/  265]
train() client id: f_00001-0-5 loss: 0.647690  [  192/  265]
train() client id: f_00001-0-6 loss: 0.481935  [  224/  265]
train() client id: f_00001-0-7 loss: 0.524349  [  256/  265]
train() client id: f_00001-1-0 loss: 0.574834  [   32/  265]
train() client id: f_00001-1-1 loss: 0.485235  [   64/  265]
train() client id: f_00001-1-2 loss: 0.522717  [   96/  265]
train() client id: f_00001-1-3 loss: 0.568421  [  128/  265]
train() client id: f_00001-1-4 loss: 0.499873  [  160/  265]
train() client id: f_00001-1-5 loss: 0.521638  [  192/  265]
train() client id: f_00001-1-6 loss: 0.491372  [  224/  265]
train() client id: f_00001-1-7 loss: 0.547744  [  256/  265]
train() client id: f_00001-2-0 loss: 0.647166  [   32/  265]
train() client id: f_00001-2-1 loss: 0.447136  [   64/  265]
train() client id: f_00001-2-2 loss: 0.523195  [   96/  265]
train() client id: f_00001-2-3 loss: 0.522742  [  128/  265]
train() client id: f_00001-2-4 loss: 0.430015  [  160/  265]
train() client id: f_00001-2-5 loss: 0.610713  [  192/  265]
train() client id: f_00001-2-6 loss: 0.510825  [  224/  265]
train() client id: f_00001-2-7 loss: 0.497089  [  256/  265]
train() client id: f_00002-0-0 loss: 0.871811  [   32/  124]
train() client id: f_00002-0-1 loss: 1.214793  [   64/  124]
train() client id: f_00002-0-2 loss: 0.953933  [   96/  124]
train() client id: f_00002-1-0 loss: 0.873653  [   32/  124]
train() client id: f_00002-1-1 loss: 1.003974  [   64/  124]
train() client id: f_00002-1-2 loss: 0.968218  [   96/  124]
train() client id: f_00002-2-0 loss: 1.015097  [   32/  124]
train() client id: f_00002-2-1 loss: 0.803669  [   64/  124]
train() client id: f_00002-2-2 loss: 1.005994  [   96/  124]
train() client id: f_00003-0-0 loss: 0.786579  [   32/   43]
train() client id: f_00003-1-0 loss: 0.539976  [   32/   43]
train() client id: f_00003-2-0 loss: 0.578982  [   32/   43]
train() client id: f_00004-0-0 loss: 0.667079  [   32/  306]
train() client id: f_00004-0-1 loss: 0.716591  [   64/  306]
train() client id: f_00004-0-2 loss: 0.979104  [   96/  306]
train() client id: f_00004-0-3 loss: 0.775864  [  128/  306]
train() client id: f_00004-0-4 loss: 0.783376  [  160/  306]
train() client id: f_00004-0-5 loss: 0.876762  [  192/  306]
train() client id: f_00004-0-6 loss: 0.794233  [  224/  306]
train() client id: f_00004-0-7 loss: 0.814039  [  256/  306]
train() client id: f_00004-0-8 loss: 0.807502  [  288/  306]
train() client id: f_00004-1-0 loss: 0.840381  [   32/  306]
train() client id: f_00004-1-1 loss: 0.824318  [   64/  306]
train() client id: f_00004-1-2 loss: 0.772143  [   96/  306]
train() client id: f_00004-1-3 loss: 0.807955  [  128/  306]
train() client id: f_00004-1-4 loss: 0.924801  [  160/  306]
train() client id: f_00004-1-5 loss: 0.824908  [  192/  306]
train() client id: f_00004-1-6 loss: 0.650231  [  224/  306]
train() client id: f_00004-1-7 loss: 0.802471  [  256/  306]
train() client id: f_00004-1-8 loss: 0.753097  [  288/  306]
train() client id: f_00004-2-0 loss: 0.890047  [   32/  306]
train() client id: f_00004-2-1 loss: 0.821779  [   64/  306]
train() client id: f_00004-2-2 loss: 0.783832  [   96/  306]
train() client id: f_00004-2-3 loss: 0.778728  [  128/  306]
train() client id: f_00004-2-4 loss: 0.710002  [  160/  306]
train() client id: f_00004-2-5 loss: 0.719857  [  192/  306]
train() client id: f_00004-2-6 loss: 0.856100  [  224/  306]
train() client id: f_00004-2-7 loss: 0.876525  [  256/  306]
train() client id: f_00004-2-8 loss: 0.682294  [  288/  306]
train() client id: f_00005-0-0 loss: 0.465205  [   32/  146]
train() client id: f_00005-0-1 loss: 0.758475  [   64/  146]
train() client id: f_00005-0-2 loss: 0.534917  [   96/  146]
train() client id: f_00005-0-3 loss: 0.638800  [  128/  146]
train() client id: f_00005-1-0 loss: 0.810026  [   32/  146]
train() client id: f_00005-1-1 loss: 0.553613  [   64/  146]
train() client id: f_00005-1-2 loss: 0.390425  [   96/  146]
train() client id: f_00005-1-3 loss: 0.462179  [  128/  146]
train() client id: f_00005-2-0 loss: 0.296396  [   32/  146]
train() client id: f_00005-2-1 loss: 0.798098  [   64/  146]
train() client id: f_00005-2-2 loss: 0.478424  [   96/  146]
train() client id: f_00005-2-3 loss: 0.561698  [  128/  146]
train() client id: f_00006-0-0 loss: 0.455516  [   32/   54]
train() client id: f_00006-1-0 loss: 0.561060  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469760  [   32/   54]
train() client id: f_00007-0-0 loss: 0.641400  [   32/  179]
train() client id: f_00007-0-1 loss: 0.572475  [   64/  179]
train() client id: f_00007-0-2 loss: 0.777681  [   96/  179]
train() client id: f_00007-0-3 loss: 0.541026  [  128/  179]
train() client id: f_00007-0-4 loss: 0.563455  [  160/  179]
train() client id: f_00007-1-0 loss: 0.537781  [   32/  179]
train() client id: f_00007-1-1 loss: 0.718949  [   64/  179]
train() client id: f_00007-1-2 loss: 0.615008  [   96/  179]
train() client id: f_00007-1-3 loss: 0.678775  [  128/  179]
train() client id: f_00007-1-4 loss: 0.602676  [  160/  179]
train() client id: f_00007-2-0 loss: 0.559507  [   32/  179]
train() client id: f_00007-2-1 loss: 0.597823  [   64/  179]
train() client id: f_00007-2-2 loss: 0.579123  [   96/  179]
train() client id: f_00007-2-3 loss: 0.729311  [  128/  179]
train() client id: f_00007-2-4 loss: 0.784877  [  160/  179]
train() client id: f_00008-0-0 loss: 0.761466  [   32/  130]
train() client id: f_00008-0-1 loss: 0.791580  [   64/  130]
train() client id: f_00008-0-2 loss: 0.708463  [   96/  130]
train() client id: f_00008-0-3 loss: 0.585120  [  128/  130]
train() client id: f_00008-1-0 loss: 0.636118  [   32/  130]
train() client id: f_00008-1-1 loss: 0.651813  [   64/  130]
train() client id: f_00008-1-2 loss: 0.804891  [   96/  130]
train() client id: f_00008-1-3 loss: 0.780546  [  128/  130]
train() client id: f_00008-2-0 loss: 0.719268  [   32/  130]
train() client id: f_00008-2-1 loss: 0.768135  [   64/  130]
train() client id: f_00008-2-2 loss: 0.598262  [   96/  130]
train() client id: f_00008-2-3 loss: 0.759229  [  128/  130]
train() client id: f_00009-0-0 loss: 0.606077  [   32/  118]
train() client id: f_00009-0-1 loss: 0.718444  [   64/  118]
train() client id: f_00009-0-2 loss: 0.630930  [   96/  118]
train() client id: f_00009-1-0 loss: 0.676527  [   32/  118]
train() client id: f_00009-1-1 loss: 0.654665  [   64/  118]
train() client id: f_00009-1-2 loss: 0.604597  [   96/  118]
train() client id: f_00009-2-0 loss: 0.663023  [   32/  118]
train() client id: f_00009-2-1 loss: 0.617782  [   64/  118]
train() client id: f_00009-2-2 loss: 0.636003  [   96/  118]
At round 82 accuracy: 0.6445623342175066
At round 82 training accuracy: 0.5902079141515761
At round 82 training loss: 0.8155881015730035
Done!
