v = 22.697160301531774
a_0 = 27.840057009285058	a_alpha = -0.3425458469693173
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
num_samples = [126 265 124  43 306 146  54 179 130 118]
msize = 502400
xs = [  6.0943416  -20.79968212  15.00902392  18.81129433 -39.02070377
 -26.04359014   2.55680806  -6.32485185  -0.33602315 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  22.54482414   9.35018685
 -17.18584926   7.37501568 -19.17765202  17.56900603  -0.99851822]
dists_uav = [101.71763524 103.31800857 101.12870423 104.22156154 107.7499017
 104.75505717 100.304178   102.01855757 101.5321766  101.44984286]
dists_bs = [239.94522511 221.81113273 257.42563646 246.58750669 214.31339109
 243.15306433 244.18139867 257.20860434 235.14255819 236.47461703]
uav_gains = [9.58312886e-11 9.21631668e-11 9.72326274e-11 9.01785356e-11
 8.29761259e-11 8.90347214e-11 9.92432040e-11 9.51261523e-11
 9.62695134e-11 9.64649612e-11]
bs_gains = [2.39320649e-11 2.98222725e-11 1.96548195e-11 2.21704798e-11
 3.28364367e-11 2.30584863e-11 2.27876148e-11 1.97012919e-11
 2.53260008e-11 2.49285719e-11]
SystemModel __init__!
t_co_uav = [0.06351163 0.06396501 0.06334461 0.06422068 0.06521719 0.06437154
 0.0631106  0.06359693 0.06345904 0.0634357 ]
t_co_bs = [0.08476871 0.08051968 0.08895491 0.08634835 0.0787878  0.08552995
 0.08577462 0.08890235 0.0836346  0.08394849]
difference = [-0.02125708 -0.01655467 -0.0256103  -0.02212767 -0.01357061 -0.0211584
 -0.02266402 -0.02530541 -0.02017555 -0.0205128 ]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 6.796163711028166	bf = 20.32894796997589	zeta = 7.475780082130983	eta = 0.9090909090909091
af = 6.796163711028166	bf = 20.32894796997589	zeta = 230.74360710778967	eta = 0.02945331312192499
af = 6.796163711028166	bf = 20.32894796997589	zeta = 45.629002909774904	eta = 0.1489439452461113
af = 6.796163711028166	bf = 20.32894796997589	zeta = 39.092727771299245	eta = 0.173847262610764
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.999286881910415	eta = 0.17426379440236703
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.99926333319505	eta = 0.17426389962713645
eta_opt = 0.17426389962713645
initialize_feasible_solution eta = 0.17426389962713645, tau = 10.000839233398438
ti_comp = [0.03604337 0.0758055  0.03547125 0.01230051 0.0875339  0.04176454
 0.01544716 0.05120447 0.0371876  0.0337549 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [3.35654071 5.27057237 3.33162034 2.5799458  5.60760928 4.29178968
 2.64860954 3.87057992 4.07357378 3.96842191]
system_model train() tau = 30	t0 = 0.050004196166992185	t_min = 10.000839233398438
Round 0
-------------------------------
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86489914 22.7015787  10.6948057   3.82750737 26.17399534 12.62305496
  4.75731633 15.36016845 11.26073498 10.24387519]
obj_prev = 128.5079361631131
eta_min = 2.1972428731623064e-09	eta_max = 0.9187482009227209
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 50.862560519193266	eta = 0.5344727942639534
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 40.950101804889684	eta = 0.6638482847646219
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.18020327767913	eta = 0.6938364931761265
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094800455198204	eta = 0.6953521830931377
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094588160812094	eta = 0.6953559590470941
eta = 0.6953559590470941
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.02998124 0.06305577 0.02950534 0.01023169 0.07281157 0.03474016
 0.0128491  0.04259239 0.03093302 0.02807767]
ene_total = [3.3202555  6.49821862 3.27523947 1.52191262 7.37393057 3.95636804
 1.75096148 4.4735121  3.5911297  3.33306006]
ti_comp = [0.26476791 0.24775986 0.26493494 0.26405887 0.24949175 0.2427496
 0.26516894 0.26468261 0.24464495 0.24433105]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.40269086e-05 2.55265831e-04 2.28719695e-05 9.60109636e-07
 3.87586715e-04 4.44691157e-05 1.88561461e-06 6.89326925e-05
 3.09082577e-05 2.31742490e-05]
ene_total = [0.58260011 0.75916097 0.58096823 0.58697181 0.75542628 0.78568377
 0.5769119  0.5874834  0.76712376 0.76928555]
optimize_network iter = 0 obj = 6.7516157792990965
eta = 0.6953559590470941
freqs = [5.66179550e+07 1.27251793e+08 5.56841306e+07 1.93738838e+07
 1.45919802e+08 7.15555497e+07 2.42281409e+07 8.04593666e+07
 6.32202327e+07 5.74582420e+07]
eta_min = 0.6595211103034527	eta_max = 0.6953559590470929
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 0.07249356690995133	eta = 0.909090909090909
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 22.43093651594189	eta = 0.002938051320263521
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.424233800512977	eta = 0.027185184296772417
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333929777668074	eta = 0.028237028926918685
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333881523150504	eta = 0.02823761274584649
eta = 0.02823761274584649
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.35890421e-04 2.50613865e-03 2.24551507e-04 9.42612594e-06
 3.80523333e-03 4.36587104e-04 1.85125116e-05 6.76764628e-04
 3.03449855e-04 2.27519213e-04]
ene_total = [0.18871274 0.3024796  0.18790939 0.18425611 0.33473573 0.25754286
 0.18133617 0.20158775 0.24829862 0.24702256]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 1 obj = 6.050058200424808
eta = 0.6595211103034527
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
eta_min = 0.6595211103034622	eta_max = 0.6595211103034516
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 0.07141177968017555	eta = 0.909090909090909
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 22.429905462588092	eta = 0.0028943412096646293
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.4193533145705937	eta = 0.026833534117679286
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3303118142037405	eta = 0.02785884674898469
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3302654452731733	eta = 0.02785940109996355
eta = 0.02785940109996355
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.36690987e-04 2.47126971e-03 2.25349755e-04 9.45165461e-06
 3.75932384e-03 4.28126361e-04 1.85824929e-05 6.79005729e-04
 2.98203014e-04 2.23506966e-04]
ene_total = [0.18866225 0.30136334 0.18785915 0.18418516 0.33329075 0.25720037
 0.18126762 0.2015735  0.24805176 0.24681156]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 2 obj = 6.0500582004249726
eta = 0.6595211103034622
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
Done!
At round 0 energy consumption: 6.050058200424808
At round 0 eta: 0.6595211103034622
At round 0 local rounds: 13.62985484868938
At round 0 global rounds: 81.76735137411414
At round 0 a_n: 27.840057009285058
gradient difference: 0.9177300333976746
train() client id: f_00000-0-0 loss: 1.057538  [   32/  126]
train() client id: f_00000-0-1 loss: 1.018161  [   64/  126]
train() client id: f_00000-0-2 loss: 1.000646  [   96/  126]
train() client id: f_00000-1-0 loss: 1.009784  [   32/  126]
train() client id: f_00000-1-1 loss: 1.043961  [   64/  126]
train() client id: f_00000-1-2 loss: 0.993182  [   96/  126]
train() client id: f_00000-2-0 loss: 1.060839  [   32/  126]
train() client id: f_00000-2-1 loss: 1.007776  [   64/  126]
train() client id: f_00000-2-2 loss: 0.982346  [   96/  126]
train() client id: f_00000-3-0 loss: 1.013737  [   32/  126]
train() client id: f_00000-3-1 loss: 1.055651  [   64/  126]
train() client id: f_00000-3-2 loss: 0.983902  [   96/  126]
train() client id: f_00000-4-0 loss: 1.025657  [   32/  126]
train() client id: f_00000-4-1 loss: 1.033212  [   64/  126]
train() client id: f_00000-4-2 loss: 1.010873  [   96/  126]
train() client id: f_00000-5-0 loss: 1.099555  [   32/  126]
train() client id: f_00000-5-1 loss: 1.009254  [   64/  126]
train() client id: f_00000-5-2 loss: 1.038160  [   96/  126]
train() client id: f_00000-6-0 loss: 1.036046  [   32/  126]
train() client id: f_00000-6-1 loss: 0.986481  [   64/  126]
train() client id: f_00000-6-2 loss: 1.104536  [   96/  126]
train() client id: f_00000-7-0 loss: 1.169789  [   32/  126]
train() client id: f_00000-7-1 loss: 0.959101  [   64/  126]
train() client id: f_00000-7-2 loss: 0.995100  [   96/  126]
train() client id: f_00000-8-0 loss: 1.052118  [   32/  126]
train() client id: f_00000-8-1 loss: 1.082994  [   64/  126]
train() client id: f_00000-8-2 loss: 1.013058  [   96/  126]
train() client id: f_00000-9-0 loss: 1.057274  [   32/  126]
train() client id: f_00000-9-1 loss: 1.016460  [   64/  126]
train() client id: f_00000-9-2 loss: 1.090370  [   96/  126]
train() client id: f_00000-10-0 loss: 1.036933  [   32/  126]
train() client id: f_00000-10-1 loss: 1.059092  [   64/  126]
train() client id: f_00000-10-2 loss: 1.088156  [   96/  126]
train() client id: f_00000-11-0 loss: 1.054616  [   32/  126]
train() client id: f_00000-11-1 loss: 0.939772  [   64/  126]
train() client id: f_00000-11-2 loss: 1.094752  [   96/  126]
train() client id: f_00000-12-0 loss: 1.114932  [   32/  126]
train() client id: f_00000-12-1 loss: 1.093055  [   64/  126]
train() client id: f_00000-12-2 loss: 1.005133  [   96/  126]
train() client id: f_00001-0-0 loss: 0.986740  [   32/  265]
train() client id: f_00001-0-1 loss: 0.951979  [   64/  265]
train() client id: f_00001-0-2 loss: 0.972499  [   96/  265]
train() client id: f_00001-0-3 loss: 0.951288  [  128/  265]
train() client id: f_00001-0-4 loss: 0.955685  [  160/  265]
train() client id: f_00001-0-5 loss: 0.961387  [  192/  265]
train() client id: f_00001-0-6 loss: 1.025412  [  224/  265]
train() client id: f_00001-0-7 loss: 1.087307  [  256/  265]
train() client id: f_00001-1-0 loss: 1.101927  [   32/  265]
train() client id: f_00001-1-1 loss: 1.070760  [   64/  265]
train() client id: f_00001-1-2 loss: 0.968277  [   96/  265]
train() client id: f_00001-1-3 loss: 0.921300  [  128/  265]
train() client id: f_00001-1-4 loss: 0.970161  [  160/  265]
train() client id: f_00001-1-5 loss: 0.970512  [  192/  265]
train() client id: f_00001-1-6 loss: 0.975345  [  224/  265]
train() client id: f_00001-1-7 loss: 0.995520  [  256/  265]
train() client id: f_00001-2-0 loss: 0.964059  [   32/  265]
train() client id: f_00001-2-1 loss: 1.012467  [   64/  265]
train() client id: f_00001-2-2 loss: 1.010575  [   96/  265]
train() client id: f_00001-2-3 loss: 1.087277  [  128/  265]
train() client id: f_00001-2-4 loss: 0.926086  [  160/  265]
train() client id: f_00001-2-5 loss: 1.003000  [  192/  265]
train() client id: f_00001-2-6 loss: 1.086518  [  224/  265]
train() client id: f_00001-2-7 loss: 0.969011  [  256/  265]
train() client id: f_00001-3-0 loss: 0.988527  [   32/  265]
train() client id: f_00001-3-1 loss: 0.937936  [   64/  265]
train() client id: f_00001-3-2 loss: 1.051415  [   96/  265]
train() client id: f_00001-3-3 loss: 1.066067  [  128/  265]
train() client id: f_00001-3-4 loss: 1.160308  [  160/  265]
train() client id: f_00001-3-5 loss: 1.003749  [  192/  265]
train() client id: f_00001-3-6 loss: 1.012000  [  224/  265]
train() client id: f_00001-3-7 loss: 0.980789  [  256/  265]
train() client id: f_00001-4-0 loss: 1.064309  [   32/  265]
train() client id: f_00001-4-1 loss: 1.058913  [   64/  265]
train() client id: f_00001-4-2 loss: 1.018432  [   96/  265]
train() client id: f_00001-4-3 loss: 1.038091  [  128/  265]
train() client id: f_00001-4-4 loss: 1.044637  [  160/  265]
train() client id: f_00001-4-5 loss: 1.084578  [  192/  265]
train() client id: f_00001-4-6 loss: 1.006050  [  224/  265]
train() client id: f_00001-4-7 loss: 1.045244  [  256/  265]
train() client id: f_00001-5-0 loss: 1.049043  [   32/  265]
train() client id: f_00001-5-1 loss: 1.102253  [   64/  265]
train() client id: f_00001-5-2 loss: 0.982038  [   96/  265]
train() client id: f_00001-5-3 loss: 1.091601  [  128/  265]
train() client id: f_00001-5-4 loss: 1.031471  [  160/  265]
train() client id: f_00001-5-5 loss: 1.108396  [  192/  265]
train() client id: f_00001-5-6 loss: 1.108336  [  224/  265]
train() client id: f_00001-5-7 loss: 1.048921  [  256/  265]
train() client id: f_00001-6-0 loss: 1.162396  [   32/  265]
train() client id: f_00001-6-1 loss: 1.110064  [   64/  265]
train() client id: f_00001-6-2 loss: 0.993415  [   96/  265]
train() client id: f_00001-6-3 loss: 1.239433  [  128/  265]
train() client id: f_00001-6-4 loss: 1.007111  [  160/  265]
train() client id: f_00001-6-5 loss: 1.094130  [  192/  265]
train() client id: f_00001-6-6 loss: 1.015620  [  224/  265]
train() client id: f_00001-6-7 loss: 1.041473  [  256/  265]
train() client id: f_00001-7-0 loss: 1.128068  [   32/  265]
train() client id: f_00001-7-1 loss: 1.095498  [   64/  265]
train() client id: f_00001-7-2 loss: 1.055174  [   96/  265]
train() client id: f_00001-7-3 loss: 1.083174  [  128/  265]
train() client id: f_00001-7-4 loss: 1.181381  [  160/  265]
train() client id: f_00001-7-5 loss: 1.031714  [  192/  265]
train() client id: f_00001-7-6 loss: 1.129813  [  224/  265]
train() client id: f_00001-7-7 loss: 1.072293  [  256/  265]
train() client id: f_00001-8-0 loss: 1.058876  [   32/  265]
train() client id: f_00001-8-1 loss: 1.126538  [   64/  265]
train() client id: f_00001-8-2 loss: 1.066419  [   96/  265]
train() client id: f_00001-8-3 loss: 1.228733  [  128/  265]
train() client id: f_00001-8-4 loss: 1.159629  [  160/  265]
train() client id: f_00001-8-5 loss: 1.054361  [  192/  265]
train() client id: f_00001-8-6 loss: 1.135111  [  224/  265]
train() client id: f_00001-8-7 loss: 1.140469  [  256/  265]
train() client id: f_00001-9-0 loss: 1.216369  [   32/  265]
train() client id: f_00001-9-1 loss: 1.152191  [   64/  265]
train() client id: f_00001-9-2 loss: 1.036365  [   96/  265]
train() client id: f_00001-9-3 loss: 1.125506  [  128/  265]
train() client id: f_00001-9-4 loss: 1.175745  [  160/  265]
train() client id: f_00001-9-5 loss: 1.093858  [  192/  265]
train() client id: f_00001-9-6 loss: 1.221921  [  224/  265]
train() client id: f_00001-9-7 loss: 1.083256  [  256/  265]
train() client id: f_00001-10-0 loss: 1.174596  [   32/  265]
train() client id: f_00001-10-1 loss: 1.093761  [   64/  265]
train() client id: f_00001-10-2 loss: 1.123922  [   96/  265]
train() client id: f_00001-10-3 loss: 1.165018  [  128/  265]
train() client id: f_00001-10-4 loss: 1.083278  [  160/  265]
train() client id: f_00001-10-5 loss: 1.191616  [  192/  265]
train() client id: f_00001-10-6 loss: 1.190200  [  224/  265]
train() client id: f_00001-10-7 loss: 1.200970  [  256/  265]
train() client id: f_00001-11-0 loss: 1.253438  [   32/  265]
train() client id: f_00001-11-1 loss: 1.179525  [   64/  265]
train() client id: f_00001-11-2 loss: 1.097667  [   96/  265]
train() client id: f_00001-11-3 loss: 1.104852  [  128/  265]
train() client id: f_00001-11-4 loss: 1.184504  [  160/  265]
train() client id: f_00001-11-5 loss: 1.094507  [  192/  265]
train() client id: f_00001-11-6 loss: 1.158555  [  224/  265]
train() client id: f_00001-11-7 loss: 1.290021  [  256/  265]
train() client id: f_00001-12-0 loss: 1.172990  [   32/  265]
train() client id: f_00001-12-1 loss: 1.159707  [   64/  265]
train() client id: f_00001-12-2 loss: 1.119028  [   96/  265]
train() client id: f_00001-12-3 loss: 1.299970  [  128/  265]
train() client id: f_00001-12-4 loss: 1.270370  [  160/  265]
train() client id: f_00001-12-5 loss: 1.193700  [  192/  265]
train() client id: f_00001-12-6 loss: 1.192015  [  224/  265]
train() client id: f_00001-12-7 loss: 1.149290  [  256/  265]
train() client id: f_00002-0-0 loss: 0.958034  [   32/  124]
train() client id: f_00002-0-1 loss: 0.977164  [   64/  124]
train() client id: f_00002-0-2 loss: 0.989498  [   96/  124]
train() client id: f_00002-1-0 loss: 0.899381  [   32/  124]
train() client id: f_00002-1-1 loss: 1.002015  [   64/  124]
train() client id: f_00002-1-2 loss: 0.922007  [   96/  124]
train() client id: f_00002-2-0 loss: 1.016219  [   32/  124]
train() client id: f_00002-2-1 loss: 0.955019  [   64/  124]
train() client id: f_00002-2-2 loss: 0.937442  [   96/  124]
train() client id: f_00002-3-0 loss: 0.974947  [   32/  124]
train() client id: f_00002-3-1 loss: 0.935589  [   64/  124]
train() client id: f_00002-3-2 loss: 0.993260  [   96/  124]
train() client id: f_00002-4-0 loss: 0.993429  [   32/  124]
train() client id: f_00002-4-1 loss: 0.939615  [   64/  124]
train() client id: f_00002-4-2 loss: 0.987254  [   96/  124]
train() client id: f_00002-5-0 loss: 1.040853  [   32/  124]
train() client id: f_00002-5-1 loss: 0.946052  [   64/  124]
train() client id: f_00002-5-2 loss: 0.978302  [   96/  124]
train() client id: f_00002-6-0 loss: 1.029502  [   32/  124]
train() client id: f_00002-6-1 loss: 1.022198  [   64/  124]
train() client id: f_00002-6-2 loss: 0.954698  [   96/  124]
train() client id: f_00002-7-0 loss: 1.001190  [   32/  124]
train() client id: f_00002-7-1 loss: 1.039346  [   64/  124]
train() client id: f_00002-7-2 loss: 0.952837  [   96/  124]
train() client id: f_00002-8-0 loss: 1.063160  [   32/  124]
train() client id: f_00002-8-1 loss: 1.049682  [   64/  124]
train() client id: f_00002-8-2 loss: 0.914098  [   96/  124]
train() client id: f_00002-9-0 loss: 0.933636  [   32/  124]
train() client id: f_00002-9-1 loss: 1.062545  [   64/  124]
train() client id: f_00002-9-2 loss: 0.942174  [   96/  124]
train() client id: f_00002-10-0 loss: 1.053001  [   32/  124]
train() client id: f_00002-10-1 loss: 0.988845  [   64/  124]
train() client id: f_00002-10-2 loss: 0.952172  [   96/  124]
train() client id: f_00002-11-0 loss: 1.016834  [   32/  124]
train() client id: f_00002-11-1 loss: 0.954269  [   64/  124]
train() client id: f_00002-11-2 loss: 1.088088  [   96/  124]
train() client id: f_00002-12-0 loss: 1.080628  [   32/  124]
train() client id: f_00002-12-1 loss: 1.003448  [   64/  124]
train() client id: f_00002-12-2 loss: 0.902238  [   96/  124]
train() client id: f_00003-0-0 loss: 1.088405  [   32/   43]
train() client id: f_00003-1-0 loss: 0.982839  [   32/   43]
train() client id: f_00003-2-0 loss: 1.042401  [   32/   43]
train() client id: f_00003-3-0 loss: 1.035890  [   32/   43]
train() client id: f_00003-4-0 loss: 1.095001  [   32/   43]
train() client id: f_00003-5-0 loss: 1.057761  [   32/   43]
train() client id: f_00003-6-0 loss: 1.100061  [   32/   43]
train() client id: f_00003-7-0 loss: 1.019533  [   32/   43]
train() client id: f_00003-8-0 loss: 1.056062  [   32/   43]
train() client id: f_00003-9-0 loss: 1.048402  [   32/   43]
train() client id: f_00003-10-0 loss: 1.121090  [   32/   43]
train() client id: f_00003-11-0 loss: 1.049990  [   32/   43]
train() client id: f_00003-12-0 loss: 1.052762  [   32/   43]
train() client id: f_00004-0-0 loss: 1.024077  [   32/  306]
train() client id: f_00004-0-1 loss: 1.062005  [   64/  306]
train() client id: f_00004-0-2 loss: 1.094312  [   96/  306]
train() client id: f_00004-0-3 loss: 1.175837  [  128/  306]
train() client id: f_00004-0-4 loss: 1.082511  [  160/  306]
train() client id: f_00004-0-5 loss: 1.062473  [  192/  306]
train() client id: f_00004-0-6 loss: 1.062883  [  224/  306]
train() client id: f_00004-0-7 loss: 1.132116  [  256/  306]
train() client id: f_00004-0-8 loss: 1.073783  [  288/  306]
train() client id: f_00004-1-0 loss: 1.039968  [   32/  306]
train() client id: f_00004-1-1 loss: 1.085627  [   64/  306]
train() client id: f_00004-1-2 loss: 1.079513  [   96/  306]
train() client id: f_00004-1-3 loss: 1.097515  [  128/  306]
train() client id: f_00004-1-4 loss: 1.137481  [  160/  306]
train() client id: f_00004-1-5 loss: 1.085044  [  192/  306]
train() client id: f_00004-1-6 loss: 1.195244  [  224/  306]
train() client id: f_00004-1-7 loss: 1.059257  [  256/  306]
train() client id: f_00004-1-8 loss: 1.039071  [  288/  306]
train() client id: f_00004-2-0 loss: 1.082813  [   32/  306]
train() client id: f_00004-2-1 loss: 1.068288  [   64/  306]
train() client id: f_00004-2-2 loss: 1.051097  [   96/  306]
train() client id: f_00004-2-3 loss: 1.180587  [  128/  306]
train() client id: f_00004-2-4 loss: 1.128080  [  160/  306]
train() client id: f_00004-2-5 loss: 1.116410  [  192/  306]
train() client id: f_00004-2-6 loss: 1.099871  [  224/  306]
train() client id: f_00004-2-7 loss: 1.039562  [  256/  306]
train() client id: f_00004-2-8 loss: 1.074926  [  288/  306]
train() client id: f_00004-3-0 loss: 1.026508  [   32/  306]
train() client id: f_00004-3-1 loss: 1.072178  [   64/  306]
train() client id: f_00004-3-2 loss: 1.111249  [   96/  306]
train() client id: f_00004-3-3 loss: 1.138053  [  128/  306]
train() client id: f_00004-3-4 loss: 1.076892  [  160/  306]
train() client id: f_00004-3-5 loss: 1.140507  [  192/  306]
train() client id: f_00004-3-6 loss: 1.070229  [  224/  306]
train() client id: f_00004-3-7 loss: 1.140935  [  256/  306]
train() client id: f_00004-3-8 loss: 1.109631  [  288/  306]
train() client id: f_00004-4-0 loss: 1.104303  [   32/  306]
train() client id: f_00004-4-1 loss: 1.161721  [   64/  306]
train() client id: f_00004-4-2 loss: 1.079551  [   96/  306]
train() client id: f_00004-4-3 loss: 1.061114  [  128/  306]
train() client id: f_00004-4-4 loss: 1.165408  [  160/  306]
train() client id: f_00004-4-5 loss: 1.043460  [  192/  306]
train() client id: f_00004-4-6 loss: 1.153667  [  224/  306]
train() client id: f_00004-4-7 loss: 1.078225  [  256/  306]
train() client id: f_00004-4-8 loss: 1.090009  [  288/  306]
train() client id: f_00004-5-0 loss: 1.106700  [   32/  306]
train() client id: f_00004-5-1 loss: 1.108804  [   64/  306]
train() client id: f_00004-5-2 loss: 1.142102  [   96/  306]
train() client id: f_00004-5-3 loss: 1.030803  [  128/  306]
train() client id: f_00004-5-4 loss: 1.125395  [  160/  306]
train() client id: f_00004-5-5 loss: 1.137481  [  192/  306]
train() client id: f_00004-5-6 loss: 1.174978  [  224/  306]
train() client id: f_00004-5-7 loss: 1.109265  [  256/  306]
train() client id: f_00004-5-8 loss: 1.087950  [  288/  306]
train() client id: f_00004-6-0 loss: 1.131535  [   32/  306]
train() client id: f_00004-6-1 loss: 1.136778  [   64/  306]
train() client id: f_00004-6-2 loss: 1.144812  [   96/  306]
train() client id: f_00004-6-3 loss: 1.186089  [  128/  306]
train() client id: f_00004-6-4 loss: 1.126657  [  160/  306]
train() client id: f_00004-6-5 loss: 1.057562  [  192/  306]
train() client id: f_00004-6-6 loss: 1.061355  [  224/  306]
train() client id: f_00004-6-7 loss: 1.088110  [  256/  306]
train() client id: f_00004-6-8 loss: 1.104131  [  288/  306]
train() client id: f_00004-7-0 loss: 1.126204  [   32/  306]
train() client id: f_00004-7-1 loss: 1.124897  [   64/  306]
train() client id: f_00004-7-2 loss: 1.148616  [   96/  306]
train() client id: f_00004-7-3 loss: 1.110829  [  128/  306]
train() client id: f_00004-7-4 loss: 1.030913  [  160/  306]
train() client id: f_00004-7-5 loss: 1.140496  [  192/  306]
train() client id: f_00004-7-6 loss: 1.130233  [  224/  306]
train() client id: f_00004-7-7 loss: 1.178831  [  256/  306]
train() client id: f_00004-7-8 loss: 1.123863  [  288/  306]
train() client id: f_00004-8-0 loss: 1.176549  [   32/  306]
train() client id: f_00004-8-1 loss: 1.241778  [   64/  306]
train() client id: f_00004-8-2 loss: 1.027963  [   96/  306]
train() client id: f_00004-8-3 loss: 1.198643  [  128/  306]
train() client id: f_00004-8-4 loss: 1.114606  [  160/  306]
train() client id: f_00004-8-5 loss: 1.147556  [  192/  306]
train() client id: f_00004-8-6 loss: 1.075544  [  224/  306]
train() client id: f_00004-8-7 loss: 1.047275  [  256/  306]
train() client id: f_00004-8-8 loss: 1.207989  [  288/  306]
train() client id: f_00004-9-0 loss: 1.116465  [   32/  306]
train() client id: f_00004-9-1 loss: 1.180093  [   64/  306]
train() client id: f_00004-9-2 loss: 1.164402  [   96/  306]
train() client id: f_00004-9-3 loss: 1.184383  [  128/  306]
train() client id: f_00004-9-4 loss: 1.175282  [  160/  306]
train() client id: f_00004-9-5 loss: 1.188461  [  192/  306]
train() client id: f_00004-9-6 loss: 1.140802  [  224/  306]
train() client id: f_00004-9-7 loss: 1.100117  [  256/  306]
train() client id: f_00004-9-8 loss: 1.062220  [  288/  306]
train() client id: f_00004-10-0 loss: 1.154950  [   32/  306]
train() client id: f_00004-10-1 loss: 1.128742  [   64/  306]
train() client id: f_00004-10-2 loss: 1.180431  [   96/  306]
train() client id: f_00004-10-3 loss: 1.220116  [  128/  306]
train() client id: f_00004-10-4 loss: 1.074348  [  160/  306]
train() client id: f_00004-10-5 loss: 1.104066  [  192/  306]
train() client id: f_00004-10-6 loss: 1.170507  [  224/  306]
train() client id: f_00004-10-7 loss: 1.147193  [  256/  306]
train() client id: f_00004-10-8 loss: 1.071959  [  288/  306]
train() client id: f_00004-11-0 loss: 1.154013  [   32/  306]
train() client id: f_00004-11-1 loss: 1.190967  [   64/  306]
train() client id: f_00004-11-2 loss: 1.158120  [   96/  306]
train() client id: f_00004-11-3 loss: 1.205721  [  128/  306]
train() client id: f_00004-11-4 loss: 1.088641  [  160/  306]
train() client id: f_00004-11-5 loss: 1.093362  [  192/  306]
train() client id: f_00004-11-6 loss: 1.200974  [  224/  306]
train() client id: f_00004-11-7 loss: 1.158034  [  256/  306]
train() client id: f_00004-11-8 loss: 1.098674  [  288/  306]
train() client id: f_00004-12-0 loss: 1.144811  [   32/  306]
train() client id: f_00004-12-1 loss: 1.203375  [   64/  306]
train() client id: f_00004-12-2 loss: 1.168006  [   96/  306]
train() client id: f_00004-12-3 loss: 1.138006  [  128/  306]
train() client id: f_00004-12-4 loss: 1.033802  [  160/  306]
train() client id: f_00004-12-5 loss: 1.106858  [  192/  306]
train() client id: f_00004-12-6 loss: 1.187936  [  224/  306]
train() client id: f_00004-12-7 loss: 1.275212  [  256/  306]
train() client id: f_00004-12-8 loss: 1.208045  [  288/  306]
train() client id: f_00005-0-0 loss: 1.170114  [   32/  146]
train() client id: f_00005-0-1 loss: 1.093175  [   64/  146]
train() client id: f_00005-0-2 loss: 1.037273  [   96/  146]
train() client id: f_00005-0-3 loss: 1.110820  [  128/  146]
train() client id: f_00005-1-0 loss: 1.073997  [   32/  146]
train() client id: f_00005-1-1 loss: 1.092444  [   64/  146]
train() client id: f_00005-1-2 loss: 1.063804  [   96/  146]
train() client id: f_00005-1-3 loss: 1.133300  [  128/  146]
train() client id: f_00005-2-0 loss: 1.043465  [   32/  146]
train() client id: f_00005-2-1 loss: 1.194774  [   64/  146]
train() client id: f_00005-2-2 loss: 1.129935  [   96/  146]
train() client id: f_00005-2-3 loss: 1.058099  [  128/  146]
train() client id: f_00005-3-0 loss: 1.140501  [   32/  146]
train() client id: f_00005-3-1 loss: 1.102170  [   64/  146]
train() client id: f_00005-3-2 loss: 1.058469  [   96/  146]
train() client id: f_00005-3-3 loss: 1.103528  [  128/  146]
train() client id: f_00005-4-0 loss: 1.013364  [   32/  146]
train() client id: f_00005-4-1 loss: 1.169135  [   64/  146]
train() client id: f_00005-4-2 loss: 1.081403  [   96/  146]
train() client id: f_00005-4-3 loss: 1.195379  [  128/  146]
train() client id: f_00005-5-0 loss: 1.135684  [   32/  146]
train() client id: f_00005-5-1 loss: 1.138646  [   64/  146]
train() client id: f_00005-5-2 loss: 1.168234  [   96/  146]
train() client id: f_00005-5-3 loss: 1.096765  [  128/  146]
train() client id: f_00005-6-0 loss: 1.095935  [   32/  146]
train() client id: f_00005-6-1 loss: 1.232970  [   64/  146]
train() client id: f_00005-6-2 loss: 1.158875  [   96/  146]
train() client id: f_00005-6-3 loss: 1.137658  [  128/  146]
train() client id: f_00005-7-0 loss: 1.195803  [   32/  146]
train() client id: f_00005-7-1 loss: 1.160706  [   64/  146]
train() client id: f_00005-7-2 loss: 1.165493  [   96/  146]
train() client id: f_00005-7-3 loss: 1.111026  [  128/  146]
train() client id: f_00005-8-0 loss: 1.092868  [   32/  146]
train() client id: f_00005-8-1 loss: 1.132673  [   64/  146]
train() client id: f_00005-8-2 loss: 1.278360  [   96/  146]
train() client id: f_00005-8-3 loss: 1.170928  [  128/  146]
train() client id: f_00005-9-0 loss: 1.272593  [   32/  146]
train() client id: f_00005-9-1 loss: 1.205216  [   64/  146]
train() client id: f_00005-9-2 loss: 1.167740  [   96/  146]
train() client id: f_00005-9-3 loss: 1.039455  [  128/  146]
train() client id: f_00005-10-0 loss: 1.228926  [   32/  146]
train() client id: f_00005-10-1 loss: 1.252796  [   64/  146]
train() client id: f_00005-10-2 loss: 1.063930  [   96/  146]
train() client id: f_00005-10-3 loss: 1.169917  [  128/  146]
train() client id: f_00005-11-0 loss: 1.105359  [   32/  146]
train() client id: f_00005-11-1 loss: 1.117276  [   64/  146]
train() client id: f_00005-11-2 loss: 1.293039  [   96/  146]
train() client id: f_00005-11-3 loss: 1.327108  [  128/  146]
train() client id: f_00005-12-0 loss: 1.270815  [   32/  146]
train() client id: f_00005-12-1 loss: 1.082478  [   64/  146]
train() client id: f_00005-12-2 loss: 1.389056  [   96/  146]
train() client id: f_00005-12-3 loss: 1.060174  [  128/  146]
train() client id: f_00006-0-0 loss: 1.059868  [   32/   54]
train() client id: f_00006-1-0 loss: 1.035321  [   32/   54]
train() client id: f_00006-2-0 loss: 1.025795  [   32/   54]
train() client id: f_00006-3-0 loss: 1.052392  [   32/   54]
train() client id: f_00006-4-0 loss: 1.043892  [   32/   54]
train() client id: f_00006-5-0 loss: 1.089005  [   32/   54]
train() client id: f_00006-6-0 loss: 1.023906  [   32/   54]
train() client id: f_00006-7-0 loss: 1.040926  [   32/   54]
train() client id: f_00006-8-0 loss: 1.057786  [   32/   54]
train() client id: f_00006-9-0 loss: 1.070258  [   32/   54]
train() client id: f_00006-10-0 loss: 1.064715  [   32/   54]
train() client id: f_00006-11-0 loss: 1.120352  [   32/   54]
train() client id: f_00006-12-0 loss: 1.069897  [   32/   54]
train() client id: f_00007-0-0 loss: 1.016805  [   32/  179]
train() client id: f_00007-0-1 loss: 1.091320  [   64/  179]
train() client id: f_00007-0-2 loss: 1.084863  [   96/  179]
train() client id: f_00007-0-3 loss: 1.062049  [  128/  179]
train() client id: f_00007-0-4 loss: 1.026670  [  160/  179]
train() client id: f_00007-1-0 loss: 1.045285  [   32/  179]
train() client id: f_00007-1-1 loss: 1.055796  [   64/  179]
train() client id: f_00007-1-2 loss: 1.066275  [   96/  179]
train() client id: f_00007-1-3 loss: 1.060415  [  128/  179]
train() client id: f_00007-1-4 loss: 1.059721  [  160/  179]
train() client id: f_00007-2-0 loss: 1.099007  [   32/  179]
train() client id: f_00007-2-1 loss: 1.042287  [   64/  179]
train() client id: f_00007-2-2 loss: 1.084576  [   96/  179]
train() client id: f_00007-2-3 loss: 1.052340  [  128/  179]
train() client id: f_00007-2-4 loss: 1.062479  [  160/  179]
train() client id: f_00007-3-0 loss: 1.083719  [   32/  179]
train() client id: f_00007-3-1 loss: 1.103363  [   64/  179]
train() client id: f_00007-3-2 loss: 1.105049  [   96/  179]
train() client id: f_00007-3-3 loss: 1.009814  [  128/  179]
train() client id: f_00007-3-4 loss: 1.106830  [  160/  179]
train() client id: f_00007-4-0 loss: 1.071463  [   32/  179]
train() client id: f_00007-4-1 loss: 1.050245  [   64/  179]
train() client id: f_00007-4-2 loss: 1.081827  [   96/  179]
train() client id: f_00007-4-3 loss: 1.149324  [  128/  179]
train() client id: f_00007-4-4 loss: 1.118766  [  160/  179]
train() client id: f_00007-5-0 loss: 1.167596  [   32/  179]
train() client id: f_00007-5-1 loss: 1.140715  [   64/  179]
train() client id: f_00007-5-2 loss: 1.113191  [   96/  179]
train() client id: f_00007-5-3 loss: 1.111380  [  128/  179]
train() client id: f_00007-5-4 loss: 1.079781  [  160/  179]
train() client id: f_00007-6-0 loss: 1.186062  [   32/  179]
train() client id: f_00007-6-1 loss: 1.193436  [   64/  179]
train() client id: f_00007-6-2 loss: 1.112518  [   96/  179]
train() client id: f_00007-6-3 loss: 1.172274  [  128/  179]
train() client id: f_00007-6-4 loss: 1.041079  [  160/  179]
train() client id: f_00007-7-0 loss: 1.131739  [   32/  179]
train() client id: f_00007-7-1 loss: 1.170156  [   64/  179]
train() client id: f_00007-7-2 loss: 1.182833  [   96/  179]
train() client id: f_00007-7-3 loss: 1.098978  [  128/  179]
train() client id: f_00007-7-4 loss: 1.115190  [  160/  179]
train() client id: f_00007-8-0 loss: 1.167336  [   32/  179]
train() client id: f_00007-8-1 loss: 1.308796  [   64/  179]
train() client id: f_00007-8-2 loss: 1.153404  [   96/  179]
train() client id: f_00007-8-3 loss: 1.184318  [  128/  179]
train() client id: f_00007-8-4 loss: 1.161195  [  160/  179]
train() client id: f_00007-9-0 loss: 1.331328  [   32/  179]
train() client id: f_00007-9-1 loss: 1.160058  [   64/  179]
train() client id: f_00007-9-2 loss: 1.200363  [   96/  179]
train() client id: f_00007-9-3 loss: 1.141781  [  128/  179]
train() client id: f_00007-9-4 loss: 1.222274  [  160/  179]
train() client id: f_00007-10-0 loss: 1.199033  [   32/  179]
train() client id: f_00007-10-1 loss: 1.234365  [   64/  179]
train() client id: f_00007-10-2 loss: 1.173102  [   96/  179]
train() client id: f_00007-10-3 loss: 1.318178  [  128/  179]
train() client id: f_00007-10-4 loss: 1.225613  [  160/  179]
train() client id: f_00007-11-0 loss: 1.256382  [   32/  179]
train() client id: f_00007-11-1 loss: 1.174044  [   64/  179]
train() client id: f_00007-11-2 loss: 1.229084  [   96/  179]
train() client id: f_00007-11-3 loss: 1.294200  [  128/  179]
train() client id: f_00007-11-4 loss: 1.217947  [  160/  179]
train() client id: f_00007-12-0 loss: 1.244689  [   32/  179]
train() client id: f_00007-12-1 loss: 1.271822  [   64/  179]
train() client id: f_00007-12-2 loss: 1.320699  [   96/  179]
train() client id: f_00007-12-3 loss: 1.243741  [  128/  179]
train() client id: f_00007-12-4 loss: 1.224530  [  160/  179]
train() client id: f_00008-0-0 loss: 0.966509  [   32/  130]
train() client id: f_00008-0-1 loss: 0.978871  [   64/  130]
train() client id: f_00008-0-2 loss: 1.008279  [   96/  130]
train() client id: f_00008-0-3 loss: 1.092632  [  128/  130]
train() client id: f_00008-1-0 loss: 1.009642  [   32/  130]
train() client id: f_00008-1-1 loss: 1.056078  [   64/  130]
train() client id: f_00008-1-2 loss: 1.041669  [   96/  130]
train() client id: f_00008-1-3 loss: 0.922619  [  128/  130]
train() client id: f_00008-2-0 loss: 1.048916  [   32/  130]
train() client id: f_00008-2-1 loss: 0.977707  [   64/  130]
train() client id: f_00008-2-2 loss: 1.033599  [   96/  130]
train() client id: f_00008-2-3 loss: 0.984042  [  128/  130]
train() client id: f_00008-3-0 loss: 1.084381  [   32/  130]
train() client id: f_00008-3-1 loss: 1.029150  [   64/  130]
train() client id: f_00008-3-2 loss: 0.978520  [   96/  130]
train() client id: f_00008-3-3 loss: 0.951020  [  128/  130]
train() client id: f_00008-4-0 loss: 1.018555  [   32/  130]
train() client id: f_00008-4-1 loss: 0.944174  [   64/  130]
train() client id: f_00008-4-2 loss: 0.999848  [   96/  130]
train() client id: f_00008-4-3 loss: 1.084999  [  128/  130]
train() client id: f_00008-5-0 loss: 0.998046  [   32/  130]
train() client id: f_00008-5-1 loss: 1.062383  [   64/  130]
train() client id: f_00008-5-2 loss: 1.033124  [   96/  130]
train() client id: f_00008-5-3 loss: 0.952405  [  128/  130]
train() client id: f_00008-6-0 loss: 1.035654  [   32/  130]
train() client id: f_00008-6-1 loss: 0.959035  [   64/  130]
train() client id: f_00008-6-2 loss: 0.988555  [   96/  130]
train() client id: f_00008-6-3 loss: 1.077764  [  128/  130]
train() client id: f_00008-7-0 loss: 1.076096  [   32/  130]
train() client id: f_00008-7-1 loss: 0.976291  [   64/  130]
train() client id: f_00008-7-2 loss: 1.025115  [   96/  130]
train() client id: f_00008-7-3 loss: 0.966318  [  128/  130]
train() client id: f_00008-8-0 loss: 0.943234  [   32/  130]
train() client id: f_00008-8-1 loss: 1.098920  [   64/  130]
train() client id: f_00008-8-2 loss: 0.962289  [   96/  130]
train() client id: f_00008-8-3 loss: 1.076921  [  128/  130]
train() client id: f_00008-9-0 loss: 0.923622  [   32/  130]
train() client id: f_00008-9-1 loss: 0.968035  [   64/  130]
train() client id: f_00008-9-2 loss: 1.052966  [   96/  130]
train() client id: f_00008-9-3 loss: 1.147376  [  128/  130]
train() client id: f_00008-10-0 loss: 1.000737  [   32/  130]
train() client id: f_00008-10-1 loss: 1.134160  [   64/  130]
train() client id: f_00008-10-2 loss: 1.004362  [   96/  130]
train() client id: f_00008-10-3 loss: 0.971167  [  128/  130]
train() client id: f_00008-11-0 loss: 1.030989  [   32/  130]
train() client id: f_00008-11-1 loss: 1.110992  [   64/  130]
train() client id: f_00008-11-2 loss: 0.945047  [   96/  130]
train() client id: f_00008-11-3 loss: 1.037949  [  128/  130]
train() client id: f_00008-12-0 loss: 1.161179  [   32/  130]
train() client id: f_00008-12-1 loss: 0.963248  [   64/  130]
train() client id: f_00008-12-2 loss: 1.023869  [   96/  130]
train() client id: f_00008-12-3 loss: 1.003496  [  128/  130]
train() client id: f_00009-0-0 loss: 0.978102  [   32/  118]
train() client id: f_00009-0-1 loss: 0.992484  [   64/  118]
train() client id: f_00009-0-2 loss: 0.989556  [   96/  118]
train() client id: f_00009-1-0 loss: 1.022264  [   32/  118]
train() client id: f_00009-1-1 loss: 1.043344  [   64/  118]
train() client id: f_00009-1-2 loss: 0.976775  [   96/  118]
train() client id: f_00009-2-0 loss: 0.968120  [   32/  118]
train() client id: f_00009-2-1 loss: 1.080312  [   64/  118]
train() client id: f_00009-2-2 loss: 1.013088  [   96/  118]
train() client id: f_00009-3-0 loss: 0.987108  [   32/  118]
train() client id: f_00009-3-1 loss: 1.066132  [   64/  118]
train() client id: f_00009-3-2 loss: 1.031976  [   96/  118]
train() client id: f_00009-4-0 loss: 1.029250  [   32/  118]
train() client id: f_00009-4-1 loss: 0.951742  [   64/  118]
train() client id: f_00009-4-2 loss: 1.079810  [   96/  118]
train() client id: f_00009-5-0 loss: 1.010517  [   32/  118]
train() client id: f_00009-5-1 loss: 1.022149  [   64/  118]
train() client id: f_00009-5-2 loss: 1.011131  [   96/  118]
train() client id: f_00009-6-0 loss: 1.037798  [   32/  118]
train() client id: f_00009-6-1 loss: 0.966188  [   64/  118]
train() client id: f_00009-6-2 loss: 1.113231  [   96/  118]
train() client id: f_00009-7-0 loss: 1.053220  [   32/  118]
train() client id: f_00009-7-1 loss: 0.995056  [   64/  118]
train() client id: f_00009-7-2 loss: 0.997433  [   96/  118]
train() client id: f_00009-8-0 loss: 1.075157  [   32/  118]
train() client id: f_00009-8-1 loss: 1.126538  [   64/  118]
train() client id: f_00009-8-2 loss: 1.041210  [   96/  118]
train() client id: f_00009-9-0 loss: 1.050810  [   32/  118]
train() client id: f_00009-9-1 loss: 1.074101  [   64/  118]
train() client id: f_00009-9-2 loss: 0.996267  [   96/  118]
train() client id: f_00009-10-0 loss: 1.078888  [   32/  118]
train() client id: f_00009-10-1 loss: 0.983761  [   64/  118]
train() client id: f_00009-10-2 loss: 1.089473  [   96/  118]
train() client id: f_00009-11-0 loss: 1.148387  [   32/  118]
train() client id: f_00009-11-1 loss: 1.051481  [   64/  118]
train() client id: f_00009-11-2 loss: 1.102619  [   96/  118]
train() client id: f_00009-12-0 loss: 1.046036  [   32/  118]
train() client id: f_00009-12-1 loss: 1.179024  [   64/  118]
train() client id: f_00009-12-2 loss: 1.052935  [   96/  118]
At round 0 accuracy: 0.4854111405835544
At round 0 training accuracy: 0.4473507712944333
At round 0 training loss: 1.0425885183936978
update_location
xs = [  1.0943416  -15.79968212  20.00902392  18.81129433 -34.02070377
 -21.04359014   2.55680806  -6.32485185   4.66397685 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  17.54482414   9.35018685
 -17.18584926   2.37501568 -14.17765202  17.56900603   4.00148178]
dists_uav = [101.5407992  102.42858035 101.99071065 103.25543883 106.04166294
 103.62521942 100.06087131 101.19787334 101.63868679 101.52381707]
dists_bs = [236.19434294 225.31573798 261.13798756 249.70973192 217.52016588
 246.24779551 247.64047589 253.45510377 238.88001683 232.77727406]
uav_gains = [9.62490767e-11 9.41770035e-11 9.51910990e-11 9.23028553e-11
 8.63584788e-11 9.14815864e-11 9.98476140e-11 9.70665613e-11
 9.60174958e-11 9.62893323e-11]
bs_gains = [2.50114868e-11 2.85415667e-11 1.88824323e-11 2.14030060e-11
 3.14989018e-11 2.22562249e-11 2.19075354e-11 2.05291596e-11
 2.42320740e-11 2.60531636e-11]
Round 1
-------------------------------
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86475814 22.70386864 10.69549308  3.82673858 26.17607366 12.62512831
  4.75712203 15.35951406 11.26321529 10.24142851]
obj_prev = 128.51334030366516
eta_min = 2.311289869413348e-09	eta_max = 0.9179068358889176
af = 27.184654844112664	bf = 2.038244896144109	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.038244896144109	zeta = 50.92141160980498	eta = 0.5338550913006942
af = 27.184654844112664	bf = 2.038244896144109	zeta = 40.97475341706718	eta = 0.6634488941863763
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.19821510905128	eta = 0.693517670855259
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11235513228073	eta = 0.6950400903288041
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11214097403846	eta = 0.6950438960157429
eta = 0.6950438960157429
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.03001828 0.06313368 0.0295418  0.01024433 0.07290153 0.03478308
 0.01286498 0.04264501 0.03097124 0.02811235]
ene_total = [3.31978164 6.5061149  3.27744721 1.51901348 7.38134405 3.96296862
 1.74998819 4.47161385 3.59899574 3.32487331]
ti_comp = [0.26460659 0.24673409 0.26447904 0.26412078 0.24854123 0.24180084
 0.26502657 0.26470385 0.24355148 0.24498963]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.41454867e-05 2.58347910e-04 2.30360589e-05 9.63220790e-07
 3.92006301e-04 4.49850892e-05 1.89464606e-06 6.91773975e-05
 3.13021490e-05 2.31353594e-05]
ene_total = [0.58155701 0.76609949 0.58262017 0.58387567 0.76180371 0.79165776
 0.57569161 0.58478017 0.77442671 0.76055195]
optimize_network iter = 0 obj = 6.763064260913875
eta = 0.6950438960157429
freqs = [5.67224657e+07 1.27938699e+08 5.58490306e+07 1.93932720e+07
 1.46658825e+08 7.19250640e+07 2.42711055e+07 8.05523076e+07
 6.35825283e+07 5.73745792e+07]
eta_min = 0.6596845988039914	eta_max = 0.6950438960157337
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 0.07315436494122368	eta = 0.9090909090909091
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 22.49041741434538	eta = 0.0029569912778037635
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.432729834459089	eta = 0.02733717784291988
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3416402766936963	eta = 0.028400591154114474
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3415910621216036	eta = 0.028401188065745816
eta = 0.028401188065745816
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.36378699e-04 2.52916596e-03 2.25517659e-04 9.42970754e-06
 3.83765052e-03 4.40393562e-04 1.85481443e-05 6.77230634e-04
 3.06440760e-04 2.26489788e-04]
ene_total = [0.18861483 0.30552365 0.1886691  0.18350388 0.33783864 0.25980791
 0.18116974 0.20096826 0.2509534  0.24454163]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 1 obj = 6.069316940771649
eta = 0.6596845988039914
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
eta_min = 0.659684598803993	eta_max = 0.6596845988039878
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 0.072022402521563	eta = 0.9090909090909091
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 22.489338538927885	eta = 0.002911375595591902
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.427629129214501	eta = 0.026970722420201213
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.3378597367303393	eta = 0.028006347153576604
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.337812514296872	eta = 0.028006912865265216
eta = 0.028006912865265216
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.37212058e-04 2.49249066e-03 2.26285293e-04 9.45857837e-06
 3.78935809e-03 4.31651518e-04 1.86209531e-05 6.79681002e-04
 3.00945552e-04 2.22781478e-04]
ene_total = [0.18856219 0.30434926 0.18861456 0.18343028 0.3363184  0.25945213
 0.18109834 0.20095692 0.25069421 0.24433622]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 2 obj = 6.069316940771676
eta = 0.659684598803993
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
Done!
At round 1 energy consumption: 6.069316940771649
At round 1 eta: 0.659684598803993
At round 1 local rounds: 13.621738685881681
At round 1 global rounds: 81.80663264560978
At round 1 a_n: 27.49751116231574
gradient difference: 0.3845100402832031
train() client id: f_00000-0-0 loss: 1.273896  [   32/  126]
train() client id: f_00000-0-1 loss: 1.261794  [   64/  126]
train() client id: f_00000-0-2 loss: 1.115978  [   96/  126]
train() client id: f_00000-1-0 loss: 1.177351  [   32/  126]
train() client id: f_00000-1-1 loss: 1.123619  [   64/  126]
train() client id: f_00000-1-2 loss: 1.066474  [   96/  126]
train() client id: f_00000-2-0 loss: 1.097243  [   32/  126]
train() client id: f_00000-2-1 loss: 1.005001  [   64/  126]
train() client id: f_00000-2-2 loss: 1.018914  [   96/  126]
train() client id: f_00000-3-0 loss: 1.054217  [   32/  126]
train() client id: f_00000-3-1 loss: 0.950825  [   64/  126]
train() client id: f_00000-3-2 loss: 0.963881  [   96/  126]
train() client id: f_00000-4-0 loss: 0.949240  [   32/  126]
train() client id: f_00000-4-1 loss: 0.951413  [   64/  126]
train() client id: f_00000-4-2 loss: 0.963571  [   96/  126]
train() client id: f_00000-5-0 loss: 0.941882  [   32/  126]
train() client id: f_00000-5-1 loss: 0.943201  [   64/  126]
train() client id: f_00000-5-2 loss: 0.926995  [   96/  126]
train() client id: f_00000-6-0 loss: 0.915171  [   32/  126]
train() client id: f_00000-6-1 loss: 0.875769  [   64/  126]
train() client id: f_00000-6-2 loss: 0.910684  [   96/  126]
train() client id: f_00000-7-0 loss: 0.862893  [   32/  126]
train() client id: f_00000-7-1 loss: 0.881200  [   64/  126]
train() client id: f_00000-7-2 loss: 0.887129  [   96/  126]
train() client id: f_00000-8-0 loss: 0.843696  [   32/  126]
train() client id: f_00000-8-1 loss: 0.893066  [   64/  126]
train() client id: f_00000-8-2 loss: 0.868630  [   96/  126]
train() client id: f_00000-9-0 loss: 0.832731  [   32/  126]
train() client id: f_00000-9-1 loss: 0.799600  [   64/  126]
train() client id: f_00000-9-2 loss: 0.903938  [   96/  126]
train() client id: f_00000-10-0 loss: 0.884429  [   32/  126]
train() client id: f_00000-10-1 loss: 0.826503  [   64/  126]
train() client id: f_00000-10-2 loss: 0.822873  [   96/  126]
train() client id: f_00000-11-0 loss: 0.841348  [   32/  126]
train() client id: f_00000-11-1 loss: 0.809090  [   64/  126]
train() client id: f_00000-11-2 loss: 0.775554  [   96/  126]
train() client id: f_00000-12-0 loss: 0.798228  [   32/  126]
train() client id: f_00000-12-1 loss: 0.892046  [   64/  126]
train() client id: f_00000-12-2 loss: 0.779715  [   96/  126]
train() client id: f_00001-0-0 loss: 1.032725  [   32/  265]
train() client id: f_00001-0-1 loss: 0.976504  [   64/  265]
train() client id: f_00001-0-2 loss: 0.942626  [   96/  265]
train() client id: f_00001-0-3 loss: 0.932860  [  128/  265]
train() client id: f_00001-0-4 loss: 0.875904  [  160/  265]
train() client id: f_00001-0-5 loss: 0.905718  [  192/  265]
train() client id: f_00001-0-6 loss: 0.933177  [  224/  265]
train() client id: f_00001-0-7 loss: 0.873276  [  256/  265]
train() client id: f_00001-1-0 loss: 0.831679  [   32/  265]
train() client id: f_00001-1-1 loss: 0.857089  [   64/  265]
train() client id: f_00001-1-2 loss: 0.912484  [   96/  265]
train() client id: f_00001-1-3 loss: 0.779680  [  128/  265]
train() client id: f_00001-1-4 loss: 0.805075  [  160/  265]
train() client id: f_00001-1-5 loss: 0.788888  [  192/  265]
train() client id: f_00001-1-6 loss: 0.853998  [  224/  265]
train() client id: f_00001-1-7 loss: 0.748308  [  256/  265]
train() client id: f_00001-2-0 loss: 0.789994  [   32/  265]
train() client id: f_00001-2-1 loss: 0.807313  [   64/  265]
train() client id: f_00001-2-2 loss: 0.783757  [   96/  265]
train() client id: f_00001-2-3 loss: 0.751100  [  128/  265]
train() client id: f_00001-2-4 loss: 0.758385  [  160/  265]
train() client id: f_00001-2-5 loss: 0.688294  [  192/  265]
train() client id: f_00001-2-6 loss: 0.650369  [  224/  265]
train() client id: f_00001-2-7 loss: 0.699394  [  256/  265]
train() client id: f_00001-3-0 loss: 0.801480  [   32/  265]
train() client id: f_00001-3-1 loss: 0.733886  [   64/  265]
train() client id: f_00001-3-2 loss: 0.650692  [   96/  265]
train() client id: f_00001-3-3 loss: 0.657142  [  128/  265]
train() client id: f_00001-3-4 loss: 0.627245  [  160/  265]
train() client id: f_00001-3-5 loss: 0.676087  [  192/  265]
train() client id: f_00001-3-6 loss: 0.669335  [  224/  265]
train() client id: f_00001-3-7 loss: 0.633435  [  256/  265]
train() client id: f_00001-4-0 loss: 0.623344  [   32/  265]
train() client id: f_00001-4-1 loss: 0.668587  [   64/  265]
train() client id: f_00001-4-2 loss: 0.597391  [   96/  265]
train() client id: f_00001-4-3 loss: 0.651500  [  128/  265]
train() client id: f_00001-4-4 loss: 0.662739  [  160/  265]
train() client id: f_00001-4-5 loss: 0.723210  [  192/  265]
train() client id: f_00001-4-6 loss: 0.578284  [  224/  265]
train() client id: f_00001-4-7 loss: 0.602969  [  256/  265]
train() client id: f_00001-5-0 loss: 0.597628  [   32/  265]
train() client id: f_00001-5-1 loss: 0.682416  [   64/  265]
train() client id: f_00001-5-2 loss: 0.677519  [   96/  265]
train() client id: f_00001-5-3 loss: 0.576761  [  128/  265]
train() client id: f_00001-5-4 loss: 0.541371  [  160/  265]
train() client id: f_00001-5-5 loss: 0.678999  [  192/  265]
train() client id: f_00001-5-6 loss: 0.593834  [  224/  265]
train() client id: f_00001-5-7 loss: 0.538226  [  256/  265]
train() client id: f_00001-6-0 loss: 0.572516  [   32/  265]
train() client id: f_00001-6-1 loss: 0.588419  [   64/  265]
train() client id: f_00001-6-2 loss: 0.567177  [   96/  265]
train() client id: f_00001-6-3 loss: 0.650907  [  128/  265]
train() client id: f_00001-6-4 loss: 0.565210  [  160/  265]
train() client id: f_00001-6-5 loss: 0.577643  [  192/  265]
train() client id: f_00001-6-6 loss: 0.588915  [  224/  265]
train() client id: f_00001-6-7 loss: 0.563304  [  256/  265]
train() client id: f_00001-7-0 loss: 0.596881  [   32/  265]
train() client id: f_00001-7-1 loss: 0.650781  [   64/  265]
train() client id: f_00001-7-2 loss: 0.562271  [   96/  265]
train() client id: f_00001-7-3 loss: 0.492162  [  128/  265]
train() client id: f_00001-7-4 loss: 0.608851  [  160/  265]
train() client id: f_00001-7-5 loss: 0.563152  [  192/  265]
train() client id: f_00001-7-6 loss: 0.562698  [  224/  265]
train() client id: f_00001-7-7 loss: 0.538883  [  256/  265]
train() client id: f_00001-8-0 loss: 0.612611  [   32/  265]
train() client id: f_00001-8-1 loss: 0.508320  [   64/  265]
train() client id: f_00001-8-2 loss: 0.550391  [   96/  265]
train() client id: f_00001-8-3 loss: 0.512542  [  128/  265]
train() client id: f_00001-8-4 loss: 0.570258  [  160/  265]
train() client id: f_00001-8-5 loss: 0.546084  [  192/  265]
train() client id: f_00001-8-6 loss: 0.599900  [  224/  265]
train() client id: f_00001-8-7 loss: 0.522933  [  256/  265]
train() client id: f_00001-9-0 loss: 0.476078  [   32/  265]
train() client id: f_00001-9-1 loss: 0.598730  [   64/  265]
train() client id: f_00001-9-2 loss: 0.508663  [   96/  265]
train() client id: f_00001-9-3 loss: 0.570576  [  128/  265]
train() client id: f_00001-9-4 loss: 0.597545  [  160/  265]
train() client id: f_00001-9-5 loss: 0.520762  [  192/  265]
train() client id: f_00001-9-6 loss: 0.549234  [  224/  265]
train() client id: f_00001-9-7 loss: 0.540127  [  256/  265]
train() client id: f_00001-10-0 loss: 0.634729  [   32/  265]
train() client id: f_00001-10-1 loss: 0.547781  [   64/  265]
train() client id: f_00001-10-2 loss: 0.585618  [   96/  265]
train() client id: f_00001-10-3 loss: 0.480999  [  128/  265]
train() client id: f_00001-10-4 loss: 0.504466  [  160/  265]
train() client id: f_00001-10-5 loss: 0.577759  [  192/  265]
train() client id: f_00001-10-6 loss: 0.480147  [  224/  265]
train() client id: f_00001-10-7 loss: 0.507820  [  256/  265]
train() client id: f_00001-11-0 loss: 0.535813  [   32/  265]
train() client id: f_00001-11-1 loss: 0.619494  [   64/  265]
train() client id: f_00001-11-2 loss: 0.482495  [   96/  265]
train() client id: f_00001-11-3 loss: 0.509325  [  128/  265]
train() client id: f_00001-11-4 loss: 0.471049  [  160/  265]
train() client id: f_00001-11-5 loss: 0.675743  [  192/  265]
train() client id: f_00001-11-6 loss: 0.502442  [  224/  265]
train() client id: f_00001-11-7 loss: 0.517178  [  256/  265]
train() client id: f_00001-12-0 loss: 0.559808  [   32/  265]
train() client id: f_00001-12-1 loss: 0.502285  [   64/  265]
train() client id: f_00001-12-2 loss: 0.461744  [   96/  265]
train() client id: f_00001-12-3 loss: 0.553877  [  128/  265]
train() client id: f_00001-12-4 loss: 0.608321  [  160/  265]
train() client id: f_00001-12-5 loss: 0.521023  [  192/  265]
train() client id: f_00001-12-6 loss: 0.508864  [  224/  265]
train() client id: f_00001-12-7 loss: 0.574041  [  256/  265]
train() client id: f_00002-0-0 loss: 0.989081  [   32/  124]
train() client id: f_00002-0-1 loss: 1.038708  [   64/  124]
train() client id: f_00002-0-2 loss: 0.969650  [   96/  124]
train() client id: f_00002-1-0 loss: 0.972886  [   32/  124]
train() client id: f_00002-1-1 loss: 1.013265  [   64/  124]
train() client id: f_00002-1-2 loss: 0.925902  [   96/  124]
train() client id: f_00002-2-0 loss: 0.971379  [   32/  124]
train() client id: f_00002-2-1 loss: 0.973981  [   64/  124]
train() client id: f_00002-2-2 loss: 0.938010  [   96/  124]
train() client id: f_00002-3-0 loss: 0.947601  [   32/  124]
train() client id: f_00002-3-1 loss: 0.940695  [   64/  124]
train() client id: f_00002-3-2 loss: 0.938774  [   96/  124]
train() client id: f_00002-4-0 loss: 0.965758  [   32/  124]
train() client id: f_00002-4-1 loss: 0.951737  [   64/  124]
train() client id: f_00002-4-2 loss: 0.866934  [   96/  124]
train() client id: f_00002-5-0 loss: 0.871983  [   32/  124]
train() client id: f_00002-5-1 loss: 0.983121  [   64/  124]
train() client id: f_00002-5-2 loss: 0.892929  [   96/  124]
train() client id: f_00002-6-0 loss: 0.910064  [   32/  124]
train() client id: f_00002-6-1 loss: 0.896743  [   64/  124]
train() client id: f_00002-6-2 loss: 0.922828  [   96/  124]
train() client id: f_00002-7-0 loss: 0.872382  [   32/  124]
train() client id: f_00002-7-1 loss: 0.855235  [   64/  124]
train() client id: f_00002-7-2 loss: 0.935478  [   96/  124]
train() client id: f_00002-8-0 loss: 0.893972  [   32/  124]
train() client id: f_00002-8-1 loss: 0.842806  [   64/  124]
train() client id: f_00002-8-2 loss: 0.913860  [   96/  124]
train() client id: f_00002-9-0 loss: 0.870077  [   32/  124]
train() client id: f_00002-9-1 loss: 0.832682  [   64/  124]
train() client id: f_00002-9-2 loss: 0.866362  [   96/  124]
train() client id: f_00002-10-0 loss: 0.885061  [   32/  124]
train() client id: f_00002-10-1 loss: 0.887121  [   64/  124]
train() client id: f_00002-10-2 loss: 0.848328  [   96/  124]
train() client id: f_00002-11-0 loss: 0.906563  [   32/  124]
train() client id: f_00002-11-1 loss: 0.805414  [   64/  124]
train() client id: f_00002-11-2 loss: 0.890625  [   96/  124]
train() client id: f_00002-12-0 loss: 0.916253  [   32/  124]
train() client id: f_00002-12-1 loss: 0.849704  [   64/  124]
train() client id: f_00002-12-2 loss: 0.856335  [   96/  124]
train() client id: f_00003-0-0 loss: 1.130730  [   32/   43]
train() client id: f_00003-1-0 loss: 1.173623  [   32/   43]
train() client id: f_00003-2-0 loss: 1.111536  [   32/   43]
train() client id: f_00003-3-0 loss: 1.121342  [   32/   43]
train() client id: f_00003-4-0 loss: 1.141266  [   32/   43]
train() client id: f_00003-5-0 loss: 1.112820  [   32/   43]
train() client id: f_00003-6-0 loss: 1.086609  [   32/   43]
train() client id: f_00003-7-0 loss: 1.200347  [   32/   43]
train() client id: f_00003-8-0 loss: 1.104422  [   32/   43]
train() client id: f_00003-9-0 loss: 1.092552  [   32/   43]
train() client id: f_00003-10-0 loss: 1.138169  [   32/   43]
train() client id: f_00003-11-0 loss: 1.173564  [   32/   43]
train() client id: f_00003-12-0 loss: 1.158506  [   32/   43]
train() client id: f_00004-0-0 loss: 1.164632  [   32/  306]
train() client id: f_00004-0-1 loss: 1.132521  [   64/  306]
train() client id: f_00004-0-2 loss: 1.070178  [   96/  306]
train() client id: f_00004-0-3 loss: 1.100258  [  128/  306]
train() client id: f_00004-0-4 loss: 1.158240  [  160/  306]
train() client id: f_00004-0-5 loss: 1.093923  [  192/  306]
train() client id: f_00004-0-6 loss: 1.116272  [  224/  306]
train() client id: f_00004-0-7 loss: 1.087812  [  256/  306]
train() client id: f_00004-0-8 loss: 1.037424  [  288/  306]
train() client id: f_00004-1-0 loss: 1.042708  [   32/  306]
train() client id: f_00004-1-1 loss: 1.134239  [   64/  306]
train() client id: f_00004-1-2 loss: 1.060286  [   96/  306]
train() client id: f_00004-1-3 loss: 1.091779  [  128/  306]
train() client id: f_00004-1-4 loss: 1.064894  [  160/  306]
train() client id: f_00004-1-5 loss: 1.130060  [  192/  306]
train() client id: f_00004-1-6 loss: 1.050996  [  224/  306]
train() client id: f_00004-1-7 loss: 1.078438  [  256/  306]
train() client id: f_00004-1-8 loss: 1.030862  [  288/  306]
train() client id: f_00004-2-0 loss: 1.136726  [   32/  306]
train() client id: f_00004-2-1 loss: 1.012403  [   64/  306]
train() client id: f_00004-2-2 loss: 1.058223  [   96/  306]
train() client id: f_00004-2-3 loss: 1.031612  [  128/  306]
train() client id: f_00004-2-4 loss: 1.027701  [  160/  306]
train() client id: f_00004-2-5 loss: 1.037798  [  192/  306]
train() client id: f_00004-2-6 loss: 1.033722  [  224/  306]
train() client id: f_00004-2-7 loss: 1.066554  [  256/  306]
train() client id: f_00004-2-8 loss: 1.032507  [  288/  306]
train() client id: f_00004-3-0 loss: 1.053496  [   32/  306]
train() client id: f_00004-3-1 loss: 1.032921  [   64/  306]
train() client id: f_00004-3-2 loss: 1.006498  [   96/  306]
train() client id: f_00004-3-3 loss: 1.010182  [  128/  306]
train() client id: f_00004-3-4 loss: 1.038096  [  160/  306]
train() client id: f_00004-3-5 loss: 1.049513  [  192/  306]
train() client id: f_00004-3-6 loss: 1.004143  [  224/  306]
train() client id: f_00004-3-7 loss: 1.049249  [  256/  306]
train() client id: f_00004-3-8 loss: 1.017396  [  288/  306]
train() client id: f_00004-4-0 loss: 0.989933  [   32/  306]
train() client id: f_00004-4-1 loss: 1.007392  [   64/  306]
train() client id: f_00004-4-2 loss: 1.028580  [   96/  306]
train() client id: f_00004-4-3 loss: 1.054992  [  128/  306]
train() client id: f_00004-4-4 loss: 1.007170  [  160/  306]
train() client id: f_00004-4-5 loss: 1.011369  [  192/  306]
train() client id: f_00004-4-6 loss: 1.039700  [  224/  306]
train() client id: f_00004-4-7 loss: 0.973832  [  256/  306]
train() client id: f_00004-4-8 loss: 0.972368  [  288/  306]
train() client id: f_00004-5-0 loss: 1.028879  [   32/  306]
train() client id: f_00004-5-1 loss: 1.024366  [   64/  306]
train() client id: f_00004-5-2 loss: 1.019969  [   96/  306]
train() client id: f_00004-5-3 loss: 0.976577  [  128/  306]
train() client id: f_00004-5-4 loss: 0.998133  [  160/  306]
train() client id: f_00004-5-5 loss: 0.957379  [  192/  306]
train() client id: f_00004-5-6 loss: 1.026198  [  224/  306]
train() client id: f_00004-5-7 loss: 0.986643  [  256/  306]
train() client id: f_00004-5-8 loss: 1.038390  [  288/  306]
train() client id: f_00004-6-0 loss: 0.986831  [   32/  306]
train() client id: f_00004-6-1 loss: 0.966131  [   64/  306]
train() client id: f_00004-6-2 loss: 1.040928  [   96/  306]
train() client id: f_00004-6-3 loss: 0.996378  [  128/  306]
train() client id: f_00004-6-4 loss: 0.993822  [  160/  306]
train() client id: f_00004-6-5 loss: 0.925876  [  192/  306]
train() client id: f_00004-6-6 loss: 0.943120  [  224/  306]
train() client id: f_00004-6-7 loss: 1.064780  [  256/  306]
train() client id: f_00004-6-8 loss: 0.934897  [  288/  306]
train() client id: f_00004-7-0 loss: 1.023020  [   32/  306]
train() client id: f_00004-7-1 loss: 1.034372  [   64/  306]
train() client id: f_00004-7-2 loss: 0.943865  [   96/  306]
train() client id: f_00004-7-3 loss: 0.985542  [  128/  306]
train() client id: f_00004-7-4 loss: 1.020548  [  160/  306]
train() client id: f_00004-7-5 loss: 0.921690  [  192/  306]
train() client id: f_00004-7-6 loss: 0.880250  [  224/  306]
train() client id: f_00004-7-7 loss: 1.070483  [  256/  306]
train() client id: f_00004-7-8 loss: 0.955014  [  288/  306]
train() client id: f_00004-8-0 loss: 1.011291  [   32/  306]
train() client id: f_00004-8-1 loss: 0.888539  [   64/  306]
train() client id: f_00004-8-2 loss: 0.995224  [   96/  306]
train() client id: f_00004-8-3 loss: 0.932003  [  128/  306]
train() client id: f_00004-8-4 loss: 0.964462  [  160/  306]
train() client id: f_00004-8-5 loss: 1.003908  [  192/  306]
train() client id: f_00004-8-6 loss: 0.938651  [  224/  306]
train() client id: f_00004-8-7 loss: 1.008451  [  256/  306]
train() client id: f_00004-8-8 loss: 0.994232  [  288/  306]
train() client id: f_00004-9-0 loss: 1.049286  [   32/  306]
train() client id: f_00004-9-1 loss: 1.029202  [   64/  306]
train() client id: f_00004-9-2 loss: 0.969274  [   96/  306]
train() client id: f_00004-9-3 loss: 0.969663  [  128/  306]
train() client id: f_00004-9-4 loss: 0.995485  [  160/  306]
train() client id: f_00004-9-5 loss: 0.940421  [  192/  306]
train() client id: f_00004-9-6 loss: 0.913456  [  224/  306]
train() client id: f_00004-9-7 loss: 0.933432  [  256/  306]
train() client id: f_00004-9-8 loss: 0.919926  [  288/  306]
train() client id: f_00004-10-0 loss: 1.037655  [   32/  306]
train() client id: f_00004-10-1 loss: 1.021253  [   64/  306]
train() client id: f_00004-10-2 loss: 0.893701  [   96/  306]
train() client id: f_00004-10-3 loss: 0.910404  [  128/  306]
train() client id: f_00004-10-4 loss: 0.960689  [  160/  306]
train() client id: f_00004-10-5 loss: 0.875121  [  192/  306]
train() client id: f_00004-10-6 loss: 1.023952  [  224/  306]
train() client id: f_00004-10-7 loss: 0.984559  [  256/  306]
train() client id: f_00004-10-8 loss: 0.945373  [  288/  306]
train() client id: f_00004-11-0 loss: 1.055011  [   32/  306]
train() client id: f_00004-11-1 loss: 1.048897  [   64/  306]
train() client id: f_00004-11-2 loss: 1.017481  [   96/  306]
train() client id: f_00004-11-3 loss: 0.882425  [  128/  306]
train() client id: f_00004-11-4 loss: 0.949403  [  160/  306]
train() client id: f_00004-11-5 loss: 0.972227  [  192/  306]
train() client id: f_00004-11-6 loss: 0.938522  [  224/  306]
train() client id: f_00004-11-7 loss: 0.848580  [  256/  306]
train() client id: f_00004-11-8 loss: 0.947766  [  288/  306]
train() client id: f_00004-12-0 loss: 0.991662  [   32/  306]
train() client id: f_00004-12-1 loss: 0.949362  [   64/  306]
train() client id: f_00004-12-2 loss: 0.916246  [   96/  306]
train() client id: f_00004-12-3 loss: 1.058107  [  128/  306]
train() client id: f_00004-12-4 loss: 0.999221  [  160/  306]
train() client id: f_00004-12-5 loss: 0.961362  [  192/  306]
train() client id: f_00004-12-6 loss: 0.888041  [  224/  306]
train() client id: f_00004-12-7 loss: 0.843940  [  256/  306]
train() client id: f_00004-12-8 loss: 0.990492  [  288/  306]
train() client id: f_00005-0-0 loss: 0.985721  [   32/  146]
train() client id: f_00005-0-1 loss: 1.016521  [   64/  146]
train() client id: f_00005-0-2 loss: 0.999362  [   96/  146]
train() client id: f_00005-0-3 loss: 1.024754  [  128/  146]
train() client id: f_00005-1-0 loss: 1.003490  [   32/  146]
train() client id: f_00005-1-1 loss: 0.978937  [   64/  146]
train() client id: f_00005-1-2 loss: 0.962630  [   96/  146]
train() client id: f_00005-1-3 loss: 0.995745  [  128/  146]
train() client id: f_00005-2-0 loss: 0.958048  [   32/  146]
train() client id: f_00005-2-1 loss: 0.983504  [   64/  146]
train() client id: f_00005-2-2 loss: 0.902143  [   96/  146]
train() client id: f_00005-2-3 loss: 0.975947  [  128/  146]
train() client id: f_00005-3-0 loss: 0.920293  [   32/  146]
train() client id: f_00005-3-1 loss: 1.009331  [   64/  146]
train() client id: f_00005-3-2 loss: 0.939457  [   96/  146]
train() client id: f_00005-3-3 loss: 0.946344  [  128/  146]
train() client id: f_00005-4-0 loss: 0.953866  [   32/  146]
train() client id: f_00005-4-1 loss: 0.984843  [   64/  146]
train() client id: f_00005-4-2 loss: 0.892738  [   96/  146]
train() client id: f_00005-4-3 loss: 0.886149  [  128/  146]
train() client id: f_00005-5-0 loss: 0.901949  [   32/  146]
train() client id: f_00005-5-1 loss: 1.018785  [   64/  146]
train() client id: f_00005-5-2 loss: 0.910440  [   96/  146]
train() client id: f_00005-5-3 loss: 0.969056  [  128/  146]
train() client id: f_00005-6-0 loss: 0.940689  [   32/  146]
train() client id: f_00005-6-1 loss: 0.991574  [   64/  146]
train() client id: f_00005-6-2 loss: 0.863656  [   96/  146]
train() client id: f_00005-6-3 loss: 0.904764  [  128/  146]
train() client id: f_00005-7-0 loss: 0.971204  [   32/  146]
train() client id: f_00005-7-1 loss: 0.874498  [   64/  146]
train() client id: f_00005-7-2 loss: 0.946993  [   96/  146]
train() client id: f_00005-7-3 loss: 0.891913  [  128/  146]
train() client id: f_00005-8-0 loss: 0.885491  [   32/  146]
train() client id: f_00005-8-1 loss: 0.887757  [   64/  146]
train() client id: f_00005-8-2 loss: 0.890606  [   96/  146]
train() client id: f_00005-8-3 loss: 0.880546  [  128/  146]
train() client id: f_00005-9-0 loss: 0.939288  [   32/  146]
train() client id: f_00005-9-1 loss: 0.896273  [   64/  146]
train() client id: f_00005-9-2 loss: 0.779459  [   96/  146]
train() client id: f_00005-9-3 loss: 1.092065  [  128/  146]
train() client id: f_00005-10-0 loss: 0.898270  [   32/  146]
train() client id: f_00005-10-1 loss: 0.864470  [   64/  146]
train() client id: f_00005-10-2 loss: 0.918572  [   96/  146]
train() client id: f_00005-10-3 loss: 0.908863  [  128/  146]
train() client id: f_00005-11-0 loss: 0.905600  [   32/  146]
train() client id: f_00005-11-1 loss: 0.999509  [   64/  146]
train() client id: f_00005-11-2 loss: 0.846379  [   96/  146]
train() client id: f_00005-11-3 loss: 0.861764  [  128/  146]
train() client id: f_00005-12-0 loss: 0.827683  [   32/  146]
train() client id: f_00005-12-1 loss: 0.919319  [   64/  146]
train() client id: f_00005-12-2 loss: 1.017435  [   96/  146]
train() client id: f_00005-12-3 loss: 0.879046  [  128/  146]
train() client id: f_00006-0-0 loss: 1.070152  [   32/   54]
train() client id: f_00006-1-0 loss: 1.106780  [   32/   54]
train() client id: f_00006-2-0 loss: 1.081227  [   32/   54]
train() client id: f_00006-3-0 loss: 1.061620  [   32/   54]
train() client id: f_00006-4-0 loss: 1.051362  [   32/   54]
train() client id: f_00006-5-0 loss: 1.077023  [   32/   54]
train() client id: f_00006-6-0 loss: 1.034831  [   32/   54]
train() client id: f_00006-7-0 loss: 1.088553  [   32/   54]
train() client id: f_00006-8-0 loss: 1.076108  [   32/   54]
train() client id: f_00006-9-0 loss: 1.097733  [   32/   54]
train() client id: f_00006-10-0 loss: 1.103927  [   32/   54]
train() client id: f_00006-11-0 loss: 1.040449  [   32/   54]
train() client id: f_00006-12-0 loss: 1.101614  [   32/   54]
train() client id: f_00007-0-0 loss: 1.108647  [   32/  179]
train() client id: f_00007-0-1 loss: 1.037742  [   64/  179]
train() client id: f_00007-0-2 loss: 1.071161  [   96/  179]
train() client id: f_00007-0-3 loss: 1.029188  [  128/  179]
train() client id: f_00007-0-4 loss: 0.987114  [  160/  179]
train() client id: f_00007-1-0 loss: 1.031922  [   32/  179]
train() client id: f_00007-1-1 loss: 0.960528  [   64/  179]
train() client id: f_00007-1-2 loss: 0.926041  [   96/  179]
train() client id: f_00007-1-3 loss: 0.979625  [  128/  179]
train() client id: f_00007-1-4 loss: 0.915514  [  160/  179]
train() client id: f_00007-2-0 loss: 0.923731  [   32/  179]
train() client id: f_00007-2-1 loss: 0.931932  [   64/  179]
train() client id: f_00007-2-2 loss: 0.899370  [   96/  179]
train() client id: f_00007-2-3 loss: 0.880525  [  128/  179]
train() client id: f_00007-2-4 loss: 0.872783  [  160/  179]
train() client id: f_00007-3-0 loss: 0.900308  [   32/  179]
train() client id: f_00007-3-1 loss: 0.849234  [   64/  179]
train() client id: f_00007-3-2 loss: 0.809618  [   96/  179]
train() client id: f_00007-3-3 loss: 0.852340  [  128/  179]
train() client id: f_00007-3-4 loss: 0.807874  [  160/  179]
train() client id: f_00007-4-0 loss: 0.821251  [   32/  179]
train() client id: f_00007-4-1 loss: 0.876492  [   64/  179]
train() client id: f_00007-4-2 loss: 0.823253  [   96/  179]
train() client id: f_00007-4-3 loss: 0.756754  [  128/  179]
train() client id: f_00007-4-4 loss: 0.762917  [  160/  179]
train() client id: f_00007-5-0 loss: 0.757130  [   32/  179]
train() client id: f_00007-5-1 loss: 0.749849  [   64/  179]
train() client id: f_00007-5-2 loss: 0.830611  [   96/  179]
train() client id: f_00007-5-3 loss: 0.829198  [  128/  179]
train() client id: f_00007-5-4 loss: 0.713895  [  160/  179]
train() client id: f_00007-6-0 loss: 0.758136  [   32/  179]
train() client id: f_00007-6-1 loss: 0.744632  [   64/  179]
train() client id: f_00007-6-2 loss: 0.697854  [   96/  179]
train() client id: f_00007-6-3 loss: 0.808040  [  128/  179]
train() client id: f_00007-6-4 loss: 0.759563  [  160/  179]
train() client id: f_00007-7-0 loss: 0.778467  [   32/  179]
train() client id: f_00007-7-1 loss: 0.700554  [   64/  179]
train() client id: f_00007-7-2 loss: 0.700272  [   96/  179]
train() client id: f_00007-7-3 loss: 0.693564  [  128/  179]
train() client id: f_00007-7-4 loss: 0.786857  [  160/  179]
train() client id: f_00007-8-0 loss: 0.682394  [   32/  179]
train() client id: f_00007-8-1 loss: 0.713555  [   64/  179]
train() client id: f_00007-8-2 loss: 0.756409  [   96/  179]
train() client id: f_00007-8-3 loss: 0.715976  [  128/  179]
train() client id: f_00007-8-4 loss: 0.739174  [  160/  179]
train() client id: f_00007-9-0 loss: 0.705614  [   32/  179]
train() client id: f_00007-9-1 loss: 0.731843  [   64/  179]
train() client id: f_00007-9-2 loss: 0.734387  [   96/  179]
train() client id: f_00007-9-3 loss: 0.710250  [  128/  179]
train() client id: f_00007-9-4 loss: 0.700792  [  160/  179]
train() client id: f_00007-10-0 loss: 0.689881  [   32/  179]
train() client id: f_00007-10-1 loss: 0.724077  [   64/  179]
train() client id: f_00007-10-2 loss: 0.700105  [   96/  179]
train() client id: f_00007-10-3 loss: 0.728368  [  128/  179]
train() client id: f_00007-10-4 loss: 0.677144  [  160/  179]
train() client id: f_00007-11-0 loss: 0.709695  [   32/  179]
train() client id: f_00007-11-1 loss: 0.683831  [   64/  179]
train() client id: f_00007-11-2 loss: 0.694841  [   96/  179]
train() client id: f_00007-11-3 loss: 0.712637  [  128/  179]
train() client id: f_00007-11-4 loss: 0.671133  [  160/  179]
train() client id: f_00007-12-0 loss: 0.759160  [   32/  179]
train() client id: f_00007-12-1 loss: 0.661354  [   64/  179]
train() client id: f_00007-12-2 loss: 0.665369  [   96/  179]
train() client id: f_00007-12-3 loss: 0.625652  [  128/  179]
train() client id: f_00007-12-4 loss: 0.692815  [  160/  179]
train() client id: f_00008-0-0 loss: 1.032754  [   32/  130]
train() client id: f_00008-0-1 loss: 0.992912  [   64/  130]
train() client id: f_00008-0-2 loss: 0.960193  [   96/  130]
train() client id: f_00008-0-3 loss: 0.965937  [  128/  130]
train() client id: f_00008-1-0 loss: 0.992728  [   32/  130]
train() client id: f_00008-1-1 loss: 1.014845  [   64/  130]
train() client id: f_00008-1-2 loss: 0.952948  [   96/  130]
train() client id: f_00008-1-3 loss: 0.955838  [  128/  130]
train() client id: f_00008-2-0 loss: 0.978449  [   32/  130]
train() client id: f_00008-2-1 loss: 0.964519  [   64/  130]
train() client id: f_00008-2-2 loss: 1.003879  [   96/  130]
train() client id: f_00008-2-3 loss: 0.966990  [  128/  130]
train() client id: f_00008-3-0 loss: 0.906904  [   32/  130]
train() client id: f_00008-3-1 loss: 1.077893  [   64/  130]
train() client id: f_00008-3-2 loss: 0.971555  [   96/  130]
train() client id: f_00008-3-3 loss: 0.927002  [  128/  130]
train() client id: f_00008-4-0 loss: 1.065276  [   32/  130]
train() client id: f_00008-4-1 loss: 0.913895  [   64/  130]
train() client id: f_00008-4-2 loss: 0.927435  [   96/  130]
train() client id: f_00008-4-3 loss: 0.952868  [  128/  130]
train() client id: f_00008-5-0 loss: 0.919792  [   32/  130]
train() client id: f_00008-5-1 loss: 0.977949  [   64/  130]
train() client id: f_00008-5-2 loss: 0.896038  [   96/  130]
train() client id: f_00008-5-3 loss: 1.059135  [  128/  130]
train() client id: f_00008-6-0 loss: 0.971386  [   32/  130]
train() client id: f_00008-6-1 loss: 0.992374  [   64/  130]
train() client id: f_00008-6-2 loss: 0.976991  [   96/  130]
train() client id: f_00008-6-3 loss: 0.916482  [  128/  130]
train() client id: f_00008-7-0 loss: 0.930539  [   32/  130]
train() client id: f_00008-7-1 loss: 0.957727  [   64/  130]
train() client id: f_00008-7-2 loss: 0.976467  [   96/  130]
train() client id: f_00008-7-3 loss: 1.013766  [  128/  130]
train() client id: f_00008-8-0 loss: 0.908076  [   32/  130]
train() client id: f_00008-8-1 loss: 0.985374  [   64/  130]
train() client id: f_00008-8-2 loss: 0.978554  [   96/  130]
train() client id: f_00008-8-3 loss: 1.005131  [  128/  130]
train() client id: f_00008-9-0 loss: 1.071313  [   32/  130]
train() client id: f_00008-9-1 loss: 1.047078  [   64/  130]
train() client id: f_00008-9-2 loss: 0.828876  [   96/  130]
train() client id: f_00008-9-3 loss: 0.932643  [  128/  130]
train() client id: f_00008-10-0 loss: 1.019835  [   32/  130]
train() client id: f_00008-10-1 loss: 1.014404  [   64/  130]
train() client id: f_00008-10-2 loss: 0.980409  [   96/  130]
train() client id: f_00008-10-3 loss: 0.873972  [  128/  130]
train() client id: f_00008-11-0 loss: 0.916552  [   32/  130]
train() client id: f_00008-11-1 loss: 0.991979  [   64/  130]
train() client id: f_00008-11-2 loss: 1.022289  [   96/  130]
train() client id: f_00008-11-3 loss: 0.936790  [  128/  130]
train() client id: f_00008-12-0 loss: 0.903244  [   32/  130]
train() client id: f_00008-12-1 loss: 0.956493  [   64/  130]
train() client id: f_00008-12-2 loss: 1.046036  [   96/  130]
train() client id: f_00008-12-3 loss: 0.963064  [  128/  130]
train() client id: f_00009-0-0 loss: 1.108118  [   32/  118]
train() client id: f_00009-0-1 loss: 1.135155  [   64/  118]
train() client id: f_00009-0-2 loss: 1.066770  [   96/  118]
train() client id: f_00009-1-0 loss: 1.081709  [   32/  118]
train() client id: f_00009-1-1 loss: 1.070140  [   64/  118]
train() client id: f_00009-1-2 loss: 1.056214  [   96/  118]
train() client id: f_00009-2-0 loss: 1.040214  [   32/  118]
train() client id: f_00009-2-1 loss: 1.043951  [   64/  118]
train() client id: f_00009-2-2 loss: 1.057252  [   96/  118]
train() client id: f_00009-3-0 loss: 1.049041  [   32/  118]
train() client id: f_00009-3-1 loss: 1.038801  [   64/  118]
train() client id: f_00009-3-2 loss: 0.977052  [   96/  118]
train() client id: f_00009-4-0 loss: 1.035011  [   32/  118]
train() client id: f_00009-4-1 loss: 0.999177  [   64/  118]
train() client id: f_00009-4-2 loss: 1.008179  [   96/  118]
train() client id: f_00009-5-0 loss: 0.995161  [   32/  118]
train() client id: f_00009-5-1 loss: 0.966953  [   64/  118]
train() client id: f_00009-5-2 loss: 0.977240  [   96/  118]
train() client id: f_00009-6-0 loss: 0.959242  [   32/  118]
train() client id: f_00009-6-1 loss: 0.977348  [   64/  118]
train() client id: f_00009-6-2 loss: 0.988234  [   96/  118]
train() client id: f_00009-7-0 loss: 0.967545  [   32/  118]
train() client id: f_00009-7-1 loss: 0.964566  [   64/  118]
train() client id: f_00009-7-2 loss: 0.967613  [   96/  118]
train() client id: f_00009-8-0 loss: 0.973338  [   32/  118]
train() client id: f_00009-8-1 loss: 0.913178  [   64/  118]
train() client id: f_00009-8-2 loss: 0.954254  [   96/  118]
train() client id: f_00009-9-0 loss: 0.921433  [   32/  118]
train() client id: f_00009-9-1 loss: 0.931337  [   64/  118]
train() client id: f_00009-9-2 loss: 0.969321  [   96/  118]
train() client id: f_00009-10-0 loss: 0.921213  [   32/  118]
train() client id: f_00009-10-1 loss: 0.953020  [   64/  118]
train() client id: f_00009-10-2 loss: 0.977619  [   96/  118]
train() client id: f_00009-11-0 loss: 0.891065  [   32/  118]
train() client id: f_00009-11-1 loss: 0.987033  [   64/  118]
train() client id: f_00009-11-2 loss: 0.929113  [   96/  118]
train() client id: f_00009-12-0 loss: 0.948741  [   32/  118]
train() client id: f_00009-12-1 loss: 0.909384  [   64/  118]
train() client id: f_00009-12-2 loss: 0.971822  [   96/  118]
At round 1 accuracy: 0.6180371352785146
At round 1 training accuracy: 0.5674044265593562
At round 1 training loss: 0.9696680235374355
update_location
xs = [ -3.9056584  -10.79968212  25.00902392  18.81129433 -29.02070377
 -16.04359014   2.55680806  -6.32485185   9.66397685 -12.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  12.54482414   9.35018685
 -17.18584926  -2.62498432  -9.17765202  17.56900603   4.00148178]
dists_uav = [101.60999206 101.77729242 103.08828885 102.52432593 104.54485756
 102.72657981 100.06711653 100.6192479  101.99050162 100.80414995]
dists_bs = [232.490482   228.87591608 264.89269296 252.89227345 220.7936039
 249.40437224 251.15145857 249.74529647 242.66293951 236.19832881]
uav_gains = [9.60853004e-11 9.56909168e-11 9.26774783e-11 9.39572756e-11
 8.94829513e-11 9.34954732e-11 9.98320356e-11 9.84681076e-11
 9.51915868e-11 9.80171753e-11]
bs_gains = [2.61432507e-11 2.73157933e-11 1.81425423e-11 2.06573456e-11
 3.02086918e-11 2.14764606e-11 2.10607635e-11 2.13944731e-11
 2.31891278e-11 2.50103050e-11]
Round 2
-------------------------------
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.73113195 22.42682549 10.56475866  3.77907907 25.85610411 12.47188502
  4.69859503 15.17007359 11.12712296 10.11765294]
obj_prev = 126.94322881666007
eta_min = 1.8621250529904804e-09	eta_max = 0.9180913765363099
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 29.535190418159765	eta = 0.9090909090909091
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 50.41602139462105	eta = 0.5325722332838158
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 40.52128855107027	eta = 0.6626189360582114
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.752941123532246	eta = 0.6928551054183996
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.66719520504412	eta = 0.6943915369355615
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.6669798224976	eta = 0.694395404830551
eta = 0.694395404830551
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.0300953  0.06329568 0.0296176  0.01027062 0.07308859 0.03487234
 0.01289799 0.04275444 0.03105071 0.02818449]
ene_total = [3.27908415 6.43447607 3.23987046 1.49764204 7.29869581 3.92072634
 1.72777469 4.41559147 3.56239059 3.2907282 ]
ti_comp = [0.26882441 0.25014105 0.26840555 0.26856527 0.25202169 0.24528326
 0.26926224 0.26910548 0.24689208 0.24842218]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.35742980e-05 2.53298284e-04 2.25396212e-05 9.38793768e-07
 3.84196142e-04 4.40541962e-05 1.84967284e-06 6.74494733e-05
 3.06958933e-05 2.26740888e-05]
ene_total = [0.57330776 0.76208585 0.57698348 0.57360274 0.75694218 0.78696767
 0.5674136  0.57472659 0.77128999 0.75680081]
optimize_network iter = 0 obj = 6.700120669142431
eta = 0.694395404830551
freqs = [5.59757626e+07 1.26519974e+08 5.51732252e+07 1.91212720e+07
 1.45004571e+08 7.10858434e+07 2.39506050e+07 7.94380680e+07
 6.28831617e+07 5.67270011e+07]
eta_min = 0.6637595643039359	eta_max = 0.6943954048305468
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 0.07059948298279546	eta = 0.909090909090909
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 22.33326679569504	eta = 0.0028738002708385358
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.406693151543204	eta = 0.026667856733220745
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3186248233022884	eta = 0.02768078195366155
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3185795201975767	eta = 0.02768132281299047
eta = 0.02768132281299047
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.31855663e-04 2.49121488e-03 2.21679510e-04 9.23313400e-06
 3.77860887e-03 4.33277584e-04 1.81917241e-05 6.63372559e-04
 3.01897292e-04 2.23002013e-04]
ene_total = [0.18608375 0.30281643 0.18698052 0.18052076 0.33390587 0.25835522
 0.17880307 0.19749234 0.25008995 0.2435316 ]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 1 obj = 6.097170992212053
eta = 0.6637595643039359
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
eta_min = 0.6637595643039447	eta_max = 0.6637595643039276
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 0.06961124159514927	eta = 0.909090909090909
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 22.33232490105157	eta = 0.0028336927384439636
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.4022158004882264	eta = 0.026343572834638552
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.3153036098319895	eta = 0.027332461555343614
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.315259971692997	eta = 0.02733297671898439
eta = 0.02733297671898439
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.32590817e-04 2.45845542e-03 2.22305891e-04 9.26043939e-06
 3.73546925e-03 4.25596504e-04 1.82559506e-05 6.65629235e-04
 2.97007961e-04 2.19712069e-04]
ene_total = [0.18603789 0.30178187 0.18693126 0.18045688 0.33256672 0.25804555
 0.17874085 0.19748541 0.24986216 0.24335138]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 2 obj = 6.09717099221221
eta = 0.6637595643039447
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
Done!
At round 2 energy consumption: 6.097170992212053
At round 2 eta: 0.6637595643039447
At round 2 local rounds: 13.420089837852764
At round 2 global rounds: 81.77931100223806
At round 2 a_n: 27.154965315346423
gradient difference: 0.41007766127586365
train() client id: f_00000-0-0 loss: 1.097309  [   32/  126]
train() client id: f_00000-0-1 loss: 1.182639  [   64/  126]
train() client id: f_00000-0-2 loss: 1.168414  [   96/  126]
train() client id: f_00000-1-0 loss: 1.009157  [   32/  126]
train() client id: f_00000-1-1 loss: 1.127799  [   64/  126]
train() client id: f_00000-1-2 loss: 1.152533  [   96/  126]
train() client id: f_00000-2-0 loss: 1.029535  [   32/  126]
train() client id: f_00000-2-1 loss: 1.026372  [   64/  126]
train() client id: f_00000-2-2 loss: 0.996930  [   96/  126]
train() client id: f_00000-3-0 loss: 1.043411  [   32/  126]
train() client id: f_00000-3-1 loss: 0.970554  [   64/  126]
train() client id: f_00000-3-2 loss: 0.950858  [   96/  126]
train() client id: f_00000-4-0 loss: 0.944797  [   32/  126]
train() client id: f_00000-4-1 loss: 0.946244  [   64/  126]
train() client id: f_00000-4-2 loss: 0.965600  [   96/  126]
train() client id: f_00000-5-0 loss: 0.890021  [   32/  126]
train() client id: f_00000-5-1 loss: 0.929779  [   64/  126]
train() client id: f_00000-5-2 loss: 0.931465  [   96/  126]
train() client id: f_00000-6-0 loss: 0.896548  [   32/  126]
train() client id: f_00000-6-1 loss: 0.898945  [   64/  126]
train() client id: f_00000-6-2 loss: 0.930720  [   96/  126]
train() client id: f_00000-7-0 loss: 0.944185  [   32/  126]
train() client id: f_00000-7-1 loss: 0.918619  [   64/  126]
train() client id: f_00000-7-2 loss: 0.891749  [   96/  126]
train() client id: f_00000-8-0 loss: 0.953607  [   32/  126]
train() client id: f_00000-8-1 loss: 0.883778  [   64/  126]
train() client id: f_00000-8-2 loss: 0.841601  [   96/  126]
train() client id: f_00000-9-0 loss: 0.891700  [   32/  126]
train() client id: f_00000-9-1 loss: 0.870216  [   64/  126]
train() client id: f_00000-9-2 loss: 0.876161  [   96/  126]
train() client id: f_00000-10-0 loss: 0.859477  [   32/  126]
train() client id: f_00000-10-1 loss: 0.866704  [   64/  126]
train() client id: f_00000-10-2 loss: 0.863997  [   96/  126]
train() client id: f_00000-11-0 loss: 0.883196  [   32/  126]
train() client id: f_00000-11-1 loss: 0.882257  [   64/  126]
train() client id: f_00000-11-2 loss: 0.932531  [   96/  126]
train() client id: f_00000-12-0 loss: 0.902456  [   32/  126]
train() client id: f_00000-12-1 loss: 0.922922  [   64/  126]
train() client id: f_00000-12-2 loss: 0.927478  [   96/  126]
train() client id: f_00001-0-0 loss: 0.849648  [   32/  265]
train() client id: f_00001-0-1 loss: 0.847839  [   64/  265]
train() client id: f_00001-0-2 loss: 0.815881  [   96/  265]
train() client id: f_00001-0-3 loss: 0.791738  [  128/  265]
train() client id: f_00001-0-4 loss: 0.768167  [  160/  265]
train() client id: f_00001-0-5 loss: 0.748489  [  192/  265]
train() client id: f_00001-0-6 loss: 0.763480  [  224/  265]
train() client id: f_00001-0-7 loss: 0.782012  [  256/  265]
train() client id: f_00001-1-0 loss: 0.772003  [   32/  265]
train() client id: f_00001-1-1 loss: 0.717655  [   64/  265]
train() client id: f_00001-1-2 loss: 0.730127  [   96/  265]
train() client id: f_00001-1-3 loss: 0.738868  [  128/  265]
train() client id: f_00001-1-4 loss: 0.698904  [  160/  265]
train() client id: f_00001-1-5 loss: 0.693722  [  192/  265]
train() client id: f_00001-1-6 loss: 0.752241  [  224/  265]
train() client id: f_00001-1-7 loss: 0.731963  [  256/  265]
train() client id: f_00001-2-0 loss: 0.750664  [   32/  265]
train() client id: f_00001-2-1 loss: 0.680283  [   64/  265]
train() client id: f_00001-2-2 loss: 0.820857  [   96/  265]
train() client id: f_00001-2-3 loss: 0.619997  [  128/  265]
train() client id: f_00001-2-4 loss: 0.624067  [  160/  265]
train() client id: f_00001-2-5 loss: 0.652652  [  192/  265]
train() client id: f_00001-2-6 loss: 0.643543  [  224/  265]
train() client id: f_00001-2-7 loss: 0.621816  [  256/  265]
train() client id: f_00001-3-0 loss: 0.639612  [   32/  265]
train() client id: f_00001-3-1 loss: 0.680104  [   64/  265]
train() client id: f_00001-3-2 loss: 0.624272  [   96/  265]
train() client id: f_00001-3-3 loss: 0.651134  [  128/  265]
train() client id: f_00001-3-4 loss: 0.619285  [  160/  265]
train() client id: f_00001-3-5 loss: 0.603253  [  192/  265]
train() client id: f_00001-3-6 loss: 0.742098  [  224/  265]
train() client id: f_00001-3-7 loss: 0.618703  [  256/  265]
train() client id: f_00001-4-0 loss: 0.621472  [   32/  265]
train() client id: f_00001-4-1 loss: 0.612129  [   64/  265]
train() client id: f_00001-4-2 loss: 0.574217  [   96/  265]
train() client id: f_00001-4-3 loss: 0.647624  [  128/  265]
train() client id: f_00001-4-4 loss: 0.610669  [  160/  265]
train() client id: f_00001-4-5 loss: 0.725130  [  192/  265]
train() client id: f_00001-4-6 loss: 0.613713  [  224/  265]
train() client id: f_00001-4-7 loss: 0.589515  [  256/  265]
train() client id: f_00001-5-0 loss: 0.628910  [   32/  265]
train() client id: f_00001-5-1 loss: 0.596632  [   64/  265]
train() client id: f_00001-5-2 loss: 0.563498  [   96/  265]
train() client id: f_00001-5-3 loss: 0.533647  [  128/  265]
train() client id: f_00001-5-4 loss: 0.581790  [  160/  265]
train() client id: f_00001-5-5 loss: 0.669281  [  192/  265]
train() client id: f_00001-5-6 loss: 0.530058  [  224/  265]
train() client id: f_00001-5-7 loss: 0.657056  [  256/  265]
train() client id: f_00001-6-0 loss: 0.574447  [   32/  265]
train() client id: f_00001-6-1 loss: 0.565004  [   64/  265]
train() client id: f_00001-6-2 loss: 0.565414  [   96/  265]
train() client id: f_00001-6-3 loss: 0.540279  [  128/  265]
train() client id: f_00001-6-4 loss: 0.622151  [  160/  265]
train() client id: f_00001-6-5 loss: 0.644989  [  192/  265]
train() client id: f_00001-6-6 loss: 0.674566  [  224/  265]
train() client id: f_00001-6-7 loss: 0.584476  [  256/  265]
train() client id: f_00001-7-0 loss: 0.587321  [   32/  265]
train() client id: f_00001-7-1 loss: 0.557398  [   64/  265]
train() client id: f_00001-7-2 loss: 0.671721  [   96/  265]
train() client id: f_00001-7-3 loss: 0.635886  [  128/  265]
train() client id: f_00001-7-4 loss: 0.500000  [  160/  265]
train() client id: f_00001-7-5 loss: 0.533876  [  192/  265]
train() client id: f_00001-7-6 loss: 0.656100  [  224/  265]
train() client id: f_00001-7-7 loss: 0.511650  [  256/  265]
train() client id: f_00001-8-0 loss: 0.548823  [   32/  265]
train() client id: f_00001-8-1 loss: 0.569657  [   64/  265]
train() client id: f_00001-8-2 loss: 0.517852  [   96/  265]
train() client id: f_00001-8-3 loss: 0.614004  [  128/  265]
train() client id: f_00001-8-4 loss: 0.562770  [  160/  265]
train() client id: f_00001-8-5 loss: 0.742474  [  192/  265]
train() client id: f_00001-8-6 loss: 0.511177  [  224/  265]
train() client id: f_00001-8-7 loss: 0.578668  [  256/  265]
train() client id: f_00001-9-0 loss: 0.557280  [   32/  265]
train() client id: f_00001-9-1 loss: 0.519367  [   64/  265]
train() client id: f_00001-9-2 loss: 0.612055  [   96/  265]
train() client id: f_00001-9-3 loss: 0.580225  [  128/  265]
train() client id: f_00001-9-4 loss: 0.676734  [  160/  265]
train() client id: f_00001-9-5 loss: 0.586786  [  192/  265]
train() client id: f_00001-9-6 loss: 0.533954  [  224/  265]
train() client id: f_00001-9-7 loss: 0.557901  [  256/  265]
train() client id: f_00001-10-0 loss: 0.506684  [   32/  265]
train() client id: f_00001-10-1 loss: 0.583085  [   64/  265]
train() client id: f_00001-10-2 loss: 0.583105  [   96/  265]
train() client id: f_00001-10-3 loss: 0.545506  [  128/  265]
train() client id: f_00001-10-4 loss: 0.614407  [  160/  265]
train() client id: f_00001-10-5 loss: 0.644663  [  192/  265]
train() client id: f_00001-10-6 loss: 0.557931  [  224/  265]
train() client id: f_00001-10-7 loss: 0.586437  [  256/  265]
train() client id: f_00001-11-0 loss: 0.669452  [   32/  265]
train() client id: f_00001-11-1 loss: 0.500219  [   64/  265]
train() client id: f_00001-11-2 loss: 0.609864  [   96/  265]
train() client id: f_00001-11-3 loss: 0.584995  [  128/  265]
train() client id: f_00001-11-4 loss: 0.550015  [  160/  265]
train() client id: f_00001-11-5 loss: 0.525489  [  192/  265]
train() client id: f_00001-11-6 loss: 0.626204  [  224/  265]
train() client id: f_00001-11-7 loss: 0.540241  [  256/  265]
train() client id: f_00001-12-0 loss: 0.598177  [   32/  265]
train() client id: f_00001-12-1 loss: 0.604897  [   64/  265]
train() client id: f_00001-12-2 loss: 0.618631  [   96/  265]
train() client id: f_00001-12-3 loss: 0.598883  [  128/  265]
train() client id: f_00001-12-4 loss: 0.514001  [  160/  265]
train() client id: f_00001-12-5 loss: 0.553692  [  192/  265]
train() client id: f_00001-12-6 loss: 0.526588  [  224/  265]
train() client id: f_00001-12-7 loss: 0.530056  [  256/  265]
train() client id: f_00002-0-0 loss: 1.070556  [   32/  124]
train() client id: f_00002-0-1 loss: 1.083413  [   64/  124]
train() client id: f_00002-0-2 loss: 1.080507  [   96/  124]
train() client id: f_00002-1-0 loss: 1.056235  [   32/  124]
train() client id: f_00002-1-1 loss: 1.060759  [   64/  124]
train() client id: f_00002-1-2 loss: 1.037363  [   96/  124]
train() client id: f_00002-2-0 loss: 1.089202  [   32/  124]
train() client id: f_00002-2-1 loss: 0.995600  [   64/  124]
train() client id: f_00002-2-2 loss: 1.015074  [   96/  124]
train() client id: f_00002-3-0 loss: 1.030742  [   32/  124]
train() client id: f_00002-3-1 loss: 1.033679  [   64/  124]
train() client id: f_00002-3-2 loss: 0.992785  [   96/  124]
train() client id: f_00002-4-0 loss: 0.994164  [   32/  124]
train() client id: f_00002-4-1 loss: 0.987877  [   64/  124]
train() client id: f_00002-4-2 loss: 1.014799  [   96/  124]
train() client id: f_00002-5-0 loss: 0.959161  [   32/  124]
train() client id: f_00002-5-1 loss: 0.992274  [   64/  124]
train() client id: f_00002-5-2 loss: 0.999759  [   96/  124]
train() client id: f_00002-6-0 loss: 0.983892  [   32/  124]
train() client id: f_00002-6-1 loss: 0.967299  [   64/  124]
train() client id: f_00002-6-2 loss: 0.971848  [   96/  124]
train() client id: f_00002-7-0 loss: 0.931762  [   32/  124]
train() client id: f_00002-7-1 loss: 0.961968  [   64/  124]
train() client id: f_00002-7-2 loss: 1.000755  [   96/  124]
train() client id: f_00002-8-0 loss: 0.953540  [   32/  124]
train() client id: f_00002-8-1 loss: 0.955140  [   64/  124]
train() client id: f_00002-8-2 loss: 0.928894  [   96/  124]
train() client id: f_00002-9-0 loss: 0.902548  [   32/  124]
train() client id: f_00002-9-1 loss: 0.950213  [   64/  124]
train() client id: f_00002-9-2 loss: 0.948980  [   96/  124]
train() client id: f_00002-10-0 loss: 0.896783  [   32/  124]
train() client id: f_00002-10-1 loss: 0.955549  [   64/  124]
train() client id: f_00002-10-2 loss: 0.937442  [   96/  124]
train() client id: f_00002-11-0 loss: 0.965982  [   32/  124]
train() client id: f_00002-11-1 loss: 0.923543  [   64/  124]
train() client id: f_00002-11-2 loss: 0.877665  [   96/  124]
train() client id: f_00002-12-0 loss: 0.941186  [   32/  124]
train() client id: f_00002-12-1 loss: 0.967447  [   64/  124]
train() client id: f_00002-12-2 loss: 0.930249  [   96/  124]
train() client id: f_00003-0-0 loss: 1.010804  [   32/   43]
train() client id: f_00003-1-0 loss: 1.070379  [   32/   43]
train() client id: f_00003-2-0 loss: 1.010506  [   32/   43]
train() client id: f_00003-3-0 loss: 0.967428  [   32/   43]
train() client id: f_00003-4-0 loss: 0.982851  [   32/   43]
train() client id: f_00003-5-0 loss: 0.964252  [   32/   43]
train() client id: f_00003-6-0 loss: 1.009569  [   32/   43]
train() client id: f_00003-7-0 loss: 0.979219  [   32/   43]
train() client id: f_00003-8-0 loss: 1.059438  [   32/   43]
train() client id: f_00003-9-0 loss: 1.019099  [   32/   43]
train() client id: f_00003-10-0 loss: 1.051995  [   32/   43]
train() client id: f_00003-11-0 loss: 0.982783  [   32/   43]
train() client id: f_00003-12-0 loss: 1.030347  [   32/   43]
train() client id: f_00004-0-0 loss: 0.942910  [   32/  306]
train() client id: f_00004-0-1 loss: 0.929616  [   64/  306]
train() client id: f_00004-0-2 loss: 1.096662  [   96/  306]
train() client id: f_00004-0-3 loss: 0.977548  [  128/  306]
train() client id: f_00004-0-4 loss: 0.920592  [  160/  306]
train() client id: f_00004-0-5 loss: 0.971415  [  192/  306]
train() client id: f_00004-0-6 loss: 1.011259  [  224/  306]
train() client id: f_00004-0-7 loss: 1.040038  [  256/  306]
train() client id: f_00004-0-8 loss: 0.998323  [  288/  306]
train() client id: f_00004-1-0 loss: 1.014143  [   32/  306]
train() client id: f_00004-1-1 loss: 1.039972  [   64/  306]
train() client id: f_00004-1-2 loss: 0.908324  [   96/  306]
train() client id: f_00004-1-3 loss: 1.011586  [  128/  306]
train() client id: f_00004-1-4 loss: 0.964324  [  160/  306]
train() client id: f_00004-1-5 loss: 0.959454  [  192/  306]
train() client id: f_00004-1-6 loss: 0.945663  [  224/  306]
train() client id: f_00004-1-7 loss: 0.972179  [  256/  306]
train() client id: f_00004-1-8 loss: 1.002893  [  288/  306]
train() client id: f_00004-2-0 loss: 0.958455  [   32/  306]
train() client id: f_00004-2-1 loss: 1.068629  [   64/  306]
train() client id: f_00004-2-2 loss: 0.964959  [   96/  306]
train() client id: f_00004-2-3 loss: 0.929041  [  128/  306]
train() client id: f_00004-2-4 loss: 0.960039  [  160/  306]
train() client id: f_00004-2-5 loss: 0.964781  [  192/  306]
train() client id: f_00004-2-6 loss: 1.050327  [  224/  306]
train() client id: f_00004-2-7 loss: 0.883492  [  256/  306]
train() client id: f_00004-2-8 loss: 0.987132  [  288/  306]
train() client id: f_00004-3-0 loss: 0.910469  [   32/  306]
train() client id: f_00004-3-1 loss: 1.015842  [   64/  306]
train() client id: f_00004-3-2 loss: 0.902825  [   96/  306]
train() client id: f_00004-3-3 loss: 1.018471  [  128/  306]
train() client id: f_00004-3-4 loss: 0.934243  [  160/  306]
train() client id: f_00004-3-5 loss: 0.998616  [  192/  306]
train() client id: f_00004-3-6 loss: 0.919237  [  224/  306]
train() client id: f_00004-3-7 loss: 0.996194  [  256/  306]
train() client id: f_00004-3-8 loss: 1.004942  [  288/  306]
train() client id: f_00004-4-0 loss: 0.948399  [   32/  306]
train() client id: f_00004-4-1 loss: 0.957628  [   64/  306]
train() client id: f_00004-4-2 loss: 0.974766  [   96/  306]
train() client id: f_00004-4-3 loss: 0.860285  [  128/  306]
train() client id: f_00004-4-4 loss: 0.930227  [  160/  306]
train() client id: f_00004-4-5 loss: 1.018707  [  192/  306]
train() client id: f_00004-4-6 loss: 0.941597  [  224/  306]
train() client id: f_00004-4-7 loss: 0.916861  [  256/  306]
train() client id: f_00004-4-8 loss: 1.039632  [  288/  306]
train() client id: f_00004-5-0 loss: 0.940379  [   32/  306]
train() client id: f_00004-5-1 loss: 0.924394  [   64/  306]
train() client id: f_00004-5-2 loss: 0.999569  [   96/  306]
train() client id: f_00004-5-3 loss: 0.931172  [  128/  306]
train() client id: f_00004-5-4 loss: 0.974908  [  160/  306]
train() client id: f_00004-5-5 loss: 0.924273  [  192/  306]
train() client id: f_00004-5-6 loss: 0.942880  [  224/  306]
train() client id: f_00004-5-7 loss: 1.064223  [  256/  306]
train() client id: f_00004-5-8 loss: 0.946103  [  288/  306]
train() client id: f_00004-6-0 loss: 0.919921  [   32/  306]
train() client id: f_00004-6-1 loss: 1.027197  [   64/  306]
train() client id: f_00004-6-2 loss: 0.941420  [   96/  306]
train() client id: f_00004-6-3 loss: 0.899449  [  128/  306]
train() client id: f_00004-6-4 loss: 1.009025  [  160/  306]
train() client id: f_00004-6-5 loss: 0.932516  [  192/  306]
train() client id: f_00004-6-6 loss: 0.908406  [  224/  306]
train() client id: f_00004-6-7 loss: 0.977450  [  256/  306]
train() client id: f_00004-6-8 loss: 1.023212  [  288/  306]
train() client id: f_00004-7-0 loss: 0.931414  [   32/  306]
train() client id: f_00004-7-1 loss: 0.907798  [   64/  306]
train() client id: f_00004-7-2 loss: 0.955883  [   96/  306]
train() client id: f_00004-7-3 loss: 1.013034  [  128/  306]
train() client id: f_00004-7-4 loss: 0.970950  [  160/  306]
train() client id: f_00004-7-5 loss: 0.960266  [  192/  306]
train() client id: f_00004-7-6 loss: 1.017517  [  224/  306]
train() client id: f_00004-7-7 loss: 0.882977  [  256/  306]
train() client id: f_00004-7-8 loss: 0.938843  [  288/  306]
train() client id: f_00004-8-0 loss: 1.033850  [   32/  306]
train() client id: f_00004-8-1 loss: 0.918210  [   64/  306]
train() client id: f_00004-8-2 loss: 0.971438  [   96/  306]
train() client id: f_00004-8-3 loss: 0.964746  [  128/  306]
train() client id: f_00004-8-4 loss: 0.909790  [  160/  306]
train() client id: f_00004-8-5 loss: 1.006053  [  192/  306]
train() client id: f_00004-8-6 loss: 1.039127  [  224/  306]
train() client id: f_00004-8-7 loss: 0.891707  [  256/  306]
train() client id: f_00004-8-8 loss: 0.869779  [  288/  306]
train() client id: f_00004-9-0 loss: 0.912830  [   32/  306]
train() client id: f_00004-9-1 loss: 0.921452  [   64/  306]
train() client id: f_00004-9-2 loss: 0.952294  [   96/  306]
train() client id: f_00004-9-3 loss: 0.957241  [  128/  306]
train() client id: f_00004-9-4 loss: 0.954458  [  160/  306]
train() client id: f_00004-9-5 loss: 0.997433  [  192/  306]
train() client id: f_00004-9-6 loss: 1.034712  [  224/  306]
train() client id: f_00004-9-7 loss: 0.937610  [  256/  306]
train() client id: f_00004-9-8 loss: 0.950559  [  288/  306]
train() client id: f_00004-10-0 loss: 0.967331  [   32/  306]
train() client id: f_00004-10-1 loss: 0.978934  [   64/  306]
train() client id: f_00004-10-2 loss: 0.999181  [   96/  306]
train() client id: f_00004-10-3 loss: 1.008164  [  128/  306]
train() client id: f_00004-10-4 loss: 0.964982  [  160/  306]
train() client id: f_00004-10-5 loss: 0.901272  [  192/  306]
train() client id: f_00004-10-6 loss: 0.985413  [  224/  306]
train() client id: f_00004-10-7 loss: 0.808163  [  256/  306]
train() client id: f_00004-10-8 loss: 0.992352  [  288/  306]
train() client id: f_00004-11-0 loss: 0.951997  [   32/  306]
train() client id: f_00004-11-1 loss: 0.908034  [   64/  306]
train() client id: f_00004-11-2 loss: 0.971071  [   96/  306]
train() client id: f_00004-11-3 loss: 0.981573  [  128/  306]
train() client id: f_00004-11-4 loss: 0.903395  [  160/  306]
train() client id: f_00004-11-5 loss: 0.880657  [  192/  306]
train() client id: f_00004-11-6 loss: 1.059774  [  224/  306]
train() client id: f_00004-11-7 loss: 0.924216  [  256/  306]
train() client id: f_00004-11-8 loss: 0.989160  [  288/  306]
train() client id: f_00004-12-0 loss: 0.882746  [   32/  306]
train() client id: f_00004-12-1 loss: 0.943843  [   64/  306]
train() client id: f_00004-12-2 loss: 0.939278  [   96/  306]
train() client id: f_00004-12-3 loss: 0.935153  [  128/  306]
train() client id: f_00004-12-4 loss: 0.906229  [  160/  306]
train() client id: f_00004-12-5 loss: 0.960957  [  192/  306]
train() client id: f_00004-12-6 loss: 1.012240  [  224/  306]
train() client id: f_00004-12-7 loss: 0.967860  [  256/  306]
train() client id: f_00004-12-8 loss: 0.963482  [  288/  306]
train() client id: f_00005-0-0 loss: 0.984001  [   32/  146]
train() client id: f_00005-0-1 loss: 0.996107  [   64/  146]
train() client id: f_00005-0-2 loss: 0.929183  [   96/  146]
train() client id: f_00005-0-3 loss: 0.881954  [  128/  146]
train() client id: f_00005-1-0 loss: 0.903645  [   32/  146]
train() client id: f_00005-1-1 loss: 0.900746  [   64/  146]
train() client id: f_00005-1-2 loss: 0.976961  [   96/  146]
train() client id: f_00005-1-3 loss: 0.955543  [  128/  146]
train() client id: f_00005-2-0 loss: 0.966255  [   32/  146]
train() client id: f_00005-2-1 loss: 0.871559  [   64/  146]
train() client id: f_00005-2-2 loss: 0.903980  [   96/  146]
train() client id: f_00005-2-3 loss: 0.900849  [  128/  146]
train() client id: f_00005-3-0 loss: 0.908445  [   32/  146]
train() client id: f_00005-3-1 loss: 0.944223  [   64/  146]
train() client id: f_00005-3-2 loss: 0.879396  [   96/  146]
train() client id: f_00005-3-3 loss: 0.912973  [  128/  146]
train() client id: f_00005-4-0 loss: 0.846953  [   32/  146]
train() client id: f_00005-4-1 loss: 0.925189  [   64/  146]
train() client id: f_00005-4-2 loss: 0.916481  [   96/  146]
train() client id: f_00005-4-3 loss: 0.955519  [  128/  146]
train() client id: f_00005-5-0 loss: 0.837566  [   32/  146]
train() client id: f_00005-5-1 loss: 0.916296  [   64/  146]
train() client id: f_00005-5-2 loss: 0.853489  [   96/  146]
train() client id: f_00005-5-3 loss: 0.928604  [  128/  146]
train() client id: f_00005-6-0 loss: 0.935349  [   32/  146]
train() client id: f_00005-6-1 loss: 0.908585  [   64/  146]
train() client id: f_00005-6-2 loss: 0.809524  [   96/  146]
train() client id: f_00005-6-3 loss: 0.857814  [  128/  146]
train() client id: f_00005-7-0 loss: 0.845567  [   32/  146]
train() client id: f_00005-7-1 loss: 0.795661  [   64/  146]
train() client id: f_00005-7-2 loss: 1.132741  [   96/  146]
train() client id: f_00005-7-3 loss: 0.843264  [  128/  146]
train() client id: f_00005-8-0 loss: 0.961067  [   32/  146]
train() client id: f_00005-8-1 loss: 0.843696  [   64/  146]
train() client id: f_00005-8-2 loss: 0.861671  [   96/  146]
train() client id: f_00005-8-3 loss: 0.860439  [  128/  146]
train() client id: f_00005-9-0 loss: 0.869593  [   32/  146]
train() client id: f_00005-9-1 loss: 0.922580  [   64/  146]
train() client id: f_00005-9-2 loss: 0.882389  [   96/  146]
train() client id: f_00005-9-3 loss: 0.855797  [  128/  146]
train() client id: f_00005-10-0 loss: 0.906951  [   32/  146]
train() client id: f_00005-10-1 loss: 0.802117  [   64/  146]
train() client id: f_00005-10-2 loss: 0.896450  [   96/  146]
train() client id: f_00005-10-3 loss: 0.966822  [  128/  146]
train() client id: f_00005-11-0 loss: 0.911905  [   32/  146]
train() client id: f_00005-11-1 loss: 0.858875  [   64/  146]
train() client id: f_00005-11-2 loss: 0.989602  [   96/  146]
train() client id: f_00005-11-3 loss: 0.806775  [  128/  146]
train() client id: f_00005-12-0 loss: 1.019343  [   32/  146]
train() client id: f_00005-12-1 loss: 0.807167  [   64/  146]
train() client id: f_00005-12-2 loss: 0.845490  [   96/  146]
train() client id: f_00005-12-3 loss: 0.835929  [  128/  146]
train() client id: f_00006-0-0 loss: 1.029307  [   32/   54]
train() client id: f_00006-1-0 loss: 1.027883  [   32/   54]
train() client id: f_00006-2-0 loss: 1.026921  [   32/   54]
train() client id: f_00006-3-0 loss: 1.035667  [   32/   54]
train() client id: f_00006-4-0 loss: 1.066043  [   32/   54]
train() client id: f_00006-5-0 loss: 1.039966  [   32/   54]
train() client id: f_00006-6-0 loss: 1.060279  [   32/   54]
train() client id: f_00006-7-0 loss: 1.044883  [   32/   54]
train() client id: f_00006-8-0 loss: 1.046858  [   32/   54]
train() client id: f_00006-9-0 loss: 1.030218  [   32/   54]
train() client id: f_00006-10-0 loss: 1.030387  [   32/   54]
train() client id: f_00006-11-0 loss: 1.022539  [   32/   54]
train() client id: f_00006-12-0 loss: 1.033064  [   32/   54]
train() client id: f_00007-0-0 loss: 0.974877  [   32/  179]
train() client id: f_00007-0-1 loss: 0.991001  [   64/  179]
train() client id: f_00007-0-2 loss: 0.985113  [   96/  179]
train() client id: f_00007-0-3 loss: 0.943953  [  128/  179]
train() client id: f_00007-0-4 loss: 0.869903  [  160/  179]
train() client id: f_00007-1-0 loss: 0.925312  [   32/  179]
train() client id: f_00007-1-1 loss: 0.928658  [   64/  179]
train() client id: f_00007-1-2 loss: 0.880850  [   96/  179]
train() client id: f_00007-1-3 loss: 0.881459  [  128/  179]
train() client id: f_00007-1-4 loss: 0.862937  [  160/  179]
train() client id: f_00007-2-0 loss: 0.874210  [   32/  179]
train() client id: f_00007-2-1 loss: 0.903439  [   64/  179]
train() client id: f_00007-2-2 loss: 0.808015  [   96/  179]
train() client id: f_00007-2-3 loss: 0.887074  [  128/  179]
train() client id: f_00007-2-4 loss: 0.827890  [  160/  179]
train() client id: f_00007-3-0 loss: 0.831807  [   32/  179]
train() client id: f_00007-3-1 loss: 0.869538  [   64/  179]
train() client id: f_00007-3-2 loss: 0.826358  [   96/  179]
train() client id: f_00007-3-3 loss: 0.749876  [  128/  179]
train() client id: f_00007-3-4 loss: 0.836851  [  160/  179]
train() client id: f_00007-4-0 loss: 0.831332  [   32/  179]
train() client id: f_00007-4-1 loss: 0.764412  [   64/  179]
train() client id: f_00007-4-2 loss: 0.809227  [   96/  179]
train() client id: f_00007-4-3 loss: 0.789979  [  128/  179]
train() client id: f_00007-4-4 loss: 0.804793  [  160/  179]
train() client id: f_00007-5-0 loss: 0.792180  [   32/  179]
train() client id: f_00007-5-1 loss: 0.842038  [   64/  179]
train() client id: f_00007-5-2 loss: 0.743673  [   96/  179]
train() client id: f_00007-5-3 loss: 0.709983  [  128/  179]
train() client id: f_00007-5-4 loss: 0.752611  [  160/  179]
train() client id: f_00007-6-0 loss: 0.859790  [   32/  179]
train() client id: f_00007-6-1 loss: 0.732585  [   64/  179]
train() client id: f_00007-6-2 loss: 0.757432  [   96/  179]
train() client id: f_00007-6-3 loss: 0.722011  [  128/  179]
train() client id: f_00007-6-4 loss: 0.718663  [  160/  179]
train() client id: f_00007-7-0 loss: 0.673442  [   32/  179]
train() client id: f_00007-7-1 loss: 0.804510  [   64/  179]
train() client id: f_00007-7-2 loss: 0.866922  [   96/  179]
train() client id: f_00007-7-3 loss: 0.693355  [  128/  179]
train() client id: f_00007-7-4 loss: 0.747090  [  160/  179]
train() client id: f_00007-8-0 loss: 0.808403  [   32/  179]
train() client id: f_00007-8-1 loss: 0.689553  [   64/  179]
train() client id: f_00007-8-2 loss: 0.720775  [   96/  179]
train() client id: f_00007-8-3 loss: 0.762659  [  128/  179]
train() client id: f_00007-8-4 loss: 0.724782  [  160/  179]
train() client id: f_00007-9-0 loss: 0.777761  [   32/  179]
train() client id: f_00007-9-1 loss: 0.699415  [   64/  179]
train() client id: f_00007-9-2 loss: 0.762348  [   96/  179]
train() client id: f_00007-9-3 loss: 0.719042  [  128/  179]
train() client id: f_00007-9-4 loss: 0.774121  [  160/  179]
train() client id: f_00007-10-0 loss: 0.733539  [   32/  179]
train() client id: f_00007-10-1 loss: 0.739691  [   64/  179]
train() client id: f_00007-10-2 loss: 0.766787  [   96/  179]
train() client id: f_00007-10-3 loss: 0.698269  [  128/  179]
train() client id: f_00007-10-4 loss: 0.796267  [  160/  179]
train() client id: f_00007-11-0 loss: 0.783304  [   32/  179]
train() client id: f_00007-11-1 loss: 0.737528  [   64/  179]
train() client id: f_00007-11-2 loss: 0.721431  [   96/  179]
train() client id: f_00007-11-3 loss: 0.769305  [  128/  179]
train() client id: f_00007-11-4 loss: 0.712162  [  160/  179]
train() client id: f_00007-12-0 loss: 0.820767  [   32/  179]
train() client id: f_00007-12-1 loss: 0.757489  [   64/  179]
train() client id: f_00007-12-2 loss: 0.675197  [   96/  179]
train() client id: f_00007-12-3 loss: 0.784646  [  128/  179]
train() client id: f_00007-12-4 loss: 0.668971  [  160/  179]
train() client id: f_00008-0-0 loss: 1.050343  [   32/  130]
train() client id: f_00008-0-1 loss: 0.829406  [   64/  130]
train() client id: f_00008-0-2 loss: 0.894062  [   96/  130]
train() client id: f_00008-0-3 loss: 0.959510  [  128/  130]
train() client id: f_00008-1-0 loss: 0.974177  [   32/  130]
train() client id: f_00008-1-1 loss: 0.892514  [   64/  130]
train() client id: f_00008-1-2 loss: 0.911049  [   96/  130]
train() client id: f_00008-1-3 loss: 0.946964  [  128/  130]
train() client id: f_00008-2-0 loss: 0.911689  [   32/  130]
train() client id: f_00008-2-1 loss: 0.874162  [   64/  130]
train() client id: f_00008-2-2 loss: 1.038252  [   96/  130]
train() client id: f_00008-2-3 loss: 0.899264  [  128/  130]
train() client id: f_00008-3-0 loss: 0.861832  [   32/  130]
train() client id: f_00008-3-1 loss: 0.920644  [   64/  130]
train() client id: f_00008-3-2 loss: 0.942484  [   96/  130]
train() client id: f_00008-3-3 loss: 0.964525  [  128/  130]
train() client id: f_00008-4-0 loss: 0.850540  [   32/  130]
train() client id: f_00008-4-1 loss: 0.989917  [   64/  130]
train() client id: f_00008-4-2 loss: 0.909112  [   96/  130]
train() client id: f_00008-4-3 loss: 0.969411  [  128/  130]
train() client id: f_00008-5-0 loss: 0.902733  [   32/  130]
train() client id: f_00008-5-1 loss: 0.907198  [   64/  130]
train() client id: f_00008-5-2 loss: 0.977943  [   96/  130]
train() client id: f_00008-5-3 loss: 0.892889  [  128/  130]
train() client id: f_00008-6-0 loss: 0.937088  [   32/  130]
train() client id: f_00008-6-1 loss: 0.893565  [   64/  130]
train() client id: f_00008-6-2 loss: 0.911968  [   96/  130]
train() client id: f_00008-6-3 loss: 0.937063  [  128/  130]
train() client id: f_00008-7-0 loss: 0.970317  [   32/  130]
train() client id: f_00008-7-1 loss: 0.867161  [   64/  130]
train() client id: f_00008-7-2 loss: 0.948294  [   96/  130]
train() client id: f_00008-7-3 loss: 0.920523  [  128/  130]
train() client id: f_00008-8-0 loss: 0.964397  [   32/  130]
train() client id: f_00008-8-1 loss: 0.935920  [   64/  130]
train() client id: f_00008-8-2 loss: 0.888012  [   96/  130]
train() client id: f_00008-8-3 loss: 0.887473  [  128/  130]
train() client id: f_00008-9-0 loss: 0.908157  [   32/  130]
train() client id: f_00008-9-1 loss: 0.916963  [   64/  130]
train() client id: f_00008-9-2 loss: 0.970826  [   96/  130]
train() client id: f_00008-9-3 loss: 0.902628  [  128/  130]
train() client id: f_00008-10-0 loss: 0.934363  [   32/  130]
train() client id: f_00008-10-1 loss: 0.904958  [   64/  130]
train() client id: f_00008-10-2 loss: 0.959344  [   96/  130]
train() client id: f_00008-10-3 loss: 0.878835  [  128/  130]
train() client id: f_00008-11-0 loss: 0.793460  [   32/  130]
train() client id: f_00008-11-1 loss: 0.809993  [   64/  130]
train() client id: f_00008-11-2 loss: 1.026269  [   96/  130]
train() client id: f_00008-11-3 loss: 1.054837  [  128/  130]
train() client id: f_00008-12-0 loss: 0.912766  [   32/  130]
train() client id: f_00008-12-1 loss: 0.964344  [   64/  130]
train() client id: f_00008-12-2 loss: 0.992099  [   96/  130]
train() client id: f_00008-12-3 loss: 0.840128  [  128/  130]
train() client id: f_00009-0-0 loss: 1.161906  [   32/  118]
train() client id: f_00009-0-1 loss: 1.149542  [   64/  118]
train() client id: f_00009-0-2 loss: 1.162893  [   96/  118]
train() client id: f_00009-1-0 loss: 1.119529  [   32/  118]
train() client id: f_00009-1-1 loss: 1.133751  [   64/  118]
train() client id: f_00009-1-2 loss: 1.126396  [   96/  118]
train() client id: f_00009-2-0 loss: 1.109213  [   32/  118]
train() client id: f_00009-2-1 loss: 1.100554  [   64/  118]
train() client id: f_00009-2-2 loss: 1.057091  [   96/  118]
train() client id: f_00009-3-0 loss: 1.061323  [   32/  118]
train() client id: f_00009-3-1 loss: 1.105446  [   64/  118]
train() client id: f_00009-3-2 loss: 1.029279  [   96/  118]
train() client id: f_00009-4-0 loss: 1.040719  [   32/  118]
train() client id: f_00009-4-1 loss: 1.062530  [   64/  118]
train() client id: f_00009-4-2 loss: 1.024634  [   96/  118]
train() client id: f_00009-5-0 loss: 1.030265  [   32/  118]
train() client id: f_00009-5-1 loss: 1.021268  [   64/  118]
train() client id: f_00009-5-2 loss: 0.999542  [   96/  118]
train() client id: f_00009-6-0 loss: 1.019138  [   32/  118]
train() client id: f_00009-6-1 loss: 1.020530  [   64/  118]
train() client id: f_00009-6-2 loss: 0.945835  [   96/  118]
train() client id: f_00009-7-0 loss: 0.965777  [   32/  118]
train() client id: f_00009-7-1 loss: 0.972003  [   64/  118]
train() client id: f_00009-7-2 loss: 1.013416  [   96/  118]
train() client id: f_00009-8-0 loss: 0.980384  [   32/  118]
train() client id: f_00009-8-1 loss: 0.970284  [   64/  118]
train() client id: f_00009-8-2 loss: 0.965592  [   96/  118]
train() client id: f_00009-9-0 loss: 0.962214  [   32/  118]
train() client id: f_00009-9-1 loss: 0.930524  [   64/  118]
train() client id: f_00009-9-2 loss: 1.012541  [   96/  118]
train() client id: f_00009-10-0 loss: 0.970918  [   32/  118]
train() client id: f_00009-10-1 loss: 0.949517  [   64/  118]
train() client id: f_00009-10-2 loss: 0.941586  [   96/  118]
train() client id: f_00009-11-0 loss: 0.963059  [   32/  118]
train() client id: f_00009-11-1 loss: 0.908266  [   64/  118]
train() client id: f_00009-11-2 loss: 1.009089  [   96/  118]
train() client id: f_00009-12-0 loss: 1.012861  [   32/  118]
train() client id: f_00009-12-1 loss: 0.889388  [   64/  118]
train() client id: f_00009-12-2 loss: 0.955352  [   96/  118]
At round 2 accuracy: 0.6286472148541115
At round 2 training accuracy: 0.574111334674715
At round 2 training loss: 0.9246347023457748
update_location
xs = [ -3.9056584   -5.79968212  30.00902392  18.81129433 -24.02070377
 -11.04359014   2.55680806  -6.32485185  14.66397685  -7.06087855]
ys = [ 22.5879595   15.55583871   1.32061395   7.54482414   9.35018685
 -17.18584926   2.37501568  -4.17765202  17.56900603   4.00148178]
dists_uav = [102.59371366 101.36873498 104.41401026 102.0332748  103.26867968
 102.0652453  100.06087131 100.28687116 102.58509731 100.32879877]
dists_bs = [229.13468488 232.48911402 268.68797708 256.13288296 224.13078434
 252.62047619 247.64047589 246.08115854 246.48923299 239.67486674]
uav_gains = [9.37984848e-11 9.66580426e-11 8.97635667e-11 9.50918527e-11
 9.22732701e-11 9.50174026e-11 9.98476140e-11 9.92860273e-11
 9.38181825e-11 9.91823280e-11]
bs_gains = [2.72295051e-11 2.61436814e-11 1.74340799e-11 1.99338479e-11
 2.89660886e-11 2.07196372e-11 2.19075354e-11 2.22984497e-11
 2.21952385e-11 2.40077301e-11]
Round 3
-------------------------------
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.59821531 22.14976829 10.43417884  3.73162013 25.53613015 12.31863725
  4.64005817 14.98083601 10.99100861  9.99386679]
obj_prev = 125.37431955255585
eta_min = 1.4920442712219943e-09	eta_max = 0.9182779578592448
af = 26.51569137072327	bf = 2.011304995713791	zeta = 29.1672605077956	eta = 0.9090909090909091
af = 26.51569137072327	bf = 2.011304995713791	zeta = 49.923723386827525	eta = 0.531124074325744
af = 26.51569137072327	bf = 2.011304995713791	zeta = 40.07321095605609	eta = 0.6616812263883753
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.31157890911957	eta = 0.692106462999664
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22584319738169	eta = 0.6936587699009733
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22562613755516	eta = 0.6936627087626083
eta = 0.6936627087626083
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.03018242 0.0634789  0.02970333 0.01030035 0.07330016 0.03497328
 0.01293532 0.0428782  0.03114059 0.02826607]
ene_total = [3.24068386 6.36286468 3.20278272 1.47684991 7.21615316 3.87841894
 1.70546401 4.36028936 3.52564504 3.25647446]
ti_comp = [0.27298014 0.25372925 0.27246494 0.27313894 0.25568174 0.24894535
 0.27369854 0.27363436 0.25041517 0.25203535]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.30610989e-05 2.48328765e-04 2.20634935e-05 9.15521600e-07
 3.76526319e-04 4.31400861e-05 1.80578817e-06 6.58033963e-05
 3.00981129e-05 2.22204868e-05]
ene_total = [0.56723758 0.75785384 0.57171611 0.56386684 0.75191015 0.7820715
 0.55898529 0.56522719 0.76788634 0.75282607]
optimize_network iter = 0 obj = 6.639580919785389
eta = 0.6936627087626083
freqs = [5.52831753e+07 1.25091797e+08 5.45085400e+07 1.88555117e+07
 1.43342577e+08 7.02428831e+07 2.36306014e+07 7.83494392e+07
 6.21779230e+07 5.60756135e+07]
eta_min = 0.6679443667432773	eta_max = 0.6936627087626027
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 0.06810536589768883	eta = 0.9090909090909091
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 22.189266299545174	eta = 0.002790266616394072
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.3821060289713065	eta = 0.025991273371083312
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969923954010767	eta = 0.026954363942109684
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969507848071644	eta = 0.02695485223602507
eta = 0.02695485223602507
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.27830406e-04 2.45334550e-03 2.17974637e-04 9.04482732e-06
 3.71986367e-03 4.26199261e-04 1.78401495e-05 6.50099743e-04
 2.97352060e-04 2.19525641e-04]
ene_total = [0.18429415 0.3001259  0.18545689 0.17774529 0.33002202 0.25690445
 0.17642907 0.19425277 0.24920683 0.24251343]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 1 obj = 6.13145379564688
eta = 0.6679443667432773
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
eta_min = 0.6679443667432772	eta_max = 0.6679443667432751
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 0.06726955400122223	eta = 0.9090909090909091
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 22.188469685723824	eta = 0.002756122475650405
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.378298126804526	eta = 0.025713403762074868
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.294165378733591	eta = 0.026656378205336263
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.2941251141107752	eta = 0.02665684605646063
eta = 0.02665684605646063
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.28418763e-04 2.42502502e-03 2.18460004e-04 9.06917459e-06
 3.68255925e-03 4.19660878e-04 1.78950352e-05 6.52071157e-04
 2.93141173e-04 2.16699410e-04]
ene_total = [0.18425414 0.2992439  0.18541365 0.17769155 0.32888024 0.25664338
 0.17637659 0.19424829 0.24901305 0.24236033]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 2 obj = 6.1314537956468795
eta = 0.6679443667432772
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
Done!
At round 3 energy consumption: 6.13145379564688
At round 3 eta: 0.6679443667432772
At round 3 local rounds: 13.214290123550223
At round 3 global rounds: 81.77836059884595
At round 3 a_n: 26.812419468377104
gradient difference: 0.3963269889354706
train() client id: f_00000-0-0 loss: 1.471427  [   32/  126]
train() client id: f_00000-0-1 loss: 1.313210  [   64/  126]
train() client id: f_00000-0-2 loss: 1.271918  [   96/  126]
train() client id: f_00000-1-0 loss: 1.233976  [   32/  126]
train() client id: f_00000-1-1 loss: 1.344507  [   64/  126]
train() client id: f_00000-1-2 loss: 1.181219  [   96/  126]
train() client id: f_00000-2-0 loss: 1.224302  [   32/  126]
train() client id: f_00000-2-1 loss: 1.119074  [   64/  126]
train() client id: f_00000-2-2 loss: 1.063164  [   96/  126]
train() client id: f_00000-3-0 loss: 1.055774  [   32/  126]
train() client id: f_00000-3-1 loss: 1.072190  [   64/  126]
train() client id: f_00000-3-2 loss: 1.021412  [   96/  126]
train() client id: f_00000-4-0 loss: 0.994372  [   32/  126]
train() client id: f_00000-4-1 loss: 1.018972  [   64/  126]
train() client id: f_00000-4-2 loss: 0.969963  [   96/  126]
train() client id: f_00000-5-0 loss: 0.965751  [   32/  126]
train() client id: f_00000-5-1 loss: 0.963859  [   64/  126]
train() client id: f_00000-5-2 loss: 0.958413  [   96/  126]
train() client id: f_00000-6-0 loss: 0.938928  [   32/  126]
train() client id: f_00000-6-1 loss: 0.928290  [   64/  126]
train() client id: f_00000-6-2 loss: 0.934954  [   96/  126]
train() client id: f_00000-7-0 loss: 0.886811  [   32/  126]
train() client id: f_00000-7-1 loss: 0.953653  [   64/  126]
train() client id: f_00000-7-2 loss: 0.893049  [   96/  126]
train() client id: f_00000-8-0 loss: 0.908734  [   32/  126]
train() client id: f_00000-8-1 loss: 0.907438  [   64/  126]
train() client id: f_00000-8-2 loss: 0.903909  [   96/  126]
train() client id: f_00000-9-0 loss: 0.967801  [   32/  126]
train() client id: f_00000-9-1 loss: 0.823805  [   64/  126]
train() client id: f_00000-9-2 loss: 0.849256  [   96/  126]
train() client id: f_00000-10-0 loss: 0.878380  [   32/  126]
train() client id: f_00000-10-1 loss: 0.896461  [   64/  126]
train() client id: f_00000-10-2 loss: 0.912182  [   96/  126]
train() client id: f_00000-11-0 loss: 0.825997  [   32/  126]
train() client id: f_00000-11-1 loss: 0.953456  [   64/  126]
train() client id: f_00000-11-2 loss: 0.885135  [   96/  126]
train() client id: f_00000-12-0 loss: 0.883536  [   32/  126]
train() client id: f_00000-12-1 loss: 0.869909  [   64/  126]
train() client id: f_00000-12-2 loss: 0.829384  [   96/  126]
train() client id: f_00001-0-0 loss: 0.711735  [   32/  265]
train() client id: f_00001-0-1 loss: 0.724191  [   64/  265]
train() client id: f_00001-0-2 loss: 0.683035  [   96/  265]
train() client id: f_00001-0-3 loss: 0.691933  [  128/  265]
train() client id: f_00001-0-4 loss: 0.714590  [  160/  265]
train() client id: f_00001-0-5 loss: 0.670256  [  192/  265]
train() client id: f_00001-0-6 loss: 0.697047  [  224/  265]
train() client id: f_00001-0-7 loss: 0.654542  [  256/  265]
train() client id: f_00001-1-0 loss: 0.670442  [   32/  265]
train() client id: f_00001-1-1 loss: 0.644647  [   64/  265]
train() client id: f_00001-1-2 loss: 0.705510  [   96/  265]
train() client id: f_00001-1-3 loss: 0.632782  [  128/  265]
train() client id: f_00001-1-4 loss: 0.679221  [  160/  265]
train() client id: f_00001-1-5 loss: 0.591030  [  192/  265]
train() client id: f_00001-1-6 loss: 0.618084  [  224/  265]
train() client id: f_00001-1-7 loss: 0.630026  [  256/  265]
train() client id: f_00001-2-0 loss: 0.658594  [   32/  265]
train() client id: f_00001-2-1 loss: 0.624078  [   64/  265]
train() client id: f_00001-2-2 loss: 0.641073  [   96/  265]
train() client id: f_00001-2-3 loss: 0.634629  [  128/  265]
train() client id: f_00001-2-4 loss: 0.577157  [  160/  265]
train() client id: f_00001-2-5 loss: 0.591527  [  192/  265]
train() client id: f_00001-2-6 loss: 0.606590  [  224/  265]
train() client id: f_00001-2-7 loss: 0.549131  [  256/  265]
train() client id: f_00001-3-0 loss: 0.591536  [   32/  265]
train() client id: f_00001-3-1 loss: 0.642884  [   64/  265]
train() client id: f_00001-3-2 loss: 0.611237  [   96/  265]
train() client id: f_00001-3-3 loss: 0.593225  [  128/  265]
train() client id: f_00001-3-4 loss: 0.499817  [  160/  265]
train() client id: f_00001-3-5 loss: 0.587960  [  192/  265]
train() client id: f_00001-3-6 loss: 0.605330  [  224/  265]
train() client id: f_00001-3-7 loss: 0.557660  [  256/  265]
train() client id: f_00001-4-0 loss: 0.550732  [   32/  265]
train() client id: f_00001-4-1 loss: 0.598908  [   64/  265]
train() client id: f_00001-4-2 loss: 0.576394  [   96/  265]
train() client id: f_00001-4-3 loss: 0.603842  [  128/  265]
train() client id: f_00001-4-4 loss: 0.610656  [  160/  265]
train() client id: f_00001-4-5 loss: 0.562356  [  192/  265]
train() client id: f_00001-4-6 loss: 0.511607  [  224/  265]
train() client id: f_00001-4-7 loss: 0.548082  [  256/  265]
train() client id: f_00001-5-0 loss: 0.543969  [   32/  265]
train() client id: f_00001-5-1 loss: 0.492625  [   64/  265]
train() client id: f_00001-5-2 loss: 0.548008  [   96/  265]
train() client id: f_00001-5-3 loss: 0.602845  [  128/  265]
train() client id: f_00001-5-4 loss: 0.592953  [  160/  265]
train() client id: f_00001-5-5 loss: 0.559109  [  192/  265]
train() client id: f_00001-5-6 loss: 0.486633  [  224/  265]
train() client id: f_00001-5-7 loss: 0.630854  [  256/  265]
train() client id: f_00001-6-0 loss: 0.566178  [   32/  265]
train() client id: f_00001-6-1 loss: 0.483921  [   64/  265]
train() client id: f_00001-6-2 loss: 0.595731  [   96/  265]
train() client id: f_00001-6-3 loss: 0.573932  [  128/  265]
train() client id: f_00001-6-4 loss: 0.510638  [  160/  265]
train() client id: f_00001-6-5 loss: 0.626167  [  192/  265]
train() client id: f_00001-6-6 loss: 0.516815  [  224/  265]
train() client id: f_00001-6-7 loss: 0.515702  [  256/  265]
train() client id: f_00001-7-0 loss: 0.507147  [   32/  265]
train() client id: f_00001-7-1 loss: 0.596189  [   64/  265]
train() client id: f_00001-7-2 loss: 0.452341  [   96/  265]
train() client id: f_00001-7-3 loss: 0.585174  [  128/  265]
train() client id: f_00001-7-4 loss: 0.588708  [  160/  265]
train() client id: f_00001-7-5 loss: 0.484923  [  192/  265]
train() client id: f_00001-7-6 loss: 0.453512  [  224/  265]
train() client id: f_00001-7-7 loss: 0.628944  [  256/  265]
train() client id: f_00001-8-0 loss: 0.646552  [   32/  265]
train() client id: f_00001-8-1 loss: 0.509350  [   64/  265]
train() client id: f_00001-8-2 loss: 0.539364  [   96/  265]
train() client id: f_00001-8-3 loss: 0.555076  [  128/  265]
train() client id: f_00001-8-4 loss: 0.483028  [  160/  265]
train() client id: f_00001-8-5 loss: 0.462613  [  192/  265]
train() client id: f_00001-8-6 loss: 0.553640  [  224/  265]
train() client id: f_00001-8-7 loss: 0.570303  [  256/  265]
train() client id: f_00001-9-0 loss: 0.465702  [   32/  265]
train() client id: f_00001-9-1 loss: 0.522354  [   64/  265]
train() client id: f_00001-9-2 loss: 0.537326  [   96/  265]
train() client id: f_00001-9-3 loss: 0.569918  [  128/  265]
train() client id: f_00001-9-4 loss: 0.516640  [  160/  265]
train() client id: f_00001-9-5 loss: 0.502574  [  192/  265]
train() client id: f_00001-9-6 loss: 0.626695  [  224/  265]
train() client id: f_00001-9-7 loss: 0.499836  [  256/  265]
train() client id: f_00001-10-0 loss: 0.535906  [   32/  265]
train() client id: f_00001-10-1 loss: 0.535930  [   64/  265]
train() client id: f_00001-10-2 loss: 0.448628  [   96/  265]
train() client id: f_00001-10-3 loss: 0.587501  [  128/  265]
train() client id: f_00001-10-4 loss: 0.566940  [  160/  265]
train() client id: f_00001-10-5 loss: 0.633181  [  192/  265]
train() client id: f_00001-10-6 loss: 0.486918  [  224/  265]
train() client id: f_00001-10-7 loss: 0.429809  [  256/  265]
train() client id: f_00001-11-0 loss: 0.557054  [   32/  265]
train() client id: f_00001-11-1 loss: 0.559190  [   64/  265]
train() client id: f_00001-11-2 loss: 0.560996  [   96/  265]
train() client id: f_00001-11-3 loss: 0.447296  [  128/  265]
train() client id: f_00001-11-4 loss: 0.482206  [  160/  265]
train() client id: f_00001-11-5 loss: 0.579002  [  192/  265]
train() client id: f_00001-11-6 loss: 0.569570  [  224/  265]
train() client id: f_00001-11-7 loss: 0.511407  [  256/  265]
train() client id: f_00001-12-0 loss: 0.521211  [   32/  265]
train() client id: f_00001-12-1 loss: 0.561585  [   64/  265]
train() client id: f_00001-12-2 loss: 0.569371  [   96/  265]
train() client id: f_00001-12-3 loss: 0.498773  [  128/  265]
train() client id: f_00001-12-4 loss: 0.580567  [  160/  265]
train() client id: f_00001-12-5 loss: 0.507958  [  192/  265]
train() client id: f_00001-12-6 loss: 0.490613  [  224/  265]
train() client id: f_00001-12-7 loss: 0.545355  [  256/  265]
train() client id: f_00002-0-0 loss: 1.108029  [   32/  124]
train() client id: f_00002-0-1 loss: 1.101628  [   64/  124]
train() client id: f_00002-0-2 loss: 1.074074  [   96/  124]
train() client id: f_00002-1-0 loss: 1.087229  [   32/  124]
train() client id: f_00002-1-1 loss: 1.054093  [   64/  124]
train() client id: f_00002-1-2 loss: 1.064955  [   96/  124]
train() client id: f_00002-2-0 loss: 1.065646  [   32/  124]
train() client id: f_00002-2-1 loss: 1.053910  [   64/  124]
train() client id: f_00002-2-2 loss: 1.043350  [   96/  124]
train() client id: f_00002-3-0 loss: 1.038624  [   32/  124]
train() client id: f_00002-3-1 loss: 1.023034  [   64/  124]
train() client id: f_00002-3-2 loss: 1.029816  [   96/  124]
train() client id: f_00002-4-0 loss: 1.032542  [   32/  124]
train() client id: f_00002-4-1 loss: 0.965099  [   64/  124]
train() client id: f_00002-4-2 loss: 1.064450  [   96/  124]
train() client id: f_00002-5-0 loss: 1.015346  [   32/  124]
train() client id: f_00002-5-1 loss: 1.007189  [   64/  124]
train() client id: f_00002-5-2 loss: 0.952110  [   96/  124]
train() client id: f_00002-6-0 loss: 0.938730  [   32/  124]
train() client id: f_00002-6-1 loss: 0.998580  [   64/  124]
train() client id: f_00002-6-2 loss: 0.977876  [   96/  124]
train() client id: f_00002-7-0 loss: 0.947731  [   32/  124]
train() client id: f_00002-7-1 loss: 0.984105  [   64/  124]
train() client id: f_00002-7-2 loss: 0.966383  [   96/  124]
train() client id: f_00002-8-0 loss: 0.955745  [   32/  124]
train() client id: f_00002-8-1 loss: 0.985865  [   64/  124]
train() client id: f_00002-8-2 loss: 0.980553  [   96/  124]
train() client id: f_00002-9-0 loss: 0.891254  [   32/  124]
train() client id: f_00002-9-1 loss: 0.997659  [   64/  124]
train() client id: f_00002-9-2 loss: 0.967963  [   96/  124]
train() client id: f_00002-10-0 loss: 0.947750  [   32/  124]
train() client id: f_00002-10-1 loss: 0.924843  [   64/  124]
train() client id: f_00002-10-2 loss: 0.964654  [   96/  124]
train() client id: f_00002-11-0 loss: 0.865786  [   32/  124]
train() client id: f_00002-11-1 loss: 0.990810  [   64/  124]
train() client id: f_00002-11-2 loss: 0.916936  [   96/  124]
train() client id: f_00002-12-0 loss: 0.942612  [   32/  124]
train() client id: f_00002-12-1 loss: 0.919611  [   64/  124]
train() client id: f_00002-12-2 loss: 0.900566  [   96/  124]
train() client id: f_00003-0-0 loss: 0.970082  [   32/   43]
train() client id: f_00003-1-0 loss: 1.060204  [   32/   43]
train() client id: f_00003-2-0 loss: 1.034028  [   32/   43]
train() client id: f_00003-3-0 loss: 1.062411  [   32/   43]
train() client id: f_00003-4-0 loss: 0.936054  [   32/   43]
train() client id: f_00003-5-0 loss: 1.001500  [   32/   43]
train() client id: f_00003-6-0 loss: 0.917236  [   32/   43]
train() client id: f_00003-7-0 loss: 0.983943  [   32/   43]
train() client id: f_00003-8-0 loss: 0.959274  [   32/   43]
train() client id: f_00003-9-0 loss: 1.048182  [   32/   43]
train() client id: f_00003-10-0 loss: 1.008907  [   32/   43]
train() client id: f_00003-11-0 loss: 0.980722  [   32/   43]
train() client id: f_00003-12-0 loss: 0.942886  [   32/   43]
train() client id: f_00004-0-0 loss: 1.049374  [   32/  306]
train() client id: f_00004-0-1 loss: 0.998758  [   64/  306]
train() client id: f_00004-0-2 loss: 0.971089  [   96/  306]
train() client id: f_00004-0-3 loss: 1.040251  [  128/  306]
train() client id: f_00004-0-4 loss: 0.998051  [  160/  306]
train() client id: f_00004-0-5 loss: 0.955655  [  192/  306]
train() client id: f_00004-0-6 loss: 0.935113  [  224/  306]
train() client id: f_00004-0-7 loss: 1.153157  [  256/  306]
train() client id: f_00004-0-8 loss: 1.012085  [  288/  306]
train() client id: f_00004-1-0 loss: 1.045825  [   32/  306]
train() client id: f_00004-1-1 loss: 0.979860  [   64/  306]
train() client id: f_00004-1-2 loss: 1.044473  [   96/  306]
train() client id: f_00004-1-3 loss: 1.006522  [  128/  306]
train() client id: f_00004-1-4 loss: 0.942601  [  160/  306]
train() client id: f_00004-1-5 loss: 0.987955  [  192/  306]
train() client id: f_00004-1-6 loss: 0.993164  [  224/  306]
train() client id: f_00004-1-7 loss: 0.981950  [  256/  306]
train() client id: f_00004-1-8 loss: 1.010570  [  288/  306]
train() client id: f_00004-2-0 loss: 0.940653  [   32/  306]
train() client id: f_00004-2-1 loss: 1.043972  [   64/  306]
train() client id: f_00004-2-2 loss: 0.885309  [   96/  306]
train() client id: f_00004-2-3 loss: 1.048077  [  128/  306]
train() client id: f_00004-2-4 loss: 0.950488  [  160/  306]
train() client id: f_00004-2-5 loss: 1.051535  [  192/  306]
train() client id: f_00004-2-6 loss: 0.988961  [  224/  306]
train() client id: f_00004-2-7 loss: 0.943337  [  256/  306]
train() client id: f_00004-2-8 loss: 1.090867  [  288/  306]
train() client id: f_00004-3-0 loss: 0.965326  [   32/  306]
train() client id: f_00004-3-1 loss: 0.868386  [   64/  306]
train() client id: f_00004-3-2 loss: 0.982662  [   96/  306]
train() client id: f_00004-3-3 loss: 0.975076  [  128/  306]
train() client id: f_00004-3-4 loss: 1.041118  [  160/  306]
train() client id: f_00004-3-5 loss: 1.080596  [  192/  306]
train() client id: f_00004-3-6 loss: 0.946216  [  224/  306]
train() client id: f_00004-3-7 loss: 1.065579  [  256/  306]
train() client id: f_00004-3-8 loss: 0.968567  [  288/  306]
train() client id: f_00004-4-0 loss: 0.937181  [   32/  306]
train() client id: f_00004-4-1 loss: 1.076016  [   64/  306]
train() client id: f_00004-4-2 loss: 0.963604  [   96/  306]
train() client id: f_00004-4-3 loss: 0.985845  [  128/  306]
train() client id: f_00004-4-4 loss: 1.004714  [  160/  306]
train() client id: f_00004-4-5 loss: 0.947641  [  192/  306]
train() client id: f_00004-4-6 loss: 0.895305  [  224/  306]
train() client id: f_00004-4-7 loss: 0.938547  [  256/  306]
train() client id: f_00004-4-8 loss: 0.978484  [  288/  306]
train() client id: f_00004-5-0 loss: 1.013775  [   32/  306]
train() client id: f_00004-5-1 loss: 0.982821  [   64/  306]
train() client id: f_00004-5-2 loss: 0.936899  [   96/  306]
train() client id: f_00004-5-3 loss: 0.974003  [  128/  306]
train() client id: f_00004-5-4 loss: 0.881515  [  160/  306]
train() client id: f_00004-5-5 loss: 0.914839  [  192/  306]
train() client id: f_00004-5-6 loss: 0.983738  [  224/  306]
train() client id: f_00004-5-7 loss: 1.065557  [  256/  306]
train() client id: f_00004-5-8 loss: 1.023151  [  288/  306]
train() client id: f_00004-6-0 loss: 1.059022  [   32/  306]
train() client id: f_00004-6-1 loss: 1.038222  [   64/  306]
train() client id: f_00004-6-2 loss: 0.879091  [   96/  306]
train() client id: f_00004-6-3 loss: 0.961959  [  128/  306]
train() client id: f_00004-6-4 loss: 1.001981  [  160/  306]
train() client id: f_00004-6-5 loss: 0.917789  [  192/  306]
train() client id: f_00004-6-6 loss: 0.950751  [  224/  306]
train() client id: f_00004-6-7 loss: 0.945986  [  256/  306]
train() client id: f_00004-6-8 loss: 0.981677  [  288/  306]
train() client id: f_00004-7-0 loss: 0.986961  [   32/  306]
train() client id: f_00004-7-1 loss: 0.994870  [   64/  306]
train() client id: f_00004-7-2 loss: 0.859092  [   96/  306]
train() client id: f_00004-7-3 loss: 0.984657  [  128/  306]
train() client id: f_00004-7-4 loss: 1.111415  [  160/  306]
train() client id: f_00004-7-5 loss: 0.979926  [  192/  306]
train() client id: f_00004-7-6 loss: 1.000507  [  224/  306]
train() client id: f_00004-7-7 loss: 0.874049  [  256/  306]
train() client id: f_00004-7-8 loss: 0.964837  [  288/  306]
train() client id: f_00004-8-0 loss: 1.032025  [   32/  306]
train() client id: f_00004-8-1 loss: 0.994833  [   64/  306]
train() client id: f_00004-8-2 loss: 0.917469  [   96/  306]
train() client id: f_00004-8-3 loss: 1.017422  [  128/  306]
train() client id: f_00004-8-4 loss: 0.832951  [  160/  306]
train() client id: f_00004-8-5 loss: 1.089116  [  192/  306]
train() client id: f_00004-8-6 loss: 0.883104  [  224/  306]
train() client id: f_00004-8-7 loss: 1.038364  [  256/  306]
train() client id: f_00004-8-8 loss: 0.972949  [  288/  306]
train() client id: f_00004-9-0 loss: 1.035307  [   32/  306]
train() client id: f_00004-9-1 loss: 0.927394  [   64/  306]
train() client id: f_00004-9-2 loss: 0.974354  [   96/  306]
train() client id: f_00004-9-3 loss: 0.904212  [  128/  306]
train() client id: f_00004-9-4 loss: 1.065456  [  160/  306]
train() client id: f_00004-9-5 loss: 1.013453  [  192/  306]
train() client id: f_00004-9-6 loss: 0.939677  [  224/  306]
train() client id: f_00004-9-7 loss: 0.958199  [  256/  306]
train() client id: f_00004-9-8 loss: 0.928375  [  288/  306]
train() client id: f_00004-10-0 loss: 0.879171  [   32/  306]
train() client id: f_00004-10-1 loss: 0.859454  [   64/  306]
train() client id: f_00004-10-2 loss: 0.974959  [   96/  306]
train() client id: f_00004-10-3 loss: 0.896260  [  128/  306]
train() client id: f_00004-10-4 loss: 0.988178  [  160/  306]
train() client id: f_00004-10-5 loss: 1.021103  [  192/  306]
train() client id: f_00004-10-6 loss: 1.028112  [  224/  306]
train() client id: f_00004-10-7 loss: 1.020919  [  256/  306]
train() client id: f_00004-10-8 loss: 1.060484  [  288/  306]
train() client id: f_00004-11-0 loss: 0.893652  [   32/  306]
train() client id: f_00004-11-1 loss: 0.905855  [   64/  306]
train() client id: f_00004-11-2 loss: 0.933068  [   96/  306]
train() client id: f_00004-11-3 loss: 0.993677  [  128/  306]
train() client id: f_00004-11-4 loss: 0.904717  [  160/  306]
train() client id: f_00004-11-5 loss: 1.003704  [  192/  306]
train() client id: f_00004-11-6 loss: 1.001141  [  224/  306]
train() client id: f_00004-11-7 loss: 1.005686  [  256/  306]
train() client id: f_00004-11-8 loss: 1.017242  [  288/  306]
train() client id: f_00004-12-0 loss: 1.041359  [   32/  306]
train() client id: f_00004-12-1 loss: 1.004778  [   64/  306]
train() client id: f_00004-12-2 loss: 0.899582  [   96/  306]
train() client id: f_00004-12-3 loss: 0.860112  [  128/  306]
train() client id: f_00004-12-4 loss: 1.069041  [  160/  306]
train() client id: f_00004-12-5 loss: 0.864893  [  192/  306]
train() client id: f_00004-12-6 loss: 1.029151  [  224/  306]
train() client id: f_00004-12-7 loss: 0.988509  [  256/  306]
train() client id: f_00004-12-8 loss: 0.959499  [  288/  306]
train() client id: f_00005-0-0 loss: 0.914235  [   32/  146]
train() client id: f_00005-0-1 loss: 0.921129  [   64/  146]
train() client id: f_00005-0-2 loss: 0.860349  [   96/  146]
train() client id: f_00005-0-3 loss: 0.964813  [  128/  146]
train() client id: f_00005-1-0 loss: 0.978235  [   32/  146]
train() client id: f_00005-1-1 loss: 0.896047  [   64/  146]
train() client id: f_00005-1-2 loss: 0.823140  [   96/  146]
train() client id: f_00005-1-3 loss: 0.905671  [  128/  146]
train() client id: f_00005-2-0 loss: 0.872395  [   32/  146]
train() client id: f_00005-2-1 loss: 0.914865  [   64/  146]
train() client id: f_00005-2-2 loss: 0.871132  [   96/  146]
train() client id: f_00005-2-3 loss: 0.908878  [  128/  146]
train() client id: f_00005-3-0 loss: 0.907095  [   32/  146]
train() client id: f_00005-3-1 loss: 0.822320  [   64/  146]
train() client id: f_00005-3-2 loss: 0.835182  [   96/  146]
train() client id: f_00005-3-3 loss: 0.900885  [  128/  146]
train() client id: f_00005-4-0 loss: 0.994958  [   32/  146]
train() client id: f_00005-4-1 loss: 0.895853  [   64/  146]
train() client id: f_00005-4-2 loss: 0.812830  [   96/  146]
train() client id: f_00005-4-3 loss: 0.844945  [  128/  146]
train() client id: f_00005-5-0 loss: 0.843614  [   32/  146]
train() client id: f_00005-5-1 loss: 0.860681  [   64/  146]
train() client id: f_00005-5-2 loss: 0.850752  [   96/  146]
train() client id: f_00005-5-3 loss: 0.823385  [  128/  146]
train() client id: f_00005-6-0 loss: 0.891607  [   32/  146]
train() client id: f_00005-6-1 loss: 0.819781  [   64/  146]
train() client id: f_00005-6-2 loss: 0.866933  [   96/  146]
train() client id: f_00005-6-3 loss: 0.907836  [  128/  146]
train() client id: f_00005-7-0 loss: 0.873190  [   32/  146]
train() client id: f_00005-7-1 loss: 0.918301  [   64/  146]
train() client id: f_00005-7-2 loss: 0.829419  [   96/  146]
train() client id: f_00005-7-3 loss: 0.785749  [  128/  146]
train() client id: f_00005-8-0 loss: 0.847854  [   32/  146]
train() client id: f_00005-8-1 loss: 0.799344  [   64/  146]
train() client id: f_00005-8-2 loss: 0.839233  [   96/  146]
train() client id: f_00005-8-3 loss: 0.877678  [  128/  146]
train() client id: f_00005-9-0 loss: 0.764572  [   32/  146]
train() client id: f_00005-9-1 loss: 0.794271  [   64/  146]
train() client id: f_00005-9-2 loss: 0.894913  [   96/  146]
train() client id: f_00005-9-3 loss: 1.056689  [  128/  146]
train() client id: f_00005-10-0 loss: 0.967751  [   32/  146]
train() client id: f_00005-10-1 loss: 0.766284  [   64/  146]
train() client id: f_00005-10-2 loss: 0.723195  [   96/  146]
train() client id: f_00005-10-3 loss: 0.944185  [  128/  146]
train() client id: f_00005-11-0 loss: 0.738994  [   32/  146]
train() client id: f_00005-11-1 loss: 0.851191  [   64/  146]
train() client id: f_00005-11-2 loss: 0.883900  [   96/  146]
train() client id: f_00005-11-3 loss: 1.087296  [  128/  146]
train() client id: f_00005-12-0 loss: 0.899833  [   32/  146]
train() client id: f_00005-12-1 loss: 0.864140  [   64/  146]
train() client id: f_00005-12-2 loss: 0.812762  [   96/  146]
train() client id: f_00005-12-3 loss: 0.795430  [  128/  146]
train() client id: f_00006-0-0 loss: 1.005457  [   32/   54]
train() client id: f_00006-1-0 loss: 1.004526  [   32/   54]
train() client id: f_00006-2-0 loss: 1.047447  [   32/   54]
train() client id: f_00006-3-0 loss: 0.984544  [   32/   54]
train() client id: f_00006-4-0 loss: 1.035058  [   32/   54]
train() client id: f_00006-5-0 loss: 0.989591  [   32/   54]
train() client id: f_00006-6-0 loss: 1.004970  [   32/   54]
train() client id: f_00006-7-0 loss: 1.016741  [   32/   54]
train() client id: f_00006-8-0 loss: 1.023792  [   32/   54]
train() client id: f_00006-9-0 loss: 1.019357  [   32/   54]
train() client id: f_00006-10-0 loss: 0.960729  [   32/   54]
train() client id: f_00006-11-0 loss: 0.999254  [   32/   54]
train() client id: f_00006-12-0 loss: 1.035424  [   32/   54]
train() client id: f_00007-0-0 loss: 0.912237  [   32/  179]
train() client id: f_00007-0-1 loss: 0.931169  [   64/  179]
train() client id: f_00007-0-2 loss: 0.903145  [   96/  179]
train() client id: f_00007-0-3 loss: 0.879859  [  128/  179]
train() client id: f_00007-0-4 loss: 0.942743  [  160/  179]
train() client id: f_00007-1-0 loss: 0.904898  [   32/  179]
train() client id: f_00007-1-1 loss: 0.866980  [   64/  179]
train() client id: f_00007-1-2 loss: 0.881467  [   96/  179]
train() client id: f_00007-1-3 loss: 0.874164  [  128/  179]
train() client id: f_00007-1-4 loss: 0.857798  [  160/  179]
train() client id: f_00007-2-0 loss: 0.809689  [   32/  179]
train() client id: f_00007-2-1 loss: 0.820525  [   64/  179]
train() client id: f_00007-2-2 loss: 0.807310  [   96/  179]
train() client id: f_00007-2-3 loss: 0.864438  [  128/  179]
train() client id: f_00007-2-4 loss: 0.866350  [  160/  179]
train() client id: f_00007-3-0 loss: 0.919585  [   32/  179]
train() client id: f_00007-3-1 loss: 0.862029  [   64/  179]
train() client id: f_00007-3-2 loss: 0.775312  [   96/  179]
train() client id: f_00007-3-3 loss: 0.762536  [  128/  179]
train() client id: f_00007-3-4 loss: 0.821625  [  160/  179]
train() client id: f_00007-4-0 loss: 0.816663  [   32/  179]
train() client id: f_00007-4-1 loss: 0.778786  [   64/  179]
train() client id: f_00007-4-2 loss: 0.818350  [   96/  179]
train() client id: f_00007-4-3 loss: 0.811684  [  128/  179]
train() client id: f_00007-4-4 loss: 0.832560  [  160/  179]
train() client id: f_00007-5-0 loss: 0.824484  [   32/  179]
train() client id: f_00007-5-1 loss: 0.816018  [   64/  179]
train() client id: f_00007-5-2 loss: 0.764387  [   96/  179]
train() client id: f_00007-5-3 loss: 0.806957  [  128/  179]
train() client id: f_00007-5-4 loss: 0.796666  [  160/  179]
train() client id: f_00007-6-0 loss: 0.814995  [   32/  179]
train() client id: f_00007-6-1 loss: 0.769040  [   64/  179]
train() client id: f_00007-6-2 loss: 0.787943  [   96/  179]
train() client id: f_00007-6-3 loss: 0.793033  [  128/  179]
train() client id: f_00007-6-4 loss: 0.761110  [  160/  179]
train() client id: f_00007-7-0 loss: 0.804093  [   32/  179]
train() client id: f_00007-7-1 loss: 0.787327  [   64/  179]
train() client id: f_00007-7-2 loss: 0.858964  [   96/  179]
train() client id: f_00007-7-3 loss: 0.752031  [  128/  179]
train() client id: f_00007-7-4 loss: 0.687871  [  160/  179]
train() client id: f_00007-8-0 loss: 0.721267  [   32/  179]
train() client id: f_00007-8-1 loss: 0.774157  [   64/  179]
train() client id: f_00007-8-2 loss: 0.714765  [   96/  179]
train() client id: f_00007-8-3 loss: 0.810250  [  128/  179]
train() client id: f_00007-8-4 loss: 0.753081  [  160/  179]
train() client id: f_00007-9-0 loss: 0.747818  [   32/  179]
train() client id: f_00007-9-1 loss: 0.839196  [   64/  179]
train() client id: f_00007-9-2 loss: 0.785448  [   96/  179]
train() client id: f_00007-9-3 loss: 0.746650  [  128/  179]
train() client id: f_00007-9-4 loss: 0.756933  [  160/  179]
train() client id: f_00007-10-0 loss: 0.829586  [   32/  179]
train() client id: f_00007-10-1 loss: 0.749811  [   64/  179]
train() client id: f_00007-10-2 loss: 0.830430  [   96/  179]
train() client id: f_00007-10-3 loss: 0.699347  [  128/  179]
train() client id: f_00007-10-4 loss: 0.806871  [  160/  179]
train() client id: f_00007-11-0 loss: 0.710064  [   32/  179]
train() client id: f_00007-11-1 loss: 0.799486  [   64/  179]
train() client id: f_00007-11-2 loss: 0.760626  [   96/  179]
train() client id: f_00007-11-3 loss: 0.855808  [  128/  179]
train() client id: f_00007-11-4 loss: 0.771823  [  160/  179]
train() client id: f_00007-12-0 loss: 0.815456  [   32/  179]
train() client id: f_00007-12-1 loss: 0.811704  [   64/  179]
train() client id: f_00007-12-2 loss: 0.699623  [   96/  179]
train() client id: f_00007-12-3 loss: 0.817030  [  128/  179]
train() client id: f_00007-12-4 loss: 0.747112  [  160/  179]
train() client id: f_00008-0-0 loss: 0.966611  [   32/  130]
train() client id: f_00008-0-1 loss: 0.933574  [   64/  130]
train() client id: f_00008-0-2 loss: 1.023211  [   96/  130]
train() client id: f_00008-0-3 loss: 0.913061  [  128/  130]
train() client id: f_00008-1-0 loss: 0.886441  [   32/  130]
train() client id: f_00008-1-1 loss: 1.027377  [   64/  130]
train() client id: f_00008-1-2 loss: 0.916984  [   96/  130]
train() client id: f_00008-1-3 loss: 0.974016  [  128/  130]
train() client id: f_00008-2-0 loss: 1.023681  [   32/  130]
train() client id: f_00008-2-1 loss: 0.944659  [   64/  130]
train() client id: f_00008-2-2 loss: 0.950913  [   96/  130]
train() client id: f_00008-2-3 loss: 0.912924  [  128/  130]
train() client id: f_00008-3-0 loss: 0.907283  [   32/  130]
train() client id: f_00008-3-1 loss: 1.045996  [   64/  130]
train() client id: f_00008-3-2 loss: 0.894329  [   96/  130]
train() client id: f_00008-3-3 loss: 0.987666  [  128/  130]
train() client id: f_00008-4-0 loss: 0.981776  [   32/  130]
train() client id: f_00008-4-1 loss: 0.947225  [   64/  130]
train() client id: f_00008-4-2 loss: 0.980228  [   96/  130]
train() client id: f_00008-4-3 loss: 0.916989  [  128/  130]
train() client id: f_00008-5-0 loss: 0.948234  [   32/  130]
train() client id: f_00008-5-1 loss: 0.888641  [   64/  130]
train() client id: f_00008-5-2 loss: 1.002764  [   96/  130]
train() client id: f_00008-5-3 loss: 0.970479  [  128/  130]
train() client id: f_00008-6-0 loss: 0.866536  [   32/  130]
train() client id: f_00008-6-1 loss: 1.017905  [   64/  130]
train() client id: f_00008-6-2 loss: 0.972354  [   96/  130]
train() client id: f_00008-6-3 loss: 0.975651  [  128/  130]
train() client id: f_00008-7-0 loss: 0.971857  [   32/  130]
train() client id: f_00008-7-1 loss: 0.913906  [   64/  130]
train() client id: f_00008-7-2 loss: 0.977884  [   96/  130]
train() client id: f_00008-7-3 loss: 0.989319  [  128/  130]
train() client id: f_00008-8-0 loss: 1.055968  [   32/  130]
train() client id: f_00008-8-1 loss: 0.882048  [   64/  130]
train() client id: f_00008-8-2 loss: 0.987648  [   96/  130]
train() client id: f_00008-8-3 loss: 0.920175  [  128/  130]
train() client id: f_00008-9-0 loss: 0.932230  [   32/  130]
train() client id: f_00008-9-1 loss: 0.946502  [   64/  130]
train() client id: f_00008-9-2 loss: 0.891256  [   96/  130]
train() client id: f_00008-9-3 loss: 1.084739  [  128/  130]
train() client id: f_00008-10-0 loss: 1.081824  [   32/  130]
train() client id: f_00008-10-1 loss: 0.970681  [   64/  130]
train() client id: f_00008-10-2 loss: 0.884641  [   96/  130]
train() client id: f_00008-10-3 loss: 0.934565  [  128/  130]
train() client id: f_00008-11-0 loss: 1.099616  [   32/  130]
train() client id: f_00008-11-1 loss: 0.897581  [   64/  130]
train() client id: f_00008-11-2 loss: 0.873503  [   96/  130]
train() client id: f_00008-11-3 loss: 0.960450  [  128/  130]
train() client id: f_00008-12-0 loss: 1.024275  [   32/  130]
train() client id: f_00008-12-1 loss: 0.902253  [   64/  130]
train() client id: f_00008-12-2 loss: 0.956562  [   96/  130]
train() client id: f_00008-12-3 loss: 0.988337  [  128/  130]
train() client id: f_00009-0-0 loss: 1.190381  [   32/  118]
train() client id: f_00009-0-1 loss: 1.173250  [   64/  118]
train() client id: f_00009-0-2 loss: 1.163912  [   96/  118]
train() client id: f_00009-1-0 loss: 1.161576  [   32/  118]
train() client id: f_00009-1-1 loss: 1.135965  [   64/  118]
train() client id: f_00009-1-2 loss: 1.120943  [   96/  118]
train() client id: f_00009-2-0 loss: 1.151849  [   32/  118]
train() client id: f_00009-2-1 loss: 1.085241  [   64/  118]
train() client id: f_00009-2-2 loss: 1.096089  [   96/  118]
train() client id: f_00009-3-0 loss: 1.102411  [   32/  118]
train() client id: f_00009-3-1 loss: 1.084980  [   64/  118]
train() client id: f_00009-3-2 loss: 1.094853  [   96/  118]
train() client id: f_00009-4-0 loss: 1.063796  [   32/  118]
train() client id: f_00009-4-1 loss: 1.080034  [   64/  118]
train() client id: f_00009-4-2 loss: 1.072291  [   96/  118]
train() client id: f_00009-5-0 loss: 1.046587  [   32/  118]
train() client id: f_00009-5-1 loss: 1.023514  [   64/  118]
train() client id: f_00009-5-2 loss: 1.070138  [   96/  118]
train() client id: f_00009-6-0 loss: 1.036200  [   32/  118]
train() client id: f_00009-6-1 loss: 1.042428  [   64/  118]
train() client id: f_00009-6-2 loss: 1.021020  [   96/  118]
train() client id: f_00009-7-0 loss: 1.037267  [   32/  118]
train() client id: f_00009-7-1 loss: 0.980562  [   64/  118]
train() client id: f_00009-7-2 loss: 1.044832  [   96/  118]
train() client id: f_00009-8-0 loss: 1.019882  [   32/  118]
train() client id: f_00009-8-1 loss: 1.055153  [   64/  118]
train() client id: f_00009-8-2 loss: 0.956478  [   96/  118]
train() client id: f_00009-9-0 loss: 1.011699  [   32/  118]
train() client id: f_00009-9-1 loss: 1.019295  [   64/  118]
train() client id: f_00009-9-2 loss: 0.941814  [   96/  118]
train() client id: f_00009-10-0 loss: 1.010912  [   32/  118]
train() client id: f_00009-10-1 loss: 1.022007  [   64/  118]
train() client id: f_00009-10-2 loss: 0.964362  [   96/  118]
train() client id: f_00009-11-0 loss: 0.956551  [   32/  118]
train() client id: f_00009-11-1 loss: 0.966970  [   64/  118]
train() client id: f_00009-11-2 loss: 0.978303  [   96/  118]
train() client id: f_00009-12-0 loss: 1.010805  [   32/  118]
train() client id: f_00009-12-1 loss: 0.990552  [   64/  118]
train() client id: f_00009-12-2 loss: 0.964828  [   96/  118]
At round 3 accuracy: 0.6286472148541115
At round 3 training accuracy: 0.5727699530516432
At round 3 training loss: 0.9014604085854389
update_location
xs = [ -3.9056584   -0.79968212  35.00902392  18.81129433 -19.02070377
  -6.04359014   2.55680806  -6.32485185  19.66397685  -2.06087855]
ys = [ 27.5879595   15.55583871   1.32061395   2.54482414   9.35018685
 -17.18584926  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [103.80919842 101.2058477  105.95931189 101.78575993 102.2213929
 101.64584791 100.06711653 100.2031936  103.41828638 100.10124413]
dists_bs = [225.83972948 236.15289818 272.52214454 259.4293844  227.52890245
 255.89386294 251.15145857 242.46476046 250.35690873 243.20450851]
uav_gains = [9.10767832e-11 9.70474414e-11 8.65263782e-11 9.56710163e-11
 9.46549499e-11 9.60005847e-11 9.98320356e-11 9.94934409e-11
 9.19399105e-11 9.97469659e-11]
bs_gains = [2.83565323e-11 2.50237793e-11 1.67559498e-11 1.92327073e-11
 2.77710106e-11 1.99860260e-11 2.10607635e-11 2.32422371e-11
 2.12484502e-11 2.30448319e-11]
Round 4
-------------------------------
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.46545628 21.87269455 10.30373985  3.68435765 25.21614886 12.16538239
  4.58153103 14.79179615 10.85487015  9.87006751]
obj_prev = 123.8060444419656
eta_min = 1.1887003308068872e-09	eta_max = 0.9184674772800265
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 28.799330597431425	eta = 0.9090909090909091
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 49.438331310801296	eta = 0.5295730850913346
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 39.62791236585137	eta = 0.660675974861339
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.87221877758381	eta = 0.6913038232004766
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78643716203915	eta = 0.6928732000256066
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78621816141451	eta = 0.6928772157665561
eta = 0.6928772157665561
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.03027591 0.06367553 0.02979534 0.01033226 0.07352722 0.03508161
 0.01297539 0.04301102 0.03123705 0.02835363]
ene_total = [3.202787   6.29122282 3.16613857 1.4566637  7.13363507 3.83605127
 1.68315721 4.30566359 3.48877145 3.22212748]
ti_comp = [0.2772398  0.25747119 0.276632   0.27781288 0.25949396 0.25275957
 0.27830054 0.2782619  0.25409312 0.25580165]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.25663118e-05 2.43411145e-04 2.16033125e-05 8.93224701e-07
 3.68952017e-04 4.22380117e-05 1.76284161e-06 6.42261255e-05
 2.95055929e-05 2.17720570e-05]
ene_total = [0.56161072 0.75347478 0.56683292 0.55471554 0.74677556 0.77704532
 0.5505341  0.55632463 0.76429157 0.74870068]
optimize_network iter = 0 obj = 6.580305820795663
eta = 0.6928772157665561
freqs = [5.46023917e+07 1.23655643e+08 5.38537534e+07 1.85957107e+07
 1.41674235e+08 6.93972005e+07 2.33118325e+07 7.72851390e+07
 6.14677254e+07 5.54211297e+07]
eta_min = 0.6721669962857192	eta_max = 0.692877215766553
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 0.06566942832741904	eta = 0.9090909090909091
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 22.05222718616278	eta = 0.002707185981428379
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.358376629903991	eta = 0.02531380252868433
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276155607764052	eta = 0.02622820693542063
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276117460225635	eta = 0.02622864651797696
eta = 0.02622864651797696
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.23933498e-04 2.41545493e-03 2.14377315e-04 8.86378481e-06
 3.66124143e-03 4.19142738e-04 1.74933012e-05 6.37338573e-04
 2.92794440e-04 2.16051827e-04]
ene_total = [0.18267388 0.2974487  0.18408433 0.17517407 0.32618133 0.25545427
 0.17406893 0.19124253 0.24830343 0.241486  ]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 1 obj = 6.169387607456715
eta = 0.6721669962857192
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
eta_min = 0.6721669962857293	eta_max = 0.6721669962857161
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 0.06499207998561081	eta = 0.909090909090909
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 22.05158160424028	eta = 0.0026793411075088985
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.3552731564287246	eta = 0.02508571412048698
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.273849552272878	eta = 0.0259840010165886
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.2738124460945865	eta = 0.025984425047592612
eta = 0.025984425047592612
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.24377690e-04 2.39200478e-03 2.14730208e-04 8.88417650e-06
 3.63033979e-03 4.13806558e-04 1.75382481e-05 6.38962582e-04
 2.89320240e-04 2.13725309e-04]
ene_total = [0.1826403  0.29672859 0.18404788 0.17513071 0.32524889 0.25524333
 0.17402652 0.19123929 0.24814553 0.24136141]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 2 obj = 6.169387607456903
eta = 0.6721669962857293
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
Done!
At round 4 energy consumption: 6.169387607456715
At round 4 eta: 0.6721669962857293
At round 4 local rounds: 13.007932941342311
At round 4 global rounds: 81.78682184099438
At round 4 a_n: 26.46987362140779
gradient difference: 0.39746779203414917
train() client id: f_00000-0-0 loss: 1.444694  [   32/  126]
train() client id: f_00000-0-1 loss: 1.293920  [   64/  126]
train() client id: f_00000-0-2 loss: 1.123047  [   96/  126]
train() client id: f_00000-1-0 loss: 1.145095  [   32/  126]
train() client id: f_00000-1-1 loss: 1.191984  [   64/  126]
train() client id: f_00000-1-2 loss: 1.263088  [   96/  126]
train() client id: f_00000-2-0 loss: 1.176270  [   32/  126]
train() client id: f_00000-2-1 loss: 1.096473  [   64/  126]
train() client id: f_00000-2-2 loss: 1.101539  [   96/  126]
train() client id: f_00000-3-0 loss: 1.085434  [   32/  126]
train() client id: f_00000-3-1 loss: 0.989718  [   64/  126]
train() client id: f_00000-3-2 loss: 1.002611  [   96/  126]
train() client id: f_00000-4-0 loss: 1.017834  [   32/  126]
train() client id: f_00000-4-1 loss: 0.988691  [   64/  126]
train() client id: f_00000-4-2 loss: 0.977070  [   96/  126]
train() client id: f_00000-5-0 loss: 0.950394  [   32/  126]
train() client id: f_00000-5-1 loss: 0.937431  [   64/  126]
train() client id: f_00000-5-2 loss: 0.977409  [   96/  126]
train() client id: f_00000-6-0 loss: 0.931698  [   32/  126]
train() client id: f_00000-6-1 loss: 0.934217  [   64/  126]
train() client id: f_00000-6-2 loss: 0.937405  [   96/  126]
train() client id: f_00000-7-0 loss: 0.961034  [   32/  126]
train() client id: f_00000-7-1 loss: 0.907235  [   64/  126]
train() client id: f_00000-7-2 loss: 0.884707  [   96/  126]
train() client id: f_00000-8-0 loss: 0.908457  [   32/  126]
train() client id: f_00000-8-1 loss: 0.912257  [   64/  126]
train() client id: f_00000-8-2 loss: 0.933992  [   96/  126]
train() client id: f_00000-9-0 loss: 0.885111  [   32/  126]
train() client id: f_00000-9-1 loss: 0.910014  [   64/  126]
train() client id: f_00000-9-2 loss: 0.934147  [   96/  126]
train() client id: f_00000-10-0 loss: 0.882946  [   32/  126]
train() client id: f_00000-10-1 loss: 0.974106  [   64/  126]
train() client id: f_00000-10-2 loss: 0.819459  [   96/  126]
train() client id: f_00000-11-0 loss: 0.863895  [   32/  126]
train() client id: f_00000-11-1 loss: 0.930310  [   64/  126]
train() client id: f_00000-11-2 loss: 0.885927  [   96/  126]
train() client id: f_00000-12-0 loss: 0.888694  [   32/  126]
train() client id: f_00000-12-1 loss: 0.887169  [   64/  126]
train() client id: f_00000-12-2 loss: 0.904684  [   96/  126]
train() client id: f_00001-0-0 loss: 0.682064  [   32/  265]
train() client id: f_00001-0-1 loss: 0.655964  [   64/  265]
train() client id: f_00001-0-2 loss: 0.638471  [   96/  265]
train() client id: f_00001-0-3 loss: 0.675527  [  128/  265]
train() client id: f_00001-0-4 loss: 0.609683  [  160/  265]
train() client id: f_00001-0-5 loss: 0.586829  [  192/  265]
train() client id: f_00001-0-6 loss: 0.572160  [  224/  265]
train() client id: f_00001-0-7 loss: 0.698434  [  256/  265]
train() client id: f_00001-1-0 loss: 0.675760  [   32/  265]
train() client id: f_00001-1-1 loss: 0.681534  [   64/  265]
train() client id: f_00001-1-2 loss: 0.571630  [   96/  265]
train() client id: f_00001-1-3 loss: 0.571634  [  128/  265]
train() client id: f_00001-1-4 loss: 0.567263  [  160/  265]
train() client id: f_00001-1-5 loss: 0.567563  [  192/  265]
train() client id: f_00001-1-6 loss: 0.612347  [  224/  265]
train() client id: f_00001-1-7 loss: 0.553799  [  256/  265]
train() client id: f_00001-2-0 loss: 0.552160  [   32/  265]
train() client id: f_00001-2-1 loss: 0.585463  [   64/  265]
train() client id: f_00001-2-2 loss: 0.570482  [   96/  265]
train() client id: f_00001-2-3 loss: 0.550550  [  128/  265]
train() client id: f_00001-2-4 loss: 0.550467  [  160/  265]
train() client id: f_00001-2-5 loss: 0.596823  [  192/  265]
train() client id: f_00001-2-6 loss: 0.533281  [  224/  265]
train() client id: f_00001-2-7 loss: 0.626859  [  256/  265]
train() client id: f_00001-3-0 loss: 0.502092  [   32/  265]
train() client id: f_00001-3-1 loss: 0.510372  [   64/  265]
train() client id: f_00001-3-2 loss: 0.569391  [   96/  265]
train() client id: f_00001-3-3 loss: 0.635037  [  128/  265]
train() client id: f_00001-3-4 loss: 0.514267  [  160/  265]
train() client id: f_00001-3-5 loss: 0.489945  [  192/  265]
train() client id: f_00001-3-6 loss: 0.514372  [  224/  265]
train() client id: f_00001-3-7 loss: 0.583604  [  256/  265]
train() client id: f_00001-4-0 loss: 0.524957  [   32/  265]
train() client id: f_00001-4-1 loss: 0.535302  [   64/  265]
train() client id: f_00001-4-2 loss: 0.510041  [   96/  265]
train() client id: f_00001-4-3 loss: 0.663072  [  128/  265]
train() client id: f_00001-4-4 loss: 0.577223  [  160/  265]
train() client id: f_00001-4-5 loss: 0.479916  [  192/  265]
train() client id: f_00001-4-6 loss: 0.517049  [  224/  265]
train() client id: f_00001-4-7 loss: 0.492447  [  256/  265]
train() client id: f_00001-5-0 loss: 0.507941  [   32/  265]
train() client id: f_00001-5-1 loss: 0.607956  [   64/  265]
train() client id: f_00001-5-2 loss: 0.554668  [   96/  265]
train() client id: f_00001-5-3 loss: 0.454049  [  128/  265]
train() client id: f_00001-5-4 loss: 0.592522  [  160/  265]
train() client id: f_00001-5-5 loss: 0.484179  [  192/  265]
train() client id: f_00001-5-6 loss: 0.502525  [  224/  265]
train() client id: f_00001-5-7 loss: 0.509668  [  256/  265]
train() client id: f_00001-6-0 loss: 0.547663  [   32/  265]
train() client id: f_00001-6-1 loss: 0.527955  [   64/  265]
train() client id: f_00001-6-2 loss: 0.466474  [   96/  265]
train() client id: f_00001-6-3 loss: 0.538590  [  128/  265]
train() client id: f_00001-6-4 loss: 0.593846  [  160/  265]
train() client id: f_00001-6-5 loss: 0.491552  [  192/  265]
train() client id: f_00001-6-6 loss: 0.504327  [  224/  265]
train() client id: f_00001-6-7 loss: 0.497629  [  256/  265]
train() client id: f_00001-7-0 loss: 0.446314  [   32/  265]
train() client id: f_00001-7-1 loss: 0.538763  [   64/  265]
train() client id: f_00001-7-2 loss: 0.605700  [   96/  265]
train() client id: f_00001-7-3 loss: 0.518371  [  128/  265]
train() client id: f_00001-7-4 loss: 0.475784  [  160/  265]
train() client id: f_00001-7-5 loss: 0.562871  [  192/  265]
train() client id: f_00001-7-6 loss: 0.490009  [  224/  265]
train() client id: f_00001-7-7 loss: 0.434799  [  256/  265]
train() client id: f_00001-8-0 loss: 0.523176  [   32/  265]
train() client id: f_00001-8-1 loss: 0.473653  [   64/  265]
train() client id: f_00001-8-2 loss: 0.553872  [   96/  265]
train() client id: f_00001-8-3 loss: 0.541882  [  128/  265]
train() client id: f_00001-8-4 loss: 0.500162  [  160/  265]
train() client id: f_00001-8-5 loss: 0.453012  [  192/  265]
train() client id: f_00001-8-6 loss: 0.514654  [  224/  265]
train() client id: f_00001-8-7 loss: 0.516454  [  256/  265]
train() client id: f_00001-9-0 loss: 0.452960  [   32/  265]
train() client id: f_00001-9-1 loss: 0.426577  [   64/  265]
train() client id: f_00001-9-2 loss: 0.482622  [   96/  265]
train() client id: f_00001-9-3 loss: 0.633818  [  128/  265]
train() client id: f_00001-9-4 loss: 0.527515  [  160/  265]
train() client id: f_00001-9-5 loss: 0.497654  [  192/  265]
train() client id: f_00001-9-6 loss: 0.424232  [  224/  265]
train() client id: f_00001-9-7 loss: 0.612510  [  256/  265]
train() client id: f_00001-10-0 loss: 0.427212  [   32/  265]
train() client id: f_00001-10-1 loss: 0.593099  [   64/  265]
train() client id: f_00001-10-2 loss: 0.504330  [   96/  265]
train() client id: f_00001-10-3 loss: 0.543368  [  128/  265]
train() client id: f_00001-10-4 loss: 0.463175  [  160/  265]
train() client id: f_00001-10-5 loss: 0.586035  [  192/  265]
train() client id: f_00001-10-6 loss: 0.419435  [  224/  265]
train() client id: f_00001-10-7 loss: 0.503248  [  256/  265]
train() client id: f_00001-11-0 loss: 0.602270  [   32/  265]
train() client id: f_00001-11-1 loss: 0.474651  [   64/  265]
train() client id: f_00001-11-2 loss: 0.512432  [   96/  265]
train() client id: f_00001-11-3 loss: 0.532282  [  128/  265]
train() client id: f_00001-11-4 loss: 0.496878  [  160/  265]
train() client id: f_00001-11-5 loss: 0.426867  [  192/  265]
train() client id: f_00001-11-6 loss: 0.510356  [  224/  265]
train() client id: f_00001-11-7 loss: 0.479606  [  256/  265]
train() client id: f_00001-12-0 loss: 0.521559  [   32/  265]
train() client id: f_00001-12-1 loss: 0.521923  [   64/  265]
train() client id: f_00001-12-2 loss: 0.483700  [   96/  265]
train() client id: f_00001-12-3 loss: 0.421906  [  128/  265]
train() client id: f_00001-12-4 loss: 0.586338  [  160/  265]
train() client id: f_00001-12-5 loss: 0.476619  [  192/  265]
train() client id: f_00001-12-6 loss: 0.487922  [  224/  265]
train() client id: f_00001-12-7 loss: 0.535564  [  256/  265]
train() client id: f_00002-0-0 loss: 1.170621  [   32/  124]
train() client id: f_00002-0-1 loss: 1.115823  [   64/  124]
train() client id: f_00002-0-2 loss: 1.156629  [   96/  124]
train() client id: f_00002-1-0 loss: 1.112065  [   32/  124]
train() client id: f_00002-1-1 loss: 1.104698  [   64/  124]
train() client id: f_00002-1-2 loss: 1.088628  [   96/  124]
train() client id: f_00002-2-0 loss: 1.073260  [   32/  124]
train() client id: f_00002-2-1 loss: 1.041017  [   64/  124]
train() client id: f_00002-2-2 loss: 1.104275  [   96/  124]
train() client id: f_00002-3-0 loss: 1.039366  [   32/  124]
train() client id: f_00002-3-1 loss: 1.065586  [   64/  124]
train() client id: f_00002-3-2 loss: 1.062243  [   96/  124]
train() client id: f_00002-4-0 loss: 0.979163  [   32/  124]
train() client id: f_00002-4-1 loss: 1.066868  [   64/  124]
train() client id: f_00002-4-2 loss: 1.017529  [   96/  124]
train() client id: f_00002-5-0 loss: 1.028147  [   32/  124]
train() client id: f_00002-5-1 loss: 1.012625  [   64/  124]
train() client id: f_00002-5-2 loss: 0.971493  [   96/  124]
train() client id: f_00002-6-0 loss: 1.015757  [   32/  124]
train() client id: f_00002-6-1 loss: 1.029315  [   64/  124]
train() client id: f_00002-6-2 loss: 1.030730  [   96/  124]
train() client id: f_00002-7-0 loss: 0.998413  [   32/  124]
train() client id: f_00002-7-1 loss: 0.958273  [   64/  124]
train() client id: f_00002-7-2 loss: 1.023199  [   96/  124]
train() client id: f_00002-8-0 loss: 0.969697  [   32/  124]
train() client id: f_00002-8-1 loss: 0.992033  [   64/  124]
train() client id: f_00002-8-2 loss: 0.998614  [   96/  124]
train() client id: f_00002-9-0 loss: 0.980127  [   32/  124]
train() client id: f_00002-9-1 loss: 0.996375  [   64/  124]
train() client id: f_00002-9-2 loss: 0.963116  [   96/  124]
train() client id: f_00002-10-0 loss: 0.942684  [   32/  124]
train() client id: f_00002-10-1 loss: 1.009453  [   64/  124]
train() client id: f_00002-10-2 loss: 1.000466  [   96/  124]
train() client id: f_00002-11-0 loss: 0.932242  [   32/  124]
train() client id: f_00002-11-1 loss: 0.892742  [   64/  124]
train() client id: f_00002-11-2 loss: 1.053938  [   96/  124]
train() client id: f_00002-12-0 loss: 1.001507  [   32/  124]
train() client id: f_00002-12-1 loss: 1.002004  [   64/  124]
train() client id: f_00002-12-2 loss: 0.954245  [   96/  124]
train() client id: f_00003-0-0 loss: 0.928360  [   32/   43]
train() client id: f_00003-1-0 loss: 0.984573  [   32/   43]
train() client id: f_00003-2-0 loss: 1.058520  [   32/   43]
train() client id: f_00003-3-0 loss: 1.016821  [   32/   43]
train() client id: f_00003-4-0 loss: 0.949828  [   32/   43]
train() client id: f_00003-5-0 loss: 1.059831  [   32/   43]
train() client id: f_00003-6-0 loss: 0.992765  [   32/   43]
train() client id: f_00003-7-0 loss: 1.057366  [   32/   43]
train() client id: f_00003-8-0 loss: 1.063534  [   32/   43]
train() client id: f_00003-9-0 loss: 0.983324  [   32/   43]
train() client id: f_00003-10-0 loss: 1.006803  [   32/   43]
train() client id: f_00003-11-0 loss: 1.017070  [   32/   43]
train() client id: f_00003-12-0 loss: 1.024366  [   32/   43]
train() client id: f_00004-0-0 loss: 0.949135  [   32/  306]
train() client id: f_00004-0-1 loss: 1.035025  [   64/  306]
train() client id: f_00004-0-2 loss: 0.947947  [   96/  306]
train() client id: f_00004-0-3 loss: 0.965541  [  128/  306]
train() client id: f_00004-0-4 loss: 0.899357  [  160/  306]
train() client id: f_00004-0-5 loss: 0.961519  [  192/  306]
train() client id: f_00004-0-6 loss: 0.955590  [  224/  306]
train() client id: f_00004-0-7 loss: 1.035485  [  256/  306]
train() client id: f_00004-0-8 loss: 1.087862  [  288/  306]
train() client id: f_00004-1-0 loss: 1.054138  [   32/  306]
train() client id: f_00004-1-1 loss: 1.039180  [   64/  306]
train() client id: f_00004-1-2 loss: 0.930649  [   96/  306]
train() client id: f_00004-1-3 loss: 1.006752  [  128/  306]
train() client id: f_00004-1-4 loss: 0.970847  [  160/  306]
train() client id: f_00004-1-5 loss: 1.058506  [  192/  306]
train() client id: f_00004-1-6 loss: 0.849271  [  224/  306]
train() client id: f_00004-1-7 loss: 0.953559  [  256/  306]
train() client id: f_00004-1-8 loss: 0.914311  [  288/  306]
train() client id: f_00004-2-0 loss: 0.962561  [   32/  306]
train() client id: f_00004-2-1 loss: 1.001533  [   64/  306]
train() client id: f_00004-2-2 loss: 1.028071  [   96/  306]
train() client id: f_00004-2-3 loss: 0.871258  [  128/  306]
train() client id: f_00004-2-4 loss: 0.959020  [  160/  306]
train() client id: f_00004-2-5 loss: 0.980017  [  192/  306]
train() client id: f_00004-2-6 loss: 1.065931  [  224/  306]
train() client id: f_00004-2-7 loss: 0.870652  [  256/  306]
train() client id: f_00004-2-8 loss: 0.994280  [  288/  306]
train() client id: f_00004-3-0 loss: 0.994555  [   32/  306]
train() client id: f_00004-3-1 loss: 0.930858  [   64/  306]
train() client id: f_00004-3-2 loss: 0.927697  [   96/  306]
train() client id: f_00004-3-3 loss: 1.007236  [  128/  306]
train() client id: f_00004-3-4 loss: 0.925369  [  160/  306]
train() client id: f_00004-3-5 loss: 0.850084  [  192/  306]
train() client id: f_00004-3-6 loss: 0.965191  [  224/  306]
train() client id: f_00004-3-7 loss: 0.984992  [  256/  306]
train() client id: f_00004-3-8 loss: 1.016437  [  288/  306]
train() client id: f_00004-4-0 loss: 0.954962  [   32/  306]
train() client id: f_00004-4-1 loss: 0.989640  [   64/  306]
train() client id: f_00004-4-2 loss: 0.953312  [   96/  306]
train() client id: f_00004-4-3 loss: 1.044038  [  128/  306]
train() client id: f_00004-4-4 loss: 0.957945  [  160/  306]
train() client id: f_00004-4-5 loss: 0.881078  [  192/  306]
train() client id: f_00004-4-6 loss: 0.898093  [  224/  306]
train() client id: f_00004-4-7 loss: 0.999524  [  256/  306]
train() client id: f_00004-4-8 loss: 0.896781  [  288/  306]
train() client id: f_00004-5-0 loss: 1.081550  [   32/  306]
train() client id: f_00004-5-1 loss: 1.062598  [   64/  306]
train() client id: f_00004-5-2 loss: 1.065065  [   96/  306]
train() client id: f_00004-5-3 loss: 0.854045  [  128/  306]
train() client id: f_00004-5-4 loss: 0.865188  [  160/  306]
train() client id: f_00004-5-5 loss: 0.857616  [  192/  306]
train() client id: f_00004-5-6 loss: 0.966053  [  224/  306]
train() client id: f_00004-5-7 loss: 0.954771  [  256/  306]
train() client id: f_00004-5-8 loss: 0.989037  [  288/  306]
train() client id: f_00004-6-0 loss: 0.976265  [   32/  306]
train() client id: f_00004-6-1 loss: 1.009653  [   64/  306]
train() client id: f_00004-6-2 loss: 0.886656  [   96/  306]
train() client id: f_00004-6-3 loss: 0.922993  [  128/  306]
train() client id: f_00004-6-4 loss: 0.888276  [  160/  306]
train() client id: f_00004-6-5 loss: 0.984996  [  192/  306]
train() client id: f_00004-6-6 loss: 0.952616  [  224/  306]
train() client id: f_00004-6-7 loss: 0.966014  [  256/  306]
train() client id: f_00004-6-8 loss: 1.006728  [  288/  306]
train() client id: f_00004-7-0 loss: 0.967942  [   32/  306]
train() client id: f_00004-7-1 loss: 0.901049  [   64/  306]
train() client id: f_00004-7-2 loss: 0.997797  [   96/  306]
train() client id: f_00004-7-3 loss: 0.888054  [  128/  306]
train() client id: f_00004-7-4 loss: 1.033821  [  160/  306]
train() client id: f_00004-7-5 loss: 1.042060  [  192/  306]
train() client id: f_00004-7-6 loss: 0.917952  [  224/  306]
train() client id: f_00004-7-7 loss: 0.931010  [  256/  306]
train() client id: f_00004-7-8 loss: 0.943536  [  288/  306]
train() client id: f_00004-8-0 loss: 0.926737  [   32/  306]
train() client id: f_00004-8-1 loss: 1.045137  [   64/  306]
train() client id: f_00004-8-2 loss: 0.902800  [   96/  306]
train() client id: f_00004-8-3 loss: 0.948120  [  128/  306]
train() client id: f_00004-8-4 loss: 0.955320  [  160/  306]
train() client id: f_00004-8-5 loss: 0.967202  [  192/  306]
train() client id: f_00004-8-6 loss: 0.953951  [  224/  306]
train() client id: f_00004-8-7 loss: 0.915157  [  256/  306]
train() client id: f_00004-8-8 loss: 1.084444  [  288/  306]
train() client id: f_00004-9-0 loss: 0.976906  [   32/  306]
train() client id: f_00004-9-1 loss: 0.989934  [   64/  306]
train() client id: f_00004-9-2 loss: 0.919458  [   96/  306]
train() client id: f_00004-9-3 loss: 0.912522  [  128/  306]
train() client id: f_00004-9-4 loss: 0.896982  [  160/  306]
train() client id: f_00004-9-5 loss: 1.051124  [  192/  306]
train() client id: f_00004-9-6 loss: 0.947634  [  224/  306]
train() client id: f_00004-9-7 loss: 0.976637  [  256/  306]
train() client id: f_00004-9-8 loss: 0.989976  [  288/  306]
train() client id: f_00004-10-0 loss: 1.025395  [   32/  306]
train() client id: f_00004-10-1 loss: 0.928353  [   64/  306]
train() client id: f_00004-10-2 loss: 0.864674  [   96/  306]
train() client id: f_00004-10-3 loss: 1.042111  [  128/  306]
train() client id: f_00004-10-4 loss: 0.966642  [  160/  306]
train() client id: f_00004-10-5 loss: 0.995903  [  192/  306]
train() client id: f_00004-10-6 loss: 0.855888  [  224/  306]
train() client id: f_00004-10-7 loss: 0.879659  [  256/  306]
train() client id: f_00004-10-8 loss: 1.064249  [  288/  306]
train() client id: f_00004-11-0 loss: 0.961742  [   32/  306]
train() client id: f_00004-11-1 loss: 0.926675  [   64/  306]
train() client id: f_00004-11-2 loss: 0.942473  [   96/  306]
train() client id: f_00004-11-3 loss: 0.962026  [  128/  306]
train() client id: f_00004-11-4 loss: 0.945001  [  160/  306]
train() client id: f_00004-11-5 loss: 0.898413  [  192/  306]
train() client id: f_00004-11-6 loss: 0.943565  [  224/  306]
train() client id: f_00004-11-7 loss: 1.036111  [  256/  306]
train() client id: f_00004-11-8 loss: 0.952438  [  288/  306]
train() client id: f_00004-12-0 loss: 0.919133  [   32/  306]
train() client id: f_00004-12-1 loss: 0.908125  [   64/  306]
train() client id: f_00004-12-2 loss: 0.964804  [   96/  306]
train() client id: f_00004-12-3 loss: 1.020907  [  128/  306]
train() client id: f_00004-12-4 loss: 1.031100  [  160/  306]
train() client id: f_00004-12-5 loss: 0.979886  [  192/  306]
train() client id: f_00004-12-6 loss: 1.027573  [  224/  306]
train() client id: f_00004-12-7 loss: 0.890754  [  256/  306]
train() client id: f_00004-12-8 loss: 0.891290  [  288/  306]
train() client id: f_00005-0-0 loss: 0.935885  [   32/  146]
train() client id: f_00005-0-1 loss: 0.910699  [   64/  146]
train() client id: f_00005-0-2 loss: 0.865983  [   96/  146]
train() client id: f_00005-0-3 loss: 0.907843  [  128/  146]
train() client id: f_00005-1-0 loss: 0.867469  [   32/  146]
train() client id: f_00005-1-1 loss: 0.893305  [   64/  146]
train() client id: f_00005-1-2 loss: 0.905204  [   96/  146]
train() client id: f_00005-1-3 loss: 0.957827  [  128/  146]
train() client id: f_00005-2-0 loss: 0.873409  [   32/  146]
train() client id: f_00005-2-1 loss: 0.894832  [   64/  146]
train() client id: f_00005-2-2 loss: 0.847345  [   96/  146]
train() client id: f_00005-2-3 loss: 0.865483  [  128/  146]
train() client id: f_00005-3-0 loss: 0.881817  [   32/  146]
train() client id: f_00005-3-1 loss: 0.891572  [   64/  146]
train() client id: f_00005-3-2 loss: 0.770470  [   96/  146]
train() client id: f_00005-3-3 loss: 0.969014  [  128/  146]
train() client id: f_00005-4-0 loss: 0.885667  [   32/  146]
train() client id: f_00005-4-1 loss: 0.981509  [   64/  146]
train() client id: f_00005-4-2 loss: 0.803388  [   96/  146]
train() client id: f_00005-4-3 loss: 0.835432  [  128/  146]
train() client id: f_00005-5-0 loss: 0.822073  [   32/  146]
train() client id: f_00005-5-1 loss: 0.841803  [   64/  146]
train() client id: f_00005-5-2 loss: 0.862310  [   96/  146]
train() client id: f_00005-5-3 loss: 1.021655  [  128/  146]
train() client id: f_00005-6-0 loss: 0.949730  [   32/  146]
train() client id: f_00005-6-1 loss: 0.832143  [   64/  146]
train() client id: f_00005-6-2 loss: 0.909656  [   96/  146]
train() client id: f_00005-6-3 loss: 0.863537  [  128/  146]
train() client id: f_00005-7-0 loss: 0.906479  [   32/  146]
train() client id: f_00005-7-1 loss: 0.984666  [   64/  146]
train() client id: f_00005-7-2 loss: 0.872393  [   96/  146]
train() client id: f_00005-7-3 loss: 0.861041  [  128/  146]
train() client id: f_00005-8-0 loss: 0.761362  [   32/  146]
train() client id: f_00005-8-1 loss: 0.987112  [   64/  146]
train() client id: f_00005-8-2 loss: 1.006756  [   96/  146]
train() client id: f_00005-8-3 loss: 0.796515  [  128/  146]
train() client id: f_00005-9-0 loss: 0.912482  [   32/  146]
train() client id: f_00005-9-1 loss: 0.904013  [   64/  146]
train() client id: f_00005-9-2 loss: 0.877193  [   96/  146]
train() client id: f_00005-9-3 loss: 0.911887  [  128/  146]
train() client id: f_00005-10-0 loss: 0.811786  [   32/  146]
train() client id: f_00005-10-1 loss: 0.971304  [   64/  146]
train() client id: f_00005-10-2 loss: 0.895970  [   96/  146]
train() client id: f_00005-10-3 loss: 0.832404  [  128/  146]
train() client id: f_00005-11-0 loss: 0.754638  [   32/  146]
train() client id: f_00005-11-1 loss: 0.870318  [   64/  146]
train() client id: f_00005-11-2 loss: 1.071884  [   96/  146]
train() client id: f_00005-11-3 loss: 0.835351  [  128/  146]
train() client id: f_00005-12-0 loss: 1.008408  [   32/  146]
train() client id: f_00005-12-1 loss: 0.857412  [   64/  146]
train() client id: f_00005-12-2 loss: 0.788731  [   96/  146]
train() client id: f_00005-12-3 loss: 0.884026  [  128/  146]
train() client id: f_00006-0-0 loss: 0.921970  [   32/   54]
train() client id: f_00006-1-0 loss: 0.896853  [   32/   54]
train() client id: f_00006-2-0 loss: 0.901662  [   32/   54]
train() client id: f_00006-3-0 loss: 0.889661  [   32/   54]
train() client id: f_00006-4-0 loss: 0.917949  [   32/   54]
train() client id: f_00006-5-0 loss: 0.921655  [   32/   54]
train() client id: f_00006-6-0 loss: 0.904398  [   32/   54]
train() client id: f_00006-7-0 loss: 0.901711  [   32/   54]
train() client id: f_00006-8-0 loss: 0.914935  [   32/   54]
train() client id: f_00006-9-0 loss: 0.903878  [   32/   54]
train() client id: f_00006-10-0 loss: 0.892683  [   32/   54]
train() client id: f_00006-11-0 loss: 0.898251  [   32/   54]
train() client id: f_00006-12-0 loss: 0.934626  [   32/   54]
train() client id: f_00007-0-0 loss: 0.802400  [   32/  179]
train() client id: f_00007-0-1 loss: 0.885686  [   64/  179]
train() client id: f_00007-0-2 loss: 0.833994  [   96/  179]
train() client id: f_00007-0-3 loss: 0.876955  [  128/  179]
train() client id: f_00007-0-4 loss: 0.803184  [  160/  179]
train() client id: f_00007-1-0 loss: 0.754939  [   32/  179]
train() client id: f_00007-1-1 loss: 0.847173  [   64/  179]
train() client id: f_00007-1-2 loss: 0.774938  [   96/  179]
train() client id: f_00007-1-3 loss: 0.793948  [  128/  179]
train() client id: f_00007-1-4 loss: 0.846127  [  160/  179]
train() client id: f_00007-2-0 loss: 0.714713  [   32/  179]
train() client id: f_00007-2-1 loss: 0.849670  [   64/  179]
train() client id: f_00007-2-2 loss: 0.771516  [   96/  179]
train() client id: f_00007-2-3 loss: 0.809573  [  128/  179]
train() client id: f_00007-2-4 loss: 0.728008  [  160/  179]
train() client id: f_00007-3-0 loss: 0.733207  [   32/  179]
train() client id: f_00007-3-1 loss: 0.728839  [   64/  179]
train() client id: f_00007-3-2 loss: 0.825148  [   96/  179]
train() client id: f_00007-3-3 loss: 0.793380  [  128/  179]
train() client id: f_00007-3-4 loss: 0.738821  [  160/  179]
train() client id: f_00007-4-0 loss: 0.846119  [   32/  179]
train() client id: f_00007-4-1 loss: 0.725931  [   64/  179]
train() client id: f_00007-4-2 loss: 0.666561  [   96/  179]
train() client id: f_00007-4-3 loss: 0.696851  [  128/  179]
train() client id: f_00007-4-4 loss: 0.781806  [  160/  179]
train() client id: f_00007-5-0 loss: 0.659355  [   32/  179]
train() client id: f_00007-5-1 loss: 0.753134  [   64/  179]
train() client id: f_00007-5-2 loss: 0.804520  [   96/  179]
train() client id: f_00007-5-3 loss: 0.658776  [  128/  179]
train() client id: f_00007-5-4 loss: 0.719720  [  160/  179]
train() client id: f_00007-6-0 loss: 0.766606  [   32/  179]
train() client id: f_00007-6-1 loss: 0.741031  [   64/  179]
train() client id: f_00007-6-2 loss: 0.644886  [   96/  179]
train() client id: f_00007-6-3 loss: 0.701731  [  128/  179]
train() client id: f_00007-6-4 loss: 0.768780  [  160/  179]
train() client id: f_00007-7-0 loss: 0.824261  [   32/  179]
train() client id: f_00007-7-1 loss: 0.638888  [   64/  179]
train() client id: f_00007-7-2 loss: 0.686652  [   96/  179]
train() client id: f_00007-7-3 loss: 0.756524  [  128/  179]
train() client id: f_00007-7-4 loss: 0.702954  [  160/  179]
train() client id: f_00007-8-0 loss: 0.692652  [   32/  179]
train() client id: f_00007-8-1 loss: 0.719948  [   64/  179]
train() client id: f_00007-8-2 loss: 0.741830  [   96/  179]
train() client id: f_00007-8-3 loss: 0.774164  [  128/  179]
train() client id: f_00007-8-4 loss: 0.679545  [  160/  179]
train() client id: f_00007-9-0 loss: 0.777345  [   32/  179]
train() client id: f_00007-9-1 loss: 0.795924  [   64/  179]
train() client id: f_00007-9-2 loss: 0.629531  [   96/  179]
train() client id: f_00007-9-3 loss: 0.625916  [  128/  179]
train() client id: f_00007-9-4 loss: 0.683053  [  160/  179]
train() client id: f_00007-10-0 loss: 0.627680  [   32/  179]
train() client id: f_00007-10-1 loss: 0.697675  [   64/  179]
train() client id: f_00007-10-2 loss: 0.792395  [   96/  179]
train() client id: f_00007-10-3 loss: 0.659991  [  128/  179]
train() client id: f_00007-10-4 loss: 0.809731  [  160/  179]
train() client id: f_00007-11-0 loss: 0.755141  [   32/  179]
train() client id: f_00007-11-1 loss: 0.740853  [   64/  179]
train() client id: f_00007-11-2 loss: 0.729922  [   96/  179]
train() client id: f_00007-11-3 loss: 0.674454  [  128/  179]
train() client id: f_00007-11-4 loss: 0.606409  [  160/  179]
train() client id: f_00007-12-0 loss: 0.708635  [   32/  179]
train() client id: f_00007-12-1 loss: 0.728265  [   64/  179]
train() client id: f_00007-12-2 loss: 0.615479  [   96/  179]
train() client id: f_00007-12-3 loss: 0.707612  [  128/  179]
train() client id: f_00007-12-4 loss: 0.690846  [  160/  179]
train() client id: f_00008-0-0 loss: 0.873344  [   32/  130]
train() client id: f_00008-0-1 loss: 0.785636  [   64/  130]
train() client id: f_00008-0-2 loss: 0.861392  [   96/  130]
train() client id: f_00008-0-3 loss: 0.956014  [  128/  130]
train() client id: f_00008-1-0 loss: 0.890671  [   32/  130]
train() client id: f_00008-1-1 loss: 0.882761  [   64/  130]
train() client id: f_00008-1-2 loss: 0.819601  [   96/  130]
train() client id: f_00008-1-3 loss: 0.851269  [  128/  130]
train() client id: f_00008-2-0 loss: 0.861771  [   32/  130]
train() client id: f_00008-2-1 loss: 0.842510  [   64/  130]
train() client id: f_00008-2-2 loss: 0.833787  [   96/  130]
train() client id: f_00008-2-3 loss: 0.915163  [  128/  130]
train() client id: f_00008-3-0 loss: 0.816965  [   32/  130]
train() client id: f_00008-3-1 loss: 0.850515  [   64/  130]
train() client id: f_00008-3-2 loss: 0.817696  [   96/  130]
train() client id: f_00008-3-3 loss: 0.939904  [  128/  130]
train() client id: f_00008-4-0 loss: 0.897901  [   32/  130]
train() client id: f_00008-4-1 loss: 0.834764  [   64/  130]
train() client id: f_00008-4-2 loss: 0.784615  [   96/  130]
train() client id: f_00008-4-3 loss: 0.932639  [  128/  130]
train() client id: f_00008-5-0 loss: 0.756626  [   32/  130]
train() client id: f_00008-5-1 loss: 0.800741  [   64/  130]
train() client id: f_00008-5-2 loss: 0.897002  [   96/  130]
train() client id: f_00008-5-3 loss: 0.961633  [  128/  130]
train() client id: f_00008-6-0 loss: 0.939344  [   32/  130]
train() client id: f_00008-6-1 loss: 0.779068  [   64/  130]
train() client id: f_00008-6-2 loss: 0.873188  [   96/  130]
train() client id: f_00008-6-3 loss: 0.853184  [  128/  130]
train() client id: f_00008-7-0 loss: 0.844087  [   32/  130]
train() client id: f_00008-7-1 loss: 0.849910  [   64/  130]
train() client id: f_00008-7-2 loss: 0.901950  [   96/  130]
train() client id: f_00008-7-3 loss: 0.835735  [  128/  130]
train() client id: f_00008-8-0 loss: 0.842322  [   32/  130]
train() client id: f_00008-8-1 loss: 0.838030  [   64/  130]
train() client id: f_00008-8-2 loss: 0.769453  [   96/  130]
train() client id: f_00008-8-3 loss: 0.989547  [  128/  130]
train() client id: f_00008-9-0 loss: 0.895759  [   32/  130]
train() client id: f_00008-9-1 loss: 0.813228  [   64/  130]
train() client id: f_00008-9-2 loss: 0.896453  [   96/  130]
train() client id: f_00008-9-3 loss: 0.838149  [  128/  130]
train() client id: f_00008-10-0 loss: 0.850965  [   32/  130]
train() client id: f_00008-10-1 loss: 0.884538  [   64/  130]
train() client id: f_00008-10-2 loss: 0.793213  [   96/  130]
train() client id: f_00008-10-3 loss: 0.912188  [  128/  130]
train() client id: f_00008-11-0 loss: 0.805625  [   32/  130]
train() client id: f_00008-11-1 loss: 0.961851  [   64/  130]
train() client id: f_00008-11-2 loss: 0.927590  [   96/  130]
train() client id: f_00008-11-3 loss: 0.720152  [  128/  130]
train() client id: f_00008-12-0 loss: 0.887272  [   32/  130]
train() client id: f_00008-12-1 loss: 0.947099  [   64/  130]
train() client id: f_00008-12-2 loss: 0.805031  [   96/  130]
train() client id: f_00008-12-3 loss: 0.765556  [  128/  130]
train() client id: f_00009-0-0 loss: 1.205204  [   32/  118]
train() client id: f_00009-0-1 loss: 1.141609  [   64/  118]
train() client id: f_00009-0-2 loss: 1.188073  [   96/  118]
train() client id: f_00009-1-0 loss: 1.165498  [   32/  118]
train() client id: f_00009-1-1 loss: 1.179740  [   64/  118]
train() client id: f_00009-1-2 loss: 1.130909  [   96/  118]
train() client id: f_00009-2-0 loss: 1.059471  [   32/  118]
train() client id: f_00009-2-1 loss: 1.139085  [   64/  118]
train() client id: f_00009-2-2 loss: 1.137243  [   96/  118]
train() client id: f_00009-3-0 loss: 1.088203  [   32/  118]
train() client id: f_00009-3-1 loss: 1.113561  [   64/  118]
train() client id: f_00009-3-2 loss: 1.076480  [   96/  118]
train() client id: f_00009-4-0 loss: 1.044630  [   32/  118]
train() client id: f_00009-4-1 loss: 1.083282  [   64/  118]
train() client id: f_00009-4-2 loss: 1.077963  [   96/  118]
train() client id: f_00009-5-0 loss: 1.054806  [   32/  118]
train() client id: f_00009-5-1 loss: 1.037221  [   64/  118]
train() client id: f_00009-5-2 loss: 1.026186  [   96/  118]
train() client id: f_00009-6-0 loss: 1.049676  [   32/  118]
train() client id: f_00009-6-1 loss: 0.992194  [   64/  118]
train() client id: f_00009-6-2 loss: 1.011940  [   96/  118]
train() client id: f_00009-7-0 loss: 0.956079  [   32/  118]
train() client id: f_00009-7-1 loss: 1.022860  [   64/  118]
train() client id: f_00009-7-2 loss: 1.033386  [   96/  118]
train() client id: f_00009-8-0 loss: 1.005747  [   32/  118]
train() client id: f_00009-8-1 loss: 1.005048  [   64/  118]
train() client id: f_00009-8-2 loss: 0.969657  [   96/  118]
train() client id: f_00009-9-0 loss: 1.064881  [   32/  118]
train() client id: f_00009-9-1 loss: 0.921603  [   64/  118]
train() client id: f_00009-9-2 loss: 1.023440  [   96/  118]
train() client id: f_00009-10-0 loss: 0.968433  [   32/  118]
train() client id: f_00009-10-1 loss: 1.013080  [   64/  118]
train() client id: f_00009-10-2 loss: 1.008828  [   96/  118]
train() client id: f_00009-11-0 loss: 0.961568  [   32/  118]
train() client id: f_00009-11-1 loss: 1.021847  [   64/  118]
train() client id: f_00009-11-2 loss: 0.988606  [   96/  118]
train() client id: f_00009-12-0 loss: 0.964491  [   32/  118]
train() client id: f_00009-12-1 loss: 0.958652  [   64/  118]
train() client id: f_00009-12-2 loss: 1.026698  [   96/  118]
At round 4 accuracy: 0.6312997347480106
At round 4 training accuracy: 0.5734406438631791
At round 4 training loss: 0.8873139009782461
update_location
xs = [ -3.9056584    4.20031788  40.00902392  18.81129433 -14.02070377
  -1.04359014  -2.44319194  -6.32485185  24.66397685   2.93912145]
ys = [ 32.5879595   15.55583871   1.32061395  -2.45517586   9.35018685
 -17.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [105.24841696 101.28981582 107.71474373 101.78355802 101.41008889
 101.47138757 100.06427799 100.28687116 104.48436116 100.1231756 ]
dists_bs = [222.60831747 239.86495054 276.39357718 262.77967434 230.98526882
 259.22236244 247.64185241 246.08115854 254.2640783  246.78497559]
uav_gains = [8.79949509e-11 9.68464333e-11 8.30438549e-11 9.56761907e-11
 9.65595300e-11 9.64137637e-11 9.98391157e-11 9.92860273e-11
 8.96125391e-11 9.96923511e-11]
bs_gains = [2.95242043e-11 2.39544975e-11 1.61070436e-11 1.85539819e-11
 2.66230674e-11 1.92757456e-11 2.19071944e-11 2.22984497e-11
 2.03467974e-11 2.21208434e-11]
Round 5
-------------------------------
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.33284149 21.59560189 10.17342765  3.63728595 24.89615745 12.01211791
  4.52299688 14.602885   10.71870557  9.74625265]
obj_prev = 122.23827245410807
eta_min = 9.414027209246992e-10	eta_max = 0.9186608067299383
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 28.431400687067253	eta = 0.909090909090909
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 48.95841824133416	eta = 0.5279322499743717
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 39.18478015448109	eta = 0.6596114051281231
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.434409794184106	eta = 0.6904537306569067
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.34853700150222	eta = 0.6920412410342677
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.348315836191034	eta = 0.6920453390909805
eta = 0.6920453390909805
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.03037504 0.06388402 0.0298929  0.01036609 0.07376796 0.03519648
 0.01301788 0.04315185 0.03133933 0.02844647]
ene_total = [3.16534696 6.21953031 3.1298897  1.43707177 7.05111483 3.79361781
 1.66080998 4.25148272 3.45176733 3.18768443]
ti_comp = [0.28160493 0.26136622 0.28090865 0.2825856  0.2634578  0.25672527
 0.28307344 0.28301023 0.25792521 0.25972041]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20876875e-05 2.38538142e-04 2.11569937e-05 8.71815183e-07
 3.61460861e-04 4.13466617e-05 1.72068806e-06 6.27009232e-05
 2.89175573e-05 2.13281397e-05]
ene_total = [0.55639501 0.74895917 0.56229982 0.54614218 0.74154691 0.77190053
 0.54202196 0.54780676 0.76051829 0.74443548]
optimize_network iter = 0 obj = 6.522026104246858
eta = 0.6920453390909805
freqs = [5.39320171e+07 1.22211702e+08 5.32075115e+07 1.83414978e+07
 1.39999582e+08 6.85489188e+07 2.29938135e+07 7.62372573e+07
 6.07527495e+07 5.47636387e+07]
eta_min = 0.6764484296472902	eta_max = 0.6920453390909633
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 0.06328991478194519	eta = 0.9090909090909091
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 21.92072085717903	eta = 0.0026247442563715508
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.335363763739976	eta = 0.024636969648472683
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.25597545648311	eta = 0.025503950408706712
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.255940548856704	eta = 0.025504345047817744
eta = 0.025504345047817744
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20148930e-04 2.37751991e-03 2.10872665e-04 8.68941936e-06
 3.60269593e-03 4.12103953e-04 1.71501718e-05 6.24942794e-04
 2.88222535e-04 2.12578485e-04]
ene_total = [0.1812087  0.29478227 0.18284802 0.17280114 0.32238    0.25400243
 0.17170585 0.1883868  0.24737814 0.2404472 ]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 1 obj = 6.211119913967213
eta = 0.6764484296472902
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
eta_min = 0.6764484296473243	eta_max = 0.6764484296472864
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 0.06277694578067981	eta = 0.909090909090909
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 21.92023194550156	eta = 0.002603528596394288
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.332999905489733	eta = 0.024462045872963308
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2542173952325193	eta = 0.02531696846560011
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2541832396466193	eta = 0.02531735207056884
eta = 0.02531735207056884
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20456386e-04 2.35937920e-03 2.11105834e-04 8.70509514e-06
 3.57877769e-03 4.08031791e-04 1.71845703e-05 6.26179946e-04
 2.85544288e-04 2.10788545e-04]
ene_total = [0.18118229 0.29423306 0.18281927 0.17276841 0.32166859 0.25384311
 0.17167384 0.18838425 0.24725795 0.24035246]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 2 obj = 6.2111199139678615
eta = 0.6764484296473243
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
Done!
At round 5 energy consumption: 6.211119913967213
At round 5 eta: 0.6764484296473243
At round 5 local rounds: 12.80002140651856
At round 5 global rounds: 81.81036980458869
At round 5 a_n: 26.12732777443847
gradient difference: 0.3515395522117615
train() client id: f_00000-0-0 loss: 1.530644  [   32/  126]
train() client id: f_00000-0-1 loss: 1.325588  [   64/  126]
train() client id: f_00000-0-2 loss: 1.267658  [   96/  126]
train() client id: f_00000-1-0 loss: 1.194546  [   32/  126]
train() client id: f_00000-1-1 loss: 1.366826  [   64/  126]
train() client id: f_00000-1-2 loss: 1.249398  [   96/  126]
train() client id: f_00000-2-0 loss: 1.158383  [   32/  126]
train() client id: f_00000-2-1 loss: 1.188272  [   64/  126]
train() client id: f_00000-2-2 loss: 1.069300  [   96/  126]
train() client id: f_00000-3-0 loss: 1.092555  [   32/  126]
train() client id: f_00000-3-1 loss: 1.127790  [   64/  126]
train() client id: f_00000-3-2 loss: 1.050626  [   96/  126]
train() client id: f_00000-4-0 loss: 1.074715  [   32/  126]
train() client id: f_00000-4-1 loss: 1.001514  [   64/  126]
train() client id: f_00000-4-2 loss: 0.969661  [   96/  126]
train() client id: f_00000-5-0 loss: 1.041769  [   32/  126]
train() client id: f_00000-5-1 loss: 0.981912  [   64/  126]
train() client id: f_00000-5-2 loss: 0.944739  [   96/  126]
train() client id: f_00000-6-0 loss: 0.938750  [   32/  126]
train() client id: f_00000-6-1 loss: 0.954477  [   64/  126]
train() client id: f_00000-6-2 loss: 0.990964  [   96/  126]
train() client id: f_00000-7-0 loss: 0.918550  [   32/  126]
train() client id: f_00000-7-1 loss: 0.921608  [   64/  126]
train() client id: f_00000-7-2 loss: 0.938024  [   96/  126]
train() client id: f_00000-8-0 loss: 0.897866  [   32/  126]
train() client id: f_00000-8-1 loss: 0.886021  [   64/  126]
train() client id: f_00000-8-2 loss: 0.963134  [   96/  126]
train() client id: f_00000-9-0 loss: 0.924938  [   32/  126]
train() client id: f_00000-9-1 loss: 0.878494  [   64/  126]
train() client id: f_00000-9-2 loss: 0.924569  [   96/  126]
train() client id: f_00000-10-0 loss: 0.901649  [   32/  126]
train() client id: f_00000-10-1 loss: 0.939793  [   64/  126]
train() client id: f_00000-10-2 loss: 0.912703  [   96/  126]
train() client id: f_00000-11-0 loss: 0.877112  [   32/  126]
train() client id: f_00000-11-1 loss: 0.888267  [   64/  126]
train() client id: f_00000-11-2 loss: 0.897442  [   96/  126]
train() client id: f_00001-0-0 loss: 0.533646  [   32/  265]
train() client id: f_00001-0-1 loss: 0.620750  [   64/  265]
train() client id: f_00001-0-2 loss: 0.581269  [   96/  265]
train() client id: f_00001-0-3 loss: 0.530641  [  128/  265]
train() client id: f_00001-0-4 loss: 0.626968  [  160/  265]
train() client id: f_00001-0-5 loss: 0.566688  [  192/  265]
train() client id: f_00001-0-6 loss: 0.661594  [  224/  265]
train() client id: f_00001-0-7 loss: 0.555169  [  256/  265]
train() client id: f_00001-1-0 loss: 0.605899  [   32/  265]
train() client id: f_00001-1-1 loss: 0.549842  [   64/  265]
train() client id: f_00001-1-2 loss: 0.622127  [   96/  265]
train() client id: f_00001-1-3 loss: 0.583101  [  128/  265]
train() client id: f_00001-1-4 loss: 0.503614  [  160/  265]
train() client id: f_00001-1-5 loss: 0.544198  [  192/  265]
train() client id: f_00001-1-6 loss: 0.528699  [  224/  265]
train() client id: f_00001-1-7 loss: 0.519623  [  256/  265]
train() client id: f_00001-2-0 loss: 0.456714  [   32/  265]
train() client id: f_00001-2-1 loss: 0.673685  [   64/  265]
train() client id: f_00001-2-2 loss: 0.500976  [   96/  265]
train() client id: f_00001-2-3 loss: 0.539710  [  128/  265]
train() client id: f_00001-2-4 loss: 0.484708  [  160/  265]
train() client id: f_00001-2-5 loss: 0.515051  [  192/  265]
train() client id: f_00001-2-6 loss: 0.506909  [  224/  265]
train() client id: f_00001-2-7 loss: 0.577607  [  256/  265]
train() client id: f_00001-3-0 loss: 0.593411  [   32/  265]
train() client id: f_00001-3-1 loss: 0.587056  [   64/  265]
train() client id: f_00001-3-2 loss: 0.539676  [   96/  265]
train() client id: f_00001-3-3 loss: 0.525168  [  128/  265]
train() client id: f_00001-3-4 loss: 0.433774  [  160/  265]
train() client id: f_00001-3-5 loss: 0.567587  [  192/  265]
train() client id: f_00001-3-6 loss: 0.415997  [  224/  265]
train() client id: f_00001-3-7 loss: 0.446346  [  256/  265]
train() client id: f_00001-4-0 loss: 0.491328  [   32/  265]
train() client id: f_00001-4-1 loss: 0.685373  [   64/  265]
train() client id: f_00001-4-2 loss: 0.486933  [   96/  265]
train() client id: f_00001-4-3 loss: 0.476499  [  128/  265]
train() client id: f_00001-4-4 loss: 0.425627  [  160/  265]
train() client id: f_00001-4-5 loss: 0.469658  [  192/  265]
train() client id: f_00001-4-6 loss: 0.507967  [  224/  265]
train() client id: f_00001-4-7 loss: 0.479785  [  256/  265]
train() client id: f_00001-5-0 loss: 0.437414  [   32/  265]
train() client id: f_00001-5-1 loss: 0.466097  [   64/  265]
train() client id: f_00001-5-2 loss: 0.486995  [   96/  265]
train() client id: f_00001-5-3 loss: 0.460199  [  128/  265]
train() client id: f_00001-5-4 loss: 0.564594  [  160/  265]
train() client id: f_00001-5-5 loss: 0.447073  [  192/  265]
train() client id: f_00001-5-6 loss: 0.504787  [  224/  265]
train() client id: f_00001-5-7 loss: 0.561491  [  256/  265]
train() client id: f_00001-6-0 loss: 0.422061  [   32/  265]
train() client id: f_00001-6-1 loss: 0.508142  [   64/  265]
train() client id: f_00001-6-2 loss: 0.459463  [   96/  265]
train() client id: f_00001-6-3 loss: 0.554706  [  128/  265]
train() client id: f_00001-6-4 loss: 0.472614  [  160/  265]
train() client id: f_00001-6-5 loss: 0.531029  [  192/  265]
train() client id: f_00001-6-6 loss: 0.461221  [  224/  265]
train() client id: f_00001-6-7 loss: 0.408814  [  256/  265]
train() client id: f_00001-7-0 loss: 0.507262  [   32/  265]
train() client id: f_00001-7-1 loss: 0.443403  [   64/  265]
train() client id: f_00001-7-2 loss: 0.526571  [   96/  265]
train() client id: f_00001-7-3 loss: 0.541953  [  128/  265]
train() client id: f_00001-7-4 loss: 0.464275  [  160/  265]
train() client id: f_00001-7-5 loss: 0.459979  [  192/  265]
train() client id: f_00001-7-6 loss: 0.458538  [  224/  265]
train() client id: f_00001-7-7 loss: 0.431342  [  256/  265]
train() client id: f_00001-8-0 loss: 0.420929  [   32/  265]
train() client id: f_00001-8-1 loss: 0.439752  [   64/  265]
train() client id: f_00001-8-2 loss: 0.524984  [   96/  265]
train() client id: f_00001-8-3 loss: 0.411339  [  128/  265]
train() client id: f_00001-8-4 loss: 0.476700  [  160/  265]
train() client id: f_00001-8-5 loss: 0.507393  [  192/  265]
train() client id: f_00001-8-6 loss: 0.624106  [  224/  265]
train() client id: f_00001-8-7 loss: 0.393384  [  256/  265]
train() client id: f_00001-9-0 loss: 0.496140  [   32/  265]
train() client id: f_00001-9-1 loss: 0.396551  [   64/  265]
train() client id: f_00001-9-2 loss: 0.498444  [   96/  265]
train() client id: f_00001-9-3 loss: 0.543631  [  128/  265]
train() client id: f_00001-9-4 loss: 0.450516  [  160/  265]
train() client id: f_00001-9-5 loss: 0.435227  [  192/  265]
train() client id: f_00001-9-6 loss: 0.461628  [  224/  265]
train() client id: f_00001-9-7 loss: 0.446601  [  256/  265]
train() client id: f_00001-10-0 loss: 0.385928  [   32/  265]
train() client id: f_00001-10-1 loss: 0.485635  [   64/  265]
train() client id: f_00001-10-2 loss: 0.436078  [   96/  265]
train() client id: f_00001-10-3 loss: 0.554687  [  128/  265]
train() client id: f_00001-10-4 loss: 0.497661  [  160/  265]
train() client id: f_00001-10-5 loss: 0.503401  [  192/  265]
train() client id: f_00001-10-6 loss: 0.427602  [  224/  265]
train() client id: f_00001-10-7 loss: 0.448138  [  256/  265]
train() client id: f_00001-11-0 loss: 0.507378  [   32/  265]
train() client id: f_00001-11-1 loss: 0.393634  [   64/  265]
train() client id: f_00001-11-2 loss: 0.501185  [   96/  265]
train() client id: f_00001-11-3 loss: 0.462790  [  128/  265]
train() client id: f_00001-11-4 loss: 0.488349  [  160/  265]
train() client id: f_00001-11-5 loss: 0.455862  [  192/  265]
train() client id: f_00001-11-6 loss: 0.452843  [  224/  265]
train() client id: f_00001-11-7 loss: 0.487647  [  256/  265]
train() client id: f_00002-0-0 loss: 1.209468  [   32/  124]
train() client id: f_00002-0-1 loss: 1.178915  [   64/  124]
train() client id: f_00002-0-2 loss: 1.126454  [   96/  124]
train() client id: f_00002-1-0 loss: 1.164198  [   32/  124]
train() client id: f_00002-1-1 loss: 1.146908  [   64/  124]
train() client id: f_00002-1-2 loss: 1.092103  [   96/  124]
train() client id: f_00002-2-0 loss: 1.075950  [   32/  124]
train() client id: f_00002-2-1 loss: 1.102736  [   64/  124]
train() client id: f_00002-2-2 loss: 1.155112  [   96/  124]
train() client id: f_00002-3-0 loss: 1.111296  [   32/  124]
train() client id: f_00002-3-1 loss: 1.016414  [   64/  124]
train() client id: f_00002-3-2 loss: 1.130455  [   96/  124]
train() client id: f_00002-4-0 loss: 1.051552  [   32/  124]
train() client id: f_00002-4-1 loss: 1.079219  [   64/  124]
train() client id: f_00002-4-2 loss: 1.076319  [   96/  124]
train() client id: f_00002-5-0 loss: 1.048354  [   32/  124]
train() client id: f_00002-5-1 loss: 1.033504  [   64/  124]
train() client id: f_00002-5-2 loss: 1.003409  [   96/  124]
train() client id: f_00002-6-0 loss: 1.036649  [   32/  124]
train() client id: f_00002-6-1 loss: 0.984626  [   64/  124]
train() client id: f_00002-6-2 loss: 1.047023  [   96/  124]
train() client id: f_00002-7-0 loss: 1.035630  [   32/  124]
train() client id: f_00002-7-1 loss: 1.098172  [   64/  124]
train() client id: f_00002-7-2 loss: 0.915721  [   96/  124]
train() client id: f_00002-8-0 loss: 0.969115  [   32/  124]
train() client id: f_00002-8-1 loss: 1.012460  [   64/  124]
train() client id: f_00002-8-2 loss: 1.053884  [   96/  124]
train() client id: f_00002-9-0 loss: 1.043361  [   32/  124]
train() client id: f_00002-9-1 loss: 0.969856  [   64/  124]
train() client id: f_00002-9-2 loss: 0.974310  [   96/  124]
train() client id: f_00002-10-0 loss: 1.005511  [   32/  124]
train() client id: f_00002-10-1 loss: 0.952413  [   64/  124]
train() client id: f_00002-10-2 loss: 0.958663  [   96/  124]
train() client id: f_00002-11-0 loss: 0.970153  [   32/  124]
train() client id: f_00002-11-1 loss: 0.958723  [   64/  124]
train() client id: f_00002-11-2 loss: 0.935721  [   96/  124]
train() client id: f_00003-0-0 loss: 0.843008  [   32/   43]
train() client id: f_00003-1-0 loss: 1.022357  [   32/   43]
train() client id: f_00003-2-0 loss: 0.939807  [   32/   43]
train() client id: f_00003-3-0 loss: 0.995992  [   32/   43]
train() client id: f_00003-4-0 loss: 1.023299  [   32/   43]
train() client id: f_00003-5-0 loss: 0.959466  [   32/   43]
train() client id: f_00003-6-0 loss: 0.947954  [   32/   43]
train() client id: f_00003-7-0 loss: 0.980738  [   32/   43]
train() client id: f_00003-8-0 loss: 0.985241  [   32/   43]
train() client id: f_00003-9-0 loss: 0.940846  [   32/   43]
train() client id: f_00003-10-0 loss: 1.004753  [   32/   43]
train() client id: f_00003-11-0 loss: 0.908622  [   32/   43]
train() client id: f_00004-0-0 loss: 1.026878  [   32/  306]
train() client id: f_00004-0-1 loss: 0.963153  [   64/  306]
train() client id: f_00004-0-2 loss: 1.016232  [   96/  306]
train() client id: f_00004-0-3 loss: 1.021666  [  128/  306]
train() client id: f_00004-0-4 loss: 1.070616  [  160/  306]
train() client id: f_00004-0-5 loss: 0.951120  [  192/  306]
train() client id: f_00004-0-6 loss: 1.115852  [  224/  306]
train() client id: f_00004-0-7 loss: 0.999480  [  256/  306]
train() client id: f_00004-0-8 loss: 0.918501  [  288/  306]
train() client id: f_00004-1-0 loss: 1.049391  [   32/  306]
train() client id: f_00004-1-1 loss: 1.073679  [   64/  306]
train() client id: f_00004-1-2 loss: 0.962170  [   96/  306]
train() client id: f_00004-1-3 loss: 0.952681  [  128/  306]
train() client id: f_00004-1-4 loss: 1.029871  [  160/  306]
train() client id: f_00004-1-5 loss: 0.988461  [  192/  306]
train() client id: f_00004-1-6 loss: 1.094992  [  224/  306]
train() client id: f_00004-1-7 loss: 0.979028  [  256/  306]
train() client id: f_00004-1-8 loss: 0.975126  [  288/  306]
train() client id: f_00004-2-0 loss: 0.989427  [   32/  306]
train() client id: f_00004-2-1 loss: 1.082198  [   64/  306]
train() client id: f_00004-2-2 loss: 1.041710  [   96/  306]
train() client id: f_00004-2-3 loss: 0.982475  [  128/  306]
train() client id: f_00004-2-4 loss: 1.066570  [  160/  306]
train() client id: f_00004-2-5 loss: 1.006123  [  192/  306]
train() client id: f_00004-2-6 loss: 0.905049  [  224/  306]
train() client id: f_00004-2-7 loss: 1.002321  [  256/  306]
train() client id: f_00004-2-8 loss: 0.968465  [  288/  306]
train() client id: f_00004-3-0 loss: 1.009877  [   32/  306]
train() client id: f_00004-3-1 loss: 0.909986  [   64/  306]
train() client id: f_00004-3-2 loss: 1.017123  [   96/  306]
train() client id: f_00004-3-3 loss: 0.980235  [  128/  306]
train() client id: f_00004-3-4 loss: 0.997361  [  160/  306]
train() client id: f_00004-3-5 loss: 1.010027  [  192/  306]
train() client id: f_00004-3-6 loss: 1.081896  [  224/  306]
train() client id: f_00004-3-7 loss: 1.043854  [  256/  306]
train() client id: f_00004-3-8 loss: 1.016941  [  288/  306]
train() client id: f_00004-4-0 loss: 0.937405  [   32/  306]
train() client id: f_00004-4-1 loss: 1.090644  [   64/  306]
train() client id: f_00004-4-2 loss: 1.026327  [   96/  306]
train() client id: f_00004-4-3 loss: 1.025067  [  128/  306]
train() client id: f_00004-4-4 loss: 1.051870  [  160/  306]
train() client id: f_00004-4-5 loss: 1.007506  [  192/  306]
train() client id: f_00004-4-6 loss: 0.939264  [  224/  306]
train() client id: f_00004-4-7 loss: 1.031760  [  256/  306]
train() client id: f_00004-4-8 loss: 0.922036  [  288/  306]
train() client id: f_00004-5-0 loss: 1.009241  [   32/  306]
train() client id: f_00004-5-1 loss: 0.962223  [   64/  306]
train() client id: f_00004-5-2 loss: 1.021992  [   96/  306]
train() client id: f_00004-5-3 loss: 0.907356  [  128/  306]
train() client id: f_00004-5-4 loss: 0.993316  [  160/  306]
train() client id: f_00004-5-5 loss: 1.136134  [  192/  306]
train() client id: f_00004-5-6 loss: 0.956766  [  224/  306]
train() client id: f_00004-5-7 loss: 1.062395  [  256/  306]
train() client id: f_00004-5-8 loss: 0.993499  [  288/  306]
train() client id: f_00004-6-0 loss: 0.939865  [   32/  306]
train() client id: f_00004-6-1 loss: 1.076644  [   64/  306]
train() client id: f_00004-6-2 loss: 1.011527  [   96/  306]
train() client id: f_00004-6-3 loss: 0.944929  [  128/  306]
train() client id: f_00004-6-4 loss: 1.014104  [  160/  306]
train() client id: f_00004-6-5 loss: 1.094012  [  192/  306]
train() client id: f_00004-6-6 loss: 0.964014  [  224/  306]
train() client id: f_00004-6-7 loss: 0.994150  [  256/  306]
train() client id: f_00004-6-8 loss: 1.071319  [  288/  306]
train() client id: f_00004-7-0 loss: 0.919106  [   32/  306]
train() client id: f_00004-7-1 loss: 1.116350  [   64/  306]
train() client id: f_00004-7-2 loss: 0.935432  [   96/  306]
train() client id: f_00004-7-3 loss: 1.026390  [  128/  306]
train() client id: f_00004-7-4 loss: 1.048424  [  160/  306]
train() client id: f_00004-7-5 loss: 1.007635  [  192/  306]
train() client id: f_00004-7-6 loss: 0.968714  [  224/  306]
train() client id: f_00004-7-7 loss: 1.025726  [  256/  306]
train() client id: f_00004-7-8 loss: 1.000110  [  288/  306]
train() client id: f_00004-8-0 loss: 1.098050  [   32/  306]
train() client id: f_00004-8-1 loss: 1.065449  [   64/  306]
train() client id: f_00004-8-2 loss: 0.995730  [   96/  306]
train() client id: f_00004-8-3 loss: 1.090843  [  128/  306]
train() client id: f_00004-8-4 loss: 0.874393  [  160/  306]
train() client id: f_00004-8-5 loss: 1.138275  [  192/  306]
train() client id: f_00004-8-6 loss: 0.974429  [  224/  306]
train() client id: f_00004-8-7 loss: 0.974535  [  256/  306]
train() client id: f_00004-8-8 loss: 0.950072  [  288/  306]
train() client id: f_00004-9-0 loss: 1.020444  [   32/  306]
train() client id: f_00004-9-1 loss: 0.999543  [   64/  306]
train() client id: f_00004-9-2 loss: 0.906177  [   96/  306]
train() client id: f_00004-9-3 loss: 1.030024  [  128/  306]
train() client id: f_00004-9-4 loss: 0.978876  [  160/  306]
train() client id: f_00004-9-5 loss: 1.083771  [  192/  306]
train() client id: f_00004-9-6 loss: 0.914921  [  224/  306]
train() client id: f_00004-9-7 loss: 1.117881  [  256/  306]
train() client id: f_00004-9-8 loss: 1.005306  [  288/  306]
train() client id: f_00004-10-0 loss: 1.005694  [   32/  306]
train() client id: f_00004-10-1 loss: 0.959676  [   64/  306]
train() client id: f_00004-10-2 loss: 1.002014  [   96/  306]
train() client id: f_00004-10-3 loss: 1.152093  [  128/  306]
train() client id: f_00004-10-4 loss: 0.908953  [  160/  306]
train() client id: f_00004-10-5 loss: 1.033064  [  192/  306]
train() client id: f_00004-10-6 loss: 0.877537  [  224/  306]
train() client id: f_00004-10-7 loss: 1.023938  [  256/  306]
train() client id: f_00004-10-8 loss: 1.040646  [  288/  306]
train() client id: f_00004-11-0 loss: 1.080226  [   32/  306]
train() client id: f_00004-11-1 loss: 1.027038  [   64/  306]
train() client id: f_00004-11-2 loss: 0.972428  [   96/  306]
train() client id: f_00004-11-3 loss: 0.938145  [  128/  306]
train() client id: f_00004-11-4 loss: 1.216428  [  160/  306]
train() client id: f_00004-11-5 loss: 0.910286  [  192/  306]
train() client id: f_00004-11-6 loss: 1.017499  [  224/  306]
train() client id: f_00004-11-7 loss: 1.015821  [  256/  306]
train() client id: f_00004-11-8 loss: 0.942360  [  288/  306]
train() client id: f_00005-0-0 loss: 0.903936  [   32/  146]
train() client id: f_00005-0-1 loss: 0.725918  [   64/  146]
train() client id: f_00005-0-2 loss: 0.915030  [   96/  146]
train() client id: f_00005-0-3 loss: 0.754072  [  128/  146]
train() client id: f_00005-1-0 loss: 0.810844  [   32/  146]
train() client id: f_00005-1-1 loss: 0.910531  [   64/  146]
train() client id: f_00005-1-2 loss: 0.867327  [   96/  146]
train() client id: f_00005-1-3 loss: 0.771230  [  128/  146]
train() client id: f_00005-2-0 loss: 0.882597  [   32/  146]
train() client id: f_00005-2-1 loss: 0.854742  [   64/  146]
train() client id: f_00005-2-2 loss: 0.737179  [   96/  146]
train() client id: f_00005-2-3 loss: 0.769242  [  128/  146]
train() client id: f_00005-3-0 loss: 0.758990  [   32/  146]
train() client id: f_00005-3-1 loss: 0.836300  [   64/  146]
train() client id: f_00005-3-2 loss: 0.778528  [   96/  146]
train() client id: f_00005-3-3 loss: 0.861480  [  128/  146]
train() client id: f_00005-4-0 loss: 0.730844  [   32/  146]
train() client id: f_00005-4-1 loss: 0.767389  [   64/  146]
train() client id: f_00005-4-2 loss: 0.785949  [   96/  146]
train() client id: f_00005-4-3 loss: 0.839208  [  128/  146]
train() client id: f_00005-5-0 loss: 0.777329  [   32/  146]
train() client id: f_00005-5-1 loss: 0.802463  [   64/  146]
train() client id: f_00005-5-2 loss: 0.777223  [   96/  146]
train() client id: f_00005-5-3 loss: 0.773194  [  128/  146]
train() client id: f_00005-6-0 loss: 0.856076  [   32/  146]
train() client id: f_00005-6-1 loss: 0.763439  [   64/  146]
train() client id: f_00005-6-2 loss: 0.829900  [   96/  146]
train() client id: f_00005-6-3 loss: 0.722642  [  128/  146]
train() client id: f_00005-7-0 loss: 0.838143  [   32/  146]
train() client id: f_00005-7-1 loss: 0.793658  [   64/  146]
train() client id: f_00005-7-2 loss: 0.778823  [   96/  146]
train() client id: f_00005-7-3 loss: 0.732967  [  128/  146]
train() client id: f_00005-8-0 loss: 0.967373  [   32/  146]
train() client id: f_00005-8-1 loss: 0.775593  [   64/  146]
train() client id: f_00005-8-2 loss: 0.665408  [   96/  146]
train() client id: f_00005-8-3 loss: 0.742160  [  128/  146]
train() client id: f_00005-9-0 loss: 0.781864  [   32/  146]
train() client id: f_00005-9-1 loss: 0.777506  [   64/  146]
train() client id: f_00005-9-2 loss: 0.799130  [   96/  146]
train() client id: f_00005-9-3 loss: 0.779343  [  128/  146]
train() client id: f_00005-10-0 loss: 0.653978  [   32/  146]
train() client id: f_00005-10-1 loss: 0.752035  [   64/  146]
train() client id: f_00005-10-2 loss: 0.945636  [   96/  146]
train() client id: f_00005-10-3 loss: 0.873154  [  128/  146]
train() client id: f_00005-11-0 loss: 0.651438  [   32/  146]
train() client id: f_00005-11-1 loss: 0.867001  [   64/  146]
train() client id: f_00005-11-2 loss: 0.834310  [   96/  146]
train() client id: f_00005-11-3 loss: 0.808221  [  128/  146]
train() client id: f_00006-0-0 loss: 0.800683  [   32/   54]
train() client id: f_00006-1-0 loss: 0.807490  [   32/   54]
train() client id: f_00006-2-0 loss: 0.827534  [   32/   54]
train() client id: f_00006-3-0 loss: 0.822362  [   32/   54]
train() client id: f_00006-4-0 loss: 0.811690  [   32/   54]
train() client id: f_00006-5-0 loss: 0.838153  [   32/   54]
train() client id: f_00006-6-0 loss: 0.872944  [   32/   54]
train() client id: f_00006-7-0 loss: 0.835974  [   32/   54]
train() client id: f_00006-8-0 loss: 0.824547  [   32/   54]
train() client id: f_00006-9-0 loss: 0.871308  [   32/   54]
train() client id: f_00006-10-0 loss: 0.844013  [   32/   54]
train() client id: f_00006-11-0 loss: 0.781380  [   32/   54]
train() client id: f_00007-0-0 loss: 0.833760  [   32/  179]
train() client id: f_00007-0-1 loss: 0.773751  [   64/  179]
train() client id: f_00007-0-2 loss: 0.778026  [   96/  179]
train() client id: f_00007-0-3 loss: 0.727123  [  128/  179]
train() client id: f_00007-0-4 loss: 0.825746  [  160/  179]
train() client id: f_00007-1-0 loss: 0.783867  [   32/  179]
train() client id: f_00007-1-1 loss: 0.737435  [   64/  179]
train() client id: f_00007-1-2 loss: 0.689732  [   96/  179]
train() client id: f_00007-1-3 loss: 0.830023  [  128/  179]
train() client id: f_00007-1-4 loss: 0.676297  [  160/  179]
train() client id: f_00007-2-0 loss: 0.777135  [   32/  179]
train() client id: f_00007-2-1 loss: 0.741442  [   64/  179]
train() client id: f_00007-2-2 loss: 0.678887  [   96/  179]
train() client id: f_00007-2-3 loss: 0.848045  [  128/  179]
train() client id: f_00007-2-4 loss: 0.657508  [  160/  179]
train() client id: f_00007-3-0 loss: 0.759262  [   32/  179]
train() client id: f_00007-3-1 loss: 0.622568  [   64/  179]
train() client id: f_00007-3-2 loss: 0.676334  [   96/  179]
train() client id: f_00007-3-3 loss: 0.806082  [  128/  179]
train() client id: f_00007-3-4 loss: 0.724601  [  160/  179]
train() client id: f_00007-4-0 loss: 0.619322  [   32/  179]
train() client id: f_00007-4-1 loss: 0.727133  [   64/  179]
train() client id: f_00007-4-2 loss: 0.797996  [   96/  179]
train() client id: f_00007-4-3 loss: 0.717604  [  128/  179]
train() client id: f_00007-4-4 loss: 0.633090  [  160/  179]
train() client id: f_00007-5-0 loss: 0.645897  [   32/  179]
train() client id: f_00007-5-1 loss: 0.709497  [   64/  179]
train() client id: f_00007-5-2 loss: 0.742395  [   96/  179]
train() client id: f_00007-5-3 loss: 0.626064  [  128/  179]
train() client id: f_00007-5-4 loss: 0.717127  [  160/  179]
train() client id: f_00007-6-0 loss: 0.672584  [   32/  179]
train() client id: f_00007-6-1 loss: 0.663621  [   64/  179]
train() client id: f_00007-6-2 loss: 0.645500  [   96/  179]
train() client id: f_00007-6-3 loss: 0.695289  [  128/  179]
train() client id: f_00007-6-4 loss: 0.701297  [  160/  179]
train() client id: f_00007-7-0 loss: 0.635285  [   32/  179]
train() client id: f_00007-7-1 loss: 0.724558  [   64/  179]
train() client id: f_00007-7-2 loss: 0.742923  [   96/  179]
train() client id: f_00007-7-3 loss: 0.665740  [  128/  179]
train() client id: f_00007-7-4 loss: 0.660681  [  160/  179]
train() client id: f_00007-8-0 loss: 0.642493  [   32/  179]
train() client id: f_00007-8-1 loss: 0.757244  [   64/  179]
train() client id: f_00007-8-2 loss: 0.651294  [   96/  179]
train() client id: f_00007-8-3 loss: 0.713905  [  128/  179]
train() client id: f_00007-8-4 loss: 0.574866  [  160/  179]
train() client id: f_00007-9-0 loss: 0.709717  [   32/  179]
train() client id: f_00007-9-1 loss: 0.687653  [   64/  179]
train() client id: f_00007-9-2 loss: 0.634101  [   96/  179]
train() client id: f_00007-9-3 loss: 0.693258  [  128/  179]
train() client id: f_00007-9-4 loss: 0.652549  [  160/  179]
train() client id: f_00007-10-0 loss: 0.632241  [   32/  179]
train() client id: f_00007-10-1 loss: 0.818681  [   64/  179]
train() client id: f_00007-10-2 loss: 0.634318  [   96/  179]
train() client id: f_00007-10-3 loss: 0.607546  [  128/  179]
train() client id: f_00007-10-4 loss: 0.698840  [  160/  179]
train() client id: f_00007-11-0 loss: 0.715339  [   32/  179]
train() client id: f_00007-11-1 loss: 0.694827  [   64/  179]
train() client id: f_00007-11-2 loss: 0.695562  [   96/  179]
train() client id: f_00007-11-3 loss: 0.571915  [  128/  179]
train() client id: f_00007-11-4 loss: 0.636885  [  160/  179]
train() client id: f_00008-0-0 loss: 0.974569  [   32/  130]
train() client id: f_00008-0-1 loss: 0.891353  [   64/  130]
train() client id: f_00008-0-2 loss: 0.920842  [   96/  130]
train() client id: f_00008-0-3 loss: 0.947509  [  128/  130]
train() client id: f_00008-1-0 loss: 0.907252  [   32/  130]
train() client id: f_00008-1-1 loss: 0.963715  [   64/  130]
train() client id: f_00008-1-2 loss: 0.943145  [   96/  130]
train() client id: f_00008-1-3 loss: 0.890345  [  128/  130]
train() client id: f_00008-2-0 loss: 0.854724  [   32/  130]
train() client id: f_00008-2-1 loss: 0.952760  [   64/  130]
train() client id: f_00008-2-2 loss: 0.925424  [   96/  130]
train() client id: f_00008-2-3 loss: 0.996560  [  128/  130]
train() client id: f_00008-3-0 loss: 0.870667  [   32/  130]
train() client id: f_00008-3-1 loss: 0.892244  [   64/  130]
train() client id: f_00008-3-2 loss: 0.914830  [   96/  130]
train() client id: f_00008-3-3 loss: 1.054854  [  128/  130]
train() client id: f_00008-4-0 loss: 0.873912  [   32/  130]
train() client id: f_00008-4-1 loss: 0.955094  [   64/  130]
train() client id: f_00008-4-2 loss: 0.999134  [   96/  130]
train() client id: f_00008-4-3 loss: 0.913488  [  128/  130]
train() client id: f_00008-5-0 loss: 1.015902  [   32/  130]
train() client id: f_00008-5-1 loss: 0.884893  [   64/  130]
train() client id: f_00008-5-2 loss: 0.938241  [   96/  130]
train() client id: f_00008-5-3 loss: 0.856910  [  128/  130]
train() client id: f_00008-6-0 loss: 0.872660  [   32/  130]
train() client id: f_00008-6-1 loss: 0.923280  [   64/  130]
train() client id: f_00008-6-2 loss: 0.993680  [   96/  130]
train() client id: f_00008-6-3 loss: 0.947348  [  128/  130]
train() client id: f_00008-7-0 loss: 0.985255  [   32/  130]
train() client id: f_00008-7-1 loss: 0.919340  [   64/  130]
train() client id: f_00008-7-2 loss: 0.895726  [   96/  130]
train() client id: f_00008-7-3 loss: 0.925993  [  128/  130]
train() client id: f_00008-8-0 loss: 0.993202  [   32/  130]
train() client id: f_00008-8-1 loss: 0.991546  [   64/  130]
train() client id: f_00008-8-2 loss: 0.860789  [   96/  130]
train() client id: f_00008-8-3 loss: 0.895836  [  128/  130]
train() client id: f_00008-9-0 loss: 0.873261  [   32/  130]
train() client id: f_00008-9-1 loss: 0.926574  [   64/  130]
train() client id: f_00008-9-2 loss: 0.871484  [   96/  130]
train() client id: f_00008-9-3 loss: 1.043883  [  128/  130]
train() client id: f_00008-10-0 loss: 1.007774  [   32/  130]
train() client id: f_00008-10-1 loss: 0.911883  [   64/  130]
train() client id: f_00008-10-2 loss: 0.890526  [   96/  130]
train() client id: f_00008-10-3 loss: 0.935193  [  128/  130]
train() client id: f_00008-11-0 loss: 0.886897  [   32/  130]
train() client id: f_00008-11-1 loss: 0.880219  [   64/  130]
train() client id: f_00008-11-2 loss: 0.910113  [   96/  130]
train() client id: f_00008-11-3 loss: 1.073089  [  128/  130]
train() client id: f_00009-0-0 loss: 1.272923  [   32/  118]
train() client id: f_00009-0-1 loss: 1.206771  [   64/  118]
train() client id: f_00009-0-2 loss: 1.202605  [   96/  118]
train() client id: f_00009-1-0 loss: 1.125587  [   32/  118]
train() client id: f_00009-1-1 loss: 1.201738  [   64/  118]
train() client id: f_00009-1-2 loss: 1.205749  [   96/  118]
train() client id: f_00009-2-0 loss: 1.186005  [   32/  118]
train() client id: f_00009-2-1 loss: 1.131921  [   64/  118]
train() client id: f_00009-2-2 loss: 1.108205  [   96/  118]
train() client id: f_00009-3-0 loss: 1.061776  [   32/  118]
train() client id: f_00009-3-1 loss: 1.066765  [   64/  118]
train() client id: f_00009-3-2 loss: 1.124299  [   96/  118]
train() client id: f_00009-4-0 loss: 1.090333  [   32/  118]
train() client id: f_00009-4-1 loss: 1.098980  [   64/  118]
train() client id: f_00009-4-2 loss: 1.060668  [   96/  118]
train() client id: f_00009-5-0 loss: 1.043946  [   32/  118]
train() client id: f_00009-5-1 loss: 1.056402  [   64/  118]
train() client id: f_00009-5-2 loss: 1.047733  [   96/  118]
train() client id: f_00009-6-0 loss: 1.051694  [   32/  118]
train() client id: f_00009-6-1 loss: 1.052024  [   64/  118]
train() client id: f_00009-6-2 loss: 0.966733  [   96/  118]
train() client id: f_00009-7-0 loss: 0.989127  [   32/  118]
train() client id: f_00009-7-1 loss: 0.973767  [   64/  118]
train() client id: f_00009-7-2 loss: 1.060933  [   96/  118]
train() client id: f_00009-8-0 loss: 1.016371  [   32/  118]
train() client id: f_00009-8-1 loss: 1.006269  [   64/  118]
train() client id: f_00009-8-2 loss: 0.906215  [   96/  118]
train() client id: f_00009-9-0 loss: 0.983175  [   32/  118]
train() client id: f_00009-9-1 loss: 0.966938  [   64/  118]
train() client id: f_00009-9-2 loss: 0.962987  [   96/  118]
train() client id: f_00009-10-0 loss: 1.030393  [   32/  118]
train() client id: f_00009-10-1 loss: 0.915652  [   64/  118]
train() client id: f_00009-10-2 loss: 0.944132  [   96/  118]
train() client id: f_00009-11-0 loss: 0.932737  [   32/  118]
train() client id: f_00009-11-1 loss: 0.954098  [   64/  118]
train() client id: f_00009-11-2 loss: 0.936822  [   96/  118]
At round 5 accuracy: 0.6312997347480106
At round 5 training accuracy: 0.5767940979208585
At round 5 training loss: 0.8724410013020665
update_location
xs = [-3.9056584   4.20031788 45.00902392 18.81129433 -9.02070377  3.95640986
 -7.44319194 -1.32485185 29.66397685 -2.06087855]
ys = [ 37.5879595   20.55583871   1.32061395  -7.45517586   9.35018685
 -17.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [106.90233331 102.17722435 109.6702159  102.02668495 100.84046356
 101.54312677 100.31097472 100.09599397 105.7762804  100.10124413]
dists_bs = [219.44325599 236.57082002 280.3007309  266.18172178 234.49730782
 262.60387904 244.18419069 249.53494358 258.20894888 243.20450851]
uav_gains = [8.46306925e-11 9.47572782e-11 7.93911850e-11 9.51072087e-11
 9.79289548e-11 9.62435611e-11 9.92263936e-11 9.97600464e-11
 8.69011873e-11 9.97469659e-11]
bs_gains = [3.07320703e-11 2.49001977e-11 1.54862491e-11 1.78976100e-11
 2.55216104e-11 1.85887806e-11 2.27868853e-11 2.14450097e-11
 1.94883232e-11 2.30448319e-11]
Round 6
-------------------------------
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.20035717 21.31407924 10.04322819  3.59039783 24.57615325 11.85884139
  4.4646498  14.41376646 10.58251293  9.61787463]
obj_prev = 120.66186087163511
eta_min = 7.409354424179371e-10	eta_max = 0.91885879376386
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 28.063470776703085	eta = 0.909090909090909
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 48.384419986505044	eta = 0.5272822567213747
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 38.70245321762515	eta = 0.6591893805073021
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.96801720495558	eta = 0.6901167032896544
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88278882583632	eta = 0.6917114180576249
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88256856080987	eta = 0.6917155490018552
eta = 0.6917155490018552
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.03041438 0.06396674 0.02993161 0.01037951 0.07386349 0.03524206
 0.01303473 0.04320773 0.03137991 0.0284833 ]
ene_total = [3.12833392 6.13288912 3.09403261 1.41867893 6.96750532 3.75135634
 1.63958615 4.19625148 3.41497814 3.13895655]
ti_comp = [0.28546116 0.26646804 0.2846808  0.28683999 0.2669565  0.2602256
 0.28732669 0.28738774 0.26129447 0.26489704]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.15785733e-05 2.30384101e-04 2.06801576e-05 8.49436889e-07
 3.53417801e-04 4.03983548e-05 1.67661934e-06 6.10418604e-05
 2.82862087e-05 2.05824434e-05]
ene_total = [0.55252256 0.73118701 0.55906002 0.53908008 0.73747448 0.76799073
 0.53502532 0.53953919 0.7579054  0.72672045]
optimize_network iter = 0 obj = 6.446505230599096
eta = 0.6917155490018552
freqs = [5.32723555e+07 1.20027048e+08 5.25704736e+07 1.80928566e+07
 1.38343675e+08 6.77144291e+07 2.26827740e+07 7.51732237e+07
 6.00470262e+07 5.37629723e+07]
eta_min = 0.6801612059033121	eta_max = 0.6917155490018475
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 0.06073437862890649	eta = 0.9090909090909091
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 21.69496157536947	eta = 0.002544972079761971
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.3026899172665374	eta = 0.023977640700475235
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.226369694175787	eta = 0.024799597131268097
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.2263378919337997	eta = 0.024799951382431865
eta = 0.024799951382431865
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16436107e-04 2.31078474e-03 2.07424872e-04 8.51997075e-06
 3.54482996e-03 4.05201146e-04 1.68167264e-05 6.12258394e-04
 2.83714628e-04 2.06444785e-04]
ene_total = [0.17988639 0.28688359 0.18173569 0.17062183 0.31863717 0.25255426
 0.16954014 0.18532952 0.24643571 0.23471359]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 1 obj = 6.216126697450631
eta = 0.6801612059033121
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
eta_min = 0.6801612059033234	eta_max = 0.680161205903311
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 0.06036643683170785	eta = 0.909090909090909
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 21.694610889380957	eta = 0.0025295949864110306
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.300984648578189	eta = 0.023850041316801688
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.2251002879015918	eta = 0.024663418200206223
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.225068999906738	eta = 0.02466376500693525
eta = 0.02466376500693525
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16630671e-04 2.29842707e-03 2.07561279e-04 8.53123712e-06
 3.52647900e-03 4.02127407e-04 1.68414620e-05 6.13170349e-04
 2.81673164e-04 2.05226390e-04]
ene_total = [0.17986649 0.2865125  0.18171397 0.17059831 0.31810109 0.25243665
 0.16951713 0.18532807 0.24634661 0.23464818]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 2 obj = 6.216126697450847
eta = 0.6801612059033234
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
Done!
At round 6 energy consumption: 6.216126697450631
At round 6 eta: 0.6801612059033234
At round 6 local rounds: 12.620787147614399
At round 6 global rounds: 81.68905166188517
At round 6 a_n: 25.784781927469155
gradient difference: 0.4224415719509125
train() client id: f_00000-0-0 loss: 1.291909  [   32/  126]
train() client id: f_00000-0-1 loss: 1.256664  [   64/  126]
train() client id: f_00000-0-2 loss: 1.479534  [   96/  126]
train() client id: f_00000-1-0 loss: 1.139155  [   32/  126]
train() client id: f_00000-1-1 loss: 1.356644  [   64/  126]
train() client id: f_00000-1-2 loss: 1.256760  [   96/  126]
train() client id: f_00000-2-0 loss: 1.047399  [   32/  126]
train() client id: f_00000-2-1 loss: 1.169411  [   64/  126]
train() client id: f_00000-2-2 loss: 1.163224  [   96/  126]
train() client id: f_00000-3-0 loss: 1.126881  [   32/  126]
train() client id: f_00000-3-1 loss: 1.116567  [   64/  126]
train() client id: f_00000-3-2 loss: 0.979199  [   96/  126]
train() client id: f_00000-4-0 loss: 1.003871  [   32/  126]
train() client id: f_00000-4-1 loss: 1.053301  [   64/  126]
train() client id: f_00000-4-2 loss: 0.996431  [   96/  126]
train() client id: f_00000-5-0 loss: 1.004121  [   32/  126]
train() client id: f_00000-5-1 loss: 0.996168  [   64/  126]
train() client id: f_00000-5-2 loss: 0.973346  [   96/  126]
train() client id: f_00000-6-0 loss: 0.938664  [   32/  126]
train() client id: f_00000-6-1 loss: 1.000376  [   64/  126]
train() client id: f_00000-6-2 loss: 0.944937  [   96/  126]
train() client id: f_00000-7-0 loss: 0.896097  [   32/  126]
train() client id: f_00000-7-1 loss: 1.021849  [   64/  126]
train() client id: f_00000-7-2 loss: 0.872010  [   96/  126]
train() client id: f_00000-8-0 loss: 0.935395  [   32/  126]
train() client id: f_00000-8-1 loss: 0.968524  [   64/  126]
train() client id: f_00000-8-2 loss: 0.922432  [   96/  126]
train() client id: f_00000-9-0 loss: 0.896101  [   32/  126]
train() client id: f_00000-9-1 loss: 0.965623  [   64/  126]
train() client id: f_00000-9-2 loss: 0.892849  [   96/  126]
train() client id: f_00000-10-0 loss: 0.953168  [   32/  126]
train() client id: f_00000-10-1 loss: 0.898581  [   64/  126]
train() client id: f_00000-10-2 loss: 0.891238  [   96/  126]
train() client id: f_00000-11-0 loss: 0.819973  [   32/  126]
train() client id: f_00000-11-1 loss: 0.883897  [   64/  126]
train() client id: f_00000-11-2 loss: 0.908223  [   96/  126]
train() client id: f_00001-0-0 loss: 0.670111  [   32/  265]
train() client id: f_00001-0-1 loss: 0.581152  [   64/  265]
train() client id: f_00001-0-2 loss: 0.548418  [   96/  265]
train() client id: f_00001-0-3 loss: 0.573265  [  128/  265]
train() client id: f_00001-0-4 loss: 0.525214  [  160/  265]
train() client id: f_00001-0-5 loss: 0.531519  [  192/  265]
train() client id: f_00001-0-6 loss: 0.669242  [  224/  265]
train() client id: f_00001-0-7 loss: 0.477419  [  256/  265]
train() client id: f_00001-1-0 loss: 0.564149  [   32/  265]
train() client id: f_00001-1-1 loss: 0.537398  [   64/  265]
train() client id: f_00001-1-2 loss: 0.509880  [   96/  265]
train() client id: f_00001-1-3 loss: 0.606843  [  128/  265]
train() client id: f_00001-1-4 loss: 0.476729  [  160/  265]
train() client id: f_00001-1-5 loss: 0.598615  [  192/  265]
train() client id: f_00001-1-6 loss: 0.511247  [  224/  265]
train() client id: f_00001-1-7 loss: 0.510463  [  256/  265]
train() client id: f_00001-2-0 loss: 0.522577  [   32/  265]
train() client id: f_00001-2-1 loss: 0.445010  [   64/  265]
train() client id: f_00001-2-2 loss: 0.634292  [   96/  265]
train() client id: f_00001-2-3 loss: 0.515166  [  128/  265]
train() client id: f_00001-2-4 loss: 0.549048  [  160/  265]
train() client id: f_00001-2-5 loss: 0.504093  [  192/  265]
train() client id: f_00001-2-6 loss: 0.546736  [  224/  265]
train() client id: f_00001-2-7 loss: 0.436661  [  256/  265]
train() client id: f_00001-3-0 loss: 0.508361  [   32/  265]
train() client id: f_00001-3-1 loss: 0.590989  [   64/  265]
train() client id: f_00001-3-2 loss: 0.499400  [   96/  265]
train() client id: f_00001-3-3 loss: 0.441527  [  128/  265]
train() client id: f_00001-3-4 loss: 0.470659  [  160/  265]
train() client id: f_00001-3-5 loss: 0.534427  [  192/  265]
train() client id: f_00001-3-6 loss: 0.550993  [  224/  265]
train() client id: f_00001-3-7 loss: 0.506184  [  256/  265]
train() client id: f_00001-4-0 loss: 0.489491  [   32/  265]
train() client id: f_00001-4-1 loss: 0.427656  [   64/  265]
train() client id: f_00001-4-2 loss: 0.483472  [   96/  265]
train() client id: f_00001-4-3 loss: 0.502933  [  128/  265]
train() client id: f_00001-4-4 loss: 0.577858  [  160/  265]
train() client id: f_00001-4-5 loss: 0.515953  [  192/  265]
train() client id: f_00001-4-6 loss: 0.459828  [  224/  265]
train() client id: f_00001-4-7 loss: 0.476196  [  256/  265]
train() client id: f_00001-5-0 loss: 0.463837  [   32/  265]
train() client id: f_00001-5-1 loss: 0.510996  [   64/  265]
train() client id: f_00001-5-2 loss: 0.538606  [   96/  265]
train() client id: f_00001-5-3 loss: 0.464463  [  128/  265]
train() client id: f_00001-5-4 loss: 0.514808  [  160/  265]
train() client id: f_00001-5-5 loss: 0.417462  [  192/  265]
train() client id: f_00001-5-6 loss: 0.490414  [  224/  265]
train() client id: f_00001-5-7 loss: 0.469653  [  256/  265]
train() client id: f_00001-6-0 loss: 0.449234  [   32/  265]
train() client id: f_00001-6-1 loss: 0.458154  [   64/  265]
train() client id: f_00001-6-2 loss: 0.534279  [   96/  265]
train() client id: f_00001-6-3 loss: 0.477726  [  128/  265]
train() client id: f_00001-6-4 loss: 0.567254  [  160/  265]
train() client id: f_00001-6-5 loss: 0.456581  [  192/  265]
train() client id: f_00001-6-6 loss: 0.498233  [  224/  265]
train() client id: f_00001-6-7 loss: 0.447869  [  256/  265]
train() client id: f_00001-7-0 loss: 0.426647  [   32/  265]
train() client id: f_00001-7-1 loss: 0.713655  [   64/  265]
train() client id: f_00001-7-2 loss: 0.402870  [   96/  265]
train() client id: f_00001-7-3 loss: 0.433337  [  128/  265]
train() client id: f_00001-7-4 loss: 0.511758  [  160/  265]
train() client id: f_00001-7-5 loss: 0.380583  [  192/  265]
train() client id: f_00001-7-6 loss: 0.460825  [  224/  265]
train() client id: f_00001-7-7 loss: 0.513134  [  256/  265]
train() client id: f_00001-8-0 loss: 0.481225  [   32/  265]
train() client id: f_00001-8-1 loss: 0.504734  [   64/  265]
train() client id: f_00001-8-2 loss: 0.558491  [   96/  265]
train() client id: f_00001-8-3 loss: 0.386538  [  128/  265]
train() client id: f_00001-8-4 loss: 0.456399  [  160/  265]
train() client id: f_00001-8-5 loss: 0.562557  [  192/  265]
train() client id: f_00001-8-6 loss: 0.381819  [  224/  265]
train() client id: f_00001-8-7 loss: 0.437423  [  256/  265]
train() client id: f_00001-9-0 loss: 0.505598  [   32/  265]
train() client id: f_00001-9-1 loss: 0.505996  [   64/  265]
train() client id: f_00001-9-2 loss: 0.507167  [   96/  265]
train() client id: f_00001-9-3 loss: 0.391796  [  128/  265]
train() client id: f_00001-9-4 loss: 0.518763  [  160/  265]
train() client id: f_00001-9-5 loss: 0.498082  [  192/  265]
train() client id: f_00001-9-6 loss: 0.396977  [  224/  265]
train() client id: f_00001-9-7 loss: 0.400584  [  256/  265]
train() client id: f_00001-10-0 loss: 0.401756  [   32/  265]
train() client id: f_00001-10-1 loss: 0.522926  [   64/  265]
train() client id: f_00001-10-2 loss: 0.442451  [   96/  265]
train() client id: f_00001-10-3 loss: 0.454395  [  128/  265]
train() client id: f_00001-10-4 loss: 0.561039  [  160/  265]
train() client id: f_00001-10-5 loss: 0.498040  [  192/  265]
train() client id: f_00001-10-6 loss: 0.479388  [  224/  265]
train() client id: f_00001-10-7 loss: 0.413211  [  256/  265]
train() client id: f_00001-11-0 loss: 0.559582  [   32/  265]
train() client id: f_00001-11-1 loss: 0.500035  [   64/  265]
train() client id: f_00001-11-2 loss: 0.577751  [   96/  265]
train() client id: f_00001-11-3 loss: 0.483782  [  128/  265]
train() client id: f_00001-11-4 loss: 0.391163  [  160/  265]
train() client id: f_00001-11-5 loss: 0.381934  [  192/  265]
train() client id: f_00001-11-6 loss: 0.491879  [  224/  265]
train() client id: f_00001-11-7 loss: 0.371378  [  256/  265]
train() client id: f_00002-0-0 loss: 1.209135  [   32/  124]
train() client id: f_00002-0-1 loss: 1.223529  [   64/  124]
train() client id: f_00002-0-2 loss: 1.191418  [   96/  124]
train() client id: f_00002-1-0 loss: 1.218796  [   32/  124]
train() client id: f_00002-1-1 loss: 1.166113  [   64/  124]
train() client id: f_00002-1-2 loss: 1.159677  [   96/  124]
train() client id: f_00002-2-0 loss: 1.124815  [   32/  124]
train() client id: f_00002-2-1 loss: 1.185020  [   64/  124]
train() client id: f_00002-2-2 loss: 1.119802  [   96/  124]
train() client id: f_00002-3-0 loss: 1.146023  [   32/  124]
train() client id: f_00002-3-1 loss: 1.145012  [   64/  124]
train() client id: f_00002-3-2 loss: 1.092560  [   96/  124]
train() client id: f_00002-4-0 loss: 1.046191  [   32/  124]
train() client id: f_00002-4-1 loss: 1.130192  [   64/  124]
train() client id: f_00002-4-2 loss: 1.131609  [   96/  124]
train() client id: f_00002-5-0 loss: 1.103353  [   32/  124]
train() client id: f_00002-5-1 loss: 1.105120  [   64/  124]
train() client id: f_00002-5-2 loss: 1.052631  [   96/  124]
train() client id: f_00002-6-0 loss: 1.061540  [   32/  124]
train() client id: f_00002-6-1 loss: 1.147658  [   64/  124]
train() client id: f_00002-6-2 loss: 0.998724  [   96/  124]
train() client id: f_00002-7-0 loss: 1.069677  [   32/  124]
train() client id: f_00002-7-1 loss: 1.049551  [   64/  124]
train() client id: f_00002-7-2 loss: 0.997371  [   96/  124]
train() client id: f_00002-8-0 loss: 1.056534  [   32/  124]
train() client id: f_00002-8-1 loss: 1.029817  [   64/  124]
train() client id: f_00002-8-2 loss: 1.116017  [   96/  124]
train() client id: f_00002-9-0 loss: 0.988257  [   32/  124]
train() client id: f_00002-9-1 loss: 1.069333  [   64/  124]
train() client id: f_00002-9-2 loss: 1.106217  [   96/  124]
train() client id: f_00002-10-0 loss: 1.113166  [   32/  124]
train() client id: f_00002-10-1 loss: 1.064893  [   64/  124]
train() client id: f_00002-10-2 loss: 1.003739  [   96/  124]
train() client id: f_00002-11-0 loss: 1.097366  [   32/  124]
train() client id: f_00002-11-1 loss: 1.011157  [   64/  124]
train() client id: f_00002-11-2 loss: 1.042628  [   96/  124]
train() client id: f_00003-0-0 loss: 0.963969  [   32/   43]
train() client id: f_00003-1-0 loss: 0.923308  [   32/   43]
train() client id: f_00003-2-0 loss: 0.985078  [   32/   43]
train() client id: f_00003-3-0 loss: 1.001238  [   32/   43]
train() client id: f_00003-4-0 loss: 0.967910  [   32/   43]
train() client id: f_00003-5-0 loss: 1.033054  [   32/   43]
train() client id: f_00003-6-0 loss: 1.020142  [   32/   43]
train() client id: f_00003-7-0 loss: 0.912604  [   32/   43]
train() client id: f_00003-8-0 loss: 0.834798  [   32/   43]
train() client id: f_00003-9-0 loss: 0.893295  [   32/   43]
train() client id: f_00003-10-0 loss: 0.955667  [   32/   43]
train() client id: f_00003-11-0 loss: 0.869528  [   32/   43]
train() client id: f_00004-0-0 loss: 0.928862  [   32/  306]
train() client id: f_00004-0-1 loss: 0.992060  [   64/  306]
train() client id: f_00004-0-2 loss: 1.126395  [   96/  306]
train() client id: f_00004-0-3 loss: 1.121154  [  128/  306]
train() client id: f_00004-0-4 loss: 0.969794  [  160/  306]
train() client id: f_00004-0-5 loss: 0.938710  [  192/  306]
train() client id: f_00004-0-6 loss: 0.916160  [  224/  306]
train() client id: f_00004-0-7 loss: 1.010643  [  256/  306]
train() client id: f_00004-0-8 loss: 1.056501  [  288/  306]
train() client id: f_00004-1-0 loss: 0.955526  [   32/  306]
train() client id: f_00004-1-1 loss: 1.006892  [   64/  306]
train() client id: f_00004-1-2 loss: 1.057353  [   96/  306]
train() client id: f_00004-1-3 loss: 1.028852  [  128/  306]
train() client id: f_00004-1-4 loss: 0.979465  [  160/  306]
train() client id: f_00004-1-5 loss: 1.012084  [  192/  306]
train() client id: f_00004-1-6 loss: 0.975941  [  224/  306]
train() client id: f_00004-1-7 loss: 1.050661  [  256/  306]
train() client id: f_00004-1-8 loss: 0.994115  [  288/  306]
train() client id: f_00004-2-0 loss: 1.022609  [   32/  306]
train() client id: f_00004-2-1 loss: 1.102741  [   64/  306]
train() client id: f_00004-2-2 loss: 0.862256  [   96/  306]
train() client id: f_00004-2-3 loss: 1.099918  [  128/  306]
train() client id: f_00004-2-4 loss: 0.918444  [  160/  306]
train() client id: f_00004-2-5 loss: 1.009500  [  192/  306]
train() client id: f_00004-2-6 loss: 0.952680  [  224/  306]
train() client id: f_00004-2-7 loss: 0.979646  [  256/  306]
train() client id: f_00004-2-8 loss: 1.073497  [  288/  306]
train() client id: f_00004-3-0 loss: 1.037090  [   32/  306]
train() client id: f_00004-3-1 loss: 0.978668  [   64/  306]
train() client id: f_00004-3-2 loss: 0.905482  [   96/  306]
train() client id: f_00004-3-3 loss: 0.955317  [  128/  306]
train() client id: f_00004-3-4 loss: 0.941767  [  160/  306]
train() client id: f_00004-3-5 loss: 1.023448  [  192/  306]
train() client id: f_00004-3-6 loss: 1.094944  [  224/  306]
train() client id: f_00004-3-7 loss: 1.038797  [  256/  306]
train() client id: f_00004-3-8 loss: 1.041174  [  288/  306]
train() client id: f_00004-4-0 loss: 0.972962  [   32/  306]
train() client id: f_00004-4-1 loss: 0.942074  [   64/  306]
train() client id: f_00004-4-2 loss: 0.934857  [   96/  306]
train() client id: f_00004-4-3 loss: 1.080055  [  128/  306]
train() client id: f_00004-4-4 loss: 0.921305  [  160/  306]
train() client id: f_00004-4-5 loss: 0.960681  [  192/  306]
train() client id: f_00004-4-6 loss: 1.123658  [  224/  306]
train() client id: f_00004-4-7 loss: 1.088861  [  256/  306]
train() client id: f_00004-4-8 loss: 0.977542  [  288/  306]
train() client id: f_00004-5-0 loss: 0.944775  [   32/  306]
train() client id: f_00004-5-1 loss: 0.975506  [   64/  306]
train() client id: f_00004-5-2 loss: 1.094837  [   96/  306]
train() client id: f_00004-5-3 loss: 1.059933  [  128/  306]
train() client id: f_00004-5-4 loss: 1.005201  [  160/  306]
train() client id: f_00004-5-5 loss: 0.939013  [  192/  306]
train() client id: f_00004-5-6 loss: 0.961613  [  224/  306]
train() client id: f_00004-5-7 loss: 0.983423  [  256/  306]
train() client id: f_00004-5-8 loss: 1.012384  [  288/  306]
train() client id: f_00004-6-0 loss: 0.994755  [   32/  306]
train() client id: f_00004-6-1 loss: 0.984958  [   64/  306]
train() client id: f_00004-6-2 loss: 0.904089  [   96/  306]
train() client id: f_00004-6-3 loss: 0.959436  [  128/  306]
train() client id: f_00004-6-4 loss: 1.026537  [  160/  306]
train() client id: f_00004-6-5 loss: 1.015470  [  192/  306]
train() client id: f_00004-6-6 loss: 0.969016  [  224/  306]
train() client id: f_00004-6-7 loss: 0.971117  [  256/  306]
train() client id: f_00004-6-8 loss: 1.097411  [  288/  306]
train() client id: f_00004-7-0 loss: 0.936589  [   32/  306]
train() client id: f_00004-7-1 loss: 1.074970  [   64/  306]
train() client id: f_00004-7-2 loss: 0.997483  [   96/  306]
train() client id: f_00004-7-3 loss: 1.078817  [  128/  306]
train() client id: f_00004-7-4 loss: 0.978460  [  160/  306]
train() client id: f_00004-7-5 loss: 1.004259  [  192/  306]
train() client id: f_00004-7-6 loss: 0.973044  [  224/  306]
train() client id: f_00004-7-7 loss: 0.958475  [  256/  306]
train() client id: f_00004-7-8 loss: 0.978307  [  288/  306]
train() client id: f_00004-8-0 loss: 0.938024  [   32/  306]
train() client id: f_00004-8-1 loss: 0.989266  [   64/  306]
train() client id: f_00004-8-2 loss: 0.964555  [   96/  306]
train() client id: f_00004-8-3 loss: 1.053461  [  128/  306]
train() client id: f_00004-8-4 loss: 1.056142  [  160/  306]
train() client id: f_00004-8-5 loss: 1.035614  [  192/  306]
train() client id: f_00004-8-6 loss: 0.994521  [  224/  306]
train() client id: f_00004-8-7 loss: 0.998781  [  256/  306]
train() client id: f_00004-8-8 loss: 0.894548  [  288/  306]
train() client id: f_00004-9-0 loss: 1.068476  [   32/  306]
train() client id: f_00004-9-1 loss: 1.126189  [   64/  306]
train() client id: f_00004-9-2 loss: 0.951766  [   96/  306]
train() client id: f_00004-9-3 loss: 1.035632  [  128/  306]
train() client id: f_00004-9-4 loss: 0.958977  [  160/  306]
train() client id: f_00004-9-5 loss: 0.931809  [  192/  306]
train() client id: f_00004-9-6 loss: 0.870625  [  224/  306]
train() client id: f_00004-9-7 loss: 0.982888  [  256/  306]
train() client id: f_00004-9-8 loss: 1.029903  [  288/  306]
train() client id: f_00004-10-0 loss: 1.017763  [   32/  306]
train() client id: f_00004-10-1 loss: 0.963470  [   64/  306]
train() client id: f_00004-10-2 loss: 0.863721  [   96/  306]
train() client id: f_00004-10-3 loss: 0.982966  [  128/  306]
train() client id: f_00004-10-4 loss: 1.014678  [  160/  306]
train() client id: f_00004-10-5 loss: 1.097095  [  192/  306]
train() client id: f_00004-10-6 loss: 0.987834  [  224/  306]
train() client id: f_00004-10-7 loss: 0.944019  [  256/  306]
train() client id: f_00004-10-8 loss: 1.056447  [  288/  306]
train() client id: f_00004-11-0 loss: 0.951235  [   32/  306]
train() client id: f_00004-11-1 loss: 1.008721  [   64/  306]
train() client id: f_00004-11-2 loss: 1.087706  [   96/  306]
train() client id: f_00004-11-3 loss: 1.045782  [  128/  306]
train() client id: f_00004-11-4 loss: 1.020877  [  160/  306]
train() client id: f_00004-11-5 loss: 1.025223  [  192/  306]
train() client id: f_00004-11-6 loss: 0.882023  [  224/  306]
train() client id: f_00004-11-7 loss: 0.961969  [  256/  306]
train() client id: f_00004-11-8 loss: 0.940535  [  288/  306]
train() client id: f_00005-0-0 loss: 0.911482  [   32/  146]
train() client id: f_00005-0-1 loss: 0.727831  [   64/  146]
train() client id: f_00005-0-2 loss: 0.825286  [   96/  146]
train() client id: f_00005-0-3 loss: 0.669617  [  128/  146]
train() client id: f_00005-1-0 loss: 0.779868  [   32/  146]
train() client id: f_00005-1-1 loss: 0.772920  [   64/  146]
train() client id: f_00005-1-2 loss: 0.720612  [   96/  146]
train() client id: f_00005-1-3 loss: 0.830195  [  128/  146]
train() client id: f_00005-2-0 loss: 0.688690  [   32/  146]
train() client id: f_00005-2-1 loss: 0.839245  [   64/  146]
train() client id: f_00005-2-2 loss: 0.732374  [   96/  146]
train() client id: f_00005-2-3 loss: 0.820553  [  128/  146]
train() client id: f_00005-3-0 loss: 0.820319  [   32/  146]
train() client id: f_00005-3-1 loss: 0.725203  [   64/  146]
train() client id: f_00005-3-2 loss: 0.681771  [   96/  146]
train() client id: f_00005-3-3 loss: 0.744101  [  128/  146]
train() client id: f_00005-4-0 loss: 0.776344  [   32/  146]
train() client id: f_00005-4-1 loss: 0.679070  [   64/  146]
train() client id: f_00005-4-2 loss: 0.780950  [   96/  146]
train() client id: f_00005-4-3 loss: 0.814487  [  128/  146]
train() client id: f_00005-5-0 loss: 0.695450  [   32/  146]
train() client id: f_00005-5-1 loss: 0.800780  [   64/  146]
train() client id: f_00005-5-2 loss: 0.740472  [   96/  146]
train() client id: f_00005-5-3 loss: 0.753796  [  128/  146]
train() client id: f_00005-6-0 loss: 0.817275  [   32/  146]
train() client id: f_00005-6-1 loss: 0.733418  [   64/  146]
train() client id: f_00005-6-2 loss: 0.796362  [   96/  146]
train() client id: f_00005-6-3 loss: 0.759728  [  128/  146]
train() client id: f_00005-7-0 loss: 0.983822  [   32/  146]
train() client id: f_00005-7-1 loss: 0.723670  [   64/  146]
train() client id: f_00005-7-2 loss: 0.652670  [   96/  146]
train() client id: f_00005-7-3 loss: 0.607914  [  128/  146]
train() client id: f_00005-8-0 loss: 0.775184  [   32/  146]
train() client id: f_00005-8-1 loss: 0.654009  [   64/  146]
train() client id: f_00005-8-2 loss: 0.665767  [   96/  146]
train() client id: f_00005-8-3 loss: 0.817380  [  128/  146]
train() client id: f_00005-9-0 loss: 0.671068  [   32/  146]
train() client id: f_00005-9-1 loss: 0.742902  [   64/  146]
train() client id: f_00005-9-2 loss: 0.746400  [   96/  146]
train() client id: f_00005-9-3 loss: 0.768988  [  128/  146]
train() client id: f_00005-10-0 loss: 0.888034  [   32/  146]
train() client id: f_00005-10-1 loss: 0.758618  [   64/  146]
train() client id: f_00005-10-2 loss: 0.622314  [   96/  146]
train() client id: f_00005-10-3 loss: 0.690003  [  128/  146]
train() client id: f_00005-11-0 loss: 0.861184  [   32/  146]
train() client id: f_00005-11-1 loss: 0.693804  [   64/  146]
train() client id: f_00005-11-2 loss: 0.705137  [   96/  146]
train() client id: f_00005-11-3 loss: 0.683019  [  128/  146]
train() client id: f_00006-0-0 loss: 0.885302  [   32/   54]
train() client id: f_00006-1-0 loss: 0.880327  [   32/   54]
train() client id: f_00006-2-0 loss: 0.864393  [   32/   54]
train() client id: f_00006-3-0 loss: 0.890625  [   32/   54]
train() client id: f_00006-4-0 loss: 0.877668  [   32/   54]
train() client id: f_00006-5-0 loss: 0.851960  [   32/   54]
train() client id: f_00006-6-0 loss: 0.869602  [   32/   54]
train() client id: f_00006-7-0 loss: 0.864448  [   32/   54]
train() client id: f_00006-8-0 loss: 0.874911  [   32/   54]
train() client id: f_00006-9-0 loss: 0.907444  [   32/   54]
train() client id: f_00006-10-0 loss: 0.931948  [   32/   54]
train() client id: f_00006-11-0 loss: 0.834318  [   32/   54]
train() client id: f_00007-0-0 loss: 0.705800  [   32/  179]
train() client id: f_00007-0-1 loss: 0.793897  [   64/  179]
train() client id: f_00007-0-2 loss: 0.778631  [   96/  179]
train() client id: f_00007-0-3 loss: 0.839875  [  128/  179]
train() client id: f_00007-0-4 loss: 0.782383  [  160/  179]
train() client id: f_00007-1-0 loss: 0.832962  [   32/  179]
train() client id: f_00007-1-1 loss: 0.703141  [   64/  179]
train() client id: f_00007-1-2 loss: 0.782392  [   96/  179]
train() client id: f_00007-1-3 loss: 0.722568  [  128/  179]
train() client id: f_00007-1-4 loss: 0.674210  [  160/  179]
train() client id: f_00007-2-0 loss: 0.728917  [   32/  179]
train() client id: f_00007-2-1 loss: 0.741947  [   64/  179]
train() client id: f_00007-2-2 loss: 0.779195  [   96/  179]
train() client id: f_00007-2-3 loss: 0.683946  [  128/  179]
train() client id: f_00007-2-4 loss: 0.710734  [  160/  179]
train() client id: f_00007-3-0 loss: 0.698324  [   32/  179]
train() client id: f_00007-3-1 loss: 0.773201  [   64/  179]
train() client id: f_00007-3-2 loss: 0.644021  [   96/  179]
train() client id: f_00007-3-3 loss: 0.758099  [  128/  179]
train() client id: f_00007-3-4 loss: 0.770854  [  160/  179]
train() client id: f_00007-4-0 loss: 0.665044  [   32/  179]
train() client id: f_00007-4-1 loss: 0.765794  [   64/  179]
train() client id: f_00007-4-2 loss: 0.642271  [   96/  179]
train() client id: f_00007-4-3 loss: 0.745779  [  128/  179]
train() client id: f_00007-4-4 loss: 0.798073  [  160/  179]
train() client id: f_00007-5-0 loss: 0.766905  [   32/  179]
train() client id: f_00007-5-1 loss: 0.644626  [   64/  179]
train() client id: f_00007-5-2 loss: 0.701365  [   96/  179]
train() client id: f_00007-5-3 loss: 0.689114  [  128/  179]
train() client id: f_00007-5-4 loss: 0.705984  [  160/  179]
train() client id: f_00007-6-0 loss: 0.635464  [   32/  179]
train() client id: f_00007-6-1 loss: 0.847438  [   64/  179]
train() client id: f_00007-6-2 loss: 0.626974  [   96/  179]
train() client id: f_00007-6-3 loss: 0.793702  [  128/  179]
train() client id: f_00007-6-4 loss: 0.605078  [  160/  179]
train() client id: f_00007-7-0 loss: 0.692413  [   32/  179]
train() client id: f_00007-7-1 loss: 0.677343  [   64/  179]
train() client id: f_00007-7-2 loss: 0.719965  [   96/  179]
train() client id: f_00007-7-3 loss: 0.657184  [  128/  179]
train() client id: f_00007-7-4 loss: 0.736767  [  160/  179]
train() client id: f_00007-8-0 loss: 0.633702  [   32/  179]
train() client id: f_00007-8-1 loss: 0.770188  [   64/  179]
train() client id: f_00007-8-2 loss: 0.683614  [   96/  179]
train() client id: f_00007-8-3 loss: 0.608233  [  128/  179]
train() client id: f_00007-8-4 loss: 0.846626  [  160/  179]
train() client id: f_00007-9-0 loss: 0.664834  [   32/  179]
train() client id: f_00007-9-1 loss: 0.667242  [   64/  179]
train() client id: f_00007-9-2 loss: 0.608213  [   96/  179]
train() client id: f_00007-9-3 loss: 0.773536  [  128/  179]
train() client id: f_00007-9-4 loss: 0.815386  [  160/  179]
train() client id: f_00007-10-0 loss: 0.617970  [   32/  179]
train() client id: f_00007-10-1 loss: 0.876526  [   64/  179]
train() client id: f_00007-10-2 loss: 0.572471  [   96/  179]
train() client id: f_00007-10-3 loss: 0.710316  [  128/  179]
train() client id: f_00007-10-4 loss: 0.676214  [  160/  179]
train() client id: f_00007-11-0 loss: 0.659748  [   32/  179]
train() client id: f_00007-11-1 loss: 0.673864  [   64/  179]
train() client id: f_00007-11-2 loss: 0.724454  [   96/  179]
train() client id: f_00007-11-3 loss: 0.575904  [  128/  179]
train() client id: f_00007-11-4 loss: 0.792990  [  160/  179]
train() client id: f_00008-0-0 loss: 0.738733  [   32/  130]
train() client id: f_00008-0-1 loss: 0.607992  [   64/  130]
train() client id: f_00008-0-2 loss: 0.721082  [   96/  130]
train() client id: f_00008-0-3 loss: 0.827698  [  128/  130]
train() client id: f_00008-1-0 loss: 0.777555  [   32/  130]
train() client id: f_00008-1-1 loss: 0.731277  [   64/  130]
train() client id: f_00008-1-2 loss: 0.692137  [   96/  130]
train() client id: f_00008-1-3 loss: 0.678815  [  128/  130]
train() client id: f_00008-2-0 loss: 0.718382  [   32/  130]
train() client id: f_00008-2-1 loss: 0.791493  [   64/  130]
train() client id: f_00008-2-2 loss: 0.706180  [   96/  130]
train() client id: f_00008-2-3 loss: 0.689791  [  128/  130]
train() client id: f_00008-3-0 loss: 0.619098  [   32/  130]
train() client id: f_00008-3-1 loss: 0.670766  [   64/  130]
train() client id: f_00008-3-2 loss: 0.793108  [   96/  130]
train() client id: f_00008-3-3 loss: 0.778082  [  128/  130]
train() client id: f_00008-4-0 loss: 0.703113  [   32/  130]
train() client id: f_00008-4-1 loss: 0.768965  [   64/  130]
train() client id: f_00008-4-2 loss: 0.793776  [   96/  130]
train() client id: f_00008-4-3 loss: 0.629431  [  128/  130]
train() client id: f_00008-5-0 loss: 0.672424  [   32/  130]
train() client id: f_00008-5-1 loss: 0.677309  [   64/  130]
train() client id: f_00008-5-2 loss: 0.703979  [   96/  130]
train() client id: f_00008-5-3 loss: 0.755581  [  128/  130]
train() client id: f_00008-6-0 loss: 0.870771  [   32/  130]
train() client id: f_00008-6-1 loss: 0.696845  [   64/  130]
train() client id: f_00008-6-2 loss: 0.615682  [   96/  130]
train() client id: f_00008-6-3 loss: 0.704550  [  128/  130]
train() client id: f_00008-7-0 loss: 0.555499  [   32/  130]
train() client id: f_00008-7-1 loss: 0.706563  [   64/  130]
train() client id: f_00008-7-2 loss: 0.762634  [   96/  130]
train() client id: f_00008-7-3 loss: 0.849868  [  128/  130]
train() client id: f_00008-8-0 loss: 0.654617  [   32/  130]
train() client id: f_00008-8-1 loss: 0.724857  [   64/  130]
train() client id: f_00008-8-2 loss: 0.623287  [   96/  130]
train() client id: f_00008-8-3 loss: 0.874916  [  128/  130]
train() client id: f_00008-9-0 loss: 0.771026  [   32/  130]
train() client id: f_00008-9-1 loss: 0.639988  [   64/  130]
train() client id: f_00008-9-2 loss: 0.689894  [   96/  130]
train() client id: f_00008-9-3 loss: 0.743912  [  128/  130]
train() client id: f_00008-10-0 loss: 0.828259  [   32/  130]
train() client id: f_00008-10-1 loss: 0.589141  [   64/  130]
train() client id: f_00008-10-2 loss: 0.746860  [   96/  130]
train() client id: f_00008-10-3 loss: 0.683189  [  128/  130]
train() client id: f_00008-11-0 loss: 0.678772  [   32/  130]
train() client id: f_00008-11-1 loss: 0.814810  [   64/  130]
train() client id: f_00008-11-2 loss: 0.718410  [   96/  130]
train() client id: f_00008-11-3 loss: 0.668179  [  128/  130]
train() client id: f_00009-0-0 loss: 1.195734  [   32/  118]
train() client id: f_00009-0-1 loss: 1.195428  [   64/  118]
train() client id: f_00009-0-2 loss: 1.223849  [   96/  118]
train() client id: f_00009-1-0 loss: 1.186797  [   32/  118]
train() client id: f_00009-1-1 loss: 1.158141  [   64/  118]
train() client id: f_00009-1-2 loss: 1.140571  [   96/  118]
train() client id: f_00009-2-0 loss: 1.104982  [   32/  118]
train() client id: f_00009-2-1 loss: 1.186720  [   64/  118]
train() client id: f_00009-2-2 loss: 1.080933  [   96/  118]
train() client id: f_00009-3-0 loss: 1.151455  [   32/  118]
train() client id: f_00009-3-1 loss: 1.090667  [   64/  118]
train() client id: f_00009-3-2 loss: 1.018998  [   96/  118]
train() client id: f_00009-4-0 loss: 1.107465  [   32/  118]
train() client id: f_00009-4-1 loss: 1.020864  [   64/  118]
train() client id: f_00009-4-2 loss: 1.073580  [   96/  118]
train() client id: f_00009-5-0 loss: 1.052757  [   32/  118]
train() client id: f_00009-5-1 loss: 1.047210  [   64/  118]
train() client id: f_00009-5-2 loss: 1.052310  [   96/  118]
train() client id: f_00009-6-0 loss: 1.025401  [   32/  118]
train() client id: f_00009-6-1 loss: 1.059478  [   64/  118]
train() client id: f_00009-6-2 loss: 0.997815  [   96/  118]
train() client id: f_00009-7-0 loss: 0.990162  [   32/  118]
train() client id: f_00009-7-1 loss: 1.057435  [   64/  118]
train() client id: f_00009-7-2 loss: 1.004709  [   96/  118]
train() client id: f_00009-8-0 loss: 1.042684  [   32/  118]
train() client id: f_00009-8-1 loss: 0.973444  [   64/  118]
train() client id: f_00009-8-2 loss: 0.975171  [   96/  118]
train() client id: f_00009-9-0 loss: 0.994804  [   32/  118]
train() client id: f_00009-9-1 loss: 0.937514  [   64/  118]
train() client id: f_00009-9-2 loss: 1.054417  [   96/  118]
train() client id: f_00009-10-0 loss: 1.043310  [   32/  118]
train() client id: f_00009-10-1 loss: 0.960374  [   64/  118]
train() client id: f_00009-10-2 loss: 0.960663  [   96/  118]
train() client id: f_00009-11-0 loss: 0.968146  [   32/  118]
train() client id: f_00009-11-1 loss: 0.964985  [   64/  118]
train() client id: f_00009-11-2 loss: 0.963325  [   96/  118]
At round 6 accuracy: 0.6312997347480106
At round 6 training accuracy: 0.5754527162977867
At round 6 training loss: 0.8660762093023462
update_location
xs = [ -3.9056584    4.20031788  50.00902392  18.81129433  -4.02070377
   3.95640986 -12.44319194   3.67514815  34.66397685   2.93912145]
ys = [ 42.5879595   25.55583871   1.32061395 -12.45517586   9.35018685
 -12.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [108.76115328 103.2992912  111.81523373 102.51339522 100.51662575
 100.81739979 100.8053747  100.15467783 107.28588567 100.1231756 ]
dists_bs = [216.34745711 233.33733364 284.24213267 269.63356758 238.06255551
 258.96706121 240.78071123 253.04039114 262.18981874 246.78497559]
uav_gains = [8.10606699e-11 9.22049228e-11 7.56379401e-11 9.39823243e-11
 9.87196326e-11 9.79849732e-11 9.80141981e-11 9.96139759e-11
 8.38762801e-11 9.96923511e-11]
bs_gains = [3.19793078e-11 2.58784487e-11 1.48924591e-11 1.72634266e-11
 2.44657779e-11 1.93290009e-11 2.37002728e-11 2.06235063e-11
 1.86710946e-11 2.21208434e-11]
Round 7
-------------------------------
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.06798939 21.0326556   9.91312762  3.54368464 24.2561337  11.70105956
  4.40648092 14.22483624 10.44629039  9.49400071]
obj_prev = 119.08625877018483
eta_min = 5.793901722590025e-10	eta_max = 0.9200929959618319
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 27.695540866338916	eta = 0.909090909090909
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 47.81923698001299	eta = 0.5265195769323551
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 38.22376664223015	eta = 0.6586939654483899
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.504271658621605	eta = 0.6897210457833629
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41961812780936	eta = 0.6913242290346582
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41939846351146	eta = 0.6913283987699586
eta = 0.6913283987699586
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.03046057 0.06406391 0.02997707 0.01039528 0.07397568 0.03529559
 0.01305453 0.04327336 0.03142758 0.02852657]
ene_total = [3.09168549 6.04664955 3.05847947 1.40079217 6.88392674 3.69460946
 1.61888686 4.14166343 3.37804549 3.10465979]
ti_comp = [0.28943977 0.27173208 0.28857988 0.29120492 0.27061872 0.2656134
 0.2916892  0.29187392 0.26482942 0.26854656]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.10852063e-05 2.22555556e-04 2.02169483e-05 8.27924223e-07
 3.45486619e-04 3.89530298e-05 1.63426776e-06 5.94499024e-05
 2.76617870e-05 2.01182160e-05]
ene_total = [0.54893267 0.71368309 0.55604324 0.53249539 0.73325245 0.74945808
 0.52851731 0.53180388 0.75506382 0.72338265]
optimize_network iter = 0 obj = 6.3726325729552675
eta = 0.6913283987699586
freqs = [5.26198833e+07 1.17880646e+08 5.19389521e+07 1.78487289e+07
 1.36678791e+08 6.64416515e+07 2.23774684e+07 7.41302190e+07
 5.93355081e+07 5.31128927e+07]
eta_min = 0.6839075543866936	eta_max = 0.6913283987699287
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 0.058240867111970324	eta = 0.9090909090909091
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 21.478076657689165	eta = 0.0024651296143925966
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.271073836819586	eta = 0.023313307551114187
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977536965308997	eta = 0.02409107213089359
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977247993541873	eta = 0.0240913888966546
eta = 0.0240913888966546
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12822560e-04 2.24635427e-03 2.04058838e-04 8.35661507e-06
 3.48715330e-03 3.93170615e-04 1.64954064e-05 6.00054856e-04
 2.79202975e-04 2.03062288e-04]
ene_total = [0.17868869 0.2792033  0.18072909 0.16862269 0.31492846 0.24640612
 0.16755818 0.18248854 0.24546632 0.23363341]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 1 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
eta_min = 0.6839075543866936	eta_max = 0.6839075543866819
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 0.05801261670554691	eta = 0.9090909090909091
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 21.4778591118164	eta = 0.0024554934542136467
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.270009752860956	eta = 0.023232826375798606
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196960837665713	eta = 0.024005317507445147
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196932244064364	eta = 0.024005629942423928
eta = 0.024005629942423928
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12924512e-04 2.23906549e-03 2.04121753e-04 8.36352164e-06
 3.47496787e-03 3.91345733e-04 1.65106003e-05 6.00629173e-04
 2.77855860e-04 2.02257286e-04]
ene_total = [0.17867568 0.27898621 0.18071488 0.16860805 0.31457886 0.24633625
 0.16754386 0.18248768 0.24540916 0.23359161]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 2 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
Done!
At round 7 energy consumption: 6.224581758881931
At round 7 eta: 0.6839075543866936
At round 7 local rounds: 12.440921153911754
At round 7 global rounds: 81.57354686993413
At round 7 a_n: 25.442236080499836
gradient difference: 0.4028840661048889
train() client id: f_00000-0-0 loss: 1.445973  [   32/  126]
train() client id: f_00000-0-1 loss: 1.428666  [   64/  126]
train() client id: f_00000-0-2 loss: 1.140157  [   96/  126]
train() client id: f_00000-1-0 loss: 1.365996  [   32/  126]
train() client id: f_00000-1-1 loss: 1.238233  [   64/  126]
train() client id: f_00000-1-2 loss: 1.091843  [   96/  126]
train() client id: f_00000-2-0 loss: 1.195959  [   32/  126]
train() client id: f_00000-2-1 loss: 1.212275  [   64/  126]
train() client id: f_00000-2-2 loss: 1.037694  [   96/  126]
train() client id: f_00000-3-0 loss: 1.119823  [   32/  126]
train() client id: f_00000-3-1 loss: 1.055915  [   64/  126]
train() client id: f_00000-3-2 loss: 1.130907  [   96/  126]
train() client id: f_00000-4-0 loss: 1.012272  [   32/  126]
train() client id: f_00000-4-1 loss: 1.044077  [   64/  126]
train() client id: f_00000-4-2 loss: 1.028658  [   96/  126]
train() client id: f_00000-5-0 loss: 1.023220  [   32/  126]
train() client id: f_00000-5-1 loss: 0.966426  [   64/  126]
train() client id: f_00000-5-2 loss: 0.963164  [   96/  126]
train() client id: f_00000-6-0 loss: 0.987461  [   32/  126]
train() client id: f_00000-6-1 loss: 0.959266  [   64/  126]
train() client id: f_00000-6-2 loss: 0.986636  [   96/  126]
train() client id: f_00000-7-0 loss: 0.971970  [   32/  126]
train() client id: f_00000-7-1 loss: 0.961003  [   64/  126]
train() client id: f_00000-7-2 loss: 0.922641  [   96/  126]
train() client id: f_00000-8-0 loss: 0.974002  [   32/  126]
train() client id: f_00000-8-1 loss: 0.865700  [   64/  126]
train() client id: f_00000-8-2 loss: 0.922467  [   96/  126]
train() client id: f_00000-9-0 loss: 0.930397  [   32/  126]
train() client id: f_00000-9-1 loss: 0.906301  [   64/  126]
train() client id: f_00000-9-2 loss: 0.912491  [   96/  126]
train() client id: f_00000-10-0 loss: 0.966111  [   32/  126]
train() client id: f_00000-10-1 loss: 0.904992  [   64/  126]
train() client id: f_00000-10-2 loss: 0.916226  [   96/  126]
train() client id: f_00000-11-0 loss: 0.900902  [   32/  126]
train() client id: f_00000-11-1 loss: 0.933899  [   64/  126]
train() client id: f_00000-11-2 loss: 0.948308  [   96/  126]
train() client id: f_00001-0-0 loss: 0.683926  [   32/  265]
train() client id: f_00001-0-1 loss: 0.667885  [   64/  265]
train() client id: f_00001-0-2 loss: 0.587632  [   96/  265]
train() client id: f_00001-0-3 loss: 0.543258  [  128/  265]
train() client id: f_00001-0-4 loss: 0.564154  [  160/  265]
train() client id: f_00001-0-5 loss: 0.668446  [  192/  265]
train() client id: f_00001-0-6 loss: 0.607061  [  224/  265]
train() client id: f_00001-0-7 loss: 0.543156  [  256/  265]
train() client id: f_00001-1-0 loss: 0.529598  [   32/  265]
train() client id: f_00001-1-1 loss: 0.570258  [   64/  265]
train() client id: f_00001-1-2 loss: 0.571810  [   96/  265]
train() client id: f_00001-1-3 loss: 0.524595  [  128/  265]
train() client id: f_00001-1-4 loss: 0.601901  [  160/  265]
train() client id: f_00001-1-5 loss: 0.614452  [  192/  265]
train() client id: f_00001-1-6 loss: 0.648200  [  224/  265]
train() client id: f_00001-1-7 loss: 0.634225  [  256/  265]
train() client id: f_00001-2-0 loss: 0.554280  [   32/  265]
train() client id: f_00001-2-1 loss: 0.565147  [   64/  265]
train() client id: f_00001-2-2 loss: 0.550377  [   96/  265]
train() client id: f_00001-2-3 loss: 0.533188  [  128/  265]
train() client id: f_00001-2-4 loss: 0.566636  [  160/  265]
train() client id: f_00001-2-5 loss: 0.633353  [  192/  265]
train() client id: f_00001-2-6 loss: 0.605075  [  224/  265]
train() client id: f_00001-2-7 loss: 0.631848  [  256/  265]
train() client id: f_00001-3-0 loss: 0.540836  [   32/  265]
train() client id: f_00001-3-1 loss: 0.688214  [   64/  265]
train() client id: f_00001-3-2 loss: 0.567217  [   96/  265]
train() client id: f_00001-3-3 loss: 0.497263  [  128/  265]
train() client id: f_00001-3-4 loss: 0.652043  [  160/  265]
train() client id: f_00001-3-5 loss: 0.540414  [  192/  265]
train() client id: f_00001-3-6 loss: 0.535967  [  224/  265]
train() client id: f_00001-3-7 loss: 0.540866  [  256/  265]
train() client id: f_00001-4-0 loss: 0.549145  [   32/  265]
train() client id: f_00001-4-1 loss: 0.537507  [   64/  265]
train() client id: f_00001-4-2 loss: 0.593890  [   96/  265]
train() client id: f_00001-4-3 loss: 0.591262  [  128/  265]
train() client id: f_00001-4-4 loss: 0.486430  [  160/  265]
train() client id: f_00001-4-5 loss: 0.634112  [  192/  265]
train() client id: f_00001-4-6 loss: 0.552762  [  224/  265]
train() client id: f_00001-4-7 loss: 0.563618  [  256/  265]
train() client id: f_00001-5-0 loss: 0.488035  [   32/  265]
train() client id: f_00001-5-1 loss: 0.586819  [   64/  265]
train() client id: f_00001-5-2 loss: 0.660255  [   96/  265]
train() client id: f_00001-5-3 loss: 0.470172  [  128/  265]
train() client id: f_00001-5-4 loss: 0.693739  [  160/  265]
train() client id: f_00001-5-5 loss: 0.483003  [  192/  265]
train() client id: f_00001-5-6 loss: 0.588738  [  224/  265]
train() client id: f_00001-5-7 loss: 0.526215  [  256/  265]
train() client id: f_00001-6-0 loss: 0.535630  [   32/  265]
train() client id: f_00001-6-1 loss: 0.592419  [   64/  265]
train() client id: f_00001-6-2 loss: 0.467224  [   96/  265]
train() client id: f_00001-6-3 loss: 0.548343  [  128/  265]
train() client id: f_00001-6-4 loss: 0.466506  [  160/  265]
train() client id: f_00001-6-5 loss: 0.648388  [  192/  265]
train() client id: f_00001-6-6 loss: 0.604192  [  224/  265]
train() client id: f_00001-6-7 loss: 0.545900  [  256/  265]
train() client id: f_00001-7-0 loss: 0.657939  [   32/  265]
train() client id: f_00001-7-1 loss: 0.531953  [   64/  265]
train() client id: f_00001-7-2 loss: 0.522722  [   96/  265]
train() client id: f_00001-7-3 loss: 0.530878  [  128/  265]
train() client id: f_00001-7-4 loss: 0.586626  [  160/  265]
train() client id: f_00001-7-5 loss: 0.515305  [  192/  265]
train() client id: f_00001-7-6 loss: 0.564909  [  224/  265]
train() client id: f_00001-7-7 loss: 0.467777  [  256/  265]
train() client id: f_00001-8-0 loss: 0.473752  [   32/  265]
train() client id: f_00001-8-1 loss: 0.545381  [   64/  265]
train() client id: f_00001-8-2 loss: 0.541721  [   96/  265]
train() client id: f_00001-8-3 loss: 0.598633  [  128/  265]
train() client id: f_00001-8-4 loss: 0.588630  [  160/  265]
train() client id: f_00001-8-5 loss: 0.477899  [  192/  265]
train() client id: f_00001-8-6 loss: 0.505381  [  224/  265]
train() client id: f_00001-8-7 loss: 0.665807  [  256/  265]
train() client id: f_00001-9-0 loss: 0.674214  [   32/  265]
train() client id: f_00001-9-1 loss: 0.577626  [   64/  265]
train() client id: f_00001-9-2 loss: 0.474461  [   96/  265]
train() client id: f_00001-9-3 loss: 0.481318  [  128/  265]
train() client id: f_00001-9-4 loss: 0.474547  [  160/  265]
train() client id: f_00001-9-5 loss: 0.585010  [  192/  265]
train() client id: f_00001-9-6 loss: 0.707009  [  224/  265]
train() client id: f_00001-9-7 loss: 0.459076  [  256/  265]
train() client id: f_00001-10-0 loss: 0.506102  [   32/  265]
train() client id: f_00001-10-1 loss: 0.556250  [   64/  265]
train() client id: f_00001-10-2 loss: 0.606405  [   96/  265]
train() client id: f_00001-10-3 loss: 0.470465  [  128/  265]
train() client id: f_00001-10-4 loss: 0.589441  [  160/  265]
train() client id: f_00001-10-5 loss: 0.488168  [  192/  265]
train() client id: f_00001-10-6 loss: 0.657532  [  224/  265]
train() client id: f_00001-10-7 loss: 0.568444  [  256/  265]
train() client id: f_00001-11-0 loss: 0.588111  [   32/  265]
train() client id: f_00001-11-1 loss: 0.561331  [   64/  265]
train() client id: f_00001-11-2 loss: 0.541230  [   96/  265]
train() client id: f_00001-11-3 loss: 0.591660  [  128/  265]
train() client id: f_00001-11-4 loss: 0.524531  [  160/  265]
train() client id: f_00001-11-5 loss: 0.587152  [  192/  265]
train() client id: f_00001-11-6 loss: 0.544524  [  224/  265]
train() client id: f_00001-11-7 loss: 0.507842  [  256/  265]
train() client id: f_00002-0-0 loss: 1.224227  [   32/  124]
train() client id: f_00002-0-1 loss: 1.176600  [   64/  124]
train() client id: f_00002-0-2 loss: 1.169679  [   96/  124]
train() client id: f_00002-1-0 loss: 1.144485  [   32/  124]
train() client id: f_00002-1-1 loss: 1.158699  [   64/  124]
train() client id: f_00002-1-2 loss: 1.102559  [   96/  124]
train() client id: f_00002-2-0 loss: 1.151055  [   32/  124]
train() client id: f_00002-2-1 loss: 1.128411  [   64/  124]
train() client id: f_00002-2-2 loss: 1.094973  [   96/  124]
train() client id: f_00002-3-0 loss: 1.109835  [   32/  124]
train() client id: f_00002-3-1 loss: 1.058354  [   64/  124]
train() client id: f_00002-3-2 loss: 1.105473  [   96/  124]
train() client id: f_00002-4-0 loss: 1.062927  [   32/  124]
train() client id: f_00002-4-1 loss: 1.115013  [   64/  124]
train() client id: f_00002-4-2 loss: 1.039678  [   96/  124]
train() client id: f_00002-5-0 loss: 1.019063  [   32/  124]
train() client id: f_00002-5-1 loss: 1.082051  [   64/  124]
train() client id: f_00002-5-2 loss: 0.997266  [   96/  124]
train() client id: f_00002-6-0 loss: 1.051449  [   32/  124]
train() client id: f_00002-6-1 loss: 1.047120  [   64/  124]
train() client id: f_00002-6-2 loss: 0.950017  [   96/  124]
train() client id: f_00002-7-0 loss: 1.013687  [   32/  124]
train() client id: f_00002-7-1 loss: 1.101044  [   64/  124]
train() client id: f_00002-7-2 loss: 0.951318  [   96/  124]
train() client id: f_00002-8-0 loss: 1.036901  [   32/  124]
train() client id: f_00002-8-1 loss: 0.997886  [   64/  124]
train() client id: f_00002-8-2 loss: 0.965202  [   96/  124]
train() client id: f_00002-9-0 loss: 0.975902  [   32/  124]
train() client id: f_00002-9-1 loss: 0.884038  [   64/  124]
train() client id: f_00002-9-2 loss: 1.038087  [   96/  124]
train() client id: f_00002-10-0 loss: 0.989572  [   32/  124]
train() client id: f_00002-10-1 loss: 1.000543  [   64/  124]
train() client id: f_00002-10-2 loss: 0.982502  [   96/  124]
train() client id: f_00002-11-0 loss: 0.980297  [   32/  124]
train() client id: f_00002-11-1 loss: 0.949151  [   64/  124]
train() client id: f_00002-11-2 loss: 1.003513  [   96/  124]
train() client id: f_00003-0-0 loss: 0.943714  [   32/   43]
train() client id: f_00003-1-0 loss: 0.891341  [   32/   43]
train() client id: f_00003-2-0 loss: 0.975021  [   32/   43]
train() client id: f_00003-3-0 loss: 0.875347  [   32/   43]
train() client id: f_00003-4-0 loss: 0.940240  [   32/   43]
train() client id: f_00003-5-0 loss: 0.982069  [   32/   43]
train() client id: f_00003-6-0 loss: 0.855878  [   32/   43]
train() client id: f_00003-7-0 loss: 0.972401  [   32/   43]
train() client id: f_00003-8-0 loss: 0.991142  [   32/   43]
train() client id: f_00003-9-0 loss: 1.025609  [   32/   43]
train() client id: f_00003-10-0 loss: 1.006504  [   32/   43]
train() client id: f_00003-11-0 loss: 0.960473  [   32/   43]
train() client id: f_00004-0-0 loss: 1.030997  [   32/  306]
train() client id: f_00004-0-1 loss: 0.967286  [   64/  306]
train() client id: f_00004-0-2 loss: 0.969138  [   96/  306]
train() client id: f_00004-0-3 loss: 1.062287  [  128/  306]
train() client id: f_00004-0-4 loss: 0.896010  [  160/  306]
train() client id: f_00004-0-5 loss: 0.972113  [  192/  306]
train() client id: f_00004-0-6 loss: 0.996618  [  224/  306]
train() client id: f_00004-0-7 loss: 1.006486  [  256/  306]
train() client id: f_00004-0-8 loss: 0.907471  [  288/  306]
train() client id: f_00004-1-0 loss: 0.844361  [   32/  306]
train() client id: f_00004-1-1 loss: 0.761846  [   64/  306]
train() client id: f_00004-1-2 loss: 0.904721  [   96/  306]
train() client id: f_00004-1-3 loss: 1.130643  [  128/  306]
train() client id: f_00004-1-4 loss: 0.996929  [  160/  306]
train() client id: f_00004-1-5 loss: 1.001763  [  192/  306]
train() client id: f_00004-1-6 loss: 1.026962  [  224/  306]
train() client id: f_00004-1-7 loss: 1.062630  [  256/  306]
train() client id: f_00004-1-8 loss: 1.033528  [  288/  306]
train() client id: f_00004-2-0 loss: 0.844783  [   32/  306]
train() client id: f_00004-2-1 loss: 0.974854  [   64/  306]
train() client id: f_00004-2-2 loss: 0.926498  [   96/  306]
train() client id: f_00004-2-3 loss: 0.911319  [  128/  306]
train() client id: f_00004-2-4 loss: 1.022532  [  160/  306]
train() client id: f_00004-2-5 loss: 0.997121  [  192/  306]
train() client id: f_00004-2-6 loss: 1.122503  [  224/  306]
train() client id: f_00004-2-7 loss: 0.911717  [  256/  306]
train() client id: f_00004-2-8 loss: 1.142368  [  288/  306]
train() client id: f_00004-3-0 loss: 0.957525  [   32/  306]
train() client id: f_00004-3-1 loss: 0.948997  [   64/  306]
train() client id: f_00004-3-2 loss: 1.053896  [   96/  306]
train() client id: f_00004-3-3 loss: 0.897379  [  128/  306]
train() client id: f_00004-3-4 loss: 0.824854  [  160/  306]
train() client id: f_00004-3-5 loss: 0.909154  [  192/  306]
train() client id: f_00004-3-6 loss: 0.965160  [  224/  306]
train() client id: f_00004-3-7 loss: 1.143929  [  256/  306]
train() client id: f_00004-3-8 loss: 0.924002  [  288/  306]
train() client id: f_00004-4-0 loss: 0.982068  [   32/  306]
train() client id: f_00004-4-1 loss: 0.929419  [   64/  306]
train() client id: f_00004-4-2 loss: 0.928445  [   96/  306]
train() client id: f_00004-4-3 loss: 0.876307  [  128/  306]
train() client id: f_00004-4-4 loss: 0.933779  [  160/  306]
train() client id: f_00004-4-5 loss: 1.048452  [  192/  306]
train() client id: f_00004-4-6 loss: 0.938817  [  224/  306]
train() client id: f_00004-4-7 loss: 1.021282  [  256/  306]
train() client id: f_00004-4-8 loss: 1.047234  [  288/  306]
train() client id: f_00004-5-0 loss: 0.799308  [   32/  306]
train() client id: f_00004-5-1 loss: 0.955154  [   64/  306]
train() client id: f_00004-5-2 loss: 1.022316  [   96/  306]
train() client id: f_00004-5-3 loss: 0.891120  [  128/  306]
train() client id: f_00004-5-4 loss: 0.865738  [  160/  306]
train() client id: f_00004-5-5 loss: 0.919743  [  192/  306]
train() client id: f_00004-5-6 loss: 1.045145  [  224/  306]
train() client id: f_00004-5-7 loss: 1.094620  [  256/  306]
train() client id: f_00004-5-8 loss: 1.076106  [  288/  306]
train() client id: f_00004-6-0 loss: 1.006700  [   32/  306]
train() client id: f_00004-6-1 loss: 0.963845  [   64/  306]
train() client id: f_00004-6-2 loss: 0.973980  [   96/  306]
train() client id: f_00004-6-3 loss: 0.946429  [  128/  306]
train() client id: f_00004-6-4 loss: 0.942559  [  160/  306]
train() client id: f_00004-6-5 loss: 0.991255  [  192/  306]
train() client id: f_00004-6-6 loss: 0.991253  [  224/  306]
train() client id: f_00004-6-7 loss: 1.038756  [  256/  306]
train() client id: f_00004-6-8 loss: 0.880962  [  288/  306]
train() client id: f_00004-7-0 loss: 0.985257  [   32/  306]
train() client id: f_00004-7-1 loss: 0.947626  [   64/  306]
train() client id: f_00004-7-2 loss: 1.002086  [   96/  306]
train() client id: f_00004-7-3 loss: 0.975277  [  128/  306]
train() client id: f_00004-7-4 loss: 0.894111  [  160/  306]
train() client id: f_00004-7-5 loss: 0.958506  [  192/  306]
train() client id: f_00004-7-6 loss: 0.985791  [  224/  306]
train() client id: f_00004-7-7 loss: 0.962498  [  256/  306]
train() client id: f_00004-7-8 loss: 0.967579  [  288/  306]
train() client id: f_00004-8-0 loss: 0.930069  [   32/  306]
train() client id: f_00004-8-1 loss: 0.929898  [   64/  306]
train() client id: f_00004-8-2 loss: 0.986125  [   96/  306]
train() client id: f_00004-8-3 loss: 0.956919  [  128/  306]
train() client id: f_00004-8-4 loss: 1.014982  [  160/  306]
train() client id: f_00004-8-5 loss: 1.000463  [  192/  306]
train() client id: f_00004-8-6 loss: 0.986719  [  224/  306]
train() client id: f_00004-8-7 loss: 0.936449  [  256/  306]
train() client id: f_00004-8-8 loss: 1.034234  [  288/  306]
train() client id: f_00004-9-0 loss: 0.798261  [   32/  306]
train() client id: f_00004-9-1 loss: 0.923290  [   64/  306]
train() client id: f_00004-9-2 loss: 0.893856  [   96/  306]
train() client id: f_00004-9-3 loss: 1.004408  [  128/  306]
train() client id: f_00004-9-4 loss: 1.099233  [  160/  306]
train() client id: f_00004-9-5 loss: 1.009923  [  192/  306]
train() client id: f_00004-9-6 loss: 0.969453  [  224/  306]
train() client id: f_00004-9-7 loss: 1.008002  [  256/  306]
train() client id: f_00004-9-8 loss: 0.902650  [  288/  306]
train() client id: f_00004-10-0 loss: 0.967502  [   32/  306]
train() client id: f_00004-10-1 loss: 0.919723  [   64/  306]
train() client id: f_00004-10-2 loss: 0.999681  [   96/  306]
train() client id: f_00004-10-3 loss: 0.938076  [  128/  306]
train() client id: f_00004-10-4 loss: 0.982635  [  160/  306]
train() client id: f_00004-10-5 loss: 0.919644  [  192/  306]
train() client id: f_00004-10-6 loss: 0.967619  [  224/  306]
train() client id: f_00004-10-7 loss: 1.130536  [  256/  306]
train() client id: f_00004-10-8 loss: 0.885205  [  288/  306]
train() client id: f_00004-11-0 loss: 1.075920  [   32/  306]
train() client id: f_00004-11-1 loss: 0.909982  [   64/  306]
train() client id: f_00004-11-2 loss: 0.922895  [   96/  306]
train() client id: f_00004-11-3 loss: 0.972928  [  128/  306]
train() client id: f_00004-11-4 loss: 0.967496  [  160/  306]
train() client id: f_00004-11-5 loss: 0.989243  [  192/  306]
train() client id: f_00004-11-6 loss: 1.048082  [  224/  306]
train() client id: f_00004-11-7 loss: 0.971748  [  256/  306]
train() client id: f_00004-11-8 loss: 0.861953  [  288/  306]
train() client id: f_00005-0-0 loss: 0.978798  [   32/  146]
train() client id: f_00005-0-1 loss: 0.785975  [   64/  146]
train() client id: f_00005-0-2 loss: 0.834621  [   96/  146]
train() client id: f_00005-0-3 loss: 0.785221  [  128/  146]
train() client id: f_00005-1-0 loss: 0.739261  [   32/  146]
train() client id: f_00005-1-1 loss: 0.899305  [   64/  146]
train() client id: f_00005-1-2 loss: 0.829651  [   96/  146]
train() client id: f_00005-1-3 loss: 0.959940  [  128/  146]
train() client id: f_00005-2-0 loss: 0.830291  [   32/  146]
train() client id: f_00005-2-1 loss: 0.756968  [   64/  146]
train() client id: f_00005-2-2 loss: 0.801336  [   96/  146]
train() client id: f_00005-2-3 loss: 0.932354  [  128/  146]
train() client id: f_00005-3-0 loss: 0.794762  [   32/  146]
train() client id: f_00005-3-1 loss: 0.878733  [   64/  146]
train() client id: f_00005-3-2 loss: 0.919122  [   96/  146]
train() client id: f_00005-3-3 loss: 0.822346  [  128/  146]
train() client id: f_00005-4-0 loss: 0.723930  [   32/  146]
train() client id: f_00005-4-1 loss: 0.905663  [   64/  146]
train() client id: f_00005-4-2 loss: 0.849965  [   96/  146]
train() client id: f_00005-4-3 loss: 0.873270  [  128/  146]
train() client id: f_00005-5-0 loss: 0.864180  [   32/  146]
train() client id: f_00005-5-1 loss: 0.698877  [   64/  146]
train() client id: f_00005-5-2 loss: 0.906936  [   96/  146]
train() client id: f_00005-5-3 loss: 0.828297  [  128/  146]
train() client id: f_00005-6-0 loss: 0.729062  [   32/  146]
train() client id: f_00005-6-1 loss: 0.862258  [   64/  146]
train() client id: f_00005-6-2 loss: 0.819880  [   96/  146]
train() client id: f_00005-6-3 loss: 0.931787  [  128/  146]
train() client id: f_00005-7-0 loss: 0.873108  [   32/  146]
train() client id: f_00005-7-1 loss: 0.686474  [   64/  146]
train() client id: f_00005-7-2 loss: 0.843597  [   96/  146]
train() client id: f_00005-7-3 loss: 0.982927  [  128/  146]
train() client id: f_00005-8-0 loss: 0.749619  [   32/  146]
train() client id: f_00005-8-1 loss: 0.885764  [   64/  146]
train() client id: f_00005-8-2 loss: 0.763284  [   96/  146]
train() client id: f_00005-8-3 loss: 0.982125  [  128/  146]
train() client id: f_00005-9-0 loss: 0.729016  [   32/  146]
train() client id: f_00005-9-1 loss: 0.763947  [   64/  146]
train() client id: f_00005-9-2 loss: 0.783699  [   96/  146]
train() client id: f_00005-9-3 loss: 0.921577  [  128/  146]
train() client id: f_00005-10-0 loss: 0.823458  [   32/  146]
train() client id: f_00005-10-1 loss: 0.797429  [   64/  146]
train() client id: f_00005-10-2 loss: 0.691846  [   96/  146]
train() client id: f_00005-10-3 loss: 0.866755  [  128/  146]
train() client id: f_00005-11-0 loss: 0.962554  [   32/  146]
train() client id: f_00005-11-1 loss: 0.941056  [   64/  146]
train() client id: f_00005-11-2 loss: 0.798980  [   96/  146]
train() client id: f_00005-11-3 loss: 0.743909  [  128/  146]
train() client id: f_00006-0-0 loss: 0.844294  [   32/   54]
train() client id: f_00006-1-0 loss: 0.828881  [   32/   54]
train() client id: f_00006-2-0 loss: 0.832708  [   32/   54]
train() client id: f_00006-3-0 loss: 0.865973  [   32/   54]
train() client id: f_00006-4-0 loss: 0.814944  [   32/   54]
train() client id: f_00006-5-0 loss: 0.855633  [   32/   54]
train() client id: f_00006-6-0 loss: 0.821732  [   32/   54]
train() client id: f_00006-7-0 loss: 0.810173  [   32/   54]
train() client id: f_00006-8-0 loss: 0.783349  [   32/   54]
train() client id: f_00006-9-0 loss: 0.781982  [   32/   54]
train() client id: f_00006-10-0 loss: 0.782286  [   32/   54]
train() client id: f_00006-11-0 loss: 0.776753  [   32/   54]
train() client id: f_00007-0-0 loss: 0.624573  [   32/  179]
train() client id: f_00007-0-1 loss: 0.648791  [   64/  179]
train() client id: f_00007-0-2 loss: 0.700884  [   96/  179]
train() client id: f_00007-0-3 loss: 0.691161  [  128/  179]
train() client id: f_00007-0-4 loss: 0.613397  [  160/  179]
train() client id: f_00007-1-0 loss: 0.641691  [   32/  179]
train() client id: f_00007-1-1 loss: 0.604298  [   64/  179]
train() client id: f_00007-1-2 loss: 0.628091  [   96/  179]
train() client id: f_00007-1-3 loss: 0.590156  [  128/  179]
train() client id: f_00007-1-4 loss: 0.629197  [  160/  179]
train() client id: f_00007-2-0 loss: 0.582970  [   32/  179]
train() client id: f_00007-2-1 loss: 0.723184  [   64/  179]
train() client id: f_00007-2-2 loss: 0.672671  [   96/  179]
train() client id: f_00007-2-3 loss: 0.578717  [  128/  179]
train() client id: f_00007-2-4 loss: 0.527808  [  160/  179]
train() client id: f_00007-3-0 loss: 0.562769  [   32/  179]
train() client id: f_00007-3-1 loss: 0.554397  [   64/  179]
train() client id: f_00007-3-2 loss: 0.576495  [   96/  179]
train() client id: f_00007-3-3 loss: 0.761964  [  128/  179]
train() client id: f_00007-3-4 loss: 0.492782  [  160/  179]
train() client id: f_00007-4-0 loss: 0.512457  [   32/  179]
train() client id: f_00007-4-1 loss: 0.574046  [   64/  179]
train() client id: f_00007-4-2 loss: 0.578851  [   96/  179]
train() client id: f_00007-4-3 loss: 0.634406  [  128/  179]
train() client id: f_00007-4-4 loss: 0.510871  [  160/  179]
train() client id: f_00007-5-0 loss: 0.652200  [   32/  179]
train() client id: f_00007-5-1 loss: 0.577540  [   64/  179]
train() client id: f_00007-5-2 loss: 0.627667  [   96/  179]
train() client id: f_00007-5-3 loss: 0.510533  [  128/  179]
train() client id: f_00007-5-4 loss: 0.534668  [  160/  179]
train() client id: f_00007-6-0 loss: 0.629444  [   32/  179]
train() client id: f_00007-6-1 loss: 0.540684  [   64/  179]
train() client id: f_00007-6-2 loss: 0.636016  [   96/  179]
train() client id: f_00007-6-3 loss: 0.537992  [  128/  179]
train() client id: f_00007-6-4 loss: 0.463886  [  160/  179]
train() client id: f_00007-7-0 loss: 0.536163  [   32/  179]
train() client id: f_00007-7-1 loss: 0.619618  [   64/  179]
train() client id: f_00007-7-2 loss: 0.432836  [   96/  179]
train() client id: f_00007-7-3 loss: 0.583726  [  128/  179]
train() client id: f_00007-7-4 loss: 0.604932  [  160/  179]
train() client id: f_00007-8-0 loss: 0.512015  [   32/  179]
train() client id: f_00007-8-1 loss: 0.551146  [   64/  179]
train() client id: f_00007-8-2 loss: 0.509415  [   96/  179]
train() client id: f_00007-8-3 loss: 0.594836  [  128/  179]
train() client id: f_00007-8-4 loss: 0.503348  [  160/  179]
train() client id: f_00007-9-0 loss: 0.518601  [   32/  179]
train() client id: f_00007-9-1 loss: 0.571484  [   64/  179]
train() client id: f_00007-9-2 loss: 0.611925  [   96/  179]
train() client id: f_00007-9-3 loss: 0.562853  [  128/  179]
train() client id: f_00007-9-4 loss: 0.514618  [  160/  179]
train() client id: f_00007-10-0 loss: 0.443190  [   32/  179]
train() client id: f_00007-10-1 loss: 0.528800  [   64/  179]
train() client id: f_00007-10-2 loss: 0.519276  [   96/  179]
train() client id: f_00007-10-3 loss: 0.654892  [  128/  179]
train() client id: f_00007-10-4 loss: 0.572092  [  160/  179]
train() client id: f_00007-11-0 loss: 0.444379  [   32/  179]
train() client id: f_00007-11-1 loss: 0.479378  [   64/  179]
train() client id: f_00007-11-2 loss: 0.571135  [   96/  179]
train() client id: f_00007-11-3 loss: 0.513267  [  128/  179]
train() client id: f_00007-11-4 loss: 0.600759  [  160/  179]
train() client id: f_00008-0-0 loss: 0.796478  [   32/  130]
train() client id: f_00008-0-1 loss: 0.960756  [   64/  130]
train() client id: f_00008-0-2 loss: 0.961451  [   96/  130]
train() client id: f_00008-0-3 loss: 0.767447  [  128/  130]
train() client id: f_00008-1-0 loss: 0.887145  [   32/  130]
train() client id: f_00008-1-1 loss: 0.892375  [   64/  130]
train() client id: f_00008-1-2 loss: 0.759577  [   96/  130]
train() client id: f_00008-1-3 loss: 0.918726  [  128/  130]
train() client id: f_00008-2-0 loss: 0.807336  [   32/  130]
train() client id: f_00008-2-1 loss: 0.889127  [   64/  130]
train() client id: f_00008-2-2 loss: 0.988183  [   96/  130]
train() client id: f_00008-2-3 loss: 0.816087  [  128/  130]
train() client id: f_00008-3-0 loss: 0.878368  [   32/  130]
train() client id: f_00008-3-1 loss: 0.824559  [   64/  130]
train() client id: f_00008-3-2 loss: 0.897436  [   96/  130]
train() client id: f_00008-3-3 loss: 0.894186  [  128/  130]
train() client id: f_00008-4-0 loss: 0.966861  [   32/  130]
train() client id: f_00008-4-1 loss: 0.789172  [   64/  130]
train() client id: f_00008-4-2 loss: 0.841092  [   96/  130]
train() client id: f_00008-4-3 loss: 0.899946  [  128/  130]
train() client id: f_00008-5-0 loss: 0.811399  [   32/  130]
train() client id: f_00008-5-1 loss: 0.938214  [   64/  130]
train() client id: f_00008-5-2 loss: 0.939648  [   96/  130]
train() client id: f_00008-5-3 loss: 0.800325  [  128/  130]
train() client id: f_00008-6-0 loss: 0.931845  [   32/  130]
train() client id: f_00008-6-1 loss: 0.816809  [   64/  130]
train() client id: f_00008-6-2 loss: 0.876919  [   96/  130]
train() client id: f_00008-6-3 loss: 0.880198  [  128/  130]
train() client id: f_00008-7-0 loss: 0.897568  [   32/  130]
train() client id: f_00008-7-1 loss: 0.838134  [   64/  130]
train() client id: f_00008-7-2 loss: 0.750834  [   96/  130]
train() client id: f_00008-7-3 loss: 1.014757  [  128/  130]
train() client id: f_00008-8-0 loss: 0.886615  [   32/  130]
train() client id: f_00008-8-1 loss: 0.880832  [   64/  130]
train() client id: f_00008-8-2 loss: 0.847575  [   96/  130]
train() client id: f_00008-8-3 loss: 0.851684  [  128/  130]
train() client id: f_00008-9-0 loss: 0.875820  [   32/  130]
train() client id: f_00008-9-1 loss: 0.918906  [   64/  130]
train() client id: f_00008-9-2 loss: 0.744243  [   96/  130]
train() client id: f_00008-9-3 loss: 0.935030  [  128/  130]
train() client id: f_00008-10-0 loss: 0.928503  [   32/  130]
train() client id: f_00008-10-1 loss: 0.814584  [   64/  130]
train() client id: f_00008-10-2 loss: 0.936379  [   96/  130]
train() client id: f_00008-10-3 loss: 0.820270  [  128/  130]
train() client id: f_00008-11-0 loss: 0.880422  [   32/  130]
train() client id: f_00008-11-1 loss: 0.853725  [   64/  130]
train() client id: f_00008-11-2 loss: 0.872890  [   96/  130]
train() client id: f_00008-11-3 loss: 0.862958  [  128/  130]
train() client id: f_00009-0-0 loss: 1.281105  [   32/  118]
train() client id: f_00009-0-1 loss: 1.220062  [   64/  118]
train() client id: f_00009-0-2 loss: 1.182580  [   96/  118]
train() client id: f_00009-1-0 loss: 1.230389  [   32/  118]
train() client id: f_00009-1-1 loss: 1.207760  [   64/  118]
train() client id: f_00009-1-2 loss: 1.147044  [   96/  118]
train() client id: f_00009-2-0 loss: 1.206456  [   32/  118]
train() client id: f_00009-2-1 loss: 1.151115  [   64/  118]
train() client id: f_00009-2-2 loss: 1.180976  [   96/  118]
train() client id: f_00009-3-0 loss: 1.088813  [   32/  118]
train() client id: f_00009-3-1 loss: 1.109963  [   64/  118]
train() client id: f_00009-3-2 loss: 1.145177  [   96/  118]
train() client id: f_00009-4-0 loss: 1.076727  [   32/  118]
train() client id: f_00009-4-1 loss: 1.109480  [   64/  118]
train() client id: f_00009-4-2 loss: 1.104386  [   96/  118]
train() client id: f_00009-5-0 loss: 1.071195  [   32/  118]
train() client id: f_00009-5-1 loss: 1.077423  [   64/  118]
train() client id: f_00009-5-2 loss: 1.063052  [   96/  118]
train() client id: f_00009-6-0 loss: 1.004325  [   32/  118]
train() client id: f_00009-6-1 loss: 1.024717  [   64/  118]
train() client id: f_00009-6-2 loss: 1.074296  [   96/  118]
train() client id: f_00009-7-0 loss: 0.978235  [   32/  118]
train() client id: f_00009-7-1 loss: 1.025626  [   64/  118]
train() client id: f_00009-7-2 loss: 1.061042  [   96/  118]
train() client id: f_00009-8-0 loss: 1.043049  [   32/  118]
train() client id: f_00009-8-1 loss: 1.017582  [   64/  118]
train() client id: f_00009-8-2 loss: 1.021810  [   96/  118]
train() client id: f_00009-9-0 loss: 1.063785  [   32/  118]
train() client id: f_00009-9-1 loss: 0.984031  [   64/  118]
train() client id: f_00009-9-2 loss: 1.053633  [   96/  118]
train() client id: f_00009-10-0 loss: 0.957647  [   32/  118]
train() client id: f_00009-10-1 loss: 1.033546  [   64/  118]
train() client id: f_00009-10-2 loss: 1.088877  [   96/  118]
train() client id: f_00009-11-0 loss: 1.015164  [   32/  118]
train() client id: f_00009-11-1 loss: 0.976442  [   64/  118]
train() client id: f_00009-11-2 loss: 1.009273  [   96/  118]
At round 7 accuracy: 0.6312997347480106
At round 7 training accuracy: 0.5767940979208585
At round 7 training loss: 0.851427448503115
update_location
xs = [ -3.9056584    4.20031788  55.00902392  18.81129433   0.97929623
   3.95640986 -17.44319194  -1.32485185  39.66397685  -2.06087855]
ys = [ 47.5879595   30.55583871   1.32061395 -17.45517586   9.35018685
  -7.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [110.81456609 104.64846845 114.13911132 103.24024389 100.44095288
 100.33588395 101.54385992 100.09599397 109.0041331  100.10124413]
dists_bs = [213.32393628 230.16704729 288.21637743 273.13332372 241.6786571
 255.37635032 237.43374407 249.53494358 266.20507287 243.20450851]
uav_gains = [7.73572375e-11 8.92616175e-11 7.18461074e-11 9.23368231e-11
 9.89056819e-11 9.91648192e-11 9.62418239e-11 9.97600464e-11
 8.06096645e-11 9.97469659e-11]
bs_gains = [3.32646713e-11 2.68889148e-11 1.43245785e-11 1.66511777e-11
 2.34545361e-11 2.00996360e-11 2.46476348e-11 2.14450097e-11
 1.78932145e-11 2.30448319e-11]
Round 8
-------------------------------
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.93572436 20.7513301   9.78311252  3.49713647 23.93609638 11.54337877
  4.34848004 14.03581921 10.31003617  9.36568175]
obj_prev = 117.50679575560986
eta_min = 4.500137844098843e-10	eta_max = 0.9201793140385103
af = 24.843282687249765	bf = 1.924183044657018	zeta = 27.327610955974745	eta = 0.909090909090909
af = 24.843282687249765	bf = 1.924183044657018	zeta = 47.21200862959305	eta = 0.5262068572883742
af = 24.843282687249765	bf = 1.924183044657018	zeta = 37.72761010314322	eta = 0.6584907609925703
af = 24.843282687249765	bf = 1.924183044657018	zeta = 36.02779691629381	eta = 0.689558752231509
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.944047891502834	eta = 0.6911654124832922
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.94383021535565	eta = 0.6911695981870181
eta = 0.6911695981870181
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.03047953 0.06410378 0.02999573 0.01040174 0.07402172 0.03531755
 0.01306266 0.04330029 0.03144714 0.02854432]
ene_total = [3.05537056 5.9604125  3.0232152  1.38370094 6.79982988 3.63829942
 1.59895401 4.0866222  3.34115314 3.05627237]
ti_comp = [0.29321835 0.27683249 0.29228351 0.29535599 0.2741193  0.27083978
 0.29583662 0.2962475  0.2682047  0.2737568 ]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.05836579e-05 2.14830890e-04 1.97446596e-05 8.06319238e-07
 3.37348113e-04 3.75341750e-05 1.59173447e-06 5.78153199e-05
 2.70203220e-05 1.93958759e-05]
ene_total = [0.54608481 0.69707832 0.55371718 0.52684511 0.72952358 0.73184161
 0.52295022 0.52419719 0.75268396 0.70631616]
optimize_network iter = 0 obj = 6.291238143797367
eta = 0.6911695981870181
freqs = [5.19741187e+07 1.15780803e+08 5.13127289e+07 1.76088264e+07
 1.35017343e+08 6.52000814e+07 2.20774830e+07 7.30812690e+07
 5.86252507e+07 5.21344532e+07]
eta_min = 0.6875884070281042	eta_max = 0.6911695981870177
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 0.05581615321591653	eta = 0.909090909090909
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 21.219211967217113	eta = 0.002391321484860486
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.2358034144344674	eta = 0.022695178449689362
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1654164527348314	eta = 0.02343288627225956
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1653901507160183	eta = 0.02343317090097433
eta = 0.02343317090097433
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09174714e-04 2.18314889e-03 2.00648668e-04 8.19395643e-06
 3.42819024e-03 3.81428819e-04 1.61754827e-05 5.87529343e-04
 2.74585215e-04 1.97104266e-04]
ene_total = [0.17760768 0.27172476 0.17982108 0.16680246 0.3112301  0.24039767
 0.16575821 0.17957311 0.24447922 0.22799587]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 1 obj = 6.219849079279121
eta = 0.6875884070281042
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
eta_min = 0.6875884070281044	eta_max = 0.6875884070280862
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 0.055709725023689025	eta = 0.909090909090909
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 21.21911053031574	eta = 0.0023867732106222467
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.23530448131582	eta = 0.022656960154787298
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650443530624273	eta = 0.023392224965439173
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650181860209075	eta = 0.023392507690695646
eta = 0.023392507690695646
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09211545e-04 2.17991631e-03 2.00666013e-04 8.19706147e-06
 3.42210961e-03 3.80614259e-04 1.61823437e-05 5.87801193e-04
 2.73917274e-04 1.96746859e-04]
ene_total = [0.17760125 0.27162923 0.17981404 0.16679559 0.31105873 0.24036644
 0.16575149 0.17957271 0.24445164 0.22797706]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 2 obj = 6.219849079279126
eta = 0.6875884070281044
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
Done!
At round 8 energy consumption: 6.219849079279121
At round 8 eta: 0.6875884070281044
At round 8 local rounds: 12.265156719696218
At round 8 global rounds: 81.4381945256065
At round 8 a_n: 25.09969023353052
gradient difference: 0.38079962134361267
train() client id: f_00000-0-0 loss: 1.421872  [   32/  126]
train() client id: f_00000-0-1 loss: 1.590958  [   64/  126]
train() client id: f_00000-0-2 loss: 1.301586  [   96/  126]
train() client id: f_00000-1-0 loss: 1.338612  [   32/  126]
train() client id: f_00000-1-1 loss: 1.411388  [   64/  126]
train() client id: f_00000-1-2 loss: 1.406045  [   96/  126]
train() client id: f_00000-2-0 loss: 1.281999  [   32/  126]
train() client id: f_00000-2-1 loss: 1.102702  [   64/  126]
train() client id: f_00000-2-2 loss: 1.132093  [   96/  126]
train() client id: f_00000-3-0 loss: 1.216276  [   32/  126]
train() client id: f_00000-3-1 loss: 1.102476  [   64/  126]
train() client id: f_00000-3-2 loss: 1.095766  [   96/  126]
train() client id: f_00000-4-0 loss: 1.036284  [   32/  126]
train() client id: f_00000-4-1 loss: 1.119825  [   64/  126]
train() client id: f_00000-4-2 loss: 1.018307  [   96/  126]
train() client id: f_00000-5-0 loss: 1.063349  [   32/  126]
train() client id: f_00000-5-1 loss: 0.950998  [   64/  126]
train() client id: f_00000-5-2 loss: 1.030339  [   96/  126]
train() client id: f_00000-6-0 loss: 0.939977  [   32/  126]
train() client id: f_00000-6-1 loss: 0.996506  [   64/  126]
train() client id: f_00000-6-2 loss: 0.954602  [   96/  126]
train() client id: f_00000-7-0 loss: 0.905762  [   32/  126]
train() client id: f_00000-7-1 loss: 0.957207  [   64/  126]
train() client id: f_00000-7-2 loss: 0.968437  [   96/  126]
train() client id: f_00000-8-0 loss: 0.876261  [   32/  126]
train() client id: f_00000-8-1 loss: 0.926714  [   64/  126]
train() client id: f_00000-8-2 loss: 0.980823  [   96/  126]
train() client id: f_00000-9-0 loss: 0.933212  [   32/  126]
train() client id: f_00000-9-1 loss: 0.888137  [   64/  126]
train() client id: f_00000-9-2 loss: 0.848080  [   96/  126]
train() client id: f_00000-10-0 loss: 0.908826  [   32/  126]
train() client id: f_00000-10-1 loss: 0.866637  [   64/  126]
train() client id: f_00000-10-2 loss: 0.961049  [   96/  126]
train() client id: f_00000-11-0 loss: 0.889124  [   32/  126]
train() client id: f_00000-11-1 loss: 0.925443  [   64/  126]
train() client id: f_00000-11-2 loss: 0.905514  [   96/  126]
train() client id: f_00001-0-0 loss: 0.541796  [   32/  265]
train() client id: f_00001-0-1 loss: 0.561380  [   64/  265]
train() client id: f_00001-0-2 loss: 0.585571  [   96/  265]
train() client id: f_00001-0-3 loss: 0.569018  [  128/  265]
train() client id: f_00001-0-4 loss: 0.659717  [  160/  265]
train() client id: f_00001-0-5 loss: 0.615636  [  192/  265]
train() client id: f_00001-0-6 loss: 0.551791  [  224/  265]
train() client id: f_00001-0-7 loss: 0.565543  [  256/  265]
train() client id: f_00001-1-0 loss: 0.569541  [   32/  265]
train() client id: f_00001-1-1 loss: 0.580559  [   64/  265]
train() client id: f_00001-1-2 loss: 0.546260  [   96/  265]
train() client id: f_00001-1-3 loss: 0.475226  [  128/  265]
train() client id: f_00001-1-4 loss: 0.619539  [  160/  265]
train() client id: f_00001-1-5 loss: 0.607970  [  192/  265]
train() client id: f_00001-1-6 loss: 0.535200  [  224/  265]
train() client id: f_00001-1-7 loss: 0.576188  [  256/  265]
train() client id: f_00001-2-0 loss: 0.524280  [   32/  265]
train() client id: f_00001-2-1 loss: 0.533121  [   64/  265]
train() client id: f_00001-2-2 loss: 0.523639  [   96/  265]
train() client id: f_00001-2-3 loss: 0.484839  [  128/  265]
train() client id: f_00001-2-4 loss: 0.496808  [  160/  265]
train() client id: f_00001-2-5 loss: 0.577755  [  192/  265]
train() client id: f_00001-2-6 loss: 0.533434  [  224/  265]
train() client id: f_00001-2-7 loss: 0.706340  [  256/  265]
train() client id: f_00001-3-0 loss: 0.478960  [   32/  265]
train() client id: f_00001-3-1 loss: 0.562871  [   64/  265]
train() client id: f_00001-3-2 loss: 0.729289  [   96/  265]
train() client id: f_00001-3-3 loss: 0.564423  [  128/  265]
train() client id: f_00001-3-4 loss: 0.533675  [  160/  265]
train() client id: f_00001-3-5 loss: 0.508563  [  192/  265]
train() client id: f_00001-3-6 loss: 0.465374  [  224/  265]
train() client id: f_00001-3-7 loss: 0.525314  [  256/  265]
train() client id: f_00001-4-0 loss: 0.529954  [   32/  265]
train() client id: f_00001-4-1 loss: 0.513602  [   64/  265]
train() client id: f_00001-4-2 loss: 0.603139  [   96/  265]
train() client id: f_00001-4-3 loss: 0.466158  [  128/  265]
train() client id: f_00001-4-4 loss: 0.605130  [  160/  265]
train() client id: f_00001-4-5 loss: 0.513502  [  192/  265]
train() client id: f_00001-4-6 loss: 0.637091  [  224/  265]
train() client id: f_00001-4-7 loss: 0.442229  [  256/  265]
train() client id: f_00001-5-0 loss: 0.477493  [   32/  265]
train() client id: f_00001-5-1 loss: 0.551765  [   64/  265]
train() client id: f_00001-5-2 loss: 0.574365  [   96/  265]
train() client id: f_00001-5-3 loss: 0.565843  [  128/  265]
train() client id: f_00001-5-4 loss: 0.470670  [  160/  265]
train() client id: f_00001-5-5 loss: 0.479536  [  192/  265]
train() client id: f_00001-5-6 loss: 0.605120  [  224/  265]
train() client id: f_00001-5-7 loss: 0.509351  [  256/  265]
train() client id: f_00001-6-0 loss: 0.564431  [   32/  265]
train() client id: f_00001-6-1 loss: 0.511244  [   64/  265]
train() client id: f_00001-6-2 loss: 0.512180  [   96/  265]
train() client id: f_00001-6-3 loss: 0.472293  [  128/  265]
train() client id: f_00001-6-4 loss: 0.609338  [  160/  265]
train() client id: f_00001-6-5 loss: 0.568907  [  192/  265]
train() client id: f_00001-6-6 loss: 0.509041  [  224/  265]
train() client id: f_00001-6-7 loss: 0.511144  [  256/  265]
train() client id: f_00001-7-0 loss: 0.565957  [   32/  265]
train() client id: f_00001-7-1 loss: 0.521432  [   64/  265]
train() client id: f_00001-7-2 loss: 0.516314  [   96/  265]
train() client id: f_00001-7-3 loss: 0.563914  [  128/  265]
train() client id: f_00001-7-4 loss: 0.541403  [  160/  265]
train() client id: f_00001-7-5 loss: 0.531391  [  192/  265]
train() client id: f_00001-7-6 loss: 0.488793  [  224/  265]
train() client id: f_00001-7-7 loss: 0.457319  [  256/  265]
train() client id: f_00001-8-0 loss: 0.551856  [   32/  265]
train() client id: f_00001-8-1 loss: 0.561057  [   64/  265]
train() client id: f_00001-8-2 loss: 0.503933  [   96/  265]
train() client id: f_00001-8-3 loss: 0.623341  [  128/  265]
train() client id: f_00001-8-4 loss: 0.440706  [  160/  265]
train() client id: f_00001-8-5 loss: 0.585851  [  192/  265]
train() client id: f_00001-8-6 loss: 0.522130  [  224/  265]
train() client id: f_00001-8-7 loss: 0.448603  [  256/  265]
train() client id: f_00001-9-0 loss: 0.545555  [   32/  265]
train() client id: f_00001-9-1 loss: 0.437036  [   64/  265]
train() client id: f_00001-9-2 loss: 0.533811  [   96/  265]
train() client id: f_00001-9-3 loss: 0.549387  [  128/  265]
train() client id: f_00001-9-4 loss: 0.457970  [  160/  265]
train() client id: f_00001-9-5 loss: 0.443273  [  192/  265]
train() client id: f_00001-9-6 loss: 0.777401  [  224/  265]
train() client id: f_00001-9-7 loss: 0.434746  [  256/  265]
train() client id: f_00001-10-0 loss: 0.519573  [   32/  265]
train() client id: f_00001-10-1 loss: 0.489858  [   64/  265]
train() client id: f_00001-10-2 loss: 0.517893  [   96/  265]
train() client id: f_00001-10-3 loss: 0.629334  [  128/  265]
train() client id: f_00001-10-4 loss: 0.487388  [  160/  265]
train() client id: f_00001-10-5 loss: 0.479255  [  192/  265]
train() client id: f_00001-10-6 loss: 0.610425  [  224/  265]
train() client id: f_00001-10-7 loss: 0.450106  [  256/  265]
train() client id: f_00001-11-0 loss: 0.515343  [   32/  265]
train() client id: f_00001-11-1 loss: 0.552349  [   64/  265]
train() client id: f_00001-11-2 loss: 0.492985  [   96/  265]
train() client id: f_00001-11-3 loss: 0.562345  [  128/  265]
train() client id: f_00001-11-4 loss: 0.486920  [  160/  265]
train() client id: f_00001-11-5 loss: 0.570774  [  192/  265]
train() client id: f_00001-11-6 loss: 0.564034  [  224/  265]
train() client id: f_00001-11-7 loss: 0.430409  [  256/  265]
train() client id: f_00002-0-0 loss: 1.184195  [   32/  124]
train() client id: f_00002-0-1 loss: 1.254043  [   64/  124]
train() client id: f_00002-0-2 loss: 1.158449  [   96/  124]
train() client id: f_00002-1-0 loss: 1.136441  [   32/  124]
train() client id: f_00002-1-1 loss: 1.229396  [   64/  124]
train() client id: f_00002-1-2 loss: 1.166689  [   96/  124]
train() client id: f_00002-2-0 loss: 1.083751  [   32/  124]
train() client id: f_00002-2-1 loss: 1.191136  [   64/  124]
train() client id: f_00002-2-2 loss: 1.043077  [   96/  124]
train() client id: f_00002-3-0 loss: 1.074945  [   32/  124]
train() client id: f_00002-3-1 loss: 1.042290  [   64/  124]
train() client id: f_00002-3-2 loss: 1.138116  [   96/  124]
train() client id: f_00002-4-0 loss: 1.036297  [   32/  124]
train() client id: f_00002-4-1 loss: 1.079657  [   64/  124]
train() client id: f_00002-4-2 loss: 1.051210  [   96/  124]
train() client id: f_00002-5-0 loss: 1.036443  [   32/  124]
train() client id: f_00002-5-1 loss: 1.034504  [   64/  124]
train() client id: f_00002-5-2 loss: 1.036602  [   96/  124]
train() client id: f_00002-6-0 loss: 1.027909  [   32/  124]
train() client id: f_00002-6-1 loss: 1.034090  [   64/  124]
train() client id: f_00002-6-2 loss: 0.981050  [   96/  124]
train() client id: f_00002-7-0 loss: 1.012888  [   32/  124]
train() client id: f_00002-7-1 loss: 0.971704  [   64/  124]
train() client id: f_00002-7-2 loss: 1.045771  [   96/  124]
train() client id: f_00002-8-0 loss: 1.094902  [   32/  124]
train() client id: f_00002-8-1 loss: 1.028107  [   64/  124]
train() client id: f_00002-8-2 loss: 0.883891  [   96/  124]
train() client id: f_00002-9-0 loss: 0.917343  [   32/  124]
train() client id: f_00002-9-1 loss: 1.001813  [   64/  124]
train() client id: f_00002-9-2 loss: 0.952442  [   96/  124]
train() client id: f_00002-10-0 loss: 0.995098  [   32/  124]
train() client id: f_00002-10-1 loss: 0.930503  [   64/  124]
train() client id: f_00002-10-2 loss: 1.015060  [   96/  124]
train() client id: f_00002-11-0 loss: 0.965955  [   32/  124]
train() client id: f_00002-11-1 loss: 1.015071  [   64/  124]
train() client id: f_00002-11-2 loss: 0.985935  [   96/  124]
train() client id: f_00003-0-0 loss: 0.775290  [   32/   43]
train() client id: f_00003-1-0 loss: 0.889551  [   32/   43]
train() client id: f_00003-2-0 loss: 0.889229  [   32/   43]
train() client id: f_00003-3-0 loss: 0.844797  [   32/   43]
train() client id: f_00003-4-0 loss: 0.780056  [   32/   43]
train() client id: f_00003-5-0 loss: 0.848420  [   32/   43]
train() client id: f_00003-6-0 loss: 0.854788  [   32/   43]
train() client id: f_00003-7-0 loss: 0.714249  [   32/   43]
train() client id: f_00003-8-0 loss: 0.864070  [   32/   43]
train() client id: f_00003-9-0 loss: 0.763830  [   32/   43]
train() client id: f_00003-10-0 loss: 0.731177  [   32/   43]
train() client id: f_00003-11-0 loss: 0.797916  [   32/   43]
train() client id: f_00004-0-0 loss: 0.983537  [   32/  306]
train() client id: f_00004-0-1 loss: 1.164016  [   64/  306]
train() client id: f_00004-0-2 loss: 0.986922  [   96/  306]
train() client id: f_00004-0-3 loss: 0.983405  [  128/  306]
train() client id: f_00004-0-4 loss: 1.027032  [  160/  306]
train() client id: f_00004-0-5 loss: 1.088167  [  192/  306]
train() client id: f_00004-0-6 loss: 1.108187  [  224/  306]
train() client id: f_00004-0-7 loss: 1.030619  [  256/  306]
train() client id: f_00004-0-8 loss: 1.113636  [  288/  306]
train() client id: f_00004-1-0 loss: 0.950208  [   32/  306]
train() client id: f_00004-1-1 loss: 1.096935  [   64/  306]
train() client id: f_00004-1-2 loss: 1.087808  [   96/  306]
train() client id: f_00004-1-3 loss: 1.001618  [  128/  306]
train() client id: f_00004-1-4 loss: 0.938487  [  160/  306]
train() client id: f_00004-1-5 loss: 1.211452  [  192/  306]
train() client id: f_00004-1-6 loss: 1.128626  [  224/  306]
train() client id: f_00004-1-7 loss: 0.927026  [  256/  306]
train() client id: f_00004-1-8 loss: 1.095095  [  288/  306]
train() client id: f_00004-2-0 loss: 1.054394  [   32/  306]
train() client id: f_00004-2-1 loss: 1.050068  [   64/  306]
train() client id: f_00004-2-2 loss: 1.121058  [   96/  306]
train() client id: f_00004-2-3 loss: 1.058536  [  128/  306]
train() client id: f_00004-2-4 loss: 1.041150  [  160/  306]
train() client id: f_00004-2-5 loss: 0.931231  [  192/  306]
train() client id: f_00004-2-6 loss: 0.933631  [  224/  306]
train() client id: f_00004-2-7 loss: 1.088255  [  256/  306]
train() client id: f_00004-2-8 loss: 1.177547  [  288/  306]
train() client id: f_00004-3-0 loss: 1.087689  [   32/  306]
train() client id: f_00004-3-1 loss: 1.049280  [   64/  306]
train() client id: f_00004-3-2 loss: 1.027301  [   96/  306]
train() client id: f_00004-3-3 loss: 1.007160  [  128/  306]
train() client id: f_00004-3-4 loss: 1.063076  [  160/  306]
train() client id: f_00004-3-5 loss: 1.067579  [  192/  306]
train() client id: f_00004-3-6 loss: 0.972109  [  224/  306]
train() client id: f_00004-3-7 loss: 0.983578  [  256/  306]
train() client id: f_00004-3-8 loss: 1.098223  [  288/  306]
train() client id: f_00004-4-0 loss: 1.145149  [   32/  306]
train() client id: f_00004-4-1 loss: 1.083754  [   64/  306]
train() client id: f_00004-4-2 loss: 1.183820  [   96/  306]
train() client id: f_00004-4-3 loss: 0.963853  [  128/  306]
train() client id: f_00004-4-4 loss: 1.077272  [  160/  306]
train() client id: f_00004-4-5 loss: 1.039732  [  192/  306]
train() client id: f_00004-4-6 loss: 0.967751  [  224/  306]
train() client id: f_00004-4-7 loss: 0.948792  [  256/  306]
train() client id: f_00004-4-8 loss: 0.937018  [  288/  306]
train() client id: f_00004-5-0 loss: 1.050992  [   32/  306]
train() client id: f_00004-5-1 loss: 1.043426  [   64/  306]
train() client id: f_00004-5-2 loss: 1.077317  [   96/  306]
train() client id: f_00004-5-3 loss: 0.900417  [  128/  306]
train() client id: f_00004-5-4 loss: 0.980124  [  160/  306]
train() client id: f_00004-5-5 loss: 1.174609  [  192/  306]
train() client id: f_00004-5-6 loss: 0.948978  [  224/  306]
train() client id: f_00004-5-7 loss: 1.107332  [  256/  306]
train() client id: f_00004-5-8 loss: 1.096668  [  288/  306]
train() client id: f_00004-6-0 loss: 1.020543  [   32/  306]
train() client id: f_00004-6-1 loss: 1.065858  [   64/  306]
train() client id: f_00004-6-2 loss: 1.062495  [   96/  306]
train() client id: f_00004-6-3 loss: 1.122955  [  128/  306]
train() client id: f_00004-6-4 loss: 0.874218  [  160/  306]
train() client id: f_00004-6-5 loss: 1.110680  [  192/  306]
train() client id: f_00004-6-6 loss: 0.993852  [  224/  306]
train() client id: f_00004-6-7 loss: 0.975791  [  256/  306]
train() client id: f_00004-6-8 loss: 1.090119  [  288/  306]
train() client id: f_00004-7-0 loss: 1.135447  [   32/  306]
train() client id: f_00004-7-1 loss: 0.882500  [   64/  306]
train() client id: f_00004-7-2 loss: 0.937541  [   96/  306]
train() client id: f_00004-7-3 loss: 1.078001  [  128/  306]
train() client id: f_00004-7-4 loss: 1.034457  [  160/  306]
train() client id: f_00004-7-5 loss: 0.973086  [  192/  306]
train() client id: f_00004-7-6 loss: 1.139767  [  224/  306]
train() client id: f_00004-7-7 loss: 1.074396  [  256/  306]
train() client id: f_00004-7-8 loss: 0.955305  [  288/  306]
train() client id: f_00004-8-0 loss: 1.093062  [   32/  306]
train() client id: f_00004-8-1 loss: 0.986308  [   64/  306]
train() client id: f_00004-8-2 loss: 1.060395  [   96/  306]
train() client id: f_00004-8-3 loss: 1.035507  [  128/  306]
train() client id: f_00004-8-4 loss: 0.937490  [  160/  306]
train() client id: f_00004-8-5 loss: 1.067918  [  192/  306]
train() client id: f_00004-8-6 loss: 1.054745  [  224/  306]
train() client id: f_00004-8-7 loss: 1.121796  [  256/  306]
train() client id: f_00004-8-8 loss: 0.884296  [  288/  306]
train() client id: f_00004-9-0 loss: 1.107772  [   32/  306]
train() client id: f_00004-9-1 loss: 0.907521  [   64/  306]
train() client id: f_00004-9-2 loss: 1.047814  [   96/  306]
train() client id: f_00004-9-3 loss: 1.122005  [  128/  306]
train() client id: f_00004-9-4 loss: 0.964129  [  160/  306]
train() client id: f_00004-9-5 loss: 1.130064  [  192/  306]
train() client id: f_00004-9-6 loss: 1.030101  [  224/  306]
train() client id: f_00004-9-7 loss: 0.983826  [  256/  306]
train() client id: f_00004-9-8 loss: 0.969391  [  288/  306]
train() client id: f_00004-10-0 loss: 1.081887  [   32/  306]
train() client id: f_00004-10-1 loss: 1.052318  [   64/  306]
train() client id: f_00004-10-2 loss: 0.973514  [   96/  306]
train() client id: f_00004-10-3 loss: 1.098892  [  128/  306]
train() client id: f_00004-10-4 loss: 1.040003  [  160/  306]
train() client id: f_00004-10-5 loss: 1.058349  [  192/  306]
train() client id: f_00004-10-6 loss: 0.989588  [  224/  306]
train() client id: f_00004-10-7 loss: 0.934359  [  256/  306]
train() client id: f_00004-10-8 loss: 0.995421  [  288/  306]
train() client id: f_00004-11-0 loss: 1.151099  [   32/  306]
train() client id: f_00004-11-1 loss: 1.041981  [   64/  306]
train() client id: f_00004-11-2 loss: 0.968501  [   96/  306]
train() client id: f_00004-11-3 loss: 0.932929  [  128/  306]
train() client id: f_00004-11-4 loss: 1.106115  [  160/  306]
train() client id: f_00004-11-5 loss: 0.991635  [  192/  306]
train() client id: f_00004-11-6 loss: 0.992173  [  224/  306]
train() client id: f_00004-11-7 loss: 0.975981  [  256/  306]
train() client id: f_00004-11-8 loss: 1.139084  [  288/  306]
train() client id: f_00005-0-0 loss: 0.802768  [   32/  146]
train() client id: f_00005-0-1 loss: 0.705440  [   64/  146]
train() client id: f_00005-0-2 loss: 0.698299  [   96/  146]
train() client id: f_00005-0-3 loss: 1.010371  [  128/  146]
train() client id: f_00005-1-0 loss: 0.747202  [   32/  146]
train() client id: f_00005-1-1 loss: 0.926120  [   64/  146]
train() client id: f_00005-1-2 loss: 0.776773  [   96/  146]
train() client id: f_00005-1-3 loss: 0.914561  [  128/  146]
train() client id: f_00005-2-0 loss: 0.770436  [   32/  146]
train() client id: f_00005-2-1 loss: 0.939351  [   64/  146]
train() client id: f_00005-2-2 loss: 0.781601  [   96/  146]
train() client id: f_00005-2-3 loss: 0.741151  [  128/  146]
train() client id: f_00005-3-0 loss: 0.934261  [   32/  146]
train() client id: f_00005-3-1 loss: 0.703035  [   64/  146]
train() client id: f_00005-3-2 loss: 0.788204  [   96/  146]
train() client id: f_00005-3-3 loss: 0.770488  [  128/  146]
train() client id: f_00005-4-0 loss: 0.848936  [   32/  146]
train() client id: f_00005-4-1 loss: 0.779982  [   64/  146]
train() client id: f_00005-4-2 loss: 0.869496  [   96/  146]
train() client id: f_00005-4-3 loss: 0.712010  [  128/  146]
train() client id: f_00005-5-0 loss: 0.816995  [   32/  146]
train() client id: f_00005-5-1 loss: 0.753836  [   64/  146]
train() client id: f_00005-5-2 loss: 0.763031  [   96/  146]
train() client id: f_00005-5-3 loss: 0.908567  [  128/  146]
train() client id: f_00005-6-0 loss: 0.812489  [   32/  146]
train() client id: f_00005-6-1 loss: 0.831773  [   64/  146]
train() client id: f_00005-6-2 loss: 0.865405  [   96/  146]
train() client id: f_00005-6-3 loss: 0.698466  [  128/  146]
train() client id: f_00005-7-0 loss: 0.938452  [   32/  146]
train() client id: f_00005-7-1 loss: 0.908780  [   64/  146]
train() client id: f_00005-7-2 loss: 0.656336  [   96/  146]
train() client id: f_00005-7-3 loss: 0.805075  [  128/  146]
train() client id: f_00005-8-0 loss: 0.877673  [   32/  146]
train() client id: f_00005-8-1 loss: 0.607357  [   64/  146]
train() client id: f_00005-8-2 loss: 0.672669  [   96/  146]
train() client id: f_00005-8-3 loss: 0.930011  [  128/  146]
train() client id: f_00005-9-0 loss: 0.964798  [   32/  146]
train() client id: f_00005-9-1 loss: 0.655783  [   64/  146]
train() client id: f_00005-9-2 loss: 0.851850  [   96/  146]
train() client id: f_00005-9-3 loss: 0.882825  [  128/  146]
train() client id: f_00005-10-0 loss: 0.726497  [   32/  146]
train() client id: f_00005-10-1 loss: 0.744284  [   64/  146]
train() client id: f_00005-10-2 loss: 0.907060  [   96/  146]
train() client id: f_00005-10-3 loss: 0.909432  [  128/  146]
train() client id: f_00005-11-0 loss: 0.663094  [   32/  146]
train() client id: f_00005-11-1 loss: 0.650918  [   64/  146]
train() client id: f_00005-11-2 loss: 0.943293  [   96/  146]
train() client id: f_00005-11-3 loss: 0.889089  [  128/  146]
train() client id: f_00006-0-0 loss: 0.805753  [   32/   54]
train() client id: f_00006-1-0 loss: 0.796809  [   32/   54]
train() client id: f_00006-2-0 loss: 0.771418  [   32/   54]
train() client id: f_00006-3-0 loss: 0.833985  [   32/   54]
train() client id: f_00006-4-0 loss: 0.797232  [   32/   54]
train() client id: f_00006-5-0 loss: 0.750666  [   32/   54]
train() client id: f_00006-6-0 loss: 0.738509  [   32/   54]
train() client id: f_00006-7-0 loss: 0.797991  [   32/   54]
train() client id: f_00006-8-0 loss: 0.839083  [   32/   54]
train() client id: f_00006-9-0 loss: 0.731946  [   32/   54]
train() client id: f_00006-10-0 loss: 0.756606  [   32/   54]
train() client id: f_00006-11-0 loss: 0.752644  [   32/   54]
train() client id: f_00007-0-0 loss: 0.697262  [   32/  179]
train() client id: f_00007-0-1 loss: 0.813596  [   64/  179]
train() client id: f_00007-0-2 loss: 0.673230  [   96/  179]
train() client id: f_00007-0-3 loss: 0.624351  [  128/  179]
train() client id: f_00007-0-4 loss: 0.630309  [  160/  179]
train() client id: f_00007-1-0 loss: 0.811678  [   32/  179]
train() client id: f_00007-1-1 loss: 0.641291  [   64/  179]
train() client id: f_00007-1-2 loss: 0.686319  [   96/  179]
train() client id: f_00007-1-3 loss: 0.643816  [  128/  179]
train() client id: f_00007-1-4 loss: 0.570811  [  160/  179]
train() client id: f_00007-2-0 loss: 0.533944  [   32/  179]
train() client id: f_00007-2-1 loss: 0.677310  [   64/  179]
train() client id: f_00007-2-2 loss: 0.699543  [   96/  179]
train() client id: f_00007-2-3 loss: 0.674002  [  128/  179]
train() client id: f_00007-2-4 loss: 0.657420  [  160/  179]
train() client id: f_00007-3-0 loss: 0.658411  [   32/  179]
train() client id: f_00007-3-1 loss: 0.720961  [   64/  179]
train() client id: f_00007-3-2 loss: 0.649489  [   96/  179]
train() client id: f_00007-3-3 loss: 0.559599  [  128/  179]
train() client id: f_00007-3-4 loss: 0.606138  [  160/  179]
train() client id: f_00007-4-0 loss: 0.535282  [   32/  179]
train() client id: f_00007-4-1 loss: 0.544259  [   64/  179]
train() client id: f_00007-4-2 loss: 0.737225  [   96/  179]
train() client id: f_00007-4-3 loss: 0.513128  [  128/  179]
train() client id: f_00007-4-4 loss: 0.764133  [  160/  179]
train() client id: f_00007-5-0 loss: 0.529342  [   32/  179]
train() client id: f_00007-5-1 loss: 0.662461  [   64/  179]
train() client id: f_00007-5-2 loss: 0.650016  [   96/  179]
train() client id: f_00007-5-3 loss: 0.515347  [  128/  179]
train() client id: f_00007-5-4 loss: 0.783305  [  160/  179]
train() client id: f_00007-6-0 loss: 0.544971  [   32/  179]
train() client id: f_00007-6-1 loss: 0.645959  [   64/  179]
train() client id: f_00007-6-2 loss: 0.593719  [   96/  179]
train() client id: f_00007-6-3 loss: 0.494135  [  128/  179]
train() client id: f_00007-6-4 loss: 0.681002  [  160/  179]
train() client id: f_00007-7-0 loss: 0.482885  [   32/  179]
train() client id: f_00007-7-1 loss: 0.566923  [   64/  179]
train() client id: f_00007-7-2 loss: 0.709817  [   96/  179]
train() client id: f_00007-7-3 loss: 0.675635  [  128/  179]
train() client id: f_00007-7-4 loss: 0.584535  [  160/  179]
train() client id: f_00007-8-0 loss: 0.579064  [   32/  179]
train() client id: f_00007-8-1 loss: 0.645726  [   64/  179]
train() client id: f_00007-8-2 loss: 0.660913  [   96/  179]
train() client id: f_00007-8-3 loss: 0.535892  [  128/  179]
train() client id: f_00007-8-4 loss: 0.570064  [  160/  179]
train() client id: f_00007-9-0 loss: 0.572292  [   32/  179]
train() client id: f_00007-9-1 loss: 0.647592  [   64/  179]
train() client id: f_00007-9-2 loss: 0.613239  [   96/  179]
train() client id: f_00007-9-3 loss: 0.574333  [  128/  179]
train() client id: f_00007-9-4 loss: 0.490922  [  160/  179]
train() client id: f_00007-10-0 loss: 0.568246  [   32/  179]
train() client id: f_00007-10-1 loss: 0.551720  [   64/  179]
train() client id: f_00007-10-2 loss: 0.645186  [   96/  179]
train() client id: f_00007-10-3 loss: 0.555914  [  128/  179]
train() client id: f_00007-10-4 loss: 0.577402  [  160/  179]
train() client id: f_00007-11-0 loss: 0.708921  [   32/  179]
train() client id: f_00007-11-1 loss: 0.513655  [   64/  179]
train() client id: f_00007-11-2 loss: 0.621701  [   96/  179]
train() client id: f_00007-11-3 loss: 0.652987  [  128/  179]
train() client id: f_00007-11-4 loss: 0.559440  [  160/  179]
train() client id: f_00008-0-0 loss: 0.803474  [   32/  130]
train() client id: f_00008-0-1 loss: 0.744747  [   64/  130]
train() client id: f_00008-0-2 loss: 0.775074  [   96/  130]
train() client id: f_00008-0-3 loss: 0.826431  [  128/  130]
train() client id: f_00008-1-0 loss: 0.807901  [   32/  130]
train() client id: f_00008-1-1 loss: 0.911822  [   64/  130]
train() client id: f_00008-1-2 loss: 0.742983  [   96/  130]
train() client id: f_00008-1-3 loss: 0.737959  [  128/  130]
train() client id: f_00008-2-0 loss: 0.741420  [   32/  130]
train() client id: f_00008-2-1 loss: 0.873159  [   64/  130]
train() client id: f_00008-2-2 loss: 0.820641  [   96/  130]
train() client id: f_00008-2-3 loss: 0.729582  [  128/  130]
train() client id: f_00008-3-0 loss: 0.645487  [   32/  130]
train() client id: f_00008-3-1 loss: 0.793950  [   64/  130]
train() client id: f_00008-3-2 loss: 0.889189  [   96/  130]
train() client id: f_00008-3-3 loss: 0.832476  [  128/  130]
train() client id: f_00008-4-0 loss: 0.778833  [   32/  130]
train() client id: f_00008-4-1 loss: 0.674166  [   64/  130]
train() client id: f_00008-4-2 loss: 0.901084  [   96/  130]
train() client id: f_00008-4-3 loss: 0.816620  [  128/  130]
train() client id: f_00008-5-0 loss: 0.815658  [   32/  130]
train() client id: f_00008-5-1 loss: 0.775430  [   64/  130]
train() client id: f_00008-5-2 loss: 0.723014  [   96/  130]
train() client id: f_00008-5-3 loss: 0.870989  [  128/  130]
train() client id: f_00008-6-0 loss: 0.850494  [   32/  130]
train() client id: f_00008-6-1 loss: 0.767444  [   64/  130]
train() client id: f_00008-6-2 loss: 0.793930  [   96/  130]
train() client id: f_00008-6-3 loss: 0.776557  [  128/  130]
train() client id: f_00008-7-0 loss: 0.922791  [   32/  130]
train() client id: f_00008-7-1 loss: 0.826407  [   64/  130]
train() client id: f_00008-7-2 loss: 0.777690  [   96/  130]
train() client id: f_00008-7-3 loss: 0.659643  [  128/  130]
train() client id: f_00008-8-0 loss: 0.718990  [   32/  130]
train() client id: f_00008-8-1 loss: 0.778170  [   64/  130]
train() client id: f_00008-8-2 loss: 0.781516  [   96/  130]
train() client id: f_00008-8-3 loss: 0.874617  [  128/  130]
train() client id: f_00008-9-0 loss: 0.908266  [   32/  130]
train() client id: f_00008-9-1 loss: 0.826837  [   64/  130]
train() client id: f_00008-9-2 loss: 0.669594  [   96/  130]
train() client id: f_00008-9-3 loss: 0.745103  [  128/  130]
train() client id: f_00008-10-0 loss: 0.854321  [   32/  130]
train() client id: f_00008-10-1 loss: 0.804362  [   64/  130]
train() client id: f_00008-10-2 loss: 0.758329  [   96/  130]
train() client id: f_00008-10-3 loss: 0.726083  [  128/  130]
train() client id: f_00008-11-0 loss: 0.817355  [   32/  130]
train() client id: f_00008-11-1 loss: 0.683697  [   64/  130]
train() client id: f_00008-11-2 loss: 0.863355  [   96/  130]
train() client id: f_00008-11-3 loss: 0.785586  [  128/  130]
train() client id: f_00009-0-0 loss: 1.171509  [   32/  118]
train() client id: f_00009-0-1 loss: 1.244535  [   64/  118]
train() client id: f_00009-0-2 loss: 1.185771  [   96/  118]
train() client id: f_00009-1-0 loss: 1.127532  [   32/  118]
train() client id: f_00009-1-1 loss: 1.174184  [   64/  118]
train() client id: f_00009-1-2 loss: 1.154634  [   96/  118]
train() client id: f_00009-2-0 loss: 1.102329  [   32/  118]
train() client id: f_00009-2-1 loss: 1.132372  [   64/  118]
train() client id: f_00009-2-2 loss: 1.065490  [   96/  118]
train() client id: f_00009-3-0 loss: 1.046308  [   32/  118]
train() client id: f_00009-3-1 loss: 1.104028  [   64/  118]
train() client id: f_00009-3-2 loss: 1.037829  [   96/  118]
train() client id: f_00009-4-0 loss: 0.998607  [   32/  118]
train() client id: f_00009-4-1 loss: 1.114472  [   64/  118]
train() client id: f_00009-4-2 loss: 0.976742  [   96/  118]
train() client id: f_00009-5-0 loss: 1.041003  [   32/  118]
train() client id: f_00009-5-1 loss: 1.016565  [   64/  118]
train() client id: f_00009-5-2 loss: 1.043665  [   96/  118]
train() client id: f_00009-6-0 loss: 0.976863  [   32/  118]
train() client id: f_00009-6-1 loss: 1.036112  [   64/  118]
train() client id: f_00009-6-2 loss: 0.957896  [   96/  118]
train() client id: f_00009-7-0 loss: 1.048157  [   32/  118]
train() client id: f_00009-7-1 loss: 1.033965  [   64/  118]
train() client id: f_00009-7-2 loss: 0.889691  [   96/  118]
train() client id: f_00009-8-0 loss: 0.975580  [   32/  118]
train() client id: f_00009-8-1 loss: 0.960039  [   64/  118]
train() client id: f_00009-8-2 loss: 0.923188  [   96/  118]
train() client id: f_00009-9-0 loss: 0.989518  [   32/  118]
train() client id: f_00009-9-1 loss: 0.906884  [   64/  118]
train() client id: f_00009-9-2 loss: 0.920411  [   96/  118]
train() client id: f_00009-10-0 loss: 0.919494  [   32/  118]
train() client id: f_00009-10-1 loss: 0.978184  [   64/  118]
train() client id: f_00009-10-2 loss: 1.018717  [   96/  118]
train() client id: f_00009-11-0 loss: 0.997320  [   32/  118]
train() client id: f_00009-11-1 loss: 0.918842  [   64/  118]
train() client id: f_00009-11-2 loss: 0.996231  [   96/  118]
At round 8 accuracy: 0.6312997347480106
At round 8 training accuracy: 0.5700871898054997
At round 8 training loss: 0.8685220552856974
update_location
xs = [ -3.9056584    4.20031788  60.00902392  18.81129433   0.97929623
   3.95640986 -22.44319194  -1.32485185  44.66397685   2.93912145]
ys = [ 52.5879595   35.55583871   1.32061395 -22.45517586  14.35018685
  -2.18584926  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [113.05196881 106.21610206 116.63115781 104.20220591 101.02913878
 100.10210345 102.5211559  100.0121567  110.92132708 100.1231756 ]
dists_bs = [210.37580989 227.0626082  292.22212521 276.67917212 238.27940567
 251.83371857 234.14571262 245.9693305  270.25317868 246.78497559]
uav_gains = [7.35860472e-11 8.60043309e-11 6.80689758e-11 9.02204198e-11
 9.74723702e-11 9.97448252e-11 9.39645390e-11 9.99692471e-11
 7.71712154e-11 9.96923511e-11]
bs_gains = [3.45864385e-11 2.79309909e-11 1.37815295e-11 1.60605338e-11
 2.44034856e-11 2.09013928e-11 2.56290612e-11 2.23268472e-11
 1.71528313e-11 2.21208434e-11]
Round 9
-------------------------------
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.80354861 20.47010187  9.65317002  3.45074232 23.61178753 11.38579797
  4.29063581 13.84678522 10.17374854  9.24174878]
obj_prev = 115.9280666591941
eta_min = 3.3564500217132474e-10	eta_max = 0.9202772988105504
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 26.95968104561057	eta = 0.909090909090909
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 46.612772647926946	eta = 0.5257958185768866
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 37.23476446800202	eta = 0.6582236063723965
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.55373220830825	eta = 0.6893453774967627
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.470824829161245	eta = 0.690956612049402
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.47060887359621	eta = 0.690960818797758
eta = 0.690960818797758
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.03050446 0.06415621 0.03002026 0.01041025 0.07408226 0.03534644
 0.01307334 0.0433357  0.03147286 0.02856767]
ene_total = [3.01933378 5.87455757 2.98817258 1.36705087 6.70212565 3.5823017
 1.57947799 4.03156066 3.30411446 3.0219136 ]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 0 obj = 6.211065406906146
eta = 0.690960818797758
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
eta_min = 0.6909608187976134	eta_max = 0.6909608187977386
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 0.05319201668668125	eta = 0.9090909090909091
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 20.968149575428694	eta = 0.002306182461743776
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.200288580777995	eta = 0.021977289355824242
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330781928113622	eta = 0.022669763803801746
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330546285065273	eta = 0.022670014241468774
eta = 0.022670014241468774
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.05846218e-04 2.12451699e-03 1.97533160e-04 8.04588971e-06
 3.33298867e-03 3.70534332e-04 1.58844283e-05 5.75826930e-04
 2.70281154e-04 1.93918678e-04]
ene_total = [0.17661037 0.26448822 0.17897858 0.16512347 0.30228746 0.23450342
 0.1641027  0.17665537 0.24344476 0.22686028]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 1 obj = 6.211065406903268
eta = 0.6909608187976134
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
Done!
At round 9 energy consumption: 6.211065406906146
At round 9 eta: 0.6909608187976134
At round 9 local rounds: 12.104944645341616
At round 9 global rounds: 81.21847247936174
At round 9 a_n: 24.7571443865612
gradient difference: 0.42488574981689453
train() client id: f_00000-0-0 loss: 1.402088  [   32/  126]
train() client id: f_00000-0-1 loss: 1.369755  [   64/  126]
train() client id: f_00000-0-2 loss: 1.415271  [   96/  126]
train() client id: f_00000-1-0 loss: 1.397495  [   32/  126]
train() client id: f_00000-1-1 loss: 1.101814  [   64/  126]
train() client id: f_00000-1-2 loss: 1.333866  [   96/  126]
train() client id: f_00000-2-0 loss: 1.142508  [   32/  126]
train() client id: f_00000-2-1 loss: 1.241850  [   64/  126]
train() client id: f_00000-2-2 loss: 1.153171  [   96/  126]
train() client id: f_00000-3-0 loss: 1.156529  [   32/  126]
train() client id: f_00000-3-1 loss: 1.104373  [   64/  126]
train() client id: f_00000-3-2 loss: 1.031802  [   96/  126]
train() client id: f_00000-4-0 loss: 1.066127  [   32/  126]
train() client id: f_00000-4-1 loss: 0.979776  [   64/  126]
train() client id: f_00000-4-2 loss: 1.042474  [   96/  126]
train() client id: f_00000-5-0 loss: 1.038318  [   32/  126]
train() client id: f_00000-5-1 loss: 0.990007  [   64/  126]
train() client id: f_00000-5-2 loss: 0.969622  [   96/  126]
train() client id: f_00000-6-0 loss: 0.945332  [   32/  126]
train() client id: f_00000-6-1 loss: 0.966619  [   64/  126]
train() client id: f_00000-6-2 loss: 0.999198  [   96/  126]
train() client id: f_00000-7-0 loss: 0.932074  [   32/  126]
train() client id: f_00000-7-1 loss: 0.958192  [   64/  126]
train() client id: f_00000-7-2 loss: 0.898723  [   96/  126]
train() client id: f_00000-8-0 loss: 0.944905  [   32/  126]
train() client id: f_00000-8-1 loss: 0.920234  [   64/  126]
train() client id: f_00000-8-2 loss: 0.929533  [   96/  126]
train() client id: f_00000-9-0 loss: 0.902370  [   32/  126]
train() client id: f_00000-9-1 loss: 1.001207  [   64/  126]
train() client id: f_00000-9-2 loss: 0.888922  [   96/  126]
train() client id: f_00000-10-0 loss: 0.939541  [   32/  126]
train() client id: f_00000-10-1 loss: 0.861876  [   64/  126]
train() client id: f_00000-10-2 loss: 0.915738  [   96/  126]
train() client id: f_00000-11-0 loss: 0.862602  [   32/  126]
train() client id: f_00000-11-1 loss: 0.875139  [   64/  126]
train() client id: f_00000-11-2 loss: 0.948106  [   96/  126]
train() client id: f_00001-0-0 loss: 0.642767  [   32/  265]
train() client id: f_00001-0-1 loss: 0.619041  [   64/  265]
train() client id: f_00001-0-2 loss: 0.608961  [   96/  265]
train() client id: f_00001-0-3 loss: 0.578689  [  128/  265]
train() client id: f_00001-0-4 loss: 0.554573  [  160/  265]
train() client id: f_00001-0-5 loss: 0.599267  [  192/  265]
train() client id: f_00001-0-6 loss: 0.530359  [  224/  265]
train() client id: f_00001-0-7 loss: 0.548686  [  256/  265]
train() client id: f_00001-1-0 loss: 0.529200  [   32/  265]
train() client id: f_00001-1-1 loss: 0.510429  [   64/  265]
train() client id: f_00001-1-2 loss: 0.552776  [   96/  265]
train() client id: f_00001-1-3 loss: 0.527293  [  128/  265]
train() client id: f_00001-1-4 loss: 0.545963  [  160/  265]
train() client id: f_00001-1-5 loss: 0.526481  [  192/  265]
train() client id: f_00001-1-6 loss: 0.801234  [  224/  265]
train() client id: f_00001-1-7 loss: 0.612134  [  256/  265]
train() client id: f_00001-2-0 loss: 0.714993  [   32/  265]
train() client id: f_00001-2-1 loss: 0.581316  [   64/  265]
train() client id: f_00001-2-2 loss: 0.559637  [   96/  265]
train() client id: f_00001-2-3 loss: 0.494719  [  128/  265]
train() client id: f_00001-2-4 loss: 0.604764  [  160/  265]
train() client id: f_00001-2-5 loss: 0.478773  [  192/  265]
train() client id: f_00001-2-6 loss: 0.479388  [  224/  265]
train() client id: f_00001-2-7 loss: 0.602082  [  256/  265]
train() client id: f_00001-3-0 loss: 0.542714  [   32/  265]
train() client id: f_00001-3-1 loss: 0.576633  [   64/  265]
train() client id: f_00001-3-2 loss: 0.538525  [   96/  265]
train() client id: f_00001-3-3 loss: 0.629846  [  128/  265]
train() client id: f_00001-3-4 loss: 0.463095  [  160/  265]
train() client id: f_00001-3-5 loss: 0.597943  [  192/  265]
train() client id: f_00001-3-6 loss: 0.580931  [  224/  265]
train() client id: f_00001-3-7 loss: 0.545785  [  256/  265]
train() client id: f_00001-4-0 loss: 0.475588  [   32/  265]
train() client id: f_00001-4-1 loss: 0.622658  [   64/  265]
train() client id: f_00001-4-2 loss: 0.473968  [   96/  265]
train() client id: f_00001-4-3 loss: 0.583205  [  128/  265]
train() client id: f_00001-4-4 loss: 0.513133  [  160/  265]
train() client id: f_00001-4-5 loss: 0.567677  [  192/  265]
train() client id: f_00001-4-6 loss: 0.599229  [  224/  265]
train() client id: f_00001-4-7 loss: 0.597620  [  256/  265]
train() client id: f_00001-5-0 loss: 0.508483  [   32/  265]
train() client id: f_00001-5-1 loss: 0.471135  [   64/  265]
train() client id: f_00001-5-2 loss: 0.630214  [   96/  265]
train() client id: f_00001-5-3 loss: 0.523776  [  128/  265]
train() client id: f_00001-5-4 loss: 0.640728  [  160/  265]
train() client id: f_00001-5-5 loss: 0.517214  [  192/  265]
train() client id: f_00001-5-6 loss: 0.576688  [  224/  265]
train() client id: f_00001-5-7 loss: 0.546633  [  256/  265]
train() client id: f_00001-6-0 loss: 0.546238  [   32/  265]
train() client id: f_00001-6-1 loss: 0.527577  [   64/  265]
train() client id: f_00001-6-2 loss: 0.625789  [   96/  265]
train() client id: f_00001-6-3 loss: 0.704422  [  128/  265]
train() client id: f_00001-6-4 loss: 0.521614  [  160/  265]
train() client id: f_00001-6-5 loss: 0.455796  [  192/  265]
train() client id: f_00001-6-6 loss: 0.501398  [  224/  265]
train() client id: f_00001-6-7 loss: 0.511536  [  256/  265]
train() client id: f_00001-7-0 loss: 0.571895  [   32/  265]
train() client id: f_00001-7-1 loss: 0.634286  [   64/  265]
train() client id: f_00001-7-2 loss: 0.480724  [   96/  265]
train() client id: f_00001-7-3 loss: 0.527951  [  128/  265]
train() client id: f_00001-7-4 loss: 0.454193  [  160/  265]
train() client id: f_00001-7-5 loss: 0.597804  [  192/  265]
train() client id: f_00001-7-6 loss: 0.534421  [  224/  265]
train() client id: f_00001-7-7 loss: 0.529107  [  256/  265]
train() client id: f_00001-8-0 loss: 0.640999  [   32/  265]
train() client id: f_00001-8-1 loss: 0.463389  [   64/  265]
train() client id: f_00001-8-2 loss: 0.451713  [   96/  265]
train() client id: f_00001-8-3 loss: 0.587838  [  128/  265]
train() client id: f_00001-8-4 loss: 0.648574  [  160/  265]
train() client id: f_00001-8-5 loss: 0.526337  [  192/  265]
train() client id: f_00001-8-6 loss: 0.474276  [  224/  265]
train() client id: f_00001-8-7 loss: 0.513244  [  256/  265]
train() client id: f_00001-9-0 loss: 0.506438  [   32/  265]
train() client id: f_00001-9-1 loss: 0.505150  [   64/  265]
train() client id: f_00001-9-2 loss: 0.510189  [   96/  265]
train() client id: f_00001-9-3 loss: 0.679847  [  128/  265]
train() client id: f_00001-9-4 loss: 0.593494  [  160/  265]
train() client id: f_00001-9-5 loss: 0.520360  [  192/  265]
train() client id: f_00001-9-6 loss: 0.528088  [  224/  265]
train() client id: f_00001-9-7 loss: 0.541374  [  256/  265]
train() client id: f_00001-10-0 loss: 0.524207  [   32/  265]
train() client id: f_00001-10-1 loss: 0.615204  [   64/  265]
train() client id: f_00001-10-2 loss: 0.515126  [   96/  265]
train() client id: f_00001-10-3 loss: 0.520150  [  128/  265]
train() client id: f_00001-10-4 loss: 0.497272  [  160/  265]
train() client id: f_00001-10-5 loss: 0.468618  [  192/  265]
train() client id: f_00001-10-6 loss: 0.472726  [  224/  265]
train() client id: f_00001-10-7 loss: 0.723655  [  256/  265]
train() client id: f_00001-11-0 loss: 0.604049  [   32/  265]
train() client id: f_00001-11-1 loss: 0.578968  [   64/  265]
train() client id: f_00001-11-2 loss: 0.581698  [   96/  265]
train() client id: f_00001-11-3 loss: 0.437079  [  128/  265]
train() client id: f_00001-11-4 loss: 0.505314  [  160/  265]
train() client id: f_00001-11-5 loss: 0.643074  [  192/  265]
train() client id: f_00001-11-6 loss: 0.473805  [  224/  265]
train() client id: f_00001-11-7 loss: 0.509252  [  256/  265]
train() client id: f_00002-0-0 loss: 1.192532  [   32/  124]
train() client id: f_00002-0-1 loss: 1.223142  [   64/  124]
train() client id: f_00002-0-2 loss: 1.140188  [   96/  124]
train() client id: f_00002-1-0 loss: 1.175362  [   32/  124]
train() client id: f_00002-1-1 loss: 1.218128  [   64/  124]
train() client id: f_00002-1-2 loss: 1.061556  [   96/  124]
train() client id: f_00002-2-0 loss: 1.107545  [   32/  124]
train() client id: f_00002-2-1 loss: 1.141160  [   64/  124]
train() client id: f_00002-2-2 loss: 1.082016  [   96/  124]
train() client id: f_00002-3-0 loss: 1.063087  [   32/  124]
train() client id: f_00002-3-1 loss: 1.103705  [   64/  124]
train() client id: f_00002-3-2 loss: 1.105913  [   96/  124]
train() client id: f_00002-4-0 loss: 0.950308  [   32/  124]
train() client id: f_00002-4-1 loss: 1.185617  [   64/  124]
train() client id: f_00002-4-2 loss: 1.069464  [   96/  124]
train() client id: f_00002-5-0 loss: 1.045149  [   32/  124]
train() client id: f_00002-5-1 loss: 1.101902  [   64/  124]
train() client id: f_00002-5-2 loss: 1.025712  [   96/  124]
train() client id: f_00002-6-0 loss: 0.979731  [   32/  124]
train() client id: f_00002-6-1 loss: 1.032552  [   64/  124]
train() client id: f_00002-6-2 loss: 0.974875  [   96/  124]
train() client id: f_00002-7-0 loss: 0.985229  [   32/  124]
train() client id: f_00002-7-1 loss: 1.083711  [   64/  124]
train() client id: f_00002-7-2 loss: 0.982414  [   96/  124]
train() client id: f_00002-8-0 loss: 1.073809  [   32/  124]
train() client id: f_00002-8-1 loss: 1.011318  [   64/  124]
train() client id: f_00002-8-2 loss: 0.946000  [   96/  124]
train() client id: f_00002-9-0 loss: 0.958523  [   32/  124]
train() client id: f_00002-9-1 loss: 1.009694  [   64/  124]
train() client id: f_00002-9-2 loss: 1.031180  [   96/  124]
train() client id: f_00002-10-0 loss: 1.035660  [   32/  124]
train() client id: f_00002-10-1 loss: 0.963026  [   64/  124]
train() client id: f_00002-10-2 loss: 1.040598  [   96/  124]
train() client id: f_00002-11-0 loss: 0.924417  [   32/  124]
train() client id: f_00002-11-1 loss: 0.984635  [   64/  124]
train() client id: f_00002-11-2 loss: 1.033206  [   96/  124]
train() client id: f_00003-0-0 loss: 0.956175  [   32/   43]
train() client id: f_00003-1-0 loss: 0.838150  [   32/   43]
train() client id: f_00003-2-0 loss: 0.975538  [   32/   43]
train() client id: f_00003-3-0 loss: 0.886404  [   32/   43]
train() client id: f_00003-4-0 loss: 0.825428  [   32/   43]
train() client id: f_00003-5-0 loss: 0.834233  [   32/   43]
train() client id: f_00003-6-0 loss: 0.883879  [   32/   43]
train() client id: f_00003-7-0 loss: 0.864143  [   32/   43]
train() client id: f_00003-8-0 loss: 0.767872  [   32/   43]
train() client id: f_00003-9-0 loss: 0.852917  [   32/   43]
train() client id: f_00003-10-0 loss: 0.901720  [   32/   43]
train() client id: f_00003-11-0 loss: 0.909908  [   32/   43]
train() client id: f_00004-0-0 loss: 0.640515  [   32/  306]
train() client id: f_00004-0-1 loss: 0.780438  [   64/  306]
train() client id: f_00004-0-2 loss: 0.658749  [   96/  306]
train() client id: f_00004-0-3 loss: 0.705304  [  128/  306]
train() client id: f_00004-0-4 loss: 0.869113  [  160/  306]
train() client id: f_00004-0-5 loss: 0.653804  [  192/  306]
train() client id: f_00004-0-6 loss: 0.882151  [  224/  306]
train() client id: f_00004-0-7 loss: 0.823551  [  256/  306]
train() client id: f_00004-0-8 loss: 0.920152  [  288/  306]
train() client id: f_00004-1-0 loss: 0.730765  [   32/  306]
train() client id: f_00004-1-1 loss: 0.806563  [   64/  306]
train() client id: f_00004-1-2 loss: 0.782406  [   96/  306]
train() client id: f_00004-1-3 loss: 0.831584  [  128/  306]
train() client id: f_00004-1-4 loss: 0.806861  [  160/  306]
train() client id: f_00004-1-5 loss: 0.788728  [  192/  306]
train() client id: f_00004-1-6 loss: 0.841801  [  224/  306]
train() client id: f_00004-1-7 loss: 0.828033  [  256/  306]
train() client id: f_00004-1-8 loss: 0.726307  [  288/  306]
train() client id: f_00004-2-0 loss: 0.711541  [   32/  306]
train() client id: f_00004-2-1 loss: 0.902424  [   64/  306]
train() client id: f_00004-2-2 loss: 0.660332  [   96/  306]
train() client id: f_00004-2-3 loss: 0.773983  [  128/  306]
train() client id: f_00004-2-4 loss: 0.871812  [  160/  306]
train() client id: f_00004-2-5 loss: 0.800439  [  192/  306]
train() client id: f_00004-2-6 loss: 0.696374  [  224/  306]
train() client id: f_00004-2-7 loss: 0.897168  [  256/  306]
train() client id: f_00004-2-8 loss: 0.721771  [  288/  306]
train() client id: f_00004-3-0 loss: 0.918085  [   32/  306]
train() client id: f_00004-3-1 loss: 0.746849  [   64/  306]
train() client id: f_00004-3-2 loss: 0.785632  [   96/  306]
train() client id: f_00004-3-3 loss: 0.805615  [  128/  306]
train() client id: f_00004-3-4 loss: 0.833403  [  160/  306]
train() client id: f_00004-3-5 loss: 0.818274  [  192/  306]
train() client id: f_00004-3-6 loss: 0.752358  [  224/  306]
train() client id: f_00004-3-7 loss: 0.768353  [  256/  306]
train() client id: f_00004-3-8 loss: 0.655356  [  288/  306]
train() client id: f_00004-4-0 loss: 0.715160  [   32/  306]
train() client id: f_00004-4-1 loss: 0.786872  [   64/  306]
train() client id: f_00004-4-2 loss: 0.850759  [   96/  306]
train() client id: f_00004-4-3 loss: 0.968409  [  128/  306]
train() client id: f_00004-4-4 loss: 0.744728  [  160/  306]
train() client id: f_00004-4-5 loss: 0.722494  [  192/  306]
train() client id: f_00004-4-6 loss: 0.774808  [  224/  306]
train() client id: f_00004-4-7 loss: 0.840451  [  256/  306]
train() client id: f_00004-4-8 loss: 0.751306  [  288/  306]
train() client id: f_00004-5-0 loss: 0.709622  [   32/  306]
train() client id: f_00004-5-1 loss: 0.785518  [   64/  306]
train() client id: f_00004-5-2 loss: 0.802500  [   96/  306]
train() client id: f_00004-5-3 loss: 0.789918  [  128/  306]
train() client id: f_00004-5-4 loss: 0.776935  [  160/  306]
train() client id: f_00004-5-5 loss: 0.795942  [  192/  306]
train() client id: f_00004-5-6 loss: 0.755093  [  224/  306]
train() client id: f_00004-5-7 loss: 0.783419  [  256/  306]
train() client id: f_00004-5-8 loss: 0.837379  [  288/  306]
train() client id: f_00004-6-0 loss: 0.789975  [   32/  306]
train() client id: f_00004-6-1 loss: 0.690449  [   64/  306]
train() client id: f_00004-6-2 loss: 0.750393  [   96/  306]
train() client id: f_00004-6-3 loss: 0.745595  [  128/  306]
train() client id: f_00004-6-4 loss: 0.779598  [  160/  306]
train() client id: f_00004-6-5 loss: 0.838280  [  192/  306]
train() client id: f_00004-6-6 loss: 0.818423  [  224/  306]
train() client id: f_00004-6-7 loss: 0.804522  [  256/  306]
train() client id: f_00004-6-8 loss: 0.876416  [  288/  306]
train() client id: f_00004-7-0 loss: 0.802339  [   32/  306]
train() client id: f_00004-7-1 loss: 0.769993  [   64/  306]
train() client id: f_00004-7-2 loss: 0.701483  [   96/  306]
train() client id: f_00004-7-3 loss: 0.760508  [  128/  306]
train() client id: f_00004-7-4 loss: 0.842063  [  160/  306]
train() client id: f_00004-7-5 loss: 0.806912  [  192/  306]
train() client id: f_00004-7-6 loss: 0.786699  [  224/  306]
train() client id: f_00004-7-7 loss: 0.930828  [  256/  306]
train() client id: f_00004-7-8 loss: 0.713487  [  288/  306]
train() client id: f_00004-8-0 loss: 0.852553  [   32/  306]
train() client id: f_00004-8-1 loss: 0.832560  [   64/  306]
train() client id: f_00004-8-2 loss: 0.724551  [   96/  306]
train() client id: f_00004-8-3 loss: 0.812019  [  128/  306]
train() client id: f_00004-8-4 loss: 0.733995  [  160/  306]
train() client id: f_00004-8-5 loss: 0.870429  [  192/  306]
train() client id: f_00004-8-6 loss: 0.741571  [  224/  306]
train() client id: f_00004-8-7 loss: 0.833772  [  256/  306]
train() client id: f_00004-8-8 loss: 0.756710  [  288/  306]
train() client id: f_00004-9-0 loss: 0.800629  [   32/  306]
train() client id: f_00004-9-1 loss: 0.788637  [   64/  306]
train() client id: f_00004-9-2 loss: 0.784585  [   96/  306]
train() client id: f_00004-9-3 loss: 0.747314  [  128/  306]
train() client id: f_00004-9-4 loss: 0.869348  [  160/  306]
train() client id: f_00004-9-5 loss: 0.769694  [  192/  306]
train() client id: f_00004-9-6 loss: 0.765480  [  224/  306]
train() client id: f_00004-9-7 loss: 0.944801  [  256/  306]
train() client id: f_00004-9-8 loss: 0.761775  [  288/  306]
train() client id: f_00004-10-0 loss: 0.803015  [   32/  306]
train() client id: f_00004-10-1 loss: 0.749159  [   64/  306]
train() client id: f_00004-10-2 loss: 0.871937  [   96/  306]
train() client id: f_00004-10-3 loss: 0.756345  [  128/  306]
train() client id: f_00004-10-4 loss: 0.731775  [  160/  306]
train() client id: f_00004-10-5 loss: 0.808865  [  192/  306]
train() client id: f_00004-10-6 loss: 0.793675  [  224/  306]
train() client id: f_00004-10-7 loss: 0.821901  [  256/  306]
train() client id: f_00004-10-8 loss: 0.840380  [  288/  306]
train() client id: f_00004-11-0 loss: 0.818571  [   32/  306]
train() client id: f_00004-11-1 loss: 0.727636  [   64/  306]
train() client id: f_00004-11-2 loss: 0.946181  [   96/  306]
train() client id: f_00004-11-3 loss: 0.735264  [  128/  306]
train() client id: f_00004-11-4 loss: 0.811387  [  160/  306]
train() client id: f_00004-11-5 loss: 0.741773  [  192/  306]
train() client id: f_00004-11-6 loss: 0.766966  [  224/  306]
train() client id: f_00004-11-7 loss: 0.889721  [  256/  306]
train() client id: f_00004-11-8 loss: 0.815596  [  288/  306]
train() client id: f_00005-0-0 loss: 0.736685  [   32/  146]
train() client id: f_00005-0-1 loss: 0.665420  [   64/  146]
train() client id: f_00005-0-2 loss: 0.645955  [   96/  146]
train() client id: f_00005-0-3 loss: 0.800211  [  128/  146]
train() client id: f_00005-1-0 loss: 0.717593  [   32/  146]
train() client id: f_00005-1-1 loss: 0.729783  [   64/  146]
train() client id: f_00005-1-2 loss: 0.529332  [   96/  146]
train() client id: f_00005-1-3 loss: 0.704038  [  128/  146]
train() client id: f_00005-2-0 loss: 0.664725  [   32/  146]
train() client id: f_00005-2-1 loss: 0.615480  [   64/  146]
train() client id: f_00005-2-2 loss: 0.687399  [   96/  146]
train() client id: f_00005-2-3 loss: 0.700846  [  128/  146]
train() client id: f_00005-3-0 loss: 0.696039  [   32/  146]
train() client id: f_00005-3-1 loss: 0.635109  [   64/  146]
train() client id: f_00005-3-2 loss: 0.709513  [   96/  146]
train() client id: f_00005-3-3 loss: 0.544702  [  128/  146]
train() client id: f_00005-4-0 loss: 0.813954  [   32/  146]
train() client id: f_00005-4-1 loss: 0.748482  [   64/  146]
train() client id: f_00005-4-2 loss: 0.669338  [   96/  146]
train() client id: f_00005-4-3 loss: 0.514820  [  128/  146]
train() client id: f_00005-5-0 loss: 0.658587  [   32/  146]
train() client id: f_00005-5-1 loss: 0.724678  [   64/  146]
train() client id: f_00005-5-2 loss: 0.754377  [   96/  146]
train() client id: f_00005-5-3 loss: 0.731144  [  128/  146]
train() client id: f_00005-6-0 loss: 0.695883  [   32/  146]
train() client id: f_00005-6-1 loss: 0.798474  [   64/  146]
train() client id: f_00005-6-2 loss: 0.723942  [   96/  146]
train() client id: f_00005-6-3 loss: 0.542926  [  128/  146]
train() client id: f_00005-7-0 loss: 0.692998  [   32/  146]
train() client id: f_00005-7-1 loss: 0.618164  [   64/  146]
train() client id: f_00005-7-2 loss: 0.879024  [   96/  146]
train() client id: f_00005-7-3 loss: 0.599162  [  128/  146]
train() client id: f_00005-8-0 loss: 0.560629  [   32/  146]
train() client id: f_00005-8-1 loss: 0.669057  [   64/  146]
train() client id: f_00005-8-2 loss: 0.894996  [   96/  146]
train() client id: f_00005-8-3 loss: 0.526509  [  128/  146]
train() client id: f_00005-9-0 loss: 0.596676  [   32/  146]
train() client id: f_00005-9-1 loss: 0.537200  [   64/  146]
train() client id: f_00005-9-2 loss: 0.837059  [   96/  146]
train() client id: f_00005-9-3 loss: 0.737200  [  128/  146]
train() client id: f_00005-10-0 loss: 0.666840  [   32/  146]
train() client id: f_00005-10-1 loss: 0.699871  [   64/  146]
train() client id: f_00005-10-2 loss: 0.704981  [   96/  146]
train() client id: f_00005-10-3 loss: 0.653170  [  128/  146]
train() client id: f_00005-11-0 loss: 0.724317  [   32/  146]
train() client id: f_00005-11-1 loss: 0.765498  [   64/  146]
train() client id: f_00005-11-2 loss: 0.553426  [   96/  146]
train() client id: f_00005-11-3 loss: 0.728748  [  128/  146]
train() client id: f_00006-0-0 loss: 0.769427  [   32/   54]
train() client id: f_00006-1-0 loss: 0.785714  [   32/   54]
train() client id: f_00006-2-0 loss: 0.810495  [   32/   54]
train() client id: f_00006-3-0 loss: 0.730928  [   32/   54]
train() client id: f_00006-4-0 loss: 0.811792  [   32/   54]
train() client id: f_00006-5-0 loss: 0.726016  [   32/   54]
train() client id: f_00006-6-0 loss: 0.812429  [   32/   54]
train() client id: f_00006-7-0 loss: 0.764519  [   32/   54]
train() client id: f_00006-8-0 loss: 0.772110  [   32/   54]
train() client id: f_00006-9-0 loss: 0.720153  [   32/   54]
train() client id: f_00006-10-0 loss: 0.813629  [   32/   54]
train() client id: f_00006-11-0 loss: 0.725606  [   32/   54]
train() client id: f_00007-0-0 loss: 0.738664  [   32/  179]
train() client id: f_00007-0-1 loss: 0.762956  [   64/  179]
train() client id: f_00007-0-2 loss: 0.636150  [   96/  179]
train() client id: f_00007-0-3 loss: 0.791085  [  128/  179]
train() client id: f_00007-0-4 loss: 0.731365  [  160/  179]
train() client id: f_00007-1-0 loss: 0.630759  [   32/  179]
train() client id: f_00007-1-1 loss: 0.668966  [   64/  179]
train() client id: f_00007-1-2 loss: 0.758974  [   96/  179]
train() client id: f_00007-1-3 loss: 0.678399  [  128/  179]
train() client id: f_00007-1-4 loss: 0.738988  [  160/  179]
train() client id: f_00007-2-0 loss: 0.727900  [   32/  179]
train() client id: f_00007-2-1 loss: 0.744526  [   64/  179]
train() client id: f_00007-2-2 loss: 0.652577  [   96/  179]
train() client id: f_00007-2-3 loss: 0.730785  [  128/  179]
train() client id: f_00007-2-4 loss: 0.615435  [  160/  179]
train() client id: f_00007-3-0 loss: 0.771063  [   32/  179]
train() client id: f_00007-3-1 loss: 0.653137  [   64/  179]
train() client id: f_00007-3-2 loss: 0.586877  [   96/  179]
train() client id: f_00007-3-3 loss: 0.732109  [  128/  179]
train() client id: f_00007-3-4 loss: 0.580208  [  160/  179]
train() client id: f_00007-4-0 loss: 0.702464  [   32/  179]
train() client id: f_00007-4-1 loss: 0.751595  [   64/  179]
train() client id: f_00007-4-2 loss: 0.645809  [   96/  179]
train() client id: f_00007-4-3 loss: 0.635602  [  128/  179]
train() client id: f_00007-4-4 loss: 0.654248  [  160/  179]
train() client id: f_00007-5-0 loss: 0.675798  [   32/  179]
train() client id: f_00007-5-1 loss: 0.736818  [   64/  179]
train() client id: f_00007-5-2 loss: 0.644303  [   96/  179]
train() client id: f_00007-5-3 loss: 0.644986  [  128/  179]
train() client id: f_00007-5-4 loss: 0.663121  [  160/  179]
train() client id: f_00007-6-0 loss: 0.632720  [   32/  179]
train() client id: f_00007-6-1 loss: 0.633520  [   64/  179]
train() client id: f_00007-6-2 loss: 0.676279  [   96/  179]
train() client id: f_00007-6-3 loss: 0.665212  [  128/  179]
train() client id: f_00007-6-4 loss: 0.689534  [  160/  179]
train() client id: f_00007-7-0 loss: 0.697187  [   32/  179]
train() client id: f_00007-7-1 loss: 0.588398  [   64/  179]
train() client id: f_00007-7-2 loss: 0.787092  [   96/  179]
train() client id: f_00007-7-3 loss: 0.540195  [  128/  179]
train() client id: f_00007-7-4 loss: 0.684501  [  160/  179]
train() client id: f_00007-8-0 loss: 0.675951  [   32/  179]
train() client id: f_00007-8-1 loss: 0.779487  [   64/  179]
train() client id: f_00007-8-2 loss: 0.621986  [   96/  179]
train() client id: f_00007-8-3 loss: 0.598628  [  128/  179]
train() client id: f_00007-8-4 loss: 0.657931  [  160/  179]
train() client id: f_00007-9-0 loss: 0.640040  [   32/  179]
train() client id: f_00007-9-1 loss: 0.765777  [   64/  179]
train() client id: f_00007-9-2 loss: 0.631533  [   96/  179]
train() client id: f_00007-9-3 loss: 0.681787  [  128/  179]
train() client id: f_00007-9-4 loss: 0.559323  [  160/  179]
train() client id: f_00007-10-0 loss: 0.700133  [   32/  179]
train() client id: f_00007-10-1 loss: 0.609472  [   64/  179]
train() client id: f_00007-10-2 loss: 0.741387  [   96/  179]
train() client id: f_00007-10-3 loss: 0.607242  [  128/  179]
train() client id: f_00007-10-4 loss: 0.678664  [  160/  179]
train() client id: f_00007-11-0 loss: 0.653754  [   32/  179]
train() client id: f_00007-11-1 loss: 0.616858  [   64/  179]
train() client id: f_00007-11-2 loss: 0.602574  [   96/  179]
train() client id: f_00007-11-3 loss: 0.603379  [  128/  179]
train() client id: f_00007-11-4 loss: 0.774748  [  160/  179]
train() client id: f_00008-0-0 loss: 0.766312  [   32/  130]
train() client id: f_00008-0-1 loss: 0.877449  [   64/  130]
train() client id: f_00008-0-2 loss: 0.797496  [   96/  130]
train() client id: f_00008-0-3 loss: 0.866426  [  128/  130]
train() client id: f_00008-1-0 loss: 0.920562  [   32/  130]
train() client id: f_00008-1-1 loss: 0.787045  [   64/  130]
train() client id: f_00008-1-2 loss: 0.840726  [   96/  130]
train() client id: f_00008-1-3 loss: 0.766096  [  128/  130]
train() client id: f_00008-2-0 loss: 0.911693  [   32/  130]
train() client id: f_00008-2-1 loss: 0.773512  [   64/  130]
train() client id: f_00008-2-2 loss: 0.757439  [   96/  130]
train() client id: f_00008-2-3 loss: 0.800479  [  128/  130]
train() client id: f_00008-3-0 loss: 0.809523  [   32/  130]
train() client id: f_00008-3-1 loss: 0.818916  [   64/  130]
train() client id: f_00008-3-2 loss: 0.880819  [   96/  130]
train() client id: f_00008-3-3 loss: 0.728495  [  128/  130]
train() client id: f_00008-4-0 loss: 0.822508  [   32/  130]
train() client id: f_00008-4-1 loss: 0.839452  [   64/  130]
train() client id: f_00008-4-2 loss: 0.807873  [   96/  130]
train() client id: f_00008-4-3 loss: 0.831658  [  128/  130]
train() client id: f_00008-5-0 loss: 0.917120  [   32/  130]
train() client id: f_00008-5-1 loss: 0.767316  [   64/  130]
train() client id: f_00008-5-2 loss: 0.821692  [   96/  130]
train() client id: f_00008-5-3 loss: 0.790071  [  128/  130]
train() client id: f_00008-6-0 loss: 1.003733  [   32/  130]
train() client id: f_00008-6-1 loss: 0.755565  [   64/  130]
train() client id: f_00008-6-2 loss: 0.763570  [   96/  130]
train() client id: f_00008-6-3 loss: 0.779388  [  128/  130]
train() client id: f_00008-7-0 loss: 0.721306  [   32/  130]
train() client id: f_00008-7-1 loss: 0.805576  [   64/  130]
train() client id: f_00008-7-2 loss: 0.798231  [   96/  130]
train() client id: f_00008-7-3 loss: 0.947067  [  128/  130]
train() client id: f_00008-8-0 loss: 0.836870  [   32/  130]
train() client id: f_00008-8-1 loss: 0.812249  [   64/  130]
train() client id: f_00008-8-2 loss: 0.760100  [   96/  130]
train() client id: f_00008-8-3 loss: 0.888159  [  128/  130]
train() client id: f_00008-9-0 loss: 0.789158  [   32/  130]
train() client id: f_00008-9-1 loss: 0.869568  [   64/  130]
train() client id: f_00008-9-2 loss: 0.713868  [   96/  130]
train() client id: f_00008-9-3 loss: 0.926896  [  128/  130]
train() client id: f_00008-10-0 loss: 0.940931  [   32/  130]
train() client id: f_00008-10-1 loss: 0.809975  [   64/  130]
train() client id: f_00008-10-2 loss: 0.781953  [   96/  130]
train() client id: f_00008-10-3 loss: 0.736677  [  128/  130]
train() client id: f_00008-11-0 loss: 0.864091  [   32/  130]
train() client id: f_00008-11-1 loss: 0.688390  [   64/  130]
train() client id: f_00008-11-2 loss: 0.819926  [   96/  130]
train() client id: f_00008-11-3 loss: 0.921976  [  128/  130]
train() client id: f_00009-0-0 loss: 1.201370  [   32/  118]
train() client id: f_00009-0-1 loss: 1.031187  [   64/  118]
train() client id: f_00009-0-2 loss: 1.136949  [   96/  118]
train() client id: f_00009-1-0 loss: 1.107963  [   32/  118]
train() client id: f_00009-1-1 loss: 1.089132  [   64/  118]
train() client id: f_00009-1-2 loss: 1.007516  [   96/  118]
train() client id: f_00009-2-0 loss: 1.085480  [   32/  118]
train() client id: f_00009-2-1 loss: 1.029775  [   64/  118]
train() client id: f_00009-2-2 loss: 1.049295  [   96/  118]
train() client id: f_00009-3-0 loss: 0.988492  [   32/  118]
train() client id: f_00009-3-1 loss: 0.996683  [   64/  118]
train() client id: f_00009-3-2 loss: 0.943625  [   96/  118]
train() client id: f_00009-4-0 loss: 0.929114  [   32/  118]
train() client id: f_00009-4-1 loss: 0.999706  [   64/  118]
train() client id: f_00009-4-2 loss: 1.009895  [   96/  118]
train() client id: f_00009-5-0 loss: 0.987759  [   32/  118]
train() client id: f_00009-5-1 loss: 0.971856  [   64/  118]
train() client id: f_00009-5-2 loss: 0.953966  [   96/  118]
train() client id: f_00009-6-0 loss: 0.938629  [   32/  118]
train() client id: f_00009-6-1 loss: 0.938798  [   64/  118]
train() client id: f_00009-6-2 loss: 0.894566  [   96/  118]
train() client id: f_00009-7-0 loss: 0.826044  [   32/  118]
train() client id: f_00009-7-1 loss: 0.978072  [   64/  118]
train() client id: f_00009-7-2 loss: 0.933305  [   96/  118]
train() client id: f_00009-8-0 loss: 0.859332  [   32/  118]
train() client id: f_00009-8-1 loss: 0.926120  [   64/  118]
train() client id: f_00009-8-2 loss: 1.025456  [   96/  118]
train() client id: f_00009-9-0 loss: 0.827929  [   32/  118]
train() client id: f_00009-9-1 loss: 0.917930  [   64/  118]
train() client id: f_00009-9-2 loss: 1.014369  [   96/  118]
train() client id: f_00009-10-0 loss: 0.932151  [   32/  118]
train() client id: f_00009-10-1 loss: 0.876398  [   64/  118]
train() client id: f_00009-10-2 loss: 0.934340  [   96/  118]
train() client id: f_00009-11-0 loss: 0.941413  [   32/  118]
train() client id: f_00009-11-1 loss: 0.918902  [   64/  118]
train() client id: f_00009-11-2 loss: 0.879958  [   96/  118]
At round 9 accuracy: 0.6312997347480106
At round 9 training accuracy: 0.5754527162977867
At round 9 training loss: 0.8598534163639779
update_location
xs = [ -3.9056584    4.20031788  65.00902392  18.81129433   0.97929623
   3.95640986 -27.44319194  -6.32485185  49.66397685   2.93912145]
ys = [ 57.5879595   40.55583871   1.32061395 -27.45517586  19.35018685
   2.81415074  -2.62498432   0.82234798  17.56900603  -0.99851822]
dists_uav = [115.46266603 107.99267903 119.28083338 105.39284357 101.85965223
 100.11779374 103.730513   100.2031936  113.02734434 100.04816577]
dists_bs = [207.50629142 224.02675383 296.25809812 280.26936337 234.93738961
 248.34122355 230.91913446 242.46476046 274.3326819  250.27546695]
uav_gains = [6.98045343e-11 8.25105385e-11 6.43507590e-11 8.76937845e-11
 9.54975988e-11 9.97057494e-11 9.12496048e-11 9.94934409e-11
 7.36261378e-11 9.98793178e-11]
bs_gains = [3.59423553e-11 2.90037668e-11 1.32622570e-11 1.54911026e-11
 2.53879770e-11 2.17348876e-11 2.66444222e-11 2.32422371e-11
 1.64481458e-11 2.12678163e-11]
Round 10
-------------------------------
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.67144915 20.18897006  9.52328793  3.40449035 23.28757639 11.22831614
  4.23293599 13.65794809 10.03742584  9.11771254]
obj_prev = 114.35011248363482
eta_min = 2.485131023397881e-10	eta_max = 0.9203875705067922
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 26.5917511352464	eta = 0.909090909090909
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 46.02197555438805	eta = 0.5252777379209097
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 36.745409717816976	eta = 0.6578867782263103
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.08220734696584	eta = 0.6890763450194114
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.00007514490989	eta = 0.6906933517648766
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 34.99986062549486	eta = 0.690697585128414
eta = 0.690697585128414
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.0305359  0.06422234 0.03005121 0.01042098 0.07415862 0.03538287
 0.01308682 0.04338037 0.0315053  0.02859712]
ene_total = [2.98353072 5.7890883  2.95330941 1.35079969 6.60483527 3.5266156
 1.5604156  3.97716561 3.26691924 2.98718119]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 0 obj = 6.132529161591935
eta = 0.690697585128414
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
eta_min = 0.6906975851284028	eta_max = 0.6906975851284034
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 0.050678425323249086	eta = 0.9090909090909091
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 20.72563143256374	eta = 0.002222913974819694
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.1660187348301907	eta = 0.02127003566846808
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101859368191834	eta = 0.021919304614581324
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101838286505997	eta = 0.02191952446779107
eta = 0.02191952446779107
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [2.02550289e-04 2.06745903e-03 1.94438128e-04 7.90109671e-06
 3.24046959e-03 3.59959346e-04 1.55998826e-05 5.64483901e-04
 2.65926276e-04 1.90664962e-04]
ene_total = [0.17569726 0.2574448  0.17820355 0.16359333 0.29359605 0.22873723
 0.16259842 0.17396099 0.24238129 0.22562536]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 1 obj = 6.132529161591719
eta = 0.6906975851284028
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
Done!
At round 10 energy consumption: 6.132529161591935
At round 10 eta: 0.6906975851284028
At round 10 local rounds: 12.11742183622639
At round 10 global rounds: 80.0418722784263
At round 10 a_n: 24.414598539591886
gradient difference: 0.42577409744262695
train() client id: f_00000-0-0 loss: 1.278399  [   32/  126]
train() client id: f_00000-0-1 loss: 1.461424  [   64/  126]
train() client id: f_00000-0-2 loss: 1.203506  [   96/  126]
train() client id: f_00000-1-0 loss: 1.407009  [   32/  126]
train() client id: f_00000-1-1 loss: 1.039021  [   64/  126]
train() client id: f_00000-1-2 loss: 1.053917  [   96/  126]
train() client id: f_00000-2-0 loss: 1.086880  [   32/  126]
train() client id: f_00000-2-1 loss: 1.102279  [   64/  126]
train() client id: f_00000-2-2 loss: 1.001936  [   96/  126]
train() client id: f_00000-3-0 loss: 1.036065  [   32/  126]
train() client id: f_00000-3-1 loss: 0.993555  [   64/  126]
train() client id: f_00000-3-2 loss: 0.932016  [   96/  126]
train() client id: f_00000-4-0 loss: 0.946479  [   32/  126]
train() client id: f_00000-4-1 loss: 1.029109  [   64/  126]
train() client id: f_00000-4-2 loss: 0.901406  [   96/  126]
train() client id: f_00000-5-0 loss: 0.880797  [   32/  126]
train() client id: f_00000-5-1 loss: 0.912686  [   64/  126]
train() client id: f_00000-5-2 loss: 0.951539  [   96/  126]
train() client id: f_00000-6-0 loss: 0.936162  [   32/  126]
train() client id: f_00000-6-1 loss: 0.938285  [   64/  126]
train() client id: f_00000-6-2 loss: 0.848813  [   96/  126]
train() client id: f_00000-7-0 loss: 0.974676  [   32/  126]
train() client id: f_00000-7-1 loss: 0.898403  [   64/  126]
train() client id: f_00000-7-2 loss: 0.834273  [   96/  126]
train() client id: f_00000-8-0 loss: 0.834166  [   32/  126]
train() client id: f_00000-8-1 loss: 0.923233  [   64/  126]
train() client id: f_00000-8-2 loss: 0.931675  [   96/  126]
train() client id: f_00000-9-0 loss: 0.873041  [   32/  126]
train() client id: f_00000-9-1 loss: 0.879858  [   64/  126]
train() client id: f_00000-9-2 loss: 0.915124  [   96/  126]
train() client id: f_00000-10-0 loss: 0.853533  [   32/  126]
train() client id: f_00000-10-1 loss: 0.875352  [   64/  126]
train() client id: f_00000-10-2 loss: 0.908595  [   96/  126]
train() client id: f_00000-11-0 loss: 0.850224  [   32/  126]
train() client id: f_00000-11-1 loss: 0.869989  [   64/  126]
train() client id: f_00000-11-2 loss: 0.847519  [   96/  126]
train() client id: f_00001-0-0 loss: 0.595022  [   32/  265]
train() client id: f_00001-0-1 loss: 0.552629  [   64/  265]
train() client id: f_00001-0-2 loss: 0.553729  [   96/  265]
train() client id: f_00001-0-3 loss: 0.480665  [  128/  265]
train() client id: f_00001-0-4 loss: 0.528669  [  160/  265]
train() client id: f_00001-0-5 loss: 0.627381  [  192/  265]
train() client id: f_00001-0-6 loss: 0.645819  [  224/  265]
train() client id: f_00001-0-7 loss: 0.552262  [  256/  265]
train() client id: f_00001-1-0 loss: 0.564377  [   32/  265]
train() client id: f_00001-1-1 loss: 0.587171  [   64/  265]
train() client id: f_00001-1-2 loss: 0.546458  [   96/  265]
train() client id: f_00001-1-3 loss: 0.589460  [  128/  265]
train() client id: f_00001-1-4 loss: 0.560175  [  160/  265]
train() client id: f_00001-1-5 loss: 0.573567  [  192/  265]
train() client id: f_00001-1-6 loss: 0.555933  [  224/  265]
train() client id: f_00001-1-7 loss: 0.454152  [  256/  265]
train() client id: f_00001-2-0 loss: 0.565881  [   32/  265]
train() client id: f_00001-2-1 loss: 0.486538  [   64/  265]
train() client id: f_00001-2-2 loss: 0.581508  [   96/  265]
train() client id: f_00001-2-3 loss: 0.622275  [  128/  265]
train() client id: f_00001-2-4 loss: 0.493276  [  160/  265]
train() client id: f_00001-2-5 loss: 0.518869  [  192/  265]
train() client id: f_00001-2-6 loss: 0.470805  [  224/  265]
train() client id: f_00001-2-7 loss: 0.611619  [  256/  265]
train() client id: f_00001-3-0 loss: 0.582097  [   32/  265]
train() client id: f_00001-3-1 loss: 0.583560  [   64/  265]
train() client id: f_00001-3-2 loss: 0.539781  [   96/  265]
train() client id: f_00001-3-3 loss: 0.518762  [  128/  265]
train() client id: f_00001-3-4 loss: 0.433424  [  160/  265]
train() client id: f_00001-3-5 loss: 0.541236  [  192/  265]
train() client id: f_00001-3-6 loss: 0.497949  [  224/  265]
train() client id: f_00001-3-7 loss: 0.537821  [  256/  265]
train() client id: f_00001-4-0 loss: 0.571682  [   32/  265]
train() client id: f_00001-4-1 loss: 0.549081  [   64/  265]
train() client id: f_00001-4-2 loss: 0.544268  [   96/  265]
train() client id: f_00001-4-3 loss: 0.577080  [  128/  265]
train() client id: f_00001-4-4 loss: 0.533585  [  160/  265]
train() client id: f_00001-4-5 loss: 0.436591  [  192/  265]
train() client id: f_00001-4-6 loss: 0.462179  [  224/  265]
train() client id: f_00001-4-7 loss: 0.532737  [  256/  265]
train() client id: f_00001-5-0 loss: 0.509656  [   32/  265]
train() client id: f_00001-5-1 loss: 0.547206  [   64/  265]
train() client id: f_00001-5-2 loss: 0.624187  [   96/  265]
train() client id: f_00001-5-3 loss: 0.529291  [  128/  265]
train() client id: f_00001-5-4 loss: 0.476295  [  160/  265]
train() client id: f_00001-5-5 loss: 0.555363  [  192/  265]
train() client id: f_00001-5-6 loss: 0.484756  [  224/  265]
train() client id: f_00001-5-7 loss: 0.505002  [  256/  265]
train() client id: f_00001-6-0 loss: 0.494235  [   32/  265]
train() client id: f_00001-6-1 loss: 0.549717  [   64/  265]
train() client id: f_00001-6-2 loss: 0.620656  [   96/  265]
train() client id: f_00001-6-3 loss: 0.601976  [  128/  265]
train() client id: f_00001-6-4 loss: 0.508487  [  160/  265]
train() client id: f_00001-6-5 loss: 0.439413  [  192/  265]
train() client id: f_00001-6-6 loss: 0.482806  [  224/  265]
train() client id: f_00001-6-7 loss: 0.506138  [  256/  265]
train() client id: f_00001-7-0 loss: 0.505645  [   32/  265]
train() client id: f_00001-7-1 loss: 0.675271  [   64/  265]
train() client id: f_00001-7-2 loss: 0.519236  [   96/  265]
train() client id: f_00001-7-3 loss: 0.495000  [  128/  265]
train() client id: f_00001-7-4 loss: 0.440437  [  160/  265]
train() client id: f_00001-7-5 loss: 0.525570  [  192/  265]
train() client id: f_00001-7-6 loss: 0.484319  [  224/  265]
train() client id: f_00001-7-7 loss: 0.555261  [  256/  265]
train() client id: f_00001-8-0 loss: 0.568350  [   32/  265]
train() client id: f_00001-8-1 loss: 0.501396  [   64/  265]
train() client id: f_00001-8-2 loss: 0.442320  [   96/  265]
train() client id: f_00001-8-3 loss: 0.617098  [  128/  265]
train() client id: f_00001-8-4 loss: 0.487428  [  160/  265]
train() client id: f_00001-8-5 loss: 0.445674  [  192/  265]
train() client id: f_00001-8-6 loss: 0.539806  [  224/  265]
train() client id: f_00001-8-7 loss: 0.601455  [  256/  265]
train() client id: f_00001-9-0 loss: 0.632519  [   32/  265]
train() client id: f_00001-9-1 loss: 0.453296  [   64/  265]
train() client id: f_00001-9-2 loss: 0.563264  [   96/  265]
train() client id: f_00001-9-3 loss: 0.577804  [  128/  265]
train() client id: f_00001-9-4 loss: 0.498722  [  160/  265]
train() client id: f_00001-9-5 loss: 0.427635  [  192/  265]
train() client id: f_00001-9-6 loss: 0.424373  [  224/  265]
train() client id: f_00001-9-7 loss: 0.616488  [  256/  265]
train() client id: f_00001-10-0 loss: 0.496376  [   32/  265]
train() client id: f_00001-10-1 loss: 0.495467  [   64/  265]
train() client id: f_00001-10-2 loss: 0.481710  [   96/  265]
train() client id: f_00001-10-3 loss: 0.550412  [  128/  265]
train() client id: f_00001-10-4 loss: 0.612387  [  160/  265]
train() client id: f_00001-10-5 loss: 0.529410  [  192/  265]
train() client id: f_00001-10-6 loss: 0.577487  [  224/  265]
train() client id: f_00001-10-7 loss: 0.445473  [  256/  265]
train() client id: f_00001-11-0 loss: 0.440356  [   32/  265]
train() client id: f_00001-11-1 loss: 0.587037  [   64/  265]
train() client id: f_00001-11-2 loss: 0.563236  [   96/  265]
train() client id: f_00001-11-3 loss: 0.483068  [  128/  265]
train() client id: f_00001-11-4 loss: 0.562804  [  160/  265]
train() client id: f_00001-11-5 loss: 0.481067  [  192/  265]
train() client id: f_00001-11-6 loss: 0.554282  [  224/  265]
train() client id: f_00001-11-7 loss: 0.431442  [  256/  265]
train() client id: f_00002-0-0 loss: 1.228250  [   32/  124]
train() client id: f_00002-0-1 loss: 1.189005  [   64/  124]
train() client id: f_00002-0-2 loss: 1.110717  [   96/  124]
train() client id: f_00002-1-0 loss: 1.186351  [   32/  124]
train() client id: f_00002-1-1 loss: 1.085584  [   64/  124]
train() client id: f_00002-1-2 loss: 1.077476  [   96/  124]
train() client id: f_00002-2-0 loss: 1.021329  [   32/  124]
train() client id: f_00002-2-1 loss: 1.174541  [   64/  124]
train() client id: f_00002-2-2 loss: 1.045341  [   96/  124]
train() client id: f_00002-3-0 loss: 1.082613  [   32/  124]
train() client id: f_00002-3-1 loss: 1.041890  [   64/  124]
train() client id: f_00002-3-2 loss: 1.110451  [   96/  124]
train() client id: f_00002-4-0 loss: 1.000195  [   32/  124]
train() client id: f_00002-4-1 loss: 1.028072  [   64/  124]
train() client id: f_00002-4-2 loss: 1.024416  [   96/  124]
train() client id: f_00002-5-0 loss: 1.018452  [   32/  124]
train() client id: f_00002-5-1 loss: 1.037240  [   64/  124]
train() client id: f_00002-5-2 loss: 1.025094  [   96/  124]
train() client id: f_00002-6-0 loss: 1.036988  [   32/  124]
train() client id: f_00002-6-1 loss: 1.003311  [   64/  124]
train() client id: f_00002-6-2 loss: 0.933406  [   96/  124]
train() client id: f_00002-7-0 loss: 1.071550  [   32/  124]
train() client id: f_00002-7-1 loss: 0.887005  [   64/  124]
train() client id: f_00002-7-2 loss: 0.968145  [   96/  124]
train() client id: f_00002-8-0 loss: 0.931872  [   32/  124]
train() client id: f_00002-8-1 loss: 1.000433  [   64/  124]
train() client id: f_00002-8-2 loss: 1.028041  [   96/  124]
train() client id: f_00002-9-0 loss: 0.936712  [   32/  124]
train() client id: f_00002-9-1 loss: 0.959406  [   64/  124]
train() client id: f_00002-9-2 loss: 0.992053  [   96/  124]
train() client id: f_00002-10-0 loss: 0.961449  [   32/  124]
train() client id: f_00002-10-1 loss: 0.925278  [   64/  124]
train() client id: f_00002-10-2 loss: 0.965897  [   96/  124]
train() client id: f_00002-11-0 loss: 0.992328  [   32/  124]
train() client id: f_00002-11-1 loss: 1.009835  [   64/  124]
train() client id: f_00002-11-2 loss: 0.952383  [   96/  124]
train() client id: f_00003-0-0 loss: 0.867182  [   32/   43]
train() client id: f_00003-1-0 loss: 0.791577  [   32/   43]
train() client id: f_00003-2-0 loss: 0.743032  [   32/   43]
train() client id: f_00003-3-0 loss: 0.886307  [   32/   43]
train() client id: f_00003-4-0 loss: 0.819009  [   32/   43]
train() client id: f_00003-5-0 loss: 0.900518  [   32/   43]
train() client id: f_00003-6-0 loss: 0.912125  [   32/   43]
train() client id: f_00003-7-0 loss: 0.761459  [   32/   43]
train() client id: f_00003-8-0 loss: 0.745927  [   32/   43]
train() client id: f_00003-9-0 loss: 0.890096  [   32/   43]
train() client id: f_00003-10-0 loss: 0.795099  [   32/   43]
train() client id: f_00003-11-0 loss: 0.873035  [   32/   43]
train() client id: f_00004-0-0 loss: 0.846116  [   32/  306]
train() client id: f_00004-0-1 loss: 1.021149  [   64/  306]
train() client id: f_00004-0-2 loss: 0.873221  [   96/  306]
train() client id: f_00004-0-3 loss: 0.915387  [  128/  306]
train() client id: f_00004-0-4 loss: 0.870763  [  160/  306]
train() client id: f_00004-0-5 loss: 1.005754  [  192/  306]
train() client id: f_00004-0-6 loss: 0.799747  [  224/  306]
train() client id: f_00004-0-7 loss: 0.855060  [  256/  306]
train() client id: f_00004-0-8 loss: 0.857705  [  288/  306]
train() client id: f_00004-1-0 loss: 0.805687  [   32/  306]
train() client id: f_00004-1-1 loss: 0.854763  [   64/  306]
train() client id: f_00004-1-2 loss: 0.869832  [   96/  306]
train() client id: f_00004-1-3 loss: 0.817221  [  128/  306]
train() client id: f_00004-1-4 loss: 0.885549  [  160/  306]
train() client id: f_00004-1-5 loss: 1.214240  [  192/  306]
train() client id: f_00004-1-6 loss: 0.899292  [  224/  306]
train() client id: f_00004-1-7 loss: 0.899602  [  256/  306]
train() client id: f_00004-1-8 loss: 0.896016  [  288/  306]
train() client id: f_00004-2-0 loss: 0.836751  [   32/  306]
train() client id: f_00004-2-1 loss: 0.734068  [   64/  306]
train() client id: f_00004-2-2 loss: 0.815039  [   96/  306]
train() client id: f_00004-2-3 loss: 0.954510  [  128/  306]
train() client id: f_00004-2-4 loss: 0.975047  [  160/  306]
train() client id: f_00004-2-5 loss: 0.845936  [  192/  306]
train() client id: f_00004-2-6 loss: 0.961772  [  224/  306]
train() client id: f_00004-2-7 loss: 0.961416  [  256/  306]
train() client id: f_00004-2-8 loss: 1.092645  [  288/  306]
train() client id: f_00004-3-0 loss: 0.936648  [   32/  306]
train() client id: f_00004-3-1 loss: 0.898363  [   64/  306]
train() client id: f_00004-3-2 loss: 0.845941  [   96/  306]
train() client id: f_00004-3-3 loss: 0.930174  [  128/  306]
train() client id: f_00004-3-4 loss: 0.848047  [  160/  306]
train() client id: f_00004-3-5 loss: 0.891224  [  192/  306]
train() client id: f_00004-3-6 loss: 0.920551  [  224/  306]
train() client id: f_00004-3-7 loss: 0.907490  [  256/  306]
train() client id: f_00004-3-8 loss: 0.855090  [  288/  306]
train() client id: f_00004-4-0 loss: 0.935469  [   32/  306]
train() client id: f_00004-4-1 loss: 0.823448  [   64/  306]
train() client id: f_00004-4-2 loss: 0.818285  [   96/  306]
train() client id: f_00004-4-3 loss: 0.878330  [  128/  306]
train() client id: f_00004-4-4 loss: 0.904269  [  160/  306]
train() client id: f_00004-4-5 loss: 0.842713  [  192/  306]
train() client id: f_00004-4-6 loss: 0.914567  [  224/  306]
train() client id: f_00004-4-7 loss: 1.068974  [  256/  306]
train() client id: f_00004-4-8 loss: 0.900071  [  288/  306]
train() client id: f_00004-5-0 loss: 0.914448  [   32/  306]
train() client id: f_00004-5-1 loss: 0.865020  [   64/  306]
train() client id: f_00004-5-2 loss: 0.849552  [   96/  306]
train() client id: f_00004-5-3 loss: 0.989746  [  128/  306]
train() client id: f_00004-5-4 loss: 0.883531  [  160/  306]
train() client id: f_00004-5-5 loss: 0.845497  [  192/  306]
train() client id: f_00004-5-6 loss: 0.893791  [  224/  306]
train() client id: f_00004-5-7 loss: 0.883705  [  256/  306]
train() client id: f_00004-5-8 loss: 0.865125  [  288/  306]
train() client id: f_00004-6-0 loss: 0.852379  [   32/  306]
train() client id: f_00004-6-1 loss: 0.940997  [   64/  306]
train() client id: f_00004-6-2 loss: 0.925509  [   96/  306]
train() client id: f_00004-6-3 loss: 0.838503  [  128/  306]
train() client id: f_00004-6-4 loss: 0.884348  [  160/  306]
train() client id: f_00004-6-5 loss: 0.919054  [  192/  306]
train() client id: f_00004-6-6 loss: 0.778757  [  224/  306]
train() client id: f_00004-6-7 loss: 0.923785  [  256/  306]
train() client id: f_00004-6-8 loss: 0.932117  [  288/  306]
train() client id: f_00004-7-0 loss: 0.888825  [   32/  306]
train() client id: f_00004-7-1 loss: 0.825325  [   64/  306]
train() client id: f_00004-7-2 loss: 0.949442  [   96/  306]
train() client id: f_00004-7-3 loss: 0.913744  [  128/  306]
train() client id: f_00004-7-4 loss: 0.918714  [  160/  306]
train() client id: f_00004-7-5 loss: 0.846502  [  192/  306]
train() client id: f_00004-7-6 loss: 0.930896  [  224/  306]
train() client id: f_00004-7-7 loss: 0.854615  [  256/  306]
train() client id: f_00004-7-8 loss: 0.863156  [  288/  306]
train() client id: f_00004-8-0 loss: 0.795406  [   32/  306]
train() client id: f_00004-8-1 loss: 0.804686  [   64/  306]
train() client id: f_00004-8-2 loss: 0.947201  [   96/  306]
train() client id: f_00004-8-3 loss: 0.861672  [  128/  306]
train() client id: f_00004-8-4 loss: 1.004269  [  160/  306]
train() client id: f_00004-8-5 loss: 0.848289  [  192/  306]
train() client id: f_00004-8-6 loss: 0.918464  [  224/  306]
train() client id: f_00004-8-7 loss: 0.985633  [  256/  306]
train() client id: f_00004-8-8 loss: 0.862688  [  288/  306]
train() client id: f_00004-9-0 loss: 0.891383  [   32/  306]
train() client id: f_00004-9-1 loss: 0.844683  [   64/  306]
train() client id: f_00004-9-2 loss: 0.767913  [   96/  306]
train() client id: f_00004-9-3 loss: 0.941630  [  128/  306]
train() client id: f_00004-9-4 loss: 0.876394  [  160/  306]
train() client id: f_00004-9-5 loss: 0.835496  [  192/  306]
train() client id: f_00004-9-6 loss: 0.931266  [  224/  306]
train() client id: f_00004-9-7 loss: 0.851284  [  256/  306]
train() client id: f_00004-9-8 loss: 1.013302  [  288/  306]
train() client id: f_00004-10-0 loss: 0.884847  [   32/  306]
train() client id: f_00004-10-1 loss: 0.858779  [   64/  306]
train() client id: f_00004-10-2 loss: 0.939857  [   96/  306]
train() client id: f_00004-10-3 loss: 0.984308  [  128/  306]
train() client id: f_00004-10-4 loss: 0.821591  [  160/  306]
train() client id: f_00004-10-5 loss: 0.841755  [  192/  306]
train() client id: f_00004-10-6 loss: 0.957566  [  224/  306]
train() client id: f_00004-10-7 loss: 0.870844  [  256/  306]
train() client id: f_00004-10-8 loss: 0.914352  [  288/  306]
train() client id: f_00004-11-0 loss: 0.849767  [   32/  306]
train() client id: f_00004-11-1 loss: 0.988799  [   64/  306]
train() client id: f_00004-11-2 loss: 0.822709  [   96/  306]
train() client id: f_00004-11-3 loss: 0.908798  [  128/  306]
train() client id: f_00004-11-4 loss: 0.922336  [  160/  306]
train() client id: f_00004-11-5 loss: 0.876778  [  192/  306]
train() client id: f_00004-11-6 loss: 0.902984  [  224/  306]
train() client id: f_00004-11-7 loss: 0.958389  [  256/  306]
train() client id: f_00004-11-8 loss: 0.876255  [  288/  306]
train() client id: f_00005-0-0 loss: 0.717265  [   32/  146]
train() client id: f_00005-0-1 loss: 0.705130  [   64/  146]
train() client id: f_00005-0-2 loss: 0.775040  [   96/  146]
train() client id: f_00005-0-3 loss: 0.650417  [  128/  146]
train() client id: f_00005-1-0 loss: 0.594968  [   32/  146]
train() client id: f_00005-1-1 loss: 0.681238  [   64/  146]
train() client id: f_00005-1-2 loss: 0.635841  [   96/  146]
train() client id: f_00005-1-3 loss: 0.688328  [  128/  146]
train() client id: f_00005-2-0 loss: 0.815437  [   32/  146]
train() client id: f_00005-2-1 loss: 0.649973  [   64/  146]
train() client id: f_00005-2-2 loss: 0.658872  [   96/  146]
train() client id: f_00005-2-3 loss: 0.642075  [  128/  146]
train() client id: f_00005-3-0 loss: 0.555770  [   32/  146]
train() client id: f_00005-3-1 loss: 0.666189  [   64/  146]
train() client id: f_00005-3-2 loss: 0.634462  [   96/  146]
train() client id: f_00005-3-3 loss: 0.743579  [  128/  146]
train() client id: f_00005-4-0 loss: 0.777167  [   32/  146]
train() client id: f_00005-4-1 loss: 0.483888  [   64/  146]
train() client id: f_00005-4-2 loss: 0.723271  [   96/  146]
train() client id: f_00005-4-3 loss: 0.802439  [  128/  146]
train() client id: f_00005-5-0 loss: 0.499780  [   32/  146]
train() client id: f_00005-5-1 loss: 0.749699  [   64/  146]
train() client id: f_00005-5-2 loss: 0.664010  [   96/  146]
train() client id: f_00005-5-3 loss: 0.741681  [  128/  146]
train() client id: f_00005-6-0 loss: 0.471412  [   32/  146]
train() client id: f_00005-6-1 loss: 0.774850  [   64/  146]
train() client id: f_00005-6-2 loss: 0.625166  [   96/  146]
train() client id: f_00005-6-3 loss: 0.829982  [  128/  146]
train() client id: f_00005-7-0 loss: 0.787802  [   32/  146]
train() client id: f_00005-7-1 loss: 0.542744  [   64/  146]
train() client id: f_00005-7-2 loss: 0.562665  [   96/  146]
train() client id: f_00005-7-3 loss: 0.733213  [  128/  146]
train() client id: f_00005-8-0 loss: 0.668314  [   32/  146]
train() client id: f_00005-8-1 loss: 0.597875  [   64/  146]
train() client id: f_00005-8-2 loss: 0.635180  [   96/  146]
train() client id: f_00005-8-3 loss: 0.689144  [  128/  146]
train() client id: f_00005-9-0 loss: 0.663339  [   32/  146]
train() client id: f_00005-9-1 loss: 0.694176  [   64/  146]
train() client id: f_00005-9-2 loss: 0.501896  [   96/  146]
train() client id: f_00005-9-3 loss: 0.707146  [  128/  146]
train() client id: f_00005-10-0 loss: 0.687931  [   32/  146]
train() client id: f_00005-10-1 loss: 0.880864  [   64/  146]
train() client id: f_00005-10-2 loss: 0.664666  [   96/  146]
train() client id: f_00005-10-3 loss: 0.516082  [  128/  146]
train() client id: f_00005-11-0 loss: 0.644090  [   32/  146]
train() client id: f_00005-11-1 loss: 0.669277  [   64/  146]
train() client id: f_00005-11-2 loss: 0.766431  [   96/  146]
train() client id: f_00005-11-3 loss: 0.558897  [  128/  146]
train() client id: f_00006-0-0 loss: 0.684467  [   32/   54]
train() client id: f_00006-1-0 loss: 0.778406  [   32/   54]
train() client id: f_00006-2-0 loss: 0.780534  [   32/   54]
train() client id: f_00006-3-0 loss: 0.737424  [   32/   54]
train() client id: f_00006-4-0 loss: 0.786846  [   32/   54]
train() client id: f_00006-5-0 loss: 0.739156  [   32/   54]
train() client id: f_00006-6-0 loss: 0.789377  [   32/   54]
train() client id: f_00006-7-0 loss: 0.742120  [   32/   54]
train() client id: f_00006-8-0 loss: 0.743818  [   32/   54]
train() client id: f_00006-9-0 loss: 0.787617  [   32/   54]
train() client id: f_00006-10-0 loss: 0.773617  [   32/   54]
train() client id: f_00006-11-0 loss: 0.771688  [   32/   54]
train() client id: f_00007-0-0 loss: 0.497077  [   32/  179]
train() client id: f_00007-0-1 loss: 0.568301  [   64/  179]
train() client id: f_00007-0-2 loss: 0.506530  [   96/  179]
train() client id: f_00007-0-3 loss: 0.728439  [  128/  179]
train() client id: f_00007-0-4 loss: 0.652774  [  160/  179]
train() client id: f_00007-1-0 loss: 0.565117  [   32/  179]
train() client id: f_00007-1-1 loss: 0.666325  [   64/  179]
train() client id: f_00007-1-2 loss: 0.484393  [   96/  179]
train() client id: f_00007-1-3 loss: 0.520801  [  128/  179]
train() client id: f_00007-1-4 loss: 0.484273  [  160/  179]
train() client id: f_00007-2-0 loss: 0.502908  [   32/  179]
train() client id: f_00007-2-1 loss: 0.651206  [   64/  179]
train() client id: f_00007-2-2 loss: 0.504483  [   96/  179]
train() client id: f_00007-2-3 loss: 0.593598  [  128/  179]
train() client id: f_00007-2-4 loss: 0.497793  [  160/  179]
train() client id: f_00007-3-0 loss: 0.531886  [   32/  179]
train() client id: f_00007-3-1 loss: 0.571869  [   64/  179]
train() client id: f_00007-3-2 loss: 0.547548  [   96/  179]
train() client id: f_00007-3-3 loss: 0.484020  [  128/  179]
train() client id: f_00007-3-4 loss: 0.489827  [  160/  179]
train() client id: f_00007-4-0 loss: 0.667257  [   32/  179]
train() client id: f_00007-4-1 loss: 0.434387  [   64/  179]
train() client id: f_00007-4-2 loss: 0.588873  [   96/  179]
train() client id: f_00007-4-3 loss: 0.440804  [  128/  179]
train() client id: f_00007-4-4 loss: 0.486658  [  160/  179]
train() client id: f_00007-5-0 loss: 0.556713  [   32/  179]
train() client id: f_00007-5-1 loss: 0.612139  [   64/  179]
train() client id: f_00007-5-2 loss: 0.538208  [   96/  179]
train() client id: f_00007-5-3 loss: 0.400880  [  128/  179]
train() client id: f_00007-5-4 loss: 0.398261  [  160/  179]
train() client id: f_00007-6-0 loss: 0.391148  [   32/  179]
train() client id: f_00007-6-1 loss: 0.562125  [   64/  179]
train() client id: f_00007-6-2 loss: 0.473088  [   96/  179]
train() client id: f_00007-6-3 loss: 0.376831  [  128/  179]
train() client id: f_00007-6-4 loss: 0.660921  [  160/  179]
train() client id: f_00007-7-0 loss: 0.550618  [   32/  179]
train() client id: f_00007-7-1 loss: 0.448438  [   64/  179]
train() client id: f_00007-7-2 loss: 0.441360  [   96/  179]
train() client id: f_00007-7-3 loss: 0.459359  [  128/  179]
train() client id: f_00007-7-4 loss: 0.445518  [  160/  179]
train() client id: f_00007-8-0 loss: 0.520950  [   32/  179]
train() client id: f_00007-8-1 loss: 0.612193  [   64/  179]
train() client id: f_00007-8-2 loss: 0.541043  [   96/  179]
train() client id: f_00007-8-3 loss: 0.363897  [  128/  179]
train() client id: f_00007-8-4 loss: 0.374201  [  160/  179]
train() client id: f_00007-9-0 loss: 0.474289  [   32/  179]
train() client id: f_00007-9-1 loss: 0.529556  [   64/  179]
train() client id: f_00007-9-2 loss: 0.369965  [   96/  179]
train() client id: f_00007-9-3 loss: 0.505899  [  128/  179]
train() client id: f_00007-9-4 loss: 0.434631  [  160/  179]
train() client id: f_00007-10-0 loss: 0.393556  [   32/  179]
train() client id: f_00007-10-1 loss: 0.498560  [   64/  179]
train() client id: f_00007-10-2 loss: 0.575421  [   96/  179]
train() client id: f_00007-10-3 loss: 0.595063  [  128/  179]
train() client id: f_00007-10-4 loss: 0.374237  [  160/  179]
train() client id: f_00007-11-0 loss: 0.491088  [   32/  179]
train() client id: f_00007-11-1 loss: 0.550781  [   64/  179]
train() client id: f_00007-11-2 loss: 0.599557  [   96/  179]
train() client id: f_00007-11-3 loss: 0.360973  [  128/  179]
train() client id: f_00007-11-4 loss: 0.415385  [  160/  179]
train() client id: f_00008-0-0 loss: 0.716353  [   32/  130]
train() client id: f_00008-0-1 loss: 0.773027  [   64/  130]
train() client id: f_00008-0-2 loss: 0.726670  [   96/  130]
train() client id: f_00008-0-3 loss: 0.626143  [  128/  130]
train() client id: f_00008-1-0 loss: 0.627048  [   32/  130]
train() client id: f_00008-1-1 loss: 0.810825  [   64/  130]
train() client id: f_00008-1-2 loss: 0.532255  [   96/  130]
train() client id: f_00008-1-3 loss: 0.865303  [  128/  130]
train() client id: f_00008-2-0 loss: 0.723312  [   32/  130]
train() client id: f_00008-2-1 loss: 0.658490  [   64/  130]
train() client id: f_00008-2-2 loss: 0.714133  [   96/  130]
train() client id: f_00008-2-3 loss: 0.777040  [  128/  130]
train() client id: f_00008-3-0 loss: 0.781137  [   32/  130]
train() client id: f_00008-3-1 loss: 0.602965  [   64/  130]
train() client id: f_00008-3-2 loss: 0.806928  [   96/  130]
train() client id: f_00008-3-3 loss: 0.678125  [  128/  130]
train() client id: f_00008-4-0 loss: 0.770684  [   32/  130]
train() client id: f_00008-4-1 loss: 0.740776  [   64/  130]
train() client id: f_00008-4-2 loss: 0.675122  [   96/  130]
train() client id: f_00008-4-3 loss: 0.684248  [  128/  130]
train() client id: f_00008-5-0 loss: 0.746108  [   32/  130]
train() client id: f_00008-5-1 loss: 0.811518  [   64/  130]
train() client id: f_00008-5-2 loss: 0.620436  [   96/  130]
train() client id: f_00008-5-3 loss: 0.671474  [  128/  130]
train() client id: f_00008-6-0 loss: 0.757019  [   32/  130]
train() client id: f_00008-6-1 loss: 0.799238  [   64/  130]
train() client id: f_00008-6-2 loss: 0.679416  [   96/  130]
train() client id: f_00008-6-3 loss: 0.633828  [  128/  130]
train() client id: f_00008-7-0 loss: 0.733326  [   32/  130]
train() client id: f_00008-7-1 loss: 0.629197  [   64/  130]
train() client id: f_00008-7-2 loss: 0.757217  [   96/  130]
train() client id: f_00008-7-3 loss: 0.735189  [  128/  130]
train() client id: f_00008-8-0 loss: 0.699422  [   32/  130]
train() client id: f_00008-8-1 loss: 0.720988  [   64/  130]
train() client id: f_00008-8-2 loss: 0.790625  [   96/  130]
train() client id: f_00008-8-3 loss: 0.643407  [  128/  130]
train() client id: f_00008-9-0 loss: 0.648200  [   32/  130]
train() client id: f_00008-9-1 loss: 0.710627  [   64/  130]
train() client id: f_00008-9-2 loss: 0.752690  [   96/  130]
train() client id: f_00008-9-3 loss: 0.707665  [  128/  130]
train() client id: f_00008-10-0 loss: 0.767079  [   32/  130]
train() client id: f_00008-10-1 loss: 0.718267  [   64/  130]
train() client id: f_00008-10-2 loss: 0.677766  [   96/  130]
train() client id: f_00008-10-3 loss: 0.692047  [  128/  130]
train() client id: f_00008-11-0 loss: 0.736300  [   32/  130]
train() client id: f_00008-11-1 loss: 0.635361  [   64/  130]
train() client id: f_00008-11-2 loss: 0.870171  [   96/  130]
train() client id: f_00008-11-3 loss: 0.587926  [  128/  130]
train() client id: f_00009-0-0 loss: 1.208927  [   32/  118]
train() client id: f_00009-0-1 loss: 1.252142  [   64/  118]
train() client id: f_00009-0-2 loss: 1.260480  [   96/  118]
train() client id: f_00009-1-0 loss: 1.208114  [   32/  118]
train() client id: f_00009-1-1 loss: 1.139009  [   64/  118]
train() client id: f_00009-1-2 loss: 1.190977  [   96/  118]
train() client id: f_00009-2-0 loss: 1.235638  [   32/  118]
train() client id: f_00009-2-1 loss: 1.140567  [   64/  118]
train() client id: f_00009-2-2 loss: 1.145760  [   96/  118]
train() client id: f_00009-3-0 loss: 1.141783  [   32/  118]
train() client id: f_00009-3-1 loss: 1.072268  [   64/  118]
train() client id: f_00009-3-2 loss: 1.110221  [   96/  118]
train() client id: f_00009-4-0 loss: 1.057112  [   32/  118]
train() client id: f_00009-4-1 loss: 1.134138  [   64/  118]
train() client id: f_00009-4-2 loss: 1.050974  [   96/  118]
train() client id: f_00009-5-0 loss: 1.078232  [   32/  118]
train() client id: f_00009-5-1 loss: 1.002315  [   64/  118]
train() client id: f_00009-5-2 loss: 1.064009  [   96/  118]
train() client id: f_00009-6-0 loss: 1.023364  [   32/  118]
train() client id: f_00009-6-1 loss: 0.959143  [   64/  118]
train() client id: f_00009-6-2 loss: 1.058638  [   96/  118]
train() client id: f_00009-7-0 loss: 0.978296  [   32/  118]
train() client id: f_00009-7-1 loss: 1.092893  [   64/  118]
train() client id: f_00009-7-2 loss: 1.003482  [   96/  118]
train() client id: f_00009-8-0 loss: 1.012753  [   32/  118]
train() client id: f_00009-8-1 loss: 0.991943  [   64/  118]
train() client id: f_00009-8-2 loss: 1.019400  [   96/  118]
train() client id: f_00009-9-0 loss: 0.955687  [   32/  118]
train() client id: f_00009-9-1 loss: 1.032310  [   64/  118]
train() client id: f_00009-9-2 loss: 0.970837  [   96/  118]
train() client id: f_00009-10-0 loss: 1.044899  [   32/  118]
train() client id: f_00009-10-1 loss: 0.995146  [   64/  118]
train() client id: f_00009-10-2 loss: 0.948352  [   96/  118]
train() client id: f_00009-11-0 loss: 1.025346  [   32/  118]
train() client id: f_00009-11-1 loss: 0.976941  [   64/  118]
train() client id: f_00009-11-2 loss: 0.982292  [   96/  118]
At round 10 accuracy: 0.6312997347480106
At round 10 training accuracy: 0.5788061703554661
At round 10 training loss: 0.8585238343568196
update_location
xs = [ -3.9056584    4.20031788  70.00902392  18.81129433   0.97929623
   3.95640986 -32.44319194 -11.32485185  54.66397685  -2.06087855]
ys = [ 62.5879595   45.55583871   1.32061395 -32.45517586  24.35018685
   7.81415074  -2.62498432   0.82234798  17.56900603  -0.99851822]
dists_uav = [118.03604044 109.96807314 122.07787453 106.80450943 102.92662736
 100.38283783 105.16392559 100.64257809 115.31183954 100.02621786]
dists_bs = [204.71868644 221.06230981 300.3230776  283.90221521 231.65508607
 244.90101025 227.75662137 239.01340671 278.44220249 246.74565476]
uav_gains = [6.60612004e-11 7.88546405e-11 6.07267938e-11 8.48246225e-11
 9.30418280e-11 9.90488970e-11 8.81718085e-11 9.84110509e-11
 7.00330537e-11 9.99341172e-11]
bs_gains = [3.73295817e-11 3.01059902e-11 1.27657316e-11 1.49424392e-11
 2.64080865e-11 2.26006261e-11 2.76933346e-11 2.41942270e-11
 1.57774174e-11 2.21307151e-11]
Round 11
-------------------------------
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.53941363 19.90793381  9.39345483  3.35836813 22.96346205 11.07093228
  4.17536772 13.46928103  9.90106646  8.98947369]
obj_prev = 112.76875362310263
eta_min = 1.8259794231621306e-10	eta_max = 0.9205107351620903
af = 23.839837477165666	bf = 1.854557298226717	zeta = 26.223821224882236	eta = 0.909090909090909
af = 23.839837477165666	bf = 1.854557298226717	zeta = 45.394101441493795	eta = 0.5251747852723034
af = 23.839837477165666	bf = 1.854557298226717	zeta = 36.24067924685511	eta = 0.6578198304391449
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.59948643670541	eta = 0.6890228709254712
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51842070411875	eta = 0.6906410255994443
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51820885585075	eta = 0.6906452642639038
eta = 0.6906452642639038
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.03054215 0.06423548 0.03005736 0.01042312 0.0741738  0.03539011
 0.01308949 0.04338925 0.03151175 0.02860297]
ene_total = [2.94794512 5.70363107 2.91862696 1.33519898 6.50745954 3.47131979
 1.5419756  3.92315854 3.2297488  2.93914446]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 0 obj = 6.04741367166706
eta = 0.6906452642639038
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
eta_min = 0.6906452642638832	eta_max = 0.6906452642639023
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 0.04826057494319857	eta = 0.909090909090909
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 20.44612752124682	eta = 0.0021457975307437047
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.1286929945616637	eta = 0.020610416842846153
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.067483129885282	eta = 0.02122060843649869
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.0674642356817103	eta = 0.021220802368025433
eta = 0.021220802368025433
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.99188226e-04 2.01117734e-03 1.91271134e-04 7.75548441e-06
 3.14943156e-03 3.49577711e-04 1.53136962e-05 5.53174501e-04
 2.61453995e-04 1.85124181e-04]
ene_total = [0.17486013 0.25058139 0.17748901 0.16220541 0.28513266 0.22310299
 0.16123818 0.17146061 0.24129522 0.22009864]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 1 obj = 6.047413671666661
eta = 0.6906452642638832
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
Done!
At round 11 energy consumption: 6.04741367166706
At round 11 eta: 0.6906452642638832
At round 11 local rounds: 12.119902394804324
At round 11 global rounds: 78.921043447086
At round 11 a_n: 24.072052692622567
gradient difference: 0.4705381989479065
train() client id: f_00000-0-0 loss: 1.501185  [   32/  126]
train() client id: f_00000-0-1 loss: 1.456505  [   64/  126]
train() client id: f_00000-0-2 loss: 1.428051  [   96/  126]
train() client id: f_00000-1-0 loss: 1.454732  [   32/  126]
train() client id: f_00000-1-1 loss: 1.222852  [   64/  126]
train() client id: f_00000-1-2 loss: 1.283040  [   96/  126]
train() client id: f_00000-2-0 loss: 1.399996  [   32/  126]
train() client id: f_00000-2-1 loss: 1.078051  [   64/  126]
train() client id: f_00000-2-2 loss: 1.171669  [   96/  126]
train() client id: f_00000-3-0 loss: 1.248921  [   32/  126]
train() client id: f_00000-3-1 loss: 1.093758  [   64/  126]
train() client id: f_00000-3-2 loss: 1.128386  [   96/  126]
train() client id: f_00000-4-0 loss: 1.063527  [   32/  126]
train() client id: f_00000-4-1 loss: 1.151020  [   64/  126]
train() client id: f_00000-4-2 loss: 1.086650  [   96/  126]
train() client id: f_00000-5-0 loss: 1.021572  [   32/  126]
train() client id: f_00000-5-1 loss: 1.024343  [   64/  126]
train() client id: f_00000-5-2 loss: 1.010714  [   96/  126]
train() client id: f_00000-6-0 loss: 1.034746  [   32/  126]
train() client id: f_00000-6-1 loss: 0.973900  [   64/  126]
train() client id: f_00000-6-2 loss: 1.018490  [   96/  126]
train() client id: f_00000-7-0 loss: 1.006801  [   32/  126]
train() client id: f_00000-7-1 loss: 0.991141  [   64/  126]
train() client id: f_00000-7-2 loss: 0.988149  [   96/  126]
train() client id: f_00000-8-0 loss: 0.960326  [   32/  126]
train() client id: f_00000-8-1 loss: 0.933738  [   64/  126]
train() client id: f_00000-8-2 loss: 1.002085  [   96/  126]
train() client id: f_00000-9-0 loss: 0.878110  [   32/  126]
train() client id: f_00000-9-1 loss: 0.975479  [   64/  126]
train() client id: f_00000-9-2 loss: 0.993714  [   96/  126]
train() client id: f_00000-10-0 loss: 0.899900  [   32/  126]
train() client id: f_00000-10-1 loss: 0.998106  [   64/  126]
train() client id: f_00000-10-2 loss: 0.982371  [   96/  126]
train() client id: f_00000-11-0 loss: 0.917563  [   32/  126]
train() client id: f_00000-11-1 loss: 0.935253  [   64/  126]
train() client id: f_00000-11-2 loss: 0.983081  [   96/  126]
train() client id: f_00001-0-0 loss: 0.685540  [   32/  265]
train() client id: f_00001-0-1 loss: 0.550221  [   64/  265]
train() client id: f_00001-0-2 loss: 0.524112  [   96/  265]
train() client id: f_00001-0-3 loss: 0.480949  [  128/  265]
train() client id: f_00001-0-4 loss: 0.549785  [  160/  265]
train() client id: f_00001-0-5 loss: 0.497912  [  192/  265]
train() client id: f_00001-0-6 loss: 0.615781  [  224/  265]
train() client id: f_00001-0-7 loss: 0.647985  [  256/  265]
train() client id: f_00001-1-0 loss: 0.563539  [   32/  265]
train() client id: f_00001-1-1 loss: 0.518819  [   64/  265]
train() client id: f_00001-1-2 loss: 0.513636  [   96/  265]
train() client id: f_00001-1-3 loss: 0.689464  [  128/  265]
train() client id: f_00001-1-4 loss: 0.589775  [  160/  265]
train() client id: f_00001-1-5 loss: 0.475971  [  192/  265]
train() client id: f_00001-1-6 loss: 0.553766  [  224/  265]
train() client id: f_00001-1-7 loss: 0.551147  [  256/  265]
train() client id: f_00001-2-0 loss: 0.592720  [   32/  265]
train() client id: f_00001-2-1 loss: 0.592659  [   64/  265]
train() client id: f_00001-2-2 loss: 0.544767  [   96/  265]
train() client id: f_00001-2-3 loss: 0.547913  [  128/  265]
train() client id: f_00001-2-4 loss: 0.569971  [  160/  265]
train() client id: f_00001-2-5 loss: 0.481500  [  192/  265]
train() client id: f_00001-2-6 loss: 0.541445  [  224/  265]
train() client id: f_00001-2-7 loss: 0.522421  [  256/  265]
train() client id: f_00001-3-0 loss: 0.537048  [   32/  265]
train() client id: f_00001-3-1 loss: 0.506479  [   64/  265]
train() client id: f_00001-3-2 loss: 0.523768  [   96/  265]
train() client id: f_00001-3-3 loss: 0.663567  [  128/  265]
train() client id: f_00001-3-4 loss: 0.559499  [  160/  265]
train() client id: f_00001-3-5 loss: 0.565134  [  192/  265]
train() client id: f_00001-3-6 loss: 0.521184  [  224/  265]
train() client id: f_00001-3-7 loss: 0.515399  [  256/  265]
train() client id: f_00001-4-0 loss: 0.647480  [   32/  265]
train() client id: f_00001-4-1 loss: 0.528116  [   64/  265]
train() client id: f_00001-4-2 loss: 0.450332  [   96/  265]
train() client id: f_00001-4-3 loss: 0.515614  [  128/  265]
train() client id: f_00001-4-4 loss: 0.559573  [  160/  265]
train() client id: f_00001-4-5 loss: 0.529928  [  192/  265]
train() client id: f_00001-4-6 loss: 0.550524  [  224/  265]
train() client id: f_00001-4-7 loss: 0.586298  [  256/  265]
train() client id: f_00001-5-0 loss: 0.480370  [   32/  265]
train() client id: f_00001-5-1 loss: 0.579721  [   64/  265]
train() client id: f_00001-5-2 loss: 0.581737  [   96/  265]
train() client id: f_00001-5-3 loss: 0.533386  [  128/  265]
train() client id: f_00001-5-4 loss: 0.510811  [  160/  265]
train() client id: f_00001-5-5 loss: 0.503548  [  192/  265]
train() client id: f_00001-5-6 loss: 0.576809  [  224/  265]
train() client id: f_00001-5-7 loss: 0.576032  [  256/  265]
train() client id: f_00001-6-0 loss: 0.597037  [   32/  265]
train() client id: f_00001-6-1 loss: 0.449629  [   64/  265]
train() client id: f_00001-6-2 loss: 0.492062  [   96/  265]
train() client id: f_00001-6-3 loss: 0.550896  [  128/  265]
train() client id: f_00001-6-4 loss: 0.462555  [  160/  265]
train() client id: f_00001-6-5 loss: 0.561243  [  192/  265]
train() client id: f_00001-6-6 loss: 0.615374  [  224/  265]
train() client id: f_00001-6-7 loss: 0.583325  [  256/  265]
train() client id: f_00001-7-0 loss: 0.608525  [   32/  265]
train() client id: f_00001-7-1 loss: 0.471621  [   64/  265]
train() client id: f_00001-7-2 loss: 0.587848  [   96/  265]
train() client id: f_00001-7-3 loss: 0.555989  [  128/  265]
train() client id: f_00001-7-4 loss: 0.454588  [  160/  265]
train() client id: f_00001-7-5 loss: 0.486719  [  192/  265]
train() client id: f_00001-7-6 loss: 0.517840  [  224/  265]
train() client id: f_00001-7-7 loss: 0.638936  [  256/  265]
train() client id: f_00001-8-0 loss: 0.603120  [   32/  265]
train() client id: f_00001-8-1 loss: 0.559742  [   64/  265]
train() client id: f_00001-8-2 loss: 0.475056  [   96/  265]
train() client id: f_00001-8-3 loss: 0.458774  [  128/  265]
train() client id: f_00001-8-4 loss: 0.657220  [  160/  265]
train() client id: f_00001-8-5 loss: 0.509081  [  192/  265]
train() client id: f_00001-8-6 loss: 0.542219  [  224/  265]
train() client id: f_00001-8-7 loss: 0.499259  [  256/  265]
train() client id: f_00001-9-0 loss: 0.582068  [   32/  265]
train() client id: f_00001-9-1 loss: 0.782572  [   64/  265]
train() client id: f_00001-9-2 loss: 0.441654  [   96/  265]
train() client id: f_00001-9-3 loss: 0.449407  [  128/  265]
train() client id: f_00001-9-4 loss: 0.506367  [  160/  265]
train() client id: f_00001-9-5 loss: 0.494193  [  192/  265]
train() client id: f_00001-9-6 loss: 0.558550  [  224/  265]
train() client id: f_00001-9-7 loss: 0.437220  [  256/  265]
train() client id: f_00001-10-0 loss: 0.509507  [   32/  265]
train() client id: f_00001-10-1 loss: 0.624979  [   64/  265]
train() client id: f_00001-10-2 loss: 0.457989  [   96/  265]
train() client id: f_00001-10-3 loss: 0.501883  [  128/  265]
train() client id: f_00001-10-4 loss: 0.601427  [  160/  265]
train() client id: f_00001-10-5 loss: 0.638449  [  192/  265]
train() client id: f_00001-10-6 loss: 0.442769  [  224/  265]
train() client id: f_00001-10-7 loss: 0.531257  [  256/  265]
train() client id: f_00001-11-0 loss: 0.627087  [   32/  265]
train() client id: f_00001-11-1 loss: 0.525132  [   64/  265]
train() client id: f_00001-11-2 loss: 0.551667  [   96/  265]
train() client id: f_00001-11-3 loss: 0.456539  [  128/  265]
train() client id: f_00001-11-4 loss: 0.580141  [  160/  265]
train() client id: f_00001-11-5 loss: 0.571206  [  192/  265]
train() client id: f_00001-11-6 loss: 0.435631  [  224/  265]
train() client id: f_00001-11-7 loss: 0.503383  [  256/  265]
train() client id: f_00002-0-0 loss: 1.061369  [   32/  124]
train() client id: f_00002-0-1 loss: 1.096290  [   64/  124]
train() client id: f_00002-0-2 loss: 1.244469  [   96/  124]
train() client id: f_00002-1-0 loss: 1.122318  [   32/  124]
train() client id: f_00002-1-1 loss: 1.210865  [   64/  124]
train() client id: f_00002-1-2 loss: 1.026761  [   96/  124]
train() client id: f_00002-2-0 loss: 1.074549  [   32/  124]
train() client id: f_00002-2-1 loss: 1.153786  [   64/  124]
train() client id: f_00002-2-2 loss: 1.052545  [   96/  124]
train() client id: f_00002-3-0 loss: 1.119822  [   32/  124]
train() client id: f_00002-3-1 loss: 1.030722  [   64/  124]
train() client id: f_00002-3-2 loss: 1.058722  [   96/  124]
train() client id: f_00002-4-0 loss: 1.046111  [   32/  124]
train() client id: f_00002-4-1 loss: 1.077440  [   64/  124]
train() client id: f_00002-4-2 loss: 1.033909  [   96/  124]
train() client id: f_00002-5-0 loss: 1.022620  [   32/  124]
train() client id: f_00002-5-1 loss: 1.075806  [   64/  124]
train() client id: f_00002-5-2 loss: 0.950606  [   96/  124]
train() client id: f_00002-6-0 loss: 0.960873  [   32/  124]
train() client id: f_00002-6-1 loss: 1.109544  [   64/  124]
train() client id: f_00002-6-2 loss: 0.989694  [   96/  124]
train() client id: f_00002-7-0 loss: 1.014260  [   32/  124]
train() client id: f_00002-7-1 loss: 0.903908  [   64/  124]
train() client id: f_00002-7-2 loss: 1.123340  [   96/  124]
train() client id: f_00002-8-0 loss: 1.045918  [   32/  124]
train() client id: f_00002-8-1 loss: 0.930862  [   64/  124]
train() client id: f_00002-8-2 loss: 0.935079  [   96/  124]
train() client id: f_00002-9-0 loss: 0.972586  [   32/  124]
train() client id: f_00002-9-1 loss: 0.990795  [   64/  124]
train() client id: f_00002-9-2 loss: 1.085309  [   96/  124]
train() client id: f_00002-10-0 loss: 0.955896  [   32/  124]
train() client id: f_00002-10-1 loss: 0.999689  [   64/  124]
train() client id: f_00002-10-2 loss: 1.052376  [   96/  124]
train() client id: f_00002-11-0 loss: 1.084071  [   32/  124]
train() client id: f_00002-11-1 loss: 0.959829  [   64/  124]
train() client id: f_00002-11-2 loss: 1.029386  [   96/  124]
train() client id: f_00003-0-0 loss: 0.794903  [   32/   43]
train() client id: f_00003-1-0 loss: 0.656737  [   32/   43]
train() client id: f_00003-2-0 loss: 0.892321  [   32/   43]
train() client id: f_00003-3-0 loss: 0.812041  [   32/   43]
train() client id: f_00003-4-0 loss: 0.802369  [   32/   43]
train() client id: f_00003-5-0 loss: 0.704685  [   32/   43]
train() client id: f_00003-6-0 loss: 0.622923  [   32/   43]
train() client id: f_00003-7-0 loss: 0.762508  [   32/   43]
train() client id: f_00003-8-0 loss: 0.757242  [   32/   43]
train() client id: f_00003-9-0 loss: 0.739388  [   32/   43]
train() client id: f_00003-10-0 loss: 0.688754  [   32/   43]
train() client id: f_00003-11-0 loss: 0.805591  [   32/   43]
train() client id: f_00004-0-0 loss: 1.109411  [   32/  306]
train() client id: f_00004-0-1 loss: 0.948454  [   64/  306]
train() client id: f_00004-0-2 loss: 0.992389  [   96/  306]
train() client id: f_00004-0-3 loss: 0.836689  [  128/  306]
train() client id: f_00004-0-4 loss: 0.952116  [  160/  306]
train() client id: f_00004-0-5 loss: 0.772710  [  192/  306]
train() client id: f_00004-0-6 loss: 0.984741  [  224/  306]
train() client id: f_00004-0-7 loss: 1.026486  [  256/  306]
train() client id: f_00004-0-8 loss: 0.841562  [  288/  306]
train() client id: f_00004-1-0 loss: 0.887955  [   32/  306]
train() client id: f_00004-1-1 loss: 0.956376  [   64/  306]
train() client id: f_00004-1-2 loss: 0.836584  [   96/  306]
train() client id: f_00004-1-3 loss: 0.958098  [  128/  306]
train() client id: f_00004-1-4 loss: 0.941635  [  160/  306]
train() client id: f_00004-1-5 loss: 0.959414  [  192/  306]
train() client id: f_00004-1-6 loss: 0.924654  [  224/  306]
train() client id: f_00004-1-7 loss: 0.977357  [  256/  306]
train() client id: f_00004-1-8 loss: 0.886644  [  288/  306]
train() client id: f_00004-2-0 loss: 0.848612  [   32/  306]
train() client id: f_00004-2-1 loss: 0.879800  [   64/  306]
train() client id: f_00004-2-2 loss: 0.952767  [   96/  306]
train() client id: f_00004-2-3 loss: 0.910604  [  128/  306]
train() client id: f_00004-2-4 loss: 0.975635  [  160/  306]
train() client id: f_00004-2-5 loss: 1.001353  [  192/  306]
train() client id: f_00004-2-6 loss: 0.905810  [  224/  306]
train() client id: f_00004-2-7 loss: 0.979479  [  256/  306]
train() client id: f_00004-2-8 loss: 0.927601  [  288/  306]
train() client id: f_00004-3-0 loss: 0.921745  [   32/  306]
train() client id: f_00004-3-1 loss: 1.013996  [   64/  306]
train() client id: f_00004-3-2 loss: 0.871915  [   96/  306]
train() client id: f_00004-3-3 loss: 0.942680  [  128/  306]
train() client id: f_00004-3-4 loss: 0.905293  [  160/  306]
train() client id: f_00004-3-5 loss: 0.807389  [  192/  306]
train() client id: f_00004-3-6 loss: 0.920374  [  224/  306]
train() client id: f_00004-3-7 loss: 0.906930  [  256/  306]
train() client id: f_00004-3-8 loss: 0.945906  [  288/  306]
train() client id: f_00004-4-0 loss: 0.849786  [   32/  306]
train() client id: f_00004-4-1 loss: 0.969645  [   64/  306]
train() client id: f_00004-4-2 loss: 0.819220  [   96/  306]
train() client id: f_00004-4-3 loss: 0.884190  [  128/  306]
train() client id: f_00004-4-4 loss: 0.862194  [  160/  306]
train() client id: f_00004-4-5 loss: 0.938165  [  192/  306]
train() client id: f_00004-4-6 loss: 0.849813  [  224/  306]
train() client id: f_00004-4-7 loss: 1.138805  [  256/  306]
train() client id: f_00004-4-8 loss: 0.923899  [  288/  306]
train() client id: f_00004-5-0 loss: 0.874699  [   32/  306]
train() client id: f_00004-5-1 loss: 0.928395  [   64/  306]
train() client id: f_00004-5-2 loss: 0.928316  [   96/  306]
train() client id: f_00004-5-3 loss: 0.901428  [  128/  306]
train() client id: f_00004-5-4 loss: 0.835733  [  160/  306]
train() client id: f_00004-5-5 loss: 1.030283  [  192/  306]
train() client id: f_00004-5-6 loss: 0.896026  [  224/  306]
train() client id: f_00004-5-7 loss: 1.003798  [  256/  306]
train() client id: f_00004-5-8 loss: 0.830594  [  288/  306]
train() client id: f_00004-6-0 loss: 0.948814  [   32/  306]
train() client id: f_00004-6-1 loss: 0.950204  [   64/  306]
train() client id: f_00004-6-2 loss: 0.947616  [   96/  306]
train() client id: f_00004-6-3 loss: 0.954430  [  128/  306]
train() client id: f_00004-6-4 loss: 0.895931  [  160/  306]
train() client id: f_00004-6-5 loss: 0.859791  [  192/  306]
train() client id: f_00004-6-6 loss: 0.840716  [  224/  306]
train() client id: f_00004-6-7 loss: 0.921286  [  256/  306]
train() client id: f_00004-6-8 loss: 0.856090  [  288/  306]
train() client id: f_00004-7-0 loss: 0.925587  [   32/  306]
train() client id: f_00004-7-1 loss: 0.836880  [   64/  306]
train() client id: f_00004-7-2 loss: 1.028322  [   96/  306]
train() client id: f_00004-7-3 loss: 0.901776  [  128/  306]
train() client id: f_00004-7-4 loss: 0.969426  [  160/  306]
train() client id: f_00004-7-5 loss: 0.923260  [  192/  306]
train() client id: f_00004-7-6 loss: 0.720879  [  224/  306]
train() client id: f_00004-7-7 loss: 0.878473  [  256/  306]
train() client id: f_00004-7-8 loss: 1.097324  [  288/  306]
train() client id: f_00004-8-0 loss: 0.977704  [   32/  306]
train() client id: f_00004-8-1 loss: 0.822303  [   64/  306]
train() client id: f_00004-8-2 loss: 0.835696  [   96/  306]
train() client id: f_00004-8-3 loss: 0.970061  [  128/  306]
train() client id: f_00004-8-4 loss: 0.937841  [  160/  306]
train() client id: f_00004-8-5 loss: 0.841150  [  192/  306]
train() client id: f_00004-8-6 loss: 0.877494  [  224/  306]
train() client id: f_00004-8-7 loss: 0.952758  [  256/  306]
train() client id: f_00004-8-8 loss: 0.917679  [  288/  306]
train() client id: f_00004-9-0 loss: 0.858974  [   32/  306]
train() client id: f_00004-9-1 loss: 0.880345  [   64/  306]
train() client id: f_00004-9-2 loss: 0.887306  [   96/  306]
train() client id: f_00004-9-3 loss: 0.949908  [  128/  306]
train() client id: f_00004-9-4 loss: 0.980257  [  160/  306]
train() client id: f_00004-9-5 loss: 0.861748  [  192/  306]
train() client id: f_00004-9-6 loss: 0.950282  [  224/  306]
train() client id: f_00004-9-7 loss: 0.931387  [  256/  306]
train() client id: f_00004-9-8 loss: 0.867403  [  288/  306]
train() client id: f_00004-10-0 loss: 0.823184  [   32/  306]
train() client id: f_00004-10-1 loss: 1.025289  [   64/  306]
train() client id: f_00004-10-2 loss: 0.853478  [   96/  306]
train() client id: f_00004-10-3 loss: 0.876607  [  128/  306]
train() client id: f_00004-10-4 loss: 0.823973  [  160/  306]
train() client id: f_00004-10-5 loss: 1.056221  [  192/  306]
train() client id: f_00004-10-6 loss: 0.802852  [  224/  306]
train() client id: f_00004-10-7 loss: 1.006535  [  256/  306]
train() client id: f_00004-10-8 loss: 0.985222  [  288/  306]
train() client id: f_00004-11-0 loss: 0.847679  [   32/  306]
train() client id: f_00004-11-1 loss: 0.896241  [   64/  306]
train() client id: f_00004-11-2 loss: 1.009286  [   96/  306]
train() client id: f_00004-11-3 loss: 1.048018  [  128/  306]
train() client id: f_00004-11-4 loss: 0.844131  [  160/  306]
train() client id: f_00004-11-5 loss: 0.884012  [  192/  306]
train() client id: f_00004-11-6 loss: 0.794432  [  224/  306]
train() client id: f_00004-11-7 loss: 0.920800  [  256/  306]
train() client id: f_00004-11-8 loss: 0.953393  [  288/  306]
train() client id: f_00005-0-0 loss: 0.738869  [   32/  146]
train() client id: f_00005-0-1 loss: 0.916722  [   64/  146]
train() client id: f_00005-0-2 loss: 0.760642  [   96/  146]
train() client id: f_00005-0-3 loss: 0.714270  [  128/  146]
train() client id: f_00005-1-0 loss: 0.884831  [   32/  146]
train() client id: f_00005-1-1 loss: 0.857338  [   64/  146]
train() client id: f_00005-1-2 loss: 1.027355  [   96/  146]
train() client id: f_00005-1-3 loss: 0.564836  [  128/  146]
train() client id: f_00005-2-0 loss: 0.838336  [   32/  146]
train() client id: f_00005-2-1 loss: 0.879149  [   64/  146]
train() client id: f_00005-2-2 loss: 0.641365  [   96/  146]
train() client id: f_00005-2-3 loss: 0.961292  [  128/  146]
train() client id: f_00005-3-0 loss: 0.812032  [   32/  146]
train() client id: f_00005-3-1 loss: 0.788802  [   64/  146]
train() client id: f_00005-3-2 loss: 0.725329  [   96/  146]
train() client id: f_00005-3-3 loss: 1.018386  [  128/  146]
train() client id: f_00005-4-0 loss: 0.881944  [   32/  146]
train() client id: f_00005-4-1 loss: 0.823574  [   64/  146]
train() client id: f_00005-4-2 loss: 0.920484  [   96/  146]
train() client id: f_00005-4-3 loss: 0.698837  [  128/  146]
train() client id: f_00005-5-0 loss: 0.842628  [   32/  146]
train() client id: f_00005-5-1 loss: 0.771145  [   64/  146]
train() client id: f_00005-5-2 loss: 0.825182  [   96/  146]
train() client id: f_00005-5-3 loss: 0.829250  [  128/  146]
train() client id: f_00005-6-0 loss: 0.833376  [   32/  146]
train() client id: f_00005-6-1 loss: 0.679963  [   64/  146]
train() client id: f_00005-6-2 loss: 0.933871  [   96/  146]
train() client id: f_00005-6-3 loss: 0.835873  [  128/  146]
train() client id: f_00005-7-0 loss: 0.854265  [   32/  146]
train() client id: f_00005-7-1 loss: 1.013633  [   64/  146]
train() client id: f_00005-7-2 loss: 0.744179  [   96/  146]
train() client id: f_00005-7-3 loss: 0.673028  [  128/  146]
train() client id: f_00005-8-0 loss: 0.837969  [   32/  146]
train() client id: f_00005-8-1 loss: 0.868601  [   64/  146]
train() client id: f_00005-8-2 loss: 0.742956  [   96/  146]
train() client id: f_00005-8-3 loss: 0.814884  [  128/  146]
train() client id: f_00005-9-0 loss: 0.832755  [   32/  146]
train() client id: f_00005-9-1 loss: 0.894912  [   64/  146]
train() client id: f_00005-9-2 loss: 0.663994  [   96/  146]
train() client id: f_00005-9-3 loss: 0.881771  [  128/  146]
train() client id: f_00005-10-0 loss: 0.877170  [   32/  146]
train() client id: f_00005-10-1 loss: 0.862659  [   64/  146]
train() client id: f_00005-10-2 loss: 0.904914  [   96/  146]
train() client id: f_00005-10-3 loss: 0.728557  [  128/  146]
train() client id: f_00005-11-0 loss: 0.811502  [   32/  146]
train() client id: f_00005-11-1 loss: 0.904563  [   64/  146]
train() client id: f_00005-11-2 loss: 0.949068  [   96/  146]
train() client id: f_00005-11-3 loss: 0.670108  [  128/  146]
train() client id: f_00006-0-0 loss: 0.667477  [   32/   54]
train() client id: f_00006-1-0 loss: 0.618280  [   32/   54]
train() client id: f_00006-2-0 loss: 0.610598  [   32/   54]
train() client id: f_00006-3-0 loss: 0.631003  [   32/   54]
train() client id: f_00006-4-0 loss: 0.611516  [   32/   54]
train() client id: f_00006-5-0 loss: 0.661192  [   32/   54]
train() client id: f_00006-6-0 loss: 0.628765  [   32/   54]
train() client id: f_00006-7-0 loss: 0.602174  [   32/   54]
train() client id: f_00006-8-0 loss: 0.556245  [   32/   54]
train() client id: f_00006-9-0 loss: 0.623923  [   32/   54]
train() client id: f_00006-10-0 loss: 0.667488  [   32/   54]
train() client id: f_00006-11-0 loss: 0.552558  [   32/   54]
train() client id: f_00007-0-0 loss: 0.733227  [   32/  179]
train() client id: f_00007-0-1 loss: 0.725492  [   64/  179]
train() client id: f_00007-0-2 loss: 0.596626  [   96/  179]
train() client id: f_00007-0-3 loss: 0.681783  [  128/  179]
train() client id: f_00007-0-4 loss: 0.581093  [  160/  179]
train() client id: f_00007-1-0 loss: 0.737143  [   32/  179]
train() client id: f_00007-1-1 loss: 0.587064  [   64/  179]
train() client id: f_00007-1-2 loss: 0.682245  [   96/  179]
train() client id: f_00007-1-3 loss: 0.578016  [  128/  179]
train() client id: f_00007-1-4 loss: 0.716439  [  160/  179]
train() client id: f_00007-2-0 loss: 0.667242  [   32/  179]
train() client id: f_00007-2-1 loss: 0.636886  [   64/  179]
train() client id: f_00007-2-2 loss: 0.702697  [   96/  179]
train() client id: f_00007-2-3 loss: 0.650194  [  128/  179]
train() client id: f_00007-2-4 loss: 0.626975  [  160/  179]
train() client id: f_00007-3-0 loss: 0.691574  [   32/  179]
train() client id: f_00007-3-1 loss: 0.635025  [   64/  179]
train() client id: f_00007-3-2 loss: 0.543979  [   96/  179]
train() client id: f_00007-3-3 loss: 0.600672  [  128/  179]
train() client id: f_00007-3-4 loss: 0.805614  [  160/  179]
train() client id: f_00007-4-0 loss: 0.656900  [   32/  179]
train() client id: f_00007-4-1 loss: 0.608023  [   64/  179]
train() client id: f_00007-4-2 loss: 0.695619  [   96/  179]
train() client id: f_00007-4-3 loss: 0.605907  [  128/  179]
train() client id: f_00007-4-4 loss: 0.596756  [  160/  179]
train() client id: f_00007-5-0 loss: 0.607814  [   32/  179]
train() client id: f_00007-5-1 loss: 0.650987  [   64/  179]
train() client id: f_00007-5-2 loss: 0.566664  [   96/  179]
train() client id: f_00007-5-3 loss: 0.615723  [  128/  179]
train() client id: f_00007-5-4 loss: 0.655082  [  160/  179]
train() client id: f_00007-6-0 loss: 0.724585  [   32/  179]
train() client id: f_00007-6-1 loss: 0.579022  [   64/  179]
train() client id: f_00007-6-2 loss: 0.639747  [   96/  179]
train() client id: f_00007-6-3 loss: 0.746868  [  128/  179]
train() client id: f_00007-6-4 loss: 0.531214  [  160/  179]
train() client id: f_00007-7-0 loss: 0.647884  [   32/  179]
train() client id: f_00007-7-1 loss: 0.718503  [   64/  179]
train() client id: f_00007-7-2 loss: 0.635492  [   96/  179]
train() client id: f_00007-7-3 loss: 0.512203  [  128/  179]
train() client id: f_00007-7-4 loss: 0.503928  [  160/  179]
train() client id: f_00007-8-0 loss: 0.525267  [   32/  179]
train() client id: f_00007-8-1 loss: 0.628622  [   64/  179]
train() client id: f_00007-8-2 loss: 0.649266  [   96/  179]
train() client id: f_00007-8-3 loss: 0.579968  [  128/  179]
train() client id: f_00007-8-4 loss: 0.654349  [  160/  179]
train() client id: f_00007-9-0 loss: 0.512841  [   32/  179]
train() client id: f_00007-9-1 loss: 0.592385  [   64/  179]
train() client id: f_00007-9-2 loss: 0.691753  [   96/  179]
train() client id: f_00007-9-3 loss: 0.741098  [  128/  179]
train() client id: f_00007-9-4 loss: 0.550739  [  160/  179]
train() client id: f_00007-10-0 loss: 0.565510  [   32/  179]
train() client id: f_00007-10-1 loss: 0.528308  [   64/  179]
train() client id: f_00007-10-2 loss: 0.641253  [   96/  179]
train() client id: f_00007-10-3 loss: 0.901017  [  128/  179]
train() client id: f_00007-10-4 loss: 0.514837  [  160/  179]
train() client id: f_00007-11-0 loss: 0.596317  [   32/  179]
train() client id: f_00007-11-1 loss: 0.653055  [   64/  179]
train() client id: f_00007-11-2 loss: 0.503559  [   96/  179]
train() client id: f_00007-11-3 loss: 0.781967  [  128/  179]
train() client id: f_00007-11-4 loss: 0.623090  [  160/  179]
train() client id: f_00008-0-0 loss: 0.848555  [   32/  130]
train() client id: f_00008-0-1 loss: 0.907911  [   64/  130]
train() client id: f_00008-0-2 loss: 0.888564  [   96/  130]
train() client id: f_00008-0-3 loss: 0.919234  [  128/  130]
train() client id: f_00008-1-0 loss: 0.949662  [   32/  130]
train() client id: f_00008-1-1 loss: 0.971198  [   64/  130]
train() client id: f_00008-1-2 loss: 0.785779  [   96/  130]
train() client id: f_00008-1-3 loss: 0.895304  [  128/  130]
train() client id: f_00008-2-0 loss: 0.823320  [   32/  130]
train() client id: f_00008-2-1 loss: 0.922949  [   64/  130]
train() client id: f_00008-2-2 loss: 0.965987  [   96/  130]
train() client id: f_00008-2-3 loss: 0.850620  [  128/  130]
train() client id: f_00008-3-0 loss: 0.958711  [   32/  130]
train() client id: f_00008-3-1 loss: 0.785920  [   64/  130]
train() client id: f_00008-3-2 loss: 0.905130  [   96/  130]
train() client id: f_00008-3-3 loss: 0.910778  [  128/  130]
train() client id: f_00008-4-0 loss: 0.826433  [   32/  130]
train() client id: f_00008-4-1 loss: 0.871028  [   64/  130]
train() client id: f_00008-4-2 loss: 1.020287  [   96/  130]
train() client id: f_00008-4-3 loss: 0.883741  [  128/  130]
train() client id: f_00008-5-0 loss: 0.768184  [   32/  130]
train() client id: f_00008-5-1 loss: 0.927112  [   64/  130]
train() client id: f_00008-5-2 loss: 0.960953  [   96/  130]
train() client id: f_00008-5-3 loss: 0.903597  [  128/  130]
train() client id: f_00008-6-0 loss: 0.876707  [   32/  130]
train() client id: f_00008-6-1 loss: 0.910770  [   64/  130]
train() client id: f_00008-6-2 loss: 0.841549  [   96/  130]
train() client id: f_00008-6-3 loss: 0.903641  [  128/  130]
train() client id: f_00008-7-0 loss: 0.966913  [   32/  130]
train() client id: f_00008-7-1 loss: 0.840892  [   64/  130]
train() client id: f_00008-7-2 loss: 0.967976  [   96/  130]
train() client id: f_00008-7-3 loss: 0.824086  [  128/  130]
train() client id: f_00008-8-0 loss: 0.906192  [   32/  130]
train() client id: f_00008-8-1 loss: 0.891708  [   64/  130]
train() client id: f_00008-8-2 loss: 0.830258  [   96/  130]
train() client id: f_00008-8-3 loss: 0.975370  [  128/  130]
train() client id: f_00008-9-0 loss: 0.811927  [   32/  130]
train() client id: f_00008-9-1 loss: 1.063069  [   64/  130]
train() client id: f_00008-9-2 loss: 0.776841  [   96/  130]
train() client id: f_00008-9-3 loss: 0.958636  [  128/  130]
train() client id: f_00008-10-0 loss: 0.926259  [   32/  130]
train() client id: f_00008-10-1 loss: 0.942004  [   64/  130]
train() client id: f_00008-10-2 loss: 0.874371  [   96/  130]
train() client id: f_00008-10-3 loss: 0.851503  [  128/  130]
train() client id: f_00008-11-0 loss: 0.781981  [   32/  130]
train() client id: f_00008-11-1 loss: 0.990362  [   64/  130]
train() client id: f_00008-11-2 loss: 0.854207  [   96/  130]
train() client id: f_00008-11-3 loss: 0.931410  [  128/  130]
train() client id: f_00009-0-0 loss: 1.243872  [   32/  118]
train() client id: f_00009-0-1 loss: 1.208923  [   64/  118]
train() client id: f_00009-0-2 loss: 1.062117  [   96/  118]
train() client id: f_00009-1-0 loss: 1.069337  [   32/  118]
train() client id: f_00009-1-1 loss: 1.023173  [   64/  118]
train() client id: f_00009-1-2 loss: 1.192366  [   96/  118]
train() client id: f_00009-2-0 loss: 1.042462  [   32/  118]
train() client id: f_00009-2-1 loss: 1.025389  [   64/  118]
train() client id: f_00009-2-2 loss: 1.159941  [   96/  118]
train() client id: f_00009-3-0 loss: 1.084136  [   32/  118]
train() client id: f_00009-3-1 loss: 1.063641  [   64/  118]
train() client id: f_00009-3-2 loss: 1.067155  [   96/  118]
train() client id: f_00009-4-0 loss: 1.019581  [   32/  118]
train() client id: f_00009-4-1 loss: 1.035176  [   64/  118]
train() client id: f_00009-4-2 loss: 0.923940  [   96/  118]
train() client id: f_00009-5-0 loss: 0.973054  [   32/  118]
train() client id: f_00009-5-1 loss: 0.994646  [   64/  118]
train() client id: f_00009-5-2 loss: 0.996318  [   96/  118]
train() client id: f_00009-6-0 loss: 0.896331  [   32/  118]
train() client id: f_00009-6-1 loss: 1.047845  [   64/  118]
train() client id: f_00009-6-2 loss: 0.916704  [   96/  118]
train() client id: f_00009-7-0 loss: 1.018039  [   32/  118]
train() client id: f_00009-7-1 loss: 0.904837  [   64/  118]
train() client id: f_00009-7-2 loss: 0.969258  [   96/  118]
train() client id: f_00009-8-0 loss: 0.902620  [   32/  118]
train() client id: f_00009-8-1 loss: 0.959892  [   64/  118]
train() client id: f_00009-8-2 loss: 0.938256  [   96/  118]
train() client id: f_00009-9-0 loss: 0.989755  [   32/  118]
train() client id: f_00009-9-1 loss: 0.965037  [   64/  118]
train() client id: f_00009-9-2 loss: 0.912289  [   96/  118]
train() client id: f_00009-10-0 loss: 0.930598  [   32/  118]
train() client id: f_00009-10-1 loss: 0.869119  [   64/  118]
train() client id: f_00009-10-2 loss: 1.001220  [   96/  118]
train() client id: f_00009-11-0 loss: 0.986783  [   32/  118]
train() client id: f_00009-11-1 loss: 0.865335  [   64/  118]
train() client id: f_00009-11-2 loss: 0.924878  [   96/  118]
At round 11 accuracy: 0.6312997347480106
At round 11 training accuracy: 0.5801475519785378
At round 11 training loss: 0.8465103032356829
update_location
xs = [ -3.9056584    4.20031788  75.00902392  18.81129433   0.97929623
   3.95640986 -37.44319194 -16.32485185  59.66397685  -2.06087855]
ys = [ 67.5879595   50.55583871   1.32061395 -37.45517586  29.35018685
  12.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [120.76169275 112.13177738 125.01239015 108.42857093 104.22280215
 100.89527064 106.81237365 101.32707952 117.76442632 100.10124413]
dists_bs = [202.0163859  218.17218706 304.41590165 287.5761109  228.43506905
 241.51531283 224.66087888 235.6176078  282.58043084 243.20450851]
uav_gains = [6.23955507e-11 7.51052065e-11 5.72241493e-11 8.16837356e-11
 9.01758519e-11 9.77960164e-11 8.48090092e-11 9.67574158e-11
 6.64428547e-11 9.97469659e-11]
bs_gains = [3.87446385e-11 3.12360282e-11 1.22909527e-11 1.44140568e-11
 2.74636534e-11 2.34989796e-11 2.87751248e-11 2.51832865e-11
 1.51389674e-11 2.30448319e-11]
Round 12
-------------------------------
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.40743039 19.62699225  9.26366007  3.31236285 22.63944362 10.91364539
  4.11791779 13.28077427  9.76466884  8.86129599]
obj_prev = 111.18819146740302
eta_min = 1.3309903798194387e-10	eta_max = 0.9206473861818304
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 25.855891314518058	eta = 0.9090909090909091
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 44.77490344422677	eta = 0.5249672010962586
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 35.73954375539702	eta = 0.6576848294802706
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.11938254858583	eta = 0.6889150384535667
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039315064938776	eta = 0.6905355085914164
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039105597683566	eta = 0.6905397579562299
eta = 0.6905397579562299
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.03055476 0.064262   0.03006976 0.01042742 0.07420442 0.03540472
 0.0130949  0.04340716 0.03152475 0.02861478]
ene_total = [2.91251884 5.6185508  2.88405621 1.31991488 6.41048895 3.41632264
 1.52386453 3.86970026 3.19241569 2.89127279]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 0 obj = 5.96397047673148
eta = 0.6905397579562299
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
eta_min = 0.6905397579563017	eta_max = 0.6905397579562269
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 0.04594401078389732	eta = 0.909090909090909
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 20.175396261636216	eta = 0.002070208782478131
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0926044394311156	eta = 0.019959473345172857
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.034227485351597	eta = 0.020532257494100595
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0342105771628316	eta = 0.02053242815651352
eta = 0.02053242815651352
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.95847449e-04 1.95635877e-03 1.88115503e-04 7.61251424e-06
 3.06087387e-03 3.39489881e-04 1.50327361e-05 5.42150909e-04
 2.56937143e-04 1.79723710e-04]
ene_total = [0.17408175 0.24390138 0.17681862 0.16093954 0.27690584 0.21759158
 0.16000151 0.16914252 0.24017663 0.21465121]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 1 obj = 5.963970476732851
eta = 0.6905397579563017
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
Done!
At round 12 energy consumption: 5.96397047673148
At round 12 eta: 0.6905397579563017
At round 12 local rounds: 12.124905073660063
At round 12 global rounds: 77.78722246725123
At round 12 a_n: 23.72950684565325
gradient difference: 0.4969092905521393
train() client id: f_00000-0-0 loss: 1.354231  [   32/  126]
train() client id: f_00000-0-1 loss: 1.327604  [   64/  126]
train() client id: f_00000-0-2 loss: 0.935214  [   96/  126]
train() client id: f_00000-1-0 loss: 1.027000  [   32/  126]
train() client id: f_00000-1-1 loss: 1.026594  [   64/  126]
train() client id: f_00000-1-2 loss: 1.185473  [   96/  126]
train() client id: f_00000-2-0 loss: 1.178416  [   32/  126]
train() client id: f_00000-2-1 loss: 0.962475  [   64/  126]
train() client id: f_00000-2-2 loss: 1.021221  [   96/  126]
train() client id: f_00000-3-0 loss: 1.010553  [   32/  126]
train() client id: f_00000-3-1 loss: 0.986851  [   64/  126]
train() client id: f_00000-3-2 loss: 0.817697  [   96/  126]
train() client id: f_00000-4-0 loss: 0.921580  [   32/  126]
train() client id: f_00000-4-1 loss: 0.909469  [   64/  126]
train() client id: f_00000-4-2 loss: 0.887759  [   96/  126]
train() client id: f_00000-5-0 loss: 0.779229  [   32/  126]
train() client id: f_00000-5-1 loss: 0.995529  [   64/  126]
train() client id: f_00000-5-2 loss: 0.919280  [   96/  126]
train() client id: f_00000-6-0 loss: 0.938059  [   32/  126]
train() client id: f_00000-6-1 loss: 0.856372  [   64/  126]
train() client id: f_00000-6-2 loss: 0.854126  [   96/  126]
train() client id: f_00000-7-0 loss: 0.857502  [   32/  126]
train() client id: f_00000-7-1 loss: 0.814110  [   64/  126]
train() client id: f_00000-7-2 loss: 0.908429  [   96/  126]
train() client id: f_00000-8-0 loss: 0.846087  [   32/  126]
train() client id: f_00000-8-1 loss: 0.855050  [   64/  126]
train() client id: f_00000-8-2 loss: 0.925738  [   96/  126]
train() client id: f_00000-9-0 loss: 0.883144  [   32/  126]
train() client id: f_00000-9-1 loss: 0.846104  [   64/  126]
train() client id: f_00000-9-2 loss: 0.896184  [   96/  126]
train() client id: f_00000-10-0 loss: 0.943223  [   32/  126]
train() client id: f_00000-10-1 loss: 0.852344  [   64/  126]
train() client id: f_00000-10-2 loss: 0.791003  [   96/  126]
train() client id: f_00000-11-0 loss: 0.923619  [   32/  126]
train() client id: f_00000-11-1 loss: 0.856256  [   64/  126]
train() client id: f_00000-11-2 loss: 0.815093  [   96/  126]
train() client id: f_00001-0-0 loss: 0.631373  [   32/  265]
train() client id: f_00001-0-1 loss: 0.524416  [   64/  265]
train() client id: f_00001-0-2 loss: 0.526370  [   96/  265]
train() client id: f_00001-0-3 loss: 0.491438  [  128/  265]
train() client id: f_00001-0-4 loss: 0.505061  [  160/  265]
train() client id: f_00001-0-5 loss: 0.559333  [  192/  265]
train() client id: f_00001-0-6 loss: 0.569527  [  224/  265]
train() client id: f_00001-0-7 loss: 0.476451  [  256/  265]
train() client id: f_00001-1-0 loss: 0.586213  [   32/  265]
train() client id: f_00001-1-1 loss: 0.540701  [   64/  265]
train() client id: f_00001-1-2 loss: 0.497848  [   96/  265]
train() client id: f_00001-1-3 loss: 0.631427  [  128/  265]
train() client id: f_00001-1-4 loss: 0.499175  [  160/  265]
train() client id: f_00001-1-5 loss: 0.462147  [  192/  265]
train() client id: f_00001-1-6 loss: 0.496071  [  224/  265]
train() client id: f_00001-1-7 loss: 0.530033  [  256/  265]
train() client id: f_00001-2-0 loss: 0.571947  [   32/  265]
train() client id: f_00001-2-1 loss: 0.451589  [   64/  265]
train() client id: f_00001-2-2 loss: 0.578736  [   96/  265]
train() client id: f_00001-2-3 loss: 0.496649  [  128/  265]
train() client id: f_00001-2-4 loss: 0.598360  [  160/  265]
train() client id: f_00001-2-5 loss: 0.438774  [  192/  265]
train() client id: f_00001-2-6 loss: 0.525506  [  224/  265]
train() client id: f_00001-2-7 loss: 0.565827  [  256/  265]
train() client id: f_00001-3-0 loss: 0.551868  [   32/  265]
train() client id: f_00001-3-1 loss: 0.544853  [   64/  265]
train() client id: f_00001-3-2 loss: 0.520814  [   96/  265]
train() client id: f_00001-3-3 loss: 0.615276  [  128/  265]
train() client id: f_00001-3-4 loss: 0.526441  [  160/  265]
train() client id: f_00001-3-5 loss: 0.501061  [  192/  265]
train() client id: f_00001-3-6 loss: 0.462500  [  224/  265]
train() client id: f_00001-3-7 loss: 0.461585  [  256/  265]
train() client id: f_00001-4-0 loss: 0.499225  [   32/  265]
train() client id: f_00001-4-1 loss: 0.449558  [   64/  265]
train() client id: f_00001-4-2 loss: 0.482807  [   96/  265]
train() client id: f_00001-4-3 loss: 0.570384  [  128/  265]
train() client id: f_00001-4-4 loss: 0.482115  [  160/  265]
train() client id: f_00001-4-5 loss: 0.587593  [  192/  265]
train() client id: f_00001-4-6 loss: 0.603537  [  224/  265]
train() client id: f_00001-4-7 loss: 0.474297  [  256/  265]
train() client id: f_00001-5-0 loss: 0.492351  [   32/  265]
train() client id: f_00001-5-1 loss: 0.558331  [   64/  265]
train() client id: f_00001-5-2 loss: 0.452096  [   96/  265]
train() client id: f_00001-5-3 loss: 0.536274  [  128/  265]
train() client id: f_00001-5-4 loss: 0.543645  [  160/  265]
train() client id: f_00001-5-5 loss: 0.468363  [  192/  265]
train() client id: f_00001-5-6 loss: 0.462559  [  224/  265]
train() client id: f_00001-5-7 loss: 0.633498  [  256/  265]
train() client id: f_00001-6-0 loss: 0.438095  [   32/  265]
train() client id: f_00001-6-1 loss: 0.565644  [   64/  265]
train() client id: f_00001-6-2 loss: 0.539414  [   96/  265]
train() client id: f_00001-6-3 loss: 0.492023  [  128/  265]
train() client id: f_00001-6-4 loss: 0.490341  [  160/  265]
train() client id: f_00001-6-5 loss: 0.491045  [  192/  265]
train() client id: f_00001-6-6 loss: 0.490141  [  224/  265]
train() client id: f_00001-6-7 loss: 0.591382  [  256/  265]
train() client id: f_00001-7-0 loss: 0.502934  [   32/  265]
train() client id: f_00001-7-1 loss: 0.475802  [   64/  265]
train() client id: f_00001-7-2 loss: 0.527548  [   96/  265]
train() client id: f_00001-7-3 loss: 0.481897  [  128/  265]
train() client id: f_00001-7-4 loss: 0.471381  [  160/  265]
train() client id: f_00001-7-5 loss: 0.639278  [  192/  265]
train() client id: f_00001-7-6 loss: 0.533239  [  224/  265]
train() client id: f_00001-7-7 loss: 0.487194  [  256/  265]
train() client id: f_00001-8-0 loss: 0.479587  [   32/  265]
train() client id: f_00001-8-1 loss: 0.568065  [   64/  265]
train() client id: f_00001-8-2 loss: 0.643709  [   96/  265]
train() client id: f_00001-8-3 loss: 0.445914  [  128/  265]
train() client id: f_00001-8-4 loss: 0.490630  [  160/  265]
train() client id: f_00001-8-5 loss: 0.458799  [  192/  265]
train() client id: f_00001-8-6 loss: 0.557235  [  224/  265]
train() client id: f_00001-8-7 loss: 0.475768  [  256/  265]
train() client id: f_00001-9-0 loss: 0.414569  [   32/  265]
train() client id: f_00001-9-1 loss: 0.542386  [   64/  265]
train() client id: f_00001-9-2 loss: 0.485968  [   96/  265]
train() client id: f_00001-9-3 loss: 0.427332  [  128/  265]
train() client id: f_00001-9-4 loss: 0.550972  [  160/  265]
train() client id: f_00001-9-5 loss: 0.488113  [  192/  265]
train() client id: f_00001-9-6 loss: 0.675270  [  224/  265]
train() client id: f_00001-9-7 loss: 0.505398  [  256/  265]
train() client id: f_00001-10-0 loss: 0.530779  [   32/  265]
train() client id: f_00001-10-1 loss: 0.428656  [   64/  265]
train() client id: f_00001-10-2 loss: 0.492245  [   96/  265]
train() client id: f_00001-10-3 loss: 0.437940  [  128/  265]
train() client id: f_00001-10-4 loss: 0.604709  [  160/  265]
train() client id: f_00001-10-5 loss: 0.419543  [  192/  265]
train() client id: f_00001-10-6 loss: 0.669395  [  224/  265]
train() client id: f_00001-10-7 loss: 0.499016  [  256/  265]
train() client id: f_00001-11-0 loss: 0.611310  [   32/  265]
train() client id: f_00001-11-1 loss: 0.510212  [   64/  265]
train() client id: f_00001-11-2 loss: 0.473536  [   96/  265]
train() client id: f_00001-11-3 loss: 0.521612  [  128/  265]
train() client id: f_00001-11-4 loss: 0.434903  [  160/  265]
train() client id: f_00001-11-5 loss: 0.489727  [  192/  265]
train() client id: f_00001-11-6 loss: 0.490872  [  224/  265]
train() client id: f_00001-11-7 loss: 0.548047  [  256/  265]
train() client id: f_00002-0-0 loss: 1.257996  [   32/  124]
train() client id: f_00002-0-1 loss: 1.085300  [   64/  124]
train() client id: f_00002-0-2 loss: 1.112843  [   96/  124]
train() client id: f_00002-1-0 loss: 1.018406  [   32/  124]
train() client id: f_00002-1-1 loss: 1.096309  [   64/  124]
train() client id: f_00002-1-2 loss: 1.093690  [   96/  124]
train() client id: f_00002-2-0 loss: 1.215384  [   32/  124]
train() client id: f_00002-2-1 loss: 1.052258  [   64/  124]
train() client id: f_00002-2-2 loss: 0.987195  [   96/  124]
train() client id: f_00002-3-0 loss: 1.033646  [   32/  124]
train() client id: f_00002-3-1 loss: 1.008009  [   64/  124]
train() client id: f_00002-3-2 loss: 1.092925  [   96/  124]
train() client id: f_00002-4-0 loss: 1.071953  [   32/  124]
train() client id: f_00002-4-1 loss: 1.059674  [   64/  124]
train() client id: f_00002-4-2 loss: 1.029089  [   96/  124]
train() client id: f_00002-5-0 loss: 0.934765  [   32/  124]
train() client id: f_00002-5-1 loss: 1.012338  [   64/  124]
train() client id: f_00002-5-2 loss: 0.983526  [   96/  124]
train() client id: f_00002-6-0 loss: 0.949895  [   32/  124]
train() client id: f_00002-6-1 loss: 1.075675  [   64/  124]
train() client id: f_00002-6-2 loss: 0.997054  [   96/  124]
train() client id: f_00002-7-0 loss: 1.048476  [   32/  124]
train() client id: f_00002-7-1 loss: 0.896901  [   64/  124]
train() client id: f_00002-7-2 loss: 1.039912  [   96/  124]
train() client id: f_00002-8-0 loss: 1.016231  [   32/  124]
train() client id: f_00002-8-1 loss: 0.967959  [   64/  124]
train() client id: f_00002-8-2 loss: 1.010662  [   96/  124]
train() client id: f_00002-9-0 loss: 1.004291  [   32/  124]
train() client id: f_00002-9-1 loss: 0.957122  [   64/  124]
train() client id: f_00002-9-2 loss: 0.987457  [   96/  124]
train() client id: f_00002-10-0 loss: 0.978129  [   32/  124]
train() client id: f_00002-10-1 loss: 0.990466  [   64/  124]
train() client id: f_00002-10-2 loss: 0.980830  [   96/  124]
train() client id: f_00002-11-0 loss: 0.906161  [   32/  124]
train() client id: f_00002-11-1 loss: 0.996294  [   64/  124]
train() client id: f_00002-11-2 loss: 1.045580  [   96/  124]
train() client id: f_00003-0-0 loss: 0.880228  [   32/   43]
train() client id: f_00003-1-0 loss: 0.688789  [   32/   43]
train() client id: f_00003-2-0 loss: 0.800042  [   32/   43]
train() client id: f_00003-3-0 loss: 0.768139  [   32/   43]
train() client id: f_00003-4-0 loss: 0.775711  [   32/   43]
train() client id: f_00003-5-0 loss: 0.758575  [   32/   43]
train() client id: f_00003-6-0 loss: 0.704460  [   32/   43]
train() client id: f_00003-7-0 loss: 0.679980  [   32/   43]
train() client id: f_00003-8-0 loss: 0.701014  [   32/   43]
train() client id: f_00003-9-0 loss: 0.825640  [   32/   43]
train() client id: f_00003-10-0 loss: 0.838887  [   32/   43]
train() client id: f_00003-11-0 loss: 0.699350  [   32/   43]
train() client id: f_00004-0-0 loss: 0.816875  [   32/  306]
train() client id: f_00004-0-1 loss: 0.826832  [   64/  306]
train() client id: f_00004-0-2 loss: 0.660602  [   96/  306]
train() client id: f_00004-0-3 loss: 0.884512  [  128/  306]
train() client id: f_00004-0-4 loss: 0.852784  [  160/  306]
train() client id: f_00004-0-5 loss: 0.843668  [  192/  306]
train() client id: f_00004-0-6 loss: 0.899502  [  224/  306]
train() client id: f_00004-0-7 loss: 0.895207  [  256/  306]
train() client id: f_00004-0-8 loss: 0.844661  [  288/  306]
train() client id: f_00004-1-0 loss: 0.725610  [   32/  306]
train() client id: f_00004-1-1 loss: 0.922869  [   64/  306]
train() client id: f_00004-1-2 loss: 0.931040  [   96/  306]
train() client id: f_00004-1-3 loss: 0.870194  [  128/  306]
train() client id: f_00004-1-4 loss: 0.789071  [  160/  306]
train() client id: f_00004-1-5 loss: 0.815861  [  192/  306]
train() client id: f_00004-1-6 loss: 0.780577  [  224/  306]
train() client id: f_00004-1-7 loss: 0.848002  [  256/  306]
train() client id: f_00004-1-8 loss: 0.931875  [  288/  306]
train() client id: f_00004-2-0 loss: 0.845476  [   32/  306]
train() client id: f_00004-2-1 loss: 0.862307  [   64/  306]
train() client id: f_00004-2-2 loss: 0.782795  [   96/  306]
train() client id: f_00004-2-3 loss: 0.841061  [  128/  306]
train() client id: f_00004-2-4 loss: 0.848903  [  160/  306]
train() client id: f_00004-2-5 loss: 0.945181  [  192/  306]
train() client id: f_00004-2-6 loss: 0.835154  [  224/  306]
train() client id: f_00004-2-7 loss: 0.833669  [  256/  306]
train() client id: f_00004-2-8 loss: 0.811954  [  288/  306]
train() client id: f_00004-3-0 loss: 0.698591  [   32/  306]
train() client id: f_00004-3-1 loss: 0.890909  [   64/  306]
train() client id: f_00004-3-2 loss: 0.837095  [   96/  306]
train() client id: f_00004-3-3 loss: 0.840923  [  128/  306]
train() client id: f_00004-3-4 loss: 0.842269  [  160/  306]
train() client id: f_00004-3-5 loss: 0.938452  [  192/  306]
train() client id: f_00004-3-6 loss: 0.774712  [  224/  306]
train() client id: f_00004-3-7 loss: 0.862281  [  256/  306]
train() client id: f_00004-3-8 loss: 0.934900  [  288/  306]
train() client id: f_00004-4-0 loss: 0.828661  [   32/  306]
train() client id: f_00004-4-1 loss: 0.823667  [   64/  306]
train() client id: f_00004-4-2 loss: 0.892937  [   96/  306]
train() client id: f_00004-4-3 loss: 0.835044  [  128/  306]
train() client id: f_00004-4-4 loss: 0.782313  [  160/  306]
train() client id: f_00004-4-5 loss: 0.823737  [  192/  306]
train() client id: f_00004-4-6 loss: 0.983528  [  224/  306]
train() client id: f_00004-4-7 loss: 0.827891  [  256/  306]
train() client id: f_00004-4-8 loss: 0.823022  [  288/  306]
train() client id: f_00004-5-0 loss: 0.828912  [   32/  306]
train() client id: f_00004-5-1 loss: 0.877715  [   64/  306]
train() client id: f_00004-5-2 loss: 0.821695  [   96/  306]
train() client id: f_00004-5-3 loss: 0.886194  [  128/  306]
train() client id: f_00004-5-4 loss: 0.853446  [  160/  306]
train() client id: f_00004-5-5 loss: 0.724594  [  192/  306]
train() client id: f_00004-5-6 loss: 0.997885  [  224/  306]
train() client id: f_00004-5-7 loss: 0.793655  [  256/  306]
train() client id: f_00004-5-8 loss: 0.925419  [  288/  306]
train() client id: f_00004-6-0 loss: 0.844889  [   32/  306]
train() client id: f_00004-6-1 loss: 0.880634  [   64/  306]
train() client id: f_00004-6-2 loss: 0.910890  [   96/  306]
train() client id: f_00004-6-3 loss: 0.889083  [  128/  306]
train() client id: f_00004-6-4 loss: 0.778894  [  160/  306]
train() client id: f_00004-6-5 loss: 0.883613  [  192/  306]
train() client id: f_00004-6-6 loss: 0.904584  [  224/  306]
train() client id: f_00004-6-7 loss: 0.698633  [  256/  306]
train() client id: f_00004-6-8 loss: 0.792220  [  288/  306]
train() client id: f_00004-7-0 loss: 0.803534  [   32/  306]
train() client id: f_00004-7-1 loss: 0.982320  [   64/  306]
train() client id: f_00004-7-2 loss: 0.834451  [   96/  306]
train() client id: f_00004-7-3 loss: 0.936113  [  128/  306]
train() client id: f_00004-7-4 loss: 0.882627  [  160/  306]
train() client id: f_00004-7-5 loss: 0.823308  [  192/  306]
train() client id: f_00004-7-6 loss: 0.819991  [  224/  306]
train() client id: f_00004-7-7 loss: 0.765494  [  256/  306]
train() client id: f_00004-7-8 loss: 0.769832  [  288/  306]
train() client id: f_00004-8-0 loss: 0.824151  [   32/  306]
train() client id: f_00004-8-1 loss: 0.832928  [   64/  306]
train() client id: f_00004-8-2 loss: 0.847329  [   96/  306]
train() client id: f_00004-8-3 loss: 0.721608  [  128/  306]
train() client id: f_00004-8-4 loss: 0.823008  [  160/  306]
train() client id: f_00004-8-5 loss: 0.835221  [  192/  306]
train() client id: f_00004-8-6 loss: 0.964161  [  224/  306]
train() client id: f_00004-8-7 loss: 0.911130  [  256/  306]
train() client id: f_00004-8-8 loss: 0.947178  [  288/  306]
train() client id: f_00004-9-0 loss: 0.884008  [   32/  306]
train() client id: f_00004-9-1 loss: 0.786863  [   64/  306]
train() client id: f_00004-9-2 loss: 0.773195  [   96/  306]
train() client id: f_00004-9-3 loss: 0.862230  [  128/  306]
train() client id: f_00004-9-4 loss: 0.928985  [  160/  306]
train() client id: f_00004-9-5 loss: 0.866909  [  192/  306]
train() client id: f_00004-9-6 loss: 0.846982  [  224/  306]
train() client id: f_00004-9-7 loss: 0.794179  [  256/  306]
train() client id: f_00004-9-8 loss: 0.936845  [  288/  306]
train() client id: f_00004-10-0 loss: 0.705209  [   32/  306]
train() client id: f_00004-10-1 loss: 0.750060  [   64/  306]
train() client id: f_00004-10-2 loss: 0.938439  [   96/  306]
train() client id: f_00004-10-3 loss: 0.861853  [  128/  306]
train() client id: f_00004-10-4 loss: 0.774127  [  160/  306]
train() client id: f_00004-10-5 loss: 0.907932  [  192/  306]
train() client id: f_00004-10-6 loss: 0.930978  [  224/  306]
train() client id: f_00004-10-7 loss: 0.920987  [  256/  306]
train() client id: f_00004-10-8 loss: 0.834764  [  288/  306]
train() client id: f_00004-11-0 loss: 0.904364  [   32/  306]
train() client id: f_00004-11-1 loss: 0.845961  [   64/  306]
train() client id: f_00004-11-2 loss: 0.882865  [   96/  306]
train() client id: f_00004-11-3 loss: 0.744177  [  128/  306]
train() client id: f_00004-11-4 loss: 0.956053  [  160/  306]
train() client id: f_00004-11-5 loss: 0.772671  [  192/  306]
train() client id: f_00004-11-6 loss: 0.895974  [  224/  306]
train() client id: f_00004-11-7 loss: 0.875745  [  256/  306]
train() client id: f_00004-11-8 loss: 0.892367  [  288/  306]
train() client id: f_00005-0-0 loss: 0.880401  [   32/  146]
train() client id: f_00005-0-1 loss: 0.735522  [   64/  146]
train() client id: f_00005-0-2 loss: 0.728247  [   96/  146]
train() client id: f_00005-0-3 loss: 0.828899  [  128/  146]
train() client id: f_00005-1-0 loss: 0.825348  [   32/  146]
train() client id: f_00005-1-1 loss: 0.868849  [   64/  146]
train() client id: f_00005-1-2 loss: 0.711827  [   96/  146]
train() client id: f_00005-1-3 loss: 0.822376  [  128/  146]
train() client id: f_00005-2-0 loss: 0.833608  [   32/  146]
train() client id: f_00005-2-1 loss: 0.948620  [   64/  146]
train() client id: f_00005-2-2 loss: 0.845072  [   96/  146]
train() client id: f_00005-2-3 loss: 0.718541  [  128/  146]
train() client id: f_00005-3-0 loss: 0.844476  [   32/  146]
train() client id: f_00005-3-1 loss: 0.719690  [   64/  146]
train() client id: f_00005-3-2 loss: 0.935526  [   96/  146]
train() client id: f_00005-3-3 loss: 0.763076  [  128/  146]
train() client id: f_00005-4-0 loss: 0.717701  [   32/  146]
train() client id: f_00005-4-1 loss: 0.948340  [   64/  146]
train() client id: f_00005-4-2 loss: 0.684307  [   96/  146]
train() client id: f_00005-4-3 loss: 0.896451  [  128/  146]
train() client id: f_00005-5-0 loss: 0.914777  [   32/  146]
train() client id: f_00005-5-1 loss: 0.625360  [   64/  146]
train() client id: f_00005-5-2 loss: 0.878332  [   96/  146]
train() client id: f_00005-5-3 loss: 0.851621  [  128/  146]
train() client id: f_00005-6-0 loss: 0.824174  [   32/  146]
train() client id: f_00005-6-1 loss: 0.718537  [   64/  146]
train() client id: f_00005-6-2 loss: 0.742038  [   96/  146]
train() client id: f_00005-6-3 loss: 0.805402  [  128/  146]
train() client id: f_00005-7-0 loss: 0.944143  [   32/  146]
train() client id: f_00005-7-1 loss: 0.760169  [   64/  146]
train() client id: f_00005-7-2 loss: 0.785450  [   96/  146]
train() client id: f_00005-7-3 loss: 0.692364  [  128/  146]
train() client id: f_00005-8-0 loss: 0.647521  [   32/  146]
train() client id: f_00005-8-1 loss: 0.833528  [   64/  146]
train() client id: f_00005-8-2 loss: 0.932256  [   96/  146]
train() client id: f_00005-8-3 loss: 0.786168  [  128/  146]
train() client id: f_00005-9-0 loss: 0.920328  [   32/  146]
train() client id: f_00005-9-1 loss: 0.718762  [   64/  146]
train() client id: f_00005-9-2 loss: 0.903150  [   96/  146]
train() client id: f_00005-9-3 loss: 0.661211  [  128/  146]
train() client id: f_00005-10-0 loss: 0.622797  [   32/  146]
train() client id: f_00005-10-1 loss: 0.790453  [   64/  146]
train() client id: f_00005-10-2 loss: 1.003012  [   96/  146]
train() client id: f_00005-10-3 loss: 0.679429  [  128/  146]
train() client id: f_00005-11-0 loss: 0.718969  [   32/  146]
train() client id: f_00005-11-1 loss: 0.841484  [   64/  146]
train() client id: f_00005-11-2 loss: 1.071607  [   96/  146]
train() client id: f_00005-11-3 loss: 0.721949  [  128/  146]
train() client id: f_00006-0-0 loss: 0.738519  [   32/   54]
train() client id: f_00006-1-0 loss: 0.638732  [   32/   54]
train() client id: f_00006-2-0 loss: 0.735520  [   32/   54]
train() client id: f_00006-3-0 loss: 0.700983  [   32/   54]
train() client id: f_00006-4-0 loss: 0.649904  [   32/   54]
train() client id: f_00006-5-0 loss: 0.689826  [   32/   54]
train() client id: f_00006-6-0 loss: 0.746308  [   32/   54]
train() client id: f_00006-7-0 loss: 0.752078  [   32/   54]
train() client id: f_00006-8-0 loss: 0.747757  [   32/   54]
train() client id: f_00006-9-0 loss: 0.760461  [   32/   54]
train() client id: f_00006-10-0 loss: 0.689648  [   32/   54]
train() client id: f_00006-11-0 loss: 0.729288  [   32/   54]
train() client id: f_00007-0-0 loss: 0.716942  [   32/  179]
train() client id: f_00007-0-1 loss: 0.820451  [   64/  179]
train() client id: f_00007-0-2 loss: 0.609646  [   96/  179]
train() client id: f_00007-0-3 loss: 0.587293  [  128/  179]
train() client id: f_00007-0-4 loss: 0.578422  [  160/  179]
train() client id: f_00007-1-0 loss: 0.620976  [   32/  179]
train() client id: f_00007-1-1 loss: 0.625456  [   64/  179]
train() client id: f_00007-1-2 loss: 0.626638  [   96/  179]
train() client id: f_00007-1-3 loss: 0.510970  [  128/  179]
train() client id: f_00007-1-4 loss: 0.711072  [  160/  179]
train() client id: f_00007-2-0 loss: 0.533862  [   32/  179]
train() client id: f_00007-2-1 loss: 0.634362  [   64/  179]
train() client id: f_00007-2-2 loss: 0.593679  [   96/  179]
train() client id: f_00007-2-3 loss: 0.770642  [  128/  179]
train() client id: f_00007-2-4 loss: 0.550339  [  160/  179]
train() client id: f_00007-3-0 loss: 0.640956  [   32/  179]
train() client id: f_00007-3-1 loss: 0.555089  [   64/  179]
train() client id: f_00007-3-2 loss: 0.485211  [   96/  179]
train() client id: f_00007-3-3 loss: 0.581674  [  128/  179]
train() client id: f_00007-3-4 loss: 0.512155  [  160/  179]
train() client id: f_00007-4-0 loss: 0.511459  [   32/  179]
train() client id: f_00007-4-1 loss: 0.696070  [   64/  179]
train() client id: f_00007-4-2 loss: 0.509169  [   96/  179]
train() client id: f_00007-4-3 loss: 0.609115  [  128/  179]
train() client id: f_00007-4-4 loss: 0.602127  [  160/  179]
train() client id: f_00007-5-0 loss: 0.567206  [   32/  179]
train() client id: f_00007-5-1 loss: 0.617904  [   64/  179]
train() client id: f_00007-5-2 loss: 0.794626  [   96/  179]
train() client id: f_00007-5-3 loss: 0.476625  [  128/  179]
train() client id: f_00007-5-4 loss: 0.467453  [  160/  179]
train() client id: f_00007-6-0 loss: 0.687483  [   32/  179]
train() client id: f_00007-6-1 loss: 0.622194  [   64/  179]
train() client id: f_00007-6-2 loss: 0.524243  [   96/  179]
train() client id: f_00007-6-3 loss: 0.495022  [  128/  179]
train() client id: f_00007-6-4 loss: 0.649262  [  160/  179]
train() client id: f_00007-7-0 loss: 0.483222  [   32/  179]
train() client id: f_00007-7-1 loss: 0.537207  [   64/  179]
train() client id: f_00007-7-2 loss: 0.477068  [   96/  179]
train() client id: f_00007-7-3 loss: 0.689676  [  128/  179]
train() client id: f_00007-7-4 loss: 0.622720  [  160/  179]
train() client id: f_00007-8-0 loss: 0.546776  [   32/  179]
train() client id: f_00007-8-1 loss: 0.563109  [   64/  179]
train() client id: f_00007-8-2 loss: 0.529903  [   96/  179]
train() client id: f_00007-8-3 loss: 0.632283  [  128/  179]
train() client id: f_00007-8-4 loss: 0.616819  [  160/  179]
train() client id: f_00007-9-0 loss: 0.615796  [   32/  179]
train() client id: f_00007-9-1 loss: 0.440792  [   64/  179]
train() client id: f_00007-9-2 loss: 0.743677  [   96/  179]
train() client id: f_00007-9-3 loss: 0.544442  [  128/  179]
train() client id: f_00007-9-4 loss: 0.537627  [  160/  179]
train() client id: f_00007-10-0 loss: 0.576489  [   32/  179]
train() client id: f_00007-10-1 loss: 0.620017  [   64/  179]
train() client id: f_00007-10-2 loss: 0.645244  [   96/  179]
train() client id: f_00007-10-3 loss: 0.554791  [  128/  179]
train() client id: f_00007-10-4 loss: 0.460750  [  160/  179]
train() client id: f_00007-11-0 loss: 0.554875  [   32/  179]
train() client id: f_00007-11-1 loss: 0.594964  [   64/  179]
train() client id: f_00007-11-2 loss: 0.598896  [   96/  179]
train() client id: f_00007-11-3 loss: 0.455835  [  128/  179]
train() client id: f_00007-11-4 loss: 0.611575  [  160/  179]
train() client id: f_00008-0-0 loss: 0.750694  [   32/  130]
train() client id: f_00008-0-1 loss: 0.785343  [   64/  130]
train() client id: f_00008-0-2 loss: 0.690896  [   96/  130]
train() client id: f_00008-0-3 loss: 0.789467  [  128/  130]
train() client id: f_00008-1-0 loss: 0.839857  [   32/  130]
train() client id: f_00008-1-1 loss: 0.768584  [   64/  130]
train() client id: f_00008-1-2 loss: 0.682414  [   96/  130]
train() client id: f_00008-1-3 loss: 0.730066  [  128/  130]
train() client id: f_00008-2-0 loss: 0.680583  [   32/  130]
train() client id: f_00008-2-1 loss: 0.808171  [   64/  130]
train() client id: f_00008-2-2 loss: 0.744660  [   96/  130]
train() client id: f_00008-2-3 loss: 0.788784  [  128/  130]
train() client id: f_00008-3-0 loss: 0.678752  [   32/  130]
train() client id: f_00008-3-1 loss: 0.871878  [   64/  130]
train() client id: f_00008-3-2 loss: 0.809844  [   96/  130]
train() client id: f_00008-3-3 loss: 0.644675  [  128/  130]
train() client id: f_00008-4-0 loss: 0.791240  [   32/  130]
train() client id: f_00008-4-1 loss: 0.694720  [   64/  130]
train() client id: f_00008-4-2 loss: 0.821804  [   96/  130]
train() client id: f_00008-4-3 loss: 0.742325  [  128/  130]
train() client id: f_00008-5-0 loss: 1.006083  [   32/  130]
train() client id: f_00008-5-1 loss: 0.666305  [   64/  130]
train() client id: f_00008-5-2 loss: 0.736857  [   96/  130]
train() client id: f_00008-5-3 loss: 0.626302  [  128/  130]
train() client id: f_00008-6-0 loss: 0.795541  [   32/  130]
train() client id: f_00008-6-1 loss: 0.709422  [   64/  130]
train() client id: f_00008-6-2 loss: 0.713633  [   96/  130]
train() client id: f_00008-6-3 loss: 0.797346  [  128/  130]
train() client id: f_00008-7-0 loss: 0.715971  [   32/  130]
train() client id: f_00008-7-1 loss: 0.836117  [   64/  130]
train() client id: f_00008-7-2 loss: 0.809811  [   96/  130]
train() client id: f_00008-7-3 loss: 0.673945  [  128/  130]
train() client id: f_00008-8-0 loss: 0.807149  [   32/  130]
train() client id: f_00008-8-1 loss: 0.654281  [   64/  130]
train() client id: f_00008-8-2 loss: 0.754964  [   96/  130]
train() client id: f_00008-8-3 loss: 0.840839  [  128/  130]
train() client id: f_00008-9-0 loss: 0.674970  [   32/  130]
train() client id: f_00008-9-1 loss: 0.826213  [   64/  130]
train() client id: f_00008-9-2 loss: 0.822177  [   96/  130]
train() client id: f_00008-9-3 loss: 0.699120  [  128/  130]
train() client id: f_00008-10-0 loss: 0.736824  [   32/  130]
train() client id: f_00008-10-1 loss: 0.762663  [   64/  130]
train() client id: f_00008-10-2 loss: 0.777060  [   96/  130]
train() client id: f_00008-10-3 loss: 0.783953  [  128/  130]
train() client id: f_00008-11-0 loss: 0.814425  [   32/  130]
train() client id: f_00008-11-1 loss: 0.659828  [   64/  130]
train() client id: f_00008-11-2 loss: 0.902527  [   96/  130]
train() client id: f_00008-11-3 loss: 0.675082  [  128/  130]
train() client id: f_00009-0-0 loss: 1.238488  [   32/  118]
train() client id: f_00009-0-1 loss: 1.175475  [   64/  118]
train() client id: f_00009-0-2 loss: 1.204305  [   96/  118]
train() client id: f_00009-1-0 loss: 1.166340  [   32/  118]
train() client id: f_00009-1-1 loss: 1.145691  [   64/  118]
train() client id: f_00009-1-2 loss: 1.097866  [   96/  118]
train() client id: f_00009-2-0 loss: 1.117376  [   32/  118]
train() client id: f_00009-2-1 loss: 1.168629  [   64/  118]
train() client id: f_00009-2-2 loss: 1.018386  [   96/  118]
train() client id: f_00009-3-0 loss: 1.111733  [   32/  118]
train() client id: f_00009-3-1 loss: 0.965282  [   64/  118]
train() client id: f_00009-3-2 loss: 1.009250  [   96/  118]
train() client id: f_00009-4-0 loss: 1.026410  [   32/  118]
train() client id: f_00009-4-1 loss: 0.956245  [   64/  118]
train() client id: f_00009-4-2 loss: 1.080902  [   96/  118]
train() client id: f_00009-5-0 loss: 1.053578  [   32/  118]
train() client id: f_00009-5-1 loss: 0.936439  [   64/  118]
train() client id: f_00009-5-2 loss: 0.965738  [   96/  118]
train() client id: f_00009-6-0 loss: 0.966534  [   32/  118]
train() client id: f_00009-6-1 loss: 0.951147  [   64/  118]
train() client id: f_00009-6-2 loss: 1.008014  [   96/  118]
train() client id: f_00009-7-0 loss: 0.989400  [   32/  118]
train() client id: f_00009-7-1 loss: 0.991420  [   64/  118]
train() client id: f_00009-7-2 loss: 0.859909  [   96/  118]
train() client id: f_00009-8-0 loss: 0.842224  [   32/  118]
train() client id: f_00009-8-1 loss: 1.031204  [   64/  118]
train() client id: f_00009-8-2 loss: 0.984422  [   96/  118]
train() client id: f_00009-9-0 loss: 0.894723  [   32/  118]
train() client id: f_00009-9-1 loss: 0.986767  [   64/  118]
train() client id: f_00009-9-2 loss: 0.911949  [   96/  118]
train() client id: f_00009-10-0 loss: 0.883891  [   32/  118]
train() client id: f_00009-10-1 loss: 0.970582  [   64/  118]
train() client id: f_00009-10-2 loss: 0.934698  [   96/  118]
train() client id: f_00009-11-0 loss: 0.926789  [   32/  118]
train() client id: f_00009-11-1 loss: 0.866748  [   64/  118]
train() client id: f_00009-11-2 loss: 1.010421  [   96/  118]
At round 12 accuracy: 0.6339522546419099
At round 12 training accuracy: 0.5774647887323944
At round 12 training loss: 0.8539642365025962
update_location
xs = [ -3.9056584    4.20031788  80.00902392  18.81129433   0.97929623
   3.95640986 -42.44319194 -21.32485185  64.66397685  -7.06087855]
ys = [ 72.5879595   55.55583871   1.32061395 -42.45517586  34.35018685
  17.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [123.62955161 114.47311424 128.07493092 110.25564272 105.73974824
 101.65135093 108.66607145 102.25177535 120.37483074 100.32879877]
dists_bs = [199.40285797 215.35937777 308.53546217 291.28949744 225.28000941
 238.18645603 221.6347049  232.27980029 286.74612406 239.67486674]
uav_gains = [5.88385192e-11 7.13231094e-11 5.38624893e-11 7.83414303e-11
 8.69762693e-11 9.59875921e-11 8.12381170e-11 9.45846506e-11
 6.28982282e-11 9.91823280e-11]
bs_gains = [4.01833579e-11 3.23918296e-11 1.18369507e-11 1.39054348e-11
 2.85542429e-11 2.44301599e-11 2.98887905e-11 2.62096995e-11
 1.45311815e-11 2.40077301e-11]
Round 13
-------------------------------
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.27548856 19.34614452  9.13389385  3.26646163 22.31552022 10.75645448
  4.0605729  13.0924169   9.62823145  8.73319217]
obj_prev = 109.60837667978073
eta_min = 9.621283683779976e-11	eta_max = 0.9207981058157783
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 25.487961404153893	eta = 0.9090909090909091
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 44.16384428532155	eta = 0.5246570894979226
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 35.24177796016038	eta = 0.6574831164866354
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.64173104871985	eta = 0.6887539160877387
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56259776728117	eta = 0.6903778475206297
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56239040785321	eta = 0.6903821129008305
eta = 0.6903821129008305
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.0305736  0.06430162 0.03008831 0.01043385 0.07425017 0.03542655
 0.01310297 0.04343393 0.03154419 0.02863242]
ene_total = [2.87721447 5.53384282 2.84956312 1.30490774 6.3139176  3.36162512
 1.50604088 3.81675253 3.15491369 2.84361242]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 0 obj = 5.882096701797337
eta = 0.6903821129008305
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
eta_min = 0.6903821129008529	eta_max = 0.6903821129008285
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 0.04372443773870311	eta = 0.909090909090909
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 19.912896282792712	eta = 0.0019961681258650003
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.05768598057864	eta = 0.019317568000433452
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.002030029818999	eta = 0.019854591720065342
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.0020149221690455	eta = 0.019854741547231138
eta = 0.019854741547231138
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.92521000e-04 1.90294690e-03 1.84965388e-04 7.47180525e-06
 2.97469966e-03 3.29684180e-04 1.47562323e-05 5.31384189e-04
 2.52376804e-04 1.74465793e-04]
ene_total = [0.17335085 0.23740048 0.17618224 0.15978245 0.26890909 0.21220106
 0.15887471 0.16699434 0.23902414 0.20929556]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 1 obj = 5.882096701797758
eta = 0.6903821129008529
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
Done!
At round 13 energy consumption: 5.882096701797337
At round 13 eta: 0.6903821129008529
At round 13 local rounds: 12.132381383768099
At round 13 global rounds: 76.641266006878
At round 13 a_n: 23.386960998683932
gradient difference: 0.42224186658859253
train() client id: f_00000-0-0 loss: 1.485131  [   32/  126]
train() client id: f_00000-0-1 loss: 1.276399  [   64/  126]
train() client id: f_00000-0-2 loss: 1.297888  [   96/  126]
train() client id: f_00000-1-0 loss: 1.222142  [   32/  126]
train() client id: f_00000-1-1 loss: 1.316454  [   64/  126]
train() client id: f_00000-1-2 loss: 1.329228  [   96/  126]
train() client id: f_00000-2-0 loss: 1.087042  [   32/  126]
train() client id: f_00000-2-1 loss: 1.212508  [   64/  126]
train() client id: f_00000-2-2 loss: 1.228550  [   96/  126]
train() client id: f_00000-3-0 loss: 0.999190  [   32/  126]
train() client id: f_00000-3-1 loss: 1.146885  [   64/  126]
train() client id: f_00000-3-2 loss: 1.110021  [   96/  126]
train() client id: f_00000-4-0 loss: 0.995552  [   32/  126]
train() client id: f_00000-4-1 loss: 1.056746  [   64/  126]
train() client id: f_00000-4-2 loss: 1.029533  [   96/  126]
train() client id: f_00000-5-0 loss: 1.017650  [   32/  126]
train() client id: f_00000-5-1 loss: 1.045330  [   64/  126]
train() client id: f_00000-5-2 loss: 1.006055  [   96/  126]
train() client id: f_00000-6-0 loss: 1.021847  [   32/  126]
train() client id: f_00000-6-1 loss: 0.967088  [   64/  126]
train() client id: f_00000-6-2 loss: 0.910271  [   96/  126]
train() client id: f_00000-7-0 loss: 0.972793  [   32/  126]
train() client id: f_00000-7-1 loss: 0.907039  [   64/  126]
train() client id: f_00000-7-2 loss: 1.018390  [   96/  126]
train() client id: f_00000-8-0 loss: 0.941848  [   32/  126]
train() client id: f_00000-8-1 loss: 0.937698  [   64/  126]
train() client id: f_00000-8-2 loss: 0.930666  [   96/  126]
train() client id: f_00000-9-0 loss: 0.899207  [   32/  126]
train() client id: f_00000-9-1 loss: 0.927130  [   64/  126]
train() client id: f_00000-9-2 loss: 0.972494  [   96/  126]
train() client id: f_00000-10-0 loss: 0.921514  [   32/  126]
train() client id: f_00000-10-1 loss: 0.993846  [   64/  126]
train() client id: f_00000-10-2 loss: 0.910263  [   96/  126]
train() client id: f_00000-11-0 loss: 0.928690  [   32/  126]
train() client id: f_00000-11-1 loss: 0.940137  [   64/  126]
train() client id: f_00000-11-2 loss: 0.879824  [   96/  126]
train() client id: f_00001-0-0 loss: 0.553598  [   32/  265]
train() client id: f_00001-0-1 loss: 0.405879  [   64/  265]
train() client id: f_00001-0-2 loss: 0.452543  [   96/  265]
train() client id: f_00001-0-3 loss: 0.537572  [  128/  265]
train() client id: f_00001-0-4 loss: 0.447898  [  160/  265]
train() client id: f_00001-0-5 loss: 0.468968  [  192/  265]
train() client id: f_00001-0-6 loss: 0.430004  [  224/  265]
train() client id: f_00001-0-7 loss: 0.517828  [  256/  265]
train() client id: f_00001-1-0 loss: 0.471034  [   32/  265]
train() client id: f_00001-1-1 loss: 0.382962  [   64/  265]
train() client id: f_00001-1-2 loss: 0.517020  [   96/  265]
train() client id: f_00001-1-3 loss: 0.430744  [  128/  265]
train() client id: f_00001-1-4 loss: 0.510551  [  160/  265]
train() client id: f_00001-1-5 loss: 0.516964  [  192/  265]
train() client id: f_00001-1-6 loss: 0.432161  [  224/  265]
train() client id: f_00001-1-7 loss: 0.455873  [  256/  265]
train() client id: f_00001-2-0 loss: 0.497645  [   32/  265]
train() client id: f_00001-2-1 loss: 0.390954  [   64/  265]
train() client id: f_00001-2-2 loss: 0.473573  [   96/  265]
train() client id: f_00001-2-3 loss: 0.555835  [  128/  265]
train() client id: f_00001-2-4 loss: 0.367821  [  160/  265]
train() client id: f_00001-2-5 loss: 0.427181  [  192/  265]
train() client id: f_00001-2-6 loss: 0.428281  [  224/  265]
train() client id: f_00001-2-7 loss: 0.422802  [  256/  265]
train() client id: f_00001-3-0 loss: 0.524043  [   32/  265]
train() client id: f_00001-3-1 loss: 0.353983  [   64/  265]
train() client id: f_00001-3-2 loss: 0.516695  [   96/  265]
train() client id: f_00001-3-3 loss: 0.468697  [  128/  265]
train() client id: f_00001-3-4 loss: 0.380672  [  160/  265]
train() client id: f_00001-3-5 loss: 0.415366  [  192/  265]
train() client id: f_00001-3-6 loss: 0.502107  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411348  [  256/  265]
train() client id: f_00001-4-0 loss: 0.469955  [   32/  265]
train() client id: f_00001-4-1 loss: 0.419071  [   64/  265]
train() client id: f_00001-4-2 loss: 0.403105  [   96/  265]
train() client id: f_00001-4-3 loss: 0.424055  [  128/  265]
train() client id: f_00001-4-4 loss: 0.431731  [  160/  265]
train() client id: f_00001-4-5 loss: 0.452995  [  192/  265]
train() client id: f_00001-4-6 loss: 0.409555  [  224/  265]
train() client id: f_00001-4-7 loss: 0.395673  [  256/  265]
train() client id: f_00001-5-0 loss: 0.393909  [   32/  265]
train() client id: f_00001-5-1 loss: 0.385017  [   64/  265]
train() client id: f_00001-5-2 loss: 0.536260  [   96/  265]
train() client id: f_00001-5-3 loss: 0.399633  [  128/  265]
train() client id: f_00001-5-4 loss: 0.480083  [  160/  265]
train() client id: f_00001-5-5 loss: 0.407486  [  192/  265]
train() client id: f_00001-5-6 loss: 0.485693  [  224/  265]
train() client id: f_00001-5-7 loss: 0.392743  [  256/  265]
train() client id: f_00001-6-0 loss: 0.389009  [   32/  265]
train() client id: f_00001-6-1 loss: 0.466798  [   64/  265]
train() client id: f_00001-6-2 loss: 0.342188  [   96/  265]
train() client id: f_00001-6-3 loss: 0.532039  [  128/  265]
train() client id: f_00001-6-4 loss: 0.372248  [  160/  265]
train() client id: f_00001-6-5 loss: 0.415129  [  192/  265]
train() client id: f_00001-6-6 loss: 0.479163  [  224/  265]
train() client id: f_00001-6-7 loss: 0.462648  [  256/  265]
train() client id: f_00001-7-0 loss: 0.485280  [   32/  265]
train() client id: f_00001-7-1 loss: 0.416030  [   64/  265]
train() client id: f_00001-7-2 loss: 0.333306  [   96/  265]
train() client id: f_00001-7-3 loss: 0.516794  [  128/  265]
train() client id: f_00001-7-4 loss: 0.456002  [  160/  265]
train() client id: f_00001-7-5 loss: 0.482018  [  192/  265]
train() client id: f_00001-7-6 loss: 0.341766  [  224/  265]
train() client id: f_00001-7-7 loss: 0.384520  [  256/  265]
train() client id: f_00001-8-0 loss: 0.418360  [   32/  265]
train() client id: f_00001-8-1 loss: 0.461644  [   64/  265]
train() client id: f_00001-8-2 loss: 0.506728  [   96/  265]
train() client id: f_00001-8-3 loss: 0.388530  [  128/  265]
train() client id: f_00001-8-4 loss: 0.334439  [  160/  265]
train() client id: f_00001-8-5 loss: 0.566690  [  192/  265]
train() client id: f_00001-8-6 loss: 0.318727  [  224/  265]
train() client id: f_00001-8-7 loss: 0.398235  [  256/  265]
train() client id: f_00001-9-0 loss: 0.455386  [   32/  265]
train() client id: f_00001-9-1 loss: 0.416640  [   64/  265]
train() client id: f_00001-9-2 loss: 0.576442  [   96/  265]
train() client id: f_00001-9-3 loss: 0.396117  [  128/  265]
train() client id: f_00001-9-4 loss: 0.450106  [  160/  265]
train() client id: f_00001-9-5 loss: 0.382762  [  192/  265]
train() client id: f_00001-9-6 loss: 0.377783  [  224/  265]
train() client id: f_00001-9-7 loss: 0.338804  [  256/  265]
train() client id: f_00001-10-0 loss: 0.368233  [   32/  265]
train() client id: f_00001-10-1 loss: 0.431798  [   64/  265]
train() client id: f_00001-10-2 loss: 0.381167  [   96/  265]
train() client id: f_00001-10-3 loss: 0.448899  [  128/  265]
train() client id: f_00001-10-4 loss: 0.386007  [  160/  265]
train() client id: f_00001-10-5 loss: 0.332531  [  192/  265]
train() client id: f_00001-10-6 loss: 0.404931  [  224/  265]
train() client id: f_00001-10-7 loss: 0.635288  [  256/  265]
train() client id: f_00001-11-0 loss: 0.388566  [   32/  265]
train() client id: f_00001-11-1 loss: 0.406948  [   64/  265]
train() client id: f_00001-11-2 loss: 0.498587  [   96/  265]
train() client id: f_00001-11-3 loss: 0.373800  [  128/  265]
train() client id: f_00001-11-4 loss: 0.397461  [  160/  265]
train() client id: f_00001-11-5 loss: 0.392280  [  192/  265]
train() client id: f_00001-11-6 loss: 0.466502  [  224/  265]
train() client id: f_00001-11-7 loss: 0.447619  [  256/  265]
train() client id: f_00002-0-0 loss: 1.086637  [   32/  124]
train() client id: f_00002-0-1 loss: 1.044075  [   64/  124]
train() client id: f_00002-0-2 loss: 1.023820  [   96/  124]
train() client id: f_00002-1-0 loss: 1.052060  [   32/  124]
train() client id: f_00002-1-1 loss: 1.166924  [   64/  124]
train() client id: f_00002-1-2 loss: 1.017857  [   96/  124]
train() client id: f_00002-2-0 loss: 0.977590  [   32/  124]
train() client id: f_00002-2-1 loss: 0.974228  [   64/  124]
train() client id: f_00002-2-2 loss: 1.047741  [   96/  124]
train() client id: f_00002-3-0 loss: 0.957539  [   32/  124]
train() client id: f_00002-3-1 loss: 1.052471  [   64/  124]
train() client id: f_00002-3-2 loss: 0.951246  [   96/  124]
train() client id: f_00002-4-0 loss: 0.925414  [   32/  124]
train() client id: f_00002-4-1 loss: 0.970643  [   64/  124]
train() client id: f_00002-4-2 loss: 1.016750  [   96/  124]
train() client id: f_00002-5-0 loss: 0.912703  [   32/  124]
train() client id: f_00002-5-1 loss: 0.929414  [   64/  124]
train() client id: f_00002-5-2 loss: 1.099026  [   96/  124]
train() client id: f_00002-6-0 loss: 0.883140  [   32/  124]
train() client id: f_00002-6-1 loss: 0.949943  [   64/  124]
train() client id: f_00002-6-2 loss: 0.994251  [   96/  124]
train() client id: f_00002-7-0 loss: 0.978829  [   32/  124]
train() client id: f_00002-7-1 loss: 0.948388  [   64/  124]
train() client id: f_00002-7-2 loss: 0.891426  [   96/  124]
train() client id: f_00002-8-0 loss: 1.006072  [   32/  124]
train() client id: f_00002-8-1 loss: 0.902346  [   64/  124]
train() client id: f_00002-8-2 loss: 0.865482  [   96/  124]
train() client id: f_00002-9-0 loss: 0.907413  [   32/  124]
train() client id: f_00002-9-1 loss: 0.894167  [   64/  124]
train() client id: f_00002-9-2 loss: 0.974324  [   96/  124]
train() client id: f_00002-10-0 loss: 0.980319  [   32/  124]
train() client id: f_00002-10-1 loss: 0.840473  [   64/  124]
train() client id: f_00002-10-2 loss: 0.884072  [   96/  124]
train() client id: f_00002-11-0 loss: 0.946562  [   32/  124]
train() client id: f_00002-11-1 loss: 0.946312  [   64/  124]
train() client id: f_00002-11-2 loss: 0.835733  [   96/  124]
train() client id: f_00003-0-0 loss: 0.886617  [   32/   43]
train() client id: f_00003-1-0 loss: 0.854916  [   32/   43]
train() client id: f_00003-2-0 loss: 0.844779  [   32/   43]
train() client id: f_00003-3-0 loss: 0.896148  [   32/   43]
train() client id: f_00003-4-0 loss: 0.912718  [   32/   43]
train() client id: f_00003-5-0 loss: 0.961305  [   32/   43]
train() client id: f_00003-6-0 loss: 0.817275  [   32/   43]
train() client id: f_00003-7-0 loss: 0.873106  [   32/   43]
train() client id: f_00003-8-0 loss: 0.793312  [   32/   43]
train() client id: f_00003-9-0 loss: 1.013118  [   32/   43]
train() client id: f_00003-10-0 loss: 0.876938  [   32/   43]
train() client id: f_00003-11-0 loss: 0.859620  [   32/   43]
train() client id: f_00004-0-0 loss: 0.809772  [   32/  306]
train() client id: f_00004-0-1 loss: 0.871253  [   64/  306]
train() client id: f_00004-0-2 loss: 0.786440  [   96/  306]
train() client id: f_00004-0-3 loss: 0.935309  [  128/  306]
train() client id: f_00004-0-4 loss: 0.889257  [  160/  306]
train() client id: f_00004-0-5 loss: 0.823571  [  192/  306]
train() client id: f_00004-0-6 loss: 1.070807  [  224/  306]
train() client id: f_00004-0-7 loss: 0.853130  [  256/  306]
train() client id: f_00004-0-8 loss: 0.841836  [  288/  306]
train() client id: f_00004-1-0 loss: 1.097191  [   32/  306]
train() client id: f_00004-1-1 loss: 0.853656  [   64/  306]
train() client id: f_00004-1-2 loss: 0.681155  [   96/  306]
train() client id: f_00004-1-3 loss: 0.776401  [  128/  306]
train() client id: f_00004-1-4 loss: 0.778739  [  160/  306]
train() client id: f_00004-1-5 loss: 1.037678  [  192/  306]
train() client id: f_00004-1-6 loss: 0.847900  [  224/  306]
train() client id: f_00004-1-7 loss: 0.981138  [  256/  306]
train() client id: f_00004-1-8 loss: 0.807813  [  288/  306]
train() client id: f_00004-2-0 loss: 0.919845  [   32/  306]
train() client id: f_00004-2-1 loss: 0.906036  [   64/  306]
train() client id: f_00004-2-2 loss: 0.868416  [   96/  306]
train() client id: f_00004-2-3 loss: 0.814684  [  128/  306]
train() client id: f_00004-2-4 loss: 0.939170  [  160/  306]
train() client id: f_00004-2-5 loss: 0.728927  [  192/  306]
train() client id: f_00004-2-6 loss: 0.916829  [  224/  306]
train() client id: f_00004-2-7 loss: 0.938320  [  256/  306]
train() client id: f_00004-2-8 loss: 0.786447  [  288/  306]
train() client id: f_00004-3-0 loss: 0.848459  [   32/  306]
train() client id: f_00004-3-1 loss: 0.798600  [   64/  306]
train() client id: f_00004-3-2 loss: 0.819200  [   96/  306]
train() client id: f_00004-3-3 loss: 0.856915  [  128/  306]
train() client id: f_00004-3-4 loss: 0.888804  [  160/  306]
train() client id: f_00004-3-5 loss: 0.934385  [  192/  306]
train() client id: f_00004-3-6 loss: 0.984423  [  224/  306]
train() client id: f_00004-3-7 loss: 0.860468  [  256/  306]
train() client id: f_00004-3-8 loss: 0.925352  [  288/  306]
train() client id: f_00004-4-0 loss: 0.896724  [   32/  306]
train() client id: f_00004-4-1 loss: 0.925647  [   64/  306]
train() client id: f_00004-4-2 loss: 0.900051  [   96/  306]
train() client id: f_00004-4-3 loss: 0.929032  [  128/  306]
train() client id: f_00004-4-4 loss: 0.932095  [  160/  306]
train() client id: f_00004-4-5 loss: 0.861365  [  192/  306]
train() client id: f_00004-4-6 loss: 0.854438  [  224/  306]
train() client id: f_00004-4-7 loss: 0.844230  [  256/  306]
train() client id: f_00004-4-8 loss: 0.751409  [  288/  306]
train() client id: f_00004-5-0 loss: 0.872262  [   32/  306]
train() client id: f_00004-5-1 loss: 0.883779  [   64/  306]
train() client id: f_00004-5-2 loss: 0.848429  [   96/  306]
train() client id: f_00004-5-3 loss: 0.837917  [  128/  306]
train() client id: f_00004-5-4 loss: 0.868039  [  160/  306]
train() client id: f_00004-5-5 loss: 0.964807  [  192/  306]
train() client id: f_00004-5-6 loss: 0.838473  [  224/  306]
train() client id: f_00004-5-7 loss: 0.772919  [  256/  306]
train() client id: f_00004-5-8 loss: 0.893059  [  288/  306]
train() client id: f_00004-6-0 loss: 0.794815  [   32/  306]
train() client id: f_00004-6-1 loss: 0.961971  [   64/  306]
train() client id: f_00004-6-2 loss: 0.883870  [   96/  306]
train() client id: f_00004-6-3 loss: 0.941005  [  128/  306]
train() client id: f_00004-6-4 loss: 0.864027  [  160/  306]
train() client id: f_00004-6-5 loss: 0.783416  [  192/  306]
train() client id: f_00004-6-6 loss: 0.966177  [  224/  306]
train() client id: f_00004-6-7 loss: 0.786542  [  256/  306]
train() client id: f_00004-6-8 loss: 0.914640  [  288/  306]
train() client id: f_00004-7-0 loss: 0.939400  [   32/  306]
train() client id: f_00004-7-1 loss: 0.797080  [   64/  306]
train() client id: f_00004-7-2 loss: 0.893133  [   96/  306]
train() client id: f_00004-7-3 loss: 0.877722  [  128/  306]
train() client id: f_00004-7-4 loss: 0.784831  [  160/  306]
train() client id: f_00004-7-5 loss: 0.990856  [  192/  306]
train() client id: f_00004-7-6 loss: 0.845371  [  224/  306]
train() client id: f_00004-7-7 loss: 0.826705  [  256/  306]
train() client id: f_00004-7-8 loss: 0.933728  [  288/  306]
train() client id: f_00004-8-0 loss: 0.877918  [   32/  306]
train() client id: f_00004-8-1 loss: 0.900072  [   64/  306]
train() client id: f_00004-8-2 loss: 0.786223  [   96/  306]
train() client id: f_00004-8-3 loss: 0.978873  [  128/  306]
train() client id: f_00004-8-4 loss: 0.753336  [  160/  306]
train() client id: f_00004-8-5 loss: 0.996408  [  192/  306]
train() client id: f_00004-8-6 loss: 0.836644  [  224/  306]
train() client id: f_00004-8-7 loss: 0.904422  [  256/  306]
train() client id: f_00004-8-8 loss: 0.887113  [  288/  306]
train() client id: f_00004-9-0 loss: 0.959561  [   32/  306]
train() client id: f_00004-9-1 loss: 0.921002  [   64/  306]
train() client id: f_00004-9-2 loss: 0.868237  [   96/  306]
train() client id: f_00004-9-3 loss: 0.859319  [  128/  306]
train() client id: f_00004-9-4 loss: 0.875249  [  160/  306]
train() client id: f_00004-9-5 loss: 0.831546  [  192/  306]
train() client id: f_00004-9-6 loss: 0.910115  [  224/  306]
train() client id: f_00004-9-7 loss: 0.782903  [  256/  306]
train() client id: f_00004-9-8 loss: 0.974848  [  288/  306]
train() client id: f_00004-10-0 loss: 0.766824  [   32/  306]
train() client id: f_00004-10-1 loss: 0.938516  [   64/  306]
train() client id: f_00004-10-2 loss: 0.862903  [   96/  306]
train() client id: f_00004-10-3 loss: 0.977292  [  128/  306]
train() client id: f_00004-10-4 loss: 0.891631  [  160/  306]
train() client id: f_00004-10-5 loss: 0.918541  [  192/  306]
train() client id: f_00004-10-6 loss: 0.874191  [  224/  306]
train() client id: f_00004-10-7 loss: 0.930212  [  256/  306]
train() client id: f_00004-10-8 loss: 0.805622  [  288/  306]
train() client id: f_00004-11-0 loss: 0.821588  [   32/  306]
train() client id: f_00004-11-1 loss: 0.869649  [   64/  306]
train() client id: f_00004-11-2 loss: 0.791269  [   96/  306]
train() client id: f_00004-11-3 loss: 0.855502  [  128/  306]
train() client id: f_00004-11-4 loss: 0.903720  [  160/  306]
train() client id: f_00004-11-5 loss: 0.853601  [  192/  306]
train() client id: f_00004-11-6 loss: 0.893017  [  224/  306]
train() client id: f_00004-11-7 loss: 1.065844  [  256/  306]
train() client id: f_00004-11-8 loss: 0.878843  [  288/  306]
train() client id: f_00005-0-0 loss: 0.779358  [   32/  146]
train() client id: f_00005-0-1 loss: 0.802908  [   64/  146]
train() client id: f_00005-0-2 loss: 0.651770  [   96/  146]
train() client id: f_00005-0-3 loss: 0.751460  [  128/  146]
train() client id: f_00005-1-0 loss: 0.975091  [   32/  146]
train() client id: f_00005-1-1 loss: 0.714232  [   64/  146]
train() client id: f_00005-1-2 loss: 0.788803  [   96/  146]
train() client id: f_00005-1-3 loss: 0.589018  [  128/  146]
train() client id: f_00005-2-0 loss: 0.704786  [   32/  146]
train() client id: f_00005-2-1 loss: 0.689609  [   64/  146]
train() client id: f_00005-2-2 loss: 0.771659  [   96/  146]
train() client id: f_00005-2-3 loss: 0.785368  [  128/  146]
train() client id: f_00005-3-0 loss: 0.723326  [   32/  146]
train() client id: f_00005-3-1 loss: 0.634370  [   64/  146]
train() client id: f_00005-3-2 loss: 0.717089  [   96/  146]
train() client id: f_00005-3-3 loss: 1.013475  [  128/  146]
train() client id: f_00005-4-0 loss: 0.812198  [   32/  146]
train() client id: f_00005-4-1 loss: 0.568612  [   64/  146]
train() client id: f_00005-4-2 loss: 0.920229  [   96/  146]
train() client id: f_00005-4-3 loss: 0.805967  [  128/  146]
train() client id: f_00005-5-0 loss: 0.666417  [   32/  146]
train() client id: f_00005-5-1 loss: 0.704985  [   64/  146]
train() client id: f_00005-5-2 loss: 0.758885  [   96/  146]
train() client id: f_00005-5-3 loss: 0.881899  [  128/  146]
train() client id: f_00005-6-0 loss: 0.653184  [   32/  146]
train() client id: f_00005-6-1 loss: 0.796675  [   64/  146]
train() client id: f_00005-6-2 loss: 0.639155  [   96/  146]
train() client id: f_00005-6-3 loss: 0.846910  [  128/  146]
train() client id: f_00005-7-0 loss: 0.743598  [   32/  146]
train() client id: f_00005-7-1 loss: 0.833226  [   64/  146]
train() client id: f_00005-7-2 loss: 0.647093  [   96/  146]
train() client id: f_00005-7-3 loss: 0.760288  [  128/  146]
train() client id: f_00005-8-0 loss: 0.711486  [   32/  146]
train() client id: f_00005-8-1 loss: 0.872342  [   64/  146]
train() client id: f_00005-8-2 loss: 0.738068  [   96/  146]
train() client id: f_00005-8-3 loss: 0.698593  [  128/  146]
train() client id: f_00005-9-0 loss: 0.935853  [   32/  146]
train() client id: f_00005-9-1 loss: 0.888100  [   64/  146]
train() client id: f_00005-9-2 loss: 0.613349  [   96/  146]
train() client id: f_00005-9-3 loss: 0.568264  [  128/  146]
train() client id: f_00005-10-0 loss: 0.686867  [   32/  146]
train() client id: f_00005-10-1 loss: 0.837632  [   64/  146]
train() client id: f_00005-10-2 loss: 0.874013  [   96/  146]
train() client id: f_00005-10-3 loss: 0.651971  [  128/  146]
train() client id: f_00005-11-0 loss: 0.854451  [   32/  146]
train() client id: f_00005-11-1 loss: 0.681026  [   64/  146]
train() client id: f_00005-11-2 loss: 0.503034  [   96/  146]
train() client id: f_00005-11-3 loss: 0.866974  [  128/  146]
train() client id: f_00006-0-0 loss: 0.594608  [   32/   54]
train() client id: f_00006-1-0 loss: 0.589290  [   32/   54]
train() client id: f_00006-2-0 loss: 0.619511  [   32/   54]
train() client id: f_00006-3-0 loss: 0.581078  [   32/   54]
train() client id: f_00006-4-0 loss: 0.636362  [   32/   54]
train() client id: f_00006-5-0 loss: 0.631198  [   32/   54]
train() client id: f_00006-6-0 loss: 0.600253  [   32/   54]
train() client id: f_00006-7-0 loss: 0.542608  [   32/   54]
train() client id: f_00006-8-0 loss: 0.583488  [   32/   54]
train() client id: f_00006-9-0 loss: 0.571660  [   32/   54]
train() client id: f_00006-10-0 loss: 0.573044  [   32/   54]
train() client id: f_00006-11-0 loss: 0.584773  [   32/   54]
train() client id: f_00007-0-0 loss: 0.555214  [   32/  179]
train() client id: f_00007-0-1 loss: 0.659834  [   64/  179]
train() client id: f_00007-0-2 loss: 0.647528  [   96/  179]
train() client id: f_00007-0-3 loss: 0.597630  [  128/  179]
train() client id: f_00007-0-4 loss: 0.542080  [  160/  179]
train() client id: f_00007-1-0 loss: 0.654140  [   32/  179]
train() client id: f_00007-1-1 loss: 0.561337  [   64/  179]
train() client id: f_00007-1-2 loss: 0.594801  [   96/  179]
train() client id: f_00007-1-3 loss: 0.583346  [  128/  179]
train() client id: f_00007-1-4 loss: 0.766556  [  160/  179]
train() client id: f_00007-2-0 loss: 0.606544  [   32/  179]
train() client id: f_00007-2-1 loss: 0.613261  [   64/  179]
train() client id: f_00007-2-2 loss: 0.503470  [   96/  179]
train() client id: f_00007-2-3 loss: 0.794921  [  128/  179]
train() client id: f_00007-2-4 loss: 0.576424  [  160/  179]
train() client id: f_00007-3-0 loss: 0.684832  [   32/  179]
train() client id: f_00007-3-1 loss: 0.635236  [   64/  179]
train() client id: f_00007-3-2 loss: 0.511283  [   96/  179]
train() client id: f_00007-3-3 loss: 0.457706  [  128/  179]
train() client id: f_00007-3-4 loss: 0.583153  [  160/  179]
train() client id: f_00007-4-0 loss: 0.533552  [   32/  179]
train() client id: f_00007-4-1 loss: 0.674057  [   64/  179]
train() client id: f_00007-4-2 loss: 0.583999  [   96/  179]
train() client id: f_00007-4-3 loss: 0.546472  [  128/  179]
train() client id: f_00007-4-4 loss: 0.525228  [  160/  179]
train() client id: f_00007-5-0 loss: 0.669893  [   32/  179]
train() client id: f_00007-5-1 loss: 0.618324  [   64/  179]
train() client id: f_00007-5-2 loss: 0.532222  [   96/  179]
train() client id: f_00007-5-3 loss: 0.568704  [  128/  179]
train() client id: f_00007-5-4 loss: 0.467846  [  160/  179]
train() client id: f_00007-6-0 loss: 0.678746  [   32/  179]
train() client id: f_00007-6-1 loss: 0.529444  [   64/  179]
train() client id: f_00007-6-2 loss: 0.608523  [   96/  179]
train() client id: f_00007-6-3 loss: 0.603042  [  128/  179]
train() client id: f_00007-6-4 loss: 0.525049  [  160/  179]
train() client id: f_00007-7-0 loss: 0.657957  [   32/  179]
train() client id: f_00007-7-1 loss: 0.538723  [   64/  179]
train() client id: f_00007-7-2 loss: 0.473417  [   96/  179]
train() client id: f_00007-7-3 loss: 0.670510  [  128/  179]
train() client id: f_00007-7-4 loss: 0.592474  [  160/  179]
train() client id: f_00007-8-0 loss: 0.630463  [   32/  179]
train() client id: f_00007-8-1 loss: 0.468024  [   64/  179]
train() client id: f_00007-8-2 loss: 0.614063  [   96/  179]
train() client id: f_00007-8-3 loss: 0.559484  [  128/  179]
train() client id: f_00007-8-4 loss: 0.506963  [  160/  179]
train() client id: f_00007-9-0 loss: 0.626463  [   32/  179]
train() client id: f_00007-9-1 loss: 0.597089  [   64/  179]
train() client id: f_00007-9-2 loss: 0.574280  [   96/  179]
train() client id: f_00007-9-3 loss: 0.652704  [  128/  179]
train() client id: f_00007-9-4 loss: 0.440629  [  160/  179]
train() client id: f_00007-10-0 loss: 0.552253  [   32/  179]
train() client id: f_00007-10-1 loss: 0.690928  [   64/  179]
train() client id: f_00007-10-2 loss: 0.691937  [   96/  179]
train() client id: f_00007-10-3 loss: 0.440568  [  128/  179]
train() client id: f_00007-10-4 loss: 0.520912  [  160/  179]
train() client id: f_00007-11-0 loss: 0.554520  [   32/  179]
train() client id: f_00007-11-1 loss: 0.636641  [   64/  179]
train() client id: f_00007-11-2 loss: 0.681779  [   96/  179]
train() client id: f_00007-11-3 loss: 0.501291  [  128/  179]
train() client id: f_00007-11-4 loss: 0.436324  [  160/  179]
train() client id: f_00008-0-0 loss: 0.829866  [   32/  130]
train() client id: f_00008-0-1 loss: 1.001014  [   64/  130]
train() client id: f_00008-0-2 loss: 0.743626  [   96/  130]
train() client id: f_00008-0-3 loss: 0.817799  [  128/  130]
train() client id: f_00008-1-0 loss: 0.796016  [   32/  130]
train() client id: f_00008-1-1 loss: 0.783261  [   64/  130]
train() client id: f_00008-1-2 loss: 0.865234  [   96/  130]
train() client id: f_00008-1-3 loss: 0.911275  [  128/  130]
train() client id: f_00008-2-0 loss: 0.861230  [   32/  130]
train() client id: f_00008-2-1 loss: 0.821511  [   64/  130]
train() client id: f_00008-2-2 loss: 0.733572  [   96/  130]
train() client id: f_00008-2-3 loss: 0.940637  [  128/  130]
train() client id: f_00008-3-0 loss: 0.934651  [   32/  130]
train() client id: f_00008-3-1 loss: 0.778857  [   64/  130]
train() client id: f_00008-3-2 loss: 0.851348  [   96/  130]
train() client id: f_00008-3-3 loss: 0.756196  [  128/  130]
train() client id: f_00008-4-0 loss: 0.778821  [   32/  130]
train() client id: f_00008-4-1 loss: 0.878088  [   64/  130]
train() client id: f_00008-4-2 loss: 0.876596  [   96/  130]
train() client id: f_00008-4-3 loss: 0.790838  [  128/  130]
train() client id: f_00008-5-0 loss: 0.761941  [   32/  130]
train() client id: f_00008-5-1 loss: 0.932019  [   64/  130]
train() client id: f_00008-5-2 loss: 0.831974  [   96/  130]
train() client id: f_00008-5-3 loss: 0.875483  [  128/  130]
train() client id: f_00008-6-0 loss: 0.772634  [   32/  130]
train() client id: f_00008-6-1 loss: 0.925696  [   64/  130]
train() client id: f_00008-6-2 loss: 0.951691  [   96/  130]
train() client id: f_00008-6-3 loss: 0.746576  [  128/  130]
train() client id: f_00008-7-0 loss: 0.845374  [   32/  130]
train() client id: f_00008-7-1 loss: 0.873620  [   64/  130]
train() client id: f_00008-7-2 loss: 0.927162  [   96/  130]
train() client id: f_00008-7-3 loss: 0.743548  [  128/  130]
train() client id: f_00008-8-0 loss: 0.832172  [   32/  130]
train() client id: f_00008-8-1 loss: 0.843966  [   64/  130]
train() client id: f_00008-8-2 loss: 0.768200  [   96/  130]
train() client id: f_00008-8-3 loss: 0.918406  [  128/  130]
train() client id: f_00008-9-0 loss: 0.839796  [   32/  130]
train() client id: f_00008-9-1 loss: 0.791875  [   64/  130]
train() client id: f_00008-9-2 loss: 0.831787  [   96/  130]
train() client id: f_00008-9-3 loss: 0.927710  [  128/  130]
train() client id: f_00008-10-0 loss: 0.860860  [   32/  130]
train() client id: f_00008-10-1 loss: 0.844523  [   64/  130]
train() client id: f_00008-10-2 loss: 0.852512  [   96/  130]
train() client id: f_00008-10-3 loss: 0.837589  [  128/  130]
train() client id: f_00008-11-0 loss: 0.796638  [   32/  130]
train() client id: f_00008-11-1 loss: 0.894433  [   64/  130]
train() client id: f_00008-11-2 loss: 0.862368  [   96/  130]
train() client id: f_00008-11-3 loss: 0.842374  [  128/  130]
train() client id: f_00009-0-0 loss: 1.206981  [   32/  118]
train() client id: f_00009-0-1 loss: 1.189249  [   64/  118]
train() client id: f_00009-0-2 loss: 0.971651  [   96/  118]
train() client id: f_00009-1-0 loss: 1.079196  [   32/  118]
train() client id: f_00009-1-1 loss: 1.046076  [   64/  118]
train() client id: f_00009-1-2 loss: 1.045562  [   96/  118]
train() client id: f_00009-2-0 loss: 0.940997  [   32/  118]
train() client id: f_00009-2-1 loss: 1.028040  [   64/  118]
train() client id: f_00009-2-2 loss: 1.014390  [   96/  118]
train() client id: f_00009-3-0 loss: 0.914817  [   32/  118]
train() client id: f_00009-3-1 loss: 0.992095  [   64/  118]
train() client id: f_00009-3-2 loss: 0.979119  [   96/  118]
train() client id: f_00009-4-0 loss: 0.931512  [   32/  118]
train() client id: f_00009-4-1 loss: 0.877823  [   64/  118]
train() client id: f_00009-4-2 loss: 0.937420  [   96/  118]
train() client id: f_00009-5-0 loss: 0.896933  [   32/  118]
train() client id: f_00009-5-1 loss: 0.958836  [   64/  118]
train() client id: f_00009-5-2 loss: 0.874607  [   96/  118]
train() client id: f_00009-6-0 loss: 0.953072  [   32/  118]
train() client id: f_00009-6-1 loss: 0.877983  [   64/  118]
train() client id: f_00009-6-2 loss: 0.852547  [   96/  118]
train() client id: f_00009-7-0 loss: 0.843287  [   32/  118]
train() client id: f_00009-7-1 loss: 0.856775  [   64/  118]
train() client id: f_00009-7-2 loss: 0.863135  [   96/  118]
train() client id: f_00009-8-0 loss: 0.838903  [   32/  118]
train() client id: f_00009-8-1 loss: 0.809654  [   64/  118]
train() client id: f_00009-8-2 loss: 0.855780  [   96/  118]
train() client id: f_00009-9-0 loss: 0.846438  [   32/  118]
train() client id: f_00009-9-1 loss: 0.801293  [   64/  118]
train() client id: f_00009-9-2 loss: 0.798186  [   96/  118]
train() client id: f_00009-10-0 loss: 0.827794  [   32/  118]
train() client id: f_00009-10-1 loss: 0.904184  [   64/  118]
train() client id: f_00009-10-2 loss: 0.781666  [   96/  118]
train() client id: f_00009-11-0 loss: 0.947468  [   32/  118]
train() client id: f_00009-11-1 loss: 0.731184  [   64/  118]
train() client id: f_00009-11-2 loss: 0.848270  [   96/  118]
At round 13 accuracy: 0.6339522546419099
At round 13 training accuracy: 0.5788061703554661
At round 13 training loss: 0.8428088353736513
update_location
xs = [ -3.9056584    4.20031788  85.00902392  18.81129433   0.97929623
   3.95640986 -47.44319194 -26.32485185  69.66397685 -12.06087855]
ys = [ 77.5879595   60.55583871   1.32061395 -47.45517586  39.35018685
  22.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [126.62995549 116.98141849 131.25653572 112.27581445 107.46811725
 102.64569476 110.71470997 103.41022232 123.13301606 100.80414995]
dists_bs = [196.88163795 212.62695027 312.6807024  295.04088374 222.19267429
 234.91685624 218.6809876  229.00251994 290.93810241 236.19832881]
uav_gains = [5.54132201e-11 6.75605000e-11 5.06550612e-11 7.48645336e-11
 8.35211431e-11 9.36797744e-11 7.75317969e-11 9.19578362e-11
 5.94337263e-11 9.80171753e-11]
bs_gains = [4.16408405e-11 3.35708871e-11 1.14027884e-11 1.34160270e-11
 2.96791066e-11 2.53941909e-11 3.10329607e-11 2.72735301e-11
 1.39525118e-11 2.50103050e-11]
Round 14
-------------------------------
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.14357806 19.06538973  9.00414718  3.22065168 21.99169094 10.5993586
  4.00331986 12.90419715  9.4917528   8.60518442]
obj_prev = 108.02927042739734
eta_min = 6.894580279323513e-11	eta_max = 0.920963466545226
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 25.120031493789718	eta = 0.9090909090909091
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 43.56050107516789	eta = 0.5242453990066629
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 34.74720289395531	eta = 0.6572152681404528
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.166400765192755	eta = 0.6885399603277947
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08814089362885	eta = 0.6901684908951391
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08793538155577	eta = 0.6901727775922599
eta = 0.6901727775922599
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.03059863 0.06435425 0.03011293 0.01044239 0.07431095 0.03545555
 0.0131137  0.04346948 0.03157001 0.02865586]
ene_total = [2.84199716 5.44950306 2.81511635 1.29013739 6.21774054 3.30722781
 1.48846279 3.76427446 3.1172366  2.79623923]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 0 obj = 5.8017097996113725
eta = 0.6901727775922599
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
eta_min = 0.6901727775922659	eta_max = 0.6901727775922564
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 0.041597769241550876	eta = 0.9090909090909091
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 19.658200800192578	eta = 0.0019236833645317572
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 2.023882036535733	eta = 0.01868495948542783
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708396260580614	eta = 0.01918783921124654
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708261485414031	eta = 0.01918797042749958
eta = 0.01918797042749958
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.89202933e-04 1.85088870e-03 1.81815891e-04 7.33300812e-06
 2.89081782e-03 3.20149601e-04 1.44834795e-05 5.20846430e-04
 2.47774470e-04 1.69356454e-04]
ene_total = [0.17265704 0.23107452 0.17557065 0.15872099 0.26113616 0.20692952
 0.15784421 0.16500284 0.23783644 0.20405377]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 1 obj = 5.801709799611484
eta = 0.6901727775922659
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
Done!
At round 14 energy consumption: 5.8017097996113725
At round 14 eta: 0.6901727775922659
At round 14 local rounds: 12.142311740979952
At round 14 global rounds: 75.48388039288098
At round 14 a_n: 23.044415151714617
gradient difference: 0.4388906955718994
train() client id: f_00000-0-0 loss: 1.539716  [   32/  126]
train() client id: f_00000-0-1 loss: 1.452693  [   64/  126]
train() client id: f_00000-0-2 loss: 1.220508  [   96/  126]
train() client id: f_00000-1-0 loss: 1.317484  [   32/  126]
train() client id: f_00000-1-1 loss: 1.115243  [   64/  126]
train() client id: f_00000-1-2 loss: 1.156668  [   96/  126]
train() client id: f_00000-2-0 loss: 1.260911  [   32/  126]
train() client id: f_00000-2-1 loss: 1.253177  [   64/  126]
train() client id: f_00000-2-2 loss: 1.051458  [   96/  126]
train() client id: f_00000-3-0 loss: 1.233171  [   32/  126]
train() client id: f_00000-3-1 loss: 1.075841  [   64/  126]
train() client id: f_00000-3-2 loss: 0.985847  [   96/  126]
train() client id: f_00000-4-0 loss: 1.040026  [   32/  126]
train() client id: f_00000-4-1 loss: 0.969695  [   64/  126]
train() client id: f_00000-4-2 loss: 1.085631  [   96/  126]
train() client id: f_00000-5-0 loss: 1.016885  [   32/  126]
train() client id: f_00000-5-1 loss: 1.066380  [   64/  126]
train() client id: f_00000-5-2 loss: 1.003662  [   96/  126]
train() client id: f_00000-6-0 loss: 0.942353  [   32/  126]
train() client id: f_00000-6-1 loss: 1.043273  [   64/  126]
train() client id: f_00000-6-2 loss: 0.944428  [   96/  126]
train() client id: f_00000-7-0 loss: 0.979913  [   32/  126]
train() client id: f_00000-7-1 loss: 0.968809  [   64/  126]
train() client id: f_00000-7-2 loss: 1.013370  [   96/  126]
train() client id: f_00000-8-0 loss: 0.930900  [   32/  126]
train() client id: f_00000-8-1 loss: 0.923553  [   64/  126]
train() client id: f_00000-8-2 loss: 0.900688  [   96/  126]
train() client id: f_00000-9-0 loss: 0.917189  [   32/  126]
train() client id: f_00000-9-1 loss: 0.962367  [   64/  126]
train() client id: f_00000-9-2 loss: 0.910508  [   96/  126]
train() client id: f_00000-10-0 loss: 0.856133  [   32/  126]
train() client id: f_00000-10-1 loss: 0.915569  [   64/  126]
train() client id: f_00000-10-2 loss: 1.033503  [   96/  126]
train() client id: f_00000-11-0 loss: 0.938522  [   32/  126]
train() client id: f_00000-11-1 loss: 0.915764  [   64/  126]
train() client id: f_00000-11-2 loss: 0.991159  [   96/  126]
train() client id: f_00001-0-0 loss: 0.415810  [   32/  265]
train() client id: f_00001-0-1 loss: 0.472700  [   64/  265]
train() client id: f_00001-0-2 loss: 0.547636  [   96/  265]
train() client id: f_00001-0-3 loss: 0.548459  [  128/  265]
train() client id: f_00001-0-4 loss: 0.441186  [  160/  265]
train() client id: f_00001-0-5 loss: 0.582576  [  192/  265]
train() client id: f_00001-0-6 loss: 0.536419  [  224/  265]
train() client id: f_00001-0-7 loss: 0.395432  [  256/  265]
train() client id: f_00001-1-0 loss: 0.414433  [   32/  265]
train() client id: f_00001-1-1 loss: 0.538319  [   64/  265]
train() client id: f_00001-1-2 loss: 0.458657  [   96/  265]
train() client id: f_00001-1-3 loss: 0.570927  [  128/  265]
train() client id: f_00001-1-4 loss: 0.406490  [  160/  265]
train() client id: f_00001-1-5 loss: 0.466666  [  192/  265]
train() client id: f_00001-1-6 loss: 0.599976  [  224/  265]
train() client id: f_00001-1-7 loss: 0.409105  [  256/  265]
train() client id: f_00001-2-0 loss: 0.400182  [   32/  265]
train() client id: f_00001-2-1 loss: 0.494721  [   64/  265]
train() client id: f_00001-2-2 loss: 0.543431  [   96/  265]
train() client id: f_00001-2-3 loss: 0.525110  [  128/  265]
train() client id: f_00001-2-4 loss: 0.580419  [  160/  265]
train() client id: f_00001-2-5 loss: 0.426434  [  192/  265]
train() client id: f_00001-2-6 loss: 0.371450  [  224/  265]
train() client id: f_00001-2-7 loss: 0.439286  [  256/  265]
train() client id: f_00001-3-0 loss: 0.529071  [   32/  265]
train() client id: f_00001-3-1 loss: 0.518413  [   64/  265]
train() client id: f_00001-3-2 loss: 0.363118  [   96/  265]
train() client id: f_00001-3-3 loss: 0.497645  [  128/  265]
train() client id: f_00001-3-4 loss: 0.402834  [  160/  265]
train() client id: f_00001-3-5 loss: 0.461961  [  192/  265]
train() client id: f_00001-3-6 loss: 0.438148  [  224/  265]
train() client id: f_00001-3-7 loss: 0.509542  [  256/  265]
train() client id: f_00001-4-0 loss: 0.496696  [   32/  265]
train() client id: f_00001-4-1 loss: 0.427723  [   64/  265]
train() client id: f_00001-4-2 loss: 0.369776  [   96/  265]
train() client id: f_00001-4-3 loss: 0.446383  [  128/  265]
train() client id: f_00001-4-4 loss: 0.448717  [  160/  265]
train() client id: f_00001-4-5 loss: 0.357336  [  192/  265]
train() client id: f_00001-4-6 loss: 0.705897  [  224/  265]
train() client id: f_00001-4-7 loss: 0.427547  [  256/  265]
train() client id: f_00001-5-0 loss: 0.567723  [   32/  265]
train() client id: f_00001-5-1 loss: 0.555162  [   64/  265]
train() client id: f_00001-5-2 loss: 0.388521  [   96/  265]
train() client id: f_00001-5-3 loss: 0.358555  [  128/  265]
train() client id: f_00001-5-4 loss: 0.422059  [  160/  265]
train() client id: f_00001-5-5 loss: 0.352690  [  192/  265]
train() client id: f_00001-5-6 loss: 0.462272  [  224/  265]
train() client id: f_00001-5-7 loss: 0.536973  [  256/  265]
train() client id: f_00001-6-0 loss: 0.470938  [   32/  265]
train() client id: f_00001-6-1 loss: 0.478223  [   64/  265]
train() client id: f_00001-6-2 loss: 0.453931  [   96/  265]
train() client id: f_00001-6-3 loss: 0.428919  [  128/  265]
train() client id: f_00001-6-4 loss: 0.576747  [  160/  265]
train() client id: f_00001-6-5 loss: 0.363294  [  192/  265]
train() client id: f_00001-6-6 loss: 0.417134  [  224/  265]
train() client id: f_00001-6-7 loss: 0.437903  [  256/  265]
train() client id: f_00001-7-0 loss: 0.419979  [   32/  265]
train() client id: f_00001-7-1 loss: 0.404149  [   64/  265]
train() client id: f_00001-7-2 loss: 0.500804  [   96/  265]
train() client id: f_00001-7-3 loss: 0.433446  [  128/  265]
train() client id: f_00001-7-4 loss: 0.545168  [  160/  265]
train() client id: f_00001-7-5 loss: 0.411344  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432756  [  224/  265]
train() client id: f_00001-7-7 loss: 0.454895  [  256/  265]
train() client id: f_00001-8-0 loss: 0.496283  [   32/  265]
train() client id: f_00001-8-1 loss: 0.487322  [   64/  265]
train() client id: f_00001-8-2 loss: 0.367131  [   96/  265]
train() client id: f_00001-8-3 loss: 0.434349  [  128/  265]
train() client id: f_00001-8-4 loss: 0.441741  [  160/  265]
train() client id: f_00001-8-5 loss: 0.475915  [  192/  265]
train() client id: f_00001-8-6 loss: 0.496993  [  224/  265]
train() client id: f_00001-8-7 loss: 0.349789  [  256/  265]
train() client id: f_00001-9-0 loss: 0.403928  [   32/  265]
train() client id: f_00001-9-1 loss: 0.378903  [   64/  265]
train() client id: f_00001-9-2 loss: 0.482913  [   96/  265]
train() client id: f_00001-9-3 loss: 0.454513  [  128/  265]
train() client id: f_00001-9-4 loss: 0.456922  [  160/  265]
train() client id: f_00001-9-5 loss: 0.548296  [  192/  265]
train() client id: f_00001-9-6 loss: 0.427141  [  224/  265]
train() client id: f_00001-9-7 loss: 0.369996  [  256/  265]
train() client id: f_00001-10-0 loss: 0.414490  [   32/  265]
train() client id: f_00001-10-1 loss: 0.480865  [   64/  265]
train() client id: f_00001-10-2 loss: 0.344505  [   96/  265]
train() client id: f_00001-10-3 loss: 0.494046  [  128/  265]
train() client id: f_00001-10-4 loss: 0.413085  [  160/  265]
train() client id: f_00001-10-5 loss: 0.402477  [  192/  265]
train() client id: f_00001-10-6 loss: 0.436672  [  224/  265]
train() client id: f_00001-10-7 loss: 0.515951  [  256/  265]
train() client id: f_00001-11-0 loss: 0.507164  [   32/  265]
train() client id: f_00001-11-1 loss: 0.350113  [   64/  265]
train() client id: f_00001-11-2 loss: 0.430155  [   96/  265]
train() client id: f_00001-11-3 loss: 0.429933  [  128/  265]
train() client id: f_00001-11-4 loss: 0.475260  [  160/  265]
train() client id: f_00001-11-5 loss: 0.357180  [  192/  265]
train() client id: f_00001-11-6 loss: 0.398865  [  224/  265]
train() client id: f_00001-11-7 loss: 0.518131  [  256/  265]
train() client id: f_00002-0-0 loss: 1.242807  [   32/  124]
train() client id: f_00002-0-1 loss: 1.267646  [   64/  124]
train() client id: f_00002-0-2 loss: 1.225684  [   96/  124]
train() client id: f_00002-1-0 loss: 1.203316  [   32/  124]
train() client id: f_00002-1-1 loss: 1.216329  [   64/  124]
train() client id: f_00002-1-2 loss: 1.179913  [   96/  124]
train() client id: f_00002-2-0 loss: 1.147912  [   32/  124]
train() client id: f_00002-2-1 loss: 1.133452  [   64/  124]
train() client id: f_00002-2-2 loss: 1.212169  [   96/  124]
train() client id: f_00002-3-0 loss: 1.222893  [   32/  124]
train() client id: f_00002-3-1 loss: 1.083481  [   64/  124]
train() client id: f_00002-3-2 loss: 1.174080  [   96/  124]
train() client id: f_00002-4-0 loss: 1.168372  [   32/  124]
train() client id: f_00002-4-1 loss: 1.159972  [   64/  124]
train() client id: f_00002-4-2 loss: 1.116934  [   96/  124]
train() client id: f_00002-5-0 loss: 1.116155  [   32/  124]
train() client id: f_00002-5-1 loss: 1.173671  [   64/  124]
train() client id: f_00002-5-2 loss: 1.019303  [   96/  124]
train() client id: f_00002-6-0 loss: 1.045794  [   32/  124]
train() client id: f_00002-6-1 loss: 1.130567  [   64/  124]
train() client id: f_00002-6-2 loss: 1.114016  [   96/  124]
train() client id: f_00002-7-0 loss: 1.160853  [   32/  124]
train() client id: f_00002-7-1 loss: 1.057494  [   64/  124]
train() client id: f_00002-7-2 loss: 1.057025  [   96/  124]
train() client id: f_00002-8-0 loss: 1.075460  [   32/  124]
train() client id: f_00002-8-1 loss: 1.131817  [   64/  124]
train() client id: f_00002-8-2 loss: 1.039694  [   96/  124]
train() client id: f_00002-9-0 loss: 1.137831  [   32/  124]
train() client id: f_00002-9-1 loss: 1.191305  [   64/  124]
train() client id: f_00002-9-2 loss: 1.027532  [   96/  124]
train() client id: f_00002-10-0 loss: 1.044664  [   32/  124]
train() client id: f_00002-10-1 loss: 0.957369  [   64/  124]
train() client id: f_00002-10-2 loss: 1.279276  [   96/  124]
train() client id: f_00002-11-0 loss: 1.098520  [   32/  124]
train() client id: f_00002-11-1 loss: 1.088359  [   64/  124]
train() client id: f_00002-11-2 loss: 1.113166  [   96/  124]
train() client id: f_00003-0-0 loss: 0.796582  [   32/   43]
train() client id: f_00003-1-0 loss: 0.783231  [   32/   43]
train() client id: f_00003-2-0 loss: 0.824166  [   32/   43]
train() client id: f_00003-3-0 loss: 0.785349  [   32/   43]
train() client id: f_00003-4-0 loss: 0.705318  [   32/   43]
train() client id: f_00003-5-0 loss: 0.665387  [   32/   43]
train() client id: f_00003-6-0 loss: 0.863358  [   32/   43]
train() client id: f_00003-7-0 loss: 0.687288  [   32/   43]
train() client id: f_00003-8-0 loss: 0.744096  [   32/   43]
train() client id: f_00003-9-0 loss: 0.770745  [   32/   43]
train() client id: f_00003-10-0 loss: 0.696694  [   32/   43]
train() client id: f_00003-11-0 loss: 0.811575  [   32/   43]
train() client id: f_00004-0-0 loss: 0.959952  [   32/  306]
train() client id: f_00004-0-1 loss: 1.127076  [   64/  306]
train() client id: f_00004-0-2 loss: 0.901024  [   96/  306]
train() client id: f_00004-0-3 loss: 0.919913  [  128/  306]
train() client id: f_00004-0-4 loss: 0.974707  [  160/  306]
train() client id: f_00004-0-5 loss: 0.893983  [  192/  306]
train() client id: f_00004-0-6 loss: 0.995858  [  224/  306]
train() client id: f_00004-0-7 loss: 0.825987  [  256/  306]
train() client id: f_00004-0-8 loss: 0.926817  [  288/  306]
train() client id: f_00004-1-0 loss: 1.043520  [   32/  306]
train() client id: f_00004-1-1 loss: 0.902646  [   64/  306]
train() client id: f_00004-1-2 loss: 0.999859  [   96/  306]
train() client id: f_00004-1-3 loss: 1.010151  [  128/  306]
train() client id: f_00004-1-4 loss: 0.959796  [  160/  306]
train() client id: f_00004-1-5 loss: 0.826619  [  192/  306]
train() client id: f_00004-1-6 loss: 0.891620  [  224/  306]
train() client id: f_00004-1-7 loss: 0.861380  [  256/  306]
train() client id: f_00004-1-8 loss: 0.965680  [  288/  306]
train() client id: f_00004-2-0 loss: 0.963094  [   32/  306]
train() client id: f_00004-2-1 loss: 1.074533  [   64/  306]
train() client id: f_00004-2-2 loss: 0.824764  [   96/  306]
train() client id: f_00004-2-3 loss: 0.907272  [  128/  306]
train() client id: f_00004-2-4 loss: 0.894209  [  160/  306]
train() client id: f_00004-2-5 loss: 0.916527  [  192/  306]
train() client id: f_00004-2-6 loss: 1.038689  [  224/  306]
train() client id: f_00004-2-7 loss: 0.914115  [  256/  306]
train() client id: f_00004-2-8 loss: 0.905198  [  288/  306]
train() client id: f_00004-3-0 loss: 0.870205  [   32/  306]
train() client id: f_00004-3-1 loss: 0.802571  [   64/  306]
train() client id: f_00004-3-2 loss: 0.935643  [   96/  306]
train() client id: f_00004-3-3 loss: 0.856986  [  128/  306]
train() client id: f_00004-3-4 loss: 1.046865  [  160/  306]
train() client id: f_00004-3-5 loss: 0.887096  [  192/  306]
train() client id: f_00004-3-6 loss: 0.959727  [  224/  306]
train() client id: f_00004-3-7 loss: 1.060255  [  256/  306]
train() client id: f_00004-3-8 loss: 0.918077  [  288/  306]
train() client id: f_00004-4-0 loss: 0.886810  [   32/  306]
train() client id: f_00004-4-1 loss: 0.939470  [   64/  306]
train() client id: f_00004-4-2 loss: 0.927562  [   96/  306]
train() client id: f_00004-4-3 loss: 0.907825  [  128/  306]
train() client id: f_00004-4-4 loss: 1.004232  [  160/  306]
train() client id: f_00004-4-5 loss: 0.934537  [  192/  306]
train() client id: f_00004-4-6 loss: 0.832032  [  224/  306]
train() client id: f_00004-4-7 loss: 0.868718  [  256/  306]
train() client id: f_00004-4-8 loss: 1.050069  [  288/  306]
train() client id: f_00004-5-0 loss: 0.869091  [   32/  306]
train() client id: f_00004-5-1 loss: 1.001158  [   64/  306]
train() client id: f_00004-5-2 loss: 0.924724  [   96/  306]
train() client id: f_00004-5-3 loss: 0.878514  [  128/  306]
train() client id: f_00004-5-4 loss: 0.855020  [  160/  306]
train() client id: f_00004-5-5 loss: 1.133011  [  192/  306]
train() client id: f_00004-5-6 loss: 0.902172  [  224/  306]
train() client id: f_00004-5-7 loss: 0.735096  [  256/  306]
train() client id: f_00004-5-8 loss: 0.943820  [  288/  306]
train() client id: f_00004-6-0 loss: 0.802364  [   32/  306]
train() client id: f_00004-6-1 loss: 0.968655  [   64/  306]
train() client id: f_00004-6-2 loss: 0.964177  [   96/  306]
train() client id: f_00004-6-3 loss: 0.825670  [  128/  306]
train() client id: f_00004-6-4 loss: 0.948725  [  160/  306]
train() client id: f_00004-6-5 loss: 0.917455  [  192/  306]
train() client id: f_00004-6-6 loss: 0.827377  [  224/  306]
train() client id: f_00004-6-7 loss: 1.091716  [  256/  306]
train() client id: f_00004-6-8 loss: 0.927259  [  288/  306]
train() client id: f_00004-7-0 loss: 0.981930  [   32/  306]
train() client id: f_00004-7-1 loss: 1.075346  [   64/  306]
train() client id: f_00004-7-2 loss: 0.841283  [   96/  306]
train() client id: f_00004-7-3 loss: 0.861680  [  128/  306]
train() client id: f_00004-7-4 loss: 0.910883  [  160/  306]
train() client id: f_00004-7-5 loss: 0.787927  [  192/  306]
train() client id: f_00004-7-6 loss: 1.012021  [  224/  306]
train() client id: f_00004-7-7 loss: 0.849277  [  256/  306]
train() client id: f_00004-7-8 loss: 0.918168  [  288/  306]
train() client id: f_00004-8-0 loss: 0.751080  [   32/  306]
train() client id: f_00004-8-1 loss: 0.917220  [   64/  306]
train() client id: f_00004-8-2 loss: 0.998362  [   96/  306]
train() client id: f_00004-8-3 loss: 0.954965  [  128/  306]
train() client id: f_00004-8-4 loss: 0.968247  [  160/  306]
train() client id: f_00004-8-5 loss: 0.796460  [  192/  306]
train() client id: f_00004-8-6 loss: 0.880596  [  224/  306]
train() client id: f_00004-8-7 loss: 0.922448  [  256/  306]
train() client id: f_00004-8-8 loss: 0.955043  [  288/  306]
train() client id: f_00004-9-0 loss: 1.043100  [   32/  306]
train() client id: f_00004-9-1 loss: 0.977633  [   64/  306]
train() client id: f_00004-9-2 loss: 0.881856  [   96/  306]
train() client id: f_00004-9-3 loss: 0.870864  [  128/  306]
train() client id: f_00004-9-4 loss: 0.889333  [  160/  306]
train() client id: f_00004-9-5 loss: 0.858971  [  192/  306]
train() client id: f_00004-9-6 loss: 0.841637  [  224/  306]
train() client id: f_00004-9-7 loss: 0.937364  [  256/  306]
train() client id: f_00004-9-8 loss: 0.866521  [  288/  306]
train() client id: f_00004-10-0 loss: 0.903883  [   32/  306]
train() client id: f_00004-10-1 loss: 0.957101  [   64/  306]
train() client id: f_00004-10-2 loss: 1.078424  [   96/  306]
train() client id: f_00004-10-3 loss: 0.832947  [  128/  306]
train() client id: f_00004-10-4 loss: 0.821311  [  160/  306]
train() client id: f_00004-10-5 loss: 0.878552  [  192/  306]
train() client id: f_00004-10-6 loss: 1.007981  [  224/  306]
train() client id: f_00004-10-7 loss: 0.811654  [  256/  306]
train() client id: f_00004-10-8 loss: 0.935332  [  288/  306]
train() client id: f_00004-11-0 loss: 0.963217  [   32/  306]
train() client id: f_00004-11-1 loss: 0.910042  [   64/  306]
train() client id: f_00004-11-2 loss: 0.926684  [   96/  306]
train() client id: f_00004-11-3 loss: 0.832166  [  128/  306]
train() client id: f_00004-11-4 loss: 0.831085  [  160/  306]
train() client id: f_00004-11-5 loss: 0.854058  [  192/  306]
train() client id: f_00004-11-6 loss: 0.835528  [  224/  306]
train() client id: f_00004-11-7 loss: 1.066528  [  256/  306]
train() client id: f_00004-11-8 loss: 1.001952  [  288/  306]
train() client id: f_00005-0-0 loss: 0.471082  [   32/  146]
train() client id: f_00005-0-1 loss: 0.531814  [   64/  146]
train() client id: f_00005-0-2 loss: 0.702148  [   96/  146]
train() client id: f_00005-0-3 loss: 0.746516  [  128/  146]
train() client id: f_00005-1-0 loss: 0.602723  [   32/  146]
train() client id: f_00005-1-1 loss: 0.590972  [   64/  146]
train() client id: f_00005-1-2 loss: 0.723967  [   96/  146]
train() client id: f_00005-1-3 loss: 0.708446  [  128/  146]
train() client id: f_00005-2-0 loss: 0.758573  [   32/  146]
train() client id: f_00005-2-1 loss: 0.549131  [   64/  146]
train() client id: f_00005-2-2 loss: 0.635544  [   96/  146]
train() client id: f_00005-2-3 loss: 0.568338  [  128/  146]
train() client id: f_00005-3-0 loss: 0.443504  [   32/  146]
train() client id: f_00005-3-1 loss: 0.790606  [   64/  146]
train() client id: f_00005-3-2 loss: 0.606160  [   96/  146]
train() client id: f_00005-3-3 loss: 0.782854  [  128/  146]
train() client id: f_00005-4-0 loss: 0.682963  [   32/  146]
train() client id: f_00005-4-1 loss: 0.624615  [   64/  146]
train() client id: f_00005-4-2 loss: 0.749586  [   96/  146]
train() client id: f_00005-4-3 loss: 0.490374  [  128/  146]
train() client id: f_00005-5-0 loss: 0.636436  [   32/  146]
train() client id: f_00005-5-1 loss: 0.922202  [   64/  146]
train() client id: f_00005-5-2 loss: 0.644963  [   96/  146]
train() client id: f_00005-5-3 loss: 0.419531  [  128/  146]
train() client id: f_00005-6-0 loss: 0.516037  [   32/  146]
train() client id: f_00005-6-1 loss: 0.708431  [   64/  146]
train() client id: f_00005-6-2 loss: 0.754492  [   96/  146]
train() client id: f_00005-6-3 loss: 0.559215  [  128/  146]
train() client id: f_00005-7-0 loss: 0.421398  [   32/  146]
train() client id: f_00005-7-1 loss: 0.587028  [   64/  146]
train() client id: f_00005-7-2 loss: 0.814861  [   96/  146]
train() client id: f_00005-7-3 loss: 0.556948  [  128/  146]
train() client id: f_00005-8-0 loss: 0.821766  [   32/  146]
train() client id: f_00005-8-1 loss: 0.538677  [   64/  146]
train() client id: f_00005-8-2 loss: 0.513750  [   96/  146]
train() client id: f_00005-8-3 loss: 0.529299  [  128/  146]
train() client id: f_00005-9-0 loss: 0.777551  [   32/  146]
train() client id: f_00005-9-1 loss: 0.459149  [   64/  146]
train() client id: f_00005-9-2 loss: 0.668996  [   96/  146]
train() client id: f_00005-9-3 loss: 0.524888  [  128/  146]
train() client id: f_00005-10-0 loss: 0.548401  [   32/  146]
train() client id: f_00005-10-1 loss: 0.609849  [   64/  146]
train() client id: f_00005-10-2 loss: 0.718691  [   96/  146]
train() client id: f_00005-10-3 loss: 0.537661  [  128/  146]
train() client id: f_00005-11-0 loss: 0.846307  [   32/  146]
train() client id: f_00005-11-1 loss: 0.617063  [   64/  146]
train() client id: f_00005-11-2 loss: 0.490114  [   96/  146]
train() client id: f_00005-11-3 loss: 0.564447  [  128/  146]
train() client id: f_00006-0-0 loss: 0.591826  [   32/   54]
train() client id: f_00006-1-0 loss: 0.609908  [   32/   54]
train() client id: f_00006-2-0 loss: 0.576868  [   32/   54]
train() client id: f_00006-3-0 loss: 0.587579  [   32/   54]
train() client id: f_00006-4-0 loss: 0.608818  [   32/   54]
train() client id: f_00006-5-0 loss: 0.644453  [   32/   54]
train() client id: f_00006-6-0 loss: 0.593844  [   32/   54]
train() client id: f_00006-7-0 loss: 0.599798  [   32/   54]
train() client id: f_00006-8-0 loss: 0.581522  [   32/   54]
train() client id: f_00006-9-0 loss: 0.599445  [   32/   54]
train() client id: f_00006-10-0 loss: 0.647192  [   32/   54]
train() client id: f_00006-11-0 loss: 0.640103  [   32/   54]
train() client id: f_00007-0-0 loss: 0.604534  [   32/  179]
train() client id: f_00007-0-1 loss: 0.757425  [   64/  179]
train() client id: f_00007-0-2 loss: 0.971480  [   96/  179]
train() client id: f_00007-0-3 loss: 0.681447  [  128/  179]
train() client id: f_00007-0-4 loss: 0.731251  [  160/  179]
train() client id: f_00007-1-0 loss: 0.678988  [   32/  179]
train() client id: f_00007-1-1 loss: 0.762389  [   64/  179]
train() client id: f_00007-1-2 loss: 0.786056  [   96/  179]
train() client id: f_00007-1-3 loss: 0.640178  [  128/  179]
train() client id: f_00007-1-4 loss: 0.738412  [  160/  179]
train() client id: f_00007-2-0 loss: 0.773510  [   32/  179]
train() client id: f_00007-2-1 loss: 0.735239  [   64/  179]
train() client id: f_00007-2-2 loss: 0.633882  [   96/  179]
train() client id: f_00007-2-3 loss: 0.895537  [  128/  179]
train() client id: f_00007-2-4 loss: 0.644726  [  160/  179]
train() client id: f_00007-3-0 loss: 0.848135  [   32/  179]
train() client id: f_00007-3-1 loss: 0.733019  [   64/  179]
train() client id: f_00007-3-2 loss: 0.660425  [   96/  179]
train() client id: f_00007-3-3 loss: 0.656948  [  128/  179]
train() client id: f_00007-3-4 loss: 0.699462  [  160/  179]
train() client id: f_00007-4-0 loss: 0.756669  [   32/  179]
train() client id: f_00007-4-1 loss: 0.772683  [   64/  179]
train() client id: f_00007-4-2 loss: 0.732601  [   96/  179]
train() client id: f_00007-4-3 loss: 0.664569  [  128/  179]
train() client id: f_00007-4-4 loss: 0.690038  [  160/  179]
train() client id: f_00007-5-0 loss: 0.622879  [   32/  179]
train() client id: f_00007-5-1 loss: 0.670276  [   64/  179]
train() client id: f_00007-5-2 loss: 0.846263  [   96/  179]
train() client id: f_00007-5-3 loss: 0.798056  [  128/  179]
train() client id: f_00007-5-4 loss: 0.652346  [  160/  179]
train() client id: f_00007-6-0 loss: 0.744432  [   32/  179]
train() client id: f_00007-6-1 loss: 0.867393  [   64/  179]
train() client id: f_00007-6-2 loss: 0.646546  [   96/  179]
train() client id: f_00007-6-3 loss: 0.742770  [  128/  179]
train() client id: f_00007-6-4 loss: 0.564718  [  160/  179]
train() client id: f_00007-7-0 loss: 0.578140  [   32/  179]
train() client id: f_00007-7-1 loss: 0.799992  [   64/  179]
train() client id: f_00007-7-2 loss: 0.772676  [   96/  179]
train() client id: f_00007-7-3 loss: 0.809282  [  128/  179]
train() client id: f_00007-7-4 loss: 0.593954  [  160/  179]
train() client id: f_00007-8-0 loss: 0.813069  [   32/  179]
train() client id: f_00007-8-1 loss: 0.736085  [   64/  179]
train() client id: f_00007-8-2 loss: 0.676245  [   96/  179]
train() client id: f_00007-8-3 loss: 0.736605  [  128/  179]
train() client id: f_00007-8-4 loss: 0.578617  [  160/  179]
train() client id: f_00007-9-0 loss: 0.604157  [   32/  179]
train() client id: f_00007-9-1 loss: 0.771806  [   64/  179]
train() client id: f_00007-9-2 loss: 0.576837  [   96/  179]
train() client id: f_00007-9-3 loss: 0.780537  [  128/  179]
train() client id: f_00007-9-4 loss: 0.827909  [  160/  179]
train() client id: f_00007-10-0 loss: 0.608647  [   32/  179]
train() client id: f_00007-10-1 loss: 0.721982  [   64/  179]
train() client id: f_00007-10-2 loss: 0.661853  [   96/  179]
train() client id: f_00007-10-3 loss: 0.743805  [  128/  179]
train() client id: f_00007-10-4 loss: 0.696687  [  160/  179]
train() client id: f_00007-11-0 loss: 0.682043  [   32/  179]
train() client id: f_00007-11-1 loss: 0.719536  [   64/  179]
train() client id: f_00007-11-2 loss: 0.651447  [   96/  179]
train() client id: f_00007-11-3 loss: 0.665464  [  128/  179]
train() client id: f_00007-11-4 loss: 0.576540  [  160/  179]
train() client id: f_00008-0-0 loss: 0.707818  [   32/  130]
train() client id: f_00008-0-1 loss: 0.779621  [   64/  130]
train() client id: f_00008-0-2 loss: 0.792024  [   96/  130]
train() client id: f_00008-0-3 loss: 0.760161  [  128/  130]
train() client id: f_00008-1-0 loss: 0.825963  [   32/  130]
train() client id: f_00008-1-1 loss: 0.811254  [   64/  130]
train() client id: f_00008-1-2 loss: 0.696546  [   96/  130]
train() client id: f_00008-1-3 loss: 0.698523  [  128/  130]
train() client id: f_00008-2-0 loss: 0.779293  [   32/  130]
train() client id: f_00008-2-1 loss: 0.756434  [   64/  130]
train() client id: f_00008-2-2 loss: 0.679801  [   96/  130]
train() client id: f_00008-2-3 loss: 0.783924  [  128/  130]
train() client id: f_00008-3-0 loss: 0.792275  [   32/  130]
train() client id: f_00008-3-1 loss: 0.852697  [   64/  130]
train() client id: f_00008-3-2 loss: 0.597506  [   96/  130]
train() client id: f_00008-3-3 loss: 0.735519  [  128/  130]
train() client id: f_00008-4-0 loss: 0.746843  [   32/  130]
train() client id: f_00008-4-1 loss: 0.654715  [   64/  130]
train() client id: f_00008-4-2 loss: 0.773976  [   96/  130]
train() client id: f_00008-4-3 loss: 0.810880  [  128/  130]
train() client id: f_00008-5-0 loss: 0.713917  [   32/  130]
train() client id: f_00008-5-1 loss: 0.781270  [   64/  130]
train() client id: f_00008-5-2 loss: 0.695051  [   96/  130]
train() client id: f_00008-5-3 loss: 0.834618  [  128/  130]
train() client id: f_00008-6-0 loss: 0.691183  [   32/  130]
train() client id: f_00008-6-1 loss: 0.691683  [   64/  130]
train() client id: f_00008-6-2 loss: 0.786677  [   96/  130]
train() client id: f_00008-6-3 loss: 0.838271  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708559  [   32/  130]
train() client id: f_00008-7-1 loss: 0.804218  [   64/  130]
train() client id: f_00008-7-2 loss: 0.824592  [   96/  130]
train() client id: f_00008-7-3 loss: 0.681466  [  128/  130]
train() client id: f_00008-8-0 loss: 0.787241  [   32/  130]
train() client id: f_00008-8-1 loss: 0.763873  [   64/  130]
train() client id: f_00008-8-2 loss: 0.799650  [   96/  130]
train() client id: f_00008-8-3 loss: 0.636525  [  128/  130]
train() client id: f_00008-9-0 loss: 0.668417  [   32/  130]
train() client id: f_00008-9-1 loss: 0.747404  [   64/  130]
train() client id: f_00008-9-2 loss: 0.771343  [   96/  130]
train() client id: f_00008-9-3 loss: 0.837925  [  128/  130]
train() client id: f_00008-10-0 loss: 0.741191  [   32/  130]
train() client id: f_00008-10-1 loss: 0.792905  [   64/  130]
train() client id: f_00008-10-2 loss: 0.779742  [   96/  130]
train() client id: f_00008-10-3 loss: 0.711271  [  128/  130]
train() client id: f_00008-11-0 loss: 0.678085  [   32/  130]
train() client id: f_00008-11-1 loss: 0.768969  [   64/  130]
train() client id: f_00008-11-2 loss: 0.748229  [   96/  130]
train() client id: f_00008-11-3 loss: 0.828611  [  128/  130]
train() client id: f_00009-0-0 loss: 1.176502  [   32/  118]
train() client id: f_00009-0-1 loss: 1.155953  [   64/  118]
train() client id: f_00009-0-2 loss: 1.076868  [   96/  118]
train() client id: f_00009-1-0 loss: 1.108648  [   32/  118]
train() client id: f_00009-1-1 loss: 1.137175  [   64/  118]
train() client id: f_00009-1-2 loss: 1.099916  [   96/  118]
train() client id: f_00009-2-0 loss: 1.071241  [   32/  118]
train() client id: f_00009-2-1 loss: 1.064370  [   64/  118]
train() client id: f_00009-2-2 loss: 1.146168  [   96/  118]
train() client id: f_00009-3-0 loss: 1.108080  [   32/  118]
train() client id: f_00009-3-1 loss: 1.014657  [   64/  118]
train() client id: f_00009-3-2 loss: 1.020839  [   96/  118]
train() client id: f_00009-4-0 loss: 1.001326  [   32/  118]
train() client id: f_00009-4-1 loss: 1.004416  [   64/  118]
train() client id: f_00009-4-2 loss: 1.026832  [   96/  118]
train() client id: f_00009-5-0 loss: 1.002926  [   32/  118]
train() client id: f_00009-5-1 loss: 0.954955  [   64/  118]
train() client id: f_00009-5-2 loss: 0.957864  [   96/  118]
train() client id: f_00009-6-0 loss: 0.943104  [   32/  118]
train() client id: f_00009-6-1 loss: 0.952505  [   64/  118]
train() client id: f_00009-6-2 loss: 0.949125  [   96/  118]
train() client id: f_00009-7-0 loss: 0.924952  [   32/  118]
train() client id: f_00009-7-1 loss: 0.947631  [   64/  118]
train() client id: f_00009-7-2 loss: 1.004067  [   96/  118]
train() client id: f_00009-8-0 loss: 1.014238  [   32/  118]
train() client id: f_00009-8-1 loss: 0.870109  [   64/  118]
train() client id: f_00009-8-2 loss: 0.930961  [   96/  118]
train() client id: f_00009-9-0 loss: 0.982826  [   32/  118]
train() client id: f_00009-9-1 loss: 0.940180  [   64/  118]
train() client id: f_00009-9-2 loss: 0.919630  [   96/  118]
train() client id: f_00009-10-0 loss: 0.876094  [   32/  118]
train() client id: f_00009-10-1 loss: 1.004042  [   64/  118]
train() client id: f_00009-10-2 loss: 0.907869  [   96/  118]
train() client id: f_00009-11-0 loss: 0.845604  [   32/  118]
train() client id: f_00009-11-1 loss: 0.988752  [   64/  118]
train() client id: f_00009-11-2 loss: 0.946741  [   96/  118]
At round 14 accuracy: 0.6339522546419099
At round 14 training accuracy: 0.5774647887323944
At round 14 training loss: 0.8522464177826294
update_location
xs = [ -3.9056584    4.20031788  90.00902392  18.81129433   0.97929623
   3.95640986 -52.44319194 -31.32485185  74.66397685 -17.06087855]
ys = [ 82.5879595   65.55583871   1.32061395 -52.45517586  44.35018685
  27.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [129.75370986 119.64618949 134.54875848 114.47886385 109.39788889
 103.8714598  112.94768224 104.79466875 126.02927998 101.52381707]
dists_bs = [194.45631632 209.97804258 316.85061448 298.82883869 219.17592563
 231.70902195 215.80270215 225.7884024  295.15524593 232.77727406]
uav_gains = [5.21358841e-11 6.38604963e-11 4.76097128e-11 7.13141531e-11
 7.98862224e-11 9.09403593e-11 7.37560435e-11 8.89506057e-11
 5.60762293e-11 9.62893323e-11]
bs_gains = [4.31114205e-11 3.47702020e-11 1.09875619e-11 1.29452686e-11
 3.08371410e-11 2.63908782e-11 3.22058545e-11 2.83745851e-11
 1.34014770e-11 2.60531636e-11]
Round 15
-------------------------------
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.01168963 18.78472696  8.87441189  3.17492049 21.66795492 10.44235678
  3.94614583 12.71610262  9.35523145  8.4772718 ]
obj_prev = 106.45081237176021
eta_min = 4.89584141592709e-11	eta_max = 0.9211440323876784
af = 22.501910530386866	bf = 1.761176745733534	zeta = 24.752101583425553	eta = 0.9090909090909091
af = 22.501910530386866	bf = 1.761176745733534	zeta = 42.96421672758091	eta = 0.5237360818902523
af = 22.501910530386866	bf = 1.761176745733534	zeta = 34.25554170777676	eta = 0.6568838035709252
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.69318907763576	eta = 0.6882751779568552
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.6157469047972	eta = 0.6899094046832094
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.61554299983585	eta = 0.6899137178399916
eta = 0.6899137178399916
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.0306296  0.06441941 0.03014342 0.01045296 0.07438618 0.03549145
 0.01312697 0.04351349 0.03160197 0.02868487]
ene_total = [2.80683503 5.36552507 2.7806876  1.27556609 6.12194985 3.25313134
 1.47109077 3.71222193 3.07937974 2.74915557]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 0 obj = 5.7226848172315465
eta = 0.6899137178399916
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
eta_min = 0.6899137178400166	eta_max = 0.6899137178399891
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 0.03956004889123434	eta = 0.9090909090909091
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 19.410648956797782	eta = 0.0018527809600934662
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9911157923761436	eta = 0.018062074012930685
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405838032428373	eta = 0.018532402852232132
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405717992779243	eta = 0.01853251748973932
eta = 0.01853251748973932
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.85887615e-04 1.80012864e-03 1.78662392e-04 7.19578012e-06
 2.80913409e-03 3.10874884e-04 1.42137870e-05 5.10509081e-04
 2.43131519e-04 1.64389475e-04]
ene_total = [0.1719908  0.2249194  0.17497549 0.15774238 0.25358093 0.2017751
 0.15689688 0.1631543  0.23661227 0.19892425]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 1 obj = 5.722684817232005
eta = 0.6899137178400166
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
Done!
At round 15 energy consumption: 5.7226848172315465
At round 15 eta: 0.6899137178400166
At round 15 local rounds: 12.154605075633594
At round 15 global rounds: 74.31613869273092
At round 15 a_n: 22.701869304745298
gradient difference: 0.4524221420288086
train() client id: f_00000-0-0 loss: 1.113176  [   32/  126]
train() client id: f_00000-0-1 loss: 1.142226  [   64/  126]
train() client id: f_00000-0-2 loss: 1.038955  [   96/  126]
train() client id: f_00000-1-0 loss: 1.119972  [   32/  126]
train() client id: f_00000-1-1 loss: 1.141694  [   64/  126]
train() client id: f_00000-1-2 loss: 0.953730  [   96/  126]
train() client id: f_00000-2-0 loss: 1.000185  [   32/  126]
train() client id: f_00000-2-1 loss: 0.929050  [   64/  126]
train() client id: f_00000-2-2 loss: 1.039472  [   96/  126]
train() client id: f_00000-3-0 loss: 0.884346  [   32/  126]
train() client id: f_00000-3-1 loss: 1.001291  [   64/  126]
train() client id: f_00000-3-2 loss: 0.885167  [   96/  126]
train() client id: f_00000-4-0 loss: 0.884919  [   32/  126]
train() client id: f_00000-4-1 loss: 1.050017  [   64/  126]
train() client id: f_00000-4-2 loss: 0.901001  [   96/  126]
train() client id: f_00000-5-0 loss: 0.834124  [   32/  126]
train() client id: f_00000-5-1 loss: 0.907363  [   64/  126]
train() client id: f_00000-5-2 loss: 0.900764  [   96/  126]
train() client id: f_00000-6-0 loss: 0.872495  [   32/  126]
train() client id: f_00000-6-1 loss: 0.900010  [   64/  126]
train() client id: f_00000-6-2 loss: 0.859621  [   96/  126]
train() client id: f_00000-7-0 loss: 0.861852  [   32/  126]
train() client id: f_00000-7-1 loss: 0.871035  [   64/  126]
train() client id: f_00000-7-2 loss: 0.853283  [   96/  126]
train() client id: f_00000-8-0 loss: 0.802970  [   32/  126]
train() client id: f_00000-8-1 loss: 0.829639  [   64/  126]
train() client id: f_00000-8-2 loss: 0.817466  [   96/  126]
train() client id: f_00000-9-0 loss: 0.890236  [   32/  126]
train() client id: f_00000-9-1 loss: 0.845387  [   64/  126]
train() client id: f_00000-9-2 loss: 0.807370  [   96/  126]
train() client id: f_00000-10-0 loss: 0.824349  [   32/  126]
train() client id: f_00000-10-1 loss: 0.853465  [   64/  126]
train() client id: f_00000-10-2 loss: 0.847085  [   96/  126]
train() client id: f_00000-11-0 loss: 0.807240  [   32/  126]
train() client id: f_00000-11-1 loss: 0.949515  [   64/  126]
train() client id: f_00000-11-2 loss: 0.824447  [   96/  126]
train() client id: f_00001-0-0 loss: 0.505701  [   32/  265]
train() client id: f_00001-0-1 loss: 0.470022  [   64/  265]
train() client id: f_00001-0-2 loss: 0.466875  [   96/  265]
train() client id: f_00001-0-3 loss: 0.389679  [  128/  265]
train() client id: f_00001-0-4 loss: 0.496271  [  160/  265]
train() client id: f_00001-0-5 loss: 0.432824  [  192/  265]
train() client id: f_00001-0-6 loss: 0.410878  [  224/  265]
train() client id: f_00001-0-7 loss: 0.327350  [  256/  265]
train() client id: f_00001-1-0 loss: 0.494558  [   32/  265]
train() client id: f_00001-1-1 loss: 0.419843  [   64/  265]
train() client id: f_00001-1-2 loss: 0.404497  [   96/  265]
train() client id: f_00001-1-3 loss: 0.448283  [  128/  265]
train() client id: f_00001-1-4 loss: 0.384981  [  160/  265]
train() client id: f_00001-1-5 loss: 0.388661  [  192/  265]
train() client id: f_00001-1-6 loss: 0.476407  [  224/  265]
train() client id: f_00001-1-7 loss: 0.402669  [  256/  265]
train() client id: f_00001-2-0 loss: 0.395029  [   32/  265]
train() client id: f_00001-2-1 loss: 0.510297  [   64/  265]
train() client id: f_00001-2-2 loss: 0.356946  [   96/  265]
train() client id: f_00001-2-3 loss: 0.424759  [  128/  265]
train() client id: f_00001-2-4 loss: 0.467857  [  160/  265]
train() client id: f_00001-2-5 loss: 0.344474  [  192/  265]
train() client id: f_00001-2-6 loss: 0.421873  [  224/  265]
train() client id: f_00001-2-7 loss: 0.404074  [  256/  265]
train() client id: f_00001-3-0 loss: 0.331861  [   32/  265]
train() client id: f_00001-3-1 loss: 0.365144  [   64/  265]
train() client id: f_00001-3-2 loss: 0.444092  [   96/  265]
train() client id: f_00001-3-3 loss: 0.399030  [  128/  265]
train() client id: f_00001-3-4 loss: 0.361666  [  160/  265]
train() client id: f_00001-3-5 loss: 0.376140  [  192/  265]
train() client id: f_00001-3-6 loss: 0.435943  [  224/  265]
train() client id: f_00001-3-7 loss: 0.553114  [  256/  265]
train() client id: f_00001-4-0 loss: 0.396876  [   32/  265]
train() client id: f_00001-4-1 loss: 0.331369  [   64/  265]
train() client id: f_00001-4-2 loss: 0.379055  [   96/  265]
train() client id: f_00001-4-3 loss: 0.433273  [  128/  265]
train() client id: f_00001-4-4 loss: 0.327627  [  160/  265]
train() client id: f_00001-4-5 loss: 0.362779  [  192/  265]
train() client id: f_00001-4-6 loss: 0.469969  [  224/  265]
train() client id: f_00001-4-7 loss: 0.436159  [  256/  265]
train() client id: f_00001-5-0 loss: 0.445075  [   32/  265]
train() client id: f_00001-5-1 loss: 0.349339  [   64/  265]
train() client id: f_00001-5-2 loss: 0.396821  [   96/  265]
train() client id: f_00001-5-3 loss: 0.314809  [  128/  265]
train() client id: f_00001-5-4 loss: 0.548546  [  160/  265]
train() client id: f_00001-5-5 loss: 0.366348  [  192/  265]
train() client id: f_00001-5-6 loss: 0.298247  [  224/  265]
train() client id: f_00001-5-7 loss: 0.389757  [  256/  265]
train() client id: f_00001-6-0 loss: 0.463906  [   32/  265]
train() client id: f_00001-6-1 loss: 0.349223  [   64/  265]
train() client id: f_00001-6-2 loss: 0.373038  [   96/  265]
train() client id: f_00001-6-3 loss: 0.293779  [  128/  265]
train() client id: f_00001-6-4 loss: 0.447821  [  160/  265]
train() client id: f_00001-6-5 loss: 0.435370  [  192/  265]
train() client id: f_00001-6-6 loss: 0.359611  [  224/  265]
train() client id: f_00001-6-7 loss: 0.422277  [  256/  265]
train() client id: f_00001-7-0 loss: 0.383992  [   32/  265]
train() client id: f_00001-7-1 loss: 0.417574  [   64/  265]
train() client id: f_00001-7-2 loss: 0.357824  [   96/  265]
train() client id: f_00001-7-3 loss: 0.319701  [  128/  265]
train() client id: f_00001-7-4 loss: 0.363287  [  160/  265]
train() client id: f_00001-7-5 loss: 0.393050  [  192/  265]
train() client id: f_00001-7-6 loss: 0.385688  [  224/  265]
train() client id: f_00001-7-7 loss: 0.437621  [  256/  265]
train() client id: f_00001-8-0 loss: 0.415396  [   32/  265]
train() client id: f_00001-8-1 loss: 0.381915  [   64/  265]
train() client id: f_00001-8-2 loss: 0.378872  [   96/  265]
train() client id: f_00001-8-3 loss: 0.419309  [  128/  265]
train() client id: f_00001-8-4 loss: 0.310812  [  160/  265]
train() client id: f_00001-8-5 loss: 0.445693  [  192/  265]
train() client id: f_00001-8-6 loss: 0.346543  [  224/  265]
train() client id: f_00001-8-7 loss: 0.401532  [  256/  265]
train() client id: f_00001-9-0 loss: 0.365965  [   32/  265]
train() client id: f_00001-9-1 loss: 0.352021  [   64/  265]
train() client id: f_00001-9-2 loss: 0.373418  [   96/  265]
train() client id: f_00001-9-3 loss: 0.378821  [  128/  265]
train() client id: f_00001-9-4 loss: 0.408369  [  160/  265]
train() client id: f_00001-9-5 loss: 0.360172  [  192/  265]
train() client id: f_00001-9-6 loss: 0.427717  [  224/  265]
train() client id: f_00001-9-7 loss: 0.410771  [  256/  265]
train() client id: f_00001-10-0 loss: 0.407640  [   32/  265]
train() client id: f_00001-10-1 loss: 0.290302  [   64/  265]
train() client id: f_00001-10-2 loss: 0.384785  [   96/  265]
train() client id: f_00001-10-3 loss: 0.351722  [  128/  265]
train() client id: f_00001-10-4 loss: 0.350094  [  160/  265]
train() client id: f_00001-10-5 loss: 0.430553  [  192/  265]
train() client id: f_00001-10-6 loss: 0.426303  [  224/  265]
train() client id: f_00001-10-7 loss: 0.433367  [  256/  265]
train() client id: f_00001-11-0 loss: 0.444071  [   32/  265]
train() client id: f_00001-11-1 loss: 0.383817  [   64/  265]
train() client id: f_00001-11-2 loss: 0.476758  [   96/  265]
train() client id: f_00001-11-3 loss: 0.294452  [  128/  265]
train() client id: f_00001-11-4 loss: 0.375223  [  160/  265]
train() client id: f_00001-11-5 loss: 0.356820  [  192/  265]
train() client id: f_00001-11-6 loss: 0.324450  [  224/  265]
train() client id: f_00001-11-7 loss: 0.396641  [  256/  265]
train() client id: f_00002-0-0 loss: 1.245555  [   32/  124]
train() client id: f_00002-0-1 loss: 1.275471  [   64/  124]
train() client id: f_00002-0-2 loss: 1.146872  [   96/  124]
train() client id: f_00002-1-0 loss: 1.254092  [   32/  124]
train() client id: f_00002-1-1 loss: 1.164415  [   64/  124]
train() client id: f_00002-1-2 loss: 1.050460  [   96/  124]
train() client id: f_00002-2-0 loss: 1.174436  [   32/  124]
train() client id: f_00002-2-1 loss: 1.024473  [   64/  124]
train() client id: f_00002-2-2 loss: 1.066805  [   96/  124]
train() client id: f_00002-3-0 loss: 1.163412  [   32/  124]
train() client id: f_00002-3-1 loss: 1.097067  [   64/  124]
train() client id: f_00002-3-2 loss: 1.095646  [   96/  124]
train() client id: f_00002-4-0 loss: 0.997892  [   32/  124]
train() client id: f_00002-4-1 loss: 1.073100  [   64/  124]
train() client id: f_00002-4-2 loss: 1.079345  [   96/  124]
train() client id: f_00002-5-0 loss: 0.941960  [   32/  124]
train() client id: f_00002-5-1 loss: 1.042484  [   64/  124]
train() client id: f_00002-5-2 loss: 1.035138  [   96/  124]
train() client id: f_00002-6-0 loss: 1.007583  [   32/  124]
train() client id: f_00002-6-1 loss: 1.055200  [   64/  124]
train() client id: f_00002-6-2 loss: 1.000436  [   96/  124]
train() client id: f_00002-7-0 loss: 0.970512  [   32/  124]
train() client id: f_00002-7-1 loss: 1.122565  [   64/  124]
train() client id: f_00002-7-2 loss: 0.954695  [   96/  124]
train() client id: f_00002-8-0 loss: 0.981096  [   32/  124]
train() client id: f_00002-8-1 loss: 1.008666  [   64/  124]
train() client id: f_00002-8-2 loss: 0.921620  [   96/  124]
train() client id: f_00002-9-0 loss: 0.857406  [   32/  124]
train() client id: f_00002-9-1 loss: 1.019014  [   64/  124]
train() client id: f_00002-9-2 loss: 1.075894  [   96/  124]
train() client id: f_00002-10-0 loss: 0.938640  [   32/  124]
train() client id: f_00002-10-1 loss: 0.934955  [   64/  124]
train() client id: f_00002-10-2 loss: 1.046436  [   96/  124]
train() client id: f_00002-11-0 loss: 0.991550  [   32/  124]
train() client id: f_00002-11-1 loss: 0.952939  [   64/  124]
train() client id: f_00002-11-2 loss: 0.939651  [   96/  124]
train() client id: f_00003-0-0 loss: 0.690441  [   32/   43]
train() client id: f_00003-1-0 loss: 0.816924  [   32/   43]
train() client id: f_00003-2-0 loss: 0.745988  [   32/   43]
train() client id: f_00003-3-0 loss: 0.910462  [   32/   43]
train() client id: f_00003-4-0 loss: 0.722648  [   32/   43]
train() client id: f_00003-5-0 loss: 0.839527  [   32/   43]
train() client id: f_00003-6-0 loss: 0.752852  [   32/   43]
train() client id: f_00003-7-0 loss: 0.718001  [   32/   43]
train() client id: f_00003-8-0 loss: 0.850827  [   32/   43]
train() client id: f_00003-9-0 loss: 0.753359  [   32/   43]
train() client id: f_00003-10-0 loss: 0.892566  [   32/   43]
train() client id: f_00003-11-0 loss: 0.679120  [   32/   43]
train() client id: f_00004-0-0 loss: 0.717303  [   32/  306]
train() client id: f_00004-0-1 loss: 0.670183  [   64/  306]
train() client id: f_00004-0-2 loss: 0.898265  [   96/  306]
train() client id: f_00004-0-3 loss: 0.867013  [  128/  306]
train() client id: f_00004-0-4 loss: 0.770384  [  160/  306]
train() client id: f_00004-0-5 loss: 0.756335  [  192/  306]
train() client id: f_00004-0-6 loss: 0.903862  [  224/  306]
train() client id: f_00004-0-7 loss: 0.727522  [  256/  306]
train() client id: f_00004-0-8 loss: 0.835601  [  288/  306]
train() client id: f_00004-1-0 loss: 0.818065  [   32/  306]
train() client id: f_00004-1-1 loss: 0.713602  [   64/  306]
train() client id: f_00004-1-2 loss: 0.780836  [   96/  306]
train() client id: f_00004-1-3 loss: 0.797035  [  128/  306]
train() client id: f_00004-1-4 loss: 0.651656  [  160/  306]
train() client id: f_00004-1-5 loss: 0.963659  [  192/  306]
train() client id: f_00004-1-6 loss: 0.843893  [  224/  306]
train() client id: f_00004-1-7 loss: 0.746883  [  256/  306]
train() client id: f_00004-1-8 loss: 0.838696  [  288/  306]
train() client id: f_00004-2-0 loss: 0.683711  [   32/  306]
train() client id: f_00004-2-1 loss: 0.837724  [   64/  306]
train() client id: f_00004-2-2 loss: 0.789844  [   96/  306]
train() client id: f_00004-2-3 loss: 0.898063  [  128/  306]
train() client id: f_00004-2-4 loss: 0.767894  [  160/  306]
train() client id: f_00004-2-5 loss: 0.860192  [  192/  306]
train() client id: f_00004-2-6 loss: 0.693449  [  224/  306]
train() client id: f_00004-2-7 loss: 0.827070  [  256/  306]
train() client id: f_00004-2-8 loss: 0.799426  [  288/  306]
train() client id: f_00004-3-0 loss: 0.788351  [   32/  306]
train() client id: f_00004-3-1 loss: 0.819490  [   64/  306]
train() client id: f_00004-3-2 loss: 0.849494  [   96/  306]
train() client id: f_00004-3-3 loss: 0.804683  [  128/  306]
train() client id: f_00004-3-4 loss: 0.808967  [  160/  306]
train() client id: f_00004-3-5 loss: 0.830391  [  192/  306]
train() client id: f_00004-3-6 loss: 0.753914  [  224/  306]
train() client id: f_00004-3-7 loss: 0.695181  [  256/  306]
train() client id: f_00004-3-8 loss: 0.701241  [  288/  306]
train() client id: f_00004-4-0 loss: 0.885382  [   32/  306]
train() client id: f_00004-4-1 loss: 0.801174  [   64/  306]
train() client id: f_00004-4-2 loss: 0.710517  [   96/  306]
train() client id: f_00004-4-3 loss: 0.869361  [  128/  306]
train() client id: f_00004-4-4 loss: 0.830246  [  160/  306]
train() client id: f_00004-4-5 loss: 0.714232  [  192/  306]
train() client id: f_00004-4-6 loss: 0.801385  [  224/  306]
train() client id: f_00004-4-7 loss: 0.789354  [  256/  306]
train() client id: f_00004-4-8 loss: 0.709716  [  288/  306]
train() client id: f_00004-5-0 loss: 0.659283  [   32/  306]
train() client id: f_00004-5-1 loss: 0.654391  [   64/  306]
train() client id: f_00004-5-2 loss: 0.798931  [   96/  306]
train() client id: f_00004-5-3 loss: 0.809267  [  128/  306]
train() client id: f_00004-5-4 loss: 0.782791  [  160/  306]
train() client id: f_00004-5-5 loss: 0.864887  [  192/  306]
train() client id: f_00004-5-6 loss: 0.790301  [  224/  306]
train() client id: f_00004-5-7 loss: 0.793044  [  256/  306]
train() client id: f_00004-5-8 loss: 0.751109  [  288/  306]
train() client id: f_00004-6-0 loss: 0.675717  [   32/  306]
train() client id: f_00004-6-1 loss: 0.880286  [   64/  306]
train() client id: f_00004-6-2 loss: 0.856134  [   96/  306]
train() client id: f_00004-6-3 loss: 0.640228  [  128/  306]
train() client id: f_00004-6-4 loss: 0.804104  [  160/  306]
train() client id: f_00004-6-5 loss: 0.813887  [  192/  306]
train() client id: f_00004-6-6 loss: 0.937576  [  224/  306]
train() client id: f_00004-6-7 loss: 0.800578  [  256/  306]
train() client id: f_00004-6-8 loss: 0.715487  [  288/  306]
train() client id: f_00004-7-0 loss: 0.845662  [   32/  306]
train() client id: f_00004-7-1 loss: 0.723334  [   64/  306]
train() client id: f_00004-7-2 loss: 0.777768  [   96/  306]
train() client id: f_00004-7-3 loss: 0.899687  [  128/  306]
train() client id: f_00004-7-4 loss: 0.771203  [  160/  306]
train() client id: f_00004-7-5 loss: 0.723545  [  192/  306]
train() client id: f_00004-7-6 loss: 0.733613  [  224/  306]
train() client id: f_00004-7-7 loss: 0.875166  [  256/  306]
train() client id: f_00004-7-8 loss: 0.688930  [  288/  306]
train() client id: f_00004-8-0 loss: 0.804644  [   32/  306]
train() client id: f_00004-8-1 loss: 0.808790  [   64/  306]
train() client id: f_00004-8-2 loss: 0.769195  [   96/  306]
train() client id: f_00004-8-3 loss: 0.696965  [  128/  306]
train() client id: f_00004-8-4 loss: 0.885679  [  160/  306]
train() client id: f_00004-8-5 loss: 0.836248  [  192/  306]
train() client id: f_00004-8-6 loss: 0.871228  [  224/  306]
train() client id: f_00004-8-7 loss: 0.743609  [  256/  306]
train() client id: f_00004-8-8 loss: 0.701289  [  288/  306]
train() client id: f_00004-9-0 loss: 0.786850  [   32/  306]
train() client id: f_00004-9-1 loss: 0.743179  [   64/  306]
train() client id: f_00004-9-2 loss: 0.817930  [   96/  306]
train() client id: f_00004-9-3 loss: 0.774562  [  128/  306]
train() client id: f_00004-9-4 loss: 0.761788  [  160/  306]
train() client id: f_00004-9-5 loss: 0.849769  [  192/  306]
train() client id: f_00004-9-6 loss: 0.705624  [  224/  306]
train() client id: f_00004-9-7 loss: 0.907647  [  256/  306]
train() client id: f_00004-9-8 loss: 0.793902  [  288/  306]
train() client id: f_00004-10-0 loss: 0.770729  [   32/  306]
train() client id: f_00004-10-1 loss: 0.788667  [   64/  306]
train() client id: f_00004-10-2 loss: 0.757739  [   96/  306]
train() client id: f_00004-10-3 loss: 0.783103  [  128/  306]
train() client id: f_00004-10-4 loss: 0.849103  [  160/  306]
train() client id: f_00004-10-5 loss: 0.798615  [  192/  306]
train() client id: f_00004-10-6 loss: 0.878845  [  224/  306]
train() client id: f_00004-10-7 loss: 0.696022  [  256/  306]
train() client id: f_00004-10-8 loss: 0.784033  [  288/  306]
train() client id: f_00004-11-0 loss: 0.886447  [   32/  306]
train() client id: f_00004-11-1 loss: 0.775067  [   64/  306]
train() client id: f_00004-11-2 loss: 0.782938  [   96/  306]
train() client id: f_00004-11-3 loss: 0.962743  [  128/  306]
train() client id: f_00004-11-4 loss: 0.759758  [  160/  306]
train() client id: f_00004-11-5 loss: 0.715497  [  192/  306]
train() client id: f_00004-11-6 loss: 0.803135  [  224/  306]
train() client id: f_00004-11-7 loss: 0.808601  [  256/  306]
train() client id: f_00004-11-8 loss: 0.690448  [  288/  306]
train() client id: f_00005-0-0 loss: 0.868411  [   32/  146]
train() client id: f_00005-0-1 loss: 0.646232  [   64/  146]
train() client id: f_00005-0-2 loss: 0.664055  [   96/  146]
train() client id: f_00005-0-3 loss: 0.786975  [  128/  146]
train() client id: f_00005-1-0 loss: 0.752809  [   32/  146]
train() client id: f_00005-1-1 loss: 0.656480  [   64/  146]
train() client id: f_00005-1-2 loss: 0.653414  [   96/  146]
train() client id: f_00005-1-3 loss: 0.750032  [  128/  146]
train() client id: f_00005-2-0 loss: 0.697425  [   32/  146]
train() client id: f_00005-2-1 loss: 0.473608  [   64/  146]
train() client id: f_00005-2-2 loss: 0.854569  [   96/  146]
train() client id: f_00005-2-3 loss: 0.834587  [  128/  146]
train() client id: f_00005-3-0 loss: 0.741833  [   32/  146]
train() client id: f_00005-3-1 loss: 0.662266  [   64/  146]
train() client id: f_00005-3-2 loss: 0.834947  [   96/  146]
train() client id: f_00005-3-3 loss: 0.617322  [  128/  146]
train() client id: f_00005-4-0 loss: 0.691936  [   32/  146]
train() client id: f_00005-4-1 loss: 0.913423  [   64/  146]
train() client id: f_00005-4-2 loss: 0.692701  [   96/  146]
train() client id: f_00005-4-3 loss: 0.614123  [  128/  146]
train() client id: f_00005-5-0 loss: 0.758241  [   32/  146]
train() client id: f_00005-5-1 loss: 0.753835  [   64/  146]
train() client id: f_00005-5-2 loss: 0.822200  [   96/  146]
train() client id: f_00005-5-3 loss: 0.589138  [  128/  146]
train() client id: f_00005-6-0 loss: 0.706608  [   32/  146]
train() client id: f_00005-6-1 loss: 0.567228  [   64/  146]
train() client id: f_00005-6-2 loss: 0.737504  [   96/  146]
train() client id: f_00005-6-3 loss: 0.801583  [  128/  146]
train() client id: f_00005-7-0 loss: 0.719012  [   32/  146]
train() client id: f_00005-7-1 loss: 0.558982  [   64/  146]
train() client id: f_00005-7-2 loss: 0.653074  [   96/  146]
train() client id: f_00005-7-3 loss: 1.042628  [  128/  146]
train() client id: f_00005-8-0 loss: 0.829388  [   32/  146]
train() client id: f_00005-8-1 loss: 0.626846  [   64/  146]
train() client id: f_00005-8-2 loss: 0.707687  [   96/  146]
train() client id: f_00005-8-3 loss: 0.609632  [  128/  146]
train() client id: f_00005-9-0 loss: 0.821182  [   32/  146]
train() client id: f_00005-9-1 loss: 0.614868  [   64/  146]
train() client id: f_00005-9-2 loss: 0.717707  [   96/  146]
train() client id: f_00005-9-3 loss: 0.676568  [  128/  146]
train() client id: f_00005-10-0 loss: 0.847795  [   32/  146]
train() client id: f_00005-10-1 loss: 0.561191  [   64/  146]
train() client id: f_00005-10-2 loss: 0.774374  [   96/  146]
train() client id: f_00005-10-3 loss: 0.741073  [  128/  146]
train() client id: f_00005-11-0 loss: 0.762674  [   32/  146]
train() client id: f_00005-11-1 loss: 0.733728  [   64/  146]
train() client id: f_00005-11-2 loss: 0.625139  [   96/  146]
train() client id: f_00005-11-3 loss: 0.540534  [  128/  146]
train() client id: f_00006-0-0 loss: 0.599935  [   32/   54]
train() client id: f_00006-1-0 loss: 0.560215  [   32/   54]
train() client id: f_00006-2-0 loss: 0.548632  [   32/   54]
train() client id: f_00006-3-0 loss: 0.602336  [   32/   54]
train() client id: f_00006-4-0 loss: 0.550591  [   32/   54]
train() client id: f_00006-5-0 loss: 0.613499  [   32/   54]
train() client id: f_00006-6-0 loss: 0.545534  [   32/   54]
train() client id: f_00006-7-0 loss: 0.568900  [   32/   54]
train() client id: f_00006-8-0 loss: 0.504655  [   32/   54]
train() client id: f_00006-9-0 loss: 0.543619  [   32/   54]
train() client id: f_00006-10-0 loss: 0.571074  [   32/   54]
train() client id: f_00006-11-0 loss: 0.613694  [   32/   54]
train() client id: f_00007-0-0 loss: 0.942989  [   32/  179]
train() client id: f_00007-0-1 loss: 0.805730  [   64/  179]
train() client id: f_00007-0-2 loss: 0.686973  [   96/  179]
train() client id: f_00007-0-3 loss: 0.671563  [  128/  179]
train() client id: f_00007-0-4 loss: 0.713397  [  160/  179]
train() client id: f_00007-1-0 loss: 0.778072  [   32/  179]
train() client id: f_00007-1-1 loss: 0.743580  [   64/  179]
train() client id: f_00007-1-2 loss: 0.815405  [   96/  179]
train() client id: f_00007-1-3 loss: 0.647005  [  128/  179]
train() client id: f_00007-1-4 loss: 0.696430  [  160/  179]
train() client id: f_00007-2-0 loss: 0.767672  [   32/  179]
train() client id: f_00007-2-1 loss: 0.693731  [   64/  179]
train() client id: f_00007-2-2 loss: 0.748962  [   96/  179]
train() client id: f_00007-2-3 loss: 0.712133  [  128/  179]
train() client id: f_00007-2-4 loss: 0.612477  [  160/  179]
train() client id: f_00007-3-0 loss: 0.615041  [   32/  179]
train() client id: f_00007-3-1 loss: 0.843839  [   64/  179]
train() client id: f_00007-3-2 loss: 0.653274  [   96/  179]
train() client id: f_00007-3-3 loss: 0.654937  [  128/  179]
train() client id: f_00007-3-4 loss: 0.772259  [  160/  179]
train() client id: f_00007-4-0 loss: 0.654410  [   32/  179]
train() client id: f_00007-4-1 loss: 0.594984  [   64/  179]
train() client id: f_00007-4-2 loss: 0.750924  [   96/  179]
train() client id: f_00007-4-3 loss: 0.815120  [  128/  179]
train() client id: f_00007-4-4 loss: 0.737764  [  160/  179]
train() client id: f_00007-5-0 loss: 0.685244  [   32/  179]
train() client id: f_00007-5-1 loss: 0.666580  [   64/  179]
train() client id: f_00007-5-2 loss: 0.610905  [   96/  179]
train() client id: f_00007-5-3 loss: 0.715393  [  128/  179]
train() client id: f_00007-5-4 loss: 0.773361  [  160/  179]
train() client id: f_00007-6-0 loss: 0.668011  [   32/  179]
train() client id: f_00007-6-1 loss: 0.684262  [   64/  179]
train() client id: f_00007-6-2 loss: 0.580539  [   96/  179]
train() client id: f_00007-6-3 loss: 0.736777  [  128/  179]
train() client id: f_00007-6-4 loss: 0.854737  [  160/  179]
train() client id: f_00007-7-0 loss: 0.710865  [   32/  179]
train() client id: f_00007-7-1 loss: 0.590217  [   64/  179]
train() client id: f_00007-7-2 loss: 0.746337  [   96/  179]
train() client id: f_00007-7-3 loss: 0.733046  [  128/  179]
train() client id: f_00007-7-4 loss: 0.666611  [  160/  179]
train() client id: f_00007-8-0 loss: 0.615926  [   32/  179]
train() client id: f_00007-8-1 loss: 0.681557  [   64/  179]
train() client id: f_00007-8-2 loss: 0.862306  [   96/  179]
train() client id: f_00007-8-3 loss: 0.706438  [  128/  179]
train() client id: f_00007-8-4 loss: 0.648252  [  160/  179]
train() client id: f_00007-9-0 loss: 0.605619  [   32/  179]
train() client id: f_00007-9-1 loss: 0.747419  [   64/  179]
train() client id: f_00007-9-2 loss: 0.656386  [   96/  179]
train() client id: f_00007-9-3 loss: 0.726203  [  128/  179]
train() client id: f_00007-9-4 loss: 0.879383  [  160/  179]
train() client id: f_00007-10-0 loss: 0.653508  [   32/  179]
train() client id: f_00007-10-1 loss: 0.604829  [   64/  179]
train() client id: f_00007-10-2 loss: 0.869850  [   96/  179]
train() client id: f_00007-10-3 loss: 0.664829  [  128/  179]
train() client id: f_00007-10-4 loss: 0.798781  [  160/  179]
train() client id: f_00007-11-0 loss: 0.768347  [   32/  179]
train() client id: f_00007-11-1 loss: 0.745901  [   64/  179]
train() client id: f_00007-11-2 loss: 0.666081  [   96/  179]
train() client id: f_00007-11-3 loss: 0.692427  [  128/  179]
train() client id: f_00007-11-4 loss: 0.727192  [  160/  179]
train() client id: f_00008-0-0 loss: 0.700438  [   32/  130]
train() client id: f_00008-0-1 loss: 0.560038  [   64/  130]
train() client id: f_00008-0-2 loss: 0.744094  [   96/  130]
train() client id: f_00008-0-3 loss: 0.783007  [  128/  130]
train() client id: f_00008-1-0 loss: 0.618488  [   32/  130]
train() client id: f_00008-1-1 loss: 0.716344  [   64/  130]
train() client id: f_00008-1-2 loss: 0.692004  [   96/  130]
train() client id: f_00008-1-3 loss: 0.680537  [  128/  130]
train() client id: f_00008-2-0 loss: 0.648590  [   32/  130]
train() client id: f_00008-2-1 loss: 0.661450  [   64/  130]
train() client id: f_00008-2-2 loss: 0.762255  [   96/  130]
train() client id: f_00008-2-3 loss: 0.704435  [  128/  130]
train() client id: f_00008-3-0 loss: 0.738023  [   32/  130]
train() client id: f_00008-3-1 loss: 0.694324  [   64/  130]
train() client id: f_00008-3-2 loss: 0.620450  [   96/  130]
train() client id: f_00008-3-3 loss: 0.729798  [  128/  130]
train() client id: f_00008-4-0 loss: 0.617044  [   32/  130]
train() client id: f_00008-4-1 loss: 0.743159  [   64/  130]
train() client id: f_00008-4-2 loss: 0.698565  [   96/  130]
train() client id: f_00008-4-3 loss: 0.714191  [  128/  130]
train() client id: f_00008-5-0 loss: 0.745727  [   32/  130]
train() client id: f_00008-5-1 loss: 0.727819  [   64/  130]
train() client id: f_00008-5-2 loss: 0.608749  [   96/  130]
train() client id: f_00008-5-3 loss: 0.693743  [  128/  130]
train() client id: f_00008-6-0 loss: 0.680977  [   32/  130]
train() client id: f_00008-6-1 loss: 0.674198  [   64/  130]
train() client id: f_00008-6-2 loss: 0.725838  [   96/  130]
train() client id: f_00008-6-3 loss: 0.689363  [  128/  130]
train() client id: f_00008-7-0 loss: 0.741366  [   32/  130]
train() client id: f_00008-7-1 loss: 0.783762  [   64/  130]
train() client id: f_00008-7-2 loss: 0.577026  [   96/  130]
train() client id: f_00008-7-3 loss: 0.633372  [  128/  130]
train() client id: f_00008-8-0 loss: 0.685313  [   32/  130]
train() client id: f_00008-8-1 loss: 0.711537  [   64/  130]
train() client id: f_00008-8-2 loss: 0.631989  [   96/  130]
train() client id: f_00008-8-3 loss: 0.665742  [  128/  130]
train() client id: f_00008-9-0 loss: 0.777856  [   32/  130]
train() client id: f_00008-9-1 loss: 0.790340  [   64/  130]
train() client id: f_00008-9-2 loss: 0.577149  [   96/  130]
train() client id: f_00008-9-3 loss: 0.631122  [  128/  130]
train() client id: f_00008-10-0 loss: 0.780548  [   32/  130]
train() client id: f_00008-10-1 loss: 0.609155  [   64/  130]
train() client id: f_00008-10-2 loss: 0.738930  [   96/  130]
train() client id: f_00008-10-3 loss: 0.653567  [  128/  130]
train() client id: f_00008-11-0 loss: 0.658587  [   32/  130]
train() client id: f_00008-11-1 loss: 0.635098  [   64/  130]
train() client id: f_00008-11-2 loss: 0.744595  [   96/  130]
train() client id: f_00008-11-3 loss: 0.733896  [  128/  130]
train() client id: f_00009-0-0 loss: 1.096691  [   32/  118]
train() client id: f_00009-0-1 loss: 1.163836  [   64/  118]
train() client id: f_00009-0-2 loss: 1.102889  [   96/  118]
train() client id: f_00009-1-0 loss: 0.999483  [   32/  118]
train() client id: f_00009-1-1 loss: 1.063227  [   64/  118]
train() client id: f_00009-1-2 loss: 1.072956  [   96/  118]
train() client id: f_00009-2-0 loss: 1.070417  [   32/  118]
train() client id: f_00009-2-1 loss: 1.000204  [   64/  118]
train() client id: f_00009-2-2 loss: 0.950662  [   96/  118]
train() client id: f_00009-3-0 loss: 0.946885  [   32/  118]
train() client id: f_00009-3-1 loss: 0.996092  [   64/  118]
train() client id: f_00009-3-2 loss: 1.010927  [   96/  118]
train() client id: f_00009-4-0 loss: 0.914402  [   32/  118]
train() client id: f_00009-4-1 loss: 1.038947  [   64/  118]
train() client id: f_00009-4-2 loss: 0.943732  [   96/  118]
train() client id: f_00009-5-0 loss: 0.915860  [   32/  118]
train() client id: f_00009-5-1 loss: 0.883015  [   64/  118]
train() client id: f_00009-5-2 loss: 0.978279  [   96/  118]
train() client id: f_00009-6-0 loss: 0.924942  [   32/  118]
train() client id: f_00009-6-1 loss: 0.888848  [   64/  118]
train() client id: f_00009-6-2 loss: 0.968313  [   96/  118]
train() client id: f_00009-7-0 loss: 0.876815  [   32/  118]
train() client id: f_00009-7-1 loss: 0.885394  [   64/  118]
train() client id: f_00009-7-2 loss: 0.939397  [   96/  118]
train() client id: f_00009-8-0 loss: 0.851821  [   32/  118]
train() client id: f_00009-8-1 loss: 0.859899  [   64/  118]
train() client id: f_00009-8-2 loss: 0.913239  [   96/  118]
train() client id: f_00009-9-0 loss: 0.931819  [   32/  118]
train() client id: f_00009-9-1 loss: 0.926281  [   64/  118]
train() client id: f_00009-9-2 loss: 0.786576  [   96/  118]
train() client id: f_00009-10-0 loss: 0.856692  [   32/  118]
train() client id: f_00009-10-1 loss: 0.924119  [   64/  118]
train() client id: f_00009-10-2 loss: 0.865530  [   96/  118]
train() client id: f_00009-11-0 loss: 0.781274  [   32/  118]
train() client id: f_00009-11-1 loss: 1.010664  [   64/  118]
train() client id: f_00009-11-2 loss: 0.902742  [   96/  118]
At round 15 accuracy: 0.6339522546419099
At round 15 training accuracy: 0.5788061703554661
At round 15 training loss: 0.8498632789172836
update_location
xs = [ -3.9056584    4.20031788  95.00902392  18.81129433   0.97929623
   3.95640986 -57.44319194 -36.32485185  79.66397685 -22.06087855]
ys = [ 87.5879595   70.55583871   1.32061395 -57.45517586  49.35018685
  32.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [132.99212314 122.45721312 137.94367926 116.85444804 111.51860815
 105.32056622 115.35428402 106.39629278 129.05432647 102.48265326]
dists_bs = [192.13052478 207.41585464 321.04423704 302.65198924 216.2327178
 228.56555375 213.0029065  222.6401832  299.39649124 229.41418462]
uav_gains = [4.90168661e-11 6.02574305e-11 4.47298682e-11 6.77442080e-11
 7.61419638e-11 8.78443206e-11 6.99686398e-11 8.56406368e-11
 5.28456671e-11 9.40528225e-11]
bs_gains = [4.45886427e-11 3.59862526e-11 1.05904015e-11 1.24925816e-11
 3.20268441e-11 2.74197759e-11 3.34052405e-11 2.95123738e-11
 1.28766623e-11 2.71367191e-11]
Round 16
-------------------------------
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.87981483 18.50415528  8.7446806   3.12925602 21.34431125 10.28544805
  3.88903844 12.52812053  9.21866598  8.34945337]
obj_prev = 104.87294435893298
eta_min = 3.443571724151839e-11	eta_max = 0.9213403601232922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 24.38417167306138	eta = 0.9090909090909091
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 42.374357946194124	eta = 0.5231330896350052
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 33.76652681787571	eta = 0.6564912320788918
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.22189998010197	eta = 0.6879615667412922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14522467977385	eta = 0.6896025463975112
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14502216089842	eta = 0.6896068910058766
eta = 0.6896068910058766
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.03066631 0.06449661 0.03017954 0.01046549 0.07447533 0.03553398
 0.0131427  0.04356563 0.03163984 0.02871924]
ene_total = [2.771699   5.28190218 2.74625142 1.26115748 6.02653747 3.19933593
 1.45388682 3.66054961 3.04133893 2.70236334]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 0 obj = 5.644901027509804
eta = 0.6896068910058766
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
eta_min = 0.6896068910059152	eta_max = 0.6896068910058611
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 0.03760750116688597	eta = 0.909090909090909
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 19.169603857786115	eta = 0.0017834816868453646
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9593134418751286	eta = 0.01744929458133157
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111928987478155	eta = 0.017888637743914634
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111822247051171	eta = 0.01788873765279858
eta = 0.01788873765279858
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.82570215e-04 1.75061300e-03 1.75501009e-04 7.05980817e-06
 2.72955744e-03 3.01849167e-04 1.39465251e-05 5.00344854e-04
 2.38449589e-04 1.59558866e-04]
ene_total = [0.17134348 0.21893109 0.17438925 0.15683431 0.24623749 0.19673596
 0.1560201  0.1614347  0.23535039 0.19390545]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 1 obj = 5.644901027510502
eta = 0.6896068910059152
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
Done!
At round 16 energy consumption: 5.644901027509804
At round 16 eta: 0.6896068910059152
At round 16 local rounds: 12.169171106060379
At round 16 global rounds: 73.13908926108901
At round 16 a_n: 22.359323457775982
gradient difference: 0.40236371755599976
train() client id: f_00000-0-0 loss: 1.463619  [   32/  126]
train() client id: f_00000-0-1 loss: 1.365917  [   64/  126]
train() client id: f_00000-0-2 loss: 1.222981  [   96/  126]
train() client id: f_00000-1-0 loss: 1.280246  [   32/  126]
train() client id: f_00000-1-1 loss: 1.369938  [   64/  126]
train() client id: f_00000-1-2 loss: 1.029787  [   96/  126]
train() client id: f_00000-2-0 loss: 1.253823  [   32/  126]
train() client id: f_00000-2-1 loss: 1.118785  [   64/  126]
train() client id: f_00000-2-2 loss: 1.068503  [   96/  126]
train() client id: f_00000-3-0 loss: 1.021215  [   32/  126]
train() client id: f_00000-3-1 loss: 1.253197  [   64/  126]
train() client id: f_00000-3-2 loss: 1.067398  [   96/  126]
train() client id: f_00000-4-0 loss: 1.071620  [   32/  126]
train() client id: f_00000-4-1 loss: 1.078479  [   64/  126]
train() client id: f_00000-4-2 loss: 1.029930  [   96/  126]
train() client id: f_00000-5-0 loss: 1.006339  [   32/  126]
train() client id: f_00000-5-1 loss: 1.020316  [   64/  126]
train() client id: f_00000-5-2 loss: 1.015505  [   96/  126]
train() client id: f_00000-6-0 loss: 0.964899  [   32/  126]
train() client id: f_00000-6-1 loss: 0.946955  [   64/  126]
train() client id: f_00000-6-2 loss: 1.007351  [   96/  126]
train() client id: f_00000-7-0 loss: 0.925168  [   32/  126]
train() client id: f_00000-7-1 loss: 0.985743  [   64/  126]
train() client id: f_00000-7-2 loss: 0.958689  [   96/  126]
train() client id: f_00000-8-0 loss: 0.937770  [   32/  126]
train() client id: f_00000-8-1 loss: 0.980967  [   64/  126]
train() client id: f_00000-8-2 loss: 0.870393  [   96/  126]
train() client id: f_00000-9-0 loss: 0.984419  [   32/  126]
train() client id: f_00000-9-1 loss: 0.954634  [   64/  126]
train() client id: f_00000-9-2 loss: 0.889887  [   96/  126]
train() client id: f_00000-10-0 loss: 0.919910  [   32/  126]
train() client id: f_00000-10-1 loss: 0.957770  [   64/  126]
train() client id: f_00000-10-2 loss: 0.962245  [   96/  126]
train() client id: f_00000-11-0 loss: 0.900907  [   32/  126]
train() client id: f_00000-11-1 loss: 0.992257  [   64/  126]
train() client id: f_00000-11-2 loss: 0.911755  [   96/  126]
train() client id: f_00001-0-0 loss: 0.368215  [   32/  265]
train() client id: f_00001-0-1 loss: 0.527551  [   64/  265]
train() client id: f_00001-0-2 loss: 0.430186  [   96/  265]
train() client id: f_00001-0-3 loss: 0.394792  [  128/  265]
train() client id: f_00001-0-4 loss: 0.506055  [  160/  265]
train() client id: f_00001-0-5 loss: 0.454929  [  192/  265]
train() client id: f_00001-0-6 loss: 0.338076  [  224/  265]
train() client id: f_00001-0-7 loss: 0.442708  [  256/  265]
train() client id: f_00001-1-0 loss: 0.397897  [   32/  265]
train() client id: f_00001-1-1 loss: 0.552062  [   64/  265]
train() client id: f_00001-1-2 loss: 0.439581  [   96/  265]
train() client id: f_00001-1-3 loss: 0.341974  [  128/  265]
train() client id: f_00001-1-4 loss: 0.351910  [  160/  265]
train() client id: f_00001-1-5 loss: 0.409808  [  192/  265]
train() client id: f_00001-1-6 loss: 0.466216  [  224/  265]
train() client id: f_00001-1-7 loss: 0.437533  [  256/  265]
train() client id: f_00001-2-0 loss: 0.432748  [   32/  265]
train() client id: f_00001-2-1 loss: 0.371848  [   64/  265]
train() client id: f_00001-2-2 loss: 0.473451  [   96/  265]
train() client id: f_00001-2-3 loss: 0.342485  [  128/  265]
train() client id: f_00001-2-4 loss: 0.419174  [  160/  265]
train() client id: f_00001-2-5 loss: 0.389802  [  192/  265]
train() client id: f_00001-2-6 loss: 0.525182  [  224/  265]
train() client id: f_00001-2-7 loss: 0.415647  [  256/  265]
train() client id: f_00001-3-0 loss: 0.446887  [   32/  265]
train() client id: f_00001-3-1 loss: 0.370925  [   64/  265]
train() client id: f_00001-3-2 loss: 0.349043  [   96/  265]
train() client id: f_00001-3-3 loss: 0.435540  [  128/  265]
train() client id: f_00001-3-4 loss: 0.430640  [  160/  265]
train() client id: f_00001-3-5 loss: 0.390216  [  192/  265]
train() client id: f_00001-3-6 loss: 0.517380  [  224/  265]
train() client id: f_00001-3-7 loss: 0.381166  [  256/  265]
train() client id: f_00001-4-0 loss: 0.465220  [   32/  265]
train() client id: f_00001-4-1 loss: 0.389081  [   64/  265]
train() client id: f_00001-4-2 loss: 0.372943  [   96/  265]
train() client id: f_00001-4-3 loss: 0.575690  [  128/  265]
train() client id: f_00001-4-4 loss: 0.381406  [  160/  265]
train() client id: f_00001-4-5 loss: 0.331745  [  192/  265]
train() client id: f_00001-4-6 loss: 0.322568  [  224/  265]
train() client id: f_00001-4-7 loss: 0.378674  [  256/  265]
train() client id: f_00001-5-0 loss: 0.323342  [   32/  265]
train() client id: f_00001-5-1 loss: 0.368043  [   64/  265]
train() client id: f_00001-5-2 loss: 0.375046  [   96/  265]
train() client id: f_00001-5-3 loss: 0.475555  [  128/  265]
train() client id: f_00001-5-4 loss: 0.516219  [  160/  265]
train() client id: f_00001-5-5 loss: 0.361555  [  192/  265]
train() client id: f_00001-5-6 loss: 0.438549  [  224/  265]
train() client id: f_00001-5-7 loss: 0.385953  [  256/  265]
train() client id: f_00001-6-0 loss: 0.365316  [   32/  265]
train() client id: f_00001-6-1 loss: 0.438514  [   64/  265]
train() client id: f_00001-6-2 loss: 0.380851  [   96/  265]
train() client id: f_00001-6-3 loss: 0.306753  [  128/  265]
train() client id: f_00001-6-4 loss: 0.409813  [  160/  265]
train() client id: f_00001-6-5 loss: 0.462989  [  192/  265]
train() client id: f_00001-6-6 loss: 0.440024  [  224/  265]
train() client id: f_00001-6-7 loss: 0.425667  [  256/  265]
train() client id: f_00001-7-0 loss: 0.356582  [   32/  265]
train() client id: f_00001-7-1 loss: 0.414655  [   64/  265]
train() client id: f_00001-7-2 loss: 0.452727  [   96/  265]
train() client id: f_00001-7-3 loss: 0.388890  [  128/  265]
train() client id: f_00001-7-4 loss: 0.384140  [  160/  265]
train() client id: f_00001-7-5 loss: 0.426090  [  192/  265]
train() client id: f_00001-7-6 loss: 0.310391  [  224/  265]
train() client id: f_00001-7-7 loss: 0.449033  [  256/  265]
train() client id: f_00001-8-0 loss: 0.342262  [   32/  265]
train() client id: f_00001-8-1 loss: 0.361520  [   64/  265]
train() client id: f_00001-8-2 loss: 0.430177  [   96/  265]
train() client id: f_00001-8-3 loss: 0.402840  [  128/  265]
train() client id: f_00001-8-4 loss: 0.400337  [  160/  265]
train() client id: f_00001-8-5 loss: 0.307640  [  192/  265]
train() client id: f_00001-8-6 loss: 0.403821  [  224/  265]
train() client id: f_00001-8-7 loss: 0.572569  [  256/  265]
train() client id: f_00001-9-0 loss: 0.376616  [   32/  265]
train() client id: f_00001-9-1 loss: 0.369475  [   64/  265]
train() client id: f_00001-9-2 loss: 0.381620  [   96/  265]
train() client id: f_00001-9-3 loss: 0.390800  [  128/  265]
train() client id: f_00001-9-4 loss: 0.524370  [  160/  265]
train() client id: f_00001-9-5 loss: 0.308799  [  192/  265]
train() client id: f_00001-9-6 loss: 0.385563  [  224/  265]
train() client id: f_00001-9-7 loss: 0.435501  [  256/  265]
train() client id: f_00001-10-0 loss: 0.353865  [   32/  265]
train() client id: f_00001-10-1 loss: 0.440529  [   64/  265]
train() client id: f_00001-10-2 loss: 0.452122  [   96/  265]
train() client id: f_00001-10-3 loss: 0.361398  [  128/  265]
train() client id: f_00001-10-4 loss: 0.355438  [  160/  265]
train() client id: f_00001-10-5 loss: 0.355959  [  192/  265]
train() client id: f_00001-10-6 loss: 0.389625  [  224/  265]
train() client id: f_00001-10-7 loss: 0.420049  [  256/  265]
train() client id: f_00001-11-0 loss: 0.295530  [   32/  265]
train() client id: f_00001-11-1 loss: 0.494477  [   64/  265]
train() client id: f_00001-11-2 loss: 0.352017  [   96/  265]
train() client id: f_00001-11-3 loss: 0.457016  [  128/  265]
train() client id: f_00001-11-4 loss: 0.436945  [  160/  265]
train() client id: f_00001-11-5 loss: 0.414556  [  192/  265]
train() client id: f_00001-11-6 loss: 0.368847  [  224/  265]
train() client id: f_00001-11-7 loss: 0.386711  [  256/  265]
train() client id: f_00002-0-0 loss: 1.276271  [   32/  124]
train() client id: f_00002-0-1 loss: 1.216379  [   64/  124]
train() client id: f_00002-0-2 loss: 1.303081  [   96/  124]
train() client id: f_00002-1-0 loss: 1.216234  [   32/  124]
train() client id: f_00002-1-1 loss: 1.286200  [   64/  124]
train() client id: f_00002-1-2 loss: 1.248192  [   96/  124]
train() client id: f_00002-2-0 loss: 1.346603  [   32/  124]
train() client id: f_00002-2-1 loss: 1.112532  [   64/  124]
train() client id: f_00002-2-2 loss: 1.166016  [   96/  124]
train() client id: f_00002-3-0 loss: 1.158242  [   32/  124]
train() client id: f_00002-3-1 loss: 1.198739  [   64/  124]
train() client id: f_00002-3-2 loss: 1.171178  [   96/  124]
train() client id: f_00002-4-0 loss: 1.151459  [   32/  124]
train() client id: f_00002-4-1 loss: 1.160948  [   64/  124]
train() client id: f_00002-4-2 loss: 1.076157  [   96/  124]
train() client id: f_00002-5-0 loss: 1.113520  [   32/  124]
train() client id: f_00002-5-1 loss: 1.084862  [   64/  124]
train() client id: f_00002-5-2 loss: 1.076066  [   96/  124]
train() client id: f_00002-6-0 loss: 1.036935  [   32/  124]
train() client id: f_00002-6-1 loss: 1.056795  [   64/  124]
train() client id: f_00002-6-2 loss: 1.051326  [   96/  124]
train() client id: f_00002-7-0 loss: 0.949800  [   32/  124]
train() client id: f_00002-7-1 loss: 1.085821  [   64/  124]
train() client id: f_00002-7-2 loss: 1.208449  [   96/  124]
train() client id: f_00002-8-0 loss: 1.075683  [   32/  124]
train() client id: f_00002-8-1 loss: 0.901666  [   64/  124]
train() client id: f_00002-8-2 loss: 1.198612  [   96/  124]
train() client id: f_00002-9-0 loss: 1.033009  [   32/  124]
train() client id: f_00002-9-1 loss: 1.205851  [   64/  124]
train() client id: f_00002-9-2 loss: 0.931782  [   96/  124]
train() client id: f_00002-10-0 loss: 0.983162  [   32/  124]
train() client id: f_00002-10-1 loss: 0.984493  [   64/  124]
train() client id: f_00002-10-2 loss: 1.033731  [   96/  124]
train() client id: f_00002-11-0 loss: 1.008928  [   32/  124]
train() client id: f_00002-11-1 loss: 1.035915  [   64/  124]
train() client id: f_00002-11-2 loss: 1.053542  [   96/  124]
train() client id: f_00003-0-0 loss: 0.873214  [   32/   43]
train() client id: f_00003-1-0 loss: 0.788895  [   32/   43]
train() client id: f_00003-2-0 loss: 0.858250  [   32/   43]
train() client id: f_00003-3-0 loss: 0.729143  [   32/   43]
train() client id: f_00003-4-0 loss: 0.837452  [   32/   43]
train() client id: f_00003-5-0 loss: 0.769008  [   32/   43]
train() client id: f_00003-6-0 loss: 0.708834  [   32/   43]
train() client id: f_00003-7-0 loss: 1.014404  [   32/   43]
train() client id: f_00003-8-0 loss: 0.785304  [   32/   43]
train() client id: f_00003-9-0 loss: 0.697019  [   32/   43]
train() client id: f_00003-10-0 loss: 0.978636  [   32/   43]
train() client id: f_00003-11-0 loss: 0.853690  [   32/   43]
train() client id: f_00004-0-0 loss: 1.092237  [   32/  306]
train() client id: f_00004-0-1 loss: 1.085930  [   64/  306]
train() client id: f_00004-0-2 loss: 0.971038  [   96/  306]
train() client id: f_00004-0-3 loss: 1.163137  [  128/  306]
train() client id: f_00004-0-4 loss: 0.974977  [  160/  306]
train() client id: f_00004-0-5 loss: 1.045309  [  192/  306]
train() client id: f_00004-0-6 loss: 1.091772  [  224/  306]
train() client id: f_00004-0-7 loss: 0.917553  [  256/  306]
train() client id: f_00004-0-8 loss: 1.008762  [  288/  306]
train() client id: f_00004-1-0 loss: 1.024960  [   32/  306]
train() client id: f_00004-1-1 loss: 1.125847  [   64/  306]
train() client id: f_00004-1-2 loss: 0.889229  [   96/  306]
train() client id: f_00004-1-3 loss: 0.913274  [  128/  306]
train() client id: f_00004-1-4 loss: 0.978010  [  160/  306]
train() client id: f_00004-1-5 loss: 1.028119  [  192/  306]
train() client id: f_00004-1-6 loss: 1.012329  [  224/  306]
train() client id: f_00004-1-7 loss: 1.093031  [  256/  306]
train() client id: f_00004-1-8 loss: 1.131193  [  288/  306]
train() client id: f_00004-2-0 loss: 1.096123  [   32/  306]
train() client id: f_00004-2-1 loss: 0.946894  [   64/  306]
train() client id: f_00004-2-2 loss: 1.044217  [   96/  306]
train() client id: f_00004-2-3 loss: 1.154704  [  128/  306]
train() client id: f_00004-2-4 loss: 0.930911  [  160/  306]
train() client id: f_00004-2-5 loss: 1.006111  [  192/  306]
train() client id: f_00004-2-6 loss: 0.984544  [  224/  306]
train() client id: f_00004-2-7 loss: 0.936442  [  256/  306]
train() client id: f_00004-2-8 loss: 0.976052  [  288/  306]
train() client id: f_00004-3-0 loss: 1.091235  [   32/  306]
train() client id: f_00004-3-1 loss: 0.957864  [   64/  306]
train() client id: f_00004-3-2 loss: 1.010221  [   96/  306]
train() client id: f_00004-3-3 loss: 1.013473  [  128/  306]
train() client id: f_00004-3-4 loss: 0.874447  [  160/  306]
train() client id: f_00004-3-5 loss: 0.912417  [  192/  306]
train() client id: f_00004-3-6 loss: 1.085894  [  224/  306]
train() client id: f_00004-3-7 loss: 1.030885  [  256/  306]
train() client id: f_00004-3-8 loss: 1.029453  [  288/  306]
train() client id: f_00004-4-0 loss: 1.088294  [   32/  306]
train() client id: f_00004-4-1 loss: 0.954105  [   64/  306]
train() client id: f_00004-4-2 loss: 1.014917  [   96/  306]
train() client id: f_00004-4-3 loss: 1.037057  [  128/  306]
train() client id: f_00004-4-4 loss: 0.886207  [  160/  306]
train() client id: f_00004-4-5 loss: 0.916107  [  192/  306]
train() client id: f_00004-4-6 loss: 1.048734  [  224/  306]
train() client id: f_00004-4-7 loss: 0.985602  [  256/  306]
train() client id: f_00004-4-8 loss: 1.016204  [  288/  306]
train() client id: f_00004-5-0 loss: 1.039855  [   32/  306]
train() client id: f_00004-5-1 loss: 0.992837  [   64/  306]
train() client id: f_00004-5-2 loss: 1.061881  [   96/  306]
train() client id: f_00004-5-3 loss: 0.995156  [  128/  306]
train() client id: f_00004-5-4 loss: 0.999795  [  160/  306]
train() client id: f_00004-5-5 loss: 1.062305  [  192/  306]
train() client id: f_00004-5-6 loss: 1.010242  [  224/  306]
train() client id: f_00004-5-7 loss: 0.894721  [  256/  306]
train() client id: f_00004-5-8 loss: 0.979060  [  288/  306]
train() client id: f_00004-6-0 loss: 1.011248  [   32/  306]
train() client id: f_00004-6-1 loss: 0.969637  [   64/  306]
train() client id: f_00004-6-2 loss: 0.943344  [   96/  306]
train() client id: f_00004-6-3 loss: 1.060950  [  128/  306]
train() client id: f_00004-6-4 loss: 1.034814  [  160/  306]
train() client id: f_00004-6-5 loss: 0.980931  [  192/  306]
train() client id: f_00004-6-6 loss: 0.985669  [  224/  306]
train() client id: f_00004-6-7 loss: 1.062103  [  256/  306]
train() client id: f_00004-6-8 loss: 0.908380  [  288/  306]
train() client id: f_00004-7-0 loss: 0.987895  [   32/  306]
train() client id: f_00004-7-1 loss: 1.052461  [   64/  306]
train() client id: f_00004-7-2 loss: 0.852394  [   96/  306]
train() client id: f_00004-7-3 loss: 0.980337  [  128/  306]
train() client id: f_00004-7-4 loss: 1.121070  [  160/  306]
train() client id: f_00004-7-5 loss: 0.858115  [  192/  306]
train() client id: f_00004-7-6 loss: 0.945332  [  224/  306]
train() client id: f_00004-7-7 loss: 1.163438  [  256/  306]
train() client id: f_00004-7-8 loss: 0.908391  [  288/  306]
train() client id: f_00004-8-0 loss: 0.965487  [   32/  306]
train() client id: f_00004-8-1 loss: 0.911964  [   64/  306]
train() client id: f_00004-8-2 loss: 0.981834  [   96/  306]
train() client id: f_00004-8-3 loss: 1.026205  [  128/  306]
train() client id: f_00004-8-4 loss: 1.003360  [  160/  306]
train() client id: f_00004-8-5 loss: 0.916175  [  192/  306]
train() client id: f_00004-8-6 loss: 1.144141  [  224/  306]
train() client id: f_00004-8-7 loss: 1.013756  [  256/  306]
train() client id: f_00004-8-8 loss: 1.003277  [  288/  306]
train() client id: f_00004-9-0 loss: 1.120564  [   32/  306]
train() client id: f_00004-9-1 loss: 0.887885  [   64/  306]
train() client id: f_00004-9-2 loss: 1.076879  [   96/  306]
train() client id: f_00004-9-3 loss: 1.037561  [  128/  306]
train() client id: f_00004-9-4 loss: 0.931648  [  160/  306]
train() client id: f_00004-9-5 loss: 0.926558  [  192/  306]
train() client id: f_00004-9-6 loss: 0.973871  [  224/  306]
train() client id: f_00004-9-7 loss: 1.015051  [  256/  306]
train() client id: f_00004-9-8 loss: 0.982204  [  288/  306]
train() client id: f_00004-10-0 loss: 0.986958  [   32/  306]
train() client id: f_00004-10-1 loss: 1.063598  [   64/  306]
train() client id: f_00004-10-2 loss: 1.016121  [   96/  306]
train() client id: f_00004-10-3 loss: 0.971105  [  128/  306]
train() client id: f_00004-10-4 loss: 1.008178  [  160/  306]
train() client id: f_00004-10-5 loss: 0.934208  [  192/  306]
train() client id: f_00004-10-6 loss: 1.025256  [  224/  306]
train() client id: f_00004-10-7 loss: 0.861198  [  256/  306]
train() client id: f_00004-10-8 loss: 0.932015  [  288/  306]
train() client id: f_00004-11-0 loss: 1.000659  [   32/  306]
train() client id: f_00004-11-1 loss: 1.035712  [   64/  306]
train() client id: f_00004-11-2 loss: 0.970379  [   96/  306]
train() client id: f_00004-11-3 loss: 0.935162  [  128/  306]
train() client id: f_00004-11-4 loss: 0.937121  [  160/  306]
train() client id: f_00004-11-5 loss: 1.069986  [  192/  306]
train() client id: f_00004-11-6 loss: 0.966995  [  224/  306]
train() client id: f_00004-11-7 loss: 0.975059  [  256/  306]
train() client id: f_00004-11-8 loss: 0.939522  [  288/  306]
train() client id: f_00005-0-0 loss: 0.716442  [   32/  146]
train() client id: f_00005-0-1 loss: 0.868714  [   64/  146]
train() client id: f_00005-0-2 loss: 0.776053  [   96/  146]
train() client id: f_00005-0-3 loss: 1.029583  [  128/  146]
train() client id: f_00005-1-0 loss: 0.715615  [   32/  146]
train() client id: f_00005-1-1 loss: 0.585465  [   64/  146]
train() client id: f_00005-1-2 loss: 0.977805  [   96/  146]
train() client id: f_00005-1-3 loss: 0.804112  [  128/  146]
train() client id: f_00005-2-0 loss: 0.770184  [   32/  146]
train() client id: f_00005-2-1 loss: 0.776146  [   64/  146]
train() client id: f_00005-2-2 loss: 0.812032  [   96/  146]
train() client id: f_00005-2-3 loss: 1.038975  [  128/  146]
train() client id: f_00005-3-0 loss: 0.994643  [   32/  146]
train() client id: f_00005-3-1 loss: 0.782042  [   64/  146]
train() client id: f_00005-3-2 loss: 0.727030  [   96/  146]
train() client id: f_00005-3-3 loss: 0.895111  [  128/  146]
train() client id: f_00005-4-0 loss: 0.909743  [   32/  146]
train() client id: f_00005-4-1 loss: 0.735018  [   64/  146]
train() client id: f_00005-4-2 loss: 0.691761  [   96/  146]
train() client id: f_00005-4-3 loss: 0.946735  [  128/  146]
train() client id: f_00005-5-0 loss: 0.765474  [   32/  146]
train() client id: f_00005-5-1 loss: 0.970214  [   64/  146]
train() client id: f_00005-5-2 loss: 0.808278  [   96/  146]
train() client id: f_00005-5-3 loss: 0.791792  [  128/  146]
train() client id: f_00005-6-0 loss: 0.758807  [   32/  146]
train() client id: f_00005-6-1 loss: 0.932466  [   64/  146]
train() client id: f_00005-6-2 loss: 0.911670  [   96/  146]
train() client id: f_00005-6-3 loss: 0.696854  [  128/  146]
train() client id: f_00005-7-0 loss: 0.859709  [   32/  146]
train() client id: f_00005-7-1 loss: 0.812926  [   64/  146]
train() client id: f_00005-7-2 loss: 0.755476  [   96/  146]
train() client id: f_00005-7-3 loss: 0.780758  [  128/  146]
train() client id: f_00005-8-0 loss: 0.851392  [   32/  146]
train() client id: f_00005-8-1 loss: 0.956073  [   64/  146]
train() client id: f_00005-8-2 loss: 0.892535  [   96/  146]
train() client id: f_00005-8-3 loss: 0.712526  [  128/  146]
train() client id: f_00005-9-0 loss: 0.863247  [   32/  146]
train() client id: f_00005-9-1 loss: 0.852479  [   64/  146]
train() client id: f_00005-9-2 loss: 0.670619  [   96/  146]
train() client id: f_00005-9-3 loss: 0.850923  [  128/  146]
train() client id: f_00005-10-0 loss: 0.684847  [   32/  146]
train() client id: f_00005-10-1 loss: 0.829106  [   64/  146]
train() client id: f_00005-10-2 loss: 0.931368  [   96/  146]
train() client id: f_00005-10-3 loss: 0.848062  [  128/  146]
train() client id: f_00005-11-0 loss: 1.086000  [   32/  146]
train() client id: f_00005-11-1 loss: 0.876508  [   64/  146]
train() client id: f_00005-11-2 loss: 0.824707  [   96/  146]
train() client id: f_00005-11-3 loss: 0.634035  [  128/  146]
train() client id: f_00006-0-0 loss: 0.539243  [   32/   54]
train() client id: f_00006-1-0 loss: 0.499356  [   32/   54]
train() client id: f_00006-2-0 loss: 0.607205  [   32/   54]
train() client id: f_00006-3-0 loss: 0.588740  [   32/   54]
train() client id: f_00006-4-0 loss: 0.555450  [   32/   54]
train() client id: f_00006-5-0 loss: 0.490068  [   32/   54]
train() client id: f_00006-6-0 loss: 0.600020  [   32/   54]
train() client id: f_00006-7-0 loss: 0.592753  [   32/   54]
train() client id: f_00006-8-0 loss: 0.548917  [   32/   54]
train() client id: f_00006-9-0 loss: 0.537309  [   32/   54]
train() client id: f_00006-10-0 loss: 0.545895  [   32/   54]
train() client id: f_00006-11-0 loss: 0.551092  [   32/   54]
train() client id: f_00007-0-0 loss: 0.578324  [   32/  179]
train() client id: f_00007-0-1 loss: 0.650040  [   64/  179]
train() client id: f_00007-0-2 loss: 0.688017  [   96/  179]
train() client id: f_00007-0-3 loss: 0.589990  [  128/  179]
train() client id: f_00007-0-4 loss: 0.627937  [  160/  179]
train() client id: f_00007-1-0 loss: 0.489945  [   32/  179]
train() client id: f_00007-1-1 loss: 0.668222  [   64/  179]
train() client id: f_00007-1-2 loss: 0.835675  [   96/  179]
train() client id: f_00007-1-3 loss: 0.562760  [  128/  179]
train() client id: f_00007-1-4 loss: 0.514172  [  160/  179]
train() client id: f_00007-2-0 loss: 0.623214  [   32/  179]
train() client id: f_00007-2-1 loss: 0.584981  [   64/  179]
train() client id: f_00007-2-2 loss: 0.697510  [   96/  179]
train() client id: f_00007-2-3 loss: 0.619484  [  128/  179]
train() client id: f_00007-2-4 loss: 0.504106  [  160/  179]
train() client id: f_00007-3-0 loss: 0.630160  [   32/  179]
train() client id: f_00007-3-1 loss: 0.519361  [   64/  179]
train() client id: f_00007-3-2 loss: 0.638304  [   96/  179]
train() client id: f_00007-3-3 loss: 0.580955  [  128/  179]
train() client id: f_00007-3-4 loss: 0.645711  [  160/  179]
train() client id: f_00007-4-0 loss: 0.583432  [   32/  179]
train() client id: f_00007-4-1 loss: 0.623972  [   64/  179]
train() client id: f_00007-4-2 loss: 0.643727  [   96/  179]
train() client id: f_00007-4-3 loss: 0.561702  [  128/  179]
train() client id: f_00007-4-4 loss: 0.511742  [  160/  179]
train() client id: f_00007-5-0 loss: 0.542544  [   32/  179]
train() client id: f_00007-5-1 loss: 0.691545  [   64/  179]
train() client id: f_00007-5-2 loss: 0.532123  [   96/  179]
train() client id: f_00007-5-3 loss: 0.716661  [  128/  179]
train() client id: f_00007-5-4 loss: 0.459906  [  160/  179]
train() client id: f_00007-6-0 loss: 0.622953  [   32/  179]
train() client id: f_00007-6-1 loss: 0.720782  [   64/  179]
train() client id: f_00007-6-2 loss: 0.585095  [   96/  179]
train() client id: f_00007-6-3 loss: 0.542112  [  128/  179]
train() client id: f_00007-6-4 loss: 0.446688  [  160/  179]
train() client id: f_00007-7-0 loss: 0.485729  [   32/  179]
train() client id: f_00007-7-1 loss: 0.540398  [   64/  179]
train() client id: f_00007-7-2 loss: 0.575045  [   96/  179]
train() client id: f_00007-7-3 loss: 0.615184  [  128/  179]
train() client id: f_00007-7-4 loss: 0.512155  [  160/  179]
train() client id: f_00007-8-0 loss: 0.419478  [   32/  179]
train() client id: f_00007-8-1 loss: 0.717764  [   64/  179]
train() client id: f_00007-8-2 loss: 0.707740  [   96/  179]
train() client id: f_00007-8-3 loss: 0.445423  [  128/  179]
train() client id: f_00007-8-4 loss: 0.534678  [  160/  179]
train() client id: f_00007-9-0 loss: 0.482503  [   32/  179]
train() client id: f_00007-9-1 loss: 0.609838  [   64/  179]
train() client id: f_00007-9-2 loss: 0.553992  [   96/  179]
train() client id: f_00007-9-3 loss: 0.581757  [  128/  179]
train() client id: f_00007-9-4 loss: 0.445964  [  160/  179]
train() client id: f_00007-10-0 loss: 0.427717  [   32/  179]
train() client id: f_00007-10-1 loss: 0.492044  [   64/  179]
train() client id: f_00007-10-2 loss: 0.483945  [   96/  179]
train() client id: f_00007-10-3 loss: 0.741534  [  128/  179]
train() client id: f_00007-10-4 loss: 0.521156  [  160/  179]
train() client id: f_00007-11-0 loss: 0.419512  [   32/  179]
train() client id: f_00007-11-1 loss: 0.673859  [   64/  179]
train() client id: f_00007-11-2 loss: 0.625099  [   96/  179]
train() client id: f_00007-11-3 loss: 0.584172  [  128/  179]
train() client id: f_00007-11-4 loss: 0.422849  [  160/  179]
train() client id: f_00008-0-0 loss: 0.833996  [   32/  130]
train() client id: f_00008-0-1 loss: 0.702692  [   64/  130]
train() client id: f_00008-0-2 loss: 0.672804  [   96/  130]
train() client id: f_00008-0-3 loss: 0.667202  [  128/  130]
train() client id: f_00008-1-0 loss: 0.581911  [   32/  130]
train() client id: f_00008-1-1 loss: 0.769215  [   64/  130]
train() client id: f_00008-1-2 loss: 0.677528  [   96/  130]
train() client id: f_00008-1-3 loss: 0.804881  [  128/  130]
train() client id: f_00008-2-0 loss: 0.752592  [   32/  130]
train() client id: f_00008-2-1 loss: 0.701346  [   64/  130]
train() client id: f_00008-2-2 loss: 0.737988  [   96/  130]
train() client id: f_00008-2-3 loss: 0.680862  [  128/  130]
train() client id: f_00008-3-0 loss: 0.805999  [   32/  130]
train() client id: f_00008-3-1 loss: 0.641063  [   64/  130]
train() client id: f_00008-3-2 loss: 0.722934  [   96/  130]
train() client id: f_00008-3-3 loss: 0.664839  [  128/  130]
train() client id: f_00008-4-0 loss: 0.758777  [   32/  130]
train() client id: f_00008-4-1 loss: 0.725188  [   64/  130]
train() client id: f_00008-4-2 loss: 0.710978  [   96/  130]
train() client id: f_00008-4-3 loss: 0.678231  [  128/  130]
train() client id: f_00008-5-0 loss: 0.671710  [   32/  130]
train() client id: f_00008-5-1 loss: 0.696211  [   64/  130]
train() client id: f_00008-5-2 loss: 0.757023  [   96/  130]
train() client id: f_00008-5-3 loss: 0.716180  [  128/  130]
train() client id: f_00008-6-0 loss: 0.678208  [   32/  130]
train() client id: f_00008-6-1 loss: 0.716772  [   64/  130]
train() client id: f_00008-6-2 loss: 0.923220  [   96/  130]
train() client id: f_00008-6-3 loss: 0.548913  [  128/  130]
train() client id: f_00008-7-0 loss: 0.780232  [   32/  130]
train() client id: f_00008-7-1 loss: 0.668236  [   64/  130]
train() client id: f_00008-7-2 loss: 0.724774  [   96/  130]
train() client id: f_00008-7-3 loss: 0.623860  [  128/  130]
train() client id: f_00008-8-0 loss: 0.717716  [   32/  130]
train() client id: f_00008-8-1 loss: 0.701983  [   64/  130]
train() client id: f_00008-8-2 loss: 0.715417  [   96/  130]
train() client id: f_00008-8-3 loss: 0.698031  [  128/  130]
train() client id: f_00008-9-0 loss: 0.570532  [   32/  130]
train() client id: f_00008-9-1 loss: 0.778960  [   64/  130]
train() client id: f_00008-9-2 loss: 0.797635  [   96/  130]
train() client id: f_00008-9-3 loss: 0.722767  [  128/  130]
train() client id: f_00008-10-0 loss: 0.670439  [   32/  130]
train() client id: f_00008-10-1 loss: 0.707410  [   64/  130]
train() client id: f_00008-10-2 loss: 0.695796  [   96/  130]
train() client id: f_00008-10-3 loss: 0.801877  [  128/  130]
train() client id: f_00008-11-0 loss: 0.706867  [   32/  130]
train() client id: f_00008-11-1 loss: 0.657610  [   64/  130]
train() client id: f_00008-11-2 loss: 0.691750  [   96/  130]
train() client id: f_00008-11-3 loss: 0.802162  [  128/  130]
train() client id: f_00009-0-0 loss: 1.119048  [   32/  118]
train() client id: f_00009-0-1 loss: 1.104076  [   64/  118]
train() client id: f_00009-0-2 loss: 1.081815  [   96/  118]
train() client id: f_00009-1-0 loss: 1.058287  [   32/  118]
train() client id: f_00009-1-1 loss: 1.150461  [   64/  118]
train() client id: f_00009-1-2 loss: 0.949396  [   96/  118]
train() client id: f_00009-2-0 loss: 1.080551  [   32/  118]
train() client id: f_00009-2-1 loss: 0.923981  [   64/  118]
train() client id: f_00009-2-2 loss: 1.140418  [   96/  118]
train() client id: f_00009-3-0 loss: 1.120993  [   32/  118]
train() client id: f_00009-3-1 loss: 0.964074  [   64/  118]
train() client id: f_00009-3-2 loss: 1.014090  [   96/  118]
train() client id: f_00009-4-0 loss: 0.940789  [   32/  118]
train() client id: f_00009-4-1 loss: 0.925289  [   64/  118]
train() client id: f_00009-4-2 loss: 0.987836  [   96/  118]
train() client id: f_00009-5-0 loss: 0.850968  [   32/  118]
train() client id: f_00009-5-1 loss: 0.901808  [   64/  118]
train() client id: f_00009-5-2 loss: 1.073307  [   96/  118]
train() client id: f_00009-6-0 loss: 0.878423  [   32/  118]
train() client id: f_00009-6-1 loss: 0.983916  [   64/  118]
train() client id: f_00009-6-2 loss: 0.900368  [   96/  118]
train() client id: f_00009-7-0 loss: 0.872106  [   32/  118]
train() client id: f_00009-7-1 loss: 0.930637  [   64/  118]
train() client id: f_00009-7-2 loss: 0.883789  [   96/  118]
train() client id: f_00009-8-0 loss: 0.863941  [   32/  118]
train() client id: f_00009-8-1 loss: 0.811872  [   64/  118]
train() client id: f_00009-8-2 loss: 0.952290  [   96/  118]
train() client id: f_00009-9-0 loss: 0.775873  [   32/  118]
train() client id: f_00009-9-1 loss: 0.964889  [   64/  118]
train() client id: f_00009-9-2 loss: 0.901290  [   96/  118]
train() client id: f_00009-10-0 loss: 0.912094  [   32/  118]
train() client id: f_00009-10-1 loss: 0.873062  [   64/  118]
train() client id: f_00009-10-2 loss: 0.917004  [   96/  118]
train() client id: f_00009-11-0 loss: 0.823356  [   32/  118]
train() client id: f_00009-11-1 loss: 0.802944  [   64/  118]
train() client id: f_00009-11-2 loss: 1.006757  [   96/  118]
At round 16 accuracy: 0.6339522546419099
At round 16 training accuracy: 0.5761234071093226
At round 16 training loss: 0.8489589442274741
update_location
xs = [ -3.9056584    4.20031788 100.00902392  18.81129433   0.97929623
   3.95640986 -62.44319194 -41.32485185  84.66397685 -27.06087855]
ys = [ 92.5879595   75.55583871   1.32061395 -62.45517586  54.35018685
  37.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [136.33702509 125.40465475 141.43390289 119.39226854 113.81960214
 106.98393887 117.92388546 108.20545105 132.19931524 103.6740228 ]
dists_bs = [189.90792018 204.94363894 325.26065297 306.50901839 213.36609411
 225.48914357 210.28473576 219.56069707 303.66082845 226.11164695]
uav_gains = [4.60616434e-11 5.67774848e-11 4.20154197e-11 6.42006706e-11
 7.23514457e-11 8.44693888e-11 6.62184205e-11 8.21054949e-11
 4.97558800e-11 9.13739608e-11]
bs_gains = [4.60652538e-11 3.72149675e-11 1.02104716e-11 1.20573805e-11
 3.32462714e-11 2.84801519e-11 3.46283970e-11 3.06860651e-11
 1.23767188e-11 2.82611527e-11]
Round 17
-------------------------------
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.74794599 18.22367373  8.61494673  3.08364678 21.02075903 10.12863146
  3.83198593 12.340238    9.082055    8.2217282 ]
obj_prev = 103.29561085136496
eta_min = 2.3980574083554427e-11	eta_max = 0.9215530004474818
af = 21.83294705699746	bf = 1.718217883710131	zeta = 24.01624176269721	eta = 0.9090909090909091
af = 21.83294705699746	bf = 1.718217883710131	zeta = 41.790319927079516	eta = 0.5224402946685754
af = 21.83294705699746	bf = 1.718217883710131	zeta = 33.27990204277066	eta = 0.6560400036315671
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.752345685579005	eta = 0.6876010759391976
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.67639108760098	eta = 0.6892498263649575
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.676189751909728	eta = 0.6892542072766557
eta = 0.6892542072766557
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.03070852 0.06458539 0.03022109 0.01047989 0.07457784 0.03558289
 0.0131608  0.0436256  0.0316834  0.02875878]
ene_total = [2.73656278 5.19862747 2.71178509 1.24687687 5.93149526 3.1458414
 1.43681478 3.6092118  3.0031103  2.655864  ]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 0 obj = 5.56824286220838
eta = 0.6892542072766557
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
eta_min = 0.6892542072766561	eta_max = 0.6892542072766527
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 0.03573652736322809	eta = 0.9090909090909091
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 18.93445726929715	eta = 0.0017158005474531947
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.9284046028512083	eta = 0.016846958413371853
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.882600477836571	eta = 0.017256848986739414
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.8825910021965795	eta = 0.01725693584558884
eta = 0.01725693584558884
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.79246673e-04 1.70228989e-03 1.72328565e-04 6.92481033e-06
 2.65200012e-03 2.93061998e-04 1.36811274e-05 4.90328058e-04
 2.33730561e-04 1.54858866e-04]
ene_total = [0.17070727 0.21310565 0.17380525 0.15598508 0.23910014 0.1918103
 0.15520187 0.15983003 0.23404959 0.18899581]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 1 obj = 5.56824286220839
eta = 0.6892542072766561
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
Done!
At round 17 energy consumption: 5.56824286220838
At round 17 eta: 0.6892542072766561
At round 17 local rounds: 12.185922115151007
At round 17 global rounds: 71.9537447693859
At round 17 a_n: 22.016777610806663
gradient difference: 0.42808741331100464
train() client id: f_00000-0-0 loss: 1.373654  [   32/  126]
train() client id: f_00000-0-1 loss: 1.264243  [   64/  126]
train() client id: f_00000-0-2 loss: 1.174378  [   96/  126]
train() client id: f_00000-1-0 loss: 1.336537  [   32/  126]
train() client id: f_00000-1-1 loss: 1.166258  [   64/  126]
train() client id: f_00000-1-2 loss: 1.046601  [   96/  126]
train() client id: f_00000-2-0 loss: 1.158548  [   32/  126]
train() client id: f_00000-2-1 loss: 1.031283  [   64/  126]
train() client id: f_00000-2-2 loss: 1.038076  [   96/  126]
train() client id: f_00000-3-0 loss: 1.018044  [   32/  126]
train() client id: f_00000-3-1 loss: 1.059650  [   64/  126]
train() client id: f_00000-3-2 loss: 0.977212  [   96/  126]
train() client id: f_00000-4-0 loss: 0.969470  [   32/  126]
train() client id: f_00000-4-1 loss: 0.943457  [   64/  126]
train() client id: f_00000-4-2 loss: 1.027095  [   96/  126]
train() client id: f_00000-5-0 loss: 0.997861  [   32/  126]
train() client id: f_00000-5-1 loss: 0.901026  [   64/  126]
train() client id: f_00000-5-2 loss: 0.956818  [   96/  126]
train() client id: f_00000-6-0 loss: 0.919619  [   32/  126]
train() client id: f_00000-6-1 loss: 1.004250  [   64/  126]
train() client id: f_00000-6-2 loss: 0.902400  [   96/  126]
train() client id: f_00000-7-0 loss: 0.900651  [   32/  126]
train() client id: f_00000-7-1 loss: 0.956044  [   64/  126]
train() client id: f_00000-7-2 loss: 0.851429  [   96/  126]
train() client id: f_00000-8-0 loss: 1.007081  [   32/  126]
train() client id: f_00000-8-1 loss: 0.838574  [   64/  126]
train() client id: f_00000-8-2 loss: 0.877661  [   96/  126]
train() client id: f_00000-9-0 loss: 0.924215  [   32/  126]
train() client id: f_00000-9-1 loss: 0.920787  [   64/  126]
train() client id: f_00000-9-2 loss: 0.878893  [   96/  126]
train() client id: f_00000-10-0 loss: 0.931513  [   32/  126]
train() client id: f_00000-10-1 loss: 0.861927  [   64/  126]
train() client id: f_00000-10-2 loss: 0.966835  [   96/  126]
train() client id: f_00000-11-0 loss: 0.838700  [   32/  126]
train() client id: f_00000-11-1 loss: 0.947871  [   64/  126]
train() client id: f_00000-11-2 loss: 0.891717  [   96/  126]
train() client id: f_00001-0-0 loss: 0.534118  [   32/  265]
train() client id: f_00001-0-1 loss: 0.566147  [   64/  265]
train() client id: f_00001-0-2 loss: 0.429212  [   96/  265]
train() client id: f_00001-0-3 loss: 0.462620  [  128/  265]
train() client id: f_00001-0-4 loss: 0.454138  [  160/  265]
train() client id: f_00001-0-5 loss: 0.487858  [  192/  265]
train() client id: f_00001-0-6 loss: 0.414963  [  224/  265]
train() client id: f_00001-0-7 loss: 0.461123  [  256/  265]
train() client id: f_00001-1-0 loss: 0.539245  [   32/  265]
train() client id: f_00001-1-1 loss: 0.435877  [   64/  265]
train() client id: f_00001-1-2 loss: 0.447723  [   96/  265]
train() client id: f_00001-1-3 loss: 0.497320  [  128/  265]
train() client id: f_00001-1-4 loss: 0.462120  [  160/  265]
train() client id: f_00001-1-5 loss: 0.460614  [  192/  265]
train() client id: f_00001-1-6 loss: 0.385627  [  224/  265]
train() client id: f_00001-1-7 loss: 0.590544  [  256/  265]
train() client id: f_00001-2-0 loss: 0.544792  [   32/  265]
train() client id: f_00001-2-1 loss: 0.407061  [   64/  265]
train() client id: f_00001-2-2 loss: 0.447636  [   96/  265]
train() client id: f_00001-2-3 loss: 0.458844  [  128/  265]
train() client id: f_00001-2-4 loss: 0.371217  [  160/  265]
train() client id: f_00001-2-5 loss: 0.536462  [  192/  265]
train() client id: f_00001-2-6 loss: 0.421460  [  224/  265]
train() client id: f_00001-2-7 loss: 0.518757  [  256/  265]
train() client id: f_00001-3-0 loss: 0.396470  [   32/  265]
train() client id: f_00001-3-1 loss: 0.643938  [   64/  265]
train() client id: f_00001-3-2 loss: 0.434277  [   96/  265]
train() client id: f_00001-3-3 loss: 0.368588  [  128/  265]
train() client id: f_00001-3-4 loss: 0.587296  [  160/  265]
train() client id: f_00001-3-5 loss: 0.459982  [  192/  265]
train() client id: f_00001-3-6 loss: 0.438409  [  224/  265]
train() client id: f_00001-3-7 loss: 0.391564  [  256/  265]
train() client id: f_00001-4-0 loss: 0.481283  [   32/  265]
train() client id: f_00001-4-1 loss: 0.503472  [   64/  265]
train() client id: f_00001-4-2 loss: 0.478647  [   96/  265]
train() client id: f_00001-4-3 loss: 0.428057  [  128/  265]
train() client id: f_00001-4-4 loss: 0.494990  [  160/  265]
train() client id: f_00001-4-5 loss: 0.469508  [  192/  265]
train() client id: f_00001-4-6 loss: 0.360899  [  224/  265]
train() client id: f_00001-4-7 loss: 0.470332  [  256/  265]
train() client id: f_00001-5-0 loss: 0.371791  [   32/  265]
train() client id: f_00001-5-1 loss: 0.535247  [   64/  265]
train() client id: f_00001-5-2 loss: 0.497169  [   96/  265]
train() client id: f_00001-5-3 loss: 0.364766  [  128/  265]
train() client id: f_00001-5-4 loss: 0.419519  [  160/  265]
train() client id: f_00001-5-5 loss: 0.395391  [  192/  265]
train() client id: f_00001-5-6 loss: 0.557231  [  224/  265]
train() client id: f_00001-5-7 loss: 0.539566  [  256/  265]
train() client id: f_00001-6-0 loss: 0.553596  [   32/  265]
train() client id: f_00001-6-1 loss: 0.416254  [   64/  265]
train() client id: f_00001-6-2 loss: 0.392014  [   96/  265]
train() client id: f_00001-6-3 loss: 0.479135  [  128/  265]
train() client id: f_00001-6-4 loss: 0.528218  [  160/  265]
train() client id: f_00001-6-5 loss: 0.347913  [  192/  265]
train() client id: f_00001-6-6 loss: 0.536825  [  224/  265]
train() client id: f_00001-6-7 loss: 0.411723  [  256/  265]
train() client id: f_00001-7-0 loss: 0.559136  [   32/  265]
train() client id: f_00001-7-1 loss: 0.405866  [   64/  265]
train() client id: f_00001-7-2 loss: 0.502579  [   96/  265]
train() client id: f_00001-7-3 loss: 0.444268  [  128/  265]
train() client id: f_00001-7-4 loss: 0.366598  [  160/  265]
train() client id: f_00001-7-5 loss: 0.516438  [  192/  265]
train() client id: f_00001-7-6 loss: 0.435224  [  224/  265]
train() client id: f_00001-7-7 loss: 0.379771  [  256/  265]
train() client id: f_00001-8-0 loss: 0.469886  [   32/  265]
train() client id: f_00001-8-1 loss: 0.439334  [   64/  265]
train() client id: f_00001-8-2 loss: 0.499280  [   96/  265]
train() client id: f_00001-8-3 loss: 0.376786  [  128/  265]
train() client id: f_00001-8-4 loss: 0.629535  [  160/  265]
train() client id: f_00001-8-5 loss: 0.432201  [  192/  265]
train() client id: f_00001-8-6 loss: 0.353936  [  224/  265]
train() client id: f_00001-8-7 loss: 0.374430  [  256/  265]
train() client id: f_00001-9-0 loss: 0.379606  [   32/  265]
train() client id: f_00001-9-1 loss: 0.515092  [   64/  265]
train() client id: f_00001-9-2 loss: 0.391079  [   96/  265]
train() client id: f_00001-9-3 loss: 0.468092  [  128/  265]
train() client id: f_00001-9-4 loss: 0.354598  [  160/  265]
train() client id: f_00001-9-5 loss: 0.544998  [  192/  265]
train() client id: f_00001-9-6 loss: 0.571749  [  224/  265]
train() client id: f_00001-9-7 loss: 0.382021  [  256/  265]
train() client id: f_00001-10-0 loss: 0.497915  [   32/  265]
train() client id: f_00001-10-1 loss: 0.357068  [   64/  265]
train() client id: f_00001-10-2 loss: 0.357112  [   96/  265]
train() client id: f_00001-10-3 loss: 0.419545  [  128/  265]
train() client id: f_00001-10-4 loss: 0.496537  [  160/  265]
train() client id: f_00001-10-5 loss: 0.511557  [  192/  265]
train() client id: f_00001-10-6 loss: 0.442035  [  224/  265]
train() client id: f_00001-10-7 loss: 0.529677  [  256/  265]
train() client id: f_00001-11-0 loss: 0.581273  [   32/  265]
train() client id: f_00001-11-1 loss: 0.521980  [   64/  265]
train() client id: f_00001-11-2 loss: 0.385599  [   96/  265]
train() client id: f_00001-11-3 loss: 0.413635  [  128/  265]
train() client id: f_00001-11-4 loss: 0.436969  [  160/  265]
train() client id: f_00001-11-5 loss: 0.478575  [  192/  265]
train() client id: f_00001-11-6 loss: 0.347183  [  224/  265]
train() client id: f_00001-11-7 loss: 0.495161  [  256/  265]
train() client id: f_00002-0-0 loss: 1.378549  [   32/  124]
train() client id: f_00002-0-1 loss: 1.224790  [   64/  124]
train() client id: f_00002-0-2 loss: 1.234935  [   96/  124]
train() client id: f_00002-1-0 loss: 1.141251  [   32/  124]
train() client id: f_00002-1-1 loss: 1.182594  [   64/  124]
train() client id: f_00002-1-2 loss: 1.323445  [   96/  124]
train() client id: f_00002-2-0 loss: 1.100710  [   32/  124]
train() client id: f_00002-2-1 loss: 1.246063  [   64/  124]
train() client id: f_00002-2-2 loss: 1.250605  [   96/  124]
train() client id: f_00002-3-0 loss: 1.112189  [   32/  124]
train() client id: f_00002-3-1 loss: 1.121667  [   64/  124]
train() client id: f_00002-3-2 loss: 1.274511  [   96/  124]
train() client id: f_00002-4-0 loss: 1.113498  [   32/  124]
train() client id: f_00002-4-1 loss: 1.158925  [   64/  124]
train() client id: f_00002-4-2 loss: 1.076109  [   96/  124]
train() client id: f_00002-5-0 loss: 1.053540  [   32/  124]
train() client id: f_00002-5-1 loss: 1.027264  [   64/  124]
train() client id: f_00002-5-2 loss: 1.073795  [   96/  124]
train() client id: f_00002-6-0 loss: 1.063889  [   32/  124]
train() client id: f_00002-6-1 loss: 1.064540  [   64/  124]
train() client id: f_00002-6-2 loss: 0.964514  [   96/  124]
train() client id: f_00002-7-0 loss: 0.955675  [   32/  124]
train() client id: f_00002-7-1 loss: 1.173775  [   64/  124]
train() client id: f_00002-7-2 loss: 0.970880  [   96/  124]
train() client id: f_00002-8-0 loss: 1.038386  [   32/  124]
train() client id: f_00002-8-1 loss: 1.037458  [   64/  124]
train() client id: f_00002-8-2 loss: 0.949257  [   96/  124]
train() client id: f_00002-9-0 loss: 0.934431  [   32/  124]
train() client id: f_00002-9-1 loss: 0.932630  [   64/  124]
train() client id: f_00002-9-2 loss: 1.069274  [   96/  124]
train() client id: f_00002-10-0 loss: 0.877593  [   32/  124]
train() client id: f_00002-10-1 loss: 1.010728  [   64/  124]
train() client id: f_00002-10-2 loss: 1.038505  [   96/  124]
train() client id: f_00002-11-0 loss: 1.066177  [   32/  124]
train() client id: f_00002-11-1 loss: 0.893836  [   64/  124]
train() client id: f_00002-11-2 loss: 1.030602  [   96/  124]
train() client id: f_00003-0-0 loss: 0.694201  [   32/   43]
train() client id: f_00003-1-0 loss: 0.612853  [   32/   43]
train() client id: f_00003-2-0 loss: 0.752967  [   32/   43]
train() client id: f_00003-3-0 loss: 0.651120  [   32/   43]
train() client id: f_00003-4-0 loss: 0.727873  [   32/   43]
train() client id: f_00003-5-0 loss: 0.717924  [   32/   43]
train() client id: f_00003-6-0 loss: 0.670620  [   32/   43]
train() client id: f_00003-7-0 loss: 0.771030  [   32/   43]
train() client id: f_00003-8-0 loss: 0.759584  [   32/   43]
train() client id: f_00003-9-0 loss: 0.858778  [   32/   43]
train() client id: f_00003-10-0 loss: 0.671604  [   32/   43]
train() client id: f_00003-11-0 loss: 0.735755  [   32/   43]
train() client id: f_00004-0-0 loss: 0.884620  [   32/  306]
train() client id: f_00004-0-1 loss: 1.023712  [   64/  306]
train() client id: f_00004-0-2 loss: 0.943678  [   96/  306]
train() client id: f_00004-0-3 loss: 1.005897  [  128/  306]
train() client id: f_00004-0-4 loss: 0.894758  [  160/  306]
train() client id: f_00004-0-5 loss: 0.829027  [  192/  306]
train() client id: f_00004-0-6 loss: 0.935789  [  224/  306]
train() client id: f_00004-0-7 loss: 1.176395  [  256/  306]
train() client id: f_00004-0-8 loss: 0.906060  [  288/  306]
train() client id: f_00004-1-0 loss: 0.857052  [   32/  306]
train() client id: f_00004-1-1 loss: 1.020831  [   64/  306]
train() client id: f_00004-1-2 loss: 0.807742  [   96/  306]
train() client id: f_00004-1-3 loss: 1.011217  [  128/  306]
train() client id: f_00004-1-4 loss: 0.931527  [  160/  306]
train() client id: f_00004-1-5 loss: 0.890498  [  192/  306]
train() client id: f_00004-1-6 loss: 0.928170  [  224/  306]
train() client id: f_00004-1-7 loss: 1.064112  [  256/  306]
train() client id: f_00004-1-8 loss: 0.995819  [  288/  306]
train() client id: f_00004-2-0 loss: 0.962763  [   32/  306]
train() client id: f_00004-2-1 loss: 0.987816  [   64/  306]
train() client id: f_00004-2-2 loss: 0.923728  [   96/  306]
train() client id: f_00004-2-3 loss: 0.795361  [  128/  306]
train() client id: f_00004-2-4 loss: 0.997377  [  160/  306]
train() client id: f_00004-2-5 loss: 1.066670  [  192/  306]
train() client id: f_00004-2-6 loss: 0.875644  [  224/  306]
train() client id: f_00004-2-7 loss: 1.067639  [  256/  306]
train() client id: f_00004-2-8 loss: 0.825931  [  288/  306]
train() client id: f_00004-3-0 loss: 1.017183  [   32/  306]
train() client id: f_00004-3-1 loss: 1.137706  [   64/  306]
train() client id: f_00004-3-2 loss: 1.042302  [   96/  306]
train() client id: f_00004-3-3 loss: 0.976404  [  128/  306]
train() client id: f_00004-3-4 loss: 0.785792  [  160/  306]
train() client id: f_00004-3-5 loss: 0.817085  [  192/  306]
train() client id: f_00004-3-6 loss: 1.030060  [  224/  306]
train() client id: f_00004-3-7 loss: 0.880431  [  256/  306]
train() client id: f_00004-3-8 loss: 0.920798  [  288/  306]
train() client id: f_00004-4-0 loss: 0.853544  [   32/  306]
train() client id: f_00004-4-1 loss: 0.981420  [   64/  306]
train() client id: f_00004-4-2 loss: 1.000578  [   96/  306]
train() client id: f_00004-4-3 loss: 0.898790  [  128/  306]
train() client id: f_00004-4-4 loss: 0.949501  [  160/  306]
train() client id: f_00004-4-5 loss: 0.993577  [  192/  306]
train() client id: f_00004-4-6 loss: 1.123530  [  224/  306]
train() client id: f_00004-4-7 loss: 0.874344  [  256/  306]
train() client id: f_00004-4-8 loss: 0.827913  [  288/  306]
train() client id: f_00004-5-0 loss: 0.985667  [   32/  306]
train() client id: f_00004-5-1 loss: 0.911687  [   64/  306]
train() client id: f_00004-5-2 loss: 0.966046  [   96/  306]
train() client id: f_00004-5-3 loss: 0.937672  [  128/  306]
train() client id: f_00004-5-4 loss: 1.023576  [  160/  306]
train() client id: f_00004-5-5 loss: 0.890566  [  192/  306]
train() client id: f_00004-5-6 loss: 0.913568  [  224/  306]
train() client id: f_00004-5-7 loss: 0.948856  [  256/  306]
train() client id: f_00004-5-8 loss: 0.934051  [  288/  306]
train() client id: f_00004-6-0 loss: 0.944185  [   32/  306]
train() client id: f_00004-6-1 loss: 0.911497  [   64/  306]
train() client id: f_00004-6-2 loss: 0.933043  [   96/  306]
train() client id: f_00004-6-3 loss: 0.952772  [  128/  306]
train() client id: f_00004-6-4 loss: 0.729479  [  160/  306]
train() client id: f_00004-6-5 loss: 1.027761  [  192/  306]
train() client id: f_00004-6-6 loss: 0.858304  [  224/  306]
train() client id: f_00004-6-7 loss: 1.061440  [  256/  306]
train() client id: f_00004-6-8 loss: 1.093951  [  288/  306]
train() client id: f_00004-7-0 loss: 0.928257  [   32/  306]
train() client id: f_00004-7-1 loss: 0.957412  [   64/  306]
train() client id: f_00004-7-2 loss: 0.961788  [   96/  306]
train() client id: f_00004-7-3 loss: 0.952320  [  128/  306]
train() client id: f_00004-7-4 loss: 1.013627  [  160/  306]
train() client id: f_00004-7-5 loss: 0.898438  [  192/  306]
train() client id: f_00004-7-6 loss: 0.881268  [  224/  306]
train() client id: f_00004-7-7 loss: 0.949110  [  256/  306]
train() client id: f_00004-7-8 loss: 0.934505  [  288/  306]
train() client id: f_00004-8-0 loss: 1.000460  [   32/  306]
train() client id: f_00004-8-1 loss: 0.946808  [   64/  306]
train() client id: f_00004-8-2 loss: 0.883058  [   96/  306]
train() client id: f_00004-8-3 loss: 0.979204  [  128/  306]
train() client id: f_00004-8-4 loss: 0.958600  [  160/  306]
train() client id: f_00004-8-5 loss: 0.975291  [  192/  306]
train() client id: f_00004-8-6 loss: 0.953406  [  224/  306]
train() client id: f_00004-8-7 loss: 0.932043  [  256/  306]
train() client id: f_00004-8-8 loss: 0.893408  [  288/  306]
train() client id: f_00004-9-0 loss: 1.006030  [   32/  306]
train() client id: f_00004-9-1 loss: 0.918416  [   64/  306]
train() client id: f_00004-9-2 loss: 1.001782  [   96/  306]
train() client id: f_00004-9-3 loss: 0.862190  [  128/  306]
train() client id: f_00004-9-4 loss: 0.966283  [  160/  306]
train() client id: f_00004-9-5 loss: 1.024872  [  192/  306]
train() client id: f_00004-9-6 loss: 0.868562  [  224/  306]
train() client id: f_00004-9-7 loss: 0.893164  [  256/  306]
train() client id: f_00004-9-8 loss: 0.873516  [  288/  306]
train() client id: f_00004-10-0 loss: 0.928619  [   32/  306]
train() client id: f_00004-10-1 loss: 0.892934  [   64/  306]
train() client id: f_00004-10-2 loss: 1.048609  [   96/  306]
train() client id: f_00004-10-3 loss: 0.881605  [  128/  306]
train() client id: f_00004-10-4 loss: 0.896196  [  160/  306]
train() client id: f_00004-10-5 loss: 0.922024  [  192/  306]
train() client id: f_00004-10-6 loss: 0.957359  [  224/  306]
train() client id: f_00004-10-7 loss: 0.891212  [  256/  306]
train() client id: f_00004-10-8 loss: 1.029938  [  288/  306]
train() client id: f_00004-11-0 loss: 1.080607  [   32/  306]
train() client id: f_00004-11-1 loss: 0.979880  [   64/  306]
train() client id: f_00004-11-2 loss: 0.983529  [   96/  306]
train() client id: f_00004-11-3 loss: 0.847633  [  128/  306]
train() client id: f_00004-11-4 loss: 0.991176  [  160/  306]
train() client id: f_00004-11-5 loss: 0.834174  [  192/  306]
train() client id: f_00004-11-6 loss: 0.926648  [  224/  306]
train() client id: f_00004-11-7 loss: 0.886120  [  256/  306]
train() client id: f_00004-11-8 loss: 0.926570  [  288/  306]
train() client id: f_00005-0-0 loss: 0.734849  [   32/  146]
train() client id: f_00005-0-1 loss: 0.819548  [   64/  146]
train() client id: f_00005-0-2 loss: 0.849080  [   96/  146]
train() client id: f_00005-0-3 loss: 0.926153  [  128/  146]
train() client id: f_00005-1-0 loss: 0.926154  [   32/  146]
train() client id: f_00005-1-1 loss: 0.736871  [   64/  146]
train() client id: f_00005-1-2 loss: 0.815046  [   96/  146]
train() client id: f_00005-1-3 loss: 0.922227  [  128/  146]
train() client id: f_00005-2-0 loss: 0.708925  [   32/  146]
train() client id: f_00005-2-1 loss: 0.705861  [   64/  146]
train() client id: f_00005-2-2 loss: 0.850747  [   96/  146]
train() client id: f_00005-2-3 loss: 0.903222  [  128/  146]
train() client id: f_00005-3-0 loss: 0.927857  [   32/  146]
train() client id: f_00005-3-1 loss: 0.609284  [   64/  146]
train() client id: f_00005-3-2 loss: 0.762003  [   96/  146]
train() client id: f_00005-3-3 loss: 0.987780  [  128/  146]
train() client id: f_00005-4-0 loss: 0.905154  [   32/  146]
train() client id: f_00005-4-1 loss: 0.732535  [   64/  146]
train() client id: f_00005-4-2 loss: 0.921493  [   96/  146]
train() client id: f_00005-4-3 loss: 0.699199  [  128/  146]
train() client id: f_00005-5-0 loss: 0.738777  [   32/  146]
train() client id: f_00005-5-1 loss: 0.912431  [   64/  146]
train() client id: f_00005-5-2 loss: 0.924637  [   96/  146]
train() client id: f_00005-5-3 loss: 0.836743  [  128/  146]
train() client id: f_00005-6-0 loss: 0.870504  [   32/  146]
train() client id: f_00005-6-1 loss: 0.856120  [   64/  146]
train() client id: f_00005-6-2 loss: 0.914964  [   96/  146]
train() client id: f_00005-6-3 loss: 0.725160  [  128/  146]
train() client id: f_00005-7-0 loss: 0.595548  [   32/  146]
train() client id: f_00005-7-1 loss: 1.084394  [   64/  146]
train() client id: f_00005-7-2 loss: 0.679152  [   96/  146]
train() client id: f_00005-7-3 loss: 0.958969  [  128/  146]
train() client id: f_00005-8-0 loss: 0.977604  [   32/  146]
train() client id: f_00005-8-1 loss: 0.886674  [   64/  146]
train() client id: f_00005-8-2 loss: 0.924385  [   96/  146]
train() client id: f_00005-8-3 loss: 0.586656  [  128/  146]
train() client id: f_00005-9-0 loss: 1.009481  [   32/  146]
train() client id: f_00005-9-1 loss: 0.881895  [   64/  146]
train() client id: f_00005-9-2 loss: 0.778113  [   96/  146]
train() client id: f_00005-9-3 loss: 0.790421  [  128/  146]
train() client id: f_00005-10-0 loss: 0.884384  [   32/  146]
train() client id: f_00005-10-1 loss: 1.056697  [   64/  146]
train() client id: f_00005-10-2 loss: 0.869052  [   96/  146]
train() client id: f_00005-10-3 loss: 0.619376  [  128/  146]
train() client id: f_00005-11-0 loss: 1.075687  [   32/  146]
train() client id: f_00005-11-1 loss: 0.836309  [   64/  146]
train() client id: f_00005-11-2 loss: 0.823949  [   96/  146]
train() client id: f_00005-11-3 loss: 0.656773  [  128/  146]
train() client id: f_00006-0-0 loss: 0.616758  [   32/   54]
train() client id: f_00006-1-0 loss: 0.571608  [   32/   54]
train() client id: f_00006-2-0 loss: 0.580979  [   32/   54]
train() client id: f_00006-3-0 loss: 0.571116  [   32/   54]
train() client id: f_00006-4-0 loss: 0.565607  [   32/   54]
train() client id: f_00006-5-0 loss: 0.561932  [   32/   54]
train() client id: f_00006-6-0 loss: 0.525059  [   32/   54]
train() client id: f_00006-7-0 loss: 0.517803  [   32/   54]
train() client id: f_00006-8-0 loss: 0.626786  [   32/   54]
train() client id: f_00006-9-0 loss: 0.623993  [   32/   54]
train() client id: f_00006-10-0 loss: 0.574063  [   32/   54]
train() client id: f_00006-11-0 loss: 0.556746  [   32/   54]
train() client id: f_00007-0-0 loss: 0.749588  [   32/  179]
train() client id: f_00007-0-1 loss: 0.715926  [   64/  179]
train() client id: f_00007-0-2 loss: 0.680801  [   96/  179]
train() client id: f_00007-0-3 loss: 0.769683  [  128/  179]
train() client id: f_00007-0-4 loss: 0.789401  [  160/  179]
train() client id: f_00007-1-0 loss: 0.907034  [   32/  179]
train() client id: f_00007-1-1 loss: 0.730544  [   64/  179]
train() client id: f_00007-1-2 loss: 0.630849  [   96/  179]
train() client id: f_00007-1-3 loss: 0.692987  [  128/  179]
train() client id: f_00007-1-4 loss: 0.620690  [  160/  179]
train() client id: f_00007-2-0 loss: 0.690559  [   32/  179]
train() client id: f_00007-2-1 loss: 0.849130  [   64/  179]
train() client id: f_00007-2-2 loss: 0.588136  [   96/  179]
train() client id: f_00007-2-3 loss: 0.707780  [  128/  179]
train() client id: f_00007-2-4 loss: 0.666546  [  160/  179]
train() client id: f_00007-3-0 loss: 0.734703  [   32/  179]
train() client id: f_00007-3-1 loss: 0.760668  [   64/  179]
train() client id: f_00007-3-2 loss: 0.787217  [   96/  179]
train() client id: f_00007-3-3 loss: 0.701671  [  128/  179]
train() client id: f_00007-3-4 loss: 0.615232  [  160/  179]
train() client id: f_00007-4-0 loss: 0.631723  [   32/  179]
train() client id: f_00007-4-1 loss: 0.665228  [   64/  179]
train() client id: f_00007-4-2 loss: 0.749478  [   96/  179]
train() client id: f_00007-4-3 loss: 0.817649  [  128/  179]
train() client id: f_00007-4-4 loss: 0.731269  [  160/  179]
train() client id: f_00007-5-0 loss: 0.659701  [   32/  179]
train() client id: f_00007-5-1 loss: 0.798646  [   64/  179]
train() client id: f_00007-5-2 loss: 0.781975  [   96/  179]
train() client id: f_00007-5-3 loss: 0.674112  [  128/  179]
train() client id: f_00007-5-4 loss: 0.666563  [  160/  179]
train() client id: f_00007-6-0 loss: 0.713531  [   32/  179]
train() client id: f_00007-6-1 loss: 0.563751  [   64/  179]
train() client id: f_00007-6-2 loss: 0.778384  [   96/  179]
train() client id: f_00007-6-3 loss: 0.740253  [  128/  179]
train() client id: f_00007-6-4 loss: 0.749344  [  160/  179]
train() client id: f_00007-7-0 loss: 0.948318  [   32/  179]
train() client id: f_00007-7-1 loss: 0.652694  [   64/  179]
train() client id: f_00007-7-2 loss: 0.665460  [   96/  179]
train() client id: f_00007-7-3 loss: 0.533107  [  128/  179]
train() client id: f_00007-7-4 loss: 0.683342  [  160/  179]
train() client id: f_00007-8-0 loss: 0.761715  [   32/  179]
train() client id: f_00007-8-1 loss: 0.583902  [   64/  179]
train() client id: f_00007-8-2 loss: 0.753505  [   96/  179]
train() client id: f_00007-8-3 loss: 0.633120  [  128/  179]
train() client id: f_00007-8-4 loss: 0.722030  [  160/  179]
train() client id: f_00007-9-0 loss: 0.704516  [   32/  179]
train() client id: f_00007-9-1 loss: 0.581249  [   64/  179]
train() client id: f_00007-9-2 loss: 0.839615  [   96/  179]
train() client id: f_00007-9-3 loss: 0.838114  [  128/  179]
train() client id: f_00007-9-4 loss: 0.556526  [  160/  179]
train() client id: f_00007-10-0 loss: 0.578543  [   32/  179]
train() client id: f_00007-10-1 loss: 0.782298  [   64/  179]
train() client id: f_00007-10-2 loss: 0.657881  [   96/  179]
train() client id: f_00007-10-3 loss: 0.723453  [  128/  179]
train() client id: f_00007-10-4 loss: 0.637341  [  160/  179]
train() client id: f_00007-11-0 loss: 0.552230  [   32/  179]
train() client id: f_00007-11-1 loss: 0.726268  [   64/  179]
train() client id: f_00007-11-2 loss: 0.708166  [   96/  179]
train() client id: f_00007-11-3 loss: 0.911339  [  128/  179]
train() client id: f_00007-11-4 loss: 0.573911  [  160/  179]
train() client id: f_00008-0-0 loss: 0.690765  [   32/  130]
train() client id: f_00008-0-1 loss: 0.680997  [   64/  130]
train() client id: f_00008-0-2 loss: 0.631718  [   96/  130]
train() client id: f_00008-0-3 loss: 0.702617  [  128/  130]
train() client id: f_00008-1-0 loss: 0.673171  [   32/  130]
train() client id: f_00008-1-1 loss: 0.734877  [   64/  130]
train() client id: f_00008-1-2 loss: 0.654000  [   96/  130]
train() client id: f_00008-1-3 loss: 0.601360  [  128/  130]
train() client id: f_00008-2-0 loss: 0.662830  [   32/  130]
train() client id: f_00008-2-1 loss: 0.678697  [   64/  130]
train() client id: f_00008-2-2 loss: 0.674460  [   96/  130]
train() client id: f_00008-2-3 loss: 0.644609  [  128/  130]
train() client id: f_00008-3-0 loss: 0.659504  [   32/  130]
train() client id: f_00008-3-1 loss: 0.545435  [   64/  130]
train() client id: f_00008-3-2 loss: 0.743788  [   96/  130]
train() client id: f_00008-3-3 loss: 0.653691  [  128/  130]
train() client id: f_00008-4-0 loss: 0.721507  [   32/  130]
train() client id: f_00008-4-1 loss: 0.570534  [   64/  130]
train() client id: f_00008-4-2 loss: 0.671390  [   96/  130]
train() client id: f_00008-4-3 loss: 0.704401  [  128/  130]
train() client id: f_00008-5-0 loss: 0.681523  [   32/  130]
train() client id: f_00008-5-1 loss: 0.777811  [   64/  130]
train() client id: f_00008-5-2 loss: 0.614626  [   96/  130]
train() client id: f_00008-5-3 loss: 0.612798  [  128/  130]
train() client id: f_00008-6-0 loss: 0.651244  [   32/  130]
train() client id: f_00008-6-1 loss: 0.656833  [   64/  130]
train() client id: f_00008-6-2 loss: 0.763245  [   96/  130]
train() client id: f_00008-6-3 loss: 0.588257  [  128/  130]
train() client id: f_00008-7-0 loss: 0.753763  [   32/  130]
train() client id: f_00008-7-1 loss: 0.489222  [   64/  130]
train() client id: f_00008-7-2 loss: 0.702527  [   96/  130]
train() client id: f_00008-7-3 loss: 0.729181  [  128/  130]
train() client id: f_00008-8-0 loss: 0.664719  [   32/  130]
train() client id: f_00008-8-1 loss: 0.836113  [   64/  130]
train() client id: f_00008-8-2 loss: 0.606814  [   96/  130]
train() client id: f_00008-8-3 loss: 0.566379  [  128/  130]
train() client id: f_00008-9-0 loss: 0.781535  [   32/  130]
train() client id: f_00008-9-1 loss: 0.615259  [   64/  130]
train() client id: f_00008-9-2 loss: 0.611959  [   96/  130]
train() client id: f_00008-9-3 loss: 0.662425  [  128/  130]
train() client id: f_00008-10-0 loss: 0.723268  [   32/  130]
train() client id: f_00008-10-1 loss: 0.647435  [   64/  130]
train() client id: f_00008-10-2 loss: 0.610843  [   96/  130]
train() client id: f_00008-10-3 loss: 0.679364  [  128/  130]
train() client id: f_00008-11-0 loss: 0.691902  [   32/  130]
train() client id: f_00008-11-1 loss: 0.630556  [   64/  130]
train() client id: f_00008-11-2 loss: 0.654675  [   96/  130]
train() client id: f_00008-11-3 loss: 0.677460  [  128/  130]
train() client id: f_00009-0-0 loss: 0.894885  [   32/  118]
train() client id: f_00009-0-1 loss: 1.085447  [   64/  118]
train() client id: f_00009-0-2 loss: 1.076858  [   96/  118]
train() client id: f_00009-1-0 loss: 1.032660  [   32/  118]
train() client id: f_00009-1-1 loss: 0.953530  [   64/  118]
train() client id: f_00009-1-2 loss: 0.994778  [   96/  118]
train() client id: f_00009-2-0 loss: 1.005786  [   32/  118]
train() client id: f_00009-2-1 loss: 0.943894  [   64/  118]
train() client id: f_00009-2-2 loss: 0.928482  [   96/  118]
train() client id: f_00009-3-0 loss: 0.799339  [   32/  118]
train() client id: f_00009-3-1 loss: 0.888939  [   64/  118]
train() client id: f_00009-3-2 loss: 0.993662  [   96/  118]
train() client id: f_00009-4-0 loss: 0.848254  [   32/  118]
train() client id: f_00009-4-1 loss: 0.926685  [   64/  118]
train() client id: f_00009-4-2 loss: 0.923841  [   96/  118]
train() client id: f_00009-5-0 loss: 0.883680  [   32/  118]
train() client id: f_00009-5-1 loss: 0.884483  [   64/  118]
train() client id: f_00009-5-2 loss: 0.764900  [   96/  118]
train() client id: f_00009-6-0 loss: 0.841193  [   32/  118]
train() client id: f_00009-6-1 loss: 0.794026  [   64/  118]
train() client id: f_00009-6-2 loss: 0.885776  [   96/  118]
train() client id: f_00009-7-0 loss: 0.767854  [   32/  118]
train() client id: f_00009-7-1 loss: 0.829311  [   64/  118]
train() client id: f_00009-7-2 loss: 0.820634  [   96/  118]
train() client id: f_00009-8-0 loss: 0.782634  [   32/  118]
train() client id: f_00009-8-1 loss: 0.759359  [   64/  118]
train() client id: f_00009-8-2 loss: 0.808834  [   96/  118]
train() client id: f_00009-9-0 loss: 0.769215  [   32/  118]
train() client id: f_00009-9-1 loss: 0.771685  [   64/  118]
train() client id: f_00009-9-2 loss: 0.832140  [   96/  118]
train() client id: f_00009-10-0 loss: 0.875567  [   32/  118]
train() client id: f_00009-10-1 loss: 0.834128  [   64/  118]
train() client id: f_00009-10-2 loss: 0.741262  [   96/  118]
train() client id: f_00009-11-0 loss: 0.771473  [   32/  118]
train() client id: f_00009-11-1 loss: 0.730134  [   64/  118]
train() client id: f_00009-11-2 loss: 0.882714  [   96/  118]
At round 17 accuracy: 0.6339522546419099
At round 17 training accuracy: 0.5788061703554661
At round 17 training loss: 0.8419239675558284
update_location
xs = [ -3.9056584    4.20031788 105.00902392  18.81129433   0.97929623
   3.95640986 -67.44319194 -46.32485185  89.66397685 -32.06087855]
ys = [ 97.5879595   80.55583871   1.32061395 -67.45517586  59.35018685
  42.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [139.78077123 128.47912601 145.01254816 122.08220814 116.29017026
 108.85175553 120.64607197 110.21192383 135.45589215 105.09001756]
dists_bs = [187.79216635 202.56468974 329.49898727 310.39866319 210.57918222
 222.48257319 207.65139541 216.55287625 307.94729826 222.87235287]
uav_gains = [4.32717486e-11 5.34395670e-11 3.94635132e-11 6.07214029e-11
 6.85691473e-11 8.08920872e-11 6.25451947e-11 7.84191512e-11
 4.68155255e-11 8.83269213e-11]
bs_gains = [4.75332122e-11 3.84517052e-11 9.84697079e-12 1.16390766e-11
 3.44929920e-11 2.95709508e-11 3.58720746e-11 3.18944419e-11
 1.19003619e-11 2.94263727e-11]
Round 18
-------------------------------
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.61607623 17.94328127  8.48520447  3.03808191 20.69729735  9.97190604
  3.77497723 12.15244223  8.94539714  8.09409538]
obj_prev = 101.71875926654823
eta_min = 1.6526197130074664e-11	eta_max = 0.921782499053972
af = 21.498465320302763	bf = 1.69748013632556	zeta = 23.64831185233304	eta = 0.9090909090909091
af = 21.498465320302763	bf = 1.69748013632556	zeta = 41.211530046727255	eta = 0.5216614208675814
af = 21.498465320302763	bf = 1.69748013632556	zeta = 32.795424289084025	eta = 0.6555324648584754
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.284347894451706	eta = 0.6871955711793989
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.2090722305542	eta = 0.6888530732821788
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.208871892116978	eta = 0.6888574952218328
eta = 0.6888574952218328
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.03075603 0.06468531 0.03026784 0.01049611 0.07469322 0.03563794
 0.01318116 0.04369309 0.03173241 0.02880327]
ene_total = [2.70140281 5.11569384 2.67726863 1.23269158 5.83681499 3.09264715
 1.41984069 3.55816331 2.96469034 2.60965854]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 0 obj = 5.492600646974247
eta = 0.6888574952218328
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
eta_min = 0.6888574952218411	eta_max = 0.6888574952218237
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 0.033943701105718926	eta = 0.909090909090909
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 18.704633302137236	eta = 0.001649746862055949
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8983226351773008	eta = 0.016255355925430464
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547436567121627	eta = 0.016637291080325793
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547352592532886	eta = 0.016637366406961725
eta = 0.016637366406961725
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.75913676e-04 1.65510932e-03 1.69142568e-04 6.79053635e-06
 2.57637780e-03 2.84503340e-04 1.34170925e-05 4.80434863e-04
 2.28976553e-04 1.50283948e-04]
ene_total = [0.17007518 0.20743924 0.17321761 0.15518366 0.23216334 0.18699632
 0.15443092 0.1583265  0.23270867 0.18419382]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 1 obj = 5.492600646974393
eta = 0.6888574952218411
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
Done!
At round 18 energy consumption: 5.492600646974247
At round 18 eta: 0.6888574952218411
At round 18 local rounds: 12.204774532627562
At round 18 global rounds: 70.7610733753795
At round 18 a_n: 21.674231763837348
gradient difference: 0.38596707582473755
train() client id: f_00000-0-0 loss: 1.765615  [   32/  126]
train() client id: f_00000-0-1 loss: 1.381699  [   64/  126]
train() client id: f_00000-0-2 loss: 1.165697  [   96/  126]
train() client id: f_00000-1-0 loss: 1.304942  [   32/  126]
train() client id: f_00000-1-1 loss: 1.300529  [   64/  126]
train() client id: f_00000-1-2 loss: 1.141159  [   96/  126]
train() client id: f_00000-2-0 loss: 1.324438  [   32/  126]
train() client id: f_00000-2-1 loss: 1.142953  [   64/  126]
train() client id: f_00000-2-2 loss: 1.231227  [   96/  126]
train() client id: f_00000-3-0 loss: 1.075746  [   32/  126]
train() client id: f_00000-3-1 loss: 1.031436  [   64/  126]
train() client id: f_00000-3-2 loss: 1.113892  [   96/  126]
train() client id: f_00000-4-0 loss: 0.954836  [   32/  126]
train() client id: f_00000-4-1 loss: 1.085288  [   64/  126]
train() client id: f_00000-4-2 loss: 1.012740  [   96/  126]
train() client id: f_00000-5-0 loss: 1.015027  [   32/  126]
train() client id: f_00000-5-1 loss: 1.022839  [   64/  126]
train() client id: f_00000-5-2 loss: 0.987543  [   96/  126]
train() client id: f_00000-6-0 loss: 1.053427  [   32/  126]
train() client id: f_00000-6-1 loss: 0.943801  [   64/  126]
train() client id: f_00000-6-2 loss: 0.943755  [   96/  126]
train() client id: f_00000-7-0 loss: 0.919765  [   32/  126]
train() client id: f_00000-7-1 loss: 0.865990  [   64/  126]
train() client id: f_00000-7-2 loss: 0.927080  [   96/  126]
train() client id: f_00000-8-0 loss: 0.915673  [   32/  126]
train() client id: f_00000-8-1 loss: 0.923609  [   64/  126]
train() client id: f_00000-8-2 loss: 0.880971  [   96/  126]
train() client id: f_00000-9-0 loss: 0.907037  [   32/  126]
train() client id: f_00000-9-1 loss: 0.851794  [   64/  126]
train() client id: f_00000-9-2 loss: 0.969364  [   96/  126]
train() client id: f_00000-10-0 loss: 0.870144  [   32/  126]
train() client id: f_00000-10-1 loss: 0.873147  [   64/  126]
train() client id: f_00000-10-2 loss: 0.925388  [   96/  126]
train() client id: f_00000-11-0 loss: 0.827523  [   32/  126]
train() client id: f_00000-11-1 loss: 0.775957  [   64/  126]
train() client id: f_00000-11-2 loss: 0.958744  [   96/  126]
train() client id: f_00001-0-0 loss: 0.414109  [   32/  265]
train() client id: f_00001-0-1 loss: 0.342279  [   64/  265]
train() client id: f_00001-0-2 loss: 0.523661  [   96/  265]
train() client id: f_00001-0-3 loss: 0.431574  [  128/  265]
train() client id: f_00001-0-4 loss: 0.356126  [  160/  265]
train() client id: f_00001-0-5 loss: 0.592770  [  192/  265]
train() client id: f_00001-0-6 loss: 0.401441  [  224/  265]
train() client id: f_00001-0-7 loss: 0.409495  [  256/  265]
train() client id: f_00001-1-0 loss: 0.489817  [   32/  265]
train() client id: f_00001-1-1 loss: 0.474789  [   64/  265]
train() client id: f_00001-1-2 loss: 0.321834  [   96/  265]
train() client id: f_00001-1-3 loss: 0.385710  [  128/  265]
train() client id: f_00001-1-4 loss: 0.374368  [  160/  265]
train() client id: f_00001-1-5 loss: 0.466873  [  192/  265]
train() client id: f_00001-1-6 loss: 0.475304  [  224/  265]
train() client id: f_00001-1-7 loss: 0.341419  [  256/  265]
train() client id: f_00001-2-0 loss: 0.533333  [   32/  265]
train() client id: f_00001-2-1 loss: 0.432097  [   64/  265]
train() client id: f_00001-2-2 loss: 0.397339  [   96/  265]
train() client id: f_00001-2-3 loss: 0.349582  [  128/  265]
train() client id: f_00001-2-4 loss: 0.359278  [  160/  265]
train() client id: f_00001-2-5 loss: 0.401048  [  192/  265]
train() client id: f_00001-2-6 loss: 0.328496  [  224/  265]
train() client id: f_00001-2-7 loss: 0.422165  [  256/  265]
train() client id: f_00001-3-0 loss: 0.439683  [   32/  265]
train() client id: f_00001-3-1 loss: 0.482553  [   64/  265]
train() client id: f_00001-3-2 loss: 0.301255  [   96/  265]
train() client id: f_00001-3-3 loss: 0.393974  [  128/  265]
train() client id: f_00001-3-4 loss: 0.440166  [  160/  265]
train() client id: f_00001-3-5 loss: 0.357661  [  192/  265]
train() client id: f_00001-3-6 loss: 0.426200  [  224/  265]
train() client id: f_00001-3-7 loss: 0.445356  [  256/  265]
train() client id: f_00001-4-0 loss: 0.386636  [   32/  265]
train() client id: f_00001-4-1 loss: 0.365660  [   64/  265]
train() client id: f_00001-4-2 loss: 0.484998  [   96/  265]
train() client id: f_00001-4-3 loss: 0.406687  [  128/  265]
train() client id: f_00001-4-4 loss: 0.374296  [  160/  265]
train() client id: f_00001-4-5 loss: 0.454450  [  192/  265]
train() client id: f_00001-4-6 loss: 0.382258  [  224/  265]
train() client id: f_00001-4-7 loss: 0.334322  [  256/  265]
train() client id: f_00001-5-0 loss: 0.500350  [   32/  265]
train() client id: f_00001-5-1 loss: 0.372139  [   64/  265]
train() client id: f_00001-5-2 loss: 0.330149  [   96/  265]
train() client id: f_00001-5-3 loss: 0.393688  [  128/  265]
train() client id: f_00001-5-4 loss: 0.358414  [  160/  265]
train() client id: f_00001-5-5 loss: 0.395930  [  192/  265]
train() client id: f_00001-5-6 loss: 0.371549  [  224/  265]
train() client id: f_00001-5-7 loss: 0.429684  [  256/  265]
train() client id: f_00001-6-0 loss: 0.336529  [   32/  265]
train() client id: f_00001-6-1 loss: 0.425369  [   64/  265]
train() client id: f_00001-6-2 loss: 0.453042  [   96/  265]
train() client id: f_00001-6-3 loss: 0.344882  [  128/  265]
train() client id: f_00001-6-4 loss: 0.391004  [  160/  265]
train() client id: f_00001-6-5 loss: 0.371876  [  192/  265]
train() client id: f_00001-6-6 loss: 0.482474  [  224/  265]
train() client id: f_00001-6-7 loss: 0.365908  [  256/  265]
train() client id: f_00001-7-0 loss: 0.329911  [   32/  265]
train() client id: f_00001-7-1 loss: 0.525676  [   64/  265]
train() client id: f_00001-7-2 loss: 0.374756  [   96/  265]
train() client id: f_00001-7-3 loss: 0.369414  [  128/  265]
train() client id: f_00001-7-4 loss: 0.366836  [  160/  265]
train() client id: f_00001-7-5 loss: 0.428543  [  192/  265]
train() client id: f_00001-7-6 loss: 0.309792  [  224/  265]
train() client id: f_00001-7-7 loss: 0.491637  [  256/  265]
train() client id: f_00001-8-0 loss: 0.414143  [   32/  265]
train() client id: f_00001-8-1 loss: 0.360025  [   64/  265]
train() client id: f_00001-8-2 loss: 0.416870  [   96/  265]
train() client id: f_00001-8-3 loss: 0.351317  [  128/  265]
train() client id: f_00001-8-4 loss: 0.444717  [  160/  265]
train() client id: f_00001-8-5 loss: 0.503272  [  192/  265]
train() client id: f_00001-8-6 loss: 0.416323  [  224/  265]
train() client id: f_00001-8-7 loss: 0.283571  [  256/  265]
train() client id: f_00001-9-0 loss: 0.354376  [   32/  265]
train() client id: f_00001-9-1 loss: 0.460572  [   64/  265]
train() client id: f_00001-9-2 loss: 0.437180  [   96/  265]
train() client id: f_00001-9-3 loss: 0.305502  [  128/  265]
train() client id: f_00001-9-4 loss: 0.446939  [  160/  265]
train() client id: f_00001-9-5 loss: 0.432687  [  192/  265]
train() client id: f_00001-9-6 loss: 0.336190  [  224/  265]
train() client id: f_00001-9-7 loss: 0.403953  [  256/  265]
train() client id: f_00001-10-0 loss: 0.408440  [   32/  265]
train() client id: f_00001-10-1 loss: 0.498515  [   64/  265]
train() client id: f_00001-10-2 loss: 0.287780  [   96/  265]
train() client id: f_00001-10-3 loss: 0.299263  [  128/  265]
train() client id: f_00001-10-4 loss: 0.305070  [  160/  265]
train() client id: f_00001-10-5 loss: 0.416276  [  192/  265]
train() client id: f_00001-10-6 loss: 0.460677  [  224/  265]
train() client id: f_00001-10-7 loss: 0.436180  [  256/  265]
train() client id: f_00001-11-0 loss: 0.366007  [   32/  265]
train() client id: f_00001-11-1 loss: 0.369022  [   64/  265]
train() client id: f_00001-11-2 loss: 0.493769  [   96/  265]
train() client id: f_00001-11-3 loss: 0.295378  [  128/  265]
train() client id: f_00001-11-4 loss: 0.374650  [  160/  265]
train() client id: f_00001-11-5 loss: 0.303125  [  192/  265]
train() client id: f_00001-11-6 loss: 0.431731  [  224/  265]
train() client id: f_00001-11-7 loss: 0.546474  [  256/  265]
train() client id: f_00002-0-0 loss: 1.341029  [   32/  124]
train() client id: f_00002-0-1 loss: 1.207245  [   64/  124]
train() client id: f_00002-0-2 loss: 1.078474  [   96/  124]
train() client id: f_00002-1-0 loss: 1.053497  [   32/  124]
train() client id: f_00002-1-1 loss: 1.061293  [   64/  124]
train() client id: f_00002-1-2 loss: 1.034994  [   96/  124]
train() client id: f_00002-2-0 loss: 1.056561  [   32/  124]
train() client id: f_00002-2-1 loss: 1.160720  [   64/  124]
train() client id: f_00002-2-2 loss: 1.062705  [   96/  124]
train() client id: f_00002-3-0 loss: 1.082846  [   32/  124]
train() client id: f_00002-3-1 loss: 0.984330  [   64/  124]
train() client id: f_00002-3-2 loss: 1.027623  [   96/  124]
train() client id: f_00002-4-0 loss: 0.948833  [   32/  124]
train() client id: f_00002-4-1 loss: 1.033898  [   64/  124]
train() client id: f_00002-4-2 loss: 0.989020  [   96/  124]
train() client id: f_00002-5-0 loss: 0.924134  [   32/  124]
train() client id: f_00002-5-1 loss: 1.007871  [   64/  124]
train() client id: f_00002-5-2 loss: 0.982923  [   96/  124]
train() client id: f_00002-6-0 loss: 0.936128  [   32/  124]
train() client id: f_00002-6-1 loss: 0.947018  [   64/  124]
train() client id: f_00002-6-2 loss: 1.053636  [   96/  124]
train() client id: f_00002-7-0 loss: 1.026160  [   32/  124]
train() client id: f_00002-7-1 loss: 1.001830  [   64/  124]
train() client id: f_00002-7-2 loss: 0.935828  [   96/  124]
train() client id: f_00002-8-0 loss: 0.931566  [   32/  124]
train() client id: f_00002-8-1 loss: 0.885636  [   64/  124]
train() client id: f_00002-8-2 loss: 0.961675  [   96/  124]
train() client id: f_00002-9-0 loss: 0.856427  [   32/  124]
train() client id: f_00002-9-1 loss: 1.088248  [   64/  124]
train() client id: f_00002-9-2 loss: 0.952681  [   96/  124]
train() client id: f_00002-10-0 loss: 0.987450  [   32/  124]
train() client id: f_00002-10-1 loss: 0.861423  [   64/  124]
train() client id: f_00002-10-2 loss: 0.932793  [   96/  124]
train() client id: f_00002-11-0 loss: 0.903165  [   32/  124]
train() client id: f_00002-11-1 loss: 0.982177  [   64/  124]
train() client id: f_00002-11-2 loss: 0.974678  [   96/  124]
train() client id: f_00003-0-0 loss: 0.846066  [   32/   43]
train() client id: f_00003-1-0 loss: 0.654692  [   32/   43]
train() client id: f_00003-2-0 loss: 0.622946  [   32/   43]
train() client id: f_00003-3-0 loss: 0.809169  [   32/   43]
train() client id: f_00003-4-0 loss: 0.796096  [   32/   43]
train() client id: f_00003-5-0 loss: 0.646542  [   32/   43]
train() client id: f_00003-6-0 loss: 0.778518  [   32/   43]
train() client id: f_00003-7-0 loss: 0.698416  [   32/   43]
train() client id: f_00003-8-0 loss: 0.610956  [   32/   43]
train() client id: f_00003-9-0 loss: 0.927969  [   32/   43]
train() client id: f_00003-10-0 loss: 0.727351  [   32/   43]
train() client id: f_00003-11-0 loss: 0.710001  [   32/   43]
train() client id: f_00004-0-0 loss: 0.973070  [   32/  306]
train() client id: f_00004-0-1 loss: 0.972666  [   64/  306]
train() client id: f_00004-0-2 loss: 0.685262  [   96/  306]
train() client id: f_00004-0-3 loss: 0.832890  [  128/  306]
train() client id: f_00004-0-4 loss: 0.920471  [  160/  306]
train() client id: f_00004-0-5 loss: 0.836125  [  192/  306]
train() client id: f_00004-0-6 loss: 0.898459  [  224/  306]
train() client id: f_00004-0-7 loss: 0.822198  [  256/  306]
train() client id: f_00004-0-8 loss: 0.904587  [  288/  306]
train() client id: f_00004-1-0 loss: 0.743551  [   32/  306]
train() client id: f_00004-1-1 loss: 0.914399  [   64/  306]
train() client id: f_00004-1-2 loss: 1.017159  [   96/  306]
train() client id: f_00004-1-3 loss: 0.899630  [  128/  306]
train() client id: f_00004-1-4 loss: 0.811856  [  160/  306]
train() client id: f_00004-1-5 loss: 0.891077  [  192/  306]
train() client id: f_00004-1-6 loss: 0.938159  [  224/  306]
train() client id: f_00004-1-7 loss: 0.804829  [  256/  306]
train() client id: f_00004-1-8 loss: 0.778326  [  288/  306]
train() client id: f_00004-2-0 loss: 0.824608  [   32/  306]
train() client id: f_00004-2-1 loss: 0.858796  [   64/  306]
train() client id: f_00004-2-2 loss: 0.831877  [   96/  306]
train() client id: f_00004-2-3 loss: 1.007130  [  128/  306]
train() client id: f_00004-2-4 loss: 0.815373  [  160/  306]
train() client id: f_00004-2-5 loss: 0.989483  [  192/  306]
train() client id: f_00004-2-6 loss: 0.708794  [  224/  306]
train() client id: f_00004-2-7 loss: 0.893597  [  256/  306]
train() client id: f_00004-2-8 loss: 0.927693  [  288/  306]
train() client id: f_00004-3-0 loss: 0.809113  [   32/  306]
train() client id: f_00004-3-1 loss: 0.836162  [   64/  306]
train() client id: f_00004-3-2 loss: 0.836933  [   96/  306]
train() client id: f_00004-3-3 loss: 1.024122  [  128/  306]
train() client id: f_00004-3-4 loss: 0.818089  [  160/  306]
train() client id: f_00004-3-5 loss: 0.829268  [  192/  306]
train() client id: f_00004-3-6 loss: 0.933597  [  224/  306]
train() client id: f_00004-3-7 loss: 0.959077  [  256/  306]
train() client id: f_00004-3-8 loss: 0.784666  [  288/  306]
train() client id: f_00004-4-0 loss: 0.856846  [   32/  306]
train() client id: f_00004-4-1 loss: 0.921834  [   64/  306]
train() client id: f_00004-4-2 loss: 0.839674  [   96/  306]
train() client id: f_00004-4-3 loss: 0.802593  [  128/  306]
train() client id: f_00004-4-4 loss: 0.991110  [  160/  306]
train() client id: f_00004-4-5 loss: 0.910183  [  192/  306]
train() client id: f_00004-4-6 loss: 0.766359  [  224/  306]
train() client id: f_00004-4-7 loss: 0.819464  [  256/  306]
train() client id: f_00004-4-8 loss: 0.930046  [  288/  306]
train() client id: f_00004-5-0 loss: 0.906692  [   32/  306]
train() client id: f_00004-5-1 loss: 0.819239  [   64/  306]
train() client id: f_00004-5-2 loss: 0.992693  [   96/  306]
train() client id: f_00004-5-3 loss: 0.716891  [  128/  306]
train() client id: f_00004-5-4 loss: 0.902816  [  160/  306]
train() client id: f_00004-5-5 loss: 0.964510  [  192/  306]
train() client id: f_00004-5-6 loss: 0.774623  [  224/  306]
train() client id: f_00004-5-7 loss: 0.963035  [  256/  306]
train() client id: f_00004-5-8 loss: 0.772645  [  288/  306]
train() client id: f_00004-6-0 loss: 0.805647  [   32/  306]
train() client id: f_00004-6-1 loss: 0.830052  [   64/  306]
train() client id: f_00004-6-2 loss: 0.912565  [   96/  306]
train() client id: f_00004-6-3 loss: 0.901874  [  128/  306]
train() client id: f_00004-6-4 loss: 0.916439  [  160/  306]
train() client id: f_00004-6-5 loss: 0.862273  [  192/  306]
train() client id: f_00004-6-6 loss: 0.991825  [  224/  306]
train() client id: f_00004-6-7 loss: 0.853406  [  256/  306]
train() client id: f_00004-6-8 loss: 0.843054  [  288/  306]
train() client id: f_00004-7-0 loss: 0.981432  [   32/  306]
train() client id: f_00004-7-1 loss: 0.795595  [   64/  306]
train() client id: f_00004-7-2 loss: 0.872222  [   96/  306]
train() client id: f_00004-7-3 loss: 0.836058  [  128/  306]
train() client id: f_00004-7-4 loss: 0.787589  [  160/  306]
train() client id: f_00004-7-5 loss: 0.842460  [  192/  306]
train() client id: f_00004-7-6 loss: 0.853581  [  224/  306]
train() client id: f_00004-7-7 loss: 0.981772  [  256/  306]
train() client id: f_00004-7-8 loss: 0.928639  [  288/  306]
train() client id: f_00004-8-0 loss: 0.859539  [   32/  306]
train() client id: f_00004-8-1 loss: 0.748470  [   64/  306]
train() client id: f_00004-8-2 loss: 0.847513  [   96/  306]
train() client id: f_00004-8-3 loss: 0.835106  [  128/  306]
train() client id: f_00004-8-4 loss: 0.865710  [  160/  306]
train() client id: f_00004-8-5 loss: 0.893780  [  192/  306]
train() client id: f_00004-8-6 loss: 0.803991  [  224/  306]
train() client id: f_00004-8-7 loss: 1.010653  [  256/  306]
train() client id: f_00004-8-8 loss: 0.950814  [  288/  306]
train() client id: f_00004-9-0 loss: 0.924267  [   32/  306]
train() client id: f_00004-9-1 loss: 0.830825  [   64/  306]
train() client id: f_00004-9-2 loss: 0.800278  [   96/  306]
train() client id: f_00004-9-3 loss: 0.900131  [  128/  306]
train() client id: f_00004-9-4 loss: 0.887770  [  160/  306]
train() client id: f_00004-9-5 loss: 0.891045  [  192/  306]
train() client id: f_00004-9-6 loss: 1.011606  [  224/  306]
train() client id: f_00004-9-7 loss: 0.817939  [  256/  306]
train() client id: f_00004-9-8 loss: 0.829743  [  288/  306]
train() client id: f_00004-10-0 loss: 0.867674  [   32/  306]
train() client id: f_00004-10-1 loss: 0.796839  [   64/  306]
train() client id: f_00004-10-2 loss: 0.901413  [   96/  306]
train() client id: f_00004-10-3 loss: 0.898042  [  128/  306]
train() client id: f_00004-10-4 loss: 0.861378  [  160/  306]
train() client id: f_00004-10-5 loss: 0.910448  [  192/  306]
train() client id: f_00004-10-6 loss: 0.875586  [  224/  306]
train() client id: f_00004-10-7 loss: 0.867606  [  256/  306]
train() client id: f_00004-10-8 loss: 0.845891  [  288/  306]
train() client id: f_00004-11-0 loss: 0.956655  [   32/  306]
train() client id: f_00004-11-1 loss: 0.991491  [   64/  306]
train() client id: f_00004-11-2 loss: 0.777548  [   96/  306]
train() client id: f_00004-11-3 loss: 0.932742  [  128/  306]
train() client id: f_00004-11-4 loss: 0.920492  [  160/  306]
train() client id: f_00004-11-5 loss: 0.781765  [  192/  306]
train() client id: f_00004-11-6 loss: 0.781197  [  224/  306]
train() client id: f_00004-11-7 loss: 0.876210  [  256/  306]
train() client id: f_00004-11-8 loss: 0.871871  [  288/  306]
train() client id: f_00005-0-0 loss: 0.819294  [   32/  146]
train() client id: f_00005-0-1 loss: 0.940658  [   64/  146]
train() client id: f_00005-0-2 loss: 0.908026  [   96/  146]
train() client id: f_00005-0-3 loss: 0.653958  [  128/  146]
train() client id: f_00005-1-0 loss: 0.964458  [   32/  146]
train() client id: f_00005-1-1 loss: 0.698394  [   64/  146]
train() client id: f_00005-1-2 loss: 0.860254  [   96/  146]
train() client id: f_00005-1-3 loss: 0.910465  [  128/  146]
train() client id: f_00005-2-0 loss: 0.719431  [   32/  146]
train() client id: f_00005-2-1 loss: 0.959544  [   64/  146]
train() client id: f_00005-2-2 loss: 0.869911  [   96/  146]
train() client id: f_00005-2-3 loss: 0.808620  [  128/  146]
train() client id: f_00005-3-0 loss: 0.976408  [   32/  146]
train() client id: f_00005-3-1 loss: 0.879867  [   64/  146]
train() client id: f_00005-3-2 loss: 0.664769  [   96/  146]
train() client id: f_00005-3-3 loss: 0.706476  [  128/  146]
train() client id: f_00005-4-0 loss: 0.698599  [   32/  146]
train() client id: f_00005-4-1 loss: 0.774767  [   64/  146]
train() client id: f_00005-4-2 loss: 0.916073  [   96/  146]
train() client id: f_00005-4-3 loss: 0.814000  [  128/  146]
train() client id: f_00005-5-0 loss: 0.713505  [   32/  146]
train() client id: f_00005-5-1 loss: 0.909405  [   64/  146]
train() client id: f_00005-5-2 loss: 0.595309  [   96/  146]
train() client id: f_00005-5-3 loss: 1.118740  [  128/  146]
train() client id: f_00005-6-0 loss: 0.681061  [   32/  146]
train() client id: f_00005-6-1 loss: 0.800192  [   64/  146]
train() client id: f_00005-6-2 loss: 0.809723  [   96/  146]
train() client id: f_00005-6-3 loss: 0.953353  [  128/  146]
train() client id: f_00005-7-0 loss: 1.011715  [   32/  146]
train() client id: f_00005-7-1 loss: 0.876810  [   64/  146]
train() client id: f_00005-7-2 loss: 0.665691  [   96/  146]
train() client id: f_00005-7-3 loss: 0.552694  [  128/  146]
train() client id: f_00005-8-0 loss: 0.820817  [   32/  146]
train() client id: f_00005-8-1 loss: 0.736525  [   64/  146]
train() client id: f_00005-8-2 loss: 1.023232  [   96/  146]
train() client id: f_00005-8-3 loss: 0.787171  [  128/  146]
train() client id: f_00005-9-0 loss: 0.973396  [   32/  146]
train() client id: f_00005-9-1 loss: 0.804426  [   64/  146]
train() client id: f_00005-9-2 loss: 0.711337  [   96/  146]
train() client id: f_00005-9-3 loss: 0.861950  [  128/  146]
train() client id: f_00005-10-0 loss: 0.823871  [   32/  146]
train() client id: f_00005-10-1 loss: 0.842164  [   64/  146]
train() client id: f_00005-10-2 loss: 0.742981  [   96/  146]
train() client id: f_00005-10-3 loss: 0.907593  [  128/  146]
train() client id: f_00005-11-0 loss: 0.847008  [   32/  146]
train() client id: f_00005-11-1 loss: 0.897433  [   64/  146]
train() client id: f_00005-11-2 loss: 0.687647  [   96/  146]
train() client id: f_00005-11-3 loss: 0.931622  [  128/  146]
train() client id: f_00006-0-0 loss: 0.540020  [   32/   54]
train() client id: f_00006-1-0 loss: 0.635687  [   32/   54]
train() client id: f_00006-2-0 loss: 0.627503  [   32/   54]
train() client id: f_00006-3-0 loss: 0.590753  [   32/   54]
train() client id: f_00006-4-0 loss: 0.624355  [   32/   54]
train() client id: f_00006-5-0 loss: 0.577869  [   32/   54]
train() client id: f_00006-6-0 loss: 0.541771  [   32/   54]
train() client id: f_00006-7-0 loss: 0.596815  [   32/   54]
train() client id: f_00006-8-0 loss: 0.530869  [   32/   54]
train() client id: f_00006-9-0 loss: 0.589366  [   32/   54]
train() client id: f_00006-10-0 loss: 0.583575  [   32/   54]
train() client id: f_00006-11-0 loss: 0.647563  [   32/   54]
train() client id: f_00007-0-0 loss: 0.657073  [   32/  179]
train() client id: f_00007-0-1 loss: 0.709944  [   64/  179]
train() client id: f_00007-0-2 loss: 0.842405  [   96/  179]
train() client id: f_00007-0-3 loss: 0.693070  [  128/  179]
train() client id: f_00007-0-4 loss: 0.849139  [  160/  179]
train() client id: f_00007-1-0 loss: 0.697025  [   32/  179]
train() client id: f_00007-1-1 loss: 0.715838  [   64/  179]
train() client id: f_00007-1-2 loss: 0.716969  [   96/  179]
train() client id: f_00007-1-3 loss: 0.851481  [  128/  179]
train() client id: f_00007-1-4 loss: 0.637144  [  160/  179]
train() client id: f_00007-2-0 loss: 0.635137  [   32/  179]
train() client id: f_00007-2-1 loss: 0.753544  [   64/  179]
train() client id: f_00007-2-2 loss: 0.837065  [   96/  179]
train() client id: f_00007-2-3 loss: 0.593291  [  128/  179]
train() client id: f_00007-2-4 loss: 0.696652  [  160/  179]
train() client id: f_00007-3-0 loss: 0.746483  [   32/  179]
train() client id: f_00007-3-1 loss: 0.699650  [   64/  179]
train() client id: f_00007-3-2 loss: 0.736041  [   96/  179]
train() client id: f_00007-3-3 loss: 0.640210  [  128/  179]
train() client id: f_00007-3-4 loss: 0.777999  [  160/  179]
train() client id: f_00007-4-0 loss: 0.584138  [   32/  179]
train() client id: f_00007-4-1 loss: 0.662147  [   64/  179]
train() client id: f_00007-4-2 loss: 0.645132  [   96/  179]
train() client id: f_00007-4-3 loss: 0.910587  [  128/  179]
train() client id: f_00007-4-4 loss: 0.790045  [  160/  179]
train() client id: f_00007-5-0 loss: 0.720744  [   32/  179]
train() client id: f_00007-5-1 loss: 0.616109  [   64/  179]
train() client id: f_00007-5-2 loss: 0.564363  [   96/  179]
train() client id: f_00007-5-3 loss: 0.838122  [  128/  179]
train() client id: f_00007-5-4 loss: 0.755454  [  160/  179]
train() client id: f_00007-6-0 loss: 0.640855  [   32/  179]
train() client id: f_00007-6-1 loss: 0.775451  [   64/  179]
train() client id: f_00007-6-2 loss: 0.655669  [   96/  179]
train() client id: f_00007-6-3 loss: 0.692185  [  128/  179]
train() client id: f_00007-6-4 loss: 0.591091  [  160/  179]
train() client id: f_00007-7-0 loss: 0.854468  [   32/  179]
train() client id: f_00007-7-1 loss: 0.771077  [   64/  179]
train() client id: f_00007-7-2 loss: 0.680761  [   96/  179]
train() client id: f_00007-7-3 loss: 0.545945  [  128/  179]
train() client id: f_00007-7-4 loss: 0.701135  [  160/  179]
train() client id: f_00007-8-0 loss: 0.638467  [   32/  179]
train() client id: f_00007-8-1 loss: 0.679901  [   64/  179]
train() client id: f_00007-8-2 loss: 0.646090  [   96/  179]
train() client id: f_00007-8-3 loss: 0.696753  [  128/  179]
train() client id: f_00007-8-4 loss: 0.736549  [  160/  179]
train() client id: f_00007-9-0 loss: 0.732366  [   32/  179]
train() client id: f_00007-9-1 loss: 0.578436  [   64/  179]
train() client id: f_00007-9-2 loss: 0.936869  [   96/  179]
train() client id: f_00007-9-3 loss: 0.654543  [  128/  179]
train() client id: f_00007-9-4 loss: 0.588329  [  160/  179]
train() client id: f_00007-10-0 loss: 0.777616  [   32/  179]
train() client id: f_00007-10-1 loss: 0.623509  [   64/  179]
train() client id: f_00007-10-2 loss: 0.860913  [   96/  179]
train() client id: f_00007-10-3 loss: 0.569508  [  128/  179]
train() client id: f_00007-10-4 loss: 0.561633  [  160/  179]
train() client id: f_00007-11-0 loss: 0.581270  [   32/  179]
train() client id: f_00007-11-1 loss: 0.565447  [   64/  179]
train() client id: f_00007-11-2 loss: 0.915396  [   96/  179]
train() client id: f_00007-11-3 loss: 0.609907  [  128/  179]
train() client id: f_00007-11-4 loss: 0.790758  [  160/  179]
train() client id: f_00008-0-0 loss: 0.843526  [   32/  130]
train() client id: f_00008-0-1 loss: 0.774221  [   64/  130]
train() client id: f_00008-0-2 loss: 0.659816  [   96/  130]
train() client id: f_00008-0-3 loss: 0.616953  [  128/  130]
train() client id: f_00008-1-0 loss: 0.705258  [   32/  130]
train() client id: f_00008-1-1 loss: 0.695546  [   64/  130]
train() client id: f_00008-1-2 loss: 0.754505  [   96/  130]
train() client id: f_00008-1-3 loss: 0.761502  [  128/  130]
train() client id: f_00008-2-0 loss: 0.685505  [   32/  130]
train() client id: f_00008-2-1 loss: 0.705420  [   64/  130]
train() client id: f_00008-2-2 loss: 0.847340  [   96/  130]
train() client id: f_00008-2-3 loss: 0.641074  [  128/  130]
train() client id: f_00008-3-0 loss: 0.725758  [   32/  130]
train() client id: f_00008-3-1 loss: 0.749617  [   64/  130]
train() client id: f_00008-3-2 loss: 0.742401  [   96/  130]
train() client id: f_00008-3-3 loss: 0.664828  [  128/  130]
train() client id: f_00008-4-0 loss: 0.726335  [   32/  130]
train() client id: f_00008-4-1 loss: 0.717673  [   64/  130]
train() client id: f_00008-4-2 loss: 0.798115  [   96/  130]
train() client id: f_00008-4-3 loss: 0.664446  [  128/  130]
train() client id: f_00008-5-0 loss: 0.723847  [   32/  130]
train() client id: f_00008-5-1 loss: 0.751392  [   64/  130]
train() client id: f_00008-5-2 loss: 0.635499  [   96/  130]
train() client id: f_00008-5-3 loss: 0.804154  [  128/  130]
train() client id: f_00008-6-0 loss: 0.624452  [   32/  130]
train() client id: f_00008-6-1 loss: 0.731507  [   64/  130]
train() client id: f_00008-6-2 loss: 0.736627  [   96/  130]
train() client id: f_00008-6-3 loss: 0.785852  [  128/  130]
train() client id: f_00008-7-0 loss: 0.672330  [   32/  130]
train() client id: f_00008-7-1 loss: 0.771102  [   64/  130]
train() client id: f_00008-7-2 loss: 0.631603  [   96/  130]
train() client id: f_00008-7-3 loss: 0.834745  [  128/  130]
train() client id: f_00008-8-0 loss: 0.664633  [   32/  130]
train() client id: f_00008-8-1 loss: 0.603703  [   64/  130]
train() client id: f_00008-8-2 loss: 0.681812  [   96/  130]
train() client id: f_00008-8-3 loss: 0.947842  [  128/  130]
train() client id: f_00008-9-0 loss: 0.799951  [   32/  130]
train() client id: f_00008-9-1 loss: 0.711980  [   64/  130]
train() client id: f_00008-9-2 loss: 0.656241  [   96/  130]
train() client id: f_00008-9-3 loss: 0.736493  [  128/  130]
train() client id: f_00008-10-0 loss: 0.779842  [   32/  130]
train() client id: f_00008-10-1 loss: 0.722154  [   64/  130]
train() client id: f_00008-10-2 loss: 0.707992  [   96/  130]
train() client id: f_00008-10-3 loss: 0.697880  [  128/  130]
train() client id: f_00008-11-0 loss: 0.699704  [   32/  130]
train() client id: f_00008-11-1 loss: 0.662021  [   64/  130]
train() client id: f_00008-11-2 loss: 0.718158  [   96/  130]
train() client id: f_00008-11-3 loss: 0.767161  [  128/  130]
train() client id: f_00009-0-0 loss: 1.159412  [   32/  118]
train() client id: f_00009-0-1 loss: 1.071818  [   64/  118]
train() client id: f_00009-0-2 loss: 1.214603  [   96/  118]
train() client id: f_00009-1-0 loss: 1.106100  [   32/  118]
train() client id: f_00009-1-1 loss: 0.985176  [   64/  118]
train() client id: f_00009-1-2 loss: 1.120749  [   96/  118]
train() client id: f_00009-2-0 loss: 1.036182  [   32/  118]
train() client id: f_00009-2-1 loss: 0.946919  [   64/  118]
train() client id: f_00009-2-2 loss: 1.064603  [   96/  118]
train() client id: f_00009-3-0 loss: 0.989829  [   32/  118]
train() client id: f_00009-3-1 loss: 0.991667  [   64/  118]
train() client id: f_00009-3-2 loss: 1.002310  [   96/  118]
train() client id: f_00009-4-0 loss: 0.941784  [   32/  118]
train() client id: f_00009-4-1 loss: 1.048089  [   64/  118]
train() client id: f_00009-4-2 loss: 0.879058  [   96/  118]
train() client id: f_00009-5-0 loss: 0.849054  [   32/  118]
train() client id: f_00009-5-1 loss: 1.016941  [   64/  118]
train() client id: f_00009-5-2 loss: 0.881082  [   96/  118]
train() client id: f_00009-6-0 loss: 0.975901  [   32/  118]
train() client id: f_00009-6-1 loss: 0.859557  [   64/  118]
train() client id: f_00009-6-2 loss: 0.952775  [   96/  118]
train() client id: f_00009-7-0 loss: 0.911511  [   32/  118]
train() client id: f_00009-7-1 loss: 0.755028  [   64/  118]
train() client id: f_00009-7-2 loss: 0.945394  [   96/  118]
train() client id: f_00009-8-0 loss: 0.878286  [   32/  118]
train() client id: f_00009-8-1 loss: 0.866822  [   64/  118]
train() client id: f_00009-8-2 loss: 0.872279  [   96/  118]
train() client id: f_00009-9-0 loss: 0.864645  [   32/  118]
train() client id: f_00009-9-1 loss: 0.778432  [   64/  118]
train() client id: f_00009-9-2 loss: 0.882724  [   96/  118]
train() client id: f_00009-10-0 loss: 0.912221  [   32/  118]
train() client id: f_00009-10-1 loss: 0.781569  [   64/  118]
train() client id: f_00009-10-2 loss: 0.741041  [   96/  118]
train() client id: f_00009-11-0 loss: 0.914946  [   32/  118]
train() client id: f_00009-11-1 loss: 0.783691  [   64/  118]
train() client id: f_00009-11-2 loss: 0.834957  [   96/  118]
At round 18 accuracy: 0.6339522546419099
At round 18 training accuracy: 0.5767940979208585
At round 18 training loss: 0.8450759092896757
update_location
xs = [ -3.9056584    4.20031788 110.00902392  18.81129433   0.97929623
   3.95640986 -72.44319194 -51.32485185  94.66397685 -37.06087855]
ys = [102.5879595   85.55583871   1.32061395 -72.45517586  64.35018685
  47.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [143.31623635 131.67172896 148.67323015 124.91443993 118.91974423
 110.91368802 123.510755   112.40514523 138.81620397 106.72169684]
dists_bs = [185.78691379 200.28233051 333.75840492 314.31971282 207.87518816
 219.54871187 205.10615284 213.61974799 312.25498919 219.69909981]
uav_gains = [4.06456051e-11 5.02562998e-11 3.70692179e-11 5.73364491e-11
 6.48404832e-11 7.71845050e-11 5.89801623e-11 7.46493522e-11
 4.40289659e-11 8.49892801e-11]
bs_gains = [4.89837182e-11 3.96912434e-11 9.49913137e-12 1.12370821e-11
 3.57640465e-11 3.06907558e-11 3.71324630e-11 3.31358541e-11
 1.14463706e-11 3.06319686e-11]
Round 19
-------------------------------
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.48419946 17.66297686  8.35544877  2.99255128 20.37392527  9.81527082
  3.718002   11.96472075  8.80869105  7.96655398]
obj_prev = 100.14234023128081
eta_min = 1.126497835179761e-11	eta_max = 0.9220293976525566
af = 21.163983583608065	bf = 1.67717061299475	zeta = 23.280381941968873	eta = 0.9090909090909091
af = 21.163983583608065	bf = 1.67717061299475	zeta = 40.637450630966356	eta = 0.5207999826514902
af = 21.163983583608065	bf = 1.67717061299475	zeta = 32.312864825181585	eta = 0.6549708203871438
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.81773875501804	eta = 0.6867468035811661
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.743104385934746	eta = 0.6884140039315868
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.74290487459993	eta = 0.6884184715119078
eta = 0.6884184715119078
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.03080864 0.06479595 0.03031961 0.01051406 0.07482098 0.0356989
 0.0132037  0.04376783 0.03178669 0.02885253]
ene_total = [2.66619823 5.03309398 2.64268475 1.21857109 5.74248846 3.03975216
 1.40293291 3.50736005 2.92607578 2.56374747]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 0 obj = 5.417871154985847
eta = 0.6884184715119078
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
eta_min = 0.6884184715119183	eta_max = 0.6884184715118975
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 0.03222576338096967	eta = 0.909090909090909
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 18.47959117596394	eta = 0.0015853244938805354
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8690048705094464	eta = 0.015674730970695012
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8275633379854408	eta = 0.016030168650925096
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.827555909002531	eta = 0.016030233813281204
eta = 0.016030233813281204
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.72568617e-04 1.60902329e-03 1.65941177e-04 6.65676762e-06
 2.50260959e-03 2.76163578e-04 1.31539832e-05 4.70643491e-04
 2.24189901e-04 1.45828825e-04]
ene_total = [0.16944101 0.20192804 0.17262124 0.15441975 0.22542177 0.18229224
 0.15369677 0.1569107  0.23132642 0.17949795]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 1 obj = 5.4178711549860274
eta = 0.6884184715119183
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
Done!
At round 19 energy consumption: 5.417871154985847
At round 19 eta: 0.6884184715119183
At round 19 local rounds: 12.225650330368824
At round 19 global rounds: 69.56199190949924
At round 19 a_n: 21.33168591686803
gradient difference: 0.3775717616081238
train() client id: f_00000-0-0 loss: 1.452883  [   32/  126]
train() client id: f_00000-0-1 loss: 1.317690  [   64/  126]
train() client id: f_00000-0-2 loss: 1.330917  [   96/  126]
train() client id: f_00000-1-0 loss: 1.292138  [   32/  126]
train() client id: f_00000-1-1 loss: 1.043485  [   64/  126]
train() client id: f_00000-1-2 loss: 1.115252  [   96/  126]
train() client id: f_00000-2-0 loss: 1.126586  [   32/  126]
train() client id: f_00000-2-1 loss: 1.044630  [   64/  126]
train() client id: f_00000-2-2 loss: 1.077970  [   96/  126]
train() client id: f_00000-3-0 loss: 1.150729  [   32/  126]
train() client id: f_00000-3-1 loss: 0.937759  [   64/  126]
train() client id: f_00000-3-2 loss: 0.930891  [   96/  126]
train() client id: f_00000-4-0 loss: 1.054221  [   32/  126]
train() client id: f_00000-4-1 loss: 0.989828  [   64/  126]
train() client id: f_00000-4-2 loss: 0.888952  [   96/  126]
train() client id: f_00000-5-0 loss: 0.889011  [   32/  126]
train() client id: f_00000-5-1 loss: 0.927518  [   64/  126]
train() client id: f_00000-5-2 loss: 0.944689  [   96/  126]
train() client id: f_00000-6-0 loss: 0.892990  [   32/  126]
train() client id: f_00000-6-1 loss: 0.893537  [   64/  126]
train() client id: f_00000-6-2 loss: 0.821170  [   96/  126]
train() client id: f_00000-7-0 loss: 0.831828  [   32/  126]
train() client id: f_00000-7-1 loss: 0.803117  [   64/  126]
train() client id: f_00000-7-2 loss: 0.844375  [   96/  126]
train() client id: f_00000-8-0 loss: 0.770419  [   32/  126]
train() client id: f_00000-8-1 loss: 0.838284  [   64/  126]
train() client id: f_00000-8-2 loss: 0.797997  [   96/  126]
train() client id: f_00000-9-0 loss: 0.865030  [   32/  126]
train() client id: f_00000-9-1 loss: 0.748559  [   64/  126]
train() client id: f_00000-9-2 loss: 0.837308  [   96/  126]
train() client id: f_00000-10-0 loss: 0.687483  [   32/  126]
train() client id: f_00000-10-1 loss: 0.942431  [   64/  126]
train() client id: f_00000-10-2 loss: 0.840067  [   96/  126]
train() client id: f_00000-11-0 loss: 0.796682  [   32/  126]
train() client id: f_00000-11-1 loss: 0.927730  [   64/  126]
train() client id: f_00000-11-2 loss: 0.790117  [   96/  126]
train() client id: f_00001-0-0 loss: 0.289996  [   32/  265]
train() client id: f_00001-0-1 loss: 0.288535  [   64/  265]
train() client id: f_00001-0-2 loss: 0.275134  [   96/  265]
train() client id: f_00001-0-3 loss: 0.369001  [  128/  265]
train() client id: f_00001-0-4 loss: 0.389648  [  160/  265]
train() client id: f_00001-0-5 loss: 0.425683  [  192/  265]
train() client id: f_00001-0-6 loss: 0.352012  [  224/  265]
train() client id: f_00001-0-7 loss: 0.438441  [  256/  265]
train() client id: f_00001-1-0 loss: 0.315332  [   32/  265]
train() client id: f_00001-1-1 loss: 0.461532  [   64/  265]
train() client id: f_00001-1-2 loss: 0.324761  [   96/  265]
train() client id: f_00001-1-3 loss: 0.262987  [  128/  265]
train() client id: f_00001-1-4 loss: 0.496838  [  160/  265]
train() client id: f_00001-1-5 loss: 0.317667  [  192/  265]
train() client id: f_00001-1-6 loss: 0.284097  [  224/  265]
train() client id: f_00001-1-7 loss: 0.317600  [  256/  265]
train() client id: f_00001-2-0 loss: 0.247680  [   32/  265]
train() client id: f_00001-2-1 loss: 0.474788  [   64/  265]
train() client id: f_00001-2-2 loss: 0.257957  [   96/  265]
train() client id: f_00001-2-3 loss: 0.254320  [  128/  265]
train() client id: f_00001-2-4 loss: 0.340538  [  160/  265]
train() client id: f_00001-2-5 loss: 0.420840  [  192/  265]
train() client id: f_00001-2-6 loss: 0.350623  [  224/  265]
train() client id: f_00001-2-7 loss: 0.289703  [  256/  265]
train() client id: f_00001-3-0 loss: 0.247135  [   32/  265]
train() client id: f_00001-3-1 loss: 0.468999  [   64/  265]
train() client id: f_00001-3-2 loss: 0.348803  [   96/  265]
train() client id: f_00001-3-3 loss: 0.305004  [  128/  265]
train() client id: f_00001-3-4 loss: 0.264797  [  160/  265]
train() client id: f_00001-3-5 loss: 0.334272  [  192/  265]
train() client id: f_00001-3-6 loss: 0.233632  [  224/  265]
train() client id: f_00001-3-7 loss: 0.384070  [  256/  265]
train() client id: f_00001-4-0 loss: 0.310244  [   32/  265]
train() client id: f_00001-4-1 loss: 0.272534  [   64/  265]
train() client id: f_00001-4-2 loss: 0.291301  [   96/  265]
train() client id: f_00001-4-3 loss: 0.223305  [  128/  265]
train() client id: f_00001-4-4 loss: 0.247522  [  160/  265]
train() client id: f_00001-4-5 loss: 0.482326  [  192/  265]
train() client id: f_00001-4-6 loss: 0.345940  [  224/  265]
train() client id: f_00001-4-7 loss: 0.355267  [  256/  265]
train() client id: f_00001-5-0 loss: 0.220696  [   32/  265]
train() client id: f_00001-5-1 loss: 0.260328  [   64/  265]
train() client id: f_00001-5-2 loss: 0.370300  [   96/  265]
train() client id: f_00001-5-3 loss: 0.348667  [  128/  265]
train() client id: f_00001-5-4 loss: 0.260693  [  160/  265]
train() client id: f_00001-5-5 loss: 0.268114  [  192/  265]
train() client id: f_00001-5-6 loss: 0.319634  [  224/  265]
train() client id: f_00001-5-7 loss: 0.306777  [  256/  265]
train() client id: f_00001-6-0 loss: 0.452590  [   32/  265]
train() client id: f_00001-6-1 loss: 0.285390  [   64/  265]
train() client id: f_00001-6-2 loss: 0.293674  [   96/  265]
train() client id: f_00001-6-3 loss: 0.309372  [  128/  265]
train() client id: f_00001-6-4 loss: 0.259782  [  160/  265]
train() client id: f_00001-6-5 loss: 0.276076  [  192/  265]
train() client id: f_00001-6-6 loss: 0.274010  [  224/  265]
train() client id: f_00001-6-7 loss: 0.267508  [  256/  265]
train() client id: f_00001-7-0 loss: 0.206909  [   32/  265]
train() client id: f_00001-7-1 loss: 0.191969  [   64/  265]
train() client id: f_00001-7-2 loss: 0.274935  [   96/  265]
train() client id: f_00001-7-3 loss: 0.326513  [  128/  265]
train() client id: f_00001-7-4 loss: 0.436953  [  160/  265]
train() client id: f_00001-7-5 loss: 0.319740  [  192/  265]
train() client id: f_00001-7-6 loss: 0.254098  [  224/  265]
train() client id: f_00001-7-7 loss: 0.368361  [  256/  265]
train() client id: f_00001-8-0 loss: 0.274617  [   32/  265]
train() client id: f_00001-8-1 loss: 0.410760  [   64/  265]
train() client id: f_00001-8-2 loss: 0.193568  [   96/  265]
train() client id: f_00001-8-3 loss: 0.257191  [  128/  265]
train() client id: f_00001-8-4 loss: 0.198580  [  160/  265]
train() client id: f_00001-8-5 loss: 0.246561  [  192/  265]
train() client id: f_00001-8-6 loss: 0.321951  [  224/  265]
train() client id: f_00001-8-7 loss: 0.367170  [  256/  265]
train() client id: f_00001-9-0 loss: 0.188098  [   32/  265]
train() client id: f_00001-9-1 loss: 0.259654  [   64/  265]
train() client id: f_00001-9-2 loss: 0.412602  [   96/  265]
train() client id: f_00001-9-3 loss: 0.376439  [  128/  265]
train() client id: f_00001-9-4 loss: 0.218583  [  160/  265]
train() client id: f_00001-9-5 loss: 0.258927  [  192/  265]
train() client id: f_00001-9-6 loss: 0.365722  [  224/  265]
train() client id: f_00001-9-7 loss: 0.228252  [  256/  265]
train() client id: f_00001-10-0 loss: 0.196622  [   32/  265]
train() client id: f_00001-10-1 loss: 0.325483  [   64/  265]
train() client id: f_00001-10-2 loss: 0.347506  [   96/  265]
train() client id: f_00001-10-3 loss: 0.320579  [  128/  265]
train() client id: f_00001-10-4 loss: 0.221134  [  160/  265]
train() client id: f_00001-10-5 loss: 0.187075  [  192/  265]
train() client id: f_00001-10-6 loss: 0.275111  [  224/  265]
train() client id: f_00001-10-7 loss: 0.431557  [  256/  265]
train() client id: f_00001-11-0 loss: 0.272302  [   32/  265]
train() client id: f_00001-11-1 loss: 0.236015  [   64/  265]
train() client id: f_00001-11-2 loss: 0.333010  [   96/  265]
train() client id: f_00001-11-3 loss: 0.253255  [  128/  265]
train() client id: f_00001-11-4 loss: 0.288591  [  160/  265]
train() client id: f_00001-11-5 loss: 0.445300  [  192/  265]
train() client id: f_00001-11-6 loss: 0.254745  [  224/  265]
train() client id: f_00001-11-7 loss: 0.192238  [  256/  265]
train() client id: f_00002-0-0 loss: 1.265246  [   32/  124]
train() client id: f_00002-0-1 loss: 1.128061  [   64/  124]
train() client id: f_00002-0-2 loss: 1.140691  [   96/  124]
train() client id: f_00002-1-0 loss: 1.126943  [   32/  124]
train() client id: f_00002-1-1 loss: 1.081080  [   64/  124]
train() client id: f_00002-1-2 loss: 1.160635  [   96/  124]
train() client id: f_00002-2-0 loss: 1.159996  [   32/  124]
train() client id: f_00002-2-1 loss: 1.082451  [   64/  124]
train() client id: f_00002-2-2 loss: 1.089666  [   96/  124]
train() client id: f_00002-3-0 loss: 0.947801  [   32/  124]
train() client id: f_00002-3-1 loss: 1.056487  [   64/  124]
train() client id: f_00002-3-2 loss: 1.171747  [   96/  124]
train() client id: f_00002-4-0 loss: 1.032236  [   32/  124]
train() client id: f_00002-4-1 loss: 1.176793  [   64/  124]
train() client id: f_00002-4-2 loss: 0.983624  [   96/  124]
train() client id: f_00002-5-0 loss: 0.956520  [   32/  124]
train() client id: f_00002-5-1 loss: 1.104056  [   64/  124]
train() client id: f_00002-5-2 loss: 1.036661  [   96/  124]
train() client id: f_00002-6-0 loss: 1.027940  [   32/  124]
train() client id: f_00002-6-1 loss: 0.962014  [   64/  124]
train() client id: f_00002-6-2 loss: 0.926894  [   96/  124]
train() client id: f_00002-7-0 loss: 1.002854  [   32/  124]
train() client id: f_00002-7-1 loss: 1.018690  [   64/  124]
train() client id: f_00002-7-2 loss: 0.961609  [   96/  124]
train() client id: f_00002-8-0 loss: 0.869298  [   32/  124]
train() client id: f_00002-8-1 loss: 0.895916  [   64/  124]
train() client id: f_00002-8-2 loss: 1.048261  [   96/  124]
train() client id: f_00002-9-0 loss: 0.896653  [   32/  124]
train() client id: f_00002-9-1 loss: 1.166051  [   64/  124]
train() client id: f_00002-9-2 loss: 0.948811  [   96/  124]
train() client id: f_00002-10-0 loss: 1.004288  [   32/  124]
train() client id: f_00002-10-1 loss: 0.969448  [   64/  124]
train() client id: f_00002-10-2 loss: 0.883775  [   96/  124]
train() client id: f_00002-11-0 loss: 0.997326  [   32/  124]
train() client id: f_00002-11-1 loss: 0.920306  [   64/  124]
train() client id: f_00002-11-2 loss: 0.919146  [   96/  124]
train() client id: f_00003-0-0 loss: 0.836118  [   32/   43]
train() client id: f_00003-1-0 loss: 0.713566  [   32/   43]
train() client id: f_00003-2-0 loss: 0.881710  [   32/   43]
train() client id: f_00003-3-0 loss: 0.640129  [   32/   43]
train() client id: f_00003-4-0 loss: 0.768725  [   32/   43]
train() client id: f_00003-5-0 loss: 0.776945  [   32/   43]
train() client id: f_00003-6-0 loss: 0.866783  [   32/   43]
train() client id: f_00003-7-0 loss: 0.882015  [   32/   43]
train() client id: f_00003-8-0 loss: 0.784807  [   32/   43]
train() client id: f_00003-9-0 loss: 0.844651  [   32/   43]
train() client id: f_00003-10-0 loss: 0.840885  [   32/   43]
train() client id: f_00003-11-0 loss: 0.863067  [   32/   43]
train() client id: f_00004-0-0 loss: 0.877996  [   32/  306]
train() client id: f_00004-0-1 loss: 0.701790  [   64/  306]
train() client id: f_00004-0-2 loss: 0.881930  [   96/  306]
train() client id: f_00004-0-3 loss: 0.735934  [  128/  306]
train() client id: f_00004-0-4 loss: 0.904996  [  160/  306]
train() client id: f_00004-0-5 loss: 0.880108  [  192/  306]
train() client id: f_00004-0-6 loss: 1.015528  [  224/  306]
train() client id: f_00004-0-7 loss: 0.925083  [  256/  306]
train() client id: f_00004-0-8 loss: 1.034975  [  288/  306]
train() client id: f_00004-1-0 loss: 0.855006  [   32/  306]
train() client id: f_00004-1-1 loss: 0.854173  [   64/  306]
train() client id: f_00004-1-2 loss: 0.847088  [   96/  306]
train() client id: f_00004-1-3 loss: 0.788203  [  128/  306]
train() client id: f_00004-1-4 loss: 0.957762  [  160/  306]
train() client id: f_00004-1-5 loss: 0.918380  [  192/  306]
train() client id: f_00004-1-6 loss: 0.927597  [  224/  306]
train() client id: f_00004-1-7 loss: 0.911974  [  256/  306]
train() client id: f_00004-1-8 loss: 0.879826  [  288/  306]
train() client id: f_00004-2-0 loss: 0.947056  [   32/  306]
train() client id: f_00004-2-1 loss: 0.910769  [   64/  306]
train() client id: f_00004-2-2 loss: 0.851096  [   96/  306]
train() client id: f_00004-2-3 loss: 0.718519  [  128/  306]
train() client id: f_00004-2-4 loss: 0.887969  [  160/  306]
train() client id: f_00004-2-5 loss: 0.912607  [  192/  306]
train() client id: f_00004-2-6 loss: 0.969421  [  224/  306]
train() client id: f_00004-2-7 loss: 0.808753  [  256/  306]
train() client id: f_00004-2-8 loss: 0.976385  [  288/  306]
train() client id: f_00004-3-0 loss: 0.812014  [   32/  306]
train() client id: f_00004-3-1 loss: 0.895529  [   64/  306]
train() client id: f_00004-3-2 loss: 0.804478  [   96/  306]
train() client id: f_00004-3-3 loss: 1.029997  [  128/  306]
train() client id: f_00004-3-4 loss: 0.887866  [  160/  306]
train() client id: f_00004-3-5 loss: 0.919049  [  192/  306]
train() client id: f_00004-3-6 loss: 0.900098  [  224/  306]
train() client id: f_00004-3-7 loss: 0.911567  [  256/  306]
train() client id: f_00004-3-8 loss: 0.857545  [  288/  306]
train() client id: f_00004-4-0 loss: 0.949777  [   32/  306]
train() client id: f_00004-4-1 loss: 0.764817  [   64/  306]
train() client id: f_00004-4-2 loss: 0.831260  [   96/  306]
train() client id: f_00004-4-3 loss: 0.993250  [  128/  306]
train() client id: f_00004-4-4 loss: 0.932600  [  160/  306]
train() client id: f_00004-4-5 loss: 0.989437  [  192/  306]
train() client id: f_00004-4-6 loss: 0.906104  [  224/  306]
train() client id: f_00004-4-7 loss: 0.785020  [  256/  306]
train() client id: f_00004-4-8 loss: 0.807105  [  288/  306]
train() client id: f_00004-5-0 loss: 0.870888  [   32/  306]
train() client id: f_00004-5-1 loss: 1.124940  [   64/  306]
train() client id: f_00004-5-2 loss: 0.838077  [   96/  306]
train() client id: f_00004-5-3 loss: 0.802096  [  128/  306]
train() client id: f_00004-5-4 loss: 0.822432  [  160/  306]
train() client id: f_00004-5-5 loss: 0.761600  [  192/  306]
train() client id: f_00004-5-6 loss: 0.936303  [  224/  306]
train() client id: f_00004-5-7 loss: 0.982344  [  256/  306]
train() client id: f_00004-5-8 loss: 0.884919  [  288/  306]
train() client id: f_00004-6-0 loss: 0.881645  [   32/  306]
train() client id: f_00004-6-1 loss: 0.890440  [   64/  306]
train() client id: f_00004-6-2 loss: 0.813388  [   96/  306]
train() client id: f_00004-6-3 loss: 0.769904  [  128/  306]
train() client id: f_00004-6-4 loss: 0.958058  [  160/  306]
train() client id: f_00004-6-5 loss: 0.871398  [  192/  306]
train() client id: f_00004-6-6 loss: 0.882368  [  224/  306]
train() client id: f_00004-6-7 loss: 0.936301  [  256/  306]
train() client id: f_00004-6-8 loss: 0.981273  [  288/  306]
train() client id: f_00004-7-0 loss: 0.895342  [   32/  306]
train() client id: f_00004-7-1 loss: 0.919566  [   64/  306]
train() client id: f_00004-7-2 loss: 1.003315  [   96/  306]
train() client id: f_00004-7-3 loss: 0.956093  [  128/  306]
train() client id: f_00004-7-4 loss: 0.867071  [  160/  306]
train() client id: f_00004-7-5 loss: 0.878385  [  192/  306]
train() client id: f_00004-7-6 loss: 0.755805  [  224/  306]
train() client id: f_00004-7-7 loss: 0.880356  [  256/  306]
train() client id: f_00004-7-8 loss: 0.804341  [  288/  306]
train() client id: f_00004-8-0 loss: 1.035843  [   32/  306]
train() client id: f_00004-8-1 loss: 0.894903  [   64/  306]
train() client id: f_00004-8-2 loss: 0.891003  [   96/  306]
train() client id: f_00004-8-3 loss: 0.890182  [  128/  306]
train() client id: f_00004-8-4 loss: 0.801214  [  160/  306]
train() client id: f_00004-8-5 loss: 0.866252  [  192/  306]
train() client id: f_00004-8-6 loss: 0.838887  [  224/  306]
train() client id: f_00004-8-7 loss: 0.919292  [  256/  306]
train() client id: f_00004-8-8 loss: 0.793071  [  288/  306]
train() client id: f_00004-9-0 loss: 0.931515  [   32/  306]
train() client id: f_00004-9-1 loss: 0.807702  [   64/  306]
train() client id: f_00004-9-2 loss: 0.970904  [   96/  306]
train() client id: f_00004-9-3 loss: 0.925350  [  128/  306]
train() client id: f_00004-9-4 loss: 0.910712  [  160/  306]
train() client id: f_00004-9-5 loss: 0.992543  [  192/  306]
train() client id: f_00004-9-6 loss: 0.877606  [  224/  306]
train() client id: f_00004-9-7 loss: 0.852463  [  256/  306]
train() client id: f_00004-9-8 loss: 0.795747  [  288/  306]
train() client id: f_00004-10-0 loss: 0.782224  [   32/  306]
train() client id: f_00004-10-1 loss: 0.807297  [   64/  306]
train() client id: f_00004-10-2 loss: 1.022443  [   96/  306]
train() client id: f_00004-10-3 loss: 0.825977  [  128/  306]
train() client id: f_00004-10-4 loss: 0.893698  [  160/  306]
train() client id: f_00004-10-5 loss: 0.866478  [  192/  306]
train() client id: f_00004-10-6 loss: 0.925999  [  224/  306]
train() client id: f_00004-10-7 loss: 0.959028  [  256/  306]
train() client id: f_00004-10-8 loss: 0.943107  [  288/  306]
train() client id: f_00004-11-0 loss: 1.120449  [   32/  306]
train() client id: f_00004-11-1 loss: 0.863732  [   64/  306]
train() client id: f_00004-11-2 loss: 0.767185  [   96/  306]
train() client id: f_00004-11-3 loss: 0.977369  [  128/  306]
train() client id: f_00004-11-4 loss: 0.810143  [  160/  306]
train() client id: f_00004-11-5 loss: 0.844804  [  192/  306]
train() client id: f_00004-11-6 loss: 0.802892  [  224/  306]
train() client id: f_00004-11-7 loss: 0.885859  [  256/  306]
train() client id: f_00004-11-8 loss: 0.915133  [  288/  306]
train() client id: f_00005-0-0 loss: 0.561490  [   32/  146]
train() client id: f_00005-0-1 loss: 0.647659  [   64/  146]
train() client id: f_00005-0-2 loss: 0.449870  [   96/  146]
train() client id: f_00005-0-3 loss: 0.510116  [  128/  146]
train() client id: f_00005-1-0 loss: 0.630755  [   32/  146]
train() client id: f_00005-1-1 loss: 0.647743  [   64/  146]
train() client id: f_00005-1-2 loss: 0.657065  [   96/  146]
train() client id: f_00005-1-3 loss: 0.369901  [  128/  146]
train() client id: f_00005-2-0 loss: 0.852892  [   32/  146]
train() client id: f_00005-2-1 loss: 0.603048  [   64/  146]
train() client id: f_00005-2-2 loss: 0.547964  [   96/  146]
train() client id: f_00005-2-3 loss: 0.564838  [  128/  146]
train() client id: f_00005-3-0 loss: 0.647286  [   32/  146]
train() client id: f_00005-3-1 loss: 0.508369  [   64/  146]
train() client id: f_00005-3-2 loss: 0.770971  [   96/  146]
train() client id: f_00005-3-3 loss: 0.579422  [  128/  146]
train() client id: f_00005-4-0 loss: 0.447702  [   32/  146]
train() client id: f_00005-4-1 loss: 0.346830  [   64/  146]
train() client id: f_00005-4-2 loss: 0.754431  [   96/  146]
train() client id: f_00005-4-3 loss: 0.708260  [  128/  146]
train() client id: f_00005-5-0 loss: 0.521345  [   32/  146]
train() client id: f_00005-5-1 loss: 0.555660  [   64/  146]
train() client id: f_00005-5-2 loss: 0.691784  [   96/  146]
train() client id: f_00005-5-3 loss: 0.710613  [  128/  146]
train() client id: f_00005-6-0 loss: 0.632954  [   32/  146]
train() client id: f_00005-6-1 loss: 0.532062  [   64/  146]
train() client id: f_00005-6-2 loss: 0.604246  [   96/  146]
train() client id: f_00005-6-3 loss: 0.603965  [  128/  146]
train() client id: f_00005-7-0 loss: 0.393436  [   32/  146]
train() client id: f_00005-7-1 loss: 0.740217  [   64/  146]
train() client id: f_00005-7-2 loss: 0.593694  [   96/  146]
train() client id: f_00005-7-3 loss: 0.641942  [  128/  146]
train() client id: f_00005-8-0 loss: 0.412463  [   32/  146]
train() client id: f_00005-8-1 loss: 0.580114  [   64/  146]
train() client id: f_00005-8-2 loss: 0.910852  [   96/  146]
train() client id: f_00005-8-3 loss: 0.446803  [  128/  146]
train() client id: f_00005-9-0 loss: 0.511181  [   32/  146]
train() client id: f_00005-9-1 loss: 0.560167  [   64/  146]
train() client id: f_00005-9-2 loss: 0.612282  [   96/  146]
train() client id: f_00005-9-3 loss: 0.580071  [  128/  146]
train() client id: f_00005-10-0 loss: 0.418012  [   32/  146]
train() client id: f_00005-10-1 loss: 0.836116  [   64/  146]
train() client id: f_00005-10-2 loss: 0.736083  [   96/  146]
train() client id: f_00005-10-3 loss: 0.453666  [  128/  146]
train() client id: f_00005-11-0 loss: 0.460110  [   32/  146]
train() client id: f_00005-11-1 loss: 0.616104  [   64/  146]
train() client id: f_00005-11-2 loss: 0.561327  [   96/  146]
train() client id: f_00005-11-3 loss: 0.672130  [  128/  146]
train() client id: f_00006-0-0 loss: 0.536576  [   32/   54]
train() client id: f_00006-1-0 loss: 0.541397  [   32/   54]
train() client id: f_00006-2-0 loss: 0.547522  [   32/   54]
train() client id: f_00006-3-0 loss: 0.590194  [   32/   54]
train() client id: f_00006-4-0 loss: 0.595870  [   32/   54]
train() client id: f_00006-5-0 loss: 0.582635  [   32/   54]
train() client id: f_00006-6-0 loss: 0.548668  [   32/   54]
train() client id: f_00006-7-0 loss: 0.538638  [   32/   54]
train() client id: f_00006-8-0 loss: 0.537016  [   32/   54]
train() client id: f_00006-9-0 loss: 0.588157  [   32/   54]
train() client id: f_00006-10-0 loss: 0.551296  [   32/   54]
train() client id: f_00006-11-0 loss: 0.593495  [   32/   54]
train() client id: f_00007-0-0 loss: 0.519087  [   32/  179]
train() client id: f_00007-0-1 loss: 0.602610  [   64/  179]
train() client id: f_00007-0-2 loss: 0.445679  [   96/  179]
train() client id: f_00007-0-3 loss: 0.707406  [  128/  179]
train() client id: f_00007-0-4 loss: 0.487986  [  160/  179]
train() client id: f_00007-1-0 loss: 0.558844  [   32/  179]
train() client id: f_00007-1-1 loss: 0.584981  [   64/  179]
train() client id: f_00007-1-2 loss: 0.385669  [   96/  179]
train() client id: f_00007-1-3 loss: 0.620137  [  128/  179]
train() client id: f_00007-1-4 loss: 0.527812  [  160/  179]
train() client id: f_00007-2-0 loss: 0.533016  [   32/  179]
train() client id: f_00007-2-1 loss: 0.471784  [   64/  179]
train() client id: f_00007-2-2 loss: 0.501854  [   96/  179]
train() client id: f_00007-2-3 loss: 0.606799  [  128/  179]
train() client id: f_00007-2-4 loss: 0.535354  [  160/  179]
train() client id: f_00007-3-0 loss: 0.590391  [   32/  179]
train() client id: f_00007-3-1 loss: 0.569364  [   64/  179]
train() client id: f_00007-3-2 loss: 0.542342  [   96/  179]
train() client id: f_00007-3-3 loss: 0.462820  [  128/  179]
train() client id: f_00007-3-4 loss: 0.450343  [  160/  179]
train() client id: f_00007-4-0 loss: 0.407788  [   32/  179]
train() client id: f_00007-4-1 loss: 0.468070  [   64/  179]
train() client id: f_00007-4-2 loss: 0.586994  [   96/  179]
train() client id: f_00007-4-3 loss: 0.572070  [  128/  179]
train() client id: f_00007-4-4 loss: 0.353356  [  160/  179]
train() client id: f_00007-5-0 loss: 0.568053  [   32/  179]
train() client id: f_00007-5-1 loss: 0.431652  [   64/  179]
train() client id: f_00007-5-2 loss: 0.506997  [   96/  179]
train() client id: f_00007-5-3 loss: 0.436769  [  128/  179]
train() client id: f_00007-5-4 loss: 0.431991  [  160/  179]
train() client id: f_00007-6-0 loss: 0.579414  [   32/  179]
train() client id: f_00007-6-1 loss: 0.365920  [   64/  179]
train() client id: f_00007-6-2 loss: 0.551389  [   96/  179]
train() client id: f_00007-6-3 loss: 0.504948  [  128/  179]
train() client id: f_00007-6-4 loss: 0.510173  [  160/  179]
train() client id: f_00007-7-0 loss: 0.449014  [   32/  179]
train() client id: f_00007-7-1 loss: 0.527449  [   64/  179]
train() client id: f_00007-7-2 loss: 0.403060  [   96/  179]
train() client id: f_00007-7-3 loss: 0.644109  [  128/  179]
train() client id: f_00007-7-4 loss: 0.374532  [  160/  179]
train() client id: f_00007-8-0 loss: 0.590719  [   32/  179]
train() client id: f_00007-8-1 loss: 0.452154  [   64/  179]
train() client id: f_00007-8-2 loss: 0.339302  [   96/  179]
train() client id: f_00007-8-3 loss: 0.551794  [  128/  179]
train() client id: f_00007-8-4 loss: 0.353866  [  160/  179]
train() client id: f_00007-9-0 loss: 0.435464  [   32/  179]
train() client id: f_00007-9-1 loss: 0.593607  [   64/  179]
train() client id: f_00007-9-2 loss: 0.511181  [   96/  179]
train() client id: f_00007-9-3 loss: 0.453680  [  128/  179]
train() client id: f_00007-9-4 loss: 0.435413  [  160/  179]
train() client id: f_00007-10-0 loss: 0.371564  [   32/  179]
train() client id: f_00007-10-1 loss: 0.542992  [   64/  179]
train() client id: f_00007-10-2 loss: 0.547685  [   96/  179]
train() client id: f_00007-10-3 loss: 0.595523  [  128/  179]
train() client id: f_00007-10-4 loss: 0.434201  [  160/  179]
train() client id: f_00007-11-0 loss: 0.429157  [   32/  179]
train() client id: f_00007-11-1 loss: 0.671275  [   64/  179]
train() client id: f_00007-11-2 loss: 0.440586  [   96/  179]
train() client id: f_00007-11-3 loss: 0.371521  [  128/  179]
train() client id: f_00007-11-4 loss: 0.516152  [  160/  179]
train() client id: f_00008-0-0 loss: 0.720320  [   32/  130]
train() client id: f_00008-0-1 loss: 0.665189  [   64/  130]
train() client id: f_00008-0-2 loss: 0.854363  [   96/  130]
train() client id: f_00008-0-3 loss: 0.611702  [  128/  130]
train() client id: f_00008-1-0 loss: 0.650367  [   32/  130]
train() client id: f_00008-1-1 loss: 0.724118  [   64/  130]
train() client id: f_00008-1-2 loss: 0.792391  [   96/  130]
train() client id: f_00008-1-3 loss: 0.645022  [  128/  130]
train() client id: f_00008-2-0 loss: 0.626983  [   32/  130]
train() client id: f_00008-2-1 loss: 0.622283  [   64/  130]
train() client id: f_00008-2-2 loss: 0.700328  [   96/  130]
train() client id: f_00008-2-3 loss: 0.846498  [  128/  130]
train() client id: f_00008-3-0 loss: 0.703518  [   32/  130]
train() client id: f_00008-3-1 loss: 0.775087  [   64/  130]
train() client id: f_00008-3-2 loss: 0.734898  [   96/  130]
train() client id: f_00008-3-3 loss: 0.605636  [  128/  130]
train() client id: f_00008-4-0 loss: 0.663779  [   32/  130]
train() client id: f_00008-4-1 loss: 0.823637  [   64/  130]
train() client id: f_00008-4-2 loss: 0.650018  [   96/  130]
train() client id: f_00008-4-3 loss: 0.685032  [  128/  130]
train() client id: f_00008-5-0 loss: 0.777479  [   32/  130]
train() client id: f_00008-5-1 loss: 0.620857  [   64/  130]
train() client id: f_00008-5-2 loss: 0.812856  [   96/  130]
train() client id: f_00008-5-3 loss: 0.597392  [  128/  130]
train() client id: f_00008-6-0 loss: 0.610832  [   32/  130]
train() client id: f_00008-6-1 loss: 0.723970  [   64/  130]
train() client id: f_00008-6-2 loss: 0.649198  [   96/  130]
train() client id: f_00008-6-3 loss: 0.848563  [  128/  130]
train() client id: f_00008-7-0 loss: 0.698355  [   32/  130]
train() client id: f_00008-7-1 loss: 0.727003  [   64/  130]
train() client id: f_00008-7-2 loss: 0.641500  [   96/  130]
train() client id: f_00008-7-3 loss: 0.773185  [  128/  130]
train() client id: f_00008-8-0 loss: 0.735514  [   32/  130]
train() client id: f_00008-8-1 loss: 0.641693  [   64/  130]
train() client id: f_00008-8-2 loss: 0.791601  [   96/  130]
train() client id: f_00008-8-3 loss: 0.679262  [  128/  130]
train() client id: f_00008-9-0 loss: 0.716716  [   32/  130]
train() client id: f_00008-9-1 loss: 0.667666  [   64/  130]
train() client id: f_00008-9-2 loss: 0.707299  [   96/  130]
train() client id: f_00008-9-3 loss: 0.759598  [  128/  130]
train() client id: f_00008-10-0 loss: 0.634569  [   32/  130]
train() client id: f_00008-10-1 loss: 0.689345  [   64/  130]
train() client id: f_00008-10-2 loss: 0.886070  [   96/  130]
train() client id: f_00008-10-3 loss: 0.610969  [  128/  130]
train() client id: f_00008-11-0 loss: 0.675616  [   32/  130]
train() client id: f_00008-11-1 loss: 0.748811  [   64/  130]
train() client id: f_00008-11-2 loss: 0.743464  [   96/  130]
train() client id: f_00008-11-3 loss: 0.686173  [  128/  130]
train() client id: f_00009-0-0 loss: 1.172543  [   32/  118]
train() client id: f_00009-0-1 loss: 1.356836  [   64/  118]
train() client id: f_00009-0-2 loss: 1.090078  [   96/  118]
train() client id: f_00009-1-0 loss: 1.093601  [   32/  118]
train() client id: f_00009-1-1 loss: 1.130368  [   64/  118]
train() client id: f_00009-1-2 loss: 1.161288  [   96/  118]
train() client id: f_00009-2-0 loss: 1.086921  [   32/  118]
train() client id: f_00009-2-1 loss: 1.176200  [   64/  118]
train() client id: f_00009-2-2 loss: 0.999438  [   96/  118]
train() client id: f_00009-3-0 loss: 0.998572  [   32/  118]
train() client id: f_00009-3-1 loss: 1.086451  [   64/  118]
train() client id: f_00009-3-2 loss: 0.945560  [   96/  118]
train() client id: f_00009-4-0 loss: 1.018148  [   32/  118]
train() client id: f_00009-4-1 loss: 1.042830  [   64/  118]
train() client id: f_00009-4-2 loss: 0.941094  [   96/  118]
train() client id: f_00009-5-0 loss: 0.912967  [   32/  118]
train() client id: f_00009-5-1 loss: 0.998288  [   64/  118]
train() client id: f_00009-5-2 loss: 1.011163  [   96/  118]
train() client id: f_00009-6-0 loss: 1.047609  [   32/  118]
train() client id: f_00009-6-1 loss: 0.911215  [   64/  118]
train() client id: f_00009-6-2 loss: 0.917109  [   96/  118]
train() client id: f_00009-7-0 loss: 0.866783  [   32/  118]
train() client id: f_00009-7-1 loss: 0.841354  [   64/  118]
train() client id: f_00009-7-2 loss: 1.119110  [   96/  118]
train() client id: f_00009-8-0 loss: 0.927326  [   32/  118]
train() client id: f_00009-8-1 loss: 0.908870  [   64/  118]
train() client id: f_00009-8-2 loss: 0.888925  [   96/  118]
train() client id: f_00009-9-0 loss: 0.937862  [   32/  118]
train() client id: f_00009-9-1 loss: 0.907056  [   64/  118]
train() client id: f_00009-9-2 loss: 0.856798  [   96/  118]
train() client id: f_00009-10-0 loss: 0.863358  [   32/  118]
train() client id: f_00009-10-1 loss: 0.892668  [   64/  118]
train() client id: f_00009-10-2 loss: 0.967068  [   96/  118]
train() client id: f_00009-11-0 loss: 0.954808  [   32/  118]
train() client id: f_00009-11-1 loss: 0.881924  [   64/  118]
train() client id: f_00009-11-2 loss: 0.809005  [   96/  118]
At round 19 accuracy: 0.6339522546419099
At round 19 training accuracy: 0.5754527162977867
At round 19 training loss: 0.8480132579833449
update_location
xs = [ -3.9056584    4.20031788 115.00902392  18.81129433   0.97929623
   3.95640986 -77.44319194 -56.32485185  99.66397685 -42.06087855]
ys = [107.5879595   90.55583871   1.32061395 -77.45517586  69.35018685
  52.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [146.93680001 134.9740812  152.41003774 127.87950994 121.69801739
 113.15912556 126.50825475 114.77441    142.27290063 108.55933567]
dists_bs = [183.89577736 198.09989981 338.03810893 318.27100658 205.25738895
 216.69051292 202.65232753 210.76443071 316.58303499 216.59479044]
uav_gains = [3.81792511e-11 4.72350303e-11 3.48260844e-11 5.40686414e-11
 6.12019359e-11 7.34119429e-11 5.55466623e-11 7.08558845e-11
 4.13970915e-11 8.14379617e-11]
bs_gains = [5.04072687e-11 4.09277782e-11 9.16621896e-12 1.08508127e-11
 3.70559068e-11 3.18377503e-11 3.84051633e-11 3.44081712e-11
 1.10135848e-11 3.18771632e-11]
Round 20
-------------------------------
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.3523103  17.38275937  8.22567535  2.94704545 20.05064182  9.65872482
  3.66105067 11.77706151  8.67193539  7.83910309]
obj_prev = 98.5663077651687
eta_min = 7.590997972408306e-12	eta_max = 0.9222942349256907
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 22.912452031604698	eta = 0.9090909090909091
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 40.06758095330687	eta = 0.519859231611392
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 31.832010204673995	eta = 0.654357098812908
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.352361561329435	eta = 0.6862563825495306
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27833469058294	eta = 0.6879341965062457
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27813585075753	eta = 0.6879387142452571
eta = 0.6879387142452571
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.03086617 0.06491693 0.03037623 0.01053369 0.07496069 0.03576556
 0.01322836 0.04384955 0.03184604 0.02890641]
ene_total = [2.63093078 4.95082043 2.60801881 1.20448716 5.64850744 2.98715498
 1.38606222 3.45675966 2.88726353 2.51813083]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 0 obj = 5.343958006285102
eta = 0.6879387142452571
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
eta_min = 0.6879387142452722	eta_max = 0.6879387142452453
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 0.030579617084646203	eta = 0.9090909090909091
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 18.258827212431594	eta = 0.001522532174257359
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8403927678865823	eta = 0.015105282079029619
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8010043726548246	eta = 0.015435638201229906
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.800997812209442	eta = 0.015435694428206238
eta = 0.015435694428206238
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.69209568e-04 1.56398576e-03 1.62723179e-04 6.52331649e-06
 2.43061804e-03 2.68033519e-04 1.28914250e-05 4.60934337e-04
 2.19373145e-04 1.41488442e-04]
ene_total = [0.16879936 0.19656835 0.17201187 0.15368379 0.21887026 0.17769628
 0.15298971 0.15556983 0.22990166 0.17490671]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 1 obj = 5.343958006285359
eta = 0.6879387142452722
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
Done!
At round 20 energy consumption: 5.343958006285102
At round 20 eta: 0.6879387142452722
At round 20 local rounds: 12.248478257840025
At round 20 global rounds: 68.35736084749131
At round 20 a_n: 20.989140069898713
gradient difference: 0.5220171213150024
train() client id: f_00000-0-0 loss: 1.205793  [   32/  126]
train() client id: f_00000-0-1 loss: 1.124826  [   64/  126]
train() client id: f_00000-0-2 loss: 1.263876  [   96/  126]
train() client id: f_00000-1-0 loss: 1.114931  [   32/  126]
train() client id: f_00000-1-1 loss: 1.104042  [   64/  126]
train() client id: f_00000-1-2 loss: 1.093674  [   96/  126]
train() client id: f_00000-2-0 loss: 0.960928  [   32/  126]
train() client id: f_00000-2-1 loss: 1.044961  [   64/  126]
train() client id: f_00000-2-2 loss: 1.099165  [   96/  126]
train() client id: f_00000-3-0 loss: 1.009220  [   32/  126]
train() client id: f_00000-3-1 loss: 1.037114  [   64/  126]
train() client id: f_00000-3-2 loss: 0.892443  [   96/  126]
train() client id: f_00000-4-0 loss: 0.995201  [   32/  126]
train() client id: f_00000-4-1 loss: 0.821093  [   64/  126]
train() client id: f_00000-4-2 loss: 1.021039  [   96/  126]
train() client id: f_00000-5-0 loss: 0.911492  [   32/  126]
train() client id: f_00000-5-1 loss: 1.007884  [   64/  126]
train() client id: f_00000-5-2 loss: 0.861516  [   96/  126]
train() client id: f_00000-6-0 loss: 0.922167  [   32/  126]
train() client id: f_00000-6-1 loss: 0.867840  [   64/  126]
train() client id: f_00000-6-2 loss: 0.896818  [   96/  126]
train() client id: f_00000-7-0 loss: 0.922954  [   32/  126]
train() client id: f_00000-7-1 loss: 0.944728  [   64/  126]
train() client id: f_00000-7-2 loss: 0.843773  [   96/  126]
train() client id: f_00000-8-0 loss: 0.924073  [   32/  126]
train() client id: f_00000-8-1 loss: 0.880291  [   64/  126]
train() client id: f_00000-8-2 loss: 0.843273  [   96/  126]
train() client id: f_00000-9-0 loss: 0.894176  [   32/  126]
train() client id: f_00000-9-1 loss: 0.803497  [   64/  126]
train() client id: f_00000-9-2 loss: 0.983683  [   96/  126]
train() client id: f_00000-10-0 loss: 0.931070  [   32/  126]
train() client id: f_00000-10-1 loss: 0.803963  [   64/  126]
train() client id: f_00000-10-2 loss: 0.892420  [   96/  126]
train() client id: f_00000-11-0 loss: 0.917704  [   32/  126]
train() client id: f_00000-11-1 loss: 0.905397  [   64/  126]
train() client id: f_00000-11-2 loss: 0.947742  [   96/  126]
train() client id: f_00001-0-0 loss: 0.567851  [   32/  265]
train() client id: f_00001-0-1 loss: 0.518917  [   64/  265]
train() client id: f_00001-0-2 loss: 0.497988  [   96/  265]
train() client id: f_00001-0-3 loss: 0.539389  [  128/  265]
train() client id: f_00001-0-4 loss: 0.472975  [  160/  265]
train() client id: f_00001-0-5 loss: 0.505636  [  192/  265]
train() client id: f_00001-0-6 loss: 0.566703  [  224/  265]
train() client id: f_00001-0-7 loss: 0.602349  [  256/  265]
train() client id: f_00001-1-0 loss: 0.534639  [   32/  265]
train() client id: f_00001-1-1 loss: 0.508031  [   64/  265]
train() client id: f_00001-1-2 loss: 0.476019  [   96/  265]
train() client id: f_00001-1-3 loss: 0.621265  [  128/  265]
train() client id: f_00001-1-4 loss: 0.526411  [  160/  265]
train() client id: f_00001-1-5 loss: 0.596123  [  192/  265]
train() client id: f_00001-1-6 loss: 0.508607  [  224/  265]
train() client id: f_00001-1-7 loss: 0.436977  [  256/  265]
train() client id: f_00001-2-0 loss: 0.507250  [   32/  265]
train() client id: f_00001-2-1 loss: 0.475130  [   64/  265]
train() client id: f_00001-2-2 loss: 0.457388  [   96/  265]
train() client id: f_00001-2-3 loss: 0.419514  [  128/  265]
train() client id: f_00001-2-4 loss: 0.548181  [  160/  265]
train() client id: f_00001-2-5 loss: 0.572045  [  192/  265]
train() client id: f_00001-2-6 loss: 0.576224  [  224/  265]
train() client id: f_00001-2-7 loss: 0.572955  [  256/  265]
train() client id: f_00001-3-0 loss: 0.509084  [   32/  265]
train() client id: f_00001-3-1 loss: 0.473748  [   64/  265]
train() client id: f_00001-3-2 loss: 0.421763  [   96/  265]
train() client id: f_00001-3-3 loss: 0.513945  [  128/  265]
train() client id: f_00001-3-4 loss: 0.556245  [  160/  265]
train() client id: f_00001-3-5 loss: 0.526104  [  192/  265]
train() client id: f_00001-3-6 loss: 0.560354  [  224/  265]
train() client id: f_00001-3-7 loss: 0.577756  [  256/  265]
train() client id: f_00001-4-0 loss: 0.553454  [   32/  265]
train() client id: f_00001-4-1 loss: 0.542108  [   64/  265]
train() client id: f_00001-4-2 loss: 0.492167  [   96/  265]
train() client id: f_00001-4-3 loss: 0.476626  [  128/  265]
train() client id: f_00001-4-4 loss: 0.545225  [  160/  265]
train() client id: f_00001-4-5 loss: 0.559164  [  192/  265]
train() client id: f_00001-4-6 loss: 0.456814  [  224/  265]
train() client id: f_00001-4-7 loss: 0.490195  [  256/  265]
train() client id: f_00001-5-0 loss: 0.635491  [   32/  265]
train() client id: f_00001-5-1 loss: 0.636284  [   64/  265]
train() client id: f_00001-5-2 loss: 0.563700  [   96/  265]
train() client id: f_00001-5-3 loss: 0.439687  [  128/  265]
train() client id: f_00001-5-4 loss: 0.426745  [  160/  265]
train() client id: f_00001-5-5 loss: 0.410883  [  192/  265]
train() client id: f_00001-5-6 loss: 0.457452  [  224/  265]
train() client id: f_00001-5-7 loss: 0.539622  [  256/  265]
train() client id: f_00001-6-0 loss: 0.460244  [   32/  265]
train() client id: f_00001-6-1 loss: 0.508724  [   64/  265]
train() client id: f_00001-6-2 loss: 0.523262  [   96/  265]
train() client id: f_00001-6-3 loss: 0.489884  [  128/  265]
train() client id: f_00001-6-4 loss: 0.532386  [  160/  265]
train() client id: f_00001-6-5 loss: 0.628353  [  192/  265]
train() client id: f_00001-6-6 loss: 0.411074  [  224/  265]
train() client id: f_00001-6-7 loss: 0.486054  [  256/  265]
train() client id: f_00001-7-0 loss: 0.424808  [   32/  265]
train() client id: f_00001-7-1 loss: 0.635338  [   64/  265]
train() client id: f_00001-7-2 loss: 0.469163  [   96/  265]
train() client id: f_00001-7-3 loss: 0.412148  [  128/  265]
train() client id: f_00001-7-4 loss: 0.540955  [  160/  265]
train() client id: f_00001-7-5 loss: 0.611240  [  192/  265]
train() client id: f_00001-7-6 loss: 0.492165  [  224/  265]
train() client id: f_00001-7-7 loss: 0.433659  [  256/  265]
train() client id: f_00001-8-0 loss: 0.606359  [   32/  265]
train() client id: f_00001-8-1 loss: 0.514884  [   64/  265]
train() client id: f_00001-8-2 loss: 0.481570  [   96/  265]
train() client id: f_00001-8-3 loss: 0.474242  [  128/  265]
train() client id: f_00001-8-4 loss: 0.434556  [  160/  265]
train() client id: f_00001-8-5 loss: 0.650623  [  192/  265]
train() client id: f_00001-8-6 loss: 0.476202  [  224/  265]
train() client id: f_00001-8-7 loss: 0.419359  [  256/  265]
train() client id: f_00001-9-0 loss: 0.542771  [   32/  265]
train() client id: f_00001-9-1 loss: 0.474243  [   64/  265]
train() client id: f_00001-9-2 loss: 0.415510  [   96/  265]
train() client id: f_00001-9-3 loss: 0.596968  [  128/  265]
train() client id: f_00001-9-4 loss: 0.494768  [  160/  265]
train() client id: f_00001-9-5 loss: 0.480712  [  192/  265]
train() client id: f_00001-9-6 loss: 0.500738  [  224/  265]
train() client id: f_00001-9-7 loss: 0.540840  [  256/  265]
train() client id: f_00001-10-0 loss: 0.537418  [   32/  265]
train() client id: f_00001-10-1 loss: 0.423129  [   64/  265]
train() client id: f_00001-10-2 loss: 0.678365  [   96/  265]
train() client id: f_00001-10-3 loss: 0.430186  [  128/  265]
train() client id: f_00001-10-4 loss: 0.534342  [  160/  265]
train() client id: f_00001-10-5 loss: 0.585971  [  192/  265]
train() client id: f_00001-10-6 loss: 0.416578  [  224/  265]
train() client id: f_00001-10-7 loss: 0.501012  [  256/  265]
train() client id: f_00001-11-0 loss: 0.612507  [   32/  265]
train() client id: f_00001-11-1 loss: 0.662750  [   64/  265]
train() client id: f_00001-11-2 loss: 0.467400  [   96/  265]
train() client id: f_00001-11-3 loss: 0.501531  [  128/  265]
train() client id: f_00001-11-4 loss: 0.523769  [  160/  265]
train() client id: f_00001-11-5 loss: 0.429888  [  192/  265]
train() client id: f_00001-11-6 loss: 0.474308  [  224/  265]
train() client id: f_00001-11-7 loss: 0.414006  [  256/  265]
train() client id: f_00002-0-0 loss: 1.242333  [   32/  124]
train() client id: f_00002-0-1 loss: 1.150745  [   64/  124]
train() client id: f_00002-0-2 loss: 1.086125  [   96/  124]
train() client id: f_00002-1-0 loss: 1.208349  [   32/  124]
train() client id: f_00002-1-1 loss: 1.219464  [   64/  124]
train() client id: f_00002-1-2 loss: 1.061659  [   96/  124]
train() client id: f_00002-2-0 loss: 1.075982  [   32/  124]
train() client id: f_00002-2-1 loss: 1.079894  [   64/  124]
train() client id: f_00002-2-2 loss: 1.108470  [   96/  124]
train() client id: f_00002-3-0 loss: 1.036799  [   32/  124]
train() client id: f_00002-3-1 loss: 1.149876  [   64/  124]
train() client id: f_00002-3-2 loss: 1.013694  [   96/  124]
train() client id: f_00002-4-0 loss: 0.996236  [   32/  124]
train() client id: f_00002-4-1 loss: 1.174097  [   64/  124]
train() client id: f_00002-4-2 loss: 1.120200  [   96/  124]
train() client id: f_00002-5-0 loss: 1.097840  [   32/  124]
train() client id: f_00002-5-1 loss: 1.047758  [   64/  124]
train() client id: f_00002-5-2 loss: 1.070057  [   96/  124]
train() client id: f_00002-6-0 loss: 1.121808  [   32/  124]
train() client id: f_00002-6-1 loss: 0.929898  [   64/  124]
train() client id: f_00002-6-2 loss: 1.055246  [   96/  124]
train() client id: f_00002-7-0 loss: 1.125062  [   32/  124]
train() client id: f_00002-7-1 loss: 0.974403  [   64/  124]
train() client id: f_00002-7-2 loss: 0.880289  [   96/  124]
train() client id: f_00002-8-0 loss: 1.146446  [   32/  124]
train() client id: f_00002-8-1 loss: 0.993643  [   64/  124]
train() client id: f_00002-8-2 loss: 0.903320  [   96/  124]
train() client id: f_00002-9-0 loss: 0.936306  [   32/  124]
train() client id: f_00002-9-1 loss: 0.928429  [   64/  124]
train() client id: f_00002-9-2 loss: 1.048508  [   96/  124]
train() client id: f_00002-10-0 loss: 0.997338  [   32/  124]
train() client id: f_00002-10-1 loss: 1.027813  [   64/  124]
train() client id: f_00002-10-2 loss: 1.027441  [   96/  124]
train() client id: f_00002-11-0 loss: 1.056077  [   32/  124]
train() client id: f_00002-11-1 loss: 0.891736  [   64/  124]
train() client id: f_00002-11-2 loss: 1.072291  [   96/  124]
train() client id: f_00003-0-0 loss: 0.601723  [   32/   43]
train() client id: f_00003-1-0 loss: 0.671119  [   32/   43]
train() client id: f_00003-2-0 loss: 0.686924  [   32/   43]
train() client id: f_00003-3-0 loss: 0.573822  [   32/   43]
train() client id: f_00003-4-0 loss: 0.747778  [   32/   43]
train() client id: f_00003-5-0 loss: 0.700743  [   32/   43]
train() client id: f_00003-6-0 loss: 0.649705  [   32/   43]
train() client id: f_00003-7-0 loss: 0.647946  [   32/   43]
train() client id: f_00003-8-0 loss: 0.714317  [   32/   43]
train() client id: f_00003-9-0 loss: 0.707331  [   32/   43]
train() client id: f_00003-10-0 loss: 0.704387  [   32/   43]
train() client id: f_00003-11-0 loss: 0.777989  [   32/   43]
train() client id: f_00004-0-0 loss: 0.868647  [   32/  306]
train() client id: f_00004-0-1 loss: 0.725399  [   64/  306]
train() client id: f_00004-0-2 loss: 0.823474  [   96/  306]
train() client id: f_00004-0-3 loss: 0.884641  [  128/  306]
train() client id: f_00004-0-4 loss: 0.950147  [  160/  306]
train() client id: f_00004-0-5 loss: 0.803470  [  192/  306]
train() client id: f_00004-0-6 loss: 0.980128  [  224/  306]
train() client id: f_00004-0-7 loss: 0.829188  [  256/  306]
train() client id: f_00004-0-8 loss: 0.922767  [  288/  306]
train() client id: f_00004-1-0 loss: 0.842288  [   32/  306]
train() client id: f_00004-1-1 loss: 0.857031  [   64/  306]
train() client id: f_00004-1-2 loss: 0.857280  [   96/  306]
train() client id: f_00004-1-3 loss: 0.834759  [  128/  306]
train() client id: f_00004-1-4 loss: 0.841956  [  160/  306]
train() client id: f_00004-1-5 loss: 0.852034  [  192/  306]
train() client id: f_00004-1-6 loss: 0.850496  [  224/  306]
train() client id: f_00004-1-7 loss: 0.930568  [  256/  306]
train() client id: f_00004-1-8 loss: 0.818658  [  288/  306]
train() client id: f_00004-2-0 loss: 0.843736  [   32/  306]
train() client id: f_00004-2-1 loss: 0.807032  [   64/  306]
train() client id: f_00004-2-2 loss: 0.906858  [   96/  306]
train() client id: f_00004-2-3 loss: 0.836165  [  128/  306]
train() client id: f_00004-2-4 loss: 0.957173  [  160/  306]
train() client id: f_00004-2-5 loss: 0.901701  [  192/  306]
train() client id: f_00004-2-6 loss: 0.714721  [  224/  306]
train() client id: f_00004-2-7 loss: 0.968921  [  256/  306]
train() client id: f_00004-2-8 loss: 0.796870  [  288/  306]
train() client id: f_00004-3-0 loss: 0.775214  [   32/  306]
train() client id: f_00004-3-1 loss: 0.880904  [   64/  306]
train() client id: f_00004-3-2 loss: 0.758657  [   96/  306]
train() client id: f_00004-3-3 loss: 0.875176  [  128/  306]
train() client id: f_00004-3-4 loss: 0.940073  [  160/  306]
train() client id: f_00004-3-5 loss: 0.831562  [  192/  306]
train() client id: f_00004-3-6 loss: 0.940824  [  224/  306]
train() client id: f_00004-3-7 loss: 0.901521  [  256/  306]
train() client id: f_00004-3-8 loss: 0.837574  [  288/  306]
train() client id: f_00004-4-0 loss: 0.882705  [   32/  306]
train() client id: f_00004-4-1 loss: 0.796980  [   64/  306]
train() client id: f_00004-4-2 loss: 0.809526  [   96/  306]
train() client id: f_00004-4-3 loss: 0.877899  [  128/  306]
train() client id: f_00004-4-4 loss: 0.919897  [  160/  306]
train() client id: f_00004-4-5 loss: 0.880132  [  192/  306]
train() client id: f_00004-4-6 loss: 0.825710  [  224/  306]
train() client id: f_00004-4-7 loss: 0.931692  [  256/  306]
train() client id: f_00004-4-8 loss: 0.865585  [  288/  306]
train() client id: f_00004-5-0 loss: 0.879187  [   32/  306]
train() client id: f_00004-5-1 loss: 0.920794  [   64/  306]
train() client id: f_00004-5-2 loss: 1.001641  [   96/  306]
train() client id: f_00004-5-3 loss: 0.764004  [  128/  306]
train() client id: f_00004-5-4 loss: 0.822876  [  160/  306]
train() client id: f_00004-5-5 loss: 0.813690  [  192/  306]
train() client id: f_00004-5-6 loss: 0.757608  [  224/  306]
train() client id: f_00004-5-7 loss: 0.869434  [  256/  306]
train() client id: f_00004-5-8 loss: 0.798993  [  288/  306]
train() client id: f_00004-6-0 loss: 0.835736  [   32/  306]
train() client id: f_00004-6-1 loss: 0.799130  [   64/  306]
train() client id: f_00004-6-2 loss: 0.876874  [   96/  306]
train() client id: f_00004-6-3 loss: 0.903865  [  128/  306]
train() client id: f_00004-6-4 loss: 0.890018  [  160/  306]
train() client id: f_00004-6-5 loss: 0.768239  [  192/  306]
train() client id: f_00004-6-6 loss: 0.798974  [  224/  306]
train() client id: f_00004-6-7 loss: 1.040443  [  256/  306]
train() client id: f_00004-6-8 loss: 0.892889  [  288/  306]
train() client id: f_00004-7-0 loss: 0.833844  [   32/  306]
train() client id: f_00004-7-1 loss: 0.900810  [   64/  306]
train() client id: f_00004-7-2 loss: 0.840274  [   96/  306]
train() client id: f_00004-7-3 loss: 0.978451  [  128/  306]
train() client id: f_00004-7-4 loss: 0.880492  [  160/  306]
train() client id: f_00004-7-5 loss: 0.849240  [  192/  306]
train() client id: f_00004-7-6 loss: 0.791350  [  224/  306]
train() client id: f_00004-7-7 loss: 0.879106  [  256/  306]
train() client id: f_00004-7-8 loss: 0.755292  [  288/  306]
train() client id: f_00004-8-0 loss: 0.887823  [   32/  306]
train() client id: f_00004-8-1 loss: 0.712831  [   64/  306]
train() client id: f_00004-8-2 loss: 0.791351  [   96/  306]
train() client id: f_00004-8-3 loss: 0.872986  [  128/  306]
train() client id: f_00004-8-4 loss: 0.943809  [  160/  306]
train() client id: f_00004-8-5 loss: 0.952599  [  192/  306]
train() client id: f_00004-8-6 loss: 0.981591  [  224/  306]
train() client id: f_00004-8-7 loss: 0.721577  [  256/  306]
train() client id: f_00004-8-8 loss: 0.835841  [  288/  306]
train() client id: f_00004-9-0 loss: 0.843712  [   32/  306]
train() client id: f_00004-9-1 loss: 0.849275  [   64/  306]
train() client id: f_00004-9-2 loss: 0.831548  [   96/  306]
train() client id: f_00004-9-3 loss: 0.897123  [  128/  306]
train() client id: f_00004-9-4 loss: 0.845873  [  160/  306]
train() client id: f_00004-9-5 loss: 0.841093  [  192/  306]
train() client id: f_00004-9-6 loss: 0.792620  [  224/  306]
train() client id: f_00004-9-7 loss: 0.981627  [  256/  306]
train() client id: f_00004-9-8 loss: 0.901280  [  288/  306]
train() client id: f_00004-10-0 loss: 0.944172  [   32/  306]
train() client id: f_00004-10-1 loss: 0.914148  [   64/  306]
train() client id: f_00004-10-2 loss: 0.745201  [   96/  306]
train() client id: f_00004-10-3 loss: 0.834254  [  128/  306]
train() client id: f_00004-10-4 loss: 0.867967  [  160/  306]
train() client id: f_00004-10-5 loss: 0.830235  [  192/  306]
train() client id: f_00004-10-6 loss: 0.828156  [  224/  306]
train() client id: f_00004-10-7 loss: 0.926977  [  256/  306]
train() client id: f_00004-10-8 loss: 0.871633  [  288/  306]
train() client id: f_00004-11-0 loss: 0.865682  [   32/  306]
train() client id: f_00004-11-1 loss: 0.840714  [   64/  306]
train() client id: f_00004-11-2 loss: 0.758535  [   96/  306]
train() client id: f_00004-11-3 loss: 0.862514  [  128/  306]
train() client id: f_00004-11-4 loss: 0.867344  [  160/  306]
train() client id: f_00004-11-5 loss: 0.851922  [  192/  306]
train() client id: f_00004-11-6 loss: 0.981134  [  224/  306]
train() client id: f_00004-11-7 loss: 0.935774  [  256/  306]
train() client id: f_00004-11-8 loss: 0.845941  [  288/  306]
train() client id: f_00005-0-0 loss: 0.418734  [   32/  146]
train() client id: f_00005-0-1 loss: 0.426884  [   64/  146]
train() client id: f_00005-0-2 loss: 0.758232  [   96/  146]
train() client id: f_00005-0-3 loss: 0.731686  [  128/  146]
train() client id: f_00005-1-0 loss: 0.618615  [   32/  146]
train() client id: f_00005-1-1 loss: 0.486035  [   64/  146]
train() client id: f_00005-1-2 loss: 0.490245  [   96/  146]
train() client id: f_00005-1-3 loss: 0.590088  [  128/  146]
train() client id: f_00005-2-0 loss: 0.546339  [   32/  146]
train() client id: f_00005-2-1 loss: 0.579917  [   64/  146]
train() client id: f_00005-2-2 loss: 0.584967  [   96/  146]
train() client id: f_00005-2-3 loss: 0.546432  [  128/  146]
train() client id: f_00005-3-0 loss: 0.668841  [   32/  146]
train() client id: f_00005-3-1 loss: 0.591333  [   64/  146]
train() client id: f_00005-3-2 loss: 0.642248  [   96/  146]
train() client id: f_00005-3-3 loss: 0.423474  [  128/  146]
train() client id: f_00005-4-0 loss: 0.635688  [   32/  146]
train() client id: f_00005-4-1 loss: 0.559097  [   64/  146]
train() client id: f_00005-4-2 loss: 0.588302  [   96/  146]
train() client id: f_00005-4-3 loss: 0.527946  [  128/  146]
train() client id: f_00005-5-0 loss: 0.650495  [   32/  146]
train() client id: f_00005-5-1 loss: 0.485740  [   64/  146]
train() client id: f_00005-5-2 loss: 0.302108  [   96/  146]
train() client id: f_00005-5-3 loss: 0.751580  [  128/  146]
train() client id: f_00005-6-0 loss: 0.606237  [   32/  146]
train() client id: f_00005-6-1 loss: 0.594738  [   64/  146]
train() client id: f_00005-6-2 loss: 0.568527  [   96/  146]
train() client id: f_00005-6-3 loss: 0.527874  [  128/  146]
train() client id: f_00005-7-0 loss: 0.763656  [   32/  146]
train() client id: f_00005-7-1 loss: 0.446389  [   64/  146]
train() client id: f_00005-7-2 loss: 0.645798  [   96/  146]
train() client id: f_00005-7-3 loss: 0.470692  [  128/  146]
train() client id: f_00005-8-0 loss: 0.610845  [   32/  146]
train() client id: f_00005-8-1 loss: 0.679629  [   64/  146]
train() client id: f_00005-8-2 loss: 0.581455  [   96/  146]
train() client id: f_00005-8-3 loss: 0.408612  [  128/  146]
train() client id: f_00005-9-0 loss: 0.583269  [   32/  146]
train() client id: f_00005-9-1 loss: 0.640156  [   64/  146]
train() client id: f_00005-9-2 loss: 0.347388  [   96/  146]
train() client id: f_00005-9-3 loss: 0.445800  [  128/  146]
train() client id: f_00005-10-0 loss: 0.520915  [   32/  146]
train() client id: f_00005-10-1 loss: 0.598306  [   64/  146]
train() client id: f_00005-10-2 loss: 0.707985  [   96/  146]
train() client id: f_00005-10-3 loss: 0.489586  [  128/  146]
train() client id: f_00005-11-0 loss: 0.469213  [   32/  146]
train() client id: f_00005-11-1 loss: 0.591809  [   64/  146]
train() client id: f_00005-11-2 loss: 0.610571  [   96/  146]
train() client id: f_00005-11-3 loss: 0.555987  [  128/  146]
train() client id: f_00006-0-0 loss: 0.568562  [   32/   54]
train() client id: f_00006-1-0 loss: 0.513222  [   32/   54]
train() client id: f_00006-2-0 loss: 0.471256  [   32/   54]
train() client id: f_00006-3-0 loss: 0.509748  [   32/   54]
train() client id: f_00006-4-0 loss: 0.562248  [   32/   54]
train() client id: f_00006-5-0 loss: 0.510349  [   32/   54]
train() client id: f_00006-6-0 loss: 0.522614  [   32/   54]
train() client id: f_00006-7-0 loss: 0.564542  [   32/   54]
train() client id: f_00006-8-0 loss: 0.557578  [   32/   54]
train() client id: f_00006-9-0 loss: 0.568844  [   32/   54]
train() client id: f_00006-10-0 loss: 0.449891  [   32/   54]
train() client id: f_00006-11-0 loss: 0.464049  [   32/   54]
train() client id: f_00007-0-0 loss: 0.656869  [   32/  179]
train() client id: f_00007-0-1 loss: 0.469199  [   64/  179]
train() client id: f_00007-0-2 loss: 0.747415  [   96/  179]
train() client id: f_00007-0-3 loss: 0.632972  [  128/  179]
train() client id: f_00007-0-4 loss: 0.487357  [  160/  179]
train() client id: f_00007-1-0 loss: 0.545948  [   32/  179]
train() client id: f_00007-1-1 loss: 0.714279  [   64/  179]
train() client id: f_00007-1-2 loss: 0.620109  [   96/  179]
train() client id: f_00007-1-3 loss: 0.684546  [  128/  179]
train() client id: f_00007-1-4 loss: 0.475640  [  160/  179]
train() client id: f_00007-2-0 loss: 0.561741  [   32/  179]
train() client id: f_00007-2-1 loss: 0.461076  [   64/  179]
train() client id: f_00007-2-2 loss: 0.677079  [   96/  179]
train() client id: f_00007-2-3 loss: 0.581972  [  128/  179]
train() client id: f_00007-2-4 loss: 0.579913  [  160/  179]
train() client id: f_00007-3-0 loss: 0.568366  [   32/  179]
train() client id: f_00007-3-1 loss: 0.440960  [   64/  179]
train() client id: f_00007-3-2 loss: 0.556819  [   96/  179]
train() client id: f_00007-3-3 loss: 0.640584  [  128/  179]
train() client id: f_00007-3-4 loss: 0.543028  [  160/  179]
train() client id: f_00007-4-0 loss: 0.463965  [   32/  179]
train() client id: f_00007-4-1 loss: 0.631621  [   64/  179]
train() client id: f_00007-4-2 loss: 0.664236  [   96/  179]
train() client id: f_00007-4-3 loss: 0.558064  [  128/  179]
train() client id: f_00007-4-4 loss: 0.522701  [  160/  179]
train() client id: f_00007-5-0 loss: 0.545914  [   32/  179]
train() client id: f_00007-5-1 loss: 0.396827  [   64/  179]
train() client id: f_00007-5-2 loss: 0.694450  [   96/  179]
train() client id: f_00007-5-3 loss: 0.577914  [  128/  179]
train() client id: f_00007-5-4 loss: 0.508841  [  160/  179]
train() client id: f_00007-6-0 loss: 0.583394  [   32/  179]
train() client id: f_00007-6-1 loss: 0.487807  [   64/  179]
train() client id: f_00007-6-2 loss: 0.498855  [   96/  179]
train() client id: f_00007-6-3 loss: 0.573924  [  128/  179]
train() client id: f_00007-6-4 loss: 0.524551  [  160/  179]
train() client id: f_00007-7-0 loss: 0.574900  [   32/  179]
train() client id: f_00007-7-1 loss: 0.760487  [   64/  179]
train() client id: f_00007-7-2 loss: 0.516540  [   96/  179]
train() client id: f_00007-7-3 loss: 0.398334  [  128/  179]
train() client id: f_00007-7-4 loss: 0.425829  [  160/  179]
train() client id: f_00007-8-0 loss: 0.403799  [   32/  179]
train() client id: f_00007-8-1 loss: 0.604710  [   64/  179]
train() client id: f_00007-8-2 loss: 0.581257  [   96/  179]
train() client id: f_00007-8-3 loss: 0.673290  [  128/  179]
train() client id: f_00007-8-4 loss: 0.490175  [  160/  179]
train() client id: f_00007-9-0 loss: 0.495871  [   32/  179]
train() client id: f_00007-9-1 loss: 0.559851  [   64/  179]
train() client id: f_00007-9-2 loss: 0.479659  [   96/  179]
train() client id: f_00007-9-3 loss: 0.553609  [  128/  179]
train() client id: f_00007-9-4 loss: 0.559237  [  160/  179]
train() client id: f_00007-10-0 loss: 0.691136  [   32/  179]
train() client id: f_00007-10-1 loss: 0.461752  [   64/  179]
train() client id: f_00007-10-2 loss: 0.495893  [   96/  179]
train() client id: f_00007-10-3 loss: 0.569349  [  128/  179]
train() client id: f_00007-10-4 loss: 0.499680  [  160/  179]
train() client id: f_00007-11-0 loss: 0.597708  [   32/  179]
train() client id: f_00007-11-1 loss: 0.522232  [   64/  179]
train() client id: f_00007-11-2 loss: 0.593537  [   96/  179]
train() client id: f_00007-11-3 loss: 0.474258  [  128/  179]
train() client id: f_00007-11-4 loss: 0.552431  [  160/  179]
train() client id: f_00008-0-0 loss: 0.745515  [   32/  130]
train() client id: f_00008-0-1 loss: 0.680919  [   64/  130]
train() client id: f_00008-0-2 loss: 0.799453  [   96/  130]
train() client id: f_00008-0-3 loss: 0.784448  [  128/  130]
train() client id: f_00008-1-0 loss: 0.858519  [   32/  130]
train() client id: f_00008-1-1 loss: 0.689268  [   64/  130]
train() client id: f_00008-1-2 loss: 0.699466  [   96/  130]
train() client id: f_00008-1-3 loss: 0.766444  [  128/  130]
train() client id: f_00008-2-0 loss: 0.668540  [   32/  130]
train() client id: f_00008-2-1 loss: 0.903677  [   64/  130]
train() client id: f_00008-2-2 loss: 0.746882  [   96/  130]
train() client id: f_00008-2-3 loss: 0.706744  [  128/  130]
train() client id: f_00008-3-0 loss: 0.684085  [   32/  130]
train() client id: f_00008-3-1 loss: 0.791988  [   64/  130]
train() client id: f_00008-3-2 loss: 0.682706  [   96/  130]
train() client id: f_00008-3-3 loss: 0.825544  [  128/  130]
train() client id: f_00008-4-0 loss: 0.774314  [   32/  130]
train() client id: f_00008-4-1 loss: 0.758227  [   64/  130]
train() client id: f_00008-4-2 loss: 0.800333  [   96/  130]
train() client id: f_00008-4-3 loss: 0.689716  [  128/  130]
train() client id: f_00008-5-0 loss: 0.823230  [   32/  130]
train() client id: f_00008-5-1 loss: 0.754360  [   64/  130]
train() client id: f_00008-5-2 loss: 0.715696  [   96/  130]
train() client id: f_00008-5-3 loss: 0.734595  [  128/  130]
train() client id: f_00008-6-0 loss: 0.658999  [   32/  130]
train() client id: f_00008-6-1 loss: 0.753648  [   64/  130]
train() client id: f_00008-6-2 loss: 0.778633  [   96/  130]
train() client id: f_00008-6-3 loss: 0.828075  [  128/  130]
train() client id: f_00008-7-0 loss: 0.735414  [   32/  130]
train() client id: f_00008-7-1 loss: 0.784621  [   64/  130]
train() client id: f_00008-7-2 loss: 0.695067  [   96/  130]
train() client id: f_00008-7-3 loss: 0.800688  [  128/  130]
train() client id: f_00008-8-0 loss: 0.653930  [   32/  130]
train() client id: f_00008-8-1 loss: 0.747379  [   64/  130]
train() client id: f_00008-8-2 loss: 0.819013  [   96/  130]
train() client id: f_00008-8-3 loss: 0.801142  [  128/  130]
train() client id: f_00008-9-0 loss: 0.820423  [   32/  130]
train() client id: f_00008-9-1 loss: 0.664370  [   64/  130]
train() client id: f_00008-9-2 loss: 0.741817  [   96/  130]
train() client id: f_00008-9-3 loss: 0.789806  [  128/  130]
train() client id: f_00008-10-0 loss: 0.818864  [   32/  130]
train() client id: f_00008-10-1 loss: 0.676752  [   64/  130]
train() client id: f_00008-10-2 loss: 0.742959  [   96/  130]
train() client id: f_00008-10-3 loss: 0.750675  [  128/  130]
train() client id: f_00008-11-0 loss: 0.803299  [   32/  130]
train() client id: f_00008-11-1 loss: 0.800424  [   64/  130]
train() client id: f_00008-11-2 loss: 0.699004  [   96/  130]
train() client id: f_00008-11-3 loss: 0.713309  [  128/  130]
train() client id: f_00009-0-0 loss: 1.141086  [   32/  118]
train() client id: f_00009-0-1 loss: 1.204451  [   64/  118]
train() client id: f_00009-0-2 loss: 1.097052  [   96/  118]
train() client id: f_00009-1-0 loss: 1.120071  [   32/  118]
train() client id: f_00009-1-1 loss: 1.084925  [   64/  118]
train() client id: f_00009-1-2 loss: 1.183890  [   96/  118]
train() client id: f_00009-2-0 loss: 1.081334  [   32/  118]
train() client id: f_00009-2-1 loss: 1.016169  [   64/  118]
train() client id: f_00009-2-2 loss: 1.050743  [   96/  118]
train() client id: f_00009-3-0 loss: 0.990950  [   32/  118]
train() client id: f_00009-3-1 loss: 1.059392  [   64/  118]
train() client id: f_00009-3-2 loss: 1.055155  [   96/  118]
train() client id: f_00009-4-0 loss: 1.099250  [   32/  118]
train() client id: f_00009-4-1 loss: 0.902836  [   64/  118]
train() client id: f_00009-4-2 loss: 1.012541  [   96/  118]
train() client id: f_00009-5-0 loss: 0.947251  [   32/  118]
train() client id: f_00009-5-1 loss: 0.959005  [   64/  118]
train() client id: f_00009-5-2 loss: 1.047690  [   96/  118]
train() client id: f_00009-6-0 loss: 0.921437  [   32/  118]
train() client id: f_00009-6-1 loss: 0.993976  [   64/  118]
train() client id: f_00009-6-2 loss: 0.960554  [   96/  118]
train() client id: f_00009-7-0 loss: 0.994008  [   32/  118]
train() client id: f_00009-7-1 loss: 0.964477  [   64/  118]
train() client id: f_00009-7-2 loss: 0.931029  [   96/  118]
train() client id: f_00009-8-0 loss: 0.978784  [   32/  118]
train() client id: f_00009-8-1 loss: 0.991408  [   64/  118]
train() client id: f_00009-8-2 loss: 0.862389  [   96/  118]
train() client id: f_00009-9-0 loss: 0.850460  [   32/  118]
train() client id: f_00009-9-1 loss: 0.946889  [   64/  118]
train() client id: f_00009-9-2 loss: 1.002803  [   96/  118]
train() client id: f_00009-10-0 loss: 0.974761  [   32/  118]
train() client id: f_00009-10-1 loss: 0.952244  [   64/  118]
train() client id: f_00009-10-2 loss: 0.976181  [   96/  118]
train() client id: f_00009-11-0 loss: 0.932235  [   32/  118]
train() client id: f_00009-11-1 loss: 1.012481  [   64/  118]
train() client id: f_00009-11-2 loss: 0.883638  [   96/  118]
At round 20 accuracy: 0.6339522546419099
At round 20 training accuracy: 0.5781354795439303
At round 20 training loss: 0.8426500333114548
update_location
xs = [ -3.9056584    4.20031788 120.00902392  18.81129433   0.97929623
   3.95640986 -82.44319194 -61.32485185 104.66397685 -47.06087855]
ys = [112.5879595   95.55583871   1.32061395 -82.45517586  74.35018685
  57.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [150.63632627 138.37832555 156.21750812 130.96839627 124.61504446
 115.57737324 129.62935794 117.30905212 145.81912777 110.59266769]
dists_bs = [182.12231199 196.02073536 342.3373385  322.25143194 202.72912368
 213.91100929 200.2932794  207.99012902 320.93061214 213.56243122]
uav_gains = [3.58669488e-11 4.43787932e-11 3.27265969e-11 5.09343950e-11
 5.76816207e-11 6.96314375e-11 5.22611087e-11 6.70896685e-11
 3.89180544e-11 7.77458897e-11]
bs_gains = [5.17937375e-11 4.21549371e-11 8.84753174e-12 1.04796913e-11
 3.83644419e-11 3.30096791e-11 3.96851685e-11 3.57087343e-11
 1.06009035e-11 3.31607609e-11]
Round 21
-------------------------------
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.22040415 17.10262761  8.09588073  2.90155574 19.72744601  9.50226703
  3.60411444 11.58945304  8.53512887  7.71174179]
obj_prev = 96.99061941020106
eta_min = 5.053940464590129e-12	eta_max = 0.9225775474279215
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 22.544522121240533	eta = 0.9090909090909091
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 39.50145864601099	eta = 0.5188421089429398
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 31.352662916909924	eta = 0.6536931221610769
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.88807124482688	eta = 0.6857257513318464
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81462162317344	eta = 0.6874150666493414
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81442331258453	eta = 0.6874196389895562
eta = 0.6874196389895562
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.03092845 0.06504793 0.03043752 0.01055495 0.07511195 0.03583773
 0.01325505 0.04393804 0.03191031 0.02896474]
ene_total = [2.59558483 4.86886559 2.57325892 1.19041384 5.55486377 2.93485376
 1.36920185 3.40632189 2.84825067 2.47280818]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 0 obj = 5.270771946473119
eta = 0.6874196389895562
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
eta_min = 0.6874196389895763	eta_max = 0.6874196389895456
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 0.029002321139164264	eta = 0.9090909090909091
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 18.041876240655995	eta = 0.0014613638924501724
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.8124320127004978	eta = 0.014547164420730322
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.775015665989668	eta = 0.01485381058620065
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.7750098831933823	eta = 0.01485385897835976
eta = 0.01485385897835976
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.65835243e-04 1.51995270e-03 1.59487963e-04 6.39002520e-06
 2.36032919e-03 2.60104392e-04 1.26291038e-05 4.51290031e-04
 2.14529013e-04 1.37257984e-04]
ene_total = [0.1681456  0.19135646 0.17138602 0.15296698 0.2125038  0.17320668
 0.15230083 0.15429171 0.22843322 0.17041859]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 1 obj = 5.270771946473455
eta = 0.6874196389895763
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
Done!
At round 21 energy consumption: 5.270771946473119
At round 21 eta: 0.6874196389895763
At round 21 local rounds: 12.273194960737252
At round 21 global rounds: 67.14798076901184
At round 21 a_n: 20.646594222929394
gradient difference: 0.4507664740085602
train() client id: f_00000-0-0 loss: 1.455952  [   32/  126]
train() client id: f_00000-0-1 loss: 1.294313  [   64/  126]
train() client id: f_00000-0-2 loss: 1.269176  [   96/  126]
train() client id: f_00000-1-0 loss: 1.128790  [   32/  126]
train() client id: f_00000-1-1 loss: 1.245954  [   64/  126]
train() client id: f_00000-1-2 loss: 1.205369  [   96/  126]
train() client id: f_00000-2-0 loss: 1.092546  [   32/  126]
train() client id: f_00000-2-1 loss: 1.246671  [   64/  126]
train() client id: f_00000-2-2 loss: 0.979745  [   96/  126]
train() client id: f_00000-3-0 loss: 1.054118  [   32/  126]
train() client id: f_00000-3-1 loss: 0.932700  [   64/  126]
train() client id: f_00000-3-2 loss: 1.039326  [   96/  126]
train() client id: f_00000-4-0 loss: 0.936390  [   32/  126]
train() client id: f_00000-4-1 loss: 0.998996  [   64/  126]
train() client id: f_00000-4-2 loss: 0.960346  [   96/  126]
train() client id: f_00000-5-0 loss: 0.951973  [   32/  126]
train() client id: f_00000-5-1 loss: 0.866879  [   64/  126]
train() client id: f_00000-5-2 loss: 0.873466  [   96/  126]
train() client id: f_00000-6-0 loss: 0.854521  [   32/  126]
train() client id: f_00000-6-1 loss: 0.815493  [   64/  126]
train() client id: f_00000-6-2 loss: 0.937470  [   96/  126]
train() client id: f_00000-7-0 loss: 0.872025  [   32/  126]
train() client id: f_00000-7-1 loss: 0.804306  [   64/  126]
train() client id: f_00000-7-2 loss: 0.935439  [   96/  126]
train() client id: f_00000-8-0 loss: 0.857370  [   32/  126]
train() client id: f_00000-8-1 loss: 0.851721  [   64/  126]
train() client id: f_00000-8-2 loss: 0.863861  [   96/  126]
train() client id: f_00000-9-0 loss: 0.820173  [   32/  126]
train() client id: f_00000-9-1 loss: 0.859185  [   64/  126]
train() client id: f_00000-9-2 loss: 0.826257  [   96/  126]
train() client id: f_00000-10-0 loss: 0.666222  [   32/  126]
train() client id: f_00000-10-1 loss: 0.910184  [   64/  126]
train() client id: f_00000-10-2 loss: 0.892480  [   96/  126]
train() client id: f_00000-11-0 loss: 0.787990  [   32/  126]
train() client id: f_00000-11-1 loss: 0.751030  [   64/  126]
train() client id: f_00000-11-2 loss: 0.912283  [   96/  126]
train() client id: f_00001-0-0 loss: 0.534500  [   32/  265]
train() client id: f_00001-0-1 loss: 0.455737  [   64/  265]
train() client id: f_00001-0-2 loss: 0.510832  [   96/  265]
train() client id: f_00001-0-3 loss: 0.529417  [  128/  265]
train() client id: f_00001-0-4 loss: 0.450726  [  160/  265]
train() client id: f_00001-0-5 loss: 0.589278  [  192/  265]
train() client id: f_00001-0-6 loss: 0.534601  [  224/  265]
train() client id: f_00001-0-7 loss: 0.491296  [  256/  265]
train() client id: f_00001-1-0 loss: 0.452497  [   32/  265]
train() client id: f_00001-1-1 loss: 0.586547  [   64/  265]
train() client id: f_00001-1-2 loss: 0.469825  [   96/  265]
train() client id: f_00001-1-3 loss: 0.418816  [  128/  265]
train() client id: f_00001-1-4 loss: 0.552612  [  160/  265]
train() client id: f_00001-1-5 loss: 0.605997  [  192/  265]
train() client id: f_00001-1-6 loss: 0.523763  [  224/  265]
train() client id: f_00001-1-7 loss: 0.478211  [  256/  265]
train() client id: f_00001-2-0 loss: 0.610635  [   32/  265]
train() client id: f_00001-2-1 loss: 0.521273  [   64/  265]
train() client id: f_00001-2-2 loss: 0.595696  [   96/  265]
train() client id: f_00001-2-3 loss: 0.563354  [  128/  265]
train() client id: f_00001-2-4 loss: 0.459242  [  160/  265]
train() client id: f_00001-2-5 loss: 0.389783  [  192/  265]
train() client id: f_00001-2-6 loss: 0.473369  [  224/  265]
train() client id: f_00001-2-7 loss: 0.416966  [  256/  265]
train() client id: f_00001-3-0 loss: 0.597067  [   32/  265]
train() client id: f_00001-3-1 loss: 0.413020  [   64/  265]
train() client id: f_00001-3-2 loss: 0.446429  [   96/  265]
train() client id: f_00001-3-3 loss: 0.594478  [  128/  265]
train() client id: f_00001-3-4 loss: 0.548819  [  160/  265]
train() client id: f_00001-3-5 loss: 0.444859  [  192/  265]
train() client id: f_00001-3-6 loss: 0.445506  [  224/  265]
train() client id: f_00001-3-7 loss: 0.479460  [  256/  265]
train() client id: f_00001-4-0 loss: 0.433498  [   32/  265]
train() client id: f_00001-4-1 loss: 0.441530  [   64/  265]
train() client id: f_00001-4-2 loss: 0.576442  [   96/  265]
train() client id: f_00001-4-3 loss: 0.580456  [  128/  265]
train() client id: f_00001-4-4 loss: 0.527462  [  160/  265]
train() client id: f_00001-4-5 loss: 0.422404  [  192/  265]
train() client id: f_00001-4-6 loss: 0.471454  [  224/  265]
train() client id: f_00001-4-7 loss: 0.530191  [  256/  265]
train() client id: f_00001-5-0 loss: 0.483193  [   32/  265]
train() client id: f_00001-5-1 loss: 0.426758  [   64/  265]
train() client id: f_00001-5-2 loss: 0.598985  [   96/  265]
train() client id: f_00001-5-3 loss: 0.454814  [  128/  265]
train() client id: f_00001-5-4 loss: 0.390312  [  160/  265]
train() client id: f_00001-5-5 loss: 0.518705  [  192/  265]
train() client id: f_00001-5-6 loss: 0.571380  [  224/  265]
train() client id: f_00001-5-7 loss: 0.453713  [  256/  265]
train() client id: f_00001-6-0 loss: 0.774396  [   32/  265]
train() client id: f_00001-6-1 loss: 0.401468  [   64/  265]
train() client id: f_00001-6-2 loss: 0.454321  [   96/  265]
train() client id: f_00001-6-3 loss: 0.398378  [  128/  265]
train() client id: f_00001-6-4 loss: 0.471116  [  160/  265]
train() client id: f_00001-6-5 loss: 0.550154  [  192/  265]
train() client id: f_00001-6-6 loss: 0.478273  [  224/  265]
train() client id: f_00001-6-7 loss: 0.431594  [  256/  265]
train() client id: f_00001-7-0 loss: 0.396379  [   32/  265]
train() client id: f_00001-7-1 loss: 0.521793  [   64/  265]
train() client id: f_00001-7-2 loss: 0.524058  [   96/  265]
train() client id: f_00001-7-3 loss: 0.574725  [  128/  265]
train() client id: f_00001-7-4 loss: 0.526867  [  160/  265]
train() client id: f_00001-7-5 loss: 0.376899  [  192/  265]
train() client id: f_00001-7-6 loss: 0.451366  [  224/  265]
train() client id: f_00001-7-7 loss: 0.515342  [  256/  265]
train() client id: f_00001-8-0 loss: 0.386636  [   32/  265]
train() client id: f_00001-8-1 loss: 0.525730  [   64/  265]
train() client id: f_00001-8-2 loss: 0.496052  [   96/  265]
train() client id: f_00001-8-3 loss: 0.503205  [  128/  265]
train() client id: f_00001-8-4 loss: 0.439204  [  160/  265]
train() client id: f_00001-8-5 loss: 0.502436  [  192/  265]
train() client id: f_00001-8-6 loss: 0.521114  [  224/  265]
train() client id: f_00001-8-7 loss: 0.558630  [  256/  265]
train() client id: f_00001-9-0 loss: 0.401413  [   32/  265]
train() client id: f_00001-9-1 loss: 0.532746  [   64/  265]
train() client id: f_00001-9-2 loss: 0.572909  [   96/  265]
train() client id: f_00001-9-3 loss: 0.489307  [  128/  265]
train() client id: f_00001-9-4 loss: 0.508355  [  160/  265]
train() client id: f_00001-9-5 loss: 0.445992  [  192/  265]
train() client id: f_00001-9-6 loss: 0.466069  [  224/  265]
train() client id: f_00001-9-7 loss: 0.523521  [  256/  265]
train() client id: f_00001-10-0 loss: 0.551453  [   32/  265]
train() client id: f_00001-10-1 loss: 0.544312  [   64/  265]
train() client id: f_00001-10-2 loss: 0.467621  [   96/  265]
train() client id: f_00001-10-3 loss: 0.453226  [  128/  265]
train() client id: f_00001-10-4 loss: 0.448686  [  160/  265]
train() client id: f_00001-10-5 loss: 0.532593  [  192/  265]
train() client id: f_00001-10-6 loss: 0.410759  [  224/  265]
train() client id: f_00001-10-7 loss: 0.531363  [  256/  265]
train() client id: f_00001-11-0 loss: 0.483399  [   32/  265]
train() client id: f_00001-11-1 loss: 0.535250  [   64/  265]
train() client id: f_00001-11-2 loss: 0.423969  [   96/  265]
train() client id: f_00001-11-3 loss: 0.494622  [  128/  265]
train() client id: f_00001-11-4 loss: 0.520267  [  160/  265]
train() client id: f_00001-11-5 loss: 0.473102  [  192/  265]
train() client id: f_00001-11-6 loss: 0.505230  [  224/  265]
train() client id: f_00001-11-7 loss: 0.458515  [  256/  265]
train() client id: f_00002-0-0 loss: 1.112756  [   32/  124]
train() client id: f_00002-0-1 loss: 1.134884  [   64/  124]
train() client id: f_00002-0-2 loss: 1.152083  [   96/  124]
train() client id: f_00002-1-0 loss: 1.126850  [   32/  124]
train() client id: f_00002-1-1 loss: 1.173087  [   64/  124]
train() client id: f_00002-1-2 loss: 1.074913  [   96/  124]
train() client id: f_00002-2-0 loss: 1.105723  [   32/  124]
train() client id: f_00002-2-1 loss: 1.042644  [   64/  124]
train() client id: f_00002-2-2 loss: 1.128551  [   96/  124]
train() client id: f_00002-3-0 loss: 1.061711  [   32/  124]
train() client id: f_00002-3-1 loss: 0.957699  [   64/  124]
train() client id: f_00002-3-2 loss: 1.025240  [   96/  124]
train() client id: f_00002-4-0 loss: 1.084078  [   32/  124]
train() client id: f_00002-4-1 loss: 0.882758  [   64/  124]
train() client id: f_00002-4-2 loss: 1.051528  [   96/  124]
train() client id: f_00002-5-0 loss: 1.051760  [   32/  124]
train() client id: f_00002-5-1 loss: 0.947939  [   64/  124]
train() client id: f_00002-5-2 loss: 0.927546  [   96/  124]
train() client id: f_00002-6-0 loss: 0.909513  [   32/  124]
train() client id: f_00002-6-1 loss: 0.902395  [   64/  124]
train() client id: f_00002-6-2 loss: 1.068185  [   96/  124]
train() client id: f_00002-7-0 loss: 0.876662  [   32/  124]
train() client id: f_00002-7-1 loss: 0.902797  [   64/  124]
train() client id: f_00002-7-2 loss: 1.028826  [   96/  124]
train() client id: f_00002-8-0 loss: 1.012194  [   32/  124]
train() client id: f_00002-8-1 loss: 0.942842  [   64/  124]
train() client id: f_00002-8-2 loss: 0.922912  [   96/  124]
train() client id: f_00002-9-0 loss: 0.971167  [   32/  124]
train() client id: f_00002-9-1 loss: 0.853076  [   64/  124]
train() client id: f_00002-9-2 loss: 1.046217  [   96/  124]
train() client id: f_00002-10-0 loss: 0.933002  [   32/  124]
train() client id: f_00002-10-1 loss: 0.967528  [   64/  124]
train() client id: f_00002-10-2 loss: 0.928716  [   96/  124]
train() client id: f_00002-11-0 loss: 0.853152  [   32/  124]
train() client id: f_00002-11-1 loss: 0.923095  [   64/  124]
train() client id: f_00002-11-2 loss: 1.005234  [   96/  124]
train() client id: f_00003-0-0 loss: 0.616068  [   32/   43]
train() client id: f_00003-1-0 loss: 0.476332  [   32/   43]
train() client id: f_00003-2-0 loss: 0.609647  [   32/   43]
train() client id: f_00003-3-0 loss: 0.593088  [   32/   43]
train() client id: f_00003-4-0 loss: 0.671482  [   32/   43]
train() client id: f_00003-5-0 loss: 0.640550  [   32/   43]
train() client id: f_00003-6-0 loss: 0.667377  [   32/   43]
train() client id: f_00003-7-0 loss: 0.642184  [   32/   43]
train() client id: f_00003-8-0 loss: 0.616784  [   32/   43]
train() client id: f_00003-9-0 loss: 0.626425  [   32/   43]
train() client id: f_00003-10-0 loss: 0.706424  [   32/   43]
train() client id: f_00003-11-0 loss: 0.577493  [   32/   43]
train() client id: f_00004-0-0 loss: 1.042007  [   32/  306]
train() client id: f_00004-0-1 loss: 0.813877  [   64/  306]
train() client id: f_00004-0-2 loss: 0.911078  [   96/  306]
train() client id: f_00004-0-3 loss: 0.979241  [  128/  306]
train() client id: f_00004-0-4 loss: 0.820121  [  160/  306]
train() client id: f_00004-0-5 loss: 0.844831  [  192/  306]
train() client id: f_00004-0-6 loss: 0.905604  [  224/  306]
train() client id: f_00004-0-7 loss: 0.913975  [  256/  306]
train() client id: f_00004-0-8 loss: 0.954979  [  288/  306]
train() client id: f_00004-1-0 loss: 0.785756  [   32/  306]
train() client id: f_00004-1-1 loss: 1.025124  [   64/  306]
train() client id: f_00004-1-2 loss: 0.924462  [   96/  306]
train() client id: f_00004-1-3 loss: 0.941380  [  128/  306]
train() client id: f_00004-1-4 loss: 0.908656  [  160/  306]
train() client id: f_00004-1-5 loss: 0.786929  [  192/  306]
train() client id: f_00004-1-6 loss: 0.949827  [  224/  306]
train() client id: f_00004-1-7 loss: 0.951249  [  256/  306]
train() client id: f_00004-1-8 loss: 0.985267  [  288/  306]
train() client id: f_00004-2-0 loss: 0.909655  [   32/  306]
train() client id: f_00004-2-1 loss: 0.939387  [   64/  306]
train() client id: f_00004-2-2 loss: 0.862109  [   96/  306]
train() client id: f_00004-2-3 loss: 0.893082  [  128/  306]
train() client id: f_00004-2-4 loss: 0.888265  [  160/  306]
train() client id: f_00004-2-5 loss: 0.891708  [  192/  306]
train() client id: f_00004-2-6 loss: 0.830833  [  224/  306]
train() client id: f_00004-2-7 loss: 0.956611  [  256/  306]
train() client id: f_00004-2-8 loss: 1.021946  [  288/  306]
train() client id: f_00004-3-0 loss: 0.874755  [   32/  306]
train() client id: f_00004-3-1 loss: 0.902111  [   64/  306]
train() client id: f_00004-3-2 loss: 0.881216  [   96/  306]
train() client id: f_00004-3-3 loss: 0.835272  [  128/  306]
train() client id: f_00004-3-4 loss: 0.938378  [  160/  306]
train() client id: f_00004-3-5 loss: 0.979707  [  192/  306]
train() client id: f_00004-3-6 loss: 1.006150  [  224/  306]
train() client id: f_00004-3-7 loss: 0.859309  [  256/  306]
train() client id: f_00004-3-8 loss: 0.901211  [  288/  306]
train() client id: f_00004-4-0 loss: 0.921037  [   32/  306]
train() client id: f_00004-4-1 loss: 0.839304  [   64/  306]
train() client id: f_00004-4-2 loss: 0.971341  [   96/  306]
train() client id: f_00004-4-3 loss: 0.795966  [  128/  306]
train() client id: f_00004-4-4 loss: 0.931985  [  160/  306]
train() client id: f_00004-4-5 loss: 0.742735  [  192/  306]
train() client id: f_00004-4-6 loss: 0.892998  [  224/  306]
train() client id: f_00004-4-7 loss: 1.011323  [  256/  306]
train() client id: f_00004-4-8 loss: 1.024354  [  288/  306]
train() client id: f_00004-5-0 loss: 0.856024  [   32/  306]
train() client id: f_00004-5-1 loss: 0.994746  [   64/  306]
train() client id: f_00004-5-2 loss: 0.982778  [   96/  306]
train() client id: f_00004-5-3 loss: 0.831417  [  128/  306]
train() client id: f_00004-5-4 loss: 0.840585  [  160/  306]
train() client id: f_00004-5-5 loss: 0.835266  [  192/  306]
train() client id: f_00004-5-6 loss: 1.070405  [  224/  306]
train() client id: f_00004-5-7 loss: 0.812777  [  256/  306]
train() client id: f_00004-5-8 loss: 0.968738  [  288/  306]
train() client id: f_00004-6-0 loss: 0.901196  [   32/  306]
train() client id: f_00004-6-1 loss: 1.044670  [   64/  306]
train() client id: f_00004-6-2 loss: 1.049730  [   96/  306]
train() client id: f_00004-6-3 loss: 0.774032  [  128/  306]
train() client id: f_00004-6-4 loss: 0.752748  [  160/  306]
train() client id: f_00004-6-5 loss: 0.857364  [  192/  306]
train() client id: f_00004-6-6 loss: 0.819274  [  224/  306]
train() client id: f_00004-6-7 loss: 0.904090  [  256/  306]
train() client id: f_00004-6-8 loss: 1.010440  [  288/  306]
train() client id: f_00004-7-0 loss: 0.875422  [   32/  306]
train() client id: f_00004-7-1 loss: 0.950098  [   64/  306]
train() client id: f_00004-7-2 loss: 0.971972  [   96/  306]
train() client id: f_00004-7-3 loss: 0.916252  [  128/  306]
train() client id: f_00004-7-4 loss: 0.874492  [  160/  306]
train() client id: f_00004-7-5 loss: 0.923762  [  192/  306]
train() client id: f_00004-7-6 loss: 0.941153  [  224/  306]
train() client id: f_00004-7-7 loss: 0.888965  [  256/  306]
train() client id: f_00004-7-8 loss: 0.801373  [  288/  306]
train() client id: f_00004-8-0 loss: 0.802807  [   32/  306]
train() client id: f_00004-8-1 loss: 1.007127  [   64/  306]
train() client id: f_00004-8-2 loss: 0.930996  [   96/  306]
train() client id: f_00004-8-3 loss: 0.914298  [  128/  306]
train() client id: f_00004-8-4 loss: 0.988361  [  160/  306]
train() client id: f_00004-8-5 loss: 0.896756  [  192/  306]
train() client id: f_00004-8-6 loss: 0.729403  [  224/  306]
train() client id: f_00004-8-7 loss: 0.991404  [  256/  306]
train() client id: f_00004-8-8 loss: 0.887100  [  288/  306]
train() client id: f_00004-9-0 loss: 0.948915  [   32/  306]
train() client id: f_00004-9-1 loss: 0.986685  [   64/  306]
train() client id: f_00004-9-2 loss: 0.858576  [   96/  306]
train() client id: f_00004-9-3 loss: 0.902803  [  128/  306]
train() client id: f_00004-9-4 loss: 0.890294  [  160/  306]
train() client id: f_00004-9-5 loss: 0.833998  [  192/  306]
train() client id: f_00004-9-6 loss: 0.913721  [  224/  306]
train() client id: f_00004-9-7 loss: 0.898903  [  256/  306]
train() client id: f_00004-9-8 loss: 0.827806  [  288/  306]
train() client id: f_00004-10-0 loss: 0.909919  [   32/  306]
train() client id: f_00004-10-1 loss: 0.955115  [   64/  306]
train() client id: f_00004-10-2 loss: 0.873107  [   96/  306]
train() client id: f_00004-10-3 loss: 0.738255  [  128/  306]
train() client id: f_00004-10-4 loss: 0.858934  [  160/  306]
train() client id: f_00004-10-5 loss: 0.932968  [  192/  306]
train() client id: f_00004-10-6 loss: 0.947093  [  224/  306]
train() client id: f_00004-10-7 loss: 0.916415  [  256/  306]
train() client id: f_00004-10-8 loss: 0.951581  [  288/  306]
train() client id: f_00004-11-0 loss: 0.972595  [   32/  306]
train() client id: f_00004-11-1 loss: 0.820924  [   64/  306]
train() client id: f_00004-11-2 loss: 0.891265  [   96/  306]
train() client id: f_00004-11-3 loss: 0.876971  [  128/  306]
train() client id: f_00004-11-4 loss: 0.942323  [  160/  306]
train() client id: f_00004-11-5 loss: 0.868513  [  192/  306]
train() client id: f_00004-11-6 loss: 0.829169  [  224/  306]
train() client id: f_00004-11-7 loss: 0.920039  [  256/  306]
train() client id: f_00004-11-8 loss: 0.914979  [  288/  306]
train() client id: f_00005-0-0 loss: 0.596780  [   32/  146]
train() client id: f_00005-0-1 loss: 0.800341  [   64/  146]
train() client id: f_00005-0-2 loss: 0.539843  [   96/  146]
train() client id: f_00005-0-3 loss: 0.727679  [  128/  146]
train() client id: f_00005-1-0 loss: 0.572863  [   32/  146]
train() client id: f_00005-1-1 loss: 0.804137  [   64/  146]
train() client id: f_00005-1-2 loss: 0.460952  [   96/  146]
train() client id: f_00005-1-3 loss: 0.601313  [  128/  146]
train() client id: f_00005-2-0 loss: 0.724699  [   32/  146]
train() client id: f_00005-2-1 loss: 0.758812  [   64/  146]
train() client id: f_00005-2-2 loss: 0.540644  [   96/  146]
train() client id: f_00005-2-3 loss: 0.711910  [  128/  146]
train() client id: f_00005-3-0 loss: 0.597299  [   32/  146]
train() client id: f_00005-3-1 loss: 0.659262  [   64/  146]
train() client id: f_00005-3-2 loss: 0.577521  [   96/  146]
train() client id: f_00005-3-3 loss: 0.821167  [  128/  146]
train() client id: f_00005-4-0 loss: 0.742629  [   32/  146]
train() client id: f_00005-4-1 loss: 0.860308  [   64/  146]
train() client id: f_00005-4-2 loss: 0.575628  [   96/  146]
train() client id: f_00005-4-3 loss: 0.415162  [  128/  146]
train() client id: f_00005-5-0 loss: 0.660215  [   32/  146]
train() client id: f_00005-5-1 loss: 0.702670  [   64/  146]
train() client id: f_00005-5-2 loss: 0.779030  [   96/  146]
train() client id: f_00005-5-3 loss: 0.582741  [  128/  146]
train() client id: f_00005-6-0 loss: 0.775850  [   32/  146]
train() client id: f_00005-6-1 loss: 0.685997  [   64/  146]
train() client id: f_00005-6-2 loss: 0.607523  [   96/  146]
train() client id: f_00005-6-3 loss: 0.612264  [  128/  146]
train() client id: f_00005-7-0 loss: 0.700532  [   32/  146]
train() client id: f_00005-7-1 loss: 0.538075  [   64/  146]
train() client id: f_00005-7-2 loss: 0.635307  [   96/  146]
train() client id: f_00005-7-3 loss: 0.627750  [  128/  146]
train() client id: f_00005-8-0 loss: 0.482891  [   32/  146]
train() client id: f_00005-8-1 loss: 0.881550  [   64/  146]
train() client id: f_00005-8-2 loss: 0.488578  [   96/  146]
train() client id: f_00005-8-3 loss: 0.763527  [  128/  146]
train() client id: f_00005-9-0 loss: 0.812111  [   32/  146]
train() client id: f_00005-9-1 loss: 0.675298  [   64/  146]
train() client id: f_00005-9-2 loss: 0.696844  [   96/  146]
train() client id: f_00005-9-3 loss: 0.529805  [  128/  146]
train() client id: f_00005-10-0 loss: 0.809950  [   32/  146]
train() client id: f_00005-10-1 loss: 0.629725  [   64/  146]
train() client id: f_00005-10-2 loss: 0.586617  [   96/  146]
train() client id: f_00005-10-3 loss: 0.585825  [  128/  146]
train() client id: f_00005-11-0 loss: 0.415664  [   32/  146]
train() client id: f_00005-11-1 loss: 0.700529  [   64/  146]
train() client id: f_00005-11-2 loss: 0.727850  [   96/  146]
train() client id: f_00005-11-3 loss: 0.814531  [  128/  146]
train() client id: f_00006-0-0 loss: 0.589547  [   32/   54]
train() client id: f_00006-1-0 loss: 0.606122  [   32/   54]
train() client id: f_00006-2-0 loss: 0.602079  [   32/   54]
train() client id: f_00006-3-0 loss: 0.638766  [   32/   54]
train() client id: f_00006-4-0 loss: 0.658031  [   32/   54]
train() client id: f_00006-5-0 loss: 0.647174  [   32/   54]
train() client id: f_00006-6-0 loss: 0.594081  [   32/   54]
train() client id: f_00006-7-0 loss: 0.598773  [   32/   54]
train() client id: f_00006-8-0 loss: 0.587506  [   32/   54]
train() client id: f_00006-9-0 loss: 0.550317  [   32/   54]
train() client id: f_00006-10-0 loss: 0.572627  [   32/   54]
train() client id: f_00006-11-0 loss: 0.596976  [   32/   54]
train() client id: f_00007-0-0 loss: 0.550127  [   32/  179]
train() client id: f_00007-0-1 loss: 0.709025  [   64/  179]
train() client id: f_00007-0-2 loss: 0.605056  [   96/  179]
train() client id: f_00007-0-3 loss: 0.668765  [  128/  179]
train() client id: f_00007-0-4 loss: 0.465017  [  160/  179]
train() client id: f_00007-1-0 loss: 0.479205  [   32/  179]
train() client id: f_00007-1-1 loss: 0.486734  [   64/  179]
train() client id: f_00007-1-2 loss: 0.598360  [   96/  179]
train() client id: f_00007-1-3 loss: 0.476525  [  128/  179]
train() client id: f_00007-1-4 loss: 0.797112  [  160/  179]
train() client id: f_00007-2-0 loss: 0.580757  [   32/  179]
train() client id: f_00007-2-1 loss: 0.437505  [   64/  179]
train() client id: f_00007-2-2 loss: 0.515131  [   96/  179]
train() client id: f_00007-2-3 loss: 0.677699  [  128/  179]
train() client id: f_00007-2-4 loss: 0.582387  [  160/  179]
train() client id: f_00007-3-0 loss: 0.578386  [   32/  179]
train() client id: f_00007-3-1 loss: 0.454668  [   64/  179]
train() client id: f_00007-3-2 loss: 0.589459  [   96/  179]
train() client id: f_00007-3-3 loss: 0.574424  [  128/  179]
train() client id: f_00007-3-4 loss: 0.593323  [  160/  179]
train() client id: f_00007-4-0 loss: 0.497448  [   32/  179]
train() client id: f_00007-4-1 loss: 0.548055  [   64/  179]
train() client id: f_00007-4-2 loss: 0.594533  [   96/  179]
train() client id: f_00007-4-3 loss: 0.492617  [  128/  179]
train() client id: f_00007-4-4 loss: 0.632341  [  160/  179]
train() client id: f_00007-5-0 loss: 0.603228  [   32/  179]
train() client id: f_00007-5-1 loss: 0.399081  [   64/  179]
train() client id: f_00007-5-2 loss: 0.698749  [   96/  179]
train() client id: f_00007-5-3 loss: 0.403468  [  128/  179]
train() client id: f_00007-5-4 loss: 0.503476  [  160/  179]
train() client id: f_00007-6-0 loss: 0.468853  [   32/  179]
train() client id: f_00007-6-1 loss: 0.498790  [   64/  179]
train() client id: f_00007-6-2 loss: 0.531468  [   96/  179]
train() client id: f_00007-6-3 loss: 0.492724  [  128/  179]
train() client id: f_00007-6-4 loss: 0.718827  [  160/  179]
train() client id: f_00007-7-0 loss: 0.615526  [   32/  179]
train() client id: f_00007-7-1 loss: 0.477532  [   64/  179]
train() client id: f_00007-7-2 loss: 0.600307  [   96/  179]
train() client id: f_00007-7-3 loss: 0.413940  [  128/  179]
train() client id: f_00007-7-4 loss: 0.580565  [  160/  179]
train() client id: f_00007-8-0 loss: 0.478120  [   32/  179]
train() client id: f_00007-8-1 loss: 0.565020  [   64/  179]
train() client id: f_00007-8-2 loss: 0.526890  [   96/  179]
train() client id: f_00007-8-3 loss: 0.502781  [  128/  179]
train() client id: f_00007-8-4 loss: 0.489151  [  160/  179]
train() client id: f_00007-9-0 loss: 0.549447  [   32/  179]
train() client id: f_00007-9-1 loss: 0.576319  [   64/  179]
train() client id: f_00007-9-2 loss: 0.379132  [   96/  179]
train() client id: f_00007-9-3 loss: 0.485014  [  128/  179]
train() client id: f_00007-9-4 loss: 0.457106  [  160/  179]
train() client id: f_00007-10-0 loss: 0.643369  [   32/  179]
train() client id: f_00007-10-1 loss: 0.523758  [   64/  179]
train() client id: f_00007-10-2 loss: 0.445372  [   96/  179]
train() client id: f_00007-10-3 loss: 0.553691  [  128/  179]
train() client id: f_00007-10-4 loss: 0.447183  [  160/  179]
train() client id: f_00007-11-0 loss: 0.447396  [   32/  179]
train() client id: f_00007-11-1 loss: 0.458731  [   64/  179]
train() client id: f_00007-11-2 loss: 0.617034  [   96/  179]
train() client id: f_00007-11-3 loss: 0.448938  [  128/  179]
train() client id: f_00007-11-4 loss: 0.584537  [  160/  179]
train() client id: f_00008-0-0 loss: 0.779698  [   32/  130]
train() client id: f_00008-0-1 loss: 0.809180  [   64/  130]
train() client id: f_00008-0-2 loss: 0.854482  [   96/  130]
train() client id: f_00008-0-3 loss: 0.644155  [  128/  130]
train() client id: f_00008-1-0 loss: 0.733190  [   32/  130]
train() client id: f_00008-1-1 loss: 0.767044  [   64/  130]
train() client id: f_00008-1-2 loss: 0.893908  [   96/  130]
train() client id: f_00008-1-3 loss: 0.697066  [  128/  130]
train() client id: f_00008-2-0 loss: 0.751626  [   32/  130]
train() client id: f_00008-2-1 loss: 0.914312  [   64/  130]
train() client id: f_00008-2-2 loss: 0.749662  [   96/  130]
train() client id: f_00008-2-3 loss: 0.673714  [  128/  130]
train() client id: f_00008-3-0 loss: 0.635042  [   32/  130]
train() client id: f_00008-3-1 loss: 0.829494  [   64/  130]
train() client id: f_00008-3-2 loss: 0.704382  [   96/  130]
train() client id: f_00008-3-3 loss: 0.884126  [  128/  130]
train() client id: f_00008-4-0 loss: 0.706358  [   32/  130]
train() client id: f_00008-4-1 loss: 0.748627  [   64/  130]
train() client id: f_00008-4-2 loss: 0.812757  [   96/  130]
train() client id: f_00008-4-3 loss: 0.785259  [  128/  130]
train() client id: f_00008-5-0 loss: 0.770726  [   32/  130]
train() client id: f_00008-5-1 loss: 0.765002  [   64/  130]
train() client id: f_00008-5-2 loss: 0.690754  [   96/  130]
train() client id: f_00008-5-3 loss: 0.862051  [  128/  130]
train() client id: f_00008-6-0 loss: 0.639867  [   32/  130]
train() client id: f_00008-6-1 loss: 1.050714  [   64/  130]
train() client id: f_00008-6-2 loss: 0.763167  [   96/  130]
train() client id: f_00008-6-3 loss: 0.632320  [  128/  130]
train() client id: f_00008-7-0 loss: 0.724972  [   32/  130]
train() client id: f_00008-7-1 loss: 0.801278  [   64/  130]
train() client id: f_00008-7-2 loss: 0.741309  [   96/  130]
train() client id: f_00008-7-3 loss: 0.799087  [  128/  130]
train() client id: f_00008-8-0 loss: 0.772545  [   32/  130]
train() client id: f_00008-8-1 loss: 0.811887  [   64/  130]
train() client id: f_00008-8-2 loss: 0.690253  [   96/  130]
train() client id: f_00008-8-3 loss: 0.803629  [  128/  130]
train() client id: f_00008-9-0 loss: 0.808184  [   32/  130]
train() client id: f_00008-9-1 loss: 0.763092  [   64/  130]
train() client id: f_00008-9-2 loss: 0.750253  [   96/  130]
train() client id: f_00008-9-3 loss: 0.715581  [  128/  130]
train() client id: f_00008-10-0 loss: 0.882127  [   32/  130]
train() client id: f_00008-10-1 loss: 0.737488  [   64/  130]
train() client id: f_00008-10-2 loss: 0.702965  [   96/  130]
train() client id: f_00008-10-3 loss: 0.741997  [  128/  130]
train() client id: f_00008-11-0 loss: 0.779801  [   32/  130]
train() client id: f_00008-11-1 loss: 0.810294  [   64/  130]
train() client id: f_00008-11-2 loss: 0.804372  [   96/  130]
train() client id: f_00008-11-3 loss: 0.673019  [  128/  130]
train() client id: f_00009-0-0 loss: 1.263029  [   32/  118]
train() client id: f_00009-0-1 loss: 1.150683  [   64/  118]
train() client id: f_00009-0-2 loss: 1.063820  [   96/  118]
train() client id: f_00009-1-0 loss: 0.992101  [   32/  118]
train() client id: f_00009-1-1 loss: 1.160462  [   64/  118]
train() client id: f_00009-1-2 loss: 1.146015  [   96/  118]
train() client id: f_00009-2-0 loss: 1.119709  [   32/  118]
train() client id: f_00009-2-1 loss: 1.056599  [   64/  118]
train() client id: f_00009-2-2 loss: 0.948600  [   96/  118]
train() client id: f_00009-3-0 loss: 0.930501  [   32/  118]
train() client id: f_00009-3-1 loss: 1.077292  [   64/  118]
train() client id: f_00009-3-2 loss: 0.970421  [   96/  118]
train() client id: f_00009-4-0 loss: 0.998414  [   32/  118]
train() client id: f_00009-4-1 loss: 1.040751  [   64/  118]
train() client id: f_00009-4-2 loss: 0.931804  [   96/  118]
train() client id: f_00009-5-0 loss: 0.967542  [   32/  118]
train() client id: f_00009-5-1 loss: 1.055085  [   64/  118]
train() client id: f_00009-5-2 loss: 0.875784  [   96/  118]
train() client id: f_00009-6-0 loss: 0.913682  [   32/  118]
train() client id: f_00009-6-1 loss: 1.007185  [   64/  118]
train() client id: f_00009-6-2 loss: 0.888015  [   96/  118]
train() client id: f_00009-7-0 loss: 0.845889  [   32/  118]
train() client id: f_00009-7-1 loss: 1.125145  [   64/  118]
train() client id: f_00009-7-2 loss: 0.950674  [   96/  118]
train() client id: f_00009-8-0 loss: 0.918512  [   32/  118]
train() client id: f_00009-8-1 loss: 0.942938  [   64/  118]
train() client id: f_00009-8-2 loss: 0.930963  [   96/  118]
train() client id: f_00009-9-0 loss: 1.031187  [   32/  118]
train() client id: f_00009-9-1 loss: 0.812740  [   64/  118]
train() client id: f_00009-9-2 loss: 0.927745  [   96/  118]
train() client id: f_00009-10-0 loss: 0.976342  [   32/  118]
train() client id: f_00009-10-1 loss: 0.796374  [   64/  118]
train() client id: f_00009-10-2 loss: 0.925550  [   96/  118]
train() client id: f_00009-11-0 loss: 0.929808  [   32/  118]
train() client id: f_00009-11-1 loss: 0.913738  [   64/  118]
train() client id: f_00009-11-2 loss: 0.896223  [   96/  118]
At round 21 accuracy: 0.6392572944297082
At round 21 training accuracy: 0.579476861167002
At round 21 training loss: 0.8395799674090392
update_location
xs = [ -3.9056584    4.20031788 125.00902392  18.81129433   0.97929623
   3.95640986 -87.44319194 -66.32485185 109.66397685 -52.06087855]
ys = [117.5879595  100.55583871   1.32061395 -87.45517586  79.35018685
  62.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [154.40913958 141.87712771 160.0905996  134.17254778 127.66131432
 118.15782121 132.86535425 119.99859261 149.44851217 112.81111174]
dists_bs = [180.46998676 194.04815659 346.65536714 326.25992268 200.29378287
 211.21330783 198.03239556 205.30012735 325.29693755 210.60513008]
uav_gains = [3.37016863e-11 4.16871884e-11 3.07625343e-11 4.79445886e-11
 5.43001226e-11 6.58910772e-11 4.91340014e-11 6.33925495e-11
 3.65879032e-11 7.39794975e-11]
bs_gains = [5.31324844e-11 4.33658063e-11 8.54239964e-12 1.01231494e-11
 3.96848892e-11 3.42038117e-11 4.09668536e-11 3.70343113e-11
 1.02072830e-11 3.44810940e-11]
Round 22
-------------------------------
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.08847719 16.82258033  7.96606227  2.85607422 19.40433678  9.34589642
  3.54718528 11.40188453  8.39827017  7.58446914]
obj_prev = 95.4152363247567
eta_min = 3.3224549121166505e-12	eta_max = 0.9228798704318716
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 22.17659221087636	eta = 0.909090909090909
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 38.93866072382321	eta = 0.5177512014733847
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 30.874641848632802	eta = 0.6529804773886541
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.424734721470024	eta = 0.6851561641714188
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351835344463147	eta = 0.6868578450692009
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351637432902866	eta = 0.6868624763988199
eta = 0.6868624763988199
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.03099536 0.06518865 0.03050337 0.01057778 0.07527445 0.03591526
 0.01328373 0.04403309 0.03197934 0.0290274 ]
ene_total = [2.56014735 4.7872217  2.53839595 1.17632747 5.46154932 2.88284624
 1.35232746 3.3560089  2.80903437 2.42777865]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 0 obj = 5.19823104157566
eta = 0.6868624763988199
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
eta_min = 0.6868624763988167	eta_max = 0.6868624763987461
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 0.02749108426556509	eta = 0.9090909090909091
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 17.828312615008755	eta = 0.0014018093201842298
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7850725781134285	eta = 0.014000492245133406
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.749550246261795	eta = 0.014284753947636938
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7495451585936168	eta = 0.014284795487626767
eta = 0.014284795487626767
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.62444973e-04 1.47688208e-03 1.56235501e-04 6.25676453e-06
 2.29167254e-03 2.52367846e-04 1.23667633e-05 4.41695438e-04
 2.09660410e-04 1.33132867e-04]
ene_total = [0.16747587 0.18628874 0.17074105 0.15226131 0.20631753 0.16882169
 0.151622   0.15306496 0.22691989 0.16603211]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 1 obj = 5.198231041575606
eta = 0.6868624763988167
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
Done!
At round 22 energy consumption: 5.19823104157566
At round 22 eta: 0.6868624763988167
At round 22 local rounds: 12.299746037466248
At round 22 global rounds: 65.93458997020494
At round 22 a_n: 20.30404837596008
gradient difference: 0.3806811273097992
train() client id: f_00000-0-0 loss: 1.235131  [   32/  126]
train() client id: f_00000-0-1 loss: 1.341231  [   64/  126]
train() client id: f_00000-0-2 loss: 1.185098  [   96/  126]
train() client id: f_00000-1-0 loss: 1.006535  [   32/  126]
train() client id: f_00000-1-1 loss: 1.116558  [   64/  126]
train() client id: f_00000-1-2 loss: 1.089607  [   96/  126]
train() client id: f_00000-2-0 loss: 1.059953  [   32/  126]
train() client id: f_00000-2-1 loss: 0.896785  [   64/  126]
train() client id: f_00000-2-2 loss: 1.080896  [   96/  126]
train() client id: f_00000-3-0 loss: 0.977835  [   32/  126]
train() client id: f_00000-3-1 loss: 0.901432  [   64/  126]
train() client id: f_00000-3-2 loss: 0.849000  [   96/  126]
train() client id: f_00000-4-0 loss: 0.920915  [   32/  126]
train() client id: f_00000-4-1 loss: 0.885112  [   64/  126]
train() client id: f_00000-4-2 loss: 0.974422  [   96/  126]
train() client id: f_00000-5-0 loss: 0.793722  [   32/  126]
train() client id: f_00000-5-1 loss: 0.891068  [   64/  126]
train() client id: f_00000-5-2 loss: 0.861045  [   96/  126]
train() client id: f_00000-6-0 loss: 0.904484  [   32/  126]
train() client id: f_00000-6-1 loss: 0.809391  [   64/  126]
train() client id: f_00000-6-2 loss: 0.801969  [   96/  126]
train() client id: f_00000-7-0 loss: 0.830694  [   32/  126]
train() client id: f_00000-7-1 loss: 0.777479  [   64/  126]
train() client id: f_00000-7-2 loss: 0.724756  [   96/  126]
train() client id: f_00000-8-0 loss: 0.747144  [   32/  126]
train() client id: f_00000-8-1 loss: 0.790394  [   64/  126]
train() client id: f_00000-8-2 loss: 0.943192  [   96/  126]
train() client id: f_00000-9-0 loss: 0.800713  [   32/  126]
train() client id: f_00000-9-1 loss: 0.858422  [   64/  126]
train() client id: f_00000-9-2 loss: 0.706810  [   96/  126]
train() client id: f_00000-10-0 loss: 0.781004  [   32/  126]
train() client id: f_00000-10-1 loss: 0.677969  [   64/  126]
train() client id: f_00000-10-2 loss: 0.782838  [   96/  126]
train() client id: f_00000-11-0 loss: 0.805868  [   32/  126]
train() client id: f_00000-11-1 loss: 0.728545  [   64/  126]
train() client id: f_00000-11-2 loss: 0.746455  [   96/  126]
train() client id: f_00001-0-0 loss: 0.518096  [   32/  265]
train() client id: f_00001-0-1 loss: 0.617730  [   64/  265]
train() client id: f_00001-0-2 loss: 0.461255  [   96/  265]
train() client id: f_00001-0-3 loss: 0.497018  [  128/  265]
train() client id: f_00001-0-4 loss: 0.399393  [  160/  265]
train() client id: f_00001-0-5 loss: 0.425637  [  192/  265]
train() client id: f_00001-0-6 loss: 0.511661  [  224/  265]
train() client id: f_00001-0-7 loss: 0.355126  [  256/  265]
train() client id: f_00001-1-0 loss: 0.665491  [   32/  265]
train() client id: f_00001-1-1 loss: 0.447380  [   64/  265]
train() client id: f_00001-1-2 loss: 0.389514  [   96/  265]
train() client id: f_00001-1-3 loss: 0.406197  [  128/  265]
train() client id: f_00001-1-4 loss: 0.457404  [  160/  265]
train() client id: f_00001-1-5 loss: 0.384500  [  192/  265]
train() client id: f_00001-1-6 loss: 0.539201  [  224/  265]
train() client id: f_00001-1-7 loss: 0.426659  [  256/  265]
train() client id: f_00001-2-0 loss: 0.546565  [   32/  265]
train() client id: f_00001-2-1 loss: 0.472254  [   64/  265]
train() client id: f_00001-2-2 loss: 0.443406  [   96/  265]
train() client id: f_00001-2-3 loss: 0.450619  [  128/  265]
train() client id: f_00001-2-4 loss: 0.374818  [  160/  265]
train() client id: f_00001-2-5 loss: 0.461435  [  192/  265]
train() client id: f_00001-2-6 loss: 0.462674  [  224/  265]
train() client id: f_00001-2-7 loss: 0.434834  [  256/  265]
train() client id: f_00001-3-0 loss: 0.408953  [   32/  265]
train() client id: f_00001-3-1 loss: 0.355587  [   64/  265]
train() client id: f_00001-3-2 loss: 0.422080  [   96/  265]
train() client id: f_00001-3-3 loss: 0.448256  [  128/  265]
train() client id: f_00001-3-4 loss: 0.445064  [  160/  265]
train() client id: f_00001-3-5 loss: 0.535685  [  192/  265]
train() client id: f_00001-3-6 loss: 0.475900  [  224/  265]
train() client id: f_00001-3-7 loss: 0.498848  [  256/  265]
train() client id: f_00001-4-0 loss: 0.581907  [   32/  265]
train() client id: f_00001-4-1 loss: 0.445317  [   64/  265]
train() client id: f_00001-4-2 loss: 0.388981  [   96/  265]
train() client id: f_00001-4-3 loss: 0.356045  [  128/  265]
train() client id: f_00001-4-4 loss: 0.481716  [  160/  265]
train() client id: f_00001-4-5 loss: 0.453764  [  192/  265]
train() client id: f_00001-4-6 loss: 0.421956  [  224/  265]
train() client id: f_00001-4-7 loss: 0.418488  [  256/  265]
train() client id: f_00001-5-0 loss: 0.522419  [   32/  265]
train() client id: f_00001-5-1 loss: 0.370352  [   64/  265]
train() client id: f_00001-5-2 loss: 0.432853  [   96/  265]
train() client id: f_00001-5-3 loss: 0.443399  [  128/  265]
train() client id: f_00001-5-4 loss: 0.434512  [  160/  265]
train() client id: f_00001-5-5 loss: 0.383084  [  192/  265]
train() client id: f_00001-5-6 loss: 0.351371  [  224/  265]
train() client id: f_00001-5-7 loss: 0.515118  [  256/  265]
train() client id: f_00001-6-0 loss: 0.453252  [   32/  265]
train() client id: f_00001-6-1 loss: 0.355321  [   64/  265]
train() client id: f_00001-6-2 loss: 0.524057  [   96/  265]
train() client id: f_00001-6-3 loss: 0.384651  [  128/  265]
train() client id: f_00001-6-4 loss: 0.339810  [  160/  265]
train() client id: f_00001-6-5 loss: 0.349470  [  192/  265]
train() client id: f_00001-6-6 loss: 0.632209  [  224/  265]
train() client id: f_00001-6-7 loss: 0.404038  [  256/  265]
train() client id: f_00001-7-0 loss: 0.418649  [   32/  265]
train() client id: f_00001-7-1 loss: 0.433843  [   64/  265]
train() client id: f_00001-7-2 loss: 0.410508  [   96/  265]
train() client id: f_00001-7-3 loss: 0.641871  [  128/  265]
train() client id: f_00001-7-4 loss: 0.513653  [  160/  265]
train() client id: f_00001-7-5 loss: 0.345501  [  192/  265]
train() client id: f_00001-7-6 loss: 0.325823  [  224/  265]
train() client id: f_00001-7-7 loss: 0.385616  [  256/  265]
train() client id: f_00001-8-0 loss: 0.384130  [   32/  265]
train() client id: f_00001-8-1 loss: 0.393611  [   64/  265]
train() client id: f_00001-8-2 loss: 0.439769  [   96/  265]
train() client id: f_00001-8-3 loss: 0.411882  [  128/  265]
train() client id: f_00001-8-4 loss: 0.570711  [  160/  265]
train() client id: f_00001-8-5 loss: 0.329460  [  192/  265]
train() client id: f_00001-8-6 loss: 0.422908  [  224/  265]
train() client id: f_00001-8-7 loss: 0.494449  [  256/  265]
train() client id: f_00001-9-0 loss: 0.424179  [   32/  265]
train() client id: f_00001-9-1 loss: 0.318361  [   64/  265]
train() client id: f_00001-9-2 loss: 0.578836  [   96/  265]
train() client id: f_00001-9-3 loss: 0.398215  [  128/  265]
train() client id: f_00001-9-4 loss: 0.377004  [  160/  265]
train() client id: f_00001-9-5 loss: 0.441568  [  192/  265]
train() client id: f_00001-9-6 loss: 0.382866  [  224/  265]
train() client id: f_00001-9-7 loss: 0.519576  [  256/  265]
train() client id: f_00001-10-0 loss: 0.374931  [   32/  265]
train() client id: f_00001-10-1 loss: 0.338183  [   64/  265]
train() client id: f_00001-10-2 loss: 0.390295  [   96/  265]
train() client id: f_00001-10-3 loss: 0.424531  [  128/  265]
train() client id: f_00001-10-4 loss: 0.680683  [  160/  265]
train() client id: f_00001-10-5 loss: 0.346352  [  192/  265]
train() client id: f_00001-10-6 loss: 0.449889  [  224/  265]
train() client id: f_00001-10-7 loss: 0.430813  [  256/  265]
train() client id: f_00001-11-0 loss: 0.378347  [   32/  265]
train() client id: f_00001-11-1 loss: 0.402759  [   64/  265]
train() client id: f_00001-11-2 loss: 0.338555  [   96/  265]
train() client id: f_00001-11-3 loss: 0.479005  [  128/  265]
train() client id: f_00001-11-4 loss: 0.474465  [  160/  265]
train() client id: f_00001-11-5 loss: 0.517598  [  192/  265]
train() client id: f_00001-11-6 loss: 0.434943  [  224/  265]
train() client id: f_00001-11-7 loss: 0.325252  [  256/  265]
train() client id: f_00002-0-0 loss: 1.199639  [   32/  124]
train() client id: f_00002-0-1 loss: 1.202461  [   64/  124]
train() client id: f_00002-0-2 loss: 1.253377  [   96/  124]
train() client id: f_00002-1-0 loss: 1.192730  [   32/  124]
train() client id: f_00002-1-1 loss: 1.149713  [   64/  124]
train() client id: f_00002-1-2 loss: 1.190395  [   96/  124]
train() client id: f_00002-2-0 loss: 1.088826  [   32/  124]
train() client id: f_00002-2-1 loss: 1.233254  [   64/  124]
train() client id: f_00002-2-2 loss: 1.082212  [   96/  124]
train() client id: f_00002-3-0 loss: 1.060337  [   32/  124]
train() client id: f_00002-3-1 loss: 1.049294  [   64/  124]
train() client id: f_00002-3-2 loss: 1.133465  [   96/  124]
train() client id: f_00002-4-0 loss: 1.001248  [   32/  124]
train() client id: f_00002-4-1 loss: 0.989849  [   64/  124]
train() client id: f_00002-4-2 loss: 1.062722  [   96/  124]
train() client id: f_00002-5-0 loss: 0.976957  [   32/  124]
train() client id: f_00002-5-1 loss: 1.154283  [   64/  124]
train() client id: f_00002-5-2 loss: 1.029434  [   96/  124]
train() client id: f_00002-6-0 loss: 0.964149  [   32/  124]
train() client id: f_00002-6-1 loss: 1.041895  [   64/  124]
train() client id: f_00002-6-2 loss: 1.000090  [   96/  124]
train() client id: f_00002-7-0 loss: 0.874713  [   32/  124]
train() client id: f_00002-7-1 loss: 0.949508  [   64/  124]
train() client id: f_00002-7-2 loss: 0.911277  [   96/  124]
train() client id: f_00002-8-0 loss: 0.943115  [   32/  124]
train() client id: f_00002-8-1 loss: 0.927921  [   64/  124]
train() client id: f_00002-8-2 loss: 1.017957  [   96/  124]
train() client id: f_00002-9-0 loss: 0.870862  [   32/  124]
train() client id: f_00002-9-1 loss: 1.056181  [   64/  124]
train() client id: f_00002-9-2 loss: 0.993402  [   96/  124]
train() client id: f_00002-10-0 loss: 1.003177  [   32/  124]
train() client id: f_00002-10-1 loss: 0.929389  [   64/  124]
train() client id: f_00002-10-2 loss: 0.934659  [   96/  124]
train() client id: f_00002-11-0 loss: 0.841231  [   32/  124]
train() client id: f_00002-11-1 loss: 0.920896  [   64/  124]
train() client id: f_00002-11-2 loss: 0.826608  [   96/  124]
train() client id: f_00003-0-0 loss: 0.737863  [   32/   43]
train() client id: f_00003-1-0 loss: 0.809849  [   32/   43]
train() client id: f_00003-2-0 loss: 0.708270  [   32/   43]
train() client id: f_00003-3-0 loss: 0.895173  [   32/   43]
train() client id: f_00003-4-0 loss: 0.557853  [   32/   43]
train() client id: f_00003-5-0 loss: 0.991901  [   32/   43]
train() client id: f_00003-6-0 loss: 0.645715  [   32/   43]
train() client id: f_00003-7-0 loss: 0.667812  [   32/   43]
train() client id: f_00003-8-0 loss: 0.707456  [   32/   43]
train() client id: f_00003-9-0 loss: 0.773840  [   32/   43]
train() client id: f_00003-10-0 loss: 0.808877  [   32/   43]
train() client id: f_00003-11-0 loss: 0.915447  [   32/   43]
train() client id: f_00004-0-0 loss: 0.830619  [   32/  306]
train() client id: f_00004-0-1 loss: 0.920230  [   64/  306]
train() client id: f_00004-0-2 loss: 0.947322  [   96/  306]
train() client id: f_00004-0-3 loss: 0.811817  [  128/  306]
train() client id: f_00004-0-4 loss: 0.732301  [  160/  306]
train() client id: f_00004-0-5 loss: 0.761742  [  192/  306]
train() client id: f_00004-0-6 loss: 0.836895  [  224/  306]
train() client id: f_00004-0-7 loss: 0.896479  [  256/  306]
train() client id: f_00004-0-8 loss: 0.842427  [  288/  306]
train() client id: f_00004-1-0 loss: 0.689344  [   32/  306]
train() client id: f_00004-1-1 loss: 0.844905  [   64/  306]
train() client id: f_00004-1-2 loss: 0.764086  [   96/  306]
train() client id: f_00004-1-3 loss: 0.808093  [  128/  306]
train() client id: f_00004-1-4 loss: 0.748205  [  160/  306]
train() client id: f_00004-1-5 loss: 0.801290  [  192/  306]
train() client id: f_00004-1-6 loss: 0.853351  [  224/  306]
train() client id: f_00004-1-7 loss: 0.936679  [  256/  306]
train() client id: f_00004-1-8 loss: 1.004626  [  288/  306]
train() client id: f_00004-2-0 loss: 0.834483  [   32/  306]
train() client id: f_00004-2-1 loss: 0.836776  [   64/  306]
train() client id: f_00004-2-2 loss: 0.793836  [   96/  306]
train() client id: f_00004-2-3 loss: 0.975382  [  128/  306]
train() client id: f_00004-2-4 loss: 0.815657  [  160/  306]
train() client id: f_00004-2-5 loss: 0.876807  [  192/  306]
train() client id: f_00004-2-6 loss: 0.826092  [  224/  306]
train() client id: f_00004-2-7 loss: 0.758717  [  256/  306]
train() client id: f_00004-2-8 loss: 0.754830  [  288/  306]
train() client id: f_00004-3-0 loss: 0.758899  [   32/  306]
train() client id: f_00004-3-1 loss: 0.947103  [   64/  306]
train() client id: f_00004-3-2 loss: 0.883219  [   96/  306]
train() client id: f_00004-3-3 loss: 0.933177  [  128/  306]
train() client id: f_00004-3-4 loss: 0.969545  [  160/  306]
train() client id: f_00004-3-5 loss: 0.794201  [  192/  306]
train() client id: f_00004-3-6 loss: 0.746348  [  224/  306]
train() client id: f_00004-3-7 loss: 0.747618  [  256/  306]
train() client id: f_00004-3-8 loss: 0.753564  [  288/  306]
train() client id: f_00004-4-0 loss: 0.905320  [   32/  306]
train() client id: f_00004-4-1 loss: 0.767575  [   64/  306]
train() client id: f_00004-4-2 loss: 0.879605  [   96/  306]
train() client id: f_00004-4-3 loss: 0.848872  [  128/  306]
train() client id: f_00004-4-4 loss: 0.765849  [  160/  306]
train() client id: f_00004-4-5 loss: 0.829344  [  192/  306]
train() client id: f_00004-4-6 loss: 0.781181  [  224/  306]
train() client id: f_00004-4-7 loss: 1.026843  [  256/  306]
train() client id: f_00004-4-8 loss: 0.776640  [  288/  306]
train() client id: f_00004-5-0 loss: 0.794968  [   32/  306]
train() client id: f_00004-5-1 loss: 0.911955  [   64/  306]
train() client id: f_00004-5-2 loss: 0.820450  [   96/  306]
train() client id: f_00004-5-3 loss: 0.976904  [  128/  306]
train() client id: f_00004-5-4 loss: 0.824308  [  160/  306]
train() client id: f_00004-5-5 loss: 0.788825  [  192/  306]
train() client id: f_00004-5-6 loss: 0.837168  [  224/  306]
train() client id: f_00004-5-7 loss: 0.837745  [  256/  306]
train() client id: f_00004-5-8 loss: 0.746340  [  288/  306]
train() client id: f_00004-6-0 loss: 0.883565  [   32/  306]
train() client id: f_00004-6-1 loss: 0.831839  [   64/  306]
train() client id: f_00004-6-2 loss: 0.743178  [   96/  306]
train() client id: f_00004-6-3 loss: 0.767995  [  128/  306]
train() client id: f_00004-6-4 loss: 0.972567  [  160/  306]
train() client id: f_00004-6-5 loss: 0.882917  [  192/  306]
train() client id: f_00004-6-6 loss: 0.786569  [  224/  306]
train() client id: f_00004-6-7 loss: 0.830357  [  256/  306]
train() client id: f_00004-6-8 loss: 0.798201  [  288/  306]
train() client id: f_00004-7-0 loss: 0.956343  [   32/  306]
train() client id: f_00004-7-1 loss: 0.947354  [   64/  306]
train() client id: f_00004-7-2 loss: 0.850748  [   96/  306]
train() client id: f_00004-7-3 loss: 0.742092  [  128/  306]
train() client id: f_00004-7-4 loss: 0.852073  [  160/  306]
train() client id: f_00004-7-5 loss: 0.764920  [  192/  306]
train() client id: f_00004-7-6 loss: 0.812032  [  224/  306]
train() client id: f_00004-7-7 loss: 0.765741  [  256/  306]
train() client id: f_00004-7-8 loss: 0.711601  [  288/  306]
train() client id: f_00004-8-0 loss: 0.893497  [   32/  306]
train() client id: f_00004-8-1 loss: 0.850645  [   64/  306]
train() client id: f_00004-8-2 loss: 0.789927  [   96/  306]
train() client id: f_00004-8-3 loss: 0.796860  [  128/  306]
train() client id: f_00004-8-4 loss: 0.793281  [  160/  306]
train() client id: f_00004-8-5 loss: 0.869929  [  192/  306]
train() client id: f_00004-8-6 loss: 0.779801  [  224/  306]
train() client id: f_00004-8-7 loss: 0.820858  [  256/  306]
train() client id: f_00004-8-8 loss: 0.847011  [  288/  306]
train() client id: f_00004-9-0 loss: 0.742870  [   32/  306]
train() client id: f_00004-9-1 loss: 0.746545  [   64/  306]
train() client id: f_00004-9-2 loss: 0.815353  [   96/  306]
train() client id: f_00004-9-3 loss: 0.922728  [  128/  306]
train() client id: f_00004-9-4 loss: 0.793945  [  160/  306]
train() client id: f_00004-9-5 loss: 0.849773  [  192/  306]
train() client id: f_00004-9-6 loss: 0.868037  [  224/  306]
train() client id: f_00004-9-7 loss: 0.876799  [  256/  306]
train() client id: f_00004-9-8 loss: 1.030056  [  288/  306]
train() client id: f_00004-10-0 loss: 0.681219  [   32/  306]
train() client id: f_00004-10-1 loss: 0.731993  [   64/  306]
train() client id: f_00004-10-2 loss: 0.849167  [   96/  306]
train() client id: f_00004-10-3 loss: 0.921463  [  128/  306]
train() client id: f_00004-10-4 loss: 0.760117  [  160/  306]
train() client id: f_00004-10-5 loss: 0.998483  [  192/  306]
train() client id: f_00004-10-6 loss: 0.858097  [  224/  306]
train() client id: f_00004-10-7 loss: 0.922283  [  256/  306]
train() client id: f_00004-10-8 loss: 0.908687  [  288/  306]
train() client id: f_00004-11-0 loss: 0.864381  [   32/  306]
train() client id: f_00004-11-1 loss: 0.863648  [   64/  306]
train() client id: f_00004-11-2 loss: 0.993454  [   96/  306]
train() client id: f_00004-11-3 loss: 0.841856  [  128/  306]
train() client id: f_00004-11-4 loss: 0.792502  [  160/  306]
train() client id: f_00004-11-5 loss: 0.845884  [  192/  306]
train() client id: f_00004-11-6 loss: 0.818352  [  224/  306]
train() client id: f_00004-11-7 loss: 0.741659  [  256/  306]
train() client id: f_00004-11-8 loss: 0.828666  [  288/  306]
train() client id: f_00005-0-0 loss: 0.534084  [   32/  146]
train() client id: f_00005-0-1 loss: 0.467127  [   64/  146]
train() client id: f_00005-0-2 loss: 0.560409  [   96/  146]
train() client id: f_00005-0-3 loss: 0.426677  [  128/  146]
train() client id: f_00005-1-0 loss: 0.600741  [   32/  146]
train() client id: f_00005-1-1 loss: 0.362272  [   64/  146]
train() client id: f_00005-1-2 loss: 0.426146  [   96/  146]
train() client id: f_00005-1-3 loss: 0.633542  [  128/  146]
train() client id: f_00005-2-0 loss: 0.519861  [   32/  146]
train() client id: f_00005-2-1 loss: 0.213456  [   64/  146]
train() client id: f_00005-2-2 loss: 0.632141  [   96/  146]
train() client id: f_00005-2-3 loss: 0.603643  [  128/  146]
train() client id: f_00005-3-0 loss: 0.338010  [   32/  146]
train() client id: f_00005-3-1 loss: 0.557685  [   64/  146]
train() client id: f_00005-3-2 loss: 0.551502  [   96/  146]
train() client id: f_00005-3-3 loss: 0.615252  [  128/  146]
train() client id: f_00005-4-0 loss: 0.428290  [   32/  146]
train() client id: f_00005-4-1 loss: 0.469342  [   64/  146]
train() client id: f_00005-4-2 loss: 0.458043  [   96/  146]
train() client id: f_00005-4-3 loss: 0.760336  [  128/  146]
train() client id: f_00005-5-0 loss: 0.609773  [   32/  146]
train() client id: f_00005-5-1 loss: 0.431088  [   64/  146]
train() client id: f_00005-5-2 loss: 0.506563  [   96/  146]
train() client id: f_00005-5-3 loss: 0.409529  [  128/  146]
train() client id: f_00005-6-0 loss: 0.573206  [   32/  146]
train() client id: f_00005-6-1 loss: 0.525827  [   64/  146]
train() client id: f_00005-6-2 loss: 0.414829  [   96/  146]
train() client id: f_00005-6-3 loss: 0.639946  [  128/  146]
train() client id: f_00005-7-0 loss: 0.543807  [   32/  146]
train() client id: f_00005-7-1 loss: 0.385599  [   64/  146]
train() client id: f_00005-7-2 loss: 0.462684  [   96/  146]
train() client id: f_00005-7-3 loss: 0.676799  [  128/  146]
train() client id: f_00005-8-0 loss: 0.458622  [   32/  146]
train() client id: f_00005-8-1 loss: 0.293297  [   64/  146]
train() client id: f_00005-8-2 loss: 0.590238  [   96/  146]
train() client id: f_00005-8-3 loss: 0.509990  [  128/  146]
train() client id: f_00005-9-0 loss: 0.607498  [   32/  146]
train() client id: f_00005-9-1 loss: 0.534981  [   64/  146]
train() client id: f_00005-9-2 loss: 0.449339  [   96/  146]
train() client id: f_00005-9-3 loss: 0.345150  [  128/  146]
train() client id: f_00005-10-0 loss: 0.772104  [   32/  146]
train() client id: f_00005-10-1 loss: 0.509022  [   64/  146]
train() client id: f_00005-10-2 loss: 0.345927  [   96/  146]
train() client id: f_00005-10-3 loss: 0.373205  [  128/  146]
train() client id: f_00005-11-0 loss: 0.288664  [   32/  146]
train() client id: f_00005-11-1 loss: 0.385172  [   64/  146]
train() client id: f_00005-11-2 loss: 0.767570  [   96/  146]
train() client id: f_00005-11-3 loss: 0.522020  [  128/  146]
train() client id: f_00006-0-0 loss: 0.587604  [   32/   54]
train() client id: f_00006-1-0 loss: 0.574227  [   32/   54]
train() client id: f_00006-2-0 loss: 0.626817  [   32/   54]
train() client id: f_00006-3-0 loss: 0.571194  [   32/   54]
train() client id: f_00006-4-0 loss: 0.573626  [   32/   54]
train() client id: f_00006-5-0 loss: 0.526695  [   32/   54]
train() client id: f_00006-6-0 loss: 0.553604  [   32/   54]
train() client id: f_00006-7-0 loss: 0.523084  [   32/   54]
train() client id: f_00006-8-0 loss: 0.630975  [   32/   54]
train() client id: f_00006-9-0 loss: 0.583891  [   32/   54]
train() client id: f_00006-10-0 loss: 0.558602  [   32/   54]
train() client id: f_00006-11-0 loss: 0.529182  [   32/   54]
train() client id: f_00007-0-0 loss: 0.692609  [   32/  179]
train() client id: f_00007-0-1 loss: 0.468049  [   64/  179]
train() client id: f_00007-0-2 loss: 0.497137  [   96/  179]
train() client id: f_00007-0-3 loss: 0.672112  [  128/  179]
train() client id: f_00007-0-4 loss: 0.464652  [  160/  179]
train() client id: f_00007-1-0 loss: 0.600809  [   32/  179]
train() client id: f_00007-1-1 loss: 0.517672  [   64/  179]
train() client id: f_00007-1-2 loss: 0.612476  [   96/  179]
train() client id: f_00007-1-3 loss: 0.649457  [  128/  179]
train() client id: f_00007-1-4 loss: 0.405134  [  160/  179]
train() client id: f_00007-2-0 loss: 0.444436  [   32/  179]
train() client id: f_00007-2-1 loss: 0.495517  [   64/  179]
train() client id: f_00007-2-2 loss: 0.514979  [   96/  179]
train() client id: f_00007-2-3 loss: 0.543485  [  128/  179]
train() client id: f_00007-2-4 loss: 0.690441  [  160/  179]
train() client id: f_00007-3-0 loss: 0.478099  [   32/  179]
train() client id: f_00007-3-1 loss: 0.385558  [   64/  179]
train() client id: f_00007-3-2 loss: 0.775694  [   96/  179]
train() client id: f_00007-3-3 loss: 0.593673  [  128/  179]
train() client id: f_00007-3-4 loss: 0.483054  [  160/  179]
train() client id: f_00007-4-0 loss: 0.414417  [   32/  179]
train() client id: f_00007-4-1 loss: 0.633968  [   64/  179]
train() client id: f_00007-4-2 loss: 0.662589  [   96/  179]
train() client id: f_00007-4-3 loss: 0.538424  [  128/  179]
train() client id: f_00007-4-4 loss: 0.443714  [  160/  179]
train() client id: f_00007-5-0 loss: 0.621575  [   32/  179]
train() client id: f_00007-5-1 loss: 0.409297  [   64/  179]
train() client id: f_00007-5-2 loss: 0.554525  [   96/  179]
train() client id: f_00007-5-3 loss: 0.384900  [  128/  179]
train() client id: f_00007-5-4 loss: 0.606588  [  160/  179]
train() client id: f_00007-6-0 loss: 0.490842  [   32/  179]
train() client id: f_00007-6-1 loss: 0.684279  [   64/  179]
train() client id: f_00007-6-2 loss: 0.503050  [   96/  179]
train() client id: f_00007-6-3 loss: 0.392439  [  128/  179]
train() client id: f_00007-6-4 loss: 0.475514  [  160/  179]
train() client id: f_00007-7-0 loss: 0.442372  [   32/  179]
train() client id: f_00007-7-1 loss: 0.743526  [   64/  179]
train() client id: f_00007-7-2 loss: 0.461941  [   96/  179]
train() client id: f_00007-7-3 loss: 0.386500  [  128/  179]
train() client id: f_00007-7-4 loss: 0.550782  [  160/  179]
train() client id: f_00007-8-0 loss: 0.685488  [   32/  179]
train() client id: f_00007-8-1 loss: 0.490312  [   64/  179]
train() client id: f_00007-8-2 loss: 0.352851  [   96/  179]
train() client id: f_00007-8-3 loss: 0.628380  [  128/  179]
train() client id: f_00007-8-4 loss: 0.395489  [  160/  179]
train() client id: f_00007-9-0 loss: 0.384661  [   32/  179]
train() client id: f_00007-9-1 loss: 0.551761  [   64/  179]
train() client id: f_00007-9-2 loss: 0.395640  [   96/  179]
train() client id: f_00007-9-3 loss: 0.539576  [  128/  179]
train() client id: f_00007-9-4 loss: 0.454787  [  160/  179]
train() client id: f_00007-10-0 loss: 0.508494  [   32/  179]
train() client id: f_00007-10-1 loss: 0.640084  [   64/  179]
train() client id: f_00007-10-2 loss: 0.381820  [   96/  179]
train() client id: f_00007-10-3 loss: 0.618943  [  128/  179]
train() client id: f_00007-10-4 loss: 0.466571  [  160/  179]
train() client id: f_00007-11-0 loss: 0.596449  [   32/  179]
train() client id: f_00007-11-1 loss: 0.538679  [   64/  179]
train() client id: f_00007-11-2 loss: 0.440916  [   96/  179]
train() client id: f_00007-11-3 loss: 0.453101  [  128/  179]
train() client id: f_00007-11-4 loss: 0.510510  [  160/  179]
train() client id: f_00008-0-0 loss: 0.850072  [   32/  130]
train() client id: f_00008-0-1 loss: 0.740514  [   64/  130]
train() client id: f_00008-0-2 loss: 0.902293  [   96/  130]
train() client id: f_00008-0-3 loss: 0.812500  [  128/  130]
train() client id: f_00008-1-0 loss: 0.915570  [   32/  130]
train() client id: f_00008-1-1 loss: 0.771008  [   64/  130]
train() client id: f_00008-1-2 loss: 0.797200  [   96/  130]
train() client id: f_00008-1-3 loss: 0.822961  [  128/  130]
train() client id: f_00008-2-0 loss: 0.935907  [   32/  130]
train() client id: f_00008-2-1 loss: 0.740808  [   64/  130]
train() client id: f_00008-2-2 loss: 0.838769  [   96/  130]
train() client id: f_00008-2-3 loss: 0.824772  [  128/  130]
train() client id: f_00008-3-0 loss: 0.876207  [   32/  130]
train() client id: f_00008-3-1 loss: 0.846699  [   64/  130]
train() client id: f_00008-3-2 loss: 0.762787  [   96/  130]
train() client id: f_00008-3-3 loss: 0.855402  [  128/  130]
train() client id: f_00008-4-0 loss: 0.920899  [   32/  130]
train() client id: f_00008-4-1 loss: 0.801244  [   64/  130]
train() client id: f_00008-4-2 loss: 0.749200  [   96/  130]
train() client id: f_00008-4-3 loss: 0.859494  [  128/  130]
train() client id: f_00008-5-0 loss: 0.866244  [   32/  130]
train() client id: f_00008-5-1 loss: 0.934889  [   64/  130]
train() client id: f_00008-5-2 loss: 0.759284  [   96/  130]
train() client id: f_00008-5-3 loss: 0.780446  [  128/  130]
train() client id: f_00008-6-0 loss: 0.856766  [   32/  130]
train() client id: f_00008-6-1 loss: 0.849412  [   64/  130]
train() client id: f_00008-6-2 loss: 0.766079  [   96/  130]
train() client id: f_00008-6-3 loss: 0.861360  [  128/  130]
train() client id: f_00008-7-0 loss: 0.882964  [   32/  130]
train() client id: f_00008-7-1 loss: 0.768870  [   64/  130]
train() client id: f_00008-7-2 loss: 0.747434  [   96/  130]
train() client id: f_00008-7-3 loss: 0.941335  [  128/  130]
train() client id: f_00008-8-0 loss: 0.846020  [   32/  130]
train() client id: f_00008-8-1 loss: 0.771006  [   64/  130]
train() client id: f_00008-8-2 loss: 0.865099  [   96/  130]
train() client id: f_00008-8-3 loss: 0.822203  [  128/  130]
train() client id: f_00008-9-0 loss: 0.736733  [   32/  130]
train() client id: f_00008-9-1 loss: 0.841347  [   64/  130]
train() client id: f_00008-9-2 loss: 0.949052  [   96/  130]
train() client id: f_00008-9-3 loss: 0.805718  [  128/  130]
train() client id: f_00008-10-0 loss: 0.946988  [   32/  130]
train() client id: f_00008-10-1 loss: 0.829757  [   64/  130]
train() client id: f_00008-10-2 loss: 0.747207  [   96/  130]
train() client id: f_00008-10-3 loss: 0.784038  [  128/  130]
train() client id: f_00008-11-0 loss: 0.801902  [   32/  130]
train() client id: f_00008-11-1 loss: 0.766628  [   64/  130]
train() client id: f_00008-11-2 loss: 0.902225  [   96/  130]
train() client id: f_00008-11-3 loss: 0.830943  [  128/  130]
train() client id: f_00009-0-0 loss: 1.184147  [   32/  118]
train() client id: f_00009-0-1 loss: 1.327561  [   64/  118]
train() client id: f_00009-0-2 loss: 1.162904  [   96/  118]
train() client id: f_00009-1-0 loss: 1.144802  [   32/  118]
train() client id: f_00009-1-1 loss: 1.150788  [   64/  118]
train() client id: f_00009-1-2 loss: 1.178094  [   96/  118]
train() client id: f_00009-2-0 loss: 1.228734  [   32/  118]
train() client id: f_00009-2-1 loss: 1.022230  [   64/  118]
train() client id: f_00009-2-2 loss: 1.109648  [   96/  118]
train() client id: f_00009-3-0 loss: 1.056864  [   32/  118]
train() client id: f_00009-3-1 loss: 1.005140  [   64/  118]
train() client id: f_00009-3-2 loss: 1.092956  [   96/  118]
train() client id: f_00009-4-0 loss: 1.003815  [   32/  118]
train() client id: f_00009-4-1 loss: 1.082312  [   64/  118]
train() client id: f_00009-4-2 loss: 1.029197  [   96/  118]
train() client id: f_00009-5-0 loss: 1.047969  [   32/  118]
train() client id: f_00009-5-1 loss: 1.038895  [   64/  118]
train() client id: f_00009-5-2 loss: 0.994918  [   96/  118]
train() client id: f_00009-6-0 loss: 0.929038  [   32/  118]
train() client id: f_00009-6-1 loss: 1.012961  [   64/  118]
train() client id: f_00009-6-2 loss: 1.094725  [   96/  118]
train() client id: f_00009-7-0 loss: 0.927133  [   32/  118]
train() client id: f_00009-7-1 loss: 0.936707  [   64/  118]
train() client id: f_00009-7-2 loss: 0.989515  [   96/  118]
train() client id: f_00009-8-0 loss: 1.077087  [   32/  118]
train() client id: f_00009-8-1 loss: 0.919042  [   64/  118]
train() client id: f_00009-8-2 loss: 0.962810  [   96/  118]
train() client id: f_00009-9-0 loss: 0.973652  [   32/  118]
train() client id: f_00009-9-1 loss: 0.942341  [   64/  118]
train() client id: f_00009-9-2 loss: 0.886100  [   96/  118]
train() client id: f_00009-10-0 loss: 0.888292  [   32/  118]
train() client id: f_00009-10-1 loss: 0.987379  [   64/  118]
train() client id: f_00009-10-2 loss: 0.922004  [   96/  118]
train() client id: f_00009-11-0 loss: 0.897283  [   32/  118]
train() client id: f_00009-11-1 loss: 1.046747  [   64/  118]
train() client id: f_00009-11-2 loss: 0.932400  [   96/  118]
At round 22 accuracy: 0.6392572944297082
At round 22 training accuracy: 0.5808182427900738
At round 22 training loss: 0.8313171439994649
update_location
xs = [ -3.9056584    4.20031788 130.00902392  18.81129433   0.97929623
   3.95640986 -92.44319194 -71.32485185 114.66397685 -57.06087855]
ys = [122.5879595  105.55583871   1.32061395 -92.45517586  84.35018685
  67.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [158.24999836 145.46366473 164.02466376 137.48390574 130.82779919
 120.89008322 136.20805511 122.83285695 153.15514213 115.2039744 ]
dists_bs = [178.94215746 192.18544551 350.99150105 330.29545698 197.95479617
 208.60058224 195.87307526 202.69778195 329.6812663  207.72609273]
uav_gains = [3.16755799e-11 3.91571505e-11 2.89252519e-11 4.51054531e-11
 5.10714755e-11 6.22299652e-11 4.61709278e-11 5.97976208e-11
 3.44011166e-11 7.01971242e-11]
bs_gains = [5.44124914e-11 4.45529740e-11 8.25018342e-12 9.78062943e-12
 4.10118354e-11 3.54169077e-11 4.22439780e-11 3.83810539e-11
 9.83173423e-12 3.58359679e-11]
Round 23
-------------------------------
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.95652636 16.54261616  7.83621819  2.81059366 19.08131306  9.18961195
  3.4902559  11.21434588  8.261358    7.45728421]
obj_prev = 93.84012336038582
eta_min = 2.1552750513700054e-12	eta_max = 0.9232017387246043
af = 19.826056636829264	bf = 1.599357197144509	zeta = 21.808662300512193	eta = 0.909090909090909
af = 19.826056636829264	bf = 1.599357197144509	zeta = 38.37880442012573	eta = 0.5165886987983539
af = 19.826056636829264	bf = 1.599357197144509	zeta = 30.397782640654498	eta = 0.6522204882902731
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.962231155820284	eta = 0.6845486637463356
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889857955433257	eta = 0.686263555446129
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889660323483184	eta = 0.6862682501224668
eta = 0.6862682501224668
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.03106678 0.06533886 0.03057366 0.01060215 0.07544789 0.03599801
 0.01331433 0.04413455 0.03205303 0.02909429]
ene_total = [2.52460799 4.70588087 2.50342372 1.16220666 5.36855605 2.83112977
 1.33541705 3.30578544 2.76961187 2.3830409 ]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 0 obj = 5.126260825771548
eta = 0.6862682501224668
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
eta_min = 0.686268250122481	eta_max = 0.6862682501224266
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 0.026043258509324534	eta = 0.9090909090909091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 17.617751045101738	eta = 0.0013438542463973541
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7582687679572628	eta = 0.01346534158224146
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245653152193903	eta = 0.013728496882659091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245608478782077	eta = 0.013728532445265985
eta = 0.013728532445265985
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.59038678e-04 1.43473385e-03 1.52966329e-04 6.12343236e-06
 2.22458099e-03 2.44815942e-04 1.21042015e-05 4.32137624e-04
 2.04770398e-04 1.29108736e-04]
ene_total = [0.16678716 0.18136157 0.17007523 0.15155948 0.20030673 0.16453951
 0.15094588 0.151879   0.22536052 0.16174577]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 1 obj = 5.126260825771778
eta = 0.686268250122481
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
Done!
At round 23 energy consumption: 5.126260825771548
At round 23 eta: 0.686268250122481
At round 23 local rounds: 12.328087095105714
At round 23 global rounds: 64.71786290003097
At round 23 a_n: 19.96150252899076
gradient difference: 0.5622169375419617
train() client id: f_00000-0-0 loss: 1.260564  [   32/  126]
train() client id: f_00000-0-1 loss: 1.096648  [   64/  126]
train() client id: f_00000-0-2 loss: 1.148537  [   96/  126]
train() client id: f_00000-1-0 loss: 1.093120  [   32/  126]
train() client id: f_00000-1-1 loss: 1.171483  [   64/  126]
train() client id: f_00000-1-2 loss: 1.028190  [   96/  126]
train() client id: f_00000-2-0 loss: 1.119928  [   32/  126]
train() client id: f_00000-2-1 loss: 0.966215  [   64/  126]
train() client id: f_00000-2-2 loss: 1.140359  [   96/  126]
train() client id: f_00000-3-0 loss: 0.969194  [   32/  126]
train() client id: f_00000-3-1 loss: 0.981206  [   64/  126]
train() client id: f_00000-3-2 loss: 0.905430  [   96/  126]
train() client id: f_00000-4-0 loss: 0.885735  [   32/  126]
train() client id: f_00000-4-1 loss: 1.001392  [   64/  126]
train() client id: f_00000-4-2 loss: 0.946919  [   96/  126]
train() client id: f_00000-5-0 loss: 0.933985  [   32/  126]
train() client id: f_00000-5-1 loss: 0.922354  [   64/  126]
train() client id: f_00000-5-2 loss: 0.853260  [   96/  126]
train() client id: f_00000-6-0 loss: 0.917504  [   32/  126]
train() client id: f_00000-6-1 loss: 0.913187  [   64/  126]
train() client id: f_00000-6-2 loss: 0.882985  [   96/  126]
train() client id: f_00000-7-0 loss: 0.847462  [   32/  126]
train() client id: f_00000-7-1 loss: 0.939006  [   64/  126]
train() client id: f_00000-7-2 loss: 0.823832  [   96/  126]
train() client id: f_00000-8-0 loss: 0.872873  [   32/  126]
train() client id: f_00000-8-1 loss: 0.829394  [   64/  126]
train() client id: f_00000-8-2 loss: 0.920504  [   96/  126]
train() client id: f_00000-9-0 loss: 0.955988  [   32/  126]
train() client id: f_00000-9-1 loss: 0.890426  [   64/  126]
train() client id: f_00000-9-2 loss: 0.994722  [   96/  126]
train() client id: f_00000-10-0 loss: 0.987187  [   32/  126]
train() client id: f_00000-10-1 loss: 0.892637  [   64/  126]
train() client id: f_00000-10-2 loss: 0.894659  [   96/  126]
train() client id: f_00000-11-0 loss: 0.934185  [   32/  126]
train() client id: f_00000-11-1 loss: 0.878443  [   64/  126]
train() client id: f_00000-11-2 loss: 0.853658  [   96/  126]
train() client id: f_00001-0-0 loss: 0.525276  [   32/  265]
train() client id: f_00001-0-1 loss: 0.677353  [   64/  265]
train() client id: f_00001-0-2 loss: 0.540453  [   96/  265]
train() client id: f_00001-0-3 loss: 0.437635  [  128/  265]
train() client id: f_00001-0-4 loss: 0.432707  [  160/  265]
train() client id: f_00001-0-5 loss: 0.441145  [  192/  265]
train() client id: f_00001-0-6 loss: 0.635548  [  224/  265]
train() client id: f_00001-0-7 loss: 0.581512  [  256/  265]
train() client id: f_00001-1-0 loss: 0.481594  [   32/  265]
train() client id: f_00001-1-1 loss: 0.451121  [   64/  265]
train() client id: f_00001-1-2 loss: 0.520014  [   96/  265]
train() client id: f_00001-1-3 loss: 0.517495  [  128/  265]
train() client id: f_00001-1-4 loss: 0.479527  [  160/  265]
train() client id: f_00001-1-5 loss: 0.523758  [  192/  265]
train() client id: f_00001-1-6 loss: 0.618131  [  224/  265]
train() client id: f_00001-1-7 loss: 0.592841  [  256/  265]
train() client id: f_00001-2-0 loss: 0.489320  [   32/  265]
train() client id: f_00001-2-1 loss: 0.416542  [   64/  265]
train() client id: f_00001-2-2 loss: 0.495840  [   96/  265]
train() client id: f_00001-2-3 loss: 0.576727  [  128/  265]
train() client id: f_00001-2-4 loss: 0.495596  [  160/  265]
train() client id: f_00001-2-5 loss: 0.534992  [  192/  265]
train() client id: f_00001-2-6 loss: 0.611145  [  224/  265]
train() client id: f_00001-2-7 loss: 0.526935  [  256/  265]
train() client id: f_00001-3-0 loss: 0.571669  [   32/  265]
train() client id: f_00001-3-1 loss: 0.485816  [   64/  265]
train() client id: f_00001-3-2 loss: 0.533598  [   96/  265]
train() client id: f_00001-3-3 loss: 0.465328  [  128/  265]
train() client id: f_00001-3-4 loss: 0.557421  [  160/  265]
train() client id: f_00001-3-5 loss: 0.432230  [  192/  265]
train() client id: f_00001-3-6 loss: 0.501110  [  224/  265]
train() client id: f_00001-3-7 loss: 0.571499  [  256/  265]
train() client id: f_00001-4-0 loss: 0.458982  [   32/  265]
train() client id: f_00001-4-1 loss: 0.633178  [   64/  265]
train() client id: f_00001-4-2 loss: 0.464583  [   96/  265]
train() client id: f_00001-4-3 loss: 0.567868  [  128/  265]
train() client id: f_00001-4-4 loss: 0.517146  [  160/  265]
train() client id: f_00001-4-5 loss: 0.417529  [  192/  265]
train() client id: f_00001-4-6 loss: 0.424610  [  224/  265]
train() client id: f_00001-4-7 loss: 0.517208  [  256/  265]
train() client id: f_00001-5-0 loss: 0.509664  [   32/  265]
train() client id: f_00001-5-1 loss: 0.406774  [   64/  265]
train() client id: f_00001-5-2 loss: 0.664257  [   96/  265]
train() client id: f_00001-5-3 loss: 0.622264  [  128/  265]
train() client id: f_00001-5-4 loss: 0.480445  [  160/  265]
train() client id: f_00001-5-5 loss: 0.467741  [  192/  265]
train() client id: f_00001-5-6 loss: 0.481750  [  224/  265]
train() client id: f_00001-5-7 loss: 0.417615  [  256/  265]
train() client id: f_00001-6-0 loss: 0.520206  [   32/  265]
train() client id: f_00001-6-1 loss: 0.589679  [   64/  265]
train() client id: f_00001-6-2 loss: 0.475770  [   96/  265]
train() client id: f_00001-6-3 loss: 0.476169  [  128/  265]
train() client id: f_00001-6-4 loss: 0.412941  [  160/  265]
train() client id: f_00001-6-5 loss: 0.536622  [  192/  265]
train() client id: f_00001-6-6 loss: 0.478220  [  224/  265]
train() client id: f_00001-6-7 loss: 0.555128  [  256/  265]
train() client id: f_00001-7-0 loss: 0.498479  [   32/  265]
train() client id: f_00001-7-1 loss: 0.501274  [   64/  265]
train() client id: f_00001-7-2 loss: 0.450712  [   96/  265]
train() client id: f_00001-7-3 loss: 0.465442  [  128/  265]
train() client id: f_00001-7-4 loss: 0.409252  [  160/  265]
train() client id: f_00001-7-5 loss: 0.493635  [  192/  265]
train() client id: f_00001-7-6 loss: 0.492511  [  224/  265]
train() client id: f_00001-7-7 loss: 0.712117  [  256/  265]
train() client id: f_00001-8-0 loss: 0.512931  [   32/  265]
train() client id: f_00001-8-1 loss: 0.467768  [   64/  265]
train() client id: f_00001-8-2 loss: 0.492787  [   96/  265]
train() client id: f_00001-8-3 loss: 0.564812  [  128/  265]
train() client id: f_00001-8-4 loss: 0.645445  [  160/  265]
train() client id: f_00001-8-5 loss: 0.509506  [  192/  265]
train() client id: f_00001-8-6 loss: 0.413799  [  224/  265]
train() client id: f_00001-8-7 loss: 0.410669  [  256/  265]
train() client id: f_00001-9-0 loss: 0.453635  [   32/  265]
train() client id: f_00001-9-1 loss: 0.524978  [   64/  265]
train() client id: f_00001-9-2 loss: 0.544009  [   96/  265]
train() client id: f_00001-9-3 loss: 0.443626  [  128/  265]
train() client id: f_00001-9-4 loss: 0.572769  [  160/  265]
train() client id: f_00001-9-5 loss: 0.421080  [  192/  265]
train() client id: f_00001-9-6 loss: 0.466750  [  224/  265]
train() client id: f_00001-9-7 loss: 0.596853  [  256/  265]
train() client id: f_00001-10-0 loss: 0.480943  [   32/  265]
train() client id: f_00001-10-1 loss: 0.411114  [   64/  265]
train() client id: f_00001-10-2 loss: 0.532881  [   96/  265]
train() client id: f_00001-10-3 loss: 0.430883  [  128/  265]
train() client id: f_00001-10-4 loss: 0.487226  [  160/  265]
train() client id: f_00001-10-5 loss: 0.651605  [  192/  265]
train() client id: f_00001-10-6 loss: 0.523401  [  224/  265]
train() client id: f_00001-10-7 loss: 0.495978  [  256/  265]
train() client id: f_00001-11-0 loss: 0.519349  [   32/  265]
train() client id: f_00001-11-1 loss: 0.417618  [   64/  265]
train() client id: f_00001-11-2 loss: 0.507852  [   96/  265]
train() client id: f_00001-11-3 loss: 0.487066  [  128/  265]
train() client id: f_00001-11-4 loss: 0.549096  [  160/  265]
train() client id: f_00001-11-5 loss: 0.419144  [  192/  265]
train() client id: f_00001-11-6 loss: 0.532700  [  224/  265]
train() client id: f_00001-11-7 loss: 0.520905  [  256/  265]
train() client id: f_00002-0-0 loss: 1.285943  [   32/  124]
train() client id: f_00002-0-1 loss: 1.299254  [   64/  124]
train() client id: f_00002-0-2 loss: 1.122528  [   96/  124]
train() client id: f_00002-1-0 loss: 1.213641  [   32/  124]
train() client id: f_00002-1-1 loss: 1.291709  [   64/  124]
train() client id: f_00002-1-2 loss: 1.186807  [   96/  124]
train() client id: f_00002-2-0 loss: 1.160199  [   32/  124]
train() client id: f_00002-2-1 loss: 1.128933  [   64/  124]
train() client id: f_00002-2-2 loss: 1.280404  [   96/  124]
train() client id: f_00002-3-0 loss: 1.351353  [   32/  124]
train() client id: f_00002-3-1 loss: 1.194862  [   64/  124]
train() client id: f_00002-3-2 loss: 1.020476  [   96/  124]
train() client id: f_00002-4-0 loss: 1.247804  [   32/  124]
train() client id: f_00002-4-1 loss: 1.133261  [   64/  124]
train() client id: f_00002-4-2 loss: 1.126465  [   96/  124]
train() client id: f_00002-5-0 loss: 1.121916  [   32/  124]
train() client id: f_00002-5-1 loss: 1.121293  [   64/  124]
train() client id: f_00002-5-2 loss: 1.231176  [   96/  124]
train() client id: f_00002-6-0 loss: 1.209051  [   32/  124]
train() client id: f_00002-6-1 loss: 1.016333  [   64/  124]
train() client id: f_00002-6-2 loss: 1.179677  [   96/  124]
train() client id: f_00002-7-0 loss: 0.980057  [   32/  124]
train() client id: f_00002-7-1 loss: 1.259528  [   64/  124]
train() client id: f_00002-7-2 loss: 1.055583  [   96/  124]
train() client id: f_00002-8-0 loss: 1.063920  [   32/  124]
train() client id: f_00002-8-1 loss: 1.086012  [   64/  124]
train() client id: f_00002-8-2 loss: 1.046307  [   96/  124]
train() client id: f_00002-9-0 loss: 1.176018  [   32/  124]
train() client id: f_00002-9-1 loss: 1.097121  [   64/  124]
train() client id: f_00002-9-2 loss: 1.121043  [   96/  124]
train() client id: f_00002-10-0 loss: 1.131741  [   32/  124]
train() client id: f_00002-10-1 loss: 1.091695  [   64/  124]
train() client id: f_00002-10-2 loss: 1.013616  [   96/  124]
train() client id: f_00002-11-0 loss: 1.117359  [   32/  124]
train() client id: f_00002-11-1 loss: 1.000314  [   64/  124]
train() client id: f_00002-11-2 loss: 1.099660  [   96/  124]
train() client id: f_00003-0-0 loss: 0.495822  [   32/   43]
train() client id: f_00003-1-0 loss: 0.540731  [   32/   43]
train() client id: f_00003-2-0 loss: 0.681974  [   32/   43]
train() client id: f_00003-3-0 loss: 0.533178  [   32/   43]
train() client id: f_00003-4-0 loss: 0.676087  [   32/   43]
train() client id: f_00003-5-0 loss: 0.486014  [   32/   43]
train() client id: f_00003-6-0 loss: 0.417919  [   32/   43]
train() client id: f_00003-7-0 loss: 0.486723  [   32/   43]
train() client id: f_00003-8-0 loss: 0.653885  [   32/   43]
train() client id: f_00003-9-0 loss: 0.598722  [   32/   43]
train() client id: f_00003-10-0 loss: 0.519476  [   32/   43]
train() client id: f_00003-11-0 loss: 0.400942  [   32/   43]
train() client id: f_00004-0-0 loss: 0.874860  [   32/  306]
train() client id: f_00004-0-1 loss: 0.834940  [   64/  306]
train() client id: f_00004-0-2 loss: 0.826451  [   96/  306]
train() client id: f_00004-0-3 loss: 0.815185  [  128/  306]
train() client id: f_00004-0-4 loss: 0.799499  [  160/  306]
train() client id: f_00004-0-5 loss: 0.869459  [  192/  306]
train() client id: f_00004-0-6 loss: 0.874712  [  224/  306]
train() client id: f_00004-0-7 loss: 0.743015  [  256/  306]
train() client id: f_00004-0-8 loss: 0.878415  [  288/  306]
train() client id: f_00004-1-0 loss: 0.657838  [   32/  306]
train() client id: f_00004-1-1 loss: 0.938418  [   64/  306]
train() client id: f_00004-1-2 loss: 1.021469  [   96/  306]
train() client id: f_00004-1-3 loss: 0.843976  [  128/  306]
train() client id: f_00004-1-4 loss: 0.746761  [  160/  306]
train() client id: f_00004-1-5 loss: 0.851420  [  192/  306]
train() client id: f_00004-1-6 loss: 0.861549  [  224/  306]
train() client id: f_00004-1-7 loss: 0.863740  [  256/  306]
train() client id: f_00004-1-8 loss: 0.922725  [  288/  306]
train() client id: f_00004-2-0 loss: 0.815891  [   32/  306]
train() client id: f_00004-2-1 loss: 0.790644  [   64/  306]
train() client id: f_00004-2-2 loss: 0.931263  [   96/  306]
train() client id: f_00004-2-3 loss: 0.921631  [  128/  306]
train() client id: f_00004-2-4 loss: 0.749418  [  160/  306]
train() client id: f_00004-2-5 loss: 0.797830  [  192/  306]
train() client id: f_00004-2-6 loss: 0.853335  [  224/  306]
train() client id: f_00004-2-7 loss: 0.788883  [  256/  306]
train() client id: f_00004-2-8 loss: 0.921414  [  288/  306]
train() client id: f_00004-3-0 loss: 0.813119  [   32/  306]
train() client id: f_00004-3-1 loss: 0.785528  [   64/  306]
train() client id: f_00004-3-2 loss: 0.929476  [   96/  306]
train() client id: f_00004-3-3 loss: 0.895674  [  128/  306]
train() client id: f_00004-3-4 loss: 0.802852  [  160/  306]
train() client id: f_00004-3-5 loss: 0.762073  [  192/  306]
train() client id: f_00004-3-6 loss: 0.808319  [  224/  306]
train() client id: f_00004-3-7 loss: 0.934520  [  256/  306]
train() client id: f_00004-3-8 loss: 0.827066  [  288/  306]
train() client id: f_00004-4-0 loss: 0.804055  [   32/  306]
train() client id: f_00004-4-1 loss: 0.820379  [   64/  306]
train() client id: f_00004-4-2 loss: 1.009996  [   96/  306]
train() client id: f_00004-4-3 loss: 0.825052  [  128/  306]
train() client id: f_00004-4-4 loss: 0.832823  [  160/  306]
train() client id: f_00004-4-5 loss: 0.785825  [  192/  306]
train() client id: f_00004-4-6 loss: 0.820521  [  224/  306]
train() client id: f_00004-4-7 loss: 0.859862  [  256/  306]
train() client id: f_00004-4-8 loss: 0.863880  [  288/  306]
train() client id: f_00004-5-0 loss: 0.813132  [   32/  306]
train() client id: f_00004-5-1 loss: 0.830436  [   64/  306]
train() client id: f_00004-5-2 loss: 0.884922  [   96/  306]
train() client id: f_00004-5-3 loss: 0.718267  [  128/  306]
train() client id: f_00004-5-4 loss: 0.786298  [  160/  306]
train() client id: f_00004-5-5 loss: 0.980354  [  192/  306]
train() client id: f_00004-5-6 loss: 0.865298  [  224/  306]
train() client id: f_00004-5-7 loss: 0.909468  [  256/  306]
train() client id: f_00004-5-8 loss: 0.866561  [  288/  306]
train() client id: f_00004-6-0 loss: 0.926554  [   32/  306]
train() client id: f_00004-6-1 loss: 0.811235  [   64/  306]
train() client id: f_00004-6-2 loss: 0.781681  [   96/  306]
train() client id: f_00004-6-3 loss: 0.895073  [  128/  306]
train() client id: f_00004-6-4 loss: 0.965958  [  160/  306]
train() client id: f_00004-6-5 loss: 0.932051  [  192/  306]
train() client id: f_00004-6-6 loss: 0.829036  [  224/  306]
train() client id: f_00004-6-7 loss: 0.758351  [  256/  306]
train() client id: f_00004-6-8 loss: 0.736064  [  288/  306]
train() client id: f_00004-7-0 loss: 0.990941  [   32/  306]
train() client id: f_00004-7-1 loss: 0.761710  [   64/  306]
train() client id: f_00004-7-2 loss: 0.786126  [   96/  306]
train() client id: f_00004-7-3 loss: 0.889375  [  128/  306]
train() client id: f_00004-7-4 loss: 0.864545  [  160/  306]
train() client id: f_00004-7-5 loss: 0.829034  [  192/  306]
train() client id: f_00004-7-6 loss: 0.897838  [  224/  306]
train() client id: f_00004-7-7 loss: 0.831791  [  256/  306]
train() client id: f_00004-7-8 loss: 0.874267  [  288/  306]
train() client id: f_00004-8-0 loss: 0.930548  [   32/  306]
train() client id: f_00004-8-1 loss: 0.766368  [   64/  306]
train() client id: f_00004-8-2 loss: 0.886245  [   96/  306]
train() client id: f_00004-8-3 loss: 0.932802  [  128/  306]
train() client id: f_00004-8-4 loss: 0.758397  [  160/  306]
train() client id: f_00004-8-5 loss: 0.865905  [  192/  306]
train() client id: f_00004-8-6 loss: 0.823781  [  224/  306]
train() client id: f_00004-8-7 loss: 0.743527  [  256/  306]
train() client id: f_00004-8-8 loss: 0.916255  [  288/  306]
train() client id: f_00004-9-0 loss: 0.770011  [   32/  306]
train() client id: f_00004-9-1 loss: 0.825170  [   64/  306]
train() client id: f_00004-9-2 loss: 0.901771  [   96/  306]
train() client id: f_00004-9-3 loss: 0.760793  [  128/  306]
train() client id: f_00004-9-4 loss: 0.768451  [  160/  306]
train() client id: f_00004-9-5 loss: 0.937649  [  192/  306]
train() client id: f_00004-9-6 loss: 0.911753  [  224/  306]
train() client id: f_00004-9-7 loss: 0.842862  [  256/  306]
train() client id: f_00004-9-8 loss: 0.905975  [  288/  306]
train() client id: f_00004-10-0 loss: 0.887221  [   32/  306]
train() client id: f_00004-10-1 loss: 0.748603  [   64/  306]
train() client id: f_00004-10-2 loss: 0.795595  [   96/  306]
train() client id: f_00004-10-3 loss: 0.742750  [  128/  306]
train() client id: f_00004-10-4 loss: 0.943232  [  160/  306]
train() client id: f_00004-10-5 loss: 0.724589  [  192/  306]
train() client id: f_00004-10-6 loss: 0.948973  [  224/  306]
train() client id: f_00004-10-7 loss: 0.893268  [  256/  306]
train() client id: f_00004-10-8 loss: 0.881628  [  288/  306]
train() client id: f_00004-11-0 loss: 0.866083  [   32/  306]
train() client id: f_00004-11-1 loss: 0.873092  [   64/  306]
train() client id: f_00004-11-2 loss: 0.904534  [   96/  306]
train() client id: f_00004-11-3 loss: 0.874178  [  128/  306]
train() client id: f_00004-11-4 loss: 0.907843  [  160/  306]
train() client id: f_00004-11-5 loss: 0.823479  [  192/  306]
train() client id: f_00004-11-6 loss: 0.753934  [  224/  306]
train() client id: f_00004-11-7 loss: 0.759065  [  256/  306]
train() client id: f_00004-11-8 loss: 0.819052  [  288/  306]
train() client id: f_00005-0-0 loss: 0.585487  [   32/  146]
train() client id: f_00005-0-1 loss: 0.510661  [   64/  146]
train() client id: f_00005-0-2 loss: 0.726269  [   96/  146]
train() client id: f_00005-0-3 loss: 0.703443  [  128/  146]
train() client id: f_00005-1-0 loss: 0.740839  [   32/  146]
train() client id: f_00005-1-1 loss: 0.632937  [   64/  146]
train() client id: f_00005-1-2 loss: 0.534748  [   96/  146]
train() client id: f_00005-1-3 loss: 0.546720  [  128/  146]
train() client id: f_00005-2-0 loss: 0.866881  [   32/  146]
train() client id: f_00005-2-1 loss: 0.637327  [   64/  146]
train() client id: f_00005-2-2 loss: 0.590120  [   96/  146]
train() client id: f_00005-2-3 loss: 0.579669  [  128/  146]
train() client id: f_00005-3-0 loss: 0.325416  [   32/  146]
train() client id: f_00005-3-1 loss: 0.380421  [   64/  146]
train() client id: f_00005-3-2 loss: 0.901106  [   96/  146]
train() client id: f_00005-3-3 loss: 0.808514  [  128/  146]
train() client id: f_00005-4-0 loss: 0.543742  [   32/  146]
train() client id: f_00005-4-1 loss: 0.475143  [   64/  146]
train() client id: f_00005-4-2 loss: 0.669691  [   96/  146]
train() client id: f_00005-4-3 loss: 0.788144  [  128/  146]
train() client id: f_00005-5-0 loss: 0.524351  [   32/  146]
train() client id: f_00005-5-1 loss: 0.772498  [   64/  146]
train() client id: f_00005-5-2 loss: 0.629323  [   96/  146]
train() client id: f_00005-5-3 loss: 0.415058  [  128/  146]
train() client id: f_00005-6-0 loss: 0.503333  [   32/  146]
train() client id: f_00005-6-1 loss: 0.497809  [   64/  146]
train() client id: f_00005-6-2 loss: 0.586426  [   96/  146]
train() client id: f_00005-6-3 loss: 0.959986  [  128/  146]
train() client id: f_00005-7-0 loss: 0.634417  [   32/  146]
train() client id: f_00005-7-1 loss: 0.431466  [   64/  146]
train() client id: f_00005-7-2 loss: 0.377535  [   96/  146]
train() client id: f_00005-7-3 loss: 0.975656  [  128/  146]
train() client id: f_00005-8-0 loss: 0.584507  [   32/  146]
train() client id: f_00005-8-1 loss: 0.630045  [   64/  146]
train() client id: f_00005-8-2 loss: 0.673087  [   96/  146]
train() client id: f_00005-8-3 loss: 0.456605  [  128/  146]
train() client id: f_00005-9-0 loss: 0.517437  [   32/  146]
train() client id: f_00005-9-1 loss: 0.616252  [   64/  146]
train() client id: f_00005-9-2 loss: 0.472311  [   96/  146]
train() client id: f_00005-9-3 loss: 0.963607  [  128/  146]
train() client id: f_00005-10-0 loss: 0.383200  [   32/  146]
train() client id: f_00005-10-1 loss: 0.898669  [   64/  146]
train() client id: f_00005-10-2 loss: 0.542736  [   96/  146]
train() client id: f_00005-10-3 loss: 0.511841  [  128/  146]
train() client id: f_00005-11-0 loss: 0.825723  [   32/  146]
train() client id: f_00005-11-1 loss: 0.560927  [   64/  146]
train() client id: f_00005-11-2 loss: 0.357370  [   96/  146]
train() client id: f_00005-11-3 loss: 0.620152  [  128/  146]
train() client id: f_00006-0-0 loss: 0.606948  [   32/   54]
train() client id: f_00006-1-0 loss: 0.612591  [   32/   54]
train() client id: f_00006-2-0 loss: 0.643734  [   32/   54]
train() client id: f_00006-3-0 loss: 0.613095  [   32/   54]
train() client id: f_00006-4-0 loss: 0.601838  [   32/   54]
train() client id: f_00006-5-0 loss: 0.603202  [   32/   54]
train() client id: f_00006-6-0 loss: 0.624216  [   32/   54]
train() client id: f_00006-7-0 loss: 0.552384  [   32/   54]
train() client id: f_00006-8-0 loss: 0.649601  [   32/   54]
train() client id: f_00006-9-0 loss: 0.558122  [   32/   54]
train() client id: f_00006-10-0 loss: 0.657173  [   32/   54]
train() client id: f_00006-11-0 loss: 0.535581  [   32/   54]
train() client id: f_00007-0-0 loss: 0.491124  [   32/  179]
train() client id: f_00007-0-1 loss: 0.719835  [   64/  179]
train() client id: f_00007-0-2 loss: 0.562245  [   96/  179]
train() client id: f_00007-0-3 loss: 0.737487  [  128/  179]
train() client id: f_00007-0-4 loss: 0.474142  [  160/  179]
train() client id: f_00007-1-0 loss: 0.611072  [   32/  179]
train() client id: f_00007-1-1 loss: 0.733908  [   64/  179]
train() client id: f_00007-1-2 loss: 0.443355  [   96/  179]
train() client id: f_00007-1-3 loss: 0.574227  [  128/  179]
train() client id: f_00007-1-4 loss: 0.730478  [  160/  179]
train() client id: f_00007-2-0 loss: 0.644825  [   32/  179]
train() client id: f_00007-2-1 loss: 0.492658  [   64/  179]
train() client id: f_00007-2-2 loss: 0.686837  [   96/  179]
train() client id: f_00007-2-3 loss: 0.489512  [  128/  179]
train() client id: f_00007-2-4 loss: 0.611100  [  160/  179]
train() client id: f_00007-3-0 loss: 0.646410  [   32/  179]
train() client id: f_00007-3-1 loss: 0.695367  [   64/  179]
train() client id: f_00007-3-2 loss: 0.666601  [   96/  179]
train() client id: f_00007-3-3 loss: 0.539693  [  128/  179]
train() client id: f_00007-3-4 loss: 0.496122  [  160/  179]
train() client id: f_00007-4-0 loss: 0.572482  [   32/  179]
train() client id: f_00007-4-1 loss: 0.550305  [   64/  179]
train() client id: f_00007-4-2 loss: 0.638727  [   96/  179]
train() client id: f_00007-4-3 loss: 0.600562  [  128/  179]
train() client id: f_00007-4-4 loss: 0.677902  [  160/  179]
train() client id: f_00007-5-0 loss: 0.697440  [   32/  179]
train() client id: f_00007-5-1 loss: 0.578499  [   64/  179]
train() client id: f_00007-5-2 loss: 0.559335  [   96/  179]
train() client id: f_00007-5-3 loss: 0.668414  [  128/  179]
train() client id: f_00007-5-4 loss: 0.521240  [  160/  179]
train() client id: f_00007-6-0 loss: 0.561878  [   32/  179]
train() client id: f_00007-6-1 loss: 0.473791  [   64/  179]
train() client id: f_00007-6-2 loss: 0.550822  [   96/  179]
train() client id: f_00007-6-3 loss: 0.615615  [  128/  179]
train() client id: f_00007-6-4 loss: 0.546185  [  160/  179]
train() client id: f_00007-7-0 loss: 0.542000  [   32/  179]
train() client id: f_00007-7-1 loss: 0.460451  [   64/  179]
train() client id: f_00007-7-2 loss: 0.438408  [   96/  179]
train() client id: f_00007-7-3 loss: 0.650574  [  128/  179]
train() client id: f_00007-7-4 loss: 0.836413  [  160/  179]
train() client id: f_00007-8-0 loss: 0.613919  [   32/  179]
train() client id: f_00007-8-1 loss: 0.610756  [   64/  179]
train() client id: f_00007-8-2 loss: 0.544719  [   96/  179]
train() client id: f_00007-8-3 loss: 0.698021  [  128/  179]
train() client id: f_00007-8-4 loss: 0.532877  [  160/  179]
train() client id: f_00007-9-0 loss: 0.475040  [   32/  179]
train() client id: f_00007-9-1 loss: 0.582801  [   64/  179]
train() client id: f_00007-9-2 loss: 0.646158  [   96/  179]
train() client id: f_00007-9-3 loss: 0.581413  [  128/  179]
train() client id: f_00007-9-4 loss: 0.530832  [  160/  179]
train() client id: f_00007-10-0 loss: 0.777063  [   32/  179]
train() client id: f_00007-10-1 loss: 0.415062  [   64/  179]
train() client id: f_00007-10-2 loss: 0.649516  [   96/  179]
train() client id: f_00007-10-3 loss: 0.543916  [  128/  179]
train() client id: f_00007-10-4 loss: 0.533810  [  160/  179]
train() client id: f_00007-11-0 loss: 0.627384  [   32/  179]
train() client id: f_00007-11-1 loss: 0.453357  [   64/  179]
train() client id: f_00007-11-2 loss: 0.626432  [   96/  179]
train() client id: f_00007-11-3 loss: 0.612149  [  128/  179]
train() client id: f_00007-11-4 loss: 0.604066  [  160/  179]
train() client id: f_00008-0-0 loss: 0.787923  [   32/  130]
train() client id: f_00008-0-1 loss: 0.727491  [   64/  130]
train() client id: f_00008-0-2 loss: 0.730936  [   96/  130]
train() client id: f_00008-0-3 loss: 0.753996  [  128/  130]
train() client id: f_00008-1-0 loss: 0.814257  [   32/  130]
train() client id: f_00008-1-1 loss: 0.729100  [   64/  130]
train() client id: f_00008-1-2 loss: 0.639065  [   96/  130]
train() client id: f_00008-1-3 loss: 0.850685  [  128/  130]
train() client id: f_00008-2-0 loss: 0.748888  [   32/  130]
train() client id: f_00008-2-1 loss: 0.780659  [   64/  130]
train() client id: f_00008-2-2 loss: 0.693633  [   96/  130]
train() client id: f_00008-2-3 loss: 0.800255  [  128/  130]
train() client id: f_00008-3-0 loss: 0.682974  [   32/  130]
train() client id: f_00008-3-1 loss: 0.932367  [   64/  130]
train() client id: f_00008-3-2 loss: 0.705357  [   96/  130]
train() client id: f_00008-3-3 loss: 0.679560  [  128/  130]
train() client id: f_00008-4-0 loss: 0.789269  [   32/  130]
train() client id: f_00008-4-1 loss: 0.727573  [   64/  130]
train() client id: f_00008-4-2 loss: 0.777907  [   96/  130]
train() client id: f_00008-4-3 loss: 0.699114  [  128/  130]
train() client id: f_00008-5-0 loss: 0.707031  [   32/  130]
train() client id: f_00008-5-1 loss: 0.744361  [   64/  130]
train() client id: f_00008-5-2 loss: 0.722280  [   96/  130]
train() client id: f_00008-5-3 loss: 0.861798  [  128/  130]
train() client id: f_00008-6-0 loss: 0.684064  [   32/  130]
train() client id: f_00008-6-1 loss: 0.764022  [   64/  130]
train() client id: f_00008-6-2 loss: 0.810631  [   96/  130]
train() client id: f_00008-6-3 loss: 0.764947  [  128/  130]
train() client id: f_00008-7-0 loss: 0.780614  [   32/  130]
train() client id: f_00008-7-1 loss: 0.694268  [   64/  130]
train() client id: f_00008-7-2 loss: 0.834342  [   96/  130]
train() client id: f_00008-7-3 loss: 0.718662  [  128/  130]
train() client id: f_00008-8-0 loss: 0.722599  [   32/  130]
train() client id: f_00008-8-1 loss: 0.776051  [   64/  130]
train() client id: f_00008-8-2 loss: 0.781667  [   96/  130]
train() client id: f_00008-8-3 loss: 0.729576  [  128/  130]
train() client id: f_00008-9-0 loss: 0.774900  [   32/  130]
train() client id: f_00008-9-1 loss: 0.869970  [   64/  130]
train() client id: f_00008-9-2 loss: 0.650691  [   96/  130]
train() client id: f_00008-9-3 loss: 0.700774  [  128/  130]
train() client id: f_00008-10-0 loss: 0.851939  [   32/  130]
train() client id: f_00008-10-1 loss: 0.707343  [   64/  130]
train() client id: f_00008-10-2 loss: 0.770731  [   96/  130]
train() client id: f_00008-10-3 loss: 0.709018  [  128/  130]
train() client id: f_00008-11-0 loss: 0.685558  [   32/  130]
train() client id: f_00008-11-1 loss: 0.704104  [   64/  130]
train() client id: f_00008-11-2 loss: 0.682126  [   96/  130]
train() client id: f_00008-11-3 loss: 0.910971  [  128/  130]
train() client id: f_00009-0-0 loss: 1.049740  [   32/  118]
train() client id: f_00009-0-1 loss: 1.044924  [   64/  118]
train() client id: f_00009-0-2 loss: 1.053612  [   96/  118]
train() client id: f_00009-1-0 loss: 0.928872  [   32/  118]
train() client id: f_00009-1-1 loss: 1.166739  [   64/  118]
train() client id: f_00009-1-2 loss: 1.026706  [   96/  118]
train() client id: f_00009-2-0 loss: 0.873470  [   32/  118]
train() client id: f_00009-2-1 loss: 1.021065  [   64/  118]
train() client id: f_00009-2-2 loss: 0.981432  [   96/  118]
train() client id: f_00009-3-0 loss: 0.989769  [   32/  118]
train() client id: f_00009-3-1 loss: 0.841823  [   64/  118]
train() client id: f_00009-3-2 loss: 0.968371  [   96/  118]
train() client id: f_00009-4-0 loss: 0.924366  [   32/  118]
train() client id: f_00009-4-1 loss: 0.985075  [   64/  118]
train() client id: f_00009-4-2 loss: 0.889045  [   96/  118]
train() client id: f_00009-5-0 loss: 0.921466  [   32/  118]
train() client id: f_00009-5-1 loss: 0.888543  [   64/  118]
train() client id: f_00009-5-2 loss: 0.913415  [   96/  118]
train() client id: f_00009-6-0 loss: 0.933972  [   32/  118]
train() client id: f_00009-6-1 loss: 0.840394  [   64/  118]
train() client id: f_00009-6-2 loss: 0.964918  [   96/  118]
train() client id: f_00009-7-0 loss: 0.870674  [   32/  118]
train() client id: f_00009-7-1 loss: 0.896073  [   64/  118]
train() client id: f_00009-7-2 loss: 0.833344  [   96/  118]
train() client id: f_00009-8-0 loss: 1.011341  [   32/  118]
train() client id: f_00009-8-1 loss: 0.821715  [   64/  118]
train() client id: f_00009-8-2 loss: 0.925811  [   96/  118]
train() client id: f_00009-9-0 loss: 0.796458  [   32/  118]
train() client id: f_00009-9-1 loss: 0.947878  [   64/  118]
train() client id: f_00009-9-2 loss: 0.902656  [   96/  118]
train() client id: f_00009-10-0 loss: 0.848977  [   32/  118]
train() client id: f_00009-10-1 loss: 0.970046  [   64/  118]
train() client id: f_00009-10-2 loss: 0.813492  [   96/  118]
train() client id: f_00009-11-0 loss: 0.868619  [   32/  118]
train() client id: f_00009-11-1 loss: 0.915216  [   64/  118]
train() client id: f_00009-11-2 loss: 0.948600  [   96/  118]
At round 23 accuracy: 0.6419098143236074
At round 23 training accuracy: 0.5841716968477532
At round 23 training loss: 0.8288398322893947
update_location
xs = [ -3.9056584    4.20031788 135.00902392  18.81129433   0.97929623
   3.95640986 -97.44319194 -76.32485185 119.66397685 -62.06087855]
ys = [127.5879595  110.55583871   1.32061395 -97.45517586  89.35018685
  72.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [162.15406741 149.13160679 168.01541763 140.89491153 134.10598387
 123.76410517 139.64979842 125.80206384 156.93354431 117.76062374]
dists_bs = [177.54203815 190.43582607 355.34507742 334.35705565 195.71561817
 206.07606464 193.81871306 200.18651135 334.08288959 204.92861778]
uav_gains = [2.97801922e-11 3.67836040e-11 2.72058983e-11 4.24194154e-11
 4.80041803e-11 5.86786623e-11 4.33735009e-11 5.63299134e-11
 3.23510419e-11 6.64482197e-11]
bs_gains = [5.56225278e-11 4.57085931e-11 7.97027370e-12 9.45158637e-12
 4.23392081e-11 3.66451867e-11 4.35097021e-11 3.97444612e-11
 9.47332053e-12 3.72226060e-11]
Round 24
-------------------------------
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.82454946 16.26273367  7.70634767  2.76510758 18.75837368  9.03341251
  3.43331975 11.02682771  8.12439108  7.33018602]
obj_prev = 92.26524913835074
eta_min = 1.3786631317009973e-12	eta_max = 0.9235436873576162
af = 19.491574900134562	bf = 1.580577038913744	zeta = 21.44073239014802	eta = 0.909090909090909
af = 19.491574900134562	bf = 1.580577038913744	zeta = 37.821548020465336	eta = 0.5153563489677266
af = 19.491574900134562	bf = 1.580577038913744	zeta = 29.921938016706985	eta = 0.65141418611493
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.500452198585428	eta = 0.6839040575328834
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428583728430333	eta = 0.6856329912995908
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428386266155645	eta = 0.685637753675084
eta = 0.685637753675084
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.03114263 0.06549838 0.0306483  0.01062804 0.07563209 0.0360859
 0.01334684 0.0442423  0.03213128 0.02916532]
ene_total = [2.48895914 4.62483507 2.46833909 1.14803217 5.27587599 2.77970132
 1.31845091 3.25561899 2.72998043 2.33859317]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 0 obj = 5.054794435766291
eta = 0.685637753675084
freqs = [41745313.04826085 86137162.39129016 41270331.48315956 14019810.25468265
 99776755.16786237 47903390.74277508 17590043.05099849 57717284.54230393
 46575586.46414848 38689551.73454573]
eta_min = 0.6856377536750926	eta_max = 0.6856377536750637
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 0.024656332621380766	eta = 0.909090909090909
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 17.409847423005747	eta = 0.0012874810038828607
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7319792587002336	eta = 0.012941753040645609
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7000222977574324	eta = 0.013185031671165255
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.700018383049954	eta = 0.013185062032920516
eta = 0.013185062032920516
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.55616847e-04 1.39346998e-03 1.49681533e-04 5.98995210e-06
 2.15899085e-03 2.37441151e-04 1.18412674e-05 4.22605781e-04
 1.99862188e-04 1.25181464e-04]
ene_total = [0.16607727 0.17657133 0.16938779 0.15085494 0.1944668  0.16035839
 0.15026585 0.15072403 0.2237539  0.15755809]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 1 obj = 5.0547944357664285
eta = 0.6856377536750926
freqs = [41745313.04826085 86137162.3912901  41270331.48315956 14019810.25468264
 99776755.16786231 47903390.74277506 17590043.05099848 57717284.54230388
 46575586.46414861 38689551.73454572]
Done!
At round 24 energy consumption: 5.054794435766291
At round 24 eta: 0.6856377536750926
At round 24 local rounds: 12.358184869136853
At round 24 global rounds: 63.498409119903215
At round 24 a_n: 19.61895668202144
gradient difference: 0.3845975697040558
train() client id: f_00000-0-0 loss: 1.166229  [   32/  126]
train() client id: f_00000-0-1 loss: 1.303865  [   64/  126]
train() client id: f_00000-0-2 loss: 1.193401  [   96/  126]
train() client id: f_00000-1-0 loss: 1.244667  [   32/  126]
train() client id: f_00000-1-1 loss: 1.034426  [   64/  126]
train() client id: f_00000-1-2 loss: 1.083156  [   96/  126]
train() client id: f_00000-2-0 loss: 0.844814  [   32/  126]
train() client id: f_00000-2-1 loss: 1.055830  [   64/  126]
train() client id: f_00000-2-2 loss: 1.072495  [   96/  126]
train() client id: f_00000-3-0 loss: 0.972071  [   32/  126]
train() client id: f_00000-3-1 loss: 1.038300  [   64/  126]
train() client id: f_00000-3-2 loss: 1.110726  [   96/  126]
train() client id: f_00000-4-0 loss: 0.910040  [   32/  126]
train() client id: f_00000-4-1 loss: 1.119637  [   64/  126]
train() client id: f_00000-4-2 loss: 0.919719  [   96/  126]
train() client id: f_00000-5-0 loss: 0.984231  [   32/  126]
train() client id: f_00000-5-1 loss: 0.836501  [   64/  126]
train() client id: f_00000-5-2 loss: 0.983993  [   96/  126]
train() client id: f_00000-6-0 loss: 0.978386  [   32/  126]
train() client id: f_00000-6-1 loss: 0.861752  [   64/  126]
train() client id: f_00000-6-2 loss: 0.894325  [   96/  126]
train() client id: f_00000-7-0 loss: 0.946385  [   32/  126]
train() client id: f_00000-7-1 loss: 0.933207  [   64/  126]
train() client id: f_00000-7-2 loss: 0.833103  [   96/  126]
train() client id: f_00000-8-0 loss: 0.908083  [   32/  126]
train() client id: f_00000-8-1 loss: 0.840119  [   64/  126]
train() client id: f_00000-8-2 loss: 0.966797  [   96/  126]
train() client id: f_00000-9-0 loss: 0.933856  [   32/  126]
train() client id: f_00000-9-1 loss: 0.840900  [   64/  126]
train() client id: f_00000-9-2 loss: 0.942243  [   96/  126]
train() client id: f_00000-10-0 loss: 0.849688  [   32/  126]
train() client id: f_00000-10-1 loss: 0.904199  [   64/  126]
train() client id: f_00000-10-2 loss: 0.952603  [   96/  126]
train() client id: f_00000-11-0 loss: 0.796580  [   32/  126]
train() client id: f_00000-11-1 loss: 0.952642  [   64/  126]
train() client id: f_00000-11-2 loss: 0.969184  [   96/  126]
train() client id: f_00001-0-0 loss: 0.521967  [   32/  265]
train() client id: f_00001-0-1 loss: 0.570742  [   64/  265]
train() client id: f_00001-0-2 loss: 0.361890  [   96/  265]
train() client id: f_00001-0-3 loss: 0.357878  [  128/  265]
train() client id: f_00001-0-4 loss: 0.409820  [  160/  265]
train() client id: f_00001-0-5 loss: 0.469430  [  192/  265]
train() client id: f_00001-0-6 loss: 0.440105  [  224/  265]
train() client id: f_00001-0-7 loss: 0.504979  [  256/  265]
train() client id: f_00001-1-0 loss: 0.461242  [   32/  265]
train() client id: f_00001-1-1 loss: 0.473212  [   64/  265]
train() client id: f_00001-1-2 loss: 0.458083  [   96/  265]
train() client id: f_00001-1-3 loss: 0.375509  [  128/  265]
train() client id: f_00001-1-4 loss: 0.459076  [  160/  265]
train() client id: f_00001-1-5 loss: 0.370429  [  192/  265]
train() client id: f_00001-1-6 loss: 0.477231  [  224/  265]
train() client id: f_00001-1-7 loss: 0.479545  [  256/  265]
train() client id: f_00001-2-0 loss: 0.431820  [   32/  265]
train() client id: f_00001-2-1 loss: 0.493794  [   64/  265]
train() client id: f_00001-2-2 loss: 0.359690  [   96/  265]
train() client id: f_00001-2-3 loss: 0.441463  [  128/  265]
train() client id: f_00001-2-4 loss: 0.394940  [  160/  265]
train() client id: f_00001-2-5 loss: 0.389004  [  192/  265]
train() client id: f_00001-2-6 loss: 0.399402  [  224/  265]
train() client id: f_00001-2-7 loss: 0.495668  [  256/  265]
train() client id: f_00001-3-0 loss: 0.410770  [   32/  265]
train() client id: f_00001-3-1 loss: 0.376538  [   64/  265]
train() client id: f_00001-3-2 loss: 0.392622  [   96/  265]
train() client id: f_00001-3-3 loss: 0.543062  [  128/  265]
train() client id: f_00001-3-4 loss: 0.504797  [  160/  265]
train() client id: f_00001-3-5 loss: 0.531358  [  192/  265]
train() client id: f_00001-3-6 loss: 0.347330  [  224/  265]
train() client id: f_00001-3-7 loss: 0.333411  [  256/  265]
train() client id: f_00001-4-0 loss: 0.338245  [   32/  265]
train() client id: f_00001-4-1 loss: 0.415936  [   64/  265]
train() client id: f_00001-4-2 loss: 0.338883  [   96/  265]
train() client id: f_00001-4-3 loss: 0.535051  [  128/  265]
train() client id: f_00001-4-4 loss: 0.501402  [  160/  265]
train() client id: f_00001-4-5 loss: 0.451641  [  192/  265]
train() client id: f_00001-4-6 loss: 0.414411  [  224/  265]
train() client id: f_00001-4-7 loss: 0.404874  [  256/  265]
train() client id: f_00001-5-0 loss: 0.383058  [   32/  265]
train() client id: f_00001-5-1 loss: 0.377860  [   64/  265]
train() client id: f_00001-5-2 loss: 0.328692  [   96/  265]
train() client id: f_00001-5-3 loss: 0.424470  [  128/  265]
train() client id: f_00001-5-4 loss: 0.451505  [  160/  265]
train() client id: f_00001-5-5 loss: 0.461367  [  192/  265]
train() client id: f_00001-5-6 loss: 0.611978  [  224/  265]
train() client id: f_00001-5-7 loss: 0.332205  [  256/  265]
train() client id: f_00001-6-0 loss: 0.390187  [   32/  265]
train() client id: f_00001-6-1 loss: 0.417390  [   64/  265]
train() client id: f_00001-6-2 loss: 0.466311  [   96/  265]
train() client id: f_00001-6-3 loss: 0.318283  [  128/  265]
train() client id: f_00001-6-4 loss: 0.397402  [  160/  265]
train() client id: f_00001-6-5 loss: 0.504146  [  192/  265]
train() client id: f_00001-6-6 loss: 0.515282  [  224/  265]
train() client id: f_00001-6-7 loss: 0.331421  [  256/  265]
train() client id: f_00001-7-0 loss: 0.331795  [   32/  265]
train() client id: f_00001-7-1 loss: 0.361949  [   64/  265]
train() client id: f_00001-7-2 loss: 0.586535  [   96/  265]
train() client id: f_00001-7-3 loss: 0.338099  [  128/  265]
train() client id: f_00001-7-4 loss: 0.526688  [  160/  265]
train() client id: f_00001-7-5 loss: 0.322724  [  192/  265]
train() client id: f_00001-7-6 loss: 0.369562  [  224/  265]
train() client id: f_00001-7-7 loss: 0.389410  [  256/  265]
train() client id: f_00001-8-0 loss: 0.495882  [   32/  265]
train() client id: f_00001-8-1 loss: 0.462869  [   64/  265]
train() client id: f_00001-8-2 loss: 0.379260  [   96/  265]
train() client id: f_00001-8-3 loss: 0.376389  [  128/  265]
train() client id: f_00001-8-4 loss: 0.414573  [  160/  265]
train() client id: f_00001-8-5 loss: 0.357434  [  192/  265]
train() client id: f_00001-8-6 loss: 0.430898  [  224/  265]
train() client id: f_00001-8-7 loss: 0.401913  [  256/  265]
train() client id: f_00001-9-0 loss: 0.354200  [   32/  265]
train() client id: f_00001-9-1 loss: 0.351616  [   64/  265]
train() client id: f_00001-9-2 loss: 0.393951  [   96/  265]
train() client id: f_00001-9-3 loss: 0.435841  [  128/  265]
train() client id: f_00001-9-4 loss: 0.422920  [  160/  265]
train() client id: f_00001-9-5 loss: 0.430344  [  192/  265]
train() client id: f_00001-9-6 loss: 0.466882  [  224/  265]
train() client id: f_00001-9-7 loss: 0.439994  [  256/  265]
train() client id: f_00001-10-0 loss: 0.390324  [   32/  265]
train() client id: f_00001-10-1 loss: 0.358672  [   64/  265]
train() client id: f_00001-10-2 loss: 0.480308  [   96/  265]
train() client id: f_00001-10-3 loss: 0.394652  [  128/  265]
train() client id: f_00001-10-4 loss: 0.366913  [  160/  265]
train() client id: f_00001-10-5 loss: 0.425721  [  192/  265]
train() client id: f_00001-10-6 loss: 0.383159  [  224/  265]
train() client id: f_00001-10-7 loss: 0.286047  [  256/  265]
train() client id: f_00001-11-0 loss: 0.310171  [   32/  265]
train() client id: f_00001-11-1 loss: 0.391477  [   64/  265]
train() client id: f_00001-11-2 loss: 0.456422  [   96/  265]
train() client id: f_00001-11-3 loss: 0.319844  [  128/  265]
train() client id: f_00001-11-4 loss: 0.369152  [  160/  265]
train() client id: f_00001-11-5 loss: 0.510747  [  192/  265]
train() client id: f_00001-11-6 loss: 0.452514  [  224/  265]
train() client id: f_00001-11-7 loss: 0.432558  [  256/  265]
train() client id: f_00002-0-0 loss: 1.208560  [   32/  124]
train() client id: f_00002-0-1 loss: 1.204258  [   64/  124]
train() client id: f_00002-0-2 loss: 1.308570  [   96/  124]
train() client id: f_00002-1-0 loss: 1.076826  [   32/  124]
train() client id: f_00002-1-1 loss: 1.177173  [   64/  124]
train() client id: f_00002-1-2 loss: 1.275204  [   96/  124]
train() client id: f_00002-2-0 loss: 1.215822  [   32/  124]
train() client id: f_00002-2-1 loss: 1.250696  [   64/  124]
train() client id: f_00002-2-2 loss: 1.101619  [   96/  124]
train() client id: f_00002-3-0 loss: 1.149441  [   32/  124]
train() client id: f_00002-3-1 loss: 1.078434  [   64/  124]
train() client id: f_00002-3-2 loss: 1.110691  [   96/  124]
train() client id: f_00002-4-0 loss: 1.081338  [   32/  124]
train() client id: f_00002-4-1 loss: 1.121712  [   64/  124]
train() client id: f_00002-4-2 loss: 1.045192  [   96/  124]
train() client id: f_00002-5-0 loss: 1.055233  [   32/  124]
train() client id: f_00002-5-1 loss: 0.961737  [   64/  124]
train() client id: f_00002-5-2 loss: 1.028862  [   96/  124]
train() client id: f_00002-6-0 loss: 1.042453  [   32/  124]
train() client id: f_00002-6-1 loss: 1.048134  [   64/  124]
train() client id: f_00002-6-2 loss: 0.980357  [   96/  124]
train() client id: f_00002-7-0 loss: 0.958008  [   32/  124]
train() client id: f_00002-7-1 loss: 1.215957  [   64/  124]
train() client id: f_00002-7-2 loss: 0.955115  [   96/  124]
train() client id: f_00002-8-0 loss: 0.953215  [   32/  124]
train() client id: f_00002-8-1 loss: 0.973793  [   64/  124]
train() client id: f_00002-8-2 loss: 1.053227  [   96/  124]
train() client id: f_00002-9-0 loss: 0.920968  [   32/  124]
train() client id: f_00002-9-1 loss: 1.049013  [   64/  124]
train() client id: f_00002-9-2 loss: 0.937933  [   96/  124]
train() client id: f_00002-10-0 loss: 0.972770  [   32/  124]
train() client id: f_00002-10-1 loss: 0.823025  [   64/  124]
train() client id: f_00002-10-2 loss: 1.129442  [   96/  124]
train() client id: f_00002-11-0 loss: 0.922254  [   32/  124]
train() client id: f_00002-11-1 loss: 0.921419  [   64/  124]
train() client id: f_00002-11-2 loss: 0.923429  [   96/  124]
train() client id: f_00003-0-0 loss: 0.804492  [   32/   43]
train() client id: f_00003-1-0 loss: 0.843150  [   32/   43]
train() client id: f_00003-2-0 loss: 0.987288  [   32/   43]
train() client id: f_00003-3-0 loss: 0.862967  [   32/   43]
train() client id: f_00003-4-0 loss: 0.672249  [   32/   43]
train() client id: f_00003-5-0 loss: 1.024340  [   32/   43]
train() client id: f_00003-6-0 loss: 0.927109  [   32/   43]
train() client id: f_00003-7-0 loss: 0.791813  [   32/   43]
train() client id: f_00003-8-0 loss: 0.981083  [   32/   43]
train() client id: f_00003-9-0 loss: 0.825436  [   32/   43]
train() client id: f_00003-10-0 loss: 0.973396  [   32/   43]
train() client id: f_00003-11-0 loss: 0.958719  [   32/   43]
train() client id: f_00004-0-0 loss: 0.949863  [   32/  306]
train() client id: f_00004-0-1 loss: 0.834476  [   64/  306]
train() client id: f_00004-0-2 loss: 0.985774  [   96/  306]
train() client id: f_00004-0-3 loss: 0.937992  [  128/  306]
train() client id: f_00004-0-4 loss: 0.925852  [  160/  306]
train() client id: f_00004-0-5 loss: 0.872618  [  192/  306]
train() client id: f_00004-0-6 loss: 0.904379  [  224/  306]
train() client id: f_00004-0-7 loss: 0.932679  [  256/  306]
train() client id: f_00004-0-8 loss: 0.940614  [  288/  306]
train() client id: f_00004-1-0 loss: 0.911528  [   32/  306]
train() client id: f_00004-1-1 loss: 0.914036  [   64/  306]
train() client id: f_00004-1-2 loss: 0.983658  [   96/  306]
train() client id: f_00004-1-3 loss: 1.049189  [  128/  306]
train() client id: f_00004-1-4 loss: 1.006948  [  160/  306]
train() client id: f_00004-1-5 loss: 0.747818  [  192/  306]
train() client id: f_00004-1-6 loss: 0.849678  [  224/  306]
train() client id: f_00004-1-7 loss: 0.903021  [  256/  306]
train() client id: f_00004-1-8 loss: 0.856500  [  288/  306]
train() client id: f_00004-2-0 loss: 0.866758  [   32/  306]
train() client id: f_00004-2-1 loss: 1.022232  [   64/  306]
train() client id: f_00004-2-2 loss: 0.880684  [   96/  306]
train() client id: f_00004-2-3 loss: 0.907842  [  128/  306]
train() client id: f_00004-2-4 loss: 0.816203  [  160/  306]
train() client id: f_00004-2-5 loss: 0.870201  [  192/  306]
train() client id: f_00004-2-6 loss: 1.108918  [  224/  306]
train() client id: f_00004-2-7 loss: 0.882711  [  256/  306]
train() client id: f_00004-2-8 loss: 0.847845  [  288/  306]
train() client id: f_00004-3-0 loss: 0.955109  [   32/  306]
train() client id: f_00004-3-1 loss: 0.785537  [   64/  306]
train() client id: f_00004-3-2 loss: 0.926332  [   96/  306]
train() client id: f_00004-3-3 loss: 0.956383  [  128/  306]
train() client id: f_00004-3-4 loss: 0.950590  [  160/  306]
train() client id: f_00004-3-5 loss: 1.012477  [  192/  306]
train() client id: f_00004-3-6 loss: 0.862523  [  224/  306]
train() client id: f_00004-3-7 loss: 0.943186  [  256/  306]
train() client id: f_00004-3-8 loss: 0.915725  [  288/  306]
train() client id: f_00004-4-0 loss: 0.877722  [   32/  306]
train() client id: f_00004-4-1 loss: 0.917515  [   64/  306]
train() client id: f_00004-4-2 loss: 0.910761  [   96/  306]
train() client id: f_00004-4-3 loss: 0.873642  [  128/  306]
train() client id: f_00004-4-4 loss: 0.865959  [  160/  306]
train() client id: f_00004-4-5 loss: 0.919956  [  192/  306]
train() client id: f_00004-4-6 loss: 0.910335  [  224/  306]
train() client id: f_00004-4-7 loss: 1.059149  [  256/  306]
train() client id: f_00004-4-8 loss: 0.919601  [  288/  306]
train() client id: f_00004-5-0 loss: 0.864151  [   32/  306]
train() client id: f_00004-5-1 loss: 0.925973  [   64/  306]
train() client id: f_00004-5-2 loss: 0.841215  [   96/  306]
train() client id: f_00004-5-3 loss: 0.877901  [  128/  306]
train() client id: f_00004-5-4 loss: 0.800069  [  160/  306]
train() client id: f_00004-5-5 loss: 0.931875  [  192/  306]
train() client id: f_00004-5-6 loss: 1.015266  [  224/  306]
train() client id: f_00004-5-7 loss: 0.907964  [  256/  306]
train() client id: f_00004-5-8 loss: 1.013830  [  288/  306]
train() client id: f_00004-6-0 loss: 0.887229  [   32/  306]
train() client id: f_00004-6-1 loss: 0.957928  [   64/  306]
train() client id: f_00004-6-2 loss: 0.938770  [   96/  306]
train() client id: f_00004-6-3 loss: 0.935246  [  128/  306]
train() client id: f_00004-6-4 loss: 0.930563  [  160/  306]
train() client id: f_00004-6-5 loss: 0.887219  [  192/  306]
train() client id: f_00004-6-6 loss: 0.942912  [  224/  306]
train() client id: f_00004-6-7 loss: 0.912780  [  256/  306]
train() client id: f_00004-6-8 loss: 0.801535  [  288/  306]
train() client id: f_00004-7-0 loss: 0.875156  [   32/  306]
train() client id: f_00004-7-1 loss: 0.967086  [   64/  306]
train() client id: f_00004-7-2 loss: 0.833200  [   96/  306]
train() client id: f_00004-7-3 loss: 0.879342  [  128/  306]
train() client id: f_00004-7-4 loss: 0.882437  [  160/  306]
train() client id: f_00004-7-5 loss: 0.861968  [  192/  306]
train() client id: f_00004-7-6 loss: 0.948785  [  224/  306]
train() client id: f_00004-7-7 loss: 0.980103  [  256/  306]
train() client id: f_00004-7-8 loss: 0.954866  [  288/  306]
train() client id: f_00004-8-0 loss: 0.870443  [   32/  306]
train() client id: f_00004-8-1 loss: 1.035930  [   64/  306]
train() client id: f_00004-8-2 loss: 0.808865  [   96/  306]
train() client id: f_00004-8-3 loss: 0.785474  [  128/  306]
train() client id: f_00004-8-4 loss: 0.951338  [  160/  306]
train() client id: f_00004-8-5 loss: 1.018009  [  192/  306]
train() client id: f_00004-8-6 loss: 0.941241  [  224/  306]
train() client id: f_00004-8-7 loss: 0.964692  [  256/  306]
train() client id: f_00004-8-8 loss: 0.887855  [  288/  306]
train() client id: f_00004-9-0 loss: 0.858598  [   32/  306]
train() client id: f_00004-9-1 loss: 0.971436  [   64/  306]
train() client id: f_00004-9-2 loss: 1.007462  [   96/  306]
train() client id: f_00004-9-3 loss: 0.929740  [  128/  306]
train() client id: f_00004-9-4 loss: 0.878095  [  160/  306]
train() client id: f_00004-9-5 loss: 0.866537  [  192/  306]
train() client id: f_00004-9-6 loss: 0.939082  [  224/  306]
train() client id: f_00004-9-7 loss: 0.837245  [  256/  306]
train() client id: f_00004-9-8 loss: 0.892781  [  288/  306]
train() client id: f_00004-10-0 loss: 0.889225  [   32/  306]
train() client id: f_00004-10-1 loss: 0.976549  [   64/  306]
train() client id: f_00004-10-2 loss: 0.847181  [   96/  306]
train() client id: f_00004-10-3 loss: 1.023726  [  128/  306]
train() client id: f_00004-10-4 loss: 0.891259  [  160/  306]
train() client id: f_00004-10-5 loss: 0.884265  [  192/  306]
train() client id: f_00004-10-6 loss: 0.941800  [  224/  306]
train() client id: f_00004-10-7 loss: 0.876905  [  256/  306]
train() client id: f_00004-10-8 loss: 0.923554  [  288/  306]
train() client id: f_00004-11-0 loss: 0.972748  [   32/  306]
train() client id: f_00004-11-1 loss: 0.878052  [   64/  306]
train() client id: f_00004-11-2 loss: 0.982530  [   96/  306]
train() client id: f_00004-11-3 loss: 0.963488  [  128/  306]
train() client id: f_00004-11-4 loss: 0.844181  [  160/  306]
train() client id: f_00004-11-5 loss: 0.905236  [  192/  306]
train() client id: f_00004-11-6 loss: 0.996282  [  224/  306]
train() client id: f_00004-11-7 loss: 0.818374  [  256/  306]
train() client id: f_00004-11-8 loss: 0.835242  [  288/  306]
train() client id: f_00005-0-0 loss: 0.663841  [   32/  146]
train() client id: f_00005-0-1 loss: 0.673283  [   64/  146]
train() client id: f_00005-0-2 loss: 0.575329  [   96/  146]
train() client id: f_00005-0-3 loss: 0.409423  [  128/  146]
train() client id: f_00005-1-0 loss: 0.540879  [   32/  146]
train() client id: f_00005-1-1 loss: 0.533924  [   64/  146]
train() client id: f_00005-1-2 loss: 0.606442  [   96/  146]
train() client id: f_00005-1-3 loss: 0.613942  [  128/  146]
train() client id: f_00005-2-0 loss: 0.689399  [   32/  146]
train() client id: f_00005-2-1 loss: 0.514974  [   64/  146]
train() client id: f_00005-2-2 loss: 0.743101  [   96/  146]
train() client id: f_00005-2-3 loss: 0.617108  [  128/  146]
train() client id: f_00005-3-0 loss: 0.465521  [   32/  146]
train() client id: f_00005-3-1 loss: 0.690822  [   64/  146]
train() client id: f_00005-3-2 loss: 0.666398  [   96/  146]
train() client id: f_00005-3-3 loss: 0.535930  [  128/  146]
train() client id: f_00005-4-0 loss: 0.779570  [   32/  146]
train() client id: f_00005-4-1 loss: 0.556933  [   64/  146]
train() client id: f_00005-4-2 loss: 0.475296  [   96/  146]
train() client id: f_00005-4-3 loss: 0.712195  [  128/  146]
train() client id: f_00005-5-0 loss: 0.626060  [   32/  146]
train() client id: f_00005-5-1 loss: 0.543490  [   64/  146]
train() client id: f_00005-5-2 loss: 0.598380  [   96/  146]
train() client id: f_00005-5-3 loss: 0.572031  [  128/  146]
train() client id: f_00005-6-0 loss: 0.789989  [   32/  146]
train() client id: f_00005-6-1 loss: 0.554726  [   64/  146]
train() client id: f_00005-6-2 loss: 0.580104  [   96/  146]
train() client id: f_00005-6-3 loss: 0.489971  [  128/  146]
train() client id: f_00005-7-0 loss: 0.870714  [   32/  146]
train() client id: f_00005-7-1 loss: 0.709199  [   64/  146]
train() client id: f_00005-7-2 loss: 0.465220  [   96/  146]
train() client id: f_00005-7-3 loss: 0.492181  [  128/  146]
train() client id: f_00005-8-0 loss: 0.627230  [   32/  146]
train() client id: f_00005-8-1 loss: 0.495525  [   64/  146]
train() client id: f_00005-8-2 loss: 0.600025  [   96/  146]
train() client id: f_00005-8-3 loss: 0.771168  [  128/  146]
train() client id: f_00005-9-0 loss: 0.627452  [   32/  146]
train() client id: f_00005-9-1 loss: 0.785590  [   64/  146]
train() client id: f_00005-9-2 loss: 0.364005  [   96/  146]
train() client id: f_00005-9-3 loss: 0.590206  [  128/  146]
train() client id: f_00005-10-0 loss: 0.843055  [   32/  146]
train() client id: f_00005-10-1 loss: 0.514762  [   64/  146]
train() client id: f_00005-10-2 loss: 0.603289  [   96/  146]
train() client id: f_00005-10-3 loss: 0.397758  [  128/  146]
train() client id: f_00005-11-0 loss: 0.509940  [   32/  146]
train() client id: f_00005-11-1 loss: 0.743475  [   64/  146]
train() client id: f_00005-11-2 loss: 0.440403  [   96/  146]
train() client id: f_00005-11-3 loss: 0.663098  [  128/  146]
train() client id: f_00006-0-0 loss: 0.525900  [   32/   54]
train() client id: f_00006-1-0 loss: 0.566759  [   32/   54]
train() client id: f_00006-2-0 loss: 0.530388  [   32/   54]
train() client id: f_00006-3-0 loss: 0.585561  [   32/   54]
train() client id: f_00006-4-0 loss: 0.642653  [   32/   54]
train() client id: f_00006-5-0 loss: 0.563167  [   32/   54]
train() client id: f_00006-6-0 loss: 0.576759  [   32/   54]
train() client id: f_00006-7-0 loss: 0.566789  [   32/   54]
train() client id: f_00006-8-0 loss: 0.520929  [   32/   54]
train() client id: f_00006-9-0 loss: 0.616462  [   32/   54]
train() client id: f_00006-10-0 loss: 0.635602  [   32/   54]
train() client id: f_00006-11-0 loss: 0.563138  [   32/   54]
train() client id: f_00007-0-0 loss: 0.475177  [   32/  179]
train() client id: f_00007-0-1 loss: 0.472044  [   64/  179]
train() client id: f_00007-0-2 loss: 0.672845  [   96/  179]
train() client id: f_00007-0-3 loss: 0.633522  [  128/  179]
train() client id: f_00007-0-4 loss: 0.535452  [  160/  179]
train() client id: f_00007-1-0 loss: 0.501941  [   32/  179]
train() client id: f_00007-1-1 loss: 0.670596  [   64/  179]
train() client id: f_00007-1-2 loss: 0.475250  [   96/  179]
train() client id: f_00007-1-3 loss: 0.690509  [  128/  179]
train() client id: f_00007-1-4 loss: 0.682427  [  160/  179]
train() client id: f_00007-2-0 loss: 0.610626  [   32/  179]
train() client id: f_00007-2-1 loss: 0.674733  [   64/  179]
train() client id: f_00007-2-2 loss: 0.558694  [   96/  179]
train() client id: f_00007-2-3 loss: 0.578898  [  128/  179]
train() client id: f_00007-2-4 loss: 0.533050  [  160/  179]
train() client id: f_00007-3-0 loss: 0.816466  [   32/  179]
train() client id: f_00007-3-1 loss: 0.593066  [   64/  179]
train() client id: f_00007-3-2 loss: 0.456624  [   96/  179]
train() client id: f_00007-3-3 loss: 0.507487  [  128/  179]
train() client id: f_00007-3-4 loss: 0.485828  [  160/  179]
train() client id: f_00007-4-0 loss: 0.546837  [   32/  179]
train() client id: f_00007-4-1 loss: 0.526116  [   64/  179]
train() client id: f_00007-4-2 loss: 0.520298  [   96/  179]
train() client id: f_00007-4-3 loss: 0.655416  [  128/  179]
train() client id: f_00007-4-4 loss: 0.551380  [  160/  179]
train() client id: f_00007-5-0 loss: 0.672220  [   32/  179]
train() client id: f_00007-5-1 loss: 0.512956  [   64/  179]
train() client id: f_00007-5-2 loss: 0.674571  [   96/  179]
train() client id: f_00007-5-3 loss: 0.567068  [  128/  179]
train() client id: f_00007-5-4 loss: 0.411764  [  160/  179]
train() client id: f_00007-6-0 loss: 0.557309  [   32/  179]
train() client id: f_00007-6-1 loss: 0.591229  [   64/  179]
train() client id: f_00007-6-2 loss: 0.501461  [   96/  179]
train() client id: f_00007-6-3 loss: 0.513417  [  128/  179]
train() client id: f_00007-6-4 loss: 0.488528  [  160/  179]
train() client id: f_00007-7-0 loss: 0.524313  [   32/  179]
train() client id: f_00007-7-1 loss: 0.641731  [   64/  179]
train() client id: f_00007-7-2 loss: 0.397819  [   96/  179]
train() client id: f_00007-7-3 loss: 0.611914  [  128/  179]
train() client id: f_00007-7-4 loss: 0.613583  [  160/  179]
train() client id: f_00007-8-0 loss: 0.566594  [   32/  179]
train() client id: f_00007-8-1 loss: 0.562351  [   64/  179]
train() client id: f_00007-8-2 loss: 0.548556  [   96/  179]
train() client id: f_00007-8-3 loss: 0.442161  [  128/  179]
train() client id: f_00007-8-4 loss: 0.638518  [  160/  179]
train() client id: f_00007-9-0 loss: 0.536040  [   32/  179]
train() client id: f_00007-9-1 loss: 0.561161  [   64/  179]
train() client id: f_00007-9-2 loss: 0.800889  [   96/  179]
train() client id: f_00007-9-3 loss: 0.380203  [  128/  179]
train() client id: f_00007-9-4 loss: 0.506718  [  160/  179]
train() client id: f_00007-10-0 loss: 0.566256  [   32/  179]
train() client id: f_00007-10-1 loss: 0.485182  [   64/  179]
train() client id: f_00007-10-2 loss: 0.399081  [   96/  179]
train() client id: f_00007-10-3 loss: 0.578727  [  128/  179]
train() client id: f_00007-10-4 loss: 0.435629  [  160/  179]
train() client id: f_00007-11-0 loss: 0.397790  [   32/  179]
train() client id: f_00007-11-1 loss: 0.499760  [   64/  179]
train() client id: f_00007-11-2 loss: 0.739987  [   96/  179]
train() client id: f_00007-11-3 loss: 0.420513  [  128/  179]
train() client id: f_00007-11-4 loss: 0.473916  [  160/  179]
train() client id: f_00008-0-0 loss: 0.574967  [   32/  130]
train() client id: f_00008-0-1 loss: 0.680197  [   64/  130]
train() client id: f_00008-0-2 loss: 0.586559  [   96/  130]
train() client id: f_00008-0-3 loss: 0.587003  [  128/  130]
train() client id: f_00008-1-0 loss: 0.564569  [   32/  130]
train() client id: f_00008-1-1 loss: 0.695468  [   64/  130]
train() client id: f_00008-1-2 loss: 0.595185  [   96/  130]
train() client id: f_00008-1-3 loss: 0.535879  [  128/  130]
train() client id: f_00008-2-0 loss: 0.578941  [   32/  130]
train() client id: f_00008-2-1 loss: 0.679491  [   64/  130]
train() client id: f_00008-2-2 loss: 0.565964  [   96/  130]
train() client id: f_00008-2-3 loss: 0.602361  [  128/  130]
train() client id: f_00008-3-0 loss: 0.628507  [   32/  130]
train() client id: f_00008-3-1 loss: 0.678752  [   64/  130]
train() client id: f_00008-3-2 loss: 0.617089  [   96/  130]
train() client id: f_00008-3-3 loss: 0.503463  [  128/  130]
train() client id: f_00008-4-0 loss: 0.561623  [   32/  130]
train() client id: f_00008-4-1 loss: 0.656426  [   64/  130]
train() client id: f_00008-4-2 loss: 0.581186  [   96/  130]
train() client id: f_00008-4-3 loss: 0.630086  [  128/  130]
train() client id: f_00008-5-0 loss: 0.679666  [   32/  130]
train() client id: f_00008-5-1 loss: 0.476800  [   64/  130]
train() client id: f_00008-5-2 loss: 0.583944  [   96/  130]
train() client id: f_00008-5-3 loss: 0.650334  [  128/  130]
train() client id: f_00008-6-0 loss: 0.631153  [   32/  130]
train() client id: f_00008-6-1 loss: 0.633720  [   64/  130]
train() client id: f_00008-6-2 loss: 0.638446  [   96/  130]
train() client id: f_00008-6-3 loss: 0.508228  [  128/  130]
train() client id: f_00008-7-0 loss: 0.610225  [   32/  130]
train() client id: f_00008-7-1 loss: 0.645603  [   64/  130]
train() client id: f_00008-7-2 loss: 0.559368  [   96/  130]
train() client id: f_00008-7-3 loss: 0.571162  [  128/  130]
train() client id: f_00008-8-0 loss: 0.630418  [   32/  130]
train() client id: f_00008-8-1 loss: 0.626161  [   64/  130]
train() client id: f_00008-8-2 loss: 0.560050  [   96/  130]
train() client id: f_00008-8-3 loss: 0.586296  [  128/  130]
train() client id: f_00008-9-0 loss: 0.558269  [   32/  130]
train() client id: f_00008-9-1 loss: 0.598690  [   64/  130]
train() client id: f_00008-9-2 loss: 0.506567  [   96/  130]
train() client id: f_00008-9-3 loss: 0.762439  [  128/  130]
train() client id: f_00008-10-0 loss: 0.420343  [   32/  130]
train() client id: f_00008-10-1 loss: 0.647836  [   64/  130]
train() client id: f_00008-10-2 loss: 0.726080  [   96/  130]
train() client id: f_00008-10-3 loss: 0.615589  [  128/  130]
train() client id: f_00008-11-0 loss: 0.585421  [   32/  130]
train() client id: f_00008-11-1 loss: 0.648980  [   64/  130]
train() client id: f_00008-11-2 loss: 0.607887  [   96/  130]
train() client id: f_00008-11-3 loss: 0.581393  [  128/  130]
train() client id: f_00009-0-0 loss: 1.100061  [   32/  118]
train() client id: f_00009-0-1 loss: 1.146615  [   64/  118]
train() client id: f_00009-0-2 loss: 1.109035  [   96/  118]
train() client id: f_00009-1-0 loss: 1.100469  [   32/  118]
train() client id: f_00009-1-1 loss: 1.031118  [   64/  118]
train() client id: f_00009-1-2 loss: 1.011792  [   96/  118]
train() client id: f_00009-2-0 loss: 1.040816  [   32/  118]
train() client id: f_00009-2-1 loss: 1.056996  [   64/  118]
train() client id: f_00009-2-2 loss: 1.005487  [   96/  118]
train() client id: f_00009-3-0 loss: 0.884516  [   32/  118]
train() client id: f_00009-3-1 loss: 1.112263  [   64/  118]
train() client id: f_00009-3-2 loss: 0.918459  [   96/  118]
train() client id: f_00009-4-0 loss: 1.013989  [   32/  118]
train() client id: f_00009-4-1 loss: 0.973445  [   64/  118]
train() client id: f_00009-4-2 loss: 0.854680  [   96/  118]
train() client id: f_00009-5-0 loss: 0.927290  [   32/  118]
train() client id: f_00009-5-1 loss: 0.954499  [   64/  118]
train() client id: f_00009-5-2 loss: 0.882856  [   96/  118]
train() client id: f_00009-6-0 loss: 0.947420  [   32/  118]
train() client id: f_00009-6-1 loss: 0.844029  [   64/  118]
train() client id: f_00009-6-2 loss: 0.835050  [   96/  118]
train() client id: f_00009-7-0 loss: 0.847201  [   32/  118]
train() client id: f_00009-7-1 loss: 0.922440  [   64/  118]
train() client id: f_00009-7-2 loss: 0.827242  [   96/  118]
train() client id: f_00009-8-0 loss: 0.732842  [   32/  118]
train() client id: f_00009-8-1 loss: 0.945413  [   64/  118]
train() client id: f_00009-8-2 loss: 0.850129  [   96/  118]
train() client id: f_00009-9-0 loss: 0.840205  [   32/  118]
train() client id: f_00009-9-1 loss: 0.928941  [   64/  118]
train() client id: f_00009-9-2 loss: 0.761158  [   96/  118]
train() client id: f_00009-10-0 loss: 0.803677  [   32/  118]
train() client id: f_00009-10-1 loss: 0.915256  [   64/  118]
train() client id: f_00009-10-2 loss: 0.884624  [   96/  118]
train() client id: f_00009-11-0 loss: 0.794703  [   32/  118]
train() client id: f_00009-11-1 loss: 0.733856  [   64/  118]
train() client id: f_00009-11-2 loss: 0.832863  [   96/  118]
At round 24 accuracy: 0.6419098143236074
At round 24 training accuracy: 0.5821596244131455
At round 24 training loss: 0.8290019584920865
update_location
xs = [  -3.9056584     4.20031788  140.00902392   18.81129433    0.97929623
    3.95640986 -102.44319194  -81.32485185  124.66397685  -67.06087855]
ys = [ 132.5879595   115.55583871    1.32061395 -102.45517586   94.35018685
   77.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [166.11689008 152.87509454 172.05891665 144.39850365 137.48787866
 126.77024586 143.18344219 128.89688819 160.77865871 120.47063247]
dists_bs = [176.27267203 188.80244235 359.71546295 338.4437803  193.57971242
 203.64303555 191.87268031 197.76978496 338.50113277 202.21609029]
uav_gains = [2.80067787e-11 3.45600080e-11 2.55955797e-11 3.98858645e-11
 4.51021890e-11 5.52599497e-11 4.07401996e-11 5.30073043e-11
 3.04302492e-11 6.27732201e-11]
bs_gains = [5.67513399e-11 4.68244606e-11 7.70208994e-12 9.13548862e-12
 4.36602816e-11 3.78843039e-11 4.47566205e-11 4.11193508e-11
 9.13115535e-12 3.86375962e-11]
Round 25
-------------------------------
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.69254516 15.98293133  7.57645092  2.71961017 18.43551744  8.87729699
  3.37637101 10.83932141  7.98736814  7.20317358]
obj_prev = 90.69058614039777
eta_min = 8.689629583387306e-13	eta_max = 0.9239062523538892
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 21.07280247978385	eta = 0.909090909090909
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 37.266591851058905	eta = 0.5140554102721235
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 29.4469781495679	eta = 0.65056227726107
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 28.03930224522773	eta = 0.683222891778642
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967919357716607	eta = 0.6849666905290988
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967721963314986	eta = 0.684971524980406
eta = 0.684971524980406
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.03122285 0.0656671  0.03072725 0.01065542 0.07582691 0.03617885
 0.01338122 0.04435627 0.03221405 0.02924044]
ene_total = [2.45319609 4.54407619 2.43314225 1.13378689 5.18350126 2.72855748
 1.30141153 3.20547972 2.69013729 2.29443326]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 0 obj = 4.983772760330714
eta = 0.684971524980406
freqs = [41087307.98029104 84478875.62799275 40620710.39935426 13797163.01214605
 97820151.49584162 46948740.71955757 17311339.16633044 56794967.35235866
 45781631.37350148 37912918.15352243]
eta_min = 0.6849715249804152	eta_max = 0.68497152498039
af = 0.02120720489719426	bf = 1.5620059925242489	zeta = 0.023327925386913688	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [3.31926004e-06 2.95119211e-05 3.19280262e-06 1.27733086e-07
 4.56912927e-05 5.02176180e-06 2.52528528e-07 9.01006858e-06
 4.25188234e-06 2.64674311e-06]
ene_total = [1.76365275 1.58149679 1.80108751 1.6294247  1.6082219  1.64868503
 1.62207175 1.53747541 2.37101979 1.64116416]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 1 obj = 4.983772760330463
eta = 0.68497152498039
freqs = [41087307.98029105 84478875.62799287 40620710.39935425 13797163.01214607
 97820151.49584176 46948740.71955761 17311339.16633046 56794967.35235876
 45781631.37350126 37912918.15352247]
Done!
At round 25 energy consumption: 4.983772760330714
At round 25 eta: 0.68497152498039
At round 25 local rounds: 12.390018469521971
At round 25 global rounds: 62.276772538737
At round 25 a_n: 19.276410835052125
gradient difference: 0.41999855637550354
train() client id: f_00000-0-0 loss: 1.123954  [   32/  126]
train() client id: f_00000-0-1 loss: 1.315839  [   64/  126]
train() client id: f_00000-0-2 loss: 1.099165  [   96/  126]
train() client id: f_00000-1-0 loss: 1.158606  [   32/  126]
train() client id: f_00000-1-1 loss: 1.037242  [   64/  126]
train() client id: f_00000-1-2 loss: 1.119979  [   96/  126]
train() client id: f_00000-2-0 loss: 1.044961  [   32/  126]
train() client id: f_00000-2-1 loss: 1.096777  [   64/  126]
train() client id: f_00000-2-2 loss: 0.885652  [   96/  126]
train() client id: f_00000-3-0 loss: 1.033332  [   32/  126]
train() client id: f_00000-3-1 loss: 0.996680  [   64/  126]
train() client id: f_00000-3-2 loss: 0.950412  [   96/  126]
train() client id: f_00000-4-0 loss: 0.942736  [   32/  126]
train() client id: f_00000-4-1 loss: 0.811704  [   64/  126]
train() client id: f_00000-4-2 loss: 0.990535  [   96/  126]
train() client id: f_00000-5-0 loss: 0.794601  [   32/  126]
train() client id: f_00000-5-1 loss: 0.917062  [   64/  126]
train() client id: f_00000-5-2 loss: 0.880942  [   96/  126]
train() client id: f_00000-6-0 loss: 0.886768  [   32/  126]
train() client id: f_00000-6-1 loss: 0.785889  [   64/  126]
train() client id: f_00000-6-2 loss: 0.862017  [   96/  126]
train() client id: f_00000-7-0 loss: 0.795766  [   32/  126]
train() client id: f_00000-7-1 loss: 0.766914  [   64/  126]
train() client id: f_00000-7-2 loss: 0.927321  [   96/  126]
train() client id: f_00000-8-0 loss: 0.861759  [   32/  126]
train() client id: f_00000-8-1 loss: 0.777826  [   64/  126]
train() client id: f_00000-8-2 loss: 0.746858  [   96/  126]
train() client id: f_00000-9-0 loss: 0.718730  [   32/  126]
train() client id: f_00000-9-1 loss: 0.840704  [   64/  126]
train() client id: f_00000-9-2 loss: 0.839365  [   96/  126]
train() client id: f_00000-10-0 loss: 0.784052  [   32/  126]
train() client id: f_00000-10-1 loss: 0.755072  [   64/  126]
train() client id: f_00000-10-2 loss: 0.852681  [   96/  126]
train() client id: f_00000-11-0 loss: 0.764419  [   32/  126]
train() client id: f_00000-11-1 loss: 0.858554  [   64/  126]
train() client id: f_00000-11-2 loss: 0.717028  [   96/  126]
train() client id: f_00001-0-0 loss: 0.390121  [   32/  265]
train() client id: f_00001-0-1 loss: 0.514191  [   64/  265]
train() client id: f_00001-0-2 loss: 0.313711  [   96/  265]
train() client id: f_00001-0-3 loss: 0.370086  [  128/  265]
train() client id: f_00001-0-4 loss: 0.474093  [  160/  265]
train() client id: f_00001-0-5 loss: 0.535420  [  192/  265]
train() client id: f_00001-0-6 loss: 0.395304  [  224/  265]
train() client id: f_00001-0-7 loss: 0.373559  [  256/  265]
train() client id: f_00001-1-0 loss: 0.528762  [   32/  265]
train() client id: f_00001-1-1 loss: 0.367099  [   64/  265]
train() client id: f_00001-1-2 loss: 0.355933  [   96/  265]
train() client id: f_00001-1-3 loss: 0.366138  [  128/  265]
train() client id: f_00001-1-4 loss: 0.442810  [  160/  265]
train() client id: f_00001-1-5 loss: 0.418395  [  192/  265]
train() client id: f_00001-1-6 loss: 0.427688  [  224/  265]
train() client id: f_00001-1-7 loss: 0.405467  [  256/  265]
train() client id: f_00001-2-0 loss: 0.454323  [   32/  265]
train() client id: f_00001-2-1 loss: 0.354679  [   64/  265]
train() client id: f_00001-2-2 loss: 0.439296  [   96/  265]
train() client id: f_00001-2-3 loss: 0.343232  [  128/  265]
train() client id: f_00001-2-4 loss: 0.381096  [  160/  265]
train() client id: f_00001-2-5 loss: 0.331160  [  192/  265]
train() client id: f_00001-2-6 loss: 0.412749  [  224/  265]
train() client id: f_00001-2-7 loss: 0.544762  [  256/  265]
train() client id: f_00001-3-0 loss: 0.425723  [   32/  265]
train() client id: f_00001-3-1 loss: 0.442669  [   64/  265]
train() client id: f_00001-3-2 loss: 0.375469  [   96/  265]
train() client id: f_00001-3-3 loss: 0.370228  [  128/  265]
train() client id: f_00001-3-4 loss: 0.442525  [  160/  265]
train() client id: f_00001-3-5 loss: 0.451565  [  192/  265]
train() client id: f_00001-3-6 loss: 0.350155  [  224/  265]
train() client id: f_00001-3-7 loss: 0.353294  [  256/  265]
train() client id: f_00001-4-0 loss: 0.336203  [   32/  265]
train() client id: f_00001-4-1 loss: 0.374604  [   64/  265]
train() client id: f_00001-4-2 loss: 0.427656  [   96/  265]
train() client id: f_00001-4-3 loss: 0.469576  [  128/  265]
train() client id: f_00001-4-4 loss: 0.433171  [  160/  265]
train() client id: f_00001-4-5 loss: 0.309539  [  192/  265]
train() client id: f_00001-4-6 loss: 0.355370  [  224/  265]
train() client id: f_00001-4-7 loss: 0.377358  [  256/  265]
train() client id: f_00001-5-0 loss: 0.454428  [   32/  265]
train() client id: f_00001-5-1 loss: 0.354219  [   64/  265]
train() client id: f_00001-5-2 loss: 0.412160  [   96/  265]
train() client id: f_00001-5-3 loss: 0.349675  [  128/  265]
train() client id: f_00001-5-4 loss: 0.308062  [  160/  265]
train() client id: f_00001-5-5 loss: 0.405199  [  192/  265]
train() client id: f_00001-5-6 loss: 0.446764  [  224/  265]
train() client id: f_00001-5-7 loss: 0.369267  [  256/  265]
train() client id: f_00001-6-0 loss: 0.302213  [   32/  265]
train() client id: f_00001-6-1 loss: 0.411806  [   64/  265]
train() client id: f_00001-6-2 loss: 0.398875  [   96/  265]
train() client id: f_00001-6-3 loss: 0.396157  [  128/  265]
train() client id: f_00001-6-4 loss: 0.510043  [  160/  265]
train() client id: f_00001-6-5 loss: 0.343040  [  192/  265]
train() client id: f_00001-6-6 loss: 0.379935  [  224/  265]
train() client id: f_00001-6-7 loss: 0.423374  [  256/  265]
train() client id: f_00001-7-0 loss: 0.448949  [   32/  265]
train() client id: f_00001-7-1 loss: 0.354287  [   64/  265]
train() client id: f_00001-7-2 loss: 0.440206  [   96/  265]
train() client id: f_00001-7-3 loss: 0.374196  [  128/  265]
train() client id: f_00001-7-4 loss: 0.394371  [  160/  265]
train() client id: f_00001-7-5 loss: 0.350057  [  192/  265]
train() client id: f_00001-7-6 loss: 0.293534  [  224/  265]
train() client id: f_00001-7-7 loss: 0.486352  [  256/  265]
train() client id: f_00001-8-0 loss: 0.418307  [   32/  265]
train() client id: f_00001-8-1 loss: 0.498529  [   64/  265]
train() client id: f_00001-8-2 loss: 0.366270  [   96/  265]
train() client id: f_00001-8-3 loss: 0.376622  [  128/  265]
train() client id: f_00001-8-4 loss: 0.309033  [  160/  265]
train() client id: f_00001-8-5 loss: 0.521375  [  192/  265]
train() client id: f_00001-8-6 loss: 0.342265  [  224/  265]
train() client id: f_00001-8-7 loss: 0.307942  [  256/  265]
train() client id: f_00001-9-0 loss: 0.490222  [   32/  265]
train() client id: f_00001-9-1 loss: 0.393811  [   64/  265]
train() client id: f_00001-9-2 loss: 0.324352  [   96/  265]
train() client id: f_00001-9-3 loss: 0.349503  [  128/  265]
train() client id: f_00001-9-4 loss: 0.419315  [  160/  265]
train() client id: f_00001-9-5 loss: 0.384937  [  192/  265]
train() client id: f_00001-9-6 loss: 0.461621  [  224/  265]
train() client id: f_00001-9-7 loss: 0.316992  [  256/  265]
train() client id: f_00001-10-0 loss: 0.453305  [   32/  265]
train() client id: f_00001-10-1 loss: 0.359374  [   64/  265]
train() client id: f_00001-10-2 loss: 0.356833  [   96/  265]
train() client id: f_00001-10-3 loss: 0.290778  [  128/  265]
train() client id: f_00001-10-4 loss: 0.464022  [  160/  265]
train() client id: f_00001-10-5 loss: 0.338629  [  192/  265]
train() client id: f_00001-10-6 loss: 0.373042  [  224/  265]
train() client id: f_00001-10-7 loss: 0.399317  [  256/  265]
train() client id: f_00001-11-0 loss: 0.374961  [   32/  265]
train() client id: f_00001-11-1 loss: 0.426865  [   64/  265]
train() client id: f_00001-11-2 loss: 0.470787  [   96/  265]
train() client id: f_00001-11-3 loss: 0.356399  [  128/  265]
train() client id: f_00001-11-4 loss: 0.300277  [  160/  265]
train() client id: f_00001-11-5 loss: 0.392585  [  192/  265]
train() client id: f_00001-11-6 loss: 0.331644  [  224/  265]
train() client id: f_00001-11-7 loss: 0.417091  [  256/  265]
train() client id: f_00002-0-0 loss: 1.089854  [   32/  124]
train() client id: f_00002-0-1 loss: 1.347825  [   64/  124]
train() client id: f_00002-0-2 loss: 1.222474  [   96/  124]
train() client id: f_00002-1-0 loss: 1.281889  [   32/  124]
train() client id: f_00002-1-1 loss: 1.146553  [   64/  124]
train() client id: f_00002-1-2 loss: 1.183004  [   96/  124]
train() client id: f_00002-2-0 loss: 1.277500  [   32/  124]
train() client id: f_00002-2-1 loss: 1.205619  [   64/  124]
train() client id: f_00002-2-2 loss: 1.105340  [   96/  124]
train() client id: f_00002-3-0 loss: 1.074759  [   32/  124]
train() client id: f_00002-3-1 loss: 1.080031  [   64/  124]
train() client id: f_00002-3-2 loss: 1.179913  [   96/  124]
train() client id: f_00002-4-0 loss: 1.171471  [   32/  124]
train() client id: f_00002-4-1 loss: 1.216150  [   64/  124]
train() client id: f_00002-4-2 loss: 0.955135  [   96/  124]
train() client id: f_00002-5-0 loss: 0.989566  [   32/  124]
train() client id: f_00002-5-1 loss: 1.232479  [   64/  124]
train() client id: f_00002-5-2 loss: 1.105676  [   96/  124]
train() client id: f_00002-6-0 loss: 1.042509  [   32/  124]
train() client id: f_00002-6-1 loss: 1.099162  [   64/  124]
train() client id: f_00002-6-2 loss: 1.033241  [   96/  124]
train() client id: f_00002-7-0 loss: 1.031616  [   32/  124]
train() client id: f_00002-7-1 loss: 1.017558  [   64/  124]
train() client id: f_00002-7-2 loss: 1.044702  [   96/  124]
train() client id: f_00002-8-0 loss: 0.949368  [   32/  124]
train() client id: f_00002-8-1 loss: 1.220708  [   64/  124]
train() client id: f_00002-8-2 loss: 1.028325  [   96/  124]
train() client id: f_00002-9-0 loss: 1.069066  [   32/  124]
train() client id: f_00002-9-1 loss: 0.965292  [   64/  124]
train() client id: f_00002-9-2 loss: 0.948308  [   96/  124]
train() client id: f_00002-10-0 loss: 1.066384  [   32/  124]
train() client id: f_00002-10-1 loss: 0.970997  [   64/  124]
train() client id: f_00002-10-2 loss: 1.036272  [   96/  124]
train() client id: f_00002-11-0 loss: 1.067194  [   32/  124]
train() client id: f_00002-11-1 loss: 1.050584  [   64/  124]
train() client id: f_00002-11-2 loss: 0.934439  [   96/  124]
train() client id: f_00003-0-0 loss: 0.595806  [   32/   43]
train() client id: f_00003-1-0 loss: 0.639951  [   32/   43]
train() client id: f_00003-2-0 loss: 0.792461  [   32/   43]
train() client id: f_00003-3-0 loss: 0.586972  [   32/   43]
train() client id: f_00003-4-0 loss: 0.807571  [   32/   43]
train() client id: f_00003-5-0 loss: 0.569131  [   32/   43]
train() client id: f_00003-6-0 loss: 0.663011  [   32/   43]
train() client id: f_00003-7-0 loss: 0.612133  [   32/   43]
train() client id: f_00003-8-0 loss: 0.682261  [   32/   43]
train() client id: f_00003-9-0 loss: 0.746634  [   32/   43]
train() client id: f_00003-10-0 loss: 0.644355  [   32/   43]
train() client id: f_00003-11-0 loss: 0.627992  [   32/   43]
train() client id: f_00004-0-0 loss: 1.011429  [   32/  306]
train() client id: f_00004-0-1 loss: 1.166842  [   64/  306]
train() client id: f_00004-0-2 loss: 0.946713  [   96/  306]
train() client id: f_00004-0-3 loss: 1.051590  [  128/  306]
train() client id: f_00004-0-4 loss: 0.911135  [  160/  306]
train() client id: f_00004-0-5 loss: 0.926419  [  192/  306]
train() client id: f_00004-0-6 loss: 1.016267  [  224/  306]
train() client id: f_00004-0-7 loss: 0.995031  [  256/  306]
train() client id: f_00004-0-8 loss: 1.022183  [  288/  306]
train() client id: f_00004-1-0 loss: 0.986408  [   32/  306]
train() client id: f_00004-1-1 loss: 0.927335  [   64/  306]
train() client id: f_00004-1-2 loss: 0.971215  [   96/  306]
train() client id: f_00004-1-3 loss: 1.055809  [  128/  306]
train() client id: f_00004-1-4 loss: 1.022837  [  160/  306]
train() client id: f_00004-1-5 loss: 0.990040  [  192/  306]
train() client id: f_00004-1-6 loss: 0.985404  [  224/  306]
train() client id: f_00004-1-7 loss: 0.986759  [  256/  306]
train() client id: f_00004-1-8 loss: 1.066002  [  288/  306]
train() client id: f_00004-2-0 loss: 1.066683  [   32/  306]
train() client id: f_00004-2-1 loss: 1.010791  [   64/  306]
train() client id: f_00004-2-2 loss: 0.950804  [   96/  306]
train() client id: f_00004-2-3 loss: 0.895020  [  128/  306]
train() client id: f_00004-2-4 loss: 1.099180  [  160/  306]
train() client id: f_00004-2-5 loss: 1.024922  [  192/  306]
train() client id: f_00004-2-6 loss: 0.920488  [  224/  306]
train() client id: f_00004-2-7 loss: 1.007590  [  256/  306]
train() client id: f_00004-2-8 loss: 1.012878  [  288/  306]
train() client id: f_00004-3-0 loss: 0.965428  [   32/  306]
train() client id: f_00004-3-1 loss: 1.052720  [   64/  306]
train() client id: f_00004-3-2 loss: 0.941278  [   96/  306]
train() client id: f_00004-3-3 loss: 1.140409  [  128/  306]
train() client id: f_00004-3-4 loss: 1.146807  [  160/  306]
train() client id: f_00004-3-5 loss: 0.930483  [  192/  306]
train() client id: f_00004-3-6 loss: 0.976347  [  224/  306]
train() client id: f_00004-3-7 loss: 0.952868  [  256/  306]
train() client id: f_00004-3-8 loss: 0.874038  [  288/  306]
train() client id: f_00004-4-0 loss: 1.005810  [   32/  306]
train() client id: f_00004-4-1 loss: 0.893116  [   64/  306]
train() client id: f_00004-4-2 loss: 0.922729  [   96/  306]
train() client id: f_00004-4-3 loss: 1.019349  [  128/  306]
train() client id: f_00004-4-4 loss: 0.936301  [  160/  306]
train() client id: f_00004-4-5 loss: 0.939472  [  192/  306]
train() client id: f_00004-4-6 loss: 1.063703  [  224/  306]
train() client id: f_00004-4-7 loss: 1.043554  [  256/  306]
train() client id: f_00004-4-8 loss: 1.115696  [  288/  306]
train() client id: f_00004-5-0 loss: 0.958010  [   32/  306]
train() client id: f_00004-5-1 loss: 0.926290  [   64/  306]
train() client id: f_00004-5-2 loss: 1.126180  [   96/  306]
train() client id: f_00004-5-3 loss: 0.957352  [  128/  306]
train() client id: f_00004-5-4 loss: 0.937552  [  160/  306]
train() client id: f_00004-5-5 loss: 1.061579  [  192/  306]
train() client id: f_00004-5-6 loss: 0.889285  [  224/  306]
train() client id: f_00004-5-7 loss: 1.169041  [  256/  306]
train() client id: f_00004-5-8 loss: 0.923299  [  288/  306]
train() client id: f_00004-6-0 loss: 1.025404  [   32/  306]
train() client id: f_00004-6-1 loss: 0.979466  [   64/  306]
train() client id: f_00004-6-2 loss: 0.980851  [   96/  306]
train() client id: f_00004-6-3 loss: 1.062120  [  128/  306]
train() client id: f_00004-6-4 loss: 0.967215  [  160/  306]
train() client id: f_00004-6-5 loss: 0.911907  [  192/  306]
train() client id: f_00004-6-6 loss: 1.072621  [  224/  306]
train() client id: f_00004-6-7 loss: 0.929823  [  256/  306]
train() client id: f_00004-6-8 loss: 0.936288  [  288/  306]
train() client id: f_00004-7-0 loss: 1.049719  [   32/  306]
train() client id: f_00004-7-1 loss: 0.948000  [   64/  306]
train() client id: f_00004-7-2 loss: 1.013660  [   96/  306]
train() client id: f_00004-7-3 loss: 1.174650  [  128/  306]
train() client id: f_00004-7-4 loss: 0.889245  [  160/  306]
train() client id: f_00004-7-5 loss: 1.036724  [  192/  306]
train() client id: f_00004-7-6 loss: 0.874996  [  224/  306]
train() client id: f_00004-7-7 loss: 0.980201  [  256/  306]
train() client id: f_00004-7-8 loss: 0.911530  [  288/  306]
train() client id: f_00004-8-0 loss: 0.832117  [   32/  306]
train() client id: f_00004-8-1 loss: 1.072511  [   64/  306]
train() client id: f_00004-8-2 loss: 1.030681  [   96/  306]
train() client id: f_00004-8-3 loss: 0.961541  [  128/  306]
train() client id: f_00004-8-4 loss: 1.024331  [  160/  306]
train() client id: f_00004-8-5 loss: 0.917972  [  192/  306]
train() client id: f_00004-8-6 loss: 0.976931  [  224/  306]
train() client id: f_00004-8-7 loss: 1.093634  [  256/  306]
train() client id: f_00004-8-8 loss: 0.882623  [  288/  306]
train() client id: f_00004-9-0 loss: 0.894148  [   32/  306]
train() client id: f_00004-9-1 loss: 0.973662  [   64/  306]
train() client id: f_00004-9-2 loss: 0.949779  [   96/  306]
train() client id: f_00004-9-3 loss: 0.943640  [  128/  306]
train() client id: f_00004-9-4 loss: 1.146413  [  160/  306]
train() client id: f_00004-9-5 loss: 0.991944  [  192/  306]
train() client id: f_00004-9-6 loss: 0.972308  [  224/  306]
train() client id: f_00004-9-7 loss: 0.869285  [  256/  306]
train() client id: f_00004-9-8 loss: 0.989775  [  288/  306]
train() client id: f_00004-10-0 loss: 0.965456  [   32/  306]
train() client id: f_00004-10-1 loss: 0.978737  [   64/  306]
train() client id: f_00004-10-2 loss: 0.933754  [   96/  306]
train() client id: f_00004-10-3 loss: 0.886070  [  128/  306]
train() client id: f_00004-10-4 loss: 0.949806  [  160/  306]
train() client id: f_00004-10-5 loss: 1.131891  [  192/  306]
train() client id: f_00004-10-6 loss: 1.129136  [  224/  306]
train() client id: f_00004-10-7 loss: 0.878260  [  256/  306]
train() client id: f_00004-10-8 loss: 0.914706  [  288/  306]
train() client id: f_00004-11-0 loss: 1.003220  [   32/  306]
train() client id: f_00004-11-1 loss: 0.928273  [   64/  306]
train() client id: f_00004-11-2 loss: 0.997304  [   96/  306]
train() client id: f_00004-11-3 loss: 0.890583  [  128/  306]
train() client id: f_00004-11-4 loss: 0.910802  [  160/  306]
train() client id: f_00004-11-5 loss: 1.015218  [  192/  306]
train() client id: f_00004-11-6 loss: 1.079886  [  224/  306]
train() client id: f_00004-11-7 loss: 0.946340  [  256/  306]
train() client id: f_00004-11-8 loss: 0.998788  [  288/  306]
train() client id: f_00005-0-0 loss: 0.614209  [   32/  146]
train() client id: f_00005-0-1 loss: 0.566360  [   64/  146]
train() client id: f_00005-0-2 loss: 0.577904  [   96/  146]
train() client id: f_00005-0-3 loss: 0.601382  [  128/  146]
train() client id: f_00005-1-0 loss: 0.694843  [   32/  146]
train() client id: f_00005-1-1 loss: 0.625894  [   64/  146]
train() client id: f_00005-1-2 loss: 0.567143  [   96/  146]
train() client id: f_00005-1-3 loss: 0.508085  [  128/  146]
train() client id: f_00005-2-0 loss: 0.791729  [   32/  146]
train() client id: f_00005-2-1 loss: 0.598559  [   64/  146]
train() client id: f_00005-2-2 loss: 0.468049  [   96/  146]
train() client id: f_00005-2-3 loss: 0.518557  [  128/  146]
train() client id: f_00005-3-0 loss: 0.573219  [   32/  146]
train() client id: f_00005-3-1 loss: 0.500026  [   64/  146]
train() client id: f_00005-3-2 loss: 0.717471  [   96/  146]
train() client id: f_00005-3-3 loss: 0.651763  [  128/  146]
train() client id: f_00005-4-0 loss: 0.665665  [   32/  146]
train() client id: f_00005-4-1 loss: 0.709283  [   64/  146]
train() client id: f_00005-4-2 loss: 0.633437  [   96/  146]
train() client id: f_00005-4-3 loss: 0.474111  [  128/  146]
train() client id: f_00005-5-0 loss: 0.658366  [   32/  146]
train() client id: f_00005-5-1 loss: 0.680712  [   64/  146]
train() client id: f_00005-5-2 loss: 0.592530  [   96/  146]
train() client id: f_00005-5-3 loss: 0.540249  [  128/  146]
train() client id: f_00005-6-0 loss: 0.569647  [   32/  146]
train() client id: f_00005-6-1 loss: 0.379102  [   64/  146]
train() client id: f_00005-6-2 loss: 0.924616  [   96/  146]
train() client id: f_00005-6-3 loss: 0.747896  [  128/  146]
train() client id: f_00005-7-0 loss: 0.600545  [   32/  146]
train() client id: f_00005-7-1 loss: 0.530201  [   64/  146]
train() client id: f_00005-7-2 loss: 0.614645  [   96/  146]
train() client id: f_00005-7-3 loss: 0.695751  [  128/  146]
train() client id: f_00005-8-0 loss: 0.876293  [   32/  146]
train() client id: f_00005-8-1 loss: 0.753113  [   64/  146]
train() client id: f_00005-8-2 loss: 0.418953  [   96/  146]
train() client id: f_00005-8-3 loss: 0.466067  [  128/  146]
train() client id: f_00005-9-0 loss: 0.676695  [   32/  146]
train() client id: f_00005-9-1 loss: 0.418678  [   64/  146]
train() client id: f_00005-9-2 loss: 0.502739  [   96/  146]
train() client id: f_00005-9-3 loss: 0.772260  [  128/  146]
train() client id: f_00005-10-0 loss: 0.456432  [   32/  146]
train() client id: f_00005-10-1 loss: 0.561077  [   64/  146]
train() client id: f_00005-10-2 loss: 0.640547  [   96/  146]
train() client id: f_00005-10-3 loss: 0.706172  [  128/  146]
train() client id: f_00005-11-0 loss: 0.481222  [   32/  146]
train() client id: f_00005-11-1 loss: 0.691461  [   64/  146]
train() client id: f_00005-11-2 loss: 0.443972  [   96/  146]
train() client id: f_00005-11-3 loss: 0.668946  [  128/  146]
train() client id: f_00006-0-0 loss: 0.535053  [   32/   54]
train() client id: f_00006-1-0 loss: 0.542768  [   32/   54]
train() client id: f_00006-2-0 loss: 0.589939  [   32/   54]
train() client id: f_00006-3-0 loss: 0.496758  [   32/   54]
train() client id: f_00006-4-0 loss: 0.477712  [   32/   54]
train() client id: f_00006-5-0 loss: 0.545719  [   32/   54]
train() client id: f_00006-6-0 loss: 0.591904  [   32/   54]
train() client id: f_00006-7-0 loss: 0.588565  [   32/   54]
train() client id: f_00006-8-0 loss: 0.582987  [   32/   54]
train() client id: f_00006-9-0 loss: 0.537806  [   32/   54]
train() client id: f_00006-10-0 loss: 0.546444  [   32/   54]
train() client id: f_00006-11-0 loss: 0.544719  [   32/   54]
train() client id: f_00007-0-0 loss: 0.624313  [   32/  179]
train() client id: f_00007-0-1 loss: 0.698056  [   64/  179]
train() client id: f_00007-0-2 loss: 0.556676  [   96/  179]
train() client id: f_00007-0-3 loss: 0.622223  [  128/  179]
train() client id: f_00007-0-4 loss: 0.771218  [  160/  179]
train() client id: f_00007-1-0 loss: 0.602414  [   32/  179]
train() client id: f_00007-1-1 loss: 0.821420  [   64/  179]
train() client id: f_00007-1-2 loss: 0.670785  [   96/  179]
train() client id: f_00007-1-3 loss: 0.712287  [  128/  179]
train() client id: f_00007-1-4 loss: 0.552765  [  160/  179]
train() client id: f_00007-2-0 loss: 0.600859  [   32/  179]
train() client id: f_00007-2-1 loss: 0.671836  [   64/  179]
train() client id: f_00007-2-2 loss: 0.614239  [   96/  179]
train() client id: f_00007-2-3 loss: 0.654212  [  128/  179]
train() client id: f_00007-2-4 loss: 0.596768  [  160/  179]
train() client id: f_00007-3-0 loss: 0.595581  [   32/  179]
train() client id: f_00007-3-1 loss: 0.696261  [   64/  179]
train() client id: f_00007-3-2 loss: 0.644197  [   96/  179]
train() client id: f_00007-3-3 loss: 0.490036  [  128/  179]
train() client id: f_00007-3-4 loss: 0.679633  [  160/  179]
train() client id: f_00007-4-0 loss: 0.507053  [   32/  179]
train() client id: f_00007-4-1 loss: 0.540566  [   64/  179]
train() client id: f_00007-4-2 loss: 0.638156  [   96/  179]
train() client id: f_00007-4-3 loss: 0.766449  [  128/  179]
train() client id: f_00007-4-4 loss: 0.778912  [  160/  179]
train() client id: f_00007-5-0 loss: 0.828161  [   32/  179]
train() client id: f_00007-5-1 loss: 0.521553  [   64/  179]
train() client id: f_00007-5-2 loss: 0.586423  [   96/  179]
train() client id: f_00007-5-3 loss: 0.669049  [  128/  179]
train() client id: f_00007-5-4 loss: 0.565486  [  160/  179]
train() client id: f_00007-6-0 loss: 0.654564  [   32/  179]
train() client id: f_00007-6-1 loss: 0.649766  [   64/  179]
train() client id: f_00007-6-2 loss: 0.602399  [   96/  179]
train() client id: f_00007-6-3 loss: 0.559072  [  128/  179]
train() client id: f_00007-6-4 loss: 0.766570  [  160/  179]
train() client id: f_00007-7-0 loss: 0.673838  [   32/  179]
train() client id: f_00007-7-1 loss: 0.494508  [   64/  179]
train() client id: f_00007-7-2 loss: 0.582658  [   96/  179]
train() client id: f_00007-7-3 loss: 0.682669  [  128/  179]
train() client id: f_00007-7-4 loss: 0.461646  [  160/  179]
train() client id: f_00007-8-0 loss: 0.501837  [   32/  179]
train() client id: f_00007-8-1 loss: 0.485926  [   64/  179]
train() client id: f_00007-8-2 loss: 0.707116  [   96/  179]
train() client id: f_00007-8-3 loss: 0.652483  [  128/  179]
train() client id: f_00007-8-4 loss: 0.702804  [  160/  179]
train() client id: f_00007-9-0 loss: 0.570717  [   32/  179]
train() client id: f_00007-9-1 loss: 0.548792  [   64/  179]
train() client id: f_00007-9-2 loss: 0.607198  [   96/  179]
train() client id: f_00007-9-3 loss: 0.562910  [  128/  179]
train() client id: f_00007-9-4 loss: 0.616409  [  160/  179]
train() client id: f_00007-10-0 loss: 0.574664  [   32/  179]
train() client id: f_00007-10-1 loss: 0.476756  [   64/  179]
train() client id: f_00007-10-2 loss: 0.518882  [   96/  179]
train() client id: f_00007-10-3 loss: 0.811367  [  128/  179]
train() client id: f_00007-10-4 loss: 0.562273  [  160/  179]
train() client id: f_00007-11-0 loss: 0.780416  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582297  [   64/  179]
train() client id: f_00007-11-2 loss: 0.556179  [   96/  179]
train() client id: f_00007-11-3 loss: 0.476413  [  128/  179]
train() client id: f_00007-11-4 loss: 0.570448  [  160/  179]
train() client id: f_00008-0-0 loss: 0.813403  [   32/  130]
train() client id: f_00008-0-1 loss: 0.651671  [   64/  130]
train() client id: f_00008-0-2 loss: 0.801036  [   96/  130]
train() client id: f_00008-0-3 loss: 0.722492  [  128/  130]
train() client id: f_00008-1-0 loss: 0.742204  [   32/  130]
train() client id: f_00008-1-1 loss: 0.672483  [   64/  130]
train() client id: f_00008-1-2 loss: 0.657541  [   96/  130]
train() client id: f_00008-1-3 loss: 0.913114  [  128/  130]
train() client id: f_00008-2-0 loss: 0.891820  [   32/  130]
train() client id: f_00008-2-1 loss: 0.764513  [   64/  130]
train() client id: f_00008-2-2 loss: 0.710317  [   96/  130]
train() client id: f_00008-2-3 loss: 0.624572  [  128/  130]
train() client id: f_00008-3-0 loss: 0.663878  [   32/  130]
train() client id: f_00008-3-1 loss: 0.848182  [   64/  130]
train() client id: f_00008-3-2 loss: 0.794611  [   96/  130]
train() client id: f_00008-3-3 loss: 0.678682  [  128/  130]
train() client id: f_00008-4-0 loss: 0.708386  [   32/  130]
train() client id: f_00008-4-1 loss: 0.802257  [   64/  130]
train() client id: f_00008-4-2 loss: 0.680258  [   96/  130]
train() client id: f_00008-4-3 loss: 0.767156  [  128/  130]
train() client id: f_00008-5-0 loss: 0.709544  [   32/  130]
train() client id: f_00008-5-1 loss: 0.749069  [   64/  130]
train() client id: f_00008-5-2 loss: 0.753948  [   96/  130]
train() client id: f_00008-5-3 loss: 0.770889  [  128/  130]
train() client id: f_00008-6-0 loss: 0.802350  [   32/  130]
train() client id: f_00008-6-1 loss: 0.680452  [   64/  130]
train() client id: f_00008-6-2 loss: 0.805486  [   96/  130]
train() client id: f_00008-6-3 loss: 0.696683  [  128/  130]
train() client id: f_00008-7-0 loss: 0.688219  [   32/  130]
train() client id: f_00008-7-1 loss: 0.779822  [   64/  130]
train() client id: f_00008-7-2 loss: 0.723638  [   96/  130]
train() client id: f_00008-7-3 loss: 0.760202  [  128/  130]
train() client id: f_00008-8-0 loss: 0.718989  [   32/  130]
train() client id: f_00008-8-1 loss: 0.808288  [   64/  130]
train() client id: f_00008-8-2 loss: 0.691677  [   96/  130]
train() client id: f_00008-8-3 loss: 0.692512  [  128/  130]
train() client id: f_00008-9-0 loss: 0.805031  [   32/  130]
train() client id: f_00008-9-1 loss: 0.744570  [   64/  130]
train() client id: f_00008-9-2 loss: 0.672708  [   96/  130]
train() client id: f_00008-9-3 loss: 0.747304  [  128/  130]
train() client id: f_00008-10-0 loss: 0.787702  [   32/  130]
train() client id: f_00008-10-1 loss: 0.752394  [   64/  130]
train() client id: f_00008-10-2 loss: 0.729666  [   96/  130]
train() client id: f_00008-10-3 loss: 0.722018  [  128/  130]
train() client id: f_00008-11-0 loss: 0.766574  [   32/  130]
train() client id: f_00008-11-1 loss: 0.664066  [   64/  130]
train() client id: f_00008-11-2 loss: 0.856189  [   96/  130]
train() client id: f_00008-11-3 loss: 0.664631  [  128/  130]
train() client id: f_00009-0-0 loss: 1.090917  [   32/  118]
train() client id: f_00009-0-1 loss: 1.284879  [   64/  118]
train() client id: f_00009-0-2 loss: 1.098135  [   96/  118]
train() client id: f_00009-1-0 loss: 1.193302  [   32/  118]
train() client id: f_00009-1-1 loss: 1.140336  [   64/  118]
train() client id: f_00009-1-2 loss: 1.033807  [   96/  118]
train() client id: f_00009-2-0 loss: 1.037050  [   32/  118]
train() client id: f_00009-2-1 loss: 1.157168  [   64/  118]
train() client id: f_00009-2-2 loss: 0.993105  [   96/  118]
train() client id: f_00009-3-0 loss: 1.041849  [   32/  118]
train() client id: f_00009-3-1 loss: 1.084775  [   64/  118]
train() client id: f_00009-3-2 loss: 0.973874  [   96/  118]
train() client id: f_00009-4-0 loss: 1.031401  [   32/  118]
train() client id: f_00009-4-1 loss: 0.910732  [   64/  118]
train() client id: f_00009-4-2 loss: 0.979413  [   96/  118]
train() client id: f_00009-5-0 loss: 1.041243  [   32/  118]
train() client id: f_00009-5-1 loss: 0.847909  [   64/  118]
train() client id: f_00009-5-2 loss: 1.002006  [   96/  118]
train() client id: f_00009-6-0 loss: 0.996140  [   32/  118]
train() client id: f_00009-6-1 loss: 0.954612  [   64/  118]
train() client id: f_00009-6-2 loss: 0.871700  [   96/  118]
train() client id: f_00009-7-0 loss: 0.861235  [   32/  118]
train() client id: f_00009-7-1 loss: 0.850694  [   64/  118]
train() client id: f_00009-7-2 loss: 0.933421  [   96/  118]
train() client id: f_00009-8-0 loss: 0.916491  [   32/  118]
train() client id: f_00009-8-1 loss: 0.841153  [   64/  118]
train() client id: f_00009-8-2 loss: 0.928693  [   96/  118]
train() client id: f_00009-9-0 loss: 0.942771  [   32/  118]
train() client id: f_00009-9-1 loss: 0.949682  [   64/  118]
train() client id: f_00009-9-2 loss: 0.830433  [   96/  118]
train() client id: f_00009-10-0 loss: 0.865615  [   32/  118]
train() client id: f_00009-10-1 loss: 0.781052  [   64/  118]
train() client id: f_00009-10-2 loss: 1.057905  [   96/  118]
train() client id: f_00009-11-0 loss: 0.865740  [   32/  118]
train() client id: f_00009-11-1 loss: 0.877716  [   64/  118]
train() client id: f_00009-11-2 loss: 0.838641  [   96/  118]
At round 25 accuracy: 0.6472148541114059
At round 25 training accuracy: 0.5821596244131455
At round 25 training loss: 0.8399768360788634
update_location
xs = [  -3.9056584     4.20031788  145.00902392   18.81129433    0.97929623
    3.95640986 -107.44319194  -86.32485185  129.66397685  -72.06087855]
ys = [ 137.5879595   120.55583871    1.32061395 -107.45517586   99.35018685
   82.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [170.13436092 156.68871343 176.15152863 147.98810632 140.96601948
 129.89933311 146.80235024 132.10850201 164.68581258 123.32389093]
dists_bs = [175.13690217 187.28833553 364.10205235 342.55473166 191.55053362
 201.30481225 190.03830501 195.4511099  342.93535346 199.59197368]
uav_gains = [2.63464755e-11 3.24787964e-11 2.40854862e-11 3.75018239e-11
 4.23658097e-11 5.19897705e-11 3.82670974e-11 4.98415206e-11
 2.86308105e-11 5.92039294e-11]
bs_gains = [5.77878620e-11 4.78921177e-11 7.44507933e-12 8.83181915e-12
 4.49676985e-11 3.91293331e-11 4.59768137e-11 4.24998396e-11
 8.80439989e-12 4.00768410e-11]
Round 26
-------------------------------
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.5605131  15.70320748  7.4465293   2.67409633 18.11274305  8.72126419
  3.31940453 10.65181908  7.8502879   7.07624586]
obj_prev = 89.11611082472227
eta_min = 5.392428774203226e-13	eta_max = 0.9242899713750001
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 20.70487256941968	eta = 0.909090909090909
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 36.71367954165409	eta = 0.5126865969778287
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 28.972791112910695	eta = 0.6496651065957375
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.57869875098564	eta = 0.6825054219090905
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.5077842638857	eta = 0.6842649064780151
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.50758684227239	eta = 0.6842698174388544
eta = 0.6842698174388544
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.03130742 0.06584498 0.03081048 0.01068428 0.07603231 0.03627686
 0.01341747 0.04447642 0.03230131 0.02931965]
ene_total = [2.4173172  4.46359599 2.39783696 1.11945579 5.09142412 2.67769452
 1.2842835  3.15534052 2.65007964 2.25055859]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 0 obj = 4.913144626268169
eta = 0.6842698174388544
freqs = [40426116.40412594 82838172.3252855  39968031.32913876 13573739.87970061
 95886067.29898961 46005714.64645749 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
eta_min = 0.6842698174388597	eta_max = 0.6842698174388548
af = 0.020050708162629324	bf = 1.5436207515894453	zeta = 0.022055778978892257	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [3.21329018e-06 2.83767235e-05 3.09102528e-06 1.23629712e-07
 4.39023530e-05 4.82205074e-06 2.44433458e-07 8.71936159e-06
 4.10498900e-06 2.54075489e-06]
ene_total = [1.757659   1.54642537 1.79534967 1.62243185 1.57005655 1.60857787
 1.61536282 1.52964443 2.35549529 1.59984681]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 1 obj = 4.913144626268177
eta = 0.6842698174388548
freqs = [40426116.40412594 82838172.32528551 39968031.32913877 13573739.87970061
 95886067.29898961 46005714.64645751 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
Done!
At round 26 energy consumption: 4.913144626268169
At round 26 eta: 0.6842698174388548
At round 26 local rounds: 12.423580809938418
At round 26 global rounds: 61.05343074483861
At round 26 a_n: 18.93386498808281
gradient difference: 0.4951755404472351
train() client id: f_00000-0-0 loss: 1.217940  [   32/  126]
train() client id: f_00000-0-1 loss: 1.342716  [   64/  126]
train() client id: f_00000-0-2 loss: 1.224122  [   96/  126]
train() client id: f_00000-1-0 loss: 1.254869  [   32/  126]
train() client id: f_00000-1-1 loss: 1.076378  [   64/  126]
train() client id: f_00000-1-2 loss: 1.194506  [   96/  126]
train() client id: f_00000-2-0 loss: 1.042572  [   32/  126]
train() client id: f_00000-2-1 loss: 1.042021  [   64/  126]
train() client id: f_00000-2-2 loss: 1.179141  [   96/  126]
train() client id: f_00000-3-0 loss: 1.125628  [   32/  126]
train() client id: f_00000-3-1 loss: 1.095271  [   64/  126]
train() client id: f_00000-3-2 loss: 1.040504  [   96/  126]
train() client id: f_00000-4-0 loss: 1.044266  [   32/  126]
train() client id: f_00000-4-1 loss: 1.042351  [   64/  126]
train() client id: f_00000-4-2 loss: 1.029159  [   96/  126]
train() client id: f_00000-5-0 loss: 0.923744  [   32/  126]
train() client id: f_00000-5-1 loss: 1.076231  [   64/  126]
train() client id: f_00000-5-2 loss: 0.972495  [   96/  126]
train() client id: f_00000-6-0 loss: 1.005785  [   32/  126]
train() client id: f_00000-6-1 loss: 0.872406  [   64/  126]
train() client id: f_00000-6-2 loss: 0.950687  [   96/  126]
train() client id: f_00000-7-0 loss: 0.993028  [   32/  126]
train() client id: f_00000-7-1 loss: 0.960316  [   64/  126]
train() client id: f_00000-7-2 loss: 0.918209  [   96/  126]
train() client id: f_00000-8-0 loss: 1.034505  [   32/  126]
train() client id: f_00000-8-1 loss: 0.930847  [   64/  126]
train() client id: f_00000-8-2 loss: 0.919732  [   96/  126]
train() client id: f_00000-9-0 loss: 0.948159  [   32/  126]
train() client id: f_00000-9-1 loss: 0.956705  [   64/  126]
train() client id: f_00000-9-2 loss: 0.948778  [   96/  126]
train() client id: f_00000-10-0 loss: 0.942804  [   32/  126]
train() client id: f_00000-10-1 loss: 0.875799  [   64/  126]
train() client id: f_00000-10-2 loss: 0.976549  [   96/  126]
train() client id: f_00000-11-0 loss: 0.847092  [   32/  126]
train() client id: f_00000-11-1 loss: 0.936357  [   64/  126]
train() client id: f_00000-11-2 loss: 1.098835  [   96/  126]
train() client id: f_00001-0-0 loss: 0.444725  [   32/  265]
train() client id: f_00001-0-1 loss: 0.590081  [   64/  265]
train() client id: f_00001-0-2 loss: 0.386358  [   96/  265]
train() client id: f_00001-0-3 loss: 0.426755  [  128/  265]
train() client id: f_00001-0-4 loss: 0.351214  [  160/  265]
train() client id: f_00001-0-5 loss: 0.386036  [  192/  265]
train() client id: f_00001-0-6 loss: 0.440213  [  224/  265]
train() client id: f_00001-0-7 loss: 0.374027  [  256/  265]
train() client id: f_00001-1-0 loss: 0.388955  [   32/  265]
train() client id: f_00001-1-1 loss: 0.421537  [   64/  265]
train() client id: f_00001-1-2 loss: 0.471290  [   96/  265]
train() client id: f_00001-1-3 loss: 0.350294  [  128/  265]
train() client id: f_00001-1-4 loss: 0.370454  [  160/  265]
train() client id: f_00001-1-5 loss: 0.495846  [  192/  265]
train() client id: f_00001-1-6 loss: 0.343302  [  224/  265]
train() client id: f_00001-1-7 loss: 0.559683  [  256/  265]
train() client id: f_00001-2-0 loss: 0.475560  [   32/  265]
train() client id: f_00001-2-1 loss: 0.443972  [   64/  265]
train() client id: f_00001-2-2 loss: 0.329158  [   96/  265]
train() client id: f_00001-2-3 loss: 0.487253  [  128/  265]
train() client id: f_00001-2-4 loss: 0.342987  [  160/  265]
train() client id: f_00001-2-5 loss: 0.423413  [  192/  265]
train() client id: f_00001-2-6 loss: 0.439706  [  224/  265]
train() client id: f_00001-2-7 loss: 0.359746  [  256/  265]
train() client id: f_00001-3-0 loss: 0.380493  [   32/  265]
train() client id: f_00001-3-1 loss: 0.393117  [   64/  265]
train() client id: f_00001-3-2 loss: 0.324708  [   96/  265]
train() client id: f_00001-3-3 loss: 0.549055  [  128/  265]
train() client id: f_00001-3-4 loss: 0.424478  [  160/  265]
train() client id: f_00001-3-5 loss: 0.372053  [  192/  265]
train() client id: f_00001-3-6 loss: 0.437830  [  224/  265]
train() client id: f_00001-3-7 loss: 0.422110  [  256/  265]
train() client id: f_00001-4-0 loss: 0.451659  [   32/  265]
train() client id: f_00001-4-1 loss: 0.454298  [   64/  265]
train() client id: f_00001-4-2 loss: 0.412433  [   96/  265]
train() client id: f_00001-4-3 loss: 0.437160  [  128/  265]
train() client id: f_00001-4-4 loss: 0.389101  [  160/  265]
train() client id: f_00001-4-5 loss: 0.374128  [  192/  265]
train() client id: f_00001-4-6 loss: 0.301016  [  224/  265]
train() client id: f_00001-4-7 loss: 0.389573  [  256/  265]
train() client id: f_00001-5-0 loss: 0.391432  [   32/  265]
train() client id: f_00001-5-1 loss: 0.459911  [   64/  265]
train() client id: f_00001-5-2 loss: 0.338532  [   96/  265]
train() client id: f_00001-5-3 loss: 0.531281  [  128/  265]
train() client id: f_00001-5-4 loss: 0.319230  [  160/  265]
train() client id: f_00001-5-5 loss: 0.372533  [  192/  265]
train() client id: f_00001-5-6 loss: 0.411099  [  224/  265]
train() client id: f_00001-5-7 loss: 0.375535  [  256/  265]
train() client id: f_00001-6-0 loss: 0.534968  [   32/  265]
train() client id: f_00001-6-1 loss: 0.326892  [   64/  265]
train() client id: f_00001-6-2 loss: 0.315359  [   96/  265]
train() client id: f_00001-6-3 loss: 0.328210  [  128/  265]
train() client id: f_00001-6-4 loss: 0.384949  [  160/  265]
train() client id: f_00001-6-5 loss: 0.366682  [  192/  265]
train() client id: f_00001-6-6 loss: 0.567084  [  224/  265]
train() client id: f_00001-6-7 loss: 0.377222  [  256/  265]
train() client id: f_00001-7-0 loss: 0.477120  [   32/  265]
train() client id: f_00001-7-1 loss: 0.321059  [   64/  265]
train() client id: f_00001-7-2 loss: 0.370932  [   96/  265]
train() client id: f_00001-7-3 loss: 0.364952  [  128/  265]
train() client id: f_00001-7-4 loss: 0.530458  [  160/  265]
train() client id: f_00001-7-5 loss: 0.372855  [  192/  265]
train() client id: f_00001-7-6 loss: 0.464792  [  224/  265]
train() client id: f_00001-7-7 loss: 0.357808  [  256/  265]
train() client id: f_00001-8-0 loss: 0.468955  [   32/  265]
train() client id: f_00001-8-1 loss: 0.325079  [   64/  265]
train() client id: f_00001-8-2 loss: 0.337798  [   96/  265]
train() client id: f_00001-8-3 loss: 0.471548  [  128/  265]
train() client id: f_00001-8-4 loss: 0.434234  [  160/  265]
train() client id: f_00001-8-5 loss: 0.367774  [  192/  265]
train() client id: f_00001-8-6 loss: 0.411780  [  224/  265]
train() client id: f_00001-8-7 loss: 0.437503  [  256/  265]
train() client id: f_00001-9-0 loss: 0.498947  [   32/  265]
train() client id: f_00001-9-1 loss: 0.456366  [   64/  265]
train() client id: f_00001-9-2 loss: 0.456554  [   96/  265]
train() client id: f_00001-9-3 loss: 0.482639  [  128/  265]
train() client id: f_00001-9-4 loss: 0.308202  [  160/  265]
train() client id: f_00001-9-5 loss: 0.292319  [  192/  265]
train() client id: f_00001-9-6 loss: 0.370384  [  224/  265]
train() client id: f_00001-9-7 loss: 0.394256  [  256/  265]
train() client id: f_00001-10-0 loss: 0.369387  [   32/  265]
train() client id: f_00001-10-1 loss: 0.383757  [   64/  265]
train() client id: f_00001-10-2 loss: 0.324726  [   96/  265]
train() client id: f_00001-10-3 loss: 0.376939  [  128/  265]
train() client id: f_00001-10-4 loss: 0.553745  [  160/  265]
train() client id: f_00001-10-5 loss: 0.467219  [  192/  265]
train() client id: f_00001-10-6 loss: 0.320661  [  224/  265]
train() client id: f_00001-10-7 loss: 0.467167  [  256/  265]
train() client id: f_00001-11-0 loss: 0.405735  [   32/  265]
train() client id: f_00001-11-1 loss: 0.413353  [   64/  265]
train() client id: f_00001-11-2 loss: 0.301784  [   96/  265]
train() client id: f_00001-11-3 loss: 0.554843  [  128/  265]
train() client id: f_00001-11-4 loss: 0.314880  [  160/  265]
train() client id: f_00001-11-5 loss: 0.435616  [  192/  265]
train() client id: f_00001-11-6 loss: 0.381782  [  224/  265]
train() client id: f_00001-11-7 loss: 0.402514  [  256/  265]
train() client id: f_00002-0-0 loss: 1.243660  [   32/  124]
train() client id: f_00002-0-1 loss: 1.223395  [   64/  124]
train() client id: f_00002-0-2 loss: 1.112325  [   96/  124]
train() client id: f_00002-1-0 loss: 1.288229  [   32/  124]
train() client id: f_00002-1-1 loss: 1.279619  [   64/  124]
train() client id: f_00002-1-2 loss: 1.018067  [   96/  124]
train() client id: f_00002-2-0 loss: 1.271231  [   32/  124]
train() client id: f_00002-2-1 loss: 1.053097  [   64/  124]
train() client id: f_00002-2-2 loss: 1.174559  [   96/  124]
train() client id: f_00002-3-0 loss: 1.038632  [   32/  124]
train() client id: f_00002-3-1 loss: 1.039029  [   64/  124]
train() client id: f_00002-3-2 loss: 1.104036  [   96/  124]
train() client id: f_00002-4-0 loss: 1.051022  [   32/  124]
train() client id: f_00002-4-1 loss: 1.160887  [   64/  124]
train() client id: f_00002-4-2 loss: 1.088646  [   96/  124]
train() client id: f_00002-5-0 loss: 1.146818  [   32/  124]
train() client id: f_00002-5-1 loss: 1.090838  [   64/  124]
train() client id: f_00002-5-2 loss: 1.074148  [   96/  124]
train() client id: f_00002-6-0 loss: 1.035764  [   32/  124]
train() client id: f_00002-6-1 loss: 1.055468  [   64/  124]
train() client id: f_00002-6-2 loss: 1.118378  [   96/  124]
train() client id: f_00002-7-0 loss: 0.997479  [   32/  124]
train() client id: f_00002-7-1 loss: 1.204539  [   64/  124]
train() client id: f_00002-7-2 loss: 1.085475  [   96/  124]
train() client id: f_00002-8-0 loss: 0.904118  [   32/  124]
train() client id: f_00002-8-1 loss: 1.080205  [   64/  124]
train() client id: f_00002-8-2 loss: 1.358153  [   96/  124]
train() client id: f_00002-9-0 loss: 0.933742  [   32/  124]
train() client id: f_00002-9-1 loss: 1.276793  [   64/  124]
train() client id: f_00002-9-2 loss: 0.897965  [   96/  124]
train() client id: f_00002-10-0 loss: 1.057512  [   32/  124]
train() client id: f_00002-10-1 loss: 1.107813  [   64/  124]
train() client id: f_00002-10-2 loss: 0.974449  [   96/  124]
train() client id: f_00002-11-0 loss: 0.936234  [   32/  124]
train() client id: f_00002-11-1 loss: 1.118531  [   64/  124]
train() client id: f_00002-11-2 loss: 1.024277  [   96/  124]
train() client id: f_00003-0-0 loss: 0.781922  [   32/   43]
train() client id: f_00003-1-0 loss: 0.687217  [   32/   43]
train() client id: f_00003-2-0 loss: 0.817255  [   32/   43]
train() client id: f_00003-3-0 loss: 0.781616  [   32/   43]
train() client id: f_00003-4-0 loss: 0.799764  [   32/   43]
train() client id: f_00003-5-0 loss: 0.751133  [   32/   43]
train() client id: f_00003-6-0 loss: 0.865864  [   32/   43]
train() client id: f_00003-7-0 loss: 0.813996  [   32/   43]
train() client id: f_00003-8-0 loss: 0.641833  [   32/   43]
train() client id: f_00003-9-0 loss: 0.757374  [   32/   43]
train() client id: f_00003-10-0 loss: 0.848495  [   32/   43]
train() client id: f_00003-11-0 loss: 0.642219  [   32/   43]
train() client id: f_00004-0-0 loss: 0.848920  [   32/  306]
train() client id: f_00004-0-1 loss: 0.902773  [   64/  306]
train() client id: f_00004-0-2 loss: 0.954791  [   96/  306]
train() client id: f_00004-0-3 loss: 0.906993  [  128/  306]
train() client id: f_00004-0-4 loss: 0.781305  [  160/  306]
train() client id: f_00004-0-5 loss: 0.834144  [  192/  306]
train() client id: f_00004-0-6 loss: 0.934530  [  224/  306]
train() client id: f_00004-0-7 loss: 0.946454  [  256/  306]
train() client id: f_00004-0-8 loss: 0.820470  [  288/  306]
train() client id: f_00004-1-0 loss: 0.991574  [   32/  306]
train() client id: f_00004-1-1 loss: 0.931200  [   64/  306]
train() client id: f_00004-1-2 loss: 0.859514  [   96/  306]
train() client id: f_00004-1-3 loss: 0.742666  [  128/  306]
train() client id: f_00004-1-4 loss: 0.884767  [  160/  306]
train() client id: f_00004-1-5 loss: 0.831639  [  192/  306]
train() client id: f_00004-1-6 loss: 0.960199  [  224/  306]
train() client id: f_00004-1-7 loss: 0.795933  [  256/  306]
train() client id: f_00004-1-8 loss: 0.856426  [  288/  306]
train() client id: f_00004-2-0 loss: 0.770703  [   32/  306]
train() client id: f_00004-2-1 loss: 0.994080  [   64/  306]
train() client id: f_00004-2-2 loss: 0.851221  [   96/  306]
train() client id: f_00004-2-3 loss: 0.855231  [  128/  306]
train() client id: f_00004-2-4 loss: 0.792391  [  160/  306]
train() client id: f_00004-2-5 loss: 0.907342  [  192/  306]
train() client id: f_00004-2-6 loss: 0.830533  [  224/  306]
train() client id: f_00004-2-7 loss: 0.899129  [  256/  306]
train() client id: f_00004-2-8 loss: 0.992018  [  288/  306]
train() client id: f_00004-3-0 loss: 0.842812  [   32/  306]
train() client id: f_00004-3-1 loss: 0.883122  [   64/  306]
train() client id: f_00004-3-2 loss: 0.865744  [   96/  306]
train() client id: f_00004-3-3 loss: 0.923722  [  128/  306]
train() client id: f_00004-3-4 loss: 0.923057  [  160/  306]
train() client id: f_00004-3-5 loss: 0.793309  [  192/  306]
train() client id: f_00004-3-6 loss: 0.869791  [  224/  306]
train() client id: f_00004-3-7 loss: 0.843762  [  256/  306]
train() client id: f_00004-3-8 loss: 0.911842  [  288/  306]
train() client id: f_00004-4-0 loss: 0.996419  [   32/  306]
train() client id: f_00004-4-1 loss: 0.891223  [   64/  306]
train() client id: f_00004-4-2 loss: 0.905076  [   96/  306]
train() client id: f_00004-4-3 loss: 0.869409  [  128/  306]
train() client id: f_00004-4-4 loss: 0.750397  [  160/  306]
train() client id: f_00004-4-5 loss: 0.934892  [  192/  306]
train() client id: f_00004-4-6 loss: 0.912381  [  224/  306]
train() client id: f_00004-4-7 loss: 0.884383  [  256/  306]
train() client id: f_00004-4-8 loss: 0.764920  [  288/  306]
train() client id: f_00004-5-0 loss: 0.951612  [   32/  306]
train() client id: f_00004-5-1 loss: 0.809803  [   64/  306]
train() client id: f_00004-5-2 loss: 0.889464  [   96/  306]
train() client id: f_00004-5-3 loss: 0.952034  [  128/  306]
train() client id: f_00004-5-4 loss: 0.937044  [  160/  306]
train() client id: f_00004-5-5 loss: 0.869829  [  192/  306]
train() client id: f_00004-5-6 loss: 0.844442  [  224/  306]
train() client id: f_00004-5-7 loss: 0.816219  [  256/  306]
train() client id: f_00004-5-8 loss: 0.786625  [  288/  306]
train() client id: f_00004-6-0 loss: 0.840499  [   32/  306]
train() client id: f_00004-6-1 loss: 0.806275  [   64/  306]
train() client id: f_00004-6-2 loss: 1.000332  [   96/  306]
train() client id: f_00004-6-3 loss: 0.784179  [  128/  306]
train() client id: f_00004-6-4 loss: 0.876145  [  160/  306]
train() client id: f_00004-6-5 loss: 0.779761  [  192/  306]
train() client id: f_00004-6-6 loss: 0.938930  [  224/  306]
train() client id: f_00004-6-7 loss: 0.818249  [  256/  306]
train() client id: f_00004-6-8 loss: 0.881906  [  288/  306]
train() client id: f_00004-7-0 loss: 0.870289  [   32/  306]
train() client id: f_00004-7-1 loss: 0.775687  [   64/  306]
train() client id: f_00004-7-2 loss: 0.827967  [   96/  306]
train() client id: f_00004-7-3 loss: 0.824812  [  128/  306]
train() client id: f_00004-7-4 loss: 0.907244  [  160/  306]
train() client id: f_00004-7-5 loss: 0.982691  [  192/  306]
train() client id: f_00004-7-6 loss: 0.900456  [  224/  306]
train() client id: f_00004-7-7 loss: 0.843154  [  256/  306]
train() client id: f_00004-7-8 loss: 0.839599  [  288/  306]
train() client id: f_00004-8-0 loss: 0.841122  [   32/  306]
train() client id: f_00004-8-1 loss: 0.941631  [   64/  306]
train() client id: f_00004-8-2 loss: 0.900388  [   96/  306]
train() client id: f_00004-8-3 loss: 0.938044  [  128/  306]
train() client id: f_00004-8-4 loss: 0.823359  [  160/  306]
train() client id: f_00004-8-5 loss: 0.716273  [  192/  306]
train() client id: f_00004-8-6 loss: 0.787612  [  224/  306]
train() client id: f_00004-8-7 loss: 0.944410  [  256/  306]
train() client id: f_00004-8-8 loss: 0.859820  [  288/  306]
train() client id: f_00004-9-0 loss: 0.964293  [   32/  306]
train() client id: f_00004-9-1 loss: 0.967891  [   64/  306]
train() client id: f_00004-9-2 loss: 0.870587  [   96/  306]
train() client id: f_00004-9-3 loss: 0.874226  [  128/  306]
train() client id: f_00004-9-4 loss: 0.851387  [  160/  306]
train() client id: f_00004-9-5 loss: 0.881076  [  192/  306]
train() client id: f_00004-9-6 loss: 0.757143  [  224/  306]
train() client id: f_00004-9-7 loss: 0.802196  [  256/  306]
train() client id: f_00004-9-8 loss: 0.870104  [  288/  306]
train() client id: f_00004-10-0 loss: 0.790809  [   32/  306]
train() client id: f_00004-10-1 loss: 0.830784  [   64/  306]
train() client id: f_00004-10-2 loss: 0.841838  [   96/  306]
train() client id: f_00004-10-3 loss: 0.927073  [  128/  306]
train() client id: f_00004-10-4 loss: 0.815001  [  160/  306]
train() client id: f_00004-10-5 loss: 0.910451  [  192/  306]
train() client id: f_00004-10-6 loss: 0.935267  [  224/  306]
train() client id: f_00004-10-7 loss: 0.890010  [  256/  306]
train() client id: f_00004-10-8 loss: 0.812613  [  288/  306]
train() client id: f_00004-11-0 loss: 0.956889  [   32/  306]
train() client id: f_00004-11-1 loss: 0.900896  [   64/  306]
train() client id: f_00004-11-2 loss: 0.842461  [   96/  306]
train() client id: f_00004-11-3 loss: 0.831420  [  128/  306]
train() client id: f_00004-11-4 loss: 0.724748  [  160/  306]
train() client id: f_00004-11-5 loss: 0.860902  [  192/  306]
train() client id: f_00004-11-6 loss: 0.891971  [  224/  306]
train() client id: f_00004-11-7 loss: 0.822915  [  256/  306]
train() client id: f_00004-11-8 loss: 0.912706  [  288/  306]
train() client id: f_00005-0-0 loss: 0.462525  [   32/  146]
train() client id: f_00005-0-1 loss: 0.710861  [   64/  146]
train() client id: f_00005-0-2 loss: 0.729088  [   96/  146]
train() client id: f_00005-0-3 loss: 0.567807  [  128/  146]
train() client id: f_00005-1-0 loss: 0.741837  [   32/  146]
train() client id: f_00005-1-1 loss: 0.408986  [   64/  146]
train() client id: f_00005-1-2 loss: 0.508123  [   96/  146]
train() client id: f_00005-1-3 loss: 0.681331  [  128/  146]
train() client id: f_00005-2-0 loss: 0.392409  [   32/  146]
train() client id: f_00005-2-1 loss: 0.380021  [   64/  146]
train() client id: f_00005-2-2 loss: 0.824088  [   96/  146]
train() client id: f_00005-2-3 loss: 0.863881  [  128/  146]
train() client id: f_00005-3-0 loss: 0.651598  [   32/  146]
train() client id: f_00005-3-1 loss: 0.643098  [   64/  146]
train() client id: f_00005-3-2 loss: 0.576715  [   96/  146]
train() client id: f_00005-3-3 loss: 0.558694  [  128/  146]
train() client id: f_00005-4-0 loss: 0.715503  [   32/  146]
train() client id: f_00005-4-1 loss: 0.515984  [   64/  146]
train() client id: f_00005-4-2 loss: 0.377078  [   96/  146]
train() client id: f_00005-4-3 loss: 0.588906  [  128/  146]
train() client id: f_00005-5-0 loss: 0.598761  [   32/  146]
train() client id: f_00005-5-1 loss: 0.609253  [   64/  146]
train() client id: f_00005-5-2 loss: 0.589912  [   96/  146]
train() client id: f_00005-5-3 loss: 0.545985  [  128/  146]
train() client id: f_00005-6-0 loss: 0.682486  [   32/  146]
train() client id: f_00005-6-1 loss: 0.669103  [   64/  146]
train() client id: f_00005-6-2 loss: 0.588194  [   96/  146]
train() client id: f_00005-6-3 loss: 0.542217  [  128/  146]
train() client id: f_00005-7-0 loss: 0.746610  [   32/  146]
train() client id: f_00005-7-1 loss: 0.634239  [   64/  146]
train() client id: f_00005-7-2 loss: 0.520748  [   96/  146]
train() client id: f_00005-7-3 loss: 0.595248  [  128/  146]
train() client id: f_00005-8-0 loss: 0.518940  [   32/  146]
train() client id: f_00005-8-1 loss: 0.559402  [   64/  146]
train() client id: f_00005-8-2 loss: 1.025041  [   96/  146]
train() client id: f_00005-8-3 loss: 0.295033  [  128/  146]
train() client id: f_00005-9-0 loss: 0.520315  [   32/  146]
train() client id: f_00005-9-1 loss: 0.415363  [   64/  146]
train() client id: f_00005-9-2 loss: 0.782469  [   96/  146]
train() client id: f_00005-9-3 loss: 0.674858  [  128/  146]
train() client id: f_00005-10-0 loss: 0.649076  [   32/  146]
train() client id: f_00005-10-1 loss: 0.646081  [   64/  146]
train() client id: f_00005-10-2 loss: 0.537760  [   96/  146]
train() client id: f_00005-10-3 loss: 0.626754  [  128/  146]
train() client id: f_00005-11-0 loss: 0.535363  [   32/  146]
train() client id: f_00005-11-1 loss: 0.482317  [   64/  146]
train() client id: f_00005-11-2 loss: 0.750152  [   96/  146]
train() client id: f_00005-11-3 loss: 0.586215  [  128/  146]
train() client id: f_00006-0-0 loss: 0.478356  [   32/   54]
train() client id: f_00006-1-0 loss: 0.523331  [   32/   54]
train() client id: f_00006-2-0 loss: 0.502711  [   32/   54]
train() client id: f_00006-3-0 loss: 0.518306  [   32/   54]
train() client id: f_00006-4-0 loss: 0.576436  [   32/   54]
train() client id: f_00006-5-0 loss: 0.528100  [   32/   54]
train() client id: f_00006-6-0 loss: 0.530729  [   32/   54]
train() client id: f_00006-7-0 loss: 0.535332  [   32/   54]
train() client id: f_00006-8-0 loss: 0.526534  [   32/   54]
train() client id: f_00006-9-0 loss: 0.530755  [   32/   54]
train() client id: f_00006-10-0 loss: 0.575359  [   32/   54]
train() client id: f_00006-11-0 loss: 0.580576  [   32/   54]
train() client id: f_00007-0-0 loss: 0.661918  [   32/  179]
train() client id: f_00007-0-1 loss: 0.625142  [   64/  179]
train() client id: f_00007-0-2 loss: 0.526234  [   96/  179]
train() client id: f_00007-0-3 loss: 0.823934  [  128/  179]
train() client id: f_00007-0-4 loss: 0.595514  [  160/  179]
train() client id: f_00007-1-0 loss: 0.735693  [   32/  179]
train() client id: f_00007-1-1 loss: 0.536783  [   64/  179]
train() client id: f_00007-1-2 loss: 0.632629  [   96/  179]
train() client id: f_00007-1-3 loss: 0.547702  [  128/  179]
train() client id: f_00007-1-4 loss: 0.512093  [  160/  179]
train() client id: f_00007-2-0 loss: 0.691836  [   32/  179]
train() client id: f_00007-2-1 loss: 0.677496  [   64/  179]
train() client id: f_00007-2-2 loss: 0.585017  [   96/  179]
train() client id: f_00007-2-3 loss: 0.500027  [  128/  179]
train() client id: f_00007-2-4 loss: 0.613865  [  160/  179]
train() client id: f_00007-3-0 loss: 0.633665  [   32/  179]
train() client id: f_00007-3-1 loss: 0.547461  [   64/  179]
train() client id: f_00007-3-2 loss: 0.643213  [   96/  179]
train() client id: f_00007-3-3 loss: 0.666833  [  128/  179]
train() client id: f_00007-3-4 loss: 0.627511  [  160/  179]
train() client id: f_00007-4-0 loss: 0.769521  [   32/  179]
train() client id: f_00007-4-1 loss: 0.458004  [   64/  179]
train() client id: f_00007-4-2 loss: 0.653667  [   96/  179]
train() client id: f_00007-4-3 loss: 0.560965  [  128/  179]
train() client id: f_00007-4-4 loss: 0.648470  [  160/  179]
train() client id: f_00007-5-0 loss: 0.542360  [   32/  179]
train() client id: f_00007-5-1 loss: 0.771549  [   64/  179]
train() client id: f_00007-5-2 loss: 0.507973  [   96/  179]
train() client id: f_00007-5-3 loss: 0.494927  [  128/  179]
train() client id: f_00007-5-4 loss: 0.594098  [  160/  179]
train() client id: f_00007-6-0 loss: 0.462160  [   32/  179]
train() client id: f_00007-6-1 loss: 0.717369  [   64/  179]
train() client id: f_00007-6-2 loss: 0.766047  [   96/  179]
train() client id: f_00007-6-3 loss: 0.571325  [  128/  179]
train() client id: f_00007-6-4 loss: 0.473691  [  160/  179]
train() client id: f_00007-7-0 loss: 0.630036  [   32/  179]
train() client id: f_00007-7-1 loss: 0.660828  [   64/  179]
train() client id: f_00007-7-2 loss: 0.487000  [   96/  179]
train() client id: f_00007-7-3 loss: 0.545632  [  128/  179]
train() client id: f_00007-7-4 loss: 0.587410  [  160/  179]
train() client id: f_00007-8-0 loss: 0.714515  [   32/  179]
train() client id: f_00007-8-1 loss: 0.482803  [   64/  179]
train() client id: f_00007-8-2 loss: 0.622551  [   96/  179]
train() client id: f_00007-8-3 loss: 0.511950  [  128/  179]
train() client id: f_00007-8-4 loss: 0.723738  [  160/  179]
train() client id: f_00007-9-0 loss: 0.583585  [   32/  179]
train() client id: f_00007-9-1 loss: 0.618214  [   64/  179]
train() client id: f_00007-9-2 loss: 0.572141  [   96/  179]
train() client id: f_00007-9-3 loss: 0.727164  [  128/  179]
train() client id: f_00007-9-4 loss: 0.538688  [  160/  179]
train() client id: f_00007-10-0 loss: 0.634937  [   32/  179]
train() client id: f_00007-10-1 loss: 0.716173  [   64/  179]
train() client id: f_00007-10-2 loss: 0.614942  [   96/  179]
train() client id: f_00007-10-3 loss: 0.447032  [  128/  179]
train() client id: f_00007-10-4 loss: 0.581843  [  160/  179]
train() client id: f_00007-11-0 loss: 0.546089  [   32/  179]
train() client id: f_00007-11-1 loss: 0.572316  [   64/  179]
train() client id: f_00007-11-2 loss: 0.584205  [   96/  179]
train() client id: f_00007-11-3 loss: 0.445154  [  128/  179]
train() client id: f_00007-11-4 loss: 0.848014  [  160/  179]
train() client id: f_00008-0-0 loss: 0.836591  [   32/  130]
train() client id: f_00008-0-1 loss: 0.782567  [   64/  130]
train() client id: f_00008-0-2 loss: 0.749374  [   96/  130]
train() client id: f_00008-0-3 loss: 0.795494  [  128/  130]
train() client id: f_00008-1-0 loss: 0.673118  [   32/  130]
train() client id: f_00008-1-1 loss: 0.914191  [   64/  130]
train() client id: f_00008-1-2 loss: 0.810543  [   96/  130]
train() client id: f_00008-1-3 loss: 0.799171  [  128/  130]
train() client id: f_00008-2-0 loss: 0.770349  [   32/  130]
train() client id: f_00008-2-1 loss: 0.776602  [   64/  130]
train() client id: f_00008-2-2 loss: 0.791968  [   96/  130]
train() client id: f_00008-2-3 loss: 0.854460  [  128/  130]
train() client id: f_00008-3-0 loss: 0.823648  [   32/  130]
train() client id: f_00008-3-1 loss: 0.746499  [   64/  130]
train() client id: f_00008-3-2 loss: 0.777877  [   96/  130]
train() client id: f_00008-3-3 loss: 0.847806  [  128/  130]
train() client id: f_00008-4-0 loss: 0.703399  [   32/  130]
train() client id: f_00008-4-1 loss: 0.816920  [   64/  130]
train() client id: f_00008-4-2 loss: 0.814535  [   96/  130]
train() client id: f_00008-4-3 loss: 0.862180  [  128/  130]
train() client id: f_00008-5-0 loss: 0.665411  [   32/  130]
train() client id: f_00008-5-1 loss: 0.835944  [   64/  130]
train() client id: f_00008-5-2 loss: 0.719210  [   96/  130]
train() client id: f_00008-5-3 loss: 0.973108  [  128/  130]
train() client id: f_00008-6-0 loss: 0.713001  [   32/  130]
train() client id: f_00008-6-1 loss: 0.977931  [   64/  130]
train() client id: f_00008-6-2 loss: 0.799550  [   96/  130]
train() client id: f_00008-6-3 loss: 0.668841  [  128/  130]
train() client id: f_00008-7-0 loss: 0.800726  [   32/  130]
train() client id: f_00008-7-1 loss: 0.763501  [   64/  130]
train() client id: f_00008-7-2 loss: 0.827723  [   96/  130]
train() client id: f_00008-7-3 loss: 0.781931  [  128/  130]
train() client id: f_00008-8-0 loss: 0.791273  [   32/  130]
train() client id: f_00008-8-1 loss: 0.811536  [   64/  130]
train() client id: f_00008-8-2 loss: 0.812049  [   96/  130]
train() client id: f_00008-8-3 loss: 0.779737  [  128/  130]
train() client id: f_00008-9-0 loss: 0.822901  [   32/  130]
train() client id: f_00008-9-1 loss: 0.758193  [   64/  130]
train() client id: f_00008-9-2 loss: 0.788503  [   96/  130]
train() client id: f_00008-9-3 loss: 0.793371  [  128/  130]
train() client id: f_00008-10-0 loss: 0.753150  [   32/  130]
train() client id: f_00008-10-1 loss: 0.865553  [   64/  130]
train() client id: f_00008-10-2 loss: 0.875214  [   96/  130]
train() client id: f_00008-10-3 loss: 0.696395  [  128/  130]
train() client id: f_00008-11-0 loss: 0.823292  [   32/  130]
train() client id: f_00008-11-1 loss: 0.830256  [   64/  130]
train() client id: f_00008-11-2 loss: 0.779924  [   96/  130]
train() client id: f_00008-11-3 loss: 0.743446  [  128/  130]
train() client id: f_00009-0-0 loss: 1.096204  [   32/  118]
train() client id: f_00009-0-1 loss: 1.220115  [   64/  118]
train() client id: f_00009-0-2 loss: 1.131402  [   96/  118]
train() client id: f_00009-1-0 loss: 1.212666  [   32/  118]
train() client id: f_00009-1-1 loss: 0.983557  [   64/  118]
train() client id: f_00009-1-2 loss: 0.942523  [   96/  118]
train() client id: f_00009-2-0 loss: 1.084344  [   32/  118]
train() client id: f_00009-2-1 loss: 1.085704  [   64/  118]
train() client id: f_00009-2-2 loss: 0.880840  [   96/  118]
train() client id: f_00009-3-0 loss: 1.015956  [   32/  118]
train() client id: f_00009-3-1 loss: 1.124735  [   64/  118]
train() client id: f_00009-3-2 loss: 0.921488  [   96/  118]
train() client id: f_00009-4-0 loss: 0.923959  [   32/  118]
train() client id: f_00009-4-1 loss: 1.119237  [   64/  118]
train() client id: f_00009-4-2 loss: 1.047673  [   96/  118]
train() client id: f_00009-5-0 loss: 0.976520  [   32/  118]
train() client id: f_00009-5-1 loss: 0.977571  [   64/  118]
train() client id: f_00009-5-2 loss: 1.020518  [   96/  118]
train() client id: f_00009-6-0 loss: 0.882340  [   32/  118]
train() client id: f_00009-6-1 loss: 0.906934  [   64/  118]
train() client id: f_00009-6-2 loss: 1.049395  [   96/  118]
train() client id: f_00009-7-0 loss: 0.936516  [   32/  118]
train() client id: f_00009-7-1 loss: 0.930736  [   64/  118]
train() client id: f_00009-7-2 loss: 0.935216  [   96/  118]
train() client id: f_00009-8-0 loss: 1.002110  [   32/  118]
train() client id: f_00009-8-1 loss: 0.958775  [   64/  118]
train() client id: f_00009-8-2 loss: 0.818151  [   96/  118]
train() client id: f_00009-9-0 loss: 1.003140  [   32/  118]
train() client id: f_00009-9-1 loss: 0.988606  [   64/  118]
train() client id: f_00009-9-2 loss: 0.809141  [   96/  118]
train() client id: f_00009-10-0 loss: 0.962094  [   32/  118]
train() client id: f_00009-10-1 loss: 0.931778  [   64/  118]
train() client id: f_00009-10-2 loss: 0.877338  [   96/  118]
train() client id: f_00009-11-0 loss: 0.944816  [   32/  118]
train() client id: f_00009-11-1 loss: 0.839148  [   64/  118]
train() client id: f_00009-11-2 loss: 0.956217  [   96/  118]
At round 26 accuracy: 0.649867374005305
At round 26 training accuracy: 0.579476861167002
At round 26 training loss: 0.8391384729073234
update_location
xs = [  -3.9056584     4.20031788  150.00902392   18.81129433    0.97929623
    3.95640986 -112.44319194  -91.32485185  134.66397685  -77.06087855]
ys = [ 142.5879595   125.55583871    1.32061395 -112.45517586  104.35018685
   87.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [174.20269906 160.56746652 180.28990897 151.65761231 144.53345812
 133.14269882 150.50037194 135.42859676 168.65069414 126.31069179]
dists_bs = [174.13734262 185.89642012 368.50426696 346.68904791 189.63150793
 199.06473556 188.31885006 193.23401585 347.38493983 197.05979992]
uav_gains = [2.47904434e-11 3.05317297e-11 2.26669887e-11 3.52625235e-11
 3.97925071e-11 4.88782381e-11 3.59484763e-11 4.68391522e-11
 2.69445184e-11 5.57642431e-11]
bs_gains = [5.87214444e-11 4.89029672e-11 7.19871577e-12 8.54007614e-12
 4.62535094e-11 4.03747604e-11 4.71619185e-11 4.38793376e-11
 8.49226080e-12 4.15355131e-11]
Round 27
-------------------------------
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.428454   15.42356035  7.31658542  2.62856167 17.79004913  8.56531288
  3.26241587 10.46431357  7.71314909  6.94940181]
obj_prev = 87.54180377366474
eta_min = 3.2918080146347544e-13	eta_max = 0.9246953843511696
af = 18.488129690050464	bf = 1.525402092626451	zeta = 20.336942659055513	eta = 0.909090909090909
af = 18.488129690050464	bf = 1.525402092626451	zeta = 36.16259963393917	eta = 0.5112500173438599
af = 18.488129690050464	bf = 1.525402092626451	zeta = 28.49928344660974	eta = 0.6487226152435002
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.11857262246232	eta = 0.6817515784269833
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.048110971781785	eta = 0.6835275745998821
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.047913433089164	eta = 0.6835325665983885
eta = 0.6835325665983885
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.03139638 0.06603207 0.03089802 0.01071464 0.07624835 0.03637993
 0.01345559 0.04460279 0.03239309 0.02940296]
ene_total = [2.38132415 4.38338617 2.36243079 1.10502586 4.99963694 2.62710832
 1.26705354 3.1051769  2.60980459 2.20696617]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 0 obj = 4.842867033053818
eta = 0.6835325665983885
freqs = [39761858.95611374 81214212.7965591  39312506.23491908 13349480.34302717
 93973501.61782414 45073814.47997239 16750787.69960297 54945517.04012107
 44182442.51361968 36388553.66776452]
eta_min = 0.6835325665983994	eta_max = 0.6835325665983844
af = 0.018943411267469068	bf = 1.525402092626451	zeta = 0.020837752394215977	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [3.10856006e-06 2.72750345e-05 2.99046350e-06 1.19578344e-07
 4.21684466e-05 4.62867660e-06 2.36439257e-07 8.43282168e-06
 3.96002704e-06 2.43818667e-06]
ene_total = [1.75137172 1.512205   1.78937036 1.61521218 1.53279247 1.56936094
 1.60841252 1.52178181 2.33933449 1.55944202]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 1 obj = 4.842867033053756
eta = 0.6835325665983844
freqs = [39761858.95611373 81214212.79655911 39312506.23491906 13349480.34302717
 93973501.61782415 45073814.4799724  16750787.69960297 54945517.04012109
 44182442.51361962 36388553.66776453]
Done!
At round 27 energy consumption: 4.842867033053818
At round 27 eta: 0.6835325665983844
At round 27 local rounds: 12.458880266672946
At round 27 global rounds: 59.82879433933611
At round 27 a_n: 18.59131914111349
gradient difference: 0.4145682454109192
train() client id: f_00000-0-0 loss: 1.299342  [   32/  126]
train() client id: f_00000-0-1 loss: 1.180813  [   64/  126]
train() client id: f_00000-0-2 loss: 1.567625  [   96/  126]
train() client id: f_00000-1-0 loss: 1.225916  [   32/  126]
train() client id: f_00000-1-1 loss: 1.186234  [   64/  126]
train() client id: f_00000-1-2 loss: 1.263970  [   96/  126]
train() client id: f_00000-2-0 loss: 1.149191  [   32/  126]
train() client id: f_00000-2-1 loss: 0.992796  [   64/  126]
train() client id: f_00000-2-2 loss: 1.099927  [   96/  126]
train() client id: f_00000-3-0 loss: 0.935996  [   32/  126]
train() client id: f_00000-3-1 loss: 1.007476  [   64/  126]
train() client id: f_00000-3-2 loss: 0.958147  [   96/  126]
train() client id: f_00000-4-0 loss: 0.955595  [   32/  126]
train() client id: f_00000-4-1 loss: 1.019279  [   64/  126]
train() client id: f_00000-4-2 loss: 0.874508  [   96/  126]
train() client id: f_00000-5-0 loss: 0.872138  [   32/  126]
train() client id: f_00000-5-1 loss: 0.843035  [   64/  126]
train() client id: f_00000-5-2 loss: 0.872172  [   96/  126]
train() client id: f_00000-6-0 loss: 0.844457  [   32/  126]
train() client id: f_00000-6-1 loss: 0.777370  [   64/  126]
train() client id: f_00000-6-2 loss: 0.789954  [   96/  126]
train() client id: f_00000-7-0 loss: 0.912876  [   32/  126]
train() client id: f_00000-7-1 loss: 0.683555  [   64/  126]
train() client id: f_00000-7-2 loss: 0.801734  [   96/  126]
train() client id: f_00000-8-0 loss: 0.698874  [   32/  126]
train() client id: f_00000-8-1 loss: 0.779235  [   64/  126]
train() client id: f_00000-8-2 loss: 0.779898  [   96/  126]
train() client id: f_00000-9-0 loss: 0.800650  [   32/  126]
train() client id: f_00000-9-1 loss: 0.784746  [   64/  126]
train() client id: f_00000-9-2 loss: 0.721186  [   96/  126]
train() client id: f_00000-10-0 loss: 0.710772  [   32/  126]
train() client id: f_00000-10-1 loss: 0.867507  [   64/  126]
train() client id: f_00000-10-2 loss: 0.595481  [   96/  126]
train() client id: f_00000-11-0 loss: 0.787113  [   32/  126]
train() client id: f_00000-11-1 loss: 0.625545  [   64/  126]
train() client id: f_00000-11-2 loss: 0.773226  [   96/  126]
train() client id: f_00001-0-0 loss: 0.370455  [   32/  265]
train() client id: f_00001-0-1 loss: 0.422335  [   64/  265]
train() client id: f_00001-0-2 loss: 0.472277  [   96/  265]
train() client id: f_00001-0-3 loss: 0.449485  [  128/  265]
train() client id: f_00001-0-4 loss: 0.557641  [  160/  265]
train() client id: f_00001-0-5 loss: 0.449108  [  192/  265]
train() client id: f_00001-0-6 loss: 0.424594  [  224/  265]
train() client id: f_00001-0-7 loss: 0.400735  [  256/  265]
train() client id: f_00001-1-0 loss: 0.502621  [   32/  265]
train() client id: f_00001-1-1 loss: 0.359653  [   64/  265]
train() client id: f_00001-1-2 loss: 0.549950  [   96/  265]
train() client id: f_00001-1-3 loss: 0.368631  [  128/  265]
train() client id: f_00001-1-4 loss: 0.401770  [  160/  265]
train() client id: f_00001-1-5 loss: 0.419607  [  192/  265]
train() client id: f_00001-1-6 loss: 0.483003  [  224/  265]
train() client id: f_00001-1-7 loss: 0.409479  [  256/  265]
train() client id: f_00001-2-0 loss: 0.353829  [   32/  265]
train() client id: f_00001-2-1 loss: 0.487292  [   64/  265]
train() client id: f_00001-2-2 loss: 0.449470  [   96/  265]
train() client id: f_00001-2-3 loss: 0.431230  [  128/  265]
train() client id: f_00001-2-4 loss: 0.400070  [  160/  265]
train() client id: f_00001-2-5 loss: 0.439786  [  192/  265]
train() client id: f_00001-2-6 loss: 0.385105  [  224/  265]
train() client id: f_00001-2-7 loss: 0.405587  [  256/  265]
train() client id: f_00001-3-0 loss: 0.363022  [   32/  265]
train() client id: f_00001-3-1 loss: 0.477286  [   64/  265]
train() client id: f_00001-3-2 loss: 0.313145  [   96/  265]
train() client id: f_00001-3-3 loss: 0.347302  [  128/  265]
train() client id: f_00001-3-4 loss: 0.409930  [  160/  265]
train() client id: f_00001-3-5 loss: 0.521878  [  192/  265]
train() client id: f_00001-3-6 loss: 0.436676  [  224/  265]
train() client id: f_00001-3-7 loss: 0.469718  [  256/  265]
train() client id: f_00001-4-0 loss: 0.317907  [   32/  265]
train() client id: f_00001-4-1 loss: 0.400086  [   64/  265]
train() client id: f_00001-4-2 loss: 0.449323  [   96/  265]
train() client id: f_00001-4-3 loss: 0.336943  [  128/  265]
train() client id: f_00001-4-4 loss: 0.400147  [  160/  265]
train() client id: f_00001-4-5 loss: 0.332137  [  192/  265]
train() client id: f_00001-4-6 loss: 0.454860  [  224/  265]
train() client id: f_00001-4-7 loss: 0.497039  [  256/  265]
train() client id: f_00001-5-0 loss: 0.378138  [   32/  265]
train() client id: f_00001-5-1 loss: 0.354246  [   64/  265]
train() client id: f_00001-5-2 loss: 0.517639  [   96/  265]
train() client id: f_00001-5-3 loss: 0.341004  [  128/  265]
train() client id: f_00001-5-4 loss: 0.331864  [  160/  265]
train() client id: f_00001-5-5 loss: 0.453258  [  192/  265]
train() client id: f_00001-5-6 loss: 0.463177  [  224/  265]
train() client id: f_00001-5-7 loss: 0.480870  [  256/  265]
train() client id: f_00001-6-0 loss: 0.404563  [   32/  265]
train() client id: f_00001-6-1 loss: 0.376607  [   64/  265]
train() client id: f_00001-6-2 loss: 0.422215  [   96/  265]
train() client id: f_00001-6-3 loss: 0.452745  [  128/  265]
train() client id: f_00001-6-4 loss: 0.325617  [  160/  265]
train() client id: f_00001-6-5 loss: 0.452903  [  192/  265]
train() client id: f_00001-6-6 loss: 0.365424  [  224/  265]
train() client id: f_00001-6-7 loss: 0.414746  [  256/  265]
train() client id: f_00001-7-0 loss: 0.381457  [   32/  265]
train() client id: f_00001-7-1 loss: 0.318960  [   64/  265]
train() client id: f_00001-7-2 loss: 0.386388  [   96/  265]
train() client id: f_00001-7-3 loss: 0.441425  [  128/  265]
train() client id: f_00001-7-4 loss: 0.430494  [  160/  265]
train() client id: f_00001-7-5 loss: 0.398980  [  192/  265]
train() client id: f_00001-7-6 loss: 0.355646  [  224/  265]
train() client id: f_00001-7-7 loss: 0.550848  [  256/  265]
train() client id: f_00001-8-0 loss: 0.448181  [   32/  265]
train() client id: f_00001-8-1 loss: 0.386331  [   64/  265]
train() client id: f_00001-8-2 loss: 0.416147  [   96/  265]
train() client id: f_00001-8-3 loss: 0.387537  [  128/  265]
train() client id: f_00001-8-4 loss: 0.551951  [  160/  265]
train() client id: f_00001-8-5 loss: 0.302876  [  192/  265]
train() client id: f_00001-8-6 loss: 0.364192  [  224/  265]
train() client id: f_00001-8-7 loss: 0.397037  [  256/  265]
train() client id: f_00001-9-0 loss: 0.349983  [   32/  265]
train() client id: f_00001-9-1 loss: 0.516084  [   64/  265]
train() client id: f_00001-9-2 loss: 0.353808  [   96/  265]
train() client id: f_00001-9-3 loss: 0.429960  [  128/  265]
train() client id: f_00001-9-4 loss: 0.388152  [  160/  265]
train() client id: f_00001-9-5 loss: 0.409358  [  192/  265]
train() client id: f_00001-9-6 loss: 0.370212  [  224/  265]
train() client id: f_00001-9-7 loss: 0.435842  [  256/  265]
train() client id: f_00001-10-0 loss: 0.381668  [   32/  265]
train() client id: f_00001-10-1 loss: 0.390084  [   64/  265]
train() client id: f_00001-10-2 loss: 0.454777  [   96/  265]
train() client id: f_00001-10-3 loss: 0.360773  [  128/  265]
train() client id: f_00001-10-4 loss: 0.414710  [  160/  265]
train() client id: f_00001-10-5 loss: 0.439645  [  192/  265]
train() client id: f_00001-10-6 loss: 0.441437  [  224/  265]
train() client id: f_00001-10-7 loss: 0.369404  [  256/  265]
train() client id: f_00001-11-0 loss: 0.476161  [   32/  265]
train() client id: f_00001-11-1 loss: 0.312139  [   64/  265]
train() client id: f_00001-11-2 loss: 0.373350  [   96/  265]
train() client id: f_00001-11-3 loss: 0.308888  [  128/  265]
train() client id: f_00001-11-4 loss: 0.380271  [  160/  265]
train() client id: f_00001-11-5 loss: 0.360303  [  192/  265]
train() client id: f_00001-11-6 loss: 0.525128  [  224/  265]
train() client id: f_00001-11-7 loss: 0.470307  [  256/  265]
train() client id: f_00002-0-0 loss: 1.094288  [   32/  124]
train() client id: f_00002-0-1 loss: 1.260422  [   64/  124]
train() client id: f_00002-0-2 loss: 1.224121  [   96/  124]
train() client id: f_00002-1-0 loss: 1.155483  [   32/  124]
train() client id: f_00002-1-1 loss: 1.205840  [   64/  124]
train() client id: f_00002-1-2 loss: 1.034380  [   96/  124]
train() client id: f_00002-2-0 loss: 1.150005  [   32/  124]
train() client id: f_00002-2-1 loss: 1.243769  [   64/  124]
train() client id: f_00002-2-2 loss: 0.981046  [   96/  124]
train() client id: f_00002-3-0 loss: 1.010263  [   32/  124]
train() client id: f_00002-3-1 loss: 1.169861  [   64/  124]
train() client id: f_00002-3-2 loss: 1.136674  [   96/  124]
train() client id: f_00002-4-0 loss: 0.985496  [   32/  124]
train() client id: f_00002-4-1 loss: 1.127501  [   64/  124]
train() client id: f_00002-4-2 loss: 1.057533  [   96/  124]
train() client id: f_00002-5-0 loss: 1.039317  [   32/  124]
train() client id: f_00002-5-1 loss: 0.989909  [   64/  124]
train() client id: f_00002-5-2 loss: 1.174167  [   96/  124]
train() client id: f_00002-6-0 loss: 0.958233  [   32/  124]
train() client id: f_00002-6-1 loss: 1.034323  [   64/  124]
train() client id: f_00002-6-2 loss: 1.088518  [   96/  124]
train() client id: f_00002-7-0 loss: 0.978417  [   32/  124]
train() client id: f_00002-7-1 loss: 0.939965  [   64/  124]
train() client id: f_00002-7-2 loss: 1.171163  [   96/  124]
train() client id: f_00002-8-0 loss: 1.049916  [   32/  124]
train() client id: f_00002-8-1 loss: 1.001833  [   64/  124]
train() client id: f_00002-8-2 loss: 1.074470  [   96/  124]
train() client id: f_00002-9-0 loss: 0.890044  [   32/  124]
train() client id: f_00002-9-1 loss: 0.996879  [   64/  124]
train() client id: f_00002-9-2 loss: 1.092129  [   96/  124]
train() client id: f_00002-10-0 loss: 0.965220  [   32/  124]
train() client id: f_00002-10-1 loss: 0.960145  [   64/  124]
train() client id: f_00002-10-2 loss: 1.029585  [   96/  124]
train() client id: f_00002-11-0 loss: 1.056181  [   32/  124]
train() client id: f_00002-11-1 loss: 0.995619  [   64/  124]
train() client id: f_00002-11-2 loss: 0.951464  [   96/  124]
train() client id: f_00003-0-0 loss: 0.757006  [   32/   43]
train() client id: f_00003-1-0 loss: 0.864181  [   32/   43]
train() client id: f_00003-2-0 loss: 0.813423  [   32/   43]
train() client id: f_00003-3-0 loss: 0.837568  [   32/   43]
train() client id: f_00003-4-0 loss: 0.858788  [   32/   43]
train() client id: f_00003-5-0 loss: 0.747332  [   32/   43]
train() client id: f_00003-6-0 loss: 0.696407  [   32/   43]
train() client id: f_00003-7-0 loss: 0.970363  [   32/   43]
train() client id: f_00003-8-0 loss: 0.824181  [   32/   43]
train() client id: f_00003-9-0 loss: 0.639986  [   32/   43]
train() client id: f_00003-10-0 loss: 0.671779  [   32/   43]
train() client id: f_00003-11-0 loss: 0.838819  [   32/   43]
train() client id: f_00004-0-0 loss: 0.856462  [   32/  306]
train() client id: f_00004-0-1 loss: 0.843446  [   64/  306]
train() client id: f_00004-0-2 loss: 0.895055  [   96/  306]
train() client id: f_00004-0-3 loss: 0.611474  [  128/  306]
train() client id: f_00004-0-4 loss: 0.818814  [  160/  306]
train() client id: f_00004-0-5 loss: 0.803703  [  192/  306]
train() client id: f_00004-0-6 loss: 0.756446  [  224/  306]
train() client id: f_00004-0-7 loss: 0.957974  [  256/  306]
train() client id: f_00004-0-8 loss: 0.851899  [  288/  306]
train() client id: f_00004-1-0 loss: 0.691981  [   32/  306]
train() client id: f_00004-1-1 loss: 0.910014  [   64/  306]
train() client id: f_00004-1-2 loss: 0.726437  [   96/  306]
train() client id: f_00004-1-3 loss: 0.840870  [  128/  306]
train() client id: f_00004-1-4 loss: 0.881083  [  160/  306]
train() client id: f_00004-1-5 loss: 0.832522  [  192/  306]
train() client id: f_00004-1-6 loss: 0.792527  [  224/  306]
train() client id: f_00004-1-7 loss: 0.879841  [  256/  306]
train() client id: f_00004-1-8 loss: 0.782418  [  288/  306]
train() client id: f_00004-2-0 loss: 0.792319  [   32/  306]
train() client id: f_00004-2-1 loss: 0.921377  [   64/  306]
train() client id: f_00004-2-2 loss: 0.759031  [   96/  306]
train() client id: f_00004-2-3 loss: 0.856512  [  128/  306]
train() client id: f_00004-2-4 loss: 0.791270  [  160/  306]
train() client id: f_00004-2-5 loss: 0.927761  [  192/  306]
train() client id: f_00004-2-6 loss: 0.785709  [  224/  306]
train() client id: f_00004-2-7 loss: 0.775584  [  256/  306]
train() client id: f_00004-2-8 loss: 0.832320  [  288/  306]
train() client id: f_00004-3-0 loss: 0.730126  [   32/  306]
train() client id: f_00004-3-1 loss: 0.833429  [   64/  306]
train() client id: f_00004-3-2 loss: 0.849302  [   96/  306]
train() client id: f_00004-3-3 loss: 0.836785  [  128/  306]
train() client id: f_00004-3-4 loss: 0.778146  [  160/  306]
train() client id: f_00004-3-5 loss: 0.785215  [  192/  306]
train() client id: f_00004-3-6 loss: 0.854864  [  224/  306]
train() client id: f_00004-3-7 loss: 0.670490  [  256/  306]
train() client id: f_00004-3-8 loss: 0.958714  [  288/  306]
train() client id: f_00004-4-0 loss: 0.843339  [   32/  306]
train() client id: f_00004-4-1 loss: 0.868091  [   64/  306]
train() client id: f_00004-4-2 loss: 0.934316  [   96/  306]
train() client id: f_00004-4-3 loss: 0.740661  [  128/  306]
train() client id: f_00004-4-4 loss: 0.738705  [  160/  306]
train() client id: f_00004-4-5 loss: 1.002961  [  192/  306]
train() client id: f_00004-4-6 loss: 0.762935  [  224/  306]
train() client id: f_00004-4-7 loss: 0.784465  [  256/  306]
train() client id: f_00004-4-8 loss: 0.767642  [  288/  306]
train() client id: f_00004-5-0 loss: 0.823783  [   32/  306]
train() client id: f_00004-5-1 loss: 0.909797  [   64/  306]
train() client id: f_00004-5-2 loss: 0.753913  [   96/  306]
train() client id: f_00004-5-3 loss: 0.890095  [  128/  306]
train() client id: f_00004-5-4 loss: 0.795126  [  160/  306]
train() client id: f_00004-5-5 loss: 0.716155  [  192/  306]
train() client id: f_00004-5-6 loss: 0.865371  [  224/  306]
train() client id: f_00004-5-7 loss: 0.720250  [  256/  306]
train() client id: f_00004-5-8 loss: 0.775031  [  288/  306]
train() client id: f_00004-6-0 loss: 0.775298  [   32/  306]
train() client id: f_00004-6-1 loss: 0.747124  [   64/  306]
train() client id: f_00004-6-2 loss: 0.845461  [   96/  306]
train() client id: f_00004-6-3 loss: 0.742544  [  128/  306]
train() client id: f_00004-6-4 loss: 0.914149  [  160/  306]
train() client id: f_00004-6-5 loss: 0.758301  [  192/  306]
train() client id: f_00004-6-6 loss: 0.743339  [  224/  306]
train() client id: f_00004-6-7 loss: 0.845404  [  256/  306]
train() client id: f_00004-6-8 loss: 0.994528  [  288/  306]
train() client id: f_00004-7-0 loss: 0.733791  [   32/  306]
train() client id: f_00004-7-1 loss: 0.786556  [   64/  306]
train() client id: f_00004-7-2 loss: 0.864567  [   96/  306]
train() client id: f_00004-7-3 loss: 0.848000  [  128/  306]
train() client id: f_00004-7-4 loss: 0.862674  [  160/  306]
train() client id: f_00004-7-5 loss: 0.762433  [  192/  306]
train() client id: f_00004-7-6 loss: 0.842578  [  224/  306]
train() client id: f_00004-7-7 loss: 0.749471  [  256/  306]
train() client id: f_00004-7-8 loss: 0.903637  [  288/  306]
train() client id: f_00004-8-0 loss: 0.856929  [   32/  306]
train() client id: f_00004-8-1 loss: 0.779009  [   64/  306]
train() client id: f_00004-8-2 loss: 0.816381  [   96/  306]
train() client id: f_00004-8-3 loss: 0.800835  [  128/  306]
train() client id: f_00004-8-4 loss: 0.817953  [  160/  306]
train() client id: f_00004-8-5 loss: 0.827960  [  192/  306]
train() client id: f_00004-8-6 loss: 0.656733  [  224/  306]
train() client id: f_00004-8-7 loss: 1.038511  [  256/  306]
train() client id: f_00004-8-8 loss: 0.810057  [  288/  306]
train() client id: f_00004-9-0 loss: 0.748273  [   32/  306]
train() client id: f_00004-9-1 loss: 0.715794  [   64/  306]
train() client id: f_00004-9-2 loss: 0.732698  [   96/  306]
train() client id: f_00004-9-3 loss: 0.823650  [  128/  306]
train() client id: f_00004-9-4 loss: 0.807408  [  160/  306]
train() client id: f_00004-9-5 loss: 0.857261  [  192/  306]
train() client id: f_00004-9-6 loss: 0.862999  [  224/  306]
train() client id: f_00004-9-7 loss: 0.893412  [  256/  306]
train() client id: f_00004-9-8 loss: 0.899347  [  288/  306]
train() client id: f_00004-10-0 loss: 0.718983  [   32/  306]
train() client id: f_00004-10-1 loss: 0.709415  [   64/  306]
train() client id: f_00004-10-2 loss: 0.730888  [   96/  306]
train() client id: f_00004-10-3 loss: 0.887972  [  128/  306]
train() client id: f_00004-10-4 loss: 0.832867  [  160/  306]
train() client id: f_00004-10-5 loss: 0.874679  [  192/  306]
train() client id: f_00004-10-6 loss: 0.885541  [  224/  306]
train() client id: f_00004-10-7 loss: 0.849659  [  256/  306]
train() client id: f_00004-10-8 loss: 0.757109  [  288/  306]
train() client id: f_00004-11-0 loss: 0.794043  [   32/  306]
train() client id: f_00004-11-1 loss: 0.906837  [   64/  306]
train() client id: f_00004-11-2 loss: 0.773901  [   96/  306]
train() client id: f_00004-11-3 loss: 0.962347  [  128/  306]
train() client id: f_00004-11-4 loss: 0.773705  [  160/  306]
train() client id: f_00004-11-5 loss: 0.810214  [  192/  306]
train() client id: f_00004-11-6 loss: 0.770437  [  224/  306]
train() client id: f_00004-11-7 loss: 0.930751  [  256/  306]
train() client id: f_00004-11-8 loss: 0.637309  [  288/  306]
train() client id: f_00005-0-0 loss: 0.440869  [   32/  146]
train() client id: f_00005-0-1 loss: 0.541109  [   64/  146]
train() client id: f_00005-0-2 loss: 0.827754  [   96/  146]
train() client id: f_00005-0-3 loss: 0.564382  [  128/  146]
train() client id: f_00005-1-0 loss: 0.379406  [   32/  146]
train() client id: f_00005-1-1 loss: 0.578649  [   64/  146]
train() client id: f_00005-1-2 loss: 0.697835  [   96/  146]
train() client id: f_00005-1-3 loss: 0.815391  [  128/  146]
train() client id: f_00005-2-0 loss: 0.644498  [   32/  146]
train() client id: f_00005-2-1 loss: 0.662645  [   64/  146]
train() client id: f_00005-2-2 loss: 0.363861  [   96/  146]
train() client id: f_00005-2-3 loss: 0.752988  [  128/  146]
train() client id: f_00005-3-0 loss: 0.477584  [   32/  146]
train() client id: f_00005-3-1 loss: 0.475462  [   64/  146]
train() client id: f_00005-3-2 loss: 0.886074  [   96/  146]
train() client id: f_00005-3-3 loss: 0.793300  [  128/  146]
train() client id: f_00005-4-0 loss: 0.543906  [   32/  146]
train() client id: f_00005-4-1 loss: 0.747670  [   64/  146]
train() client id: f_00005-4-2 loss: 0.628005  [   96/  146]
train() client id: f_00005-4-3 loss: 0.661432  [  128/  146]
train() client id: f_00005-5-0 loss: 0.426451  [   32/  146]
train() client id: f_00005-5-1 loss: 0.587079  [   64/  146]
train() client id: f_00005-5-2 loss: 0.993575  [   96/  146]
train() client id: f_00005-5-3 loss: 0.542304  [  128/  146]
train() client id: f_00005-6-0 loss: 0.871705  [   32/  146]
train() client id: f_00005-6-1 loss: 0.438597  [   64/  146]
train() client id: f_00005-6-2 loss: 0.668886  [   96/  146]
train() client id: f_00005-6-3 loss: 0.518976  [  128/  146]
train() client id: f_00005-7-0 loss: 0.624767  [   32/  146]
train() client id: f_00005-7-1 loss: 0.529028  [   64/  146]
train() client id: f_00005-7-2 loss: 0.678550  [   96/  146]
train() client id: f_00005-7-3 loss: 0.679830  [  128/  146]
train() client id: f_00005-8-0 loss: 0.957601  [   32/  146]
train() client id: f_00005-8-1 loss: 0.487298  [   64/  146]
train() client id: f_00005-8-2 loss: 0.463408  [   96/  146]
train() client id: f_00005-8-3 loss: 0.596600  [  128/  146]
train() client id: f_00005-9-0 loss: 0.821735  [   32/  146]
train() client id: f_00005-9-1 loss: 0.505380  [   64/  146]
train() client id: f_00005-9-2 loss: 0.474848  [   96/  146]
train() client id: f_00005-9-3 loss: 0.615014  [  128/  146]
train() client id: f_00005-10-0 loss: 0.634467  [   32/  146]
train() client id: f_00005-10-1 loss: 0.637382  [   64/  146]
train() client id: f_00005-10-2 loss: 0.716603  [   96/  146]
train() client id: f_00005-10-3 loss: 0.408276  [  128/  146]
train() client id: f_00005-11-0 loss: 0.421746  [   32/  146]
train() client id: f_00005-11-1 loss: 0.475973  [   64/  146]
train() client id: f_00005-11-2 loss: 0.594366  [   96/  146]
train() client id: f_00005-11-3 loss: 0.815313  [  128/  146]
train() client id: f_00006-0-0 loss: 0.487771  [   32/   54]
train() client id: f_00006-1-0 loss: 0.534898  [   32/   54]
train() client id: f_00006-2-0 loss: 0.535562  [   32/   54]
train() client id: f_00006-3-0 loss: 0.521646  [   32/   54]
train() client id: f_00006-4-0 loss: 0.528303  [   32/   54]
train() client id: f_00006-5-0 loss: 0.490768  [   32/   54]
train() client id: f_00006-6-0 loss: 0.535002  [   32/   54]
train() client id: f_00006-7-0 loss: 0.425730  [   32/   54]
train() client id: f_00006-8-0 loss: 0.432869  [   32/   54]
train() client id: f_00006-9-0 loss: 0.436842  [   32/   54]
train() client id: f_00006-10-0 loss: 0.483519  [   32/   54]
train() client id: f_00006-11-0 loss: 0.434458  [   32/   54]
train() client id: f_00007-0-0 loss: 0.368160  [   32/  179]
train() client id: f_00007-0-1 loss: 0.299253  [   64/  179]
train() client id: f_00007-0-2 loss: 0.341944  [   96/  179]
train() client id: f_00007-0-3 loss: 0.434276  [  128/  179]
train() client id: f_00007-0-4 loss: 0.308820  [  160/  179]
train() client id: f_00007-1-0 loss: 0.273937  [   32/  179]
train() client id: f_00007-1-1 loss: 0.446779  [   64/  179]
train() client id: f_00007-1-2 loss: 0.495151  [   96/  179]
train() client id: f_00007-1-3 loss: 0.376466  [  128/  179]
train() client id: f_00007-1-4 loss: 0.175833  [  160/  179]
train() client id: f_00007-2-0 loss: 0.316253  [   32/  179]
train() client id: f_00007-2-1 loss: 0.311534  [   64/  179]
train() client id: f_00007-2-2 loss: 0.348737  [   96/  179]
train() client id: f_00007-2-3 loss: 0.483264  [  128/  179]
train() client id: f_00007-2-4 loss: 0.161860  [  160/  179]
train() client id: f_00007-3-0 loss: 0.294539  [   32/  179]
train() client id: f_00007-3-1 loss: 0.238249  [   64/  179]
train() client id: f_00007-3-2 loss: 0.366447  [   96/  179]
train() client id: f_00007-3-3 loss: 0.190498  [  128/  179]
train() client id: f_00007-3-4 loss: 0.333673  [  160/  179]
train() client id: f_00007-4-0 loss: 0.297656  [   32/  179]
train() client id: f_00007-4-1 loss: 0.380479  [   64/  179]
train() client id: f_00007-4-2 loss: 0.405261  [   96/  179]
train() client id: f_00007-4-3 loss: 0.266519  [  128/  179]
train() client id: f_00007-4-4 loss: 0.139444  [  160/  179]
train() client id: f_00007-5-0 loss: 0.310977  [   32/  179]
train() client id: f_00007-5-1 loss: 0.201636  [   64/  179]
train() client id: f_00007-5-2 loss: 0.371550  [   96/  179]
train() client id: f_00007-5-3 loss: 0.271487  [  128/  179]
train() client id: f_00007-5-4 loss: 0.308756  [  160/  179]
train() client id: f_00007-6-0 loss: 0.322038  [   32/  179]
train() client id: f_00007-6-1 loss: 0.179511  [   64/  179]
train() client id: f_00007-6-2 loss: 0.311333  [   96/  179]
train() client id: f_00007-6-3 loss: 0.238361  [  128/  179]
train() client id: f_00007-6-4 loss: 0.278593  [  160/  179]
train() client id: f_00007-7-0 loss: 0.291461  [   32/  179]
train() client id: f_00007-7-1 loss: 0.293734  [   64/  179]
train() client id: f_00007-7-2 loss: 0.383775  [   96/  179]
train() client id: f_00007-7-3 loss: 0.266119  [  128/  179]
train() client id: f_00007-7-4 loss: 0.225840  [  160/  179]
train() client id: f_00007-8-0 loss: 0.108107  [   32/  179]
train() client id: f_00007-8-1 loss: 0.329784  [   64/  179]
train() client id: f_00007-8-2 loss: 0.460131  [   96/  179]
train() client id: f_00007-8-3 loss: 0.230994  [  128/  179]
train() client id: f_00007-8-4 loss: 0.236429  [  160/  179]
train() client id: f_00007-9-0 loss: 0.298597  [   32/  179]
train() client id: f_00007-9-1 loss: 0.231175  [   64/  179]
train() client id: f_00007-9-2 loss: 0.306807  [   96/  179]
train() client id: f_00007-9-3 loss: 0.322089  [  128/  179]
train() client id: f_00007-9-4 loss: 0.204147  [  160/  179]
train() client id: f_00007-10-0 loss: 0.355691  [   32/  179]
train() client id: f_00007-10-1 loss: 0.174085  [   64/  179]
train() client id: f_00007-10-2 loss: 0.290325  [   96/  179]
train() client id: f_00007-10-3 loss: 0.225195  [  128/  179]
train() client id: f_00007-10-4 loss: 0.355900  [  160/  179]
train() client id: f_00007-11-0 loss: 0.357009  [   32/  179]
train() client id: f_00007-11-1 loss: 0.295039  [   64/  179]
train() client id: f_00007-11-2 loss: 0.229478  [   96/  179]
train() client id: f_00007-11-3 loss: 0.118214  [  128/  179]
train() client id: f_00007-11-4 loss: 0.399114  [  160/  179]
train() client id: f_00008-0-0 loss: 0.830805  [   32/  130]
train() client id: f_00008-0-1 loss: 0.685667  [   64/  130]
train() client id: f_00008-0-2 loss: 0.728926  [   96/  130]
train() client id: f_00008-0-3 loss: 0.801580  [  128/  130]
train() client id: f_00008-1-0 loss: 0.819150  [   32/  130]
train() client id: f_00008-1-1 loss: 0.684152  [   64/  130]
train() client id: f_00008-1-2 loss: 0.799695  [   96/  130]
train() client id: f_00008-1-3 loss: 0.704297  [  128/  130]
train() client id: f_00008-2-0 loss: 0.688375  [   32/  130]
train() client id: f_00008-2-1 loss: 0.784161  [   64/  130]
train() client id: f_00008-2-2 loss: 0.812338  [   96/  130]
train() client id: f_00008-2-3 loss: 0.731068  [  128/  130]
train() client id: f_00008-3-0 loss: 0.729782  [   32/  130]
train() client id: f_00008-3-1 loss: 0.885957  [   64/  130]
train() client id: f_00008-3-2 loss: 0.754715  [   96/  130]
train() client id: f_00008-3-3 loss: 0.679497  [  128/  130]
train() client id: f_00008-4-0 loss: 0.789169  [   32/  130]
train() client id: f_00008-4-1 loss: 0.608348  [   64/  130]
train() client id: f_00008-4-2 loss: 0.854417  [   96/  130]
train() client id: f_00008-4-3 loss: 0.762182  [  128/  130]
train() client id: f_00008-5-0 loss: 0.628657  [   32/  130]
train() client id: f_00008-5-1 loss: 0.814172  [   64/  130]
train() client id: f_00008-5-2 loss: 0.795739  [   96/  130]
train() client id: f_00008-5-3 loss: 0.771214  [  128/  130]
train() client id: f_00008-6-0 loss: 0.819081  [   32/  130]
train() client id: f_00008-6-1 loss: 0.752542  [   64/  130]
train() client id: f_00008-6-2 loss: 0.669498  [   96/  130]
train() client id: f_00008-6-3 loss: 0.762803  [  128/  130]
train() client id: f_00008-7-0 loss: 0.795318  [   32/  130]
train() client id: f_00008-7-1 loss: 0.664100  [   64/  130]
train() client id: f_00008-7-2 loss: 0.792287  [   96/  130]
train() client id: f_00008-7-3 loss: 0.757406  [  128/  130]
train() client id: f_00008-8-0 loss: 0.759013  [   32/  130]
train() client id: f_00008-8-1 loss: 0.669880  [   64/  130]
train() client id: f_00008-8-2 loss: 0.766949  [   96/  130]
train() client id: f_00008-8-3 loss: 0.804441  [  128/  130]
train() client id: f_00008-9-0 loss: 0.795660  [   32/  130]
train() client id: f_00008-9-1 loss: 0.635214  [   64/  130]
train() client id: f_00008-9-2 loss: 0.731625  [   96/  130]
train() client id: f_00008-9-3 loss: 0.873865  [  128/  130]
train() client id: f_00008-10-0 loss: 0.756724  [   32/  130]
train() client id: f_00008-10-1 loss: 0.827504  [   64/  130]
train() client id: f_00008-10-2 loss: 0.771465  [   96/  130]
train() client id: f_00008-10-3 loss: 0.667111  [  128/  130]
train() client id: f_00008-11-0 loss: 0.844344  [   32/  130]
train() client id: f_00008-11-1 loss: 0.784704  [   64/  130]
train() client id: f_00008-11-2 loss: 0.739528  [   96/  130]
train() client id: f_00008-11-3 loss: 0.663901  [  128/  130]
train() client id: f_00009-0-0 loss: 1.010890  [   32/  118]
train() client id: f_00009-0-1 loss: 1.097828  [   64/  118]
train() client id: f_00009-0-2 loss: 0.971526  [   96/  118]
train() client id: f_00009-1-0 loss: 0.998144  [   32/  118]
train() client id: f_00009-1-1 loss: 1.023067  [   64/  118]
train() client id: f_00009-1-2 loss: 0.962522  [   96/  118]
train() client id: f_00009-2-0 loss: 1.034646  [   32/  118]
train() client id: f_00009-2-1 loss: 1.005900  [   64/  118]
train() client id: f_00009-2-2 loss: 0.877157  [   96/  118]
train() client id: f_00009-3-0 loss: 0.957891  [   32/  118]
train() client id: f_00009-3-1 loss: 0.763621  [   64/  118]
train() client id: f_00009-3-2 loss: 0.982108  [   96/  118]
train() client id: f_00009-4-0 loss: 0.987677  [   32/  118]
train() client id: f_00009-4-1 loss: 0.788969  [   64/  118]
train() client id: f_00009-4-2 loss: 0.905235  [   96/  118]
train() client id: f_00009-5-0 loss: 0.719495  [   32/  118]
train() client id: f_00009-5-1 loss: 0.954550  [   64/  118]
train() client id: f_00009-5-2 loss: 0.881388  [   96/  118]
train() client id: f_00009-6-0 loss: 0.762844  [   32/  118]
train() client id: f_00009-6-1 loss: 0.803080  [   64/  118]
train() client id: f_00009-6-2 loss: 0.893260  [   96/  118]
train() client id: f_00009-7-0 loss: 0.823872  [   32/  118]
train() client id: f_00009-7-1 loss: 0.878249  [   64/  118]
train() client id: f_00009-7-2 loss: 0.750998  [   96/  118]
train() client id: f_00009-8-0 loss: 0.796878  [   32/  118]
train() client id: f_00009-8-1 loss: 0.921417  [   64/  118]
train() client id: f_00009-8-2 loss: 0.699464  [   96/  118]
train() client id: f_00009-9-0 loss: 0.777986  [   32/  118]
train() client id: f_00009-9-1 loss: 0.988814  [   64/  118]
train() client id: f_00009-9-2 loss: 0.821112  [   96/  118]
train() client id: f_00009-10-0 loss: 0.926391  [   32/  118]
train() client id: f_00009-10-1 loss: 0.731145  [   64/  118]
train() client id: f_00009-10-2 loss: 0.764050  [   96/  118]
train() client id: f_00009-11-0 loss: 0.846015  [   32/  118]
train() client id: f_00009-11-1 loss: 0.757122  [   64/  118]
train() client id: f_00009-11-2 loss: 0.759466  [   96/  118]
At round 27 accuracy: 0.649867374005305
At round 27 training accuracy: 0.5814889336016097
At round 27 training loss: 0.8403231757351737
update_location
xs = [  -3.9056584     4.20031788  155.00902392   18.81129433    0.97929623
    3.95640986 -117.44319194  -96.32485185  139.66397685  -82.06087855]
ys = [ 147.5879595   130.55583871    1.32061395 -117.45517586  109.35018685
   92.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [178.31842293 164.50674664 184.47097744 155.40136142 148.18374534
 136.49219669 154.27181815 138.84939085 172.66932676 129.42178968]
dists_bs = [173.27635064 184.62945973 372.92155342 350.84590306 187.82601169
 196.92615481 186.71749037 191.12203797 351.84930892 194.62315774]
uav_gains = [2.33299769e-11 2.87101678e-11 2.13317186e-11 3.31618763e-11
 3.73775877e-11 4.59306330e-11 3.37773315e-11 4.40026087e-11
 2.53630547e-11 5.24710704e-11]
bs_gains = [5.95420887e-11 4.98484075e-11 6.96249872e-12 8.25977351e-12
 4.75092334e-11 4.16144887e-11 4.83032198e-11 4.52505576e-11
 8.19398791e-12 4.30080195e-11]
Round 28
-------------------------------
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.2963697  15.14398805  7.18662324  2.58300247 17.46743421  8.40944175
  3.20540132 10.27679841  7.57595044  6.8226403 ]
obj_prev = 85.96764987436393
eta_min = 1.97492431522131e-13	eta_max = 0.9251230340770827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 19.96901274869134	eta = 0.909090909090909
af = 18.15364795335576	bf = 1.507335053902618	zeta = 35.613187548855024	eta = 0.509745102946827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 28.02638083867362	eta = 0.6477342921247089
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.65886868799346	eta = 0.6809609277055236
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.58884556407932	eta = 0.682754274141212
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.588647822049044	eta = 0.6827593518426902
eta = 0.6827593518426902
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.03148978 0.0662285  0.03098994 0.01074651 0.07647517 0.03648815
 0.01349562 0.04473547 0.03248945 0.02949042]
ene_total = [2.34522213 4.30343838 2.32693545 1.09048608 4.90813228 2.57679446
 1.24971036 3.05496696 2.56930912 2.1636526 ]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 0 obj = 4.772905437718752
eta = 0.6827593518426902
freqs = [39094714.05548728 79606171.90046072 38654407.63725649 13124341.80140644
 92081472.39087442 44152552.9427729  16468811.85493763 54017423.64815775
 43377597.092848   35640012.17367139]
eta_min = 0.6827593518432681	eta_max = 0.6827593518426932
af = 0.017883468190837512	bf = 1.507335053902618	zeta = 0.019671815009921264	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [3.00512114e-06 2.62056364e-05 2.89117971e-06 1.15578985e-07
 4.04875310e-05 4.44139964e-06 2.28546015e-07 8.15034742e-06
 3.81706619e-06 2.33890753e-06]
ene_total = [1.74481114 1.47881446 1.78319283 1.60772537 1.49641265 1.53101993
 1.60118017 1.51381715 2.32252357 1.51993757]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 1 obj = 4.772905437718796
eta = 0.6827593518426932
freqs = [39094714.05548728 79606171.9004607  38654407.6372565  13124341.80140644
 92081472.39087439 44152552.94277289 16468811.85493763 54017423.64815772
 43377597.09284804 35640012.17367138]
Done!
At round 28 energy consumption: 4.772905437718752
At round 28 eta: 0.6827593518426932
At round 28 local rounds: 12.495942598469853
At round 28 global rounds: 58.60320626975522
At round 28 a_n: 18.248773294144172
gradient difference: 0.49678725004196167
train() client id: f_00000-0-0 loss: 1.290884  [   32/  126]
train() client id: f_00000-0-1 loss: 1.397653  [   64/  126]
train() client id: f_00000-0-2 loss: 1.146062  [   96/  126]
train() client id: f_00000-1-0 loss: 1.112135  [   32/  126]
train() client id: f_00000-1-1 loss: 1.120546  [   64/  126]
train() client id: f_00000-1-2 loss: 1.018474  [   96/  126]
train() client id: f_00000-2-0 loss: 0.961204  [   32/  126]
train() client id: f_00000-2-1 loss: 1.043586  [   64/  126]
train() client id: f_00000-2-2 loss: 1.000902  [   96/  126]
train() client id: f_00000-3-0 loss: 1.046592  [   32/  126]
train() client id: f_00000-3-1 loss: 1.036410  [   64/  126]
train() client id: f_00000-3-2 loss: 0.893938  [   96/  126]
train() client id: f_00000-4-0 loss: 0.898725  [   32/  126]
train() client id: f_00000-4-1 loss: 0.959980  [   64/  126]
train() client id: f_00000-4-2 loss: 0.978641  [   96/  126]
train() client id: f_00000-5-0 loss: 1.000257  [   32/  126]
train() client id: f_00000-5-1 loss: 0.727993  [   64/  126]
train() client id: f_00000-5-2 loss: 0.874824  [   96/  126]
train() client id: f_00000-6-0 loss: 0.929840  [   32/  126]
train() client id: f_00000-6-1 loss: 0.895313  [   64/  126]
train() client id: f_00000-6-2 loss: 0.878723  [   96/  126]
train() client id: f_00000-7-0 loss: 0.890444  [   32/  126]
train() client id: f_00000-7-1 loss: 0.880020  [   64/  126]
train() client id: f_00000-7-2 loss: 0.789794  [   96/  126]
train() client id: f_00000-8-0 loss: 0.793883  [   32/  126]
train() client id: f_00000-8-1 loss: 0.871681  [   64/  126]
train() client id: f_00000-8-2 loss: 0.878748  [   96/  126]
train() client id: f_00000-9-0 loss: 0.796957  [   32/  126]
train() client id: f_00000-9-1 loss: 0.903963  [   64/  126]
train() client id: f_00000-9-2 loss: 0.831843  [   96/  126]
train() client id: f_00000-10-0 loss: 0.866179  [   32/  126]
train() client id: f_00000-10-1 loss: 0.746439  [   64/  126]
train() client id: f_00000-10-2 loss: 0.880176  [   96/  126]
train() client id: f_00000-11-0 loss: 1.011147  [   32/  126]
train() client id: f_00000-11-1 loss: 0.696087  [   64/  126]
train() client id: f_00000-11-2 loss: 0.736855  [   96/  126]
train() client id: f_00001-0-0 loss: 0.364722  [   32/  265]
train() client id: f_00001-0-1 loss: 0.435558  [   64/  265]
train() client id: f_00001-0-2 loss: 0.350088  [   96/  265]
train() client id: f_00001-0-3 loss: 0.548641  [  128/  265]
train() client id: f_00001-0-4 loss: 0.392873  [  160/  265]
train() client id: f_00001-0-5 loss: 0.338510  [  192/  265]
train() client id: f_00001-0-6 loss: 0.320904  [  224/  265]
train() client id: f_00001-0-7 loss: 0.393996  [  256/  265]
train() client id: f_00001-1-0 loss: 0.386300  [   32/  265]
train() client id: f_00001-1-1 loss: 0.372924  [   64/  265]
train() client id: f_00001-1-2 loss: 0.444209  [   96/  265]
train() client id: f_00001-1-3 loss: 0.398333  [  128/  265]
train() client id: f_00001-1-4 loss: 0.358181  [  160/  265]
train() client id: f_00001-1-5 loss: 0.366013  [  192/  265]
train() client id: f_00001-1-6 loss: 0.321853  [  224/  265]
train() client id: f_00001-1-7 loss: 0.354740  [  256/  265]
train() client id: f_00001-2-0 loss: 0.411817  [   32/  265]
train() client id: f_00001-2-1 loss: 0.330523  [   64/  265]
train() client id: f_00001-2-2 loss: 0.326234  [   96/  265]
train() client id: f_00001-2-3 loss: 0.477869  [  128/  265]
train() client id: f_00001-2-4 loss: 0.266474  [  160/  265]
train() client id: f_00001-2-5 loss: 0.383265  [  192/  265]
train() client id: f_00001-2-6 loss: 0.427205  [  224/  265]
train() client id: f_00001-2-7 loss: 0.287400  [  256/  265]
train() client id: f_00001-3-0 loss: 0.317290  [   32/  265]
train() client id: f_00001-3-1 loss: 0.267006  [   64/  265]
train() client id: f_00001-3-2 loss: 0.354181  [   96/  265]
train() client id: f_00001-3-3 loss: 0.457831  [  128/  265]
train() client id: f_00001-3-4 loss: 0.483569  [  160/  265]
train() client id: f_00001-3-5 loss: 0.335162  [  192/  265]
train() client id: f_00001-3-6 loss: 0.292231  [  224/  265]
train() client id: f_00001-3-7 loss: 0.408247  [  256/  265]
train() client id: f_00001-4-0 loss: 0.462292  [   32/  265]
train() client id: f_00001-4-1 loss: 0.364183  [   64/  265]
train() client id: f_00001-4-2 loss: 0.401958  [   96/  265]
train() client id: f_00001-4-3 loss: 0.322440  [  128/  265]
train() client id: f_00001-4-4 loss: 0.426065  [  160/  265]
train() client id: f_00001-4-5 loss: 0.313596  [  192/  265]
train() client id: f_00001-4-6 loss: 0.273847  [  224/  265]
train() client id: f_00001-4-7 loss: 0.292980  [  256/  265]
train() client id: f_00001-5-0 loss: 0.385887  [   32/  265]
train() client id: f_00001-5-1 loss: 0.347775  [   64/  265]
train() client id: f_00001-5-2 loss: 0.290792  [   96/  265]
train() client id: f_00001-5-3 loss: 0.337481  [  128/  265]
train() client id: f_00001-5-4 loss: 0.273677  [  160/  265]
train() client id: f_00001-5-5 loss: 0.428714  [  192/  265]
train() client id: f_00001-5-6 loss: 0.457571  [  224/  265]
train() client id: f_00001-5-7 loss: 0.308594  [  256/  265]
train() client id: f_00001-6-0 loss: 0.377359  [   32/  265]
train() client id: f_00001-6-1 loss: 0.239963  [   64/  265]
train() client id: f_00001-6-2 loss: 0.407646  [   96/  265]
train() client id: f_00001-6-3 loss: 0.412046  [  128/  265]
train() client id: f_00001-6-4 loss: 0.331102  [  160/  265]
train() client id: f_00001-6-5 loss: 0.302358  [  192/  265]
train() client id: f_00001-6-6 loss: 0.392067  [  224/  265]
train() client id: f_00001-6-7 loss: 0.331964  [  256/  265]
train() client id: f_00001-7-0 loss: 0.403228  [   32/  265]
train() client id: f_00001-7-1 loss: 0.340590  [   64/  265]
train() client id: f_00001-7-2 loss: 0.311400  [   96/  265]
train() client id: f_00001-7-3 loss: 0.275260  [  128/  265]
train() client id: f_00001-7-4 loss: 0.419524  [  160/  265]
train() client id: f_00001-7-5 loss: 0.323260  [  192/  265]
train() client id: f_00001-7-6 loss: 0.319255  [  224/  265]
train() client id: f_00001-7-7 loss: 0.383507  [  256/  265]
train() client id: f_00001-8-0 loss: 0.250037  [   32/  265]
train() client id: f_00001-8-1 loss: 0.247557  [   64/  265]
train() client id: f_00001-8-2 loss: 0.301622  [   96/  265]
train() client id: f_00001-8-3 loss: 0.298608  [  128/  265]
train() client id: f_00001-8-4 loss: 0.458081  [  160/  265]
train() client id: f_00001-8-5 loss: 0.403765  [  192/  265]
train() client id: f_00001-8-6 loss: 0.274177  [  224/  265]
train() client id: f_00001-8-7 loss: 0.396427  [  256/  265]
train() client id: f_00001-9-0 loss: 0.306767  [   32/  265]
train() client id: f_00001-9-1 loss: 0.409761  [   64/  265]
train() client id: f_00001-9-2 loss: 0.233533  [   96/  265]
train() client id: f_00001-9-3 loss: 0.313294  [  128/  265]
train() client id: f_00001-9-4 loss: 0.368071  [  160/  265]
train() client id: f_00001-9-5 loss: 0.440103  [  192/  265]
train() client id: f_00001-9-6 loss: 0.387487  [  224/  265]
train() client id: f_00001-9-7 loss: 0.268708  [  256/  265]
train() client id: f_00001-10-0 loss: 0.300141  [   32/  265]
train() client id: f_00001-10-1 loss: 0.301414  [   64/  265]
train() client id: f_00001-10-2 loss: 0.445058  [   96/  265]
train() client id: f_00001-10-3 loss: 0.391661  [  128/  265]
train() client id: f_00001-10-4 loss: 0.355193  [  160/  265]
train() client id: f_00001-10-5 loss: 0.291722  [  192/  265]
train() client id: f_00001-10-6 loss: 0.288974  [  224/  265]
train() client id: f_00001-10-7 loss: 0.267504  [  256/  265]
train() client id: f_00001-11-0 loss: 0.317481  [   32/  265]
train() client id: f_00001-11-1 loss: 0.241957  [   64/  265]
train() client id: f_00001-11-2 loss: 0.339024  [   96/  265]
train() client id: f_00001-11-3 loss: 0.375126  [  128/  265]
train() client id: f_00001-11-4 loss: 0.293325  [  160/  265]
train() client id: f_00001-11-5 loss: 0.303444  [  192/  265]
train() client id: f_00001-11-6 loss: 0.310328  [  224/  265]
train() client id: f_00001-11-7 loss: 0.456874  [  256/  265]
train() client id: f_00002-0-0 loss: 1.326406  [   32/  124]
train() client id: f_00002-0-1 loss: 1.134374  [   64/  124]
train() client id: f_00002-0-2 loss: 1.102992  [   96/  124]
train() client id: f_00002-1-0 loss: 1.213877  [   32/  124]
train() client id: f_00002-1-1 loss: 1.063928  [   64/  124]
train() client id: f_00002-1-2 loss: 1.192541  [   96/  124]
train() client id: f_00002-2-0 loss: 1.126399  [   32/  124]
train() client id: f_00002-2-1 loss: 1.223840  [   64/  124]
train() client id: f_00002-2-2 loss: 1.141299  [   96/  124]
train() client id: f_00002-3-0 loss: 1.174878  [   32/  124]
train() client id: f_00002-3-1 loss: 1.120258  [   64/  124]
train() client id: f_00002-3-2 loss: 1.132332  [   96/  124]
train() client id: f_00002-4-0 loss: 1.267138  [   32/  124]
train() client id: f_00002-4-1 loss: 1.119953  [   64/  124]
train() client id: f_00002-4-2 loss: 1.058159  [   96/  124]
train() client id: f_00002-5-0 loss: 1.137888  [   32/  124]
train() client id: f_00002-5-1 loss: 1.096133  [   64/  124]
train() client id: f_00002-5-2 loss: 1.024681  [   96/  124]
train() client id: f_00002-6-0 loss: 1.110015  [   32/  124]
train() client id: f_00002-6-1 loss: 1.178794  [   64/  124]
train() client id: f_00002-6-2 loss: 0.977060  [   96/  124]
train() client id: f_00002-7-0 loss: 1.089097  [   32/  124]
train() client id: f_00002-7-1 loss: 1.143146  [   64/  124]
train() client id: f_00002-7-2 loss: 0.963481  [   96/  124]
train() client id: f_00002-8-0 loss: 0.945511  [   32/  124]
train() client id: f_00002-8-1 loss: 1.122318  [   64/  124]
train() client id: f_00002-8-2 loss: 1.217426  [   96/  124]
train() client id: f_00002-9-0 loss: 1.004014  [   32/  124]
train() client id: f_00002-9-1 loss: 0.996690  [   64/  124]
train() client id: f_00002-9-2 loss: 1.171571  [   96/  124]
train() client id: f_00002-10-0 loss: 0.971683  [   32/  124]
train() client id: f_00002-10-1 loss: 1.144366  [   64/  124]
train() client id: f_00002-10-2 loss: 1.099918  [   96/  124]
train() client id: f_00002-11-0 loss: 1.023306  [   32/  124]
train() client id: f_00002-11-1 loss: 1.028389  [   64/  124]
train() client id: f_00002-11-2 loss: 1.124935  [   96/  124]
train() client id: f_00003-0-0 loss: 0.645885  [   32/   43]
train() client id: f_00003-1-0 loss: 0.699989  [   32/   43]
train() client id: f_00003-2-0 loss: 0.749645  [   32/   43]
train() client id: f_00003-3-0 loss: 0.645955  [   32/   43]
train() client id: f_00003-4-0 loss: 0.642940  [   32/   43]
train() client id: f_00003-5-0 loss: 0.593280  [   32/   43]
train() client id: f_00003-6-0 loss: 0.614460  [   32/   43]
train() client id: f_00003-7-0 loss: 0.732987  [   32/   43]
train() client id: f_00003-8-0 loss: 0.581285  [   32/   43]
train() client id: f_00003-9-0 loss: 0.589335  [   32/   43]
train() client id: f_00003-10-0 loss: 0.630392  [   32/   43]
train() client id: f_00003-11-0 loss: 0.634138  [   32/   43]
train() client id: f_00004-0-0 loss: 0.970111  [   32/  306]
train() client id: f_00004-0-1 loss: 1.014699  [   64/  306]
train() client id: f_00004-0-2 loss: 1.125660  [   96/  306]
train() client id: f_00004-0-3 loss: 0.993305  [  128/  306]
train() client id: f_00004-0-4 loss: 1.163458  [  160/  306]
train() client id: f_00004-0-5 loss: 0.921768  [  192/  306]
train() client id: f_00004-0-6 loss: 1.166758  [  224/  306]
train() client id: f_00004-0-7 loss: 0.969396  [  256/  306]
train() client id: f_00004-0-8 loss: 1.313534  [  288/  306]
train() client id: f_00004-1-0 loss: 1.018686  [   32/  306]
train() client id: f_00004-1-1 loss: 1.106207  [   64/  306]
train() client id: f_00004-1-2 loss: 1.016660  [   96/  306]
train() client id: f_00004-1-3 loss: 1.099627  [  128/  306]
train() client id: f_00004-1-4 loss: 1.067404  [  160/  306]
train() client id: f_00004-1-5 loss: 1.110426  [  192/  306]
train() client id: f_00004-1-6 loss: 1.013509  [  224/  306]
train() client id: f_00004-1-7 loss: 1.069603  [  256/  306]
train() client id: f_00004-1-8 loss: 1.051932  [  288/  306]
train() client id: f_00004-2-0 loss: 1.046286  [   32/  306]
train() client id: f_00004-2-1 loss: 1.073753  [   64/  306]
train() client id: f_00004-2-2 loss: 1.037363  [   96/  306]
train() client id: f_00004-2-3 loss: 1.084409  [  128/  306]
train() client id: f_00004-2-4 loss: 1.144521  [  160/  306]
train() client id: f_00004-2-5 loss: 1.013882  [  192/  306]
train() client id: f_00004-2-6 loss: 1.136076  [  224/  306]
train() client id: f_00004-2-7 loss: 1.073265  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939164  [  288/  306]
train() client id: f_00004-3-0 loss: 1.050190  [   32/  306]
train() client id: f_00004-3-1 loss: 1.119224  [   64/  306]
train() client id: f_00004-3-2 loss: 1.129830  [   96/  306]
train() client id: f_00004-3-3 loss: 1.096265  [  128/  306]
train() client id: f_00004-3-4 loss: 1.047769  [  160/  306]
train() client id: f_00004-3-5 loss: 0.993378  [  192/  306]
train() client id: f_00004-3-6 loss: 1.155404  [  224/  306]
train() client id: f_00004-3-7 loss: 1.049828  [  256/  306]
train() client id: f_00004-3-8 loss: 0.943446  [  288/  306]
train() client id: f_00004-4-0 loss: 0.961354  [   32/  306]
train() client id: f_00004-4-1 loss: 1.125482  [   64/  306]
train() client id: f_00004-4-2 loss: 1.068615  [   96/  306]
train() client id: f_00004-4-3 loss: 1.047918  [  128/  306]
train() client id: f_00004-4-4 loss: 1.141849  [  160/  306]
train() client id: f_00004-4-5 loss: 0.993431  [  192/  306]
train() client id: f_00004-4-6 loss: 1.097788  [  224/  306]
train() client id: f_00004-4-7 loss: 1.048097  [  256/  306]
train() client id: f_00004-4-8 loss: 0.956797  [  288/  306]
train() client id: f_00004-5-0 loss: 0.973516  [   32/  306]
train() client id: f_00004-5-1 loss: 1.018707  [   64/  306]
train() client id: f_00004-5-2 loss: 0.956562  [   96/  306]
train() client id: f_00004-5-3 loss: 1.021949  [  128/  306]
train() client id: f_00004-5-4 loss: 1.037198  [  160/  306]
train() client id: f_00004-5-5 loss: 1.067928  [  192/  306]
train() client id: f_00004-5-6 loss: 1.050926  [  224/  306]
train() client id: f_00004-5-7 loss: 1.061063  [  256/  306]
train() client id: f_00004-5-8 loss: 1.283686  [  288/  306]
train() client id: f_00004-6-0 loss: 1.018734  [   32/  306]
train() client id: f_00004-6-1 loss: 0.958094  [   64/  306]
train() client id: f_00004-6-2 loss: 0.994372  [   96/  306]
train() client id: f_00004-6-3 loss: 1.073452  [  128/  306]
train() client id: f_00004-6-4 loss: 1.062974  [  160/  306]
train() client id: f_00004-6-5 loss: 1.045679  [  192/  306]
train() client id: f_00004-6-6 loss: 1.147088  [  224/  306]
train() client id: f_00004-6-7 loss: 1.099211  [  256/  306]
train() client id: f_00004-6-8 loss: 0.998460  [  288/  306]
train() client id: f_00004-7-0 loss: 1.093724  [   32/  306]
train() client id: f_00004-7-1 loss: 0.918900  [   64/  306]
train() client id: f_00004-7-2 loss: 0.963893  [   96/  306]
train() client id: f_00004-7-3 loss: 1.180677  [  128/  306]
train() client id: f_00004-7-4 loss: 1.086163  [  160/  306]
train() client id: f_00004-7-5 loss: 1.046531  [  192/  306]
train() client id: f_00004-7-6 loss: 0.987612  [  224/  306]
train() client id: f_00004-7-7 loss: 1.113833  [  256/  306]
train() client id: f_00004-7-8 loss: 0.982569  [  288/  306]
train() client id: f_00004-8-0 loss: 1.061250  [   32/  306]
train() client id: f_00004-8-1 loss: 1.095996  [   64/  306]
train() client id: f_00004-8-2 loss: 1.026215  [   96/  306]
train() client id: f_00004-8-3 loss: 0.965588  [  128/  306]
train() client id: f_00004-8-4 loss: 1.037176  [  160/  306]
train() client id: f_00004-8-5 loss: 1.108786  [  192/  306]
train() client id: f_00004-8-6 loss: 1.006650  [  224/  306]
train() client id: f_00004-8-7 loss: 1.041983  [  256/  306]
train() client id: f_00004-8-8 loss: 0.952285  [  288/  306]
train() client id: f_00004-9-0 loss: 0.975287  [   32/  306]
train() client id: f_00004-9-1 loss: 0.976426  [   64/  306]
train() client id: f_00004-9-2 loss: 0.988080  [   96/  306]
train() client id: f_00004-9-3 loss: 0.974525  [  128/  306]
train() client id: f_00004-9-4 loss: 1.108011  [  160/  306]
train() client id: f_00004-9-5 loss: 1.047987  [  192/  306]
train() client id: f_00004-9-6 loss: 1.024068  [  224/  306]
train() client id: f_00004-9-7 loss: 1.009205  [  256/  306]
train() client id: f_00004-9-8 loss: 1.233641  [  288/  306]
train() client id: f_00004-10-0 loss: 0.953181  [   32/  306]
train() client id: f_00004-10-1 loss: 1.157472  [   64/  306]
train() client id: f_00004-10-2 loss: 1.151247  [   96/  306]
train() client id: f_00004-10-3 loss: 1.002094  [  128/  306]
train() client id: f_00004-10-4 loss: 1.076009  [  160/  306]
train() client id: f_00004-10-5 loss: 1.071051  [  192/  306]
train() client id: f_00004-10-6 loss: 0.929442  [  224/  306]
train() client id: f_00004-10-7 loss: 1.085133  [  256/  306]
train() client id: f_00004-10-8 loss: 0.921462  [  288/  306]
train() client id: f_00004-11-0 loss: 1.068840  [   32/  306]
train() client id: f_00004-11-1 loss: 0.978189  [   64/  306]
train() client id: f_00004-11-2 loss: 0.994789  [   96/  306]
train() client id: f_00004-11-3 loss: 0.994873  [  128/  306]
train() client id: f_00004-11-4 loss: 1.045125  [  160/  306]
train() client id: f_00004-11-5 loss: 0.959475  [  192/  306]
train() client id: f_00004-11-6 loss: 1.003899  [  224/  306]
train() client id: f_00004-11-7 loss: 1.079376  [  256/  306]
train() client id: f_00004-11-8 loss: 1.059976  [  288/  306]
train() client id: f_00005-0-0 loss: 0.302887  [   32/  146]
train() client id: f_00005-0-1 loss: 0.704426  [   64/  146]
train() client id: f_00005-0-2 loss: 0.376707  [   96/  146]
train() client id: f_00005-0-3 loss: 0.347591  [  128/  146]
train() client id: f_00005-1-0 loss: 0.556341  [   32/  146]
train() client id: f_00005-1-1 loss: 0.457168  [   64/  146]
train() client id: f_00005-1-2 loss: 0.640197  [   96/  146]
train() client id: f_00005-1-3 loss: 0.209214  [  128/  146]
train() client id: f_00005-2-0 loss: 0.655002  [   32/  146]
train() client id: f_00005-2-1 loss: 0.461873  [   64/  146]
train() client id: f_00005-2-2 loss: 0.407531  [   96/  146]
train() client id: f_00005-2-3 loss: 0.501791  [  128/  146]
train() client id: f_00005-3-0 loss: 0.539003  [   32/  146]
train() client id: f_00005-3-1 loss: 0.400109  [   64/  146]
train() client id: f_00005-3-2 loss: 0.525037  [   96/  146]
train() client id: f_00005-3-3 loss: 0.323809  [  128/  146]
train() client id: f_00005-4-0 loss: 0.476154  [   32/  146]
train() client id: f_00005-4-1 loss: 0.319309  [   64/  146]
train() client id: f_00005-4-2 loss: 0.480006  [   96/  146]
train() client id: f_00005-4-3 loss: 0.496621  [  128/  146]
train() client id: f_00005-5-0 loss: 0.530388  [   32/  146]
train() client id: f_00005-5-1 loss: 0.265241  [   64/  146]
train() client id: f_00005-5-2 loss: 0.425721  [   96/  146]
train() client id: f_00005-5-3 loss: 0.632853  [  128/  146]
train() client id: f_00005-6-0 loss: 0.393618  [   32/  146]
train() client id: f_00005-6-1 loss: 0.423213  [   64/  146]
train() client id: f_00005-6-2 loss: 0.350406  [   96/  146]
train() client id: f_00005-6-3 loss: 0.763533  [  128/  146]
train() client id: f_00005-7-0 loss: 0.423749  [   32/  146]
train() client id: f_00005-7-1 loss: 0.553377  [   64/  146]
train() client id: f_00005-7-2 loss: 0.530911  [   96/  146]
train() client id: f_00005-7-3 loss: 0.401633  [  128/  146]
train() client id: f_00005-8-0 loss: 0.284146  [   32/  146]
train() client id: f_00005-8-1 loss: 0.768579  [   64/  146]
train() client id: f_00005-8-2 loss: 0.457297  [   96/  146]
train() client id: f_00005-8-3 loss: 0.365942  [  128/  146]
train() client id: f_00005-9-0 loss: 0.364362  [   32/  146]
train() client id: f_00005-9-1 loss: 0.577321  [   64/  146]
train() client id: f_00005-9-2 loss: 0.765460  [   96/  146]
train() client id: f_00005-9-3 loss: 0.319694  [  128/  146]
train() client id: f_00005-10-0 loss: 0.335460  [   32/  146]
train() client id: f_00005-10-1 loss: 0.463537  [   64/  146]
train() client id: f_00005-10-2 loss: 0.597626  [   96/  146]
train() client id: f_00005-10-3 loss: 0.523353  [  128/  146]
train() client id: f_00005-11-0 loss: 0.529909  [   32/  146]
train() client id: f_00005-11-1 loss: 0.355928  [   64/  146]
train() client id: f_00005-11-2 loss: 0.446540  [   96/  146]
train() client id: f_00005-11-3 loss: 0.541452  [  128/  146]
train() client id: f_00006-0-0 loss: 0.583857  [   32/   54]
train() client id: f_00006-1-0 loss: 0.617186  [   32/   54]
train() client id: f_00006-2-0 loss: 0.530413  [   32/   54]
train() client id: f_00006-3-0 loss: 0.603065  [   32/   54]
train() client id: f_00006-4-0 loss: 0.580002  [   32/   54]
train() client id: f_00006-5-0 loss: 0.558118  [   32/   54]
train() client id: f_00006-6-0 loss: 0.625422  [   32/   54]
train() client id: f_00006-7-0 loss: 0.643086  [   32/   54]
train() client id: f_00006-8-0 loss: 0.591500  [   32/   54]
train() client id: f_00006-9-0 loss: 0.625468  [   32/   54]
train() client id: f_00006-10-0 loss: 0.583685  [   32/   54]
train() client id: f_00006-11-0 loss: 0.640911  [   32/   54]
train() client id: f_00007-0-0 loss: 0.454096  [   32/  179]
train() client id: f_00007-0-1 loss: 0.366242  [   64/  179]
train() client id: f_00007-0-2 loss: 0.522152  [   96/  179]
train() client id: f_00007-0-3 loss: 0.428593  [  128/  179]
train() client id: f_00007-0-4 loss: 0.623052  [  160/  179]
train() client id: f_00007-1-0 loss: 0.505704  [   32/  179]
train() client id: f_00007-1-1 loss: 0.598477  [   64/  179]
train() client id: f_00007-1-2 loss: 0.502136  [   96/  179]
train() client id: f_00007-1-3 loss: 0.496968  [  128/  179]
train() client id: f_00007-1-4 loss: 0.367942  [  160/  179]
train() client id: f_00007-2-0 loss: 0.497589  [   32/  179]
train() client id: f_00007-2-1 loss: 0.388077  [   64/  179]
train() client id: f_00007-2-2 loss: 0.368167  [   96/  179]
train() client id: f_00007-2-3 loss: 0.425916  [  128/  179]
train() client id: f_00007-2-4 loss: 0.557342  [  160/  179]
train() client id: f_00007-3-0 loss: 0.464855  [   32/  179]
train() client id: f_00007-3-1 loss: 0.436987  [   64/  179]
train() client id: f_00007-3-2 loss: 0.472005  [   96/  179]
train() client id: f_00007-3-3 loss: 0.420201  [  128/  179]
train() client id: f_00007-3-4 loss: 0.503440  [  160/  179]
train() client id: f_00007-4-0 loss: 0.517674  [   32/  179]
train() client id: f_00007-4-1 loss: 0.539860  [   64/  179]
train() client id: f_00007-4-2 loss: 0.493551  [   96/  179]
train() client id: f_00007-4-3 loss: 0.337291  [  128/  179]
train() client id: f_00007-4-4 loss: 0.499471  [  160/  179]
train() client id: f_00007-5-0 loss: 0.415337  [   32/  179]
train() client id: f_00007-5-1 loss: 0.344769  [   64/  179]
train() client id: f_00007-5-2 loss: 0.545361  [   96/  179]
train() client id: f_00007-5-3 loss: 0.390736  [  128/  179]
train() client id: f_00007-5-4 loss: 0.704782  [  160/  179]
train() client id: f_00007-6-0 loss: 0.416475  [   32/  179]
train() client id: f_00007-6-1 loss: 0.396757  [   64/  179]
train() client id: f_00007-6-2 loss: 0.593372  [   96/  179]
train() client id: f_00007-6-3 loss: 0.563692  [  128/  179]
train() client id: f_00007-6-4 loss: 0.444634  [  160/  179]
train() client id: f_00007-7-0 loss: 0.402271  [   32/  179]
train() client id: f_00007-7-1 loss: 0.397453  [   64/  179]
train() client id: f_00007-7-2 loss: 0.605493  [   96/  179]
train() client id: f_00007-7-3 loss: 0.339473  [  128/  179]
train() client id: f_00007-7-4 loss: 0.455867  [  160/  179]
train() client id: f_00007-8-0 loss: 0.476008  [   32/  179]
train() client id: f_00007-8-1 loss: 0.386383  [   64/  179]
train() client id: f_00007-8-2 loss: 0.465157  [   96/  179]
train() client id: f_00007-8-3 loss: 0.479334  [  128/  179]
train() client id: f_00007-8-4 loss: 0.555373  [  160/  179]
train() client id: f_00007-9-0 loss: 0.604686  [   32/  179]
train() client id: f_00007-9-1 loss: 0.427788  [   64/  179]
train() client id: f_00007-9-2 loss: 0.408293  [   96/  179]
train() client id: f_00007-9-3 loss: 0.540812  [  128/  179]
train() client id: f_00007-9-4 loss: 0.301118  [  160/  179]
train() client id: f_00007-10-0 loss: 0.695952  [   32/  179]
train() client id: f_00007-10-1 loss: 0.398223  [   64/  179]
train() client id: f_00007-10-2 loss: 0.303099  [   96/  179]
train() client id: f_00007-10-3 loss: 0.368261  [  128/  179]
train() client id: f_00007-10-4 loss: 0.449897  [  160/  179]
train() client id: f_00007-11-0 loss: 0.498831  [   32/  179]
train() client id: f_00007-11-1 loss: 0.329723  [   64/  179]
train() client id: f_00007-11-2 loss: 0.299222  [   96/  179]
train() client id: f_00007-11-3 loss: 0.403381  [  128/  179]
train() client id: f_00007-11-4 loss: 0.830335  [  160/  179]
train() client id: f_00008-0-0 loss: 0.810205  [   32/  130]
train() client id: f_00008-0-1 loss: 0.722452  [   64/  130]
train() client id: f_00008-0-2 loss: 0.834219  [   96/  130]
train() client id: f_00008-0-3 loss: 0.764004  [  128/  130]
train() client id: f_00008-1-0 loss: 0.847052  [   32/  130]
train() client id: f_00008-1-1 loss: 0.832527  [   64/  130]
train() client id: f_00008-1-2 loss: 0.705310  [   96/  130]
train() client id: f_00008-1-3 loss: 0.774786  [  128/  130]
train() client id: f_00008-2-0 loss: 0.680120  [   32/  130]
train() client id: f_00008-2-1 loss: 0.772040  [   64/  130]
train() client id: f_00008-2-2 loss: 0.878117  [   96/  130]
train() client id: f_00008-2-3 loss: 0.832212  [  128/  130]
train() client id: f_00008-3-0 loss: 0.760004  [   32/  130]
train() client id: f_00008-3-1 loss: 0.719537  [   64/  130]
train() client id: f_00008-3-2 loss: 0.882663  [   96/  130]
train() client id: f_00008-3-3 loss: 0.794941  [  128/  130]
train() client id: f_00008-4-0 loss: 0.786607  [   32/  130]
train() client id: f_00008-4-1 loss: 0.732795  [   64/  130]
train() client id: f_00008-4-2 loss: 0.798555  [   96/  130]
train() client id: f_00008-4-3 loss: 0.819434  [  128/  130]
train() client id: f_00008-5-0 loss: 0.730480  [   32/  130]
train() client id: f_00008-5-1 loss: 0.856696  [   64/  130]
train() client id: f_00008-5-2 loss: 0.742326  [   96/  130]
train() client id: f_00008-5-3 loss: 0.833198  [  128/  130]
train() client id: f_00008-6-0 loss: 0.662828  [   32/  130]
train() client id: f_00008-6-1 loss: 0.843317  [   64/  130]
train() client id: f_00008-6-2 loss: 0.840838  [   96/  130]
train() client id: f_00008-6-3 loss: 0.814288  [  128/  130]
train() client id: f_00008-7-0 loss: 0.755740  [   32/  130]
train() client id: f_00008-7-1 loss: 0.882187  [   64/  130]
train() client id: f_00008-7-2 loss: 0.764451  [   96/  130]
train() client id: f_00008-7-3 loss: 0.752069  [  128/  130]
train() client id: f_00008-8-0 loss: 0.792031  [   32/  130]
train() client id: f_00008-8-1 loss: 0.809769  [   64/  130]
train() client id: f_00008-8-2 loss: 0.836819  [   96/  130]
train() client id: f_00008-8-3 loss: 0.727492  [  128/  130]
train() client id: f_00008-9-0 loss: 0.820961  [   32/  130]
train() client id: f_00008-9-1 loss: 0.890353  [   64/  130]
train() client id: f_00008-9-2 loss: 0.724492  [   96/  130]
train() client id: f_00008-9-3 loss: 0.722585  [  128/  130]
train() client id: f_00008-10-0 loss: 0.663748  [   32/  130]
train() client id: f_00008-10-1 loss: 0.852829  [   64/  130]
train() client id: f_00008-10-2 loss: 0.830336  [   96/  130]
train() client id: f_00008-10-3 loss: 0.798516  [  128/  130]
train() client id: f_00008-11-0 loss: 0.853294  [   32/  130]
train() client id: f_00008-11-1 loss: 0.706358  [   64/  130]
train() client id: f_00008-11-2 loss: 0.821224  [   96/  130]
train() client id: f_00008-11-3 loss: 0.775622  [  128/  130]
train() client id: f_00009-0-0 loss: 1.232294  [   32/  118]
train() client id: f_00009-0-1 loss: 0.917169  [   64/  118]
train() client id: f_00009-0-2 loss: 1.132496  [   96/  118]
train() client id: f_00009-1-0 loss: 1.041964  [   32/  118]
train() client id: f_00009-1-1 loss: 1.138358  [   64/  118]
train() client id: f_00009-1-2 loss: 1.073488  [   96/  118]
train() client id: f_00009-2-0 loss: 1.028319  [   32/  118]
train() client id: f_00009-2-1 loss: 1.035470  [   64/  118]
train() client id: f_00009-2-2 loss: 1.007640  [   96/  118]
train() client id: f_00009-3-0 loss: 1.025195  [   32/  118]
train() client id: f_00009-3-1 loss: 1.096793  [   64/  118]
train() client id: f_00009-3-2 loss: 0.963840  [   96/  118]
train() client id: f_00009-4-0 loss: 0.928236  [   32/  118]
train() client id: f_00009-4-1 loss: 1.007585  [   64/  118]
train() client id: f_00009-4-2 loss: 1.017045  [   96/  118]
train() client id: f_00009-5-0 loss: 0.984486  [   32/  118]
train() client id: f_00009-5-1 loss: 0.868480  [   64/  118]
train() client id: f_00009-5-2 loss: 1.021037  [   96/  118]
train() client id: f_00009-6-0 loss: 1.046314  [   32/  118]
train() client id: f_00009-6-1 loss: 0.772869  [   64/  118]
train() client id: f_00009-6-2 loss: 0.916765  [   96/  118]
train() client id: f_00009-7-0 loss: 0.835377  [   32/  118]
train() client id: f_00009-7-1 loss: 0.901457  [   64/  118]
train() client id: f_00009-7-2 loss: 0.997351  [   96/  118]
train() client id: f_00009-8-0 loss: 1.018227  [   32/  118]
train() client id: f_00009-8-1 loss: 0.922230  [   64/  118]
train() client id: f_00009-8-2 loss: 0.922270  [   96/  118]
train() client id: f_00009-9-0 loss: 0.915765  [   32/  118]
train() client id: f_00009-9-1 loss: 1.079066  [   64/  118]
train() client id: f_00009-9-2 loss: 0.843180  [   96/  118]
train() client id: f_00009-10-0 loss: 0.952544  [   32/  118]
train() client id: f_00009-10-1 loss: 0.910771  [   64/  118]
train() client id: f_00009-10-2 loss: 0.949646  [   96/  118]
train() client id: f_00009-11-0 loss: 1.074310  [   32/  118]
train() client id: f_00009-11-1 loss: 0.824522  [   64/  118]
train() client id: f_00009-11-2 loss: 0.841064  [   96/  118]
At round 28 accuracy: 0.649867374005305
At round 28 training accuracy: 0.5835010060362174
At round 28 training loss: 0.8383086889042781
update_location
xs = [  -3.9056584     4.20031788  160.00902392   18.81129433    0.97929623
    3.95640986 -122.44319194 -101.32485185  144.66397685  -87.06087855]
ys = [ 152.5879595   135.55583871    1.32061395 -122.45517586  114.35018685
   97.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [182.47832625 168.50230882 188.69189637 159.21411649 151.91090893
 139.94020603 158.11143474 142.36362547 176.73804393 132.64843923]
dists_bs = [172.55600043 183.49004274 377.35338245 355.02450543 186.13734858
 194.89241123 185.23728871 189.11869796 356.32790511 192.28567891]
uav_gains = [2.19565925e-11 2.70052809e-11 2.00716392e-11 3.11928652e-11
 3.51147715e-11 4.31483308e-11 3.17457777e-11 4.13309852e-11
 2.38781215e-11 4.93353388e-11]
bs_gains = [6.02406849e-11 5.07199808e-11 6.73595215e-12 7.99044117e-12
 4.87259393e-11 4.28418567e-11 4.93917622e-11 4.66055429e-11
 7.90887203e-12 4.44879766e-11]
Round 29
-------------------------------
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.16426333 14.86448857  7.05664821  2.53741575 17.14489673  8.25364943
  3.14835783 10.08926783  7.43869069  6.69596016]
obj_prev = 84.39363852739328
eta_min = 1.1633239161667213e-13	eta_max = 0.9255734667757686
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 19.601082838327166	eta = 0.9090909090909091
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 35.06532785864593	eta = 0.5081705292616409
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 27.55402889869821	eta = 0.6466991191078749
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.19954622843339	eta = 0.6801326275385099
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129948192651668	eta = 0.6819441847065052
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129750162898475	eta = 0.6819493529625255
eta = 0.6819493529625255
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.03158773 0.06643451 0.03108634 0.01077994 0.07671306 0.03660165
 0.0135376  0.04487463 0.03259051 0.02958216]
ene_total = [2.30902016 4.22374425 2.291367   1.07582749 4.81690287 2.52674818
 1.23224478 3.00469125 2.52859005 2.12061414]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 0 obj = 4.703234079287297
eta = 0.6819493529625255
freqs = [38424917.91998439 78013241.65854445 37994067.71862566 12898298.99218661
 90209018.95065838 43241454.64083756 16185653.82662158 53086584.80672698
 42569513.0112679  34900040.52352524]
eta_min = 0.6819493529625276	eta_max = 0.6819493529625246
af = 0.016869127524841664	bf = 1.4894091419856037	zeta = 0.018556040277325832	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [2.90303184e-06 2.51673727e-05 2.79324241e-06 1.11631997e-07
 3.88576653e-05 4.25999219e-06 2.20754524e-07 7.87187088e-06
 3.67617412e-06 2.24279318e-06]
ene_total = [1.73801127 1.44623117 1.77687644 1.59993898 1.46089904 1.49353974
 1.59363288 1.50568747 2.3050487  1.48132066]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 1 obj = 4.703234079287284
eta = 0.6819493529625246
freqs = [38424917.91998437 78013241.65854444 37994067.71862565 12898298.99218661
 90209018.95065837 43241454.64083755 16185653.82662157 53086584.80672697
 42569513.01126787 34900040.52352523]
Done!
At round 29 energy consumption: 4.703234079287297
At round 29 eta: 0.6819493529625246
At round 29 local rounds: 12.534813137868813
At round 29 global rounds: 57.376941264307334
At round 29 a_n: 17.906227447174857
gradient difference: 0.4906267523765564
train() client id: f_00000-0-0 loss: 1.227198  [   32/  126]
train() client id: f_00000-0-1 loss: 1.028135  [   64/  126]
train() client id: f_00000-0-2 loss: 1.006581  [   96/  126]
train() client id: f_00000-1-0 loss: 0.999704  [   32/  126]
train() client id: f_00000-1-1 loss: 1.079756  [   64/  126]
train() client id: f_00000-1-2 loss: 0.990060  [   96/  126]
train() client id: f_00000-2-0 loss: 1.028487  [   32/  126]
train() client id: f_00000-2-1 loss: 1.094048  [   64/  126]
train() client id: f_00000-2-2 loss: 0.945067  [   96/  126]
train() client id: f_00000-3-0 loss: 0.915975  [   32/  126]
train() client id: f_00000-3-1 loss: 0.930600  [   64/  126]
train() client id: f_00000-3-2 loss: 0.963987  [   96/  126]
train() client id: f_00000-4-0 loss: 1.059606  [   32/  126]
train() client id: f_00000-4-1 loss: 0.925692  [   64/  126]
train() client id: f_00000-4-2 loss: 0.850661  [   96/  126]
train() client id: f_00000-5-0 loss: 0.862436  [   32/  126]
train() client id: f_00000-5-1 loss: 0.902043  [   64/  126]
train() client id: f_00000-5-2 loss: 0.971340  [   96/  126]
train() client id: f_00000-6-0 loss: 0.999241  [   32/  126]
train() client id: f_00000-6-1 loss: 0.974160  [   64/  126]
train() client id: f_00000-6-2 loss: 0.855186  [   96/  126]
train() client id: f_00000-7-0 loss: 0.890773  [   32/  126]
train() client id: f_00000-7-1 loss: 0.965880  [   64/  126]
train() client id: f_00000-7-2 loss: 0.883799  [   96/  126]
train() client id: f_00000-8-0 loss: 0.905386  [   32/  126]
train() client id: f_00000-8-1 loss: 0.852662  [   64/  126]
train() client id: f_00000-8-2 loss: 0.951256  [   96/  126]
train() client id: f_00000-9-0 loss: 0.888288  [   32/  126]
train() client id: f_00000-9-1 loss: 0.962259  [   64/  126]
train() client id: f_00000-9-2 loss: 0.873858  [   96/  126]
train() client id: f_00000-10-0 loss: 0.974973  [   32/  126]
train() client id: f_00000-10-1 loss: 0.972693  [   64/  126]
train() client id: f_00000-10-2 loss: 0.924205  [   96/  126]
train() client id: f_00000-11-0 loss: 0.871387  [   32/  126]
train() client id: f_00000-11-1 loss: 0.922491  [   64/  126]
train() client id: f_00000-11-2 loss: 1.021959  [   96/  126]
train() client id: f_00001-0-0 loss: 0.363576  [   32/  265]
train() client id: f_00001-0-1 loss: 0.364667  [   64/  265]
train() client id: f_00001-0-2 loss: 0.358159  [   96/  265]
train() client id: f_00001-0-3 loss: 0.368529  [  128/  265]
train() client id: f_00001-0-4 loss: 0.331105  [  160/  265]
train() client id: f_00001-0-5 loss: 0.274037  [  192/  265]
train() client id: f_00001-0-6 loss: 0.515796  [  224/  265]
train() client id: f_00001-0-7 loss: 0.383993  [  256/  265]
train() client id: f_00001-1-0 loss: 0.373184  [   32/  265]
train() client id: f_00001-1-1 loss: 0.509111  [   64/  265]
train() client id: f_00001-1-2 loss: 0.360019  [   96/  265]
train() client id: f_00001-1-3 loss: 0.406727  [  128/  265]
train() client id: f_00001-1-4 loss: 0.248518  [  160/  265]
train() client id: f_00001-1-5 loss: 0.320258  [  192/  265]
train() client id: f_00001-1-6 loss: 0.291051  [  224/  265]
train() client id: f_00001-1-7 loss: 0.366147  [  256/  265]
train() client id: f_00001-2-0 loss: 0.308361  [   32/  265]
train() client id: f_00001-2-1 loss: 0.324707  [   64/  265]
train() client id: f_00001-2-2 loss: 0.382876  [   96/  265]
train() client id: f_00001-2-3 loss: 0.263710  [  128/  265]
train() client id: f_00001-2-4 loss: 0.433741  [  160/  265]
train() client id: f_00001-2-5 loss: 0.419549  [  192/  265]
train() client id: f_00001-2-6 loss: 0.375455  [  224/  265]
train() client id: f_00001-2-7 loss: 0.317434  [  256/  265]
train() client id: f_00001-3-0 loss: 0.356515  [   32/  265]
train() client id: f_00001-3-1 loss: 0.340694  [   64/  265]
train() client id: f_00001-3-2 loss: 0.316959  [   96/  265]
train() client id: f_00001-3-3 loss: 0.278700  [  128/  265]
train() client id: f_00001-3-4 loss: 0.247456  [  160/  265]
train() client id: f_00001-3-5 loss: 0.442521  [  192/  265]
train() client id: f_00001-3-6 loss: 0.265982  [  224/  265]
train() client id: f_00001-3-7 loss: 0.515957  [  256/  265]
train() client id: f_00001-4-0 loss: 0.320358  [   32/  265]
train() client id: f_00001-4-1 loss: 0.275004  [   64/  265]
train() client id: f_00001-4-2 loss: 0.311301  [   96/  265]
train() client id: f_00001-4-3 loss: 0.314649  [  128/  265]
train() client id: f_00001-4-4 loss: 0.366379  [  160/  265]
train() client id: f_00001-4-5 loss: 0.334587  [  192/  265]
train() client id: f_00001-4-6 loss: 0.321819  [  224/  265]
train() client id: f_00001-4-7 loss: 0.394405  [  256/  265]
train() client id: f_00001-5-0 loss: 0.365257  [   32/  265]
train() client id: f_00001-5-1 loss: 0.246016  [   64/  265]
train() client id: f_00001-5-2 loss: 0.321995  [   96/  265]
train() client id: f_00001-5-3 loss: 0.343295  [  128/  265]
train() client id: f_00001-5-4 loss: 0.379051  [  160/  265]
train() client id: f_00001-5-5 loss: 0.482586  [  192/  265]
train() client id: f_00001-5-6 loss: 0.296789  [  224/  265]
train() client id: f_00001-5-7 loss: 0.242225  [  256/  265]
train() client id: f_00001-6-0 loss: 0.467783  [   32/  265]
train() client id: f_00001-6-1 loss: 0.362764  [   64/  265]
train() client id: f_00001-6-2 loss: 0.316787  [   96/  265]
train() client id: f_00001-6-3 loss: 0.310312  [  128/  265]
train() client id: f_00001-6-4 loss: 0.241356  [  160/  265]
train() client id: f_00001-6-5 loss: 0.264959  [  192/  265]
train() client id: f_00001-6-6 loss: 0.312223  [  224/  265]
train() client id: f_00001-6-7 loss: 0.237943  [  256/  265]
train() client id: f_00001-7-0 loss: 0.405652  [   32/  265]
train() client id: f_00001-7-1 loss: 0.281947  [   64/  265]
train() client id: f_00001-7-2 loss: 0.379832  [   96/  265]
train() client id: f_00001-7-3 loss: 0.316782  [  128/  265]
train() client id: f_00001-7-4 loss: 0.244987  [  160/  265]
train() client id: f_00001-7-5 loss: 0.399262  [  192/  265]
train() client id: f_00001-7-6 loss: 0.364068  [  224/  265]
train() client id: f_00001-7-7 loss: 0.222373  [  256/  265]
train() client id: f_00001-8-0 loss: 0.324325  [   32/  265]
train() client id: f_00001-8-1 loss: 0.239396  [   64/  265]
train() client id: f_00001-8-2 loss: 0.318875  [   96/  265]
train() client id: f_00001-8-3 loss: 0.276711  [  128/  265]
train() client id: f_00001-8-4 loss: 0.255003  [  160/  265]
train() client id: f_00001-8-5 loss: 0.363649  [  192/  265]
train() client id: f_00001-8-6 loss: 0.430639  [  224/  265]
train() client id: f_00001-8-7 loss: 0.401027  [  256/  265]
train() client id: f_00001-9-0 loss: 0.307697  [   32/  265]
train() client id: f_00001-9-1 loss: 0.419589  [   64/  265]
train() client id: f_00001-9-2 loss: 0.295251  [   96/  265]
train() client id: f_00001-9-3 loss: 0.372620  [  128/  265]
train() client id: f_00001-9-4 loss: 0.307273  [  160/  265]
train() client id: f_00001-9-5 loss: 0.354824  [  192/  265]
train() client id: f_00001-9-6 loss: 0.216729  [  224/  265]
train() client id: f_00001-9-7 loss: 0.229732  [  256/  265]
train() client id: f_00001-10-0 loss: 0.336631  [   32/  265]
train() client id: f_00001-10-1 loss: 0.367165  [   64/  265]
train() client id: f_00001-10-2 loss: 0.359631  [   96/  265]
train() client id: f_00001-10-3 loss: 0.287056  [  128/  265]
train() client id: f_00001-10-4 loss: 0.384763  [  160/  265]
train() client id: f_00001-10-5 loss: 0.242547  [  192/  265]
train() client id: f_00001-10-6 loss: 0.307752  [  224/  265]
train() client id: f_00001-10-7 loss: 0.303077  [  256/  265]
train() client id: f_00001-11-0 loss: 0.332170  [   32/  265]
train() client id: f_00001-11-1 loss: 0.228202  [   64/  265]
train() client id: f_00001-11-2 loss: 0.279491  [   96/  265]
train() client id: f_00001-11-3 loss: 0.440008  [  128/  265]
train() client id: f_00001-11-4 loss: 0.368321  [  160/  265]
train() client id: f_00001-11-5 loss: 0.221387  [  192/  265]
train() client id: f_00001-11-6 loss: 0.228568  [  224/  265]
train() client id: f_00001-11-7 loss: 0.478750  [  256/  265]
train() client id: f_00002-0-0 loss: 1.181961  [   32/  124]
train() client id: f_00002-0-1 loss: 0.974751  [   64/  124]
train() client id: f_00002-0-2 loss: 1.303943  [   96/  124]
train() client id: f_00002-1-0 loss: 1.128367  [   32/  124]
train() client id: f_00002-1-1 loss: 1.167249  [   64/  124]
train() client id: f_00002-1-2 loss: 1.055088  [   96/  124]
train() client id: f_00002-2-0 loss: 0.978703  [   32/  124]
train() client id: f_00002-2-1 loss: 1.069759  [   64/  124]
train() client id: f_00002-2-2 loss: 1.074332  [   96/  124]
train() client id: f_00002-3-0 loss: 0.985759  [   32/  124]
train() client id: f_00002-3-1 loss: 1.102527  [   64/  124]
train() client id: f_00002-3-2 loss: 0.991901  [   96/  124]
train() client id: f_00002-4-0 loss: 1.032178  [   32/  124]
train() client id: f_00002-4-1 loss: 1.012751  [   64/  124]
train() client id: f_00002-4-2 loss: 0.939121  [   96/  124]
train() client id: f_00002-5-0 loss: 0.894913  [   32/  124]
train() client id: f_00002-5-1 loss: 0.976321  [   64/  124]
train() client id: f_00002-5-2 loss: 1.022680  [   96/  124]
train() client id: f_00002-6-0 loss: 0.941717  [   32/  124]
train() client id: f_00002-6-1 loss: 1.010741  [   64/  124]
train() client id: f_00002-6-2 loss: 0.967857  [   96/  124]
train() client id: f_00002-7-0 loss: 1.070671  [   32/  124]
train() client id: f_00002-7-1 loss: 0.898084  [   64/  124]
train() client id: f_00002-7-2 loss: 0.944065  [   96/  124]
train() client id: f_00002-8-0 loss: 1.052314  [   32/  124]
train() client id: f_00002-8-1 loss: 0.894007  [   64/  124]
train() client id: f_00002-8-2 loss: 0.853312  [   96/  124]
train() client id: f_00002-9-0 loss: 0.947850  [   32/  124]
train() client id: f_00002-9-1 loss: 0.787294  [   64/  124]
train() client id: f_00002-9-2 loss: 0.984414  [   96/  124]
train() client id: f_00002-10-0 loss: 0.867447  [   32/  124]
train() client id: f_00002-10-1 loss: 0.955771  [   64/  124]
train() client id: f_00002-10-2 loss: 0.961350  [   96/  124]
train() client id: f_00002-11-0 loss: 0.965822  [   32/  124]
train() client id: f_00002-11-1 loss: 0.943593  [   64/  124]
train() client id: f_00002-11-2 loss: 0.859813  [   96/  124]
train() client id: f_00003-0-0 loss: 0.822193  [   32/   43]
train() client id: f_00003-1-0 loss: 0.756803  [   32/   43]
train() client id: f_00003-2-0 loss: 0.719358  [   32/   43]
train() client id: f_00003-3-0 loss: 0.738347  [   32/   43]
train() client id: f_00003-4-0 loss: 0.780040  [   32/   43]
train() client id: f_00003-5-0 loss: 0.764074  [   32/   43]
train() client id: f_00003-6-0 loss: 0.688203  [   32/   43]
train() client id: f_00003-7-0 loss: 0.571408  [   32/   43]
train() client id: f_00003-8-0 loss: 0.668398  [   32/   43]
train() client id: f_00003-9-0 loss: 0.622872  [   32/   43]
train() client id: f_00003-10-0 loss: 0.856838  [   32/   43]
train() client id: f_00003-11-0 loss: 0.655483  [   32/   43]
train() client id: f_00004-0-0 loss: 0.806451  [   32/  306]
train() client id: f_00004-0-1 loss: 0.943519  [   64/  306]
train() client id: f_00004-0-2 loss: 0.833998  [   96/  306]
train() client id: f_00004-0-3 loss: 0.783250  [  128/  306]
train() client id: f_00004-0-4 loss: 1.017638  [  160/  306]
train() client id: f_00004-0-5 loss: 0.803064  [  192/  306]
train() client id: f_00004-0-6 loss: 0.822925  [  224/  306]
train() client id: f_00004-0-7 loss: 0.827905  [  256/  306]
train() client id: f_00004-0-8 loss: 0.835514  [  288/  306]
train() client id: f_00004-1-0 loss: 0.767364  [   32/  306]
train() client id: f_00004-1-1 loss: 0.806568  [   64/  306]
train() client id: f_00004-1-2 loss: 0.754946  [   96/  306]
train() client id: f_00004-1-3 loss: 0.873071  [  128/  306]
train() client id: f_00004-1-4 loss: 0.988955  [  160/  306]
train() client id: f_00004-1-5 loss: 0.819827  [  192/  306]
train() client id: f_00004-1-6 loss: 0.864955  [  224/  306]
train() client id: f_00004-1-7 loss: 0.964612  [  256/  306]
train() client id: f_00004-1-8 loss: 0.754609  [  288/  306]
train() client id: f_00004-2-0 loss: 0.919339  [   32/  306]
train() client id: f_00004-2-1 loss: 0.726654  [   64/  306]
train() client id: f_00004-2-2 loss: 0.899036  [   96/  306]
train() client id: f_00004-2-3 loss: 0.818683  [  128/  306]
train() client id: f_00004-2-4 loss: 0.845162  [  160/  306]
train() client id: f_00004-2-5 loss: 0.885873  [  192/  306]
train() client id: f_00004-2-6 loss: 0.834173  [  224/  306]
train() client id: f_00004-2-7 loss: 0.686608  [  256/  306]
train() client id: f_00004-2-8 loss: 0.920155  [  288/  306]
train() client id: f_00004-3-0 loss: 0.939624  [   32/  306]
train() client id: f_00004-3-1 loss: 0.900599  [   64/  306]
train() client id: f_00004-3-2 loss: 0.821787  [   96/  306]
train() client id: f_00004-3-3 loss: 0.654560  [  128/  306]
train() client id: f_00004-3-4 loss: 0.878102  [  160/  306]
train() client id: f_00004-3-5 loss: 0.831388  [  192/  306]
train() client id: f_00004-3-6 loss: 0.873652  [  224/  306]
train() client id: f_00004-3-7 loss: 0.921235  [  256/  306]
train() client id: f_00004-3-8 loss: 0.815239  [  288/  306]
train() client id: f_00004-4-0 loss: 0.823898  [   32/  306]
train() client id: f_00004-4-1 loss: 0.789795  [   64/  306]
train() client id: f_00004-4-2 loss: 0.809907  [   96/  306]
train() client id: f_00004-4-3 loss: 0.880936  [  128/  306]
train() client id: f_00004-4-4 loss: 0.735412  [  160/  306]
train() client id: f_00004-4-5 loss: 0.966588  [  192/  306]
train() client id: f_00004-4-6 loss: 0.924030  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838806  [  256/  306]
train() client id: f_00004-4-8 loss: 0.798577  [  288/  306]
train() client id: f_00004-5-0 loss: 0.772999  [   32/  306]
train() client id: f_00004-5-1 loss: 0.908481  [   64/  306]
train() client id: f_00004-5-2 loss: 0.807084  [   96/  306]
train() client id: f_00004-5-3 loss: 0.758514  [  128/  306]
train() client id: f_00004-5-4 loss: 0.936176  [  160/  306]
train() client id: f_00004-5-5 loss: 0.886220  [  192/  306]
train() client id: f_00004-5-6 loss: 0.832891  [  224/  306]
train() client id: f_00004-5-7 loss: 0.940031  [  256/  306]
train() client id: f_00004-5-8 loss: 0.772877  [  288/  306]
train() client id: f_00004-6-0 loss: 0.762526  [   32/  306]
train() client id: f_00004-6-1 loss: 0.724340  [   64/  306]
train() client id: f_00004-6-2 loss: 0.789824  [   96/  306]
train() client id: f_00004-6-3 loss: 0.836710  [  128/  306]
train() client id: f_00004-6-4 loss: 0.870575  [  160/  306]
train() client id: f_00004-6-5 loss: 1.025364  [  192/  306]
train() client id: f_00004-6-6 loss: 0.827476  [  224/  306]
train() client id: f_00004-6-7 loss: 0.842067  [  256/  306]
train() client id: f_00004-6-8 loss: 0.900553  [  288/  306]
train() client id: f_00004-7-0 loss: 0.952767  [   32/  306]
train() client id: f_00004-7-1 loss: 0.857898  [   64/  306]
train() client id: f_00004-7-2 loss: 0.770943  [   96/  306]
train() client id: f_00004-7-3 loss: 0.747588  [  128/  306]
train() client id: f_00004-7-4 loss: 0.793948  [  160/  306]
train() client id: f_00004-7-5 loss: 0.812124  [  192/  306]
train() client id: f_00004-7-6 loss: 0.826268  [  224/  306]
train() client id: f_00004-7-7 loss: 0.942263  [  256/  306]
train() client id: f_00004-7-8 loss: 0.794102  [  288/  306]
train() client id: f_00004-8-0 loss: 0.818993  [   32/  306]
train() client id: f_00004-8-1 loss: 0.840936  [   64/  306]
train() client id: f_00004-8-2 loss: 0.721273  [   96/  306]
train() client id: f_00004-8-3 loss: 0.882247  [  128/  306]
train() client id: f_00004-8-4 loss: 0.804060  [  160/  306]
train() client id: f_00004-8-5 loss: 0.798689  [  192/  306]
train() client id: f_00004-8-6 loss: 0.820054  [  224/  306]
train() client id: f_00004-8-7 loss: 0.938344  [  256/  306]
train() client id: f_00004-8-8 loss: 0.828335  [  288/  306]
train() client id: f_00004-9-0 loss: 0.792124  [   32/  306]
train() client id: f_00004-9-1 loss: 0.754215  [   64/  306]
train() client id: f_00004-9-2 loss: 0.873471  [   96/  306]
train() client id: f_00004-9-3 loss: 0.767494  [  128/  306]
train() client id: f_00004-9-4 loss: 0.830858  [  160/  306]
train() client id: f_00004-9-5 loss: 0.924893  [  192/  306]
train() client id: f_00004-9-6 loss: 0.772776  [  224/  306]
train() client id: f_00004-9-7 loss: 0.894259  [  256/  306]
train() client id: f_00004-9-8 loss: 0.864350  [  288/  306]
train() client id: f_00004-10-0 loss: 0.859175  [   32/  306]
train() client id: f_00004-10-1 loss: 0.769479  [   64/  306]
train() client id: f_00004-10-2 loss: 0.964693  [   96/  306]
train() client id: f_00004-10-3 loss: 0.919274  [  128/  306]
train() client id: f_00004-10-4 loss: 0.879296  [  160/  306]
train() client id: f_00004-10-5 loss: 0.664831  [  192/  306]
train() client id: f_00004-10-6 loss: 0.826367  [  224/  306]
train() client id: f_00004-10-7 loss: 0.905355  [  256/  306]
train() client id: f_00004-10-8 loss: 0.730373  [  288/  306]
train() client id: f_00004-11-0 loss: 0.659653  [   32/  306]
train() client id: f_00004-11-1 loss: 0.745377  [   64/  306]
train() client id: f_00004-11-2 loss: 0.898159  [   96/  306]
train() client id: f_00004-11-3 loss: 0.817014  [  128/  306]
train() client id: f_00004-11-4 loss: 0.925839  [  160/  306]
train() client id: f_00004-11-5 loss: 0.843369  [  192/  306]
train() client id: f_00004-11-6 loss: 0.850654  [  224/  306]
train() client id: f_00004-11-7 loss: 0.808102  [  256/  306]
train() client id: f_00004-11-8 loss: 0.767853  [  288/  306]
train() client id: f_00005-0-0 loss: 0.527802  [   32/  146]
train() client id: f_00005-0-1 loss: 0.729631  [   64/  146]
train() client id: f_00005-0-2 loss: 0.354506  [   96/  146]
train() client id: f_00005-0-3 loss: 0.775516  [  128/  146]
train() client id: f_00005-1-0 loss: 0.840790  [   32/  146]
train() client id: f_00005-1-1 loss: 0.611581  [   64/  146]
train() client id: f_00005-1-2 loss: 0.495202  [   96/  146]
train() client id: f_00005-1-3 loss: 0.429411  [  128/  146]
train() client id: f_00005-2-0 loss: 0.431723  [   32/  146]
train() client id: f_00005-2-1 loss: 0.450709  [   64/  146]
train() client id: f_00005-2-2 loss: 0.612929  [   96/  146]
train() client id: f_00005-2-3 loss: 0.646304  [  128/  146]
train() client id: f_00005-3-0 loss: 0.452296  [   32/  146]
train() client id: f_00005-3-1 loss: 0.694327  [   64/  146]
train() client id: f_00005-3-2 loss: 0.668345  [   96/  146]
train() client id: f_00005-3-3 loss: 0.519135  [  128/  146]
train() client id: f_00005-4-0 loss: 0.750581  [   32/  146]
train() client id: f_00005-4-1 loss: 0.496577  [   64/  146]
train() client id: f_00005-4-2 loss: 0.489621  [   96/  146]
train() client id: f_00005-4-3 loss: 0.522923  [  128/  146]
train() client id: f_00005-5-0 loss: 0.558906  [   32/  146]
train() client id: f_00005-5-1 loss: 0.567609  [   64/  146]
train() client id: f_00005-5-2 loss: 0.727022  [   96/  146]
train() client id: f_00005-5-3 loss: 0.563089  [  128/  146]
train() client id: f_00005-6-0 loss: 0.519247  [   32/  146]
train() client id: f_00005-6-1 loss: 0.658398  [   64/  146]
train() client id: f_00005-6-2 loss: 0.676909  [   96/  146]
train() client id: f_00005-6-3 loss: 0.360317  [  128/  146]
train() client id: f_00005-7-0 loss: 0.592689  [   32/  146]
train() client id: f_00005-7-1 loss: 0.554092  [   64/  146]
train() client id: f_00005-7-2 loss: 0.604745  [   96/  146]
train() client id: f_00005-7-3 loss: 0.689119  [  128/  146]
train() client id: f_00005-8-0 loss: 0.423011  [   32/  146]
train() client id: f_00005-8-1 loss: 0.557130  [   64/  146]
train() client id: f_00005-8-2 loss: 0.461150  [   96/  146]
train() client id: f_00005-8-3 loss: 0.875491  [  128/  146]
train() client id: f_00005-9-0 loss: 0.891273  [   32/  146]
train() client id: f_00005-9-1 loss: 0.401202  [   64/  146]
train() client id: f_00005-9-2 loss: 0.513070  [   96/  146]
train() client id: f_00005-9-3 loss: 0.501877  [  128/  146]
train() client id: f_00005-10-0 loss: 0.500545  [   32/  146]
train() client id: f_00005-10-1 loss: 0.555119  [   64/  146]
train() client id: f_00005-10-2 loss: 0.608485  [   96/  146]
train() client id: f_00005-10-3 loss: 0.761772  [  128/  146]
train() client id: f_00005-11-0 loss: 0.690156  [   32/  146]
train() client id: f_00005-11-1 loss: 0.571412  [   64/  146]
train() client id: f_00005-11-2 loss: 0.541391  [   96/  146]
train() client id: f_00005-11-3 loss: 0.604258  [  128/  146]
train() client id: f_00006-0-0 loss: 0.548710  [   32/   54]
train() client id: f_00006-1-0 loss: 0.445796  [   32/   54]
train() client id: f_00006-2-0 loss: 0.537940  [   32/   54]
train() client id: f_00006-3-0 loss: 0.545111  [   32/   54]
train() client id: f_00006-4-0 loss: 0.524278  [   32/   54]
train() client id: f_00006-5-0 loss: 0.558616  [   32/   54]
train() client id: f_00006-6-0 loss: 0.544939  [   32/   54]
train() client id: f_00006-7-0 loss: 0.486638  [   32/   54]
train() client id: f_00006-8-0 loss: 0.469959  [   32/   54]
train() client id: f_00006-9-0 loss: 0.470469  [   32/   54]
train() client id: f_00006-10-0 loss: 0.515255  [   32/   54]
train() client id: f_00006-11-0 loss: 0.564504  [   32/   54]
train() client id: f_00007-0-0 loss: 0.256340  [   32/  179]
train() client id: f_00007-0-1 loss: 0.144532  [   64/  179]
train() client id: f_00007-0-2 loss: 0.146686  [   96/  179]
train() client id: f_00007-0-3 loss: 0.291203  [  128/  179]
train() client id: f_00007-0-4 loss: 0.494448  [  160/  179]
train() client id: f_00007-1-0 loss: 0.410508  [   32/  179]
train() client id: f_00007-1-1 loss: 0.155038  [   64/  179]
train() client id: f_00007-1-2 loss: 0.231425  [   96/  179]
train() client id: f_00007-1-3 loss: 0.091849  [  128/  179]
train() client id: f_00007-1-4 loss: 0.442334  [  160/  179]
train() client id: f_00007-2-0 loss: 0.137613  [   32/  179]
train() client id: f_00007-2-1 loss: 0.312460  [   64/  179]
train() client id: f_00007-2-2 loss: 0.156558  [   96/  179]
train() client id: f_00007-2-3 loss: 0.328008  [  128/  179]
train() client id: f_00007-2-4 loss: 0.277222  [  160/  179]
train() client id: f_00007-3-0 loss: 0.300734  [   32/  179]
train() client id: f_00007-3-1 loss: 0.063336  [   64/  179]
train() client id: f_00007-3-2 loss: 0.162661  [   96/  179]
train() client id: f_00007-3-3 loss: 0.126572  [  128/  179]
train() client id: f_00007-3-4 loss: 0.359267  [  160/  179]
train() client id: f_00007-4-0 loss: 0.307231  [   32/  179]
train() client id: f_00007-4-1 loss: 0.089899  [   64/  179]
train() client id: f_00007-4-2 loss: 0.171720  [   96/  179]
train() client id: f_00007-4-3 loss: 0.357498  [  128/  179]
train() client id: f_00007-4-4 loss: 0.306194  [  160/  179]
train() client id: f_00007-5-0 loss: 0.392899  [   32/  179]
train() client id: f_00007-5-1 loss: 0.296743  [   64/  179]
train() client id: f_00007-5-2 loss: 0.135121  [   96/  179]
train() client id: f_00007-5-3 loss: 0.231191  [  128/  179]
train() client id: f_00007-5-4 loss: 0.106848  [  160/  179]
train() client id: f_00007-6-0 loss: 0.128087  [   32/  179]
train() client id: f_00007-6-1 loss: 0.081316  [   64/  179]
train() client id: f_00007-6-2 loss: 0.106156  [   96/  179]
train() client id: f_00007-6-3 loss: 0.308401  [  128/  179]
train() client id: f_00007-6-4 loss: 0.342571  [  160/  179]
train() client id: f_00007-7-0 loss: 0.089719  [   32/  179]
train() client id: f_00007-7-1 loss: 0.249716  [   64/  179]
train() client id: f_00007-7-2 loss: 0.290283  [   96/  179]
train() client id: f_00007-7-3 loss: 0.152117  [  128/  179]
train() client id: f_00007-7-4 loss: 0.255119  [  160/  179]
train() client id: f_00007-8-0 loss: 0.197630  [   32/  179]
train() client id: f_00007-8-1 loss: 0.163295  [   64/  179]
train() client id: f_00007-8-2 loss: 0.156333  [   96/  179]
train() client id: f_00007-8-3 loss: 0.270671  [  128/  179]
train() client id: f_00007-8-4 loss: 0.264400  [  160/  179]
train() client id: f_00007-9-0 loss: 0.354718  [   32/  179]
train() client id: f_00007-9-1 loss: 0.235534  [   64/  179]
train() client id: f_00007-9-2 loss: 0.178475  [   96/  179]
train() client id: f_00007-9-3 loss: 0.161487  [  128/  179]
train() client id: f_00007-9-4 loss: 0.109731  [  160/  179]
train() client id: f_00007-10-0 loss: 0.031672  [   32/  179]
train() client id: f_00007-10-1 loss: 0.053314  [   64/  179]
train() client id: f_00007-10-2 loss: 0.211245  [   96/  179]
train() client id: f_00007-10-3 loss: 0.381940  [  128/  179]
train() client id: f_00007-10-4 loss: 0.187727  [  160/  179]
train() client id: f_00007-11-0 loss: 0.127414  [   32/  179]
train() client id: f_00007-11-1 loss: 0.046751  [   64/  179]
train() client id: f_00007-11-2 loss: 0.359523  [   96/  179]
train() client id: f_00007-11-3 loss: 0.175456  [  128/  179]
train() client id: f_00007-11-4 loss: 0.229624  [  160/  179]
train() client id: f_00008-0-0 loss: 0.711691  [   32/  130]
train() client id: f_00008-0-1 loss: 0.885269  [   64/  130]
train() client id: f_00008-0-2 loss: 0.728816  [   96/  130]
train() client id: f_00008-0-3 loss: 0.679349  [  128/  130]
train() client id: f_00008-1-0 loss: 0.821893  [   32/  130]
train() client id: f_00008-1-1 loss: 0.766531  [   64/  130]
train() client id: f_00008-1-2 loss: 0.769432  [   96/  130]
train() client id: f_00008-1-3 loss: 0.612293  [  128/  130]
train() client id: f_00008-2-0 loss: 0.747664  [   32/  130]
train() client id: f_00008-2-1 loss: 0.760675  [   64/  130]
train() client id: f_00008-2-2 loss: 0.729459  [   96/  130]
train() client id: f_00008-2-3 loss: 0.762608  [  128/  130]
train() client id: f_00008-3-0 loss: 0.724440  [   32/  130]
train() client id: f_00008-3-1 loss: 0.778438  [   64/  130]
train() client id: f_00008-3-2 loss: 0.698458  [   96/  130]
train() client id: f_00008-3-3 loss: 0.800466  [  128/  130]
train() client id: f_00008-4-0 loss: 0.722993  [   32/  130]
train() client id: f_00008-4-1 loss: 0.864420  [   64/  130]
train() client id: f_00008-4-2 loss: 0.742347  [   96/  130]
train() client id: f_00008-4-3 loss: 0.705764  [  128/  130]
train() client id: f_00008-5-0 loss: 0.763222  [   32/  130]
train() client id: f_00008-5-1 loss: 0.701451  [   64/  130]
train() client id: f_00008-5-2 loss: 0.692576  [   96/  130]
train() client id: f_00008-5-3 loss: 0.874796  [  128/  130]
train() client id: f_00008-6-0 loss: 0.687160  [   32/  130]
train() client id: f_00008-6-1 loss: 0.821040  [   64/  130]
train() client id: f_00008-6-2 loss: 0.819897  [   96/  130]
train() client id: f_00008-6-3 loss: 0.713789  [  128/  130]
train() client id: f_00008-7-0 loss: 0.891802  [   32/  130]
train() client id: f_00008-7-1 loss: 0.735672  [   64/  130]
train() client id: f_00008-7-2 loss: 0.734169  [   96/  130]
train() client id: f_00008-7-3 loss: 0.673755  [  128/  130]
train() client id: f_00008-8-0 loss: 0.854791  [   32/  130]
train() client id: f_00008-8-1 loss: 0.707281  [   64/  130]
train() client id: f_00008-8-2 loss: 0.773113  [   96/  130]
train() client id: f_00008-8-3 loss: 0.701654  [  128/  130]
train() client id: f_00008-9-0 loss: 0.732503  [   32/  130]
train() client id: f_00008-9-1 loss: 0.814321  [   64/  130]
train() client id: f_00008-9-2 loss: 0.807684  [   96/  130]
train() client id: f_00008-9-3 loss: 0.684099  [  128/  130]
train() client id: f_00008-10-0 loss: 0.863403  [   32/  130]
train() client id: f_00008-10-1 loss: 0.808238  [   64/  130]
train() client id: f_00008-10-2 loss: 0.621908  [   96/  130]
train() client id: f_00008-10-3 loss: 0.739897  [  128/  130]
train() client id: f_00008-11-0 loss: 0.693966  [   32/  130]
train() client id: f_00008-11-1 loss: 0.806844  [   64/  130]
train() client id: f_00008-11-2 loss: 0.770299  [   96/  130]
train() client id: f_00008-11-3 loss: 0.754343  [  128/  130]
train() client id: f_00009-0-0 loss: 1.229869  [   32/  118]
train() client id: f_00009-0-1 loss: 1.107398  [   64/  118]
train() client id: f_00009-0-2 loss: 1.094372  [   96/  118]
train() client id: f_00009-1-0 loss: 0.962811  [   32/  118]
train() client id: f_00009-1-1 loss: 1.182198  [   64/  118]
train() client id: f_00009-1-2 loss: 1.057672  [   96/  118]
train() client id: f_00009-2-0 loss: 1.079175  [   32/  118]
train() client id: f_00009-2-1 loss: 1.002747  [   64/  118]
train() client id: f_00009-2-2 loss: 1.006887  [   96/  118]
train() client id: f_00009-3-0 loss: 0.976640  [   32/  118]
train() client id: f_00009-3-1 loss: 1.089985  [   64/  118]
train() client id: f_00009-3-2 loss: 0.949250  [   96/  118]
train() client id: f_00009-4-0 loss: 1.045929  [   32/  118]
train() client id: f_00009-4-1 loss: 1.090137  [   64/  118]
train() client id: f_00009-4-2 loss: 0.912765  [   96/  118]
train() client id: f_00009-5-0 loss: 0.985340  [   32/  118]
train() client id: f_00009-5-1 loss: 0.987828  [   64/  118]
train() client id: f_00009-5-2 loss: 0.898449  [   96/  118]
train() client id: f_00009-6-0 loss: 0.911550  [   32/  118]
train() client id: f_00009-6-1 loss: 0.954535  [   64/  118]
train() client id: f_00009-6-2 loss: 1.012648  [   96/  118]
train() client id: f_00009-7-0 loss: 0.917547  [   32/  118]
train() client id: f_00009-7-1 loss: 0.897024  [   64/  118]
train() client id: f_00009-7-2 loss: 1.090548  [   96/  118]
train() client id: f_00009-8-0 loss: 1.055424  [   32/  118]
train() client id: f_00009-8-1 loss: 0.751543  [   64/  118]
train() client id: f_00009-8-2 loss: 0.984258  [   96/  118]
train() client id: f_00009-9-0 loss: 0.970711  [   32/  118]
train() client id: f_00009-9-1 loss: 0.916299  [   64/  118]
train() client id: f_00009-9-2 loss: 1.079252  [   96/  118]
train() client id: f_00009-10-0 loss: 0.935540  [   32/  118]
train() client id: f_00009-10-1 loss: 1.066860  [   64/  118]
train() client id: f_00009-10-2 loss: 0.863188  [   96/  118]
train() client id: f_00009-11-0 loss: 0.792203  [   32/  118]
train() client id: f_00009-11-1 loss: 0.924837  [   64/  118]
train() client id: f_00009-11-2 loss: 1.047618  [   96/  118]
At round 29 accuracy: 0.6525198938992043
At round 29 training accuracy: 0.5814889336016097
At round 29 training loss: 0.831426029560245
update_location
xs = [  -3.9056584     4.20031788  165.00902392   18.81129433    0.97929623
    3.95640986 -127.44319194 -106.32485185  149.66397685  -92.06087855]
ys = [ 157.5879595   140.55583871    1.32061395 -127.45517586  119.35018685
  102.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [186.67945561 172.55024331 192.95005052 163.09103791 155.70942849
 143.47962494 162.01437502 145.96455178 180.85346538 135.9824151 ]
dists_bs = [171.9780593  182.48055834 381.79924762 359.22409609 184.56872542
 192.96681959 183.881171   187.22748312 360.82019861 190.05102236]
uav_gains = [2.06621021e-11 2.54082094e-11 1.88791154e-11 2.93478532e-11
 3.29966588e-11 4.05296333e-11 2.98453693e-11 3.88208176e-11
 2.24815448e-11 4.63629961e-11]
bs_gains = [6.08092382e-11 5.15095318e-11 6.51862344e-12 7.73162525e-12
 4.98943499e-11 4.40496737e-11 5.04184808e-11 4.79357148e-11
 7.63624282e-12 4.59681998e-11]
Round 30
-------------------------------
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.03213939 14.58505974  6.92666737  2.4917993  16.82243501  8.09793447
  3.09128315  9.90171672  7.30136856  6.56936016]
obj_prev = 82.8197638713389
eta_min = 6.720783557496832e-14	eta_max = 0.9260472326332857
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 19.233152927962998	eta = 0.9090909090909091
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 34.518956732515555	eta = 0.5065241286245609
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 27.08219396594083	eta = 0.6456155103960739
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.74057952697607	eta = 0.6792653779081567
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671393607462342	eta = 0.6810960381552402
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671195205593733	eta = 0.6811013020600015
eta = 0.6811013020600015
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.03169041 0.06665046 0.03118739 0.01081498 0.07696242 0.03672063
 0.0135816  0.0450205  0.03269645 0.02967832]
ene_total = [2.2727313  4.14429542 2.25574611 1.06104318 4.72594168 2.47696442
 1.21464967 2.95433273 2.48764402 2.07784665]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 0 obj = 4.633836318316938
eta = 0.6811013020600015
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718008 34168259.23522189]
eta_min = 0.6811013020600248	eta_max = 0.6811013020600005
af = 0.015898726867838657	bf = 1.4716185540757498	zeta = 0.017488599554622525	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [2.80235665e-06 2.41591478e-05 2.69672506e-06 1.07738059e-07
 3.72770088e-05 4.08423848e-06 2.13066208e-07 7.59735426e-06
 3.53741648e-06 2.14972581e-06]
ene_total = [1.73102112 1.4144311  1.77049778 1.59182881 1.42623245 1.45690442
 1.58574596 1.49733708 2.28689603 1.44357775]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 1 obj = 4.633836318316922
eta = 0.6811013020600005
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718006 34168259.23522189]
Done!
At round 30 energy consumption: 4.633836318316938
At round 30 eta: 0.6811013020600005
At round 30 local rounds: 12.575559237281574
At round 30 global rounds: 56.15020557576531
At round 30 a_n: 17.56368160020554
gradient difference: 0.5307780504226685
train() client id: f_00000-0-0 loss: 0.767794  [   32/  126]
train() client id: f_00000-0-1 loss: 1.047528  [   64/  126]
train() client id: f_00000-0-2 loss: 1.086888  [   96/  126]
train() client id: f_00000-1-0 loss: 0.935220  [   32/  126]
train() client id: f_00000-1-1 loss: 0.932227  [   64/  126]
train() client id: f_00000-1-2 loss: 0.973384  [   96/  126]
train() client id: f_00000-2-0 loss: 0.781216  [   32/  126]
train() client id: f_00000-2-1 loss: 0.785132  [   64/  126]
train() client id: f_00000-2-2 loss: 0.964990  [   96/  126]
train() client id: f_00000-3-0 loss: 0.881691  [   32/  126]
train() client id: f_00000-3-1 loss: 0.778309  [   64/  126]
train() client id: f_00000-3-2 loss: 0.878489  [   96/  126]
train() client id: f_00000-4-0 loss: 0.868061  [   32/  126]
train() client id: f_00000-4-1 loss: 0.833985  [   64/  126]
train() client id: f_00000-4-2 loss: 0.718597  [   96/  126]
train() client id: f_00000-5-0 loss: 0.831277  [   32/  126]
train() client id: f_00000-5-1 loss: 0.764594  [   64/  126]
train() client id: f_00000-5-2 loss: 0.766374  [   96/  126]
train() client id: f_00000-6-0 loss: 0.681984  [   32/  126]
train() client id: f_00000-6-1 loss: 0.920109  [   64/  126]
train() client id: f_00000-6-2 loss: 0.750675  [   96/  126]
train() client id: f_00000-7-0 loss: 0.812720  [   32/  126]
train() client id: f_00000-7-1 loss: 0.775439  [   64/  126]
train() client id: f_00000-7-2 loss: 0.793013  [   96/  126]
train() client id: f_00000-8-0 loss: 0.691963  [   32/  126]
train() client id: f_00000-8-1 loss: 0.827713  [   64/  126]
train() client id: f_00000-8-2 loss: 0.808067  [   96/  126]
train() client id: f_00000-9-0 loss: 0.854452  [   32/  126]
train() client id: f_00000-9-1 loss: 0.824848  [   64/  126]
train() client id: f_00000-9-2 loss: 0.676964  [   96/  126]
train() client id: f_00000-10-0 loss: 0.833763  [   32/  126]
train() client id: f_00000-10-1 loss: 0.766203  [   64/  126]
train() client id: f_00000-10-2 loss: 0.800403  [   96/  126]
train() client id: f_00000-11-0 loss: 0.766220  [   32/  126]
train() client id: f_00000-11-1 loss: 0.834166  [   64/  126]
train() client id: f_00000-11-2 loss: 0.764454  [   96/  126]
train() client id: f_00001-0-0 loss: 0.256253  [   32/  265]
train() client id: f_00001-0-1 loss: 0.266122  [   64/  265]
train() client id: f_00001-0-2 loss: 0.366866  [   96/  265]
train() client id: f_00001-0-3 loss: 0.372439  [  128/  265]
train() client id: f_00001-0-4 loss: 0.467203  [  160/  265]
train() client id: f_00001-0-5 loss: 0.365116  [  192/  265]
train() client id: f_00001-0-6 loss: 0.324554  [  224/  265]
train() client id: f_00001-0-7 loss: 0.324022  [  256/  265]
train() client id: f_00001-1-0 loss: 0.327444  [   32/  265]
train() client id: f_00001-1-1 loss: 0.273642  [   64/  265]
train() client id: f_00001-1-2 loss: 0.257853  [   96/  265]
train() client id: f_00001-1-3 loss: 0.411229  [  128/  265]
train() client id: f_00001-1-4 loss: 0.341065  [  160/  265]
train() client id: f_00001-1-5 loss: 0.313239  [  192/  265]
train() client id: f_00001-1-6 loss: 0.365716  [  224/  265]
train() client id: f_00001-1-7 loss: 0.334247  [  256/  265]
train() client id: f_00001-2-0 loss: 0.325186  [   32/  265]
train() client id: f_00001-2-1 loss: 0.288790  [   64/  265]
train() client id: f_00001-2-2 loss: 0.241372  [   96/  265]
train() client id: f_00001-2-3 loss: 0.314077  [  128/  265]
train() client id: f_00001-2-4 loss: 0.444498  [  160/  265]
train() client id: f_00001-2-5 loss: 0.339626  [  192/  265]
train() client id: f_00001-2-6 loss: 0.380637  [  224/  265]
train() client id: f_00001-2-7 loss: 0.284048  [  256/  265]
train() client id: f_00001-3-0 loss: 0.378755  [   32/  265]
train() client id: f_00001-3-1 loss: 0.312497  [   64/  265]
train() client id: f_00001-3-2 loss: 0.296397  [   96/  265]
train() client id: f_00001-3-3 loss: 0.312166  [  128/  265]
train() client id: f_00001-3-4 loss: 0.355201  [  160/  265]
train() client id: f_00001-3-5 loss: 0.277843  [  192/  265]
train() client id: f_00001-3-6 loss: 0.278285  [  224/  265]
train() client id: f_00001-3-7 loss: 0.283032  [  256/  265]
train() client id: f_00001-4-0 loss: 0.325976  [   32/  265]
train() client id: f_00001-4-1 loss: 0.197725  [   64/  265]
train() client id: f_00001-4-2 loss: 0.469923  [   96/  265]
train() client id: f_00001-4-3 loss: 0.296499  [  128/  265]
train() client id: f_00001-4-4 loss: 0.412464  [  160/  265]
train() client id: f_00001-4-5 loss: 0.202294  [  192/  265]
train() client id: f_00001-4-6 loss: 0.248633  [  224/  265]
train() client id: f_00001-4-7 loss: 0.380515  [  256/  265]
train() client id: f_00001-5-0 loss: 0.225333  [   32/  265]
train() client id: f_00001-5-1 loss: 0.227849  [   64/  265]
train() client id: f_00001-5-2 loss: 0.377548  [   96/  265]
train() client id: f_00001-5-3 loss: 0.281852  [  128/  265]
train() client id: f_00001-5-4 loss: 0.279674  [  160/  265]
train() client id: f_00001-5-5 loss: 0.369273  [  192/  265]
train() client id: f_00001-5-6 loss: 0.317860  [  224/  265]
train() client id: f_00001-5-7 loss: 0.434156  [  256/  265]
train() client id: f_00001-6-0 loss: 0.236248  [   32/  265]
train() client id: f_00001-6-1 loss: 0.463935  [   64/  265]
train() client id: f_00001-6-2 loss: 0.222519  [   96/  265]
train() client id: f_00001-6-3 loss: 0.461209  [  128/  265]
train() client id: f_00001-6-4 loss: 0.445662  [  160/  265]
train() client id: f_00001-6-5 loss: 0.208988  [  192/  265]
train() client id: f_00001-6-6 loss: 0.257505  [  224/  265]
train() client id: f_00001-6-7 loss: 0.196680  [  256/  265]
train() client id: f_00001-7-0 loss: 0.347674  [   32/  265]
train() client id: f_00001-7-1 loss: 0.312508  [   64/  265]
train() client id: f_00001-7-2 loss: 0.209494  [   96/  265]
train() client id: f_00001-7-3 loss: 0.257498  [  128/  265]
train() client id: f_00001-7-4 loss: 0.343266  [  160/  265]
train() client id: f_00001-7-5 loss: 0.390241  [  192/  265]
train() client id: f_00001-7-6 loss: 0.231813  [  224/  265]
train() client id: f_00001-7-7 loss: 0.373544  [  256/  265]
train() client id: f_00001-8-0 loss: 0.247296  [   32/  265]
train() client id: f_00001-8-1 loss: 0.215578  [   64/  265]
train() client id: f_00001-8-2 loss: 0.205073  [   96/  265]
train() client id: f_00001-8-3 loss: 0.241298  [  128/  265]
train() client id: f_00001-8-4 loss: 0.398137  [  160/  265]
train() client id: f_00001-8-5 loss: 0.419077  [  192/  265]
train() client id: f_00001-8-6 loss: 0.344962  [  224/  265]
train() client id: f_00001-8-7 loss: 0.313462  [  256/  265]
train() client id: f_00001-9-0 loss: 0.228783  [   32/  265]
train() client id: f_00001-9-1 loss: 0.284900  [   64/  265]
train() client id: f_00001-9-2 loss: 0.498188  [   96/  265]
train() client id: f_00001-9-3 loss: 0.268995  [  128/  265]
train() client id: f_00001-9-4 loss: 0.292246  [  160/  265]
train() client id: f_00001-9-5 loss: 0.207380  [  192/  265]
train() client id: f_00001-9-6 loss: 0.228586  [  224/  265]
train() client id: f_00001-9-7 loss: 0.411975  [  256/  265]
train() client id: f_00001-10-0 loss: 0.209556  [   32/  265]
train() client id: f_00001-10-1 loss: 0.348199  [   64/  265]
train() client id: f_00001-10-2 loss: 0.320245  [   96/  265]
train() client id: f_00001-10-3 loss: 0.383825  [  128/  265]
train() client id: f_00001-10-4 loss: 0.249086  [  160/  265]
train() client id: f_00001-10-5 loss: 0.379551  [  192/  265]
train() client id: f_00001-10-6 loss: 0.248350  [  224/  265]
train() client id: f_00001-10-7 loss: 0.241070  [  256/  265]
train() client id: f_00001-11-0 loss: 0.324729  [   32/  265]
train() client id: f_00001-11-1 loss: 0.223232  [   64/  265]
train() client id: f_00001-11-2 loss: 0.454809  [   96/  265]
train() client id: f_00001-11-3 loss: 0.435453  [  128/  265]
train() client id: f_00001-11-4 loss: 0.242582  [  160/  265]
train() client id: f_00001-11-5 loss: 0.214473  [  192/  265]
train() client id: f_00001-11-6 loss: 0.284727  [  224/  265]
train() client id: f_00001-11-7 loss: 0.271075  [  256/  265]
train() client id: f_00002-0-0 loss: 1.165045  [   32/  124]
train() client id: f_00002-0-1 loss: 1.143024  [   64/  124]
train() client id: f_00002-0-2 loss: 1.116728  [   96/  124]
train() client id: f_00002-1-0 loss: 1.147641  [   32/  124]
train() client id: f_00002-1-1 loss: 1.213853  [   64/  124]
train() client id: f_00002-1-2 loss: 1.143633  [   96/  124]
train() client id: f_00002-2-0 loss: 1.136875  [   32/  124]
train() client id: f_00002-2-1 loss: 1.006194  [   64/  124]
train() client id: f_00002-2-2 loss: 1.260496  [   96/  124]
train() client id: f_00002-3-0 loss: 1.073009  [   32/  124]
train() client id: f_00002-3-1 loss: 1.113649  [   64/  124]
train() client id: f_00002-3-2 loss: 1.185037  [   96/  124]
train() client id: f_00002-4-0 loss: 1.109331  [   32/  124]
train() client id: f_00002-4-1 loss: 1.035333  [   64/  124]
train() client id: f_00002-4-2 loss: 1.157791  [   96/  124]
train() client id: f_00002-5-0 loss: 1.077813  [   32/  124]
train() client id: f_00002-5-1 loss: 0.960373  [   64/  124]
train() client id: f_00002-5-2 loss: 1.138329  [   96/  124]
train() client id: f_00002-6-0 loss: 0.992282  [   32/  124]
train() client id: f_00002-6-1 loss: 1.040210  [   64/  124]
train() client id: f_00002-6-2 loss: 1.020941  [   96/  124]
train() client id: f_00002-7-0 loss: 1.059090  [   32/  124]
train() client id: f_00002-7-1 loss: 1.002776  [   64/  124]
train() client id: f_00002-7-2 loss: 1.082408  [   96/  124]
train() client id: f_00002-8-0 loss: 1.040148  [   32/  124]
train() client id: f_00002-8-1 loss: 1.011116  [   64/  124]
train() client id: f_00002-8-2 loss: 1.031103  [   96/  124]
train() client id: f_00002-9-0 loss: 0.905704  [   32/  124]
train() client id: f_00002-9-1 loss: 1.244161  [   64/  124]
train() client id: f_00002-9-2 loss: 0.973347  [   96/  124]
train() client id: f_00002-10-0 loss: 1.053502  [   32/  124]
train() client id: f_00002-10-1 loss: 0.979558  [   64/  124]
train() client id: f_00002-10-2 loss: 1.003715  [   96/  124]
train() client id: f_00002-11-0 loss: 1.108705  [   32/  124]
train() client id: f_00002-11-1 loss: 1.168993  [   64/  124]
train() client id: f_00002-11-2 loss: 0.928238  [   96/  124]
train() client id: f_00003-0-0 loss: 0.751971  [   32/   43]
train() client id: f_00003-1-0 loss: 0.787189  [   32/   43]
train() client id: f_00003-2-0 loss: 0.681707  [   32/   43]
train() client id: f_00003-3-0 loss: 0.752839  [   32/   43]
train() client id: f_00003-4-0 loss: 0.802453  [   32/   43]
train() client id: f_00003-5-0 loss: 0.691489  [   32/   43]
train() client id: f_00003-6-0 loss: 0.890637  [   32/   43]
train() client id: f_00003-7-0 loss: 0.769043  [   32/   43]
train() client id: f_00003-8-0 loss: 0.790572  [   32/   43]
train() client id: f_00003-9-0 loss: 0.604862  [   32/   43]
train() client id: f_00003-10-0 loss: 0.838939  [   32/   43]
train() client id: f_00003-11-0 loss: 0.730979  [   32/   43]
train() client id: f_00004-0-0 loss: 0.933517  [   32/  306]
train() client id: f_00004-0-1 loss: 0.902867  [   64/  306]
train() client id: f_00004-0-2 loss: 0.927391  [   96/  306]
train() client id: f_00004-0-3 loss: 1.027818  [  128/  306]
train() client id: f_00004-0-4 loss: 1.091668  [  160/  306]
train() client id: f_00004-0-5 loss: 0.960395  [  192/  306]
train() client id: f_00004-0-6 loss: 0.921253  [  224/  306]
train() client id: f_00004-0-7 loss: 0.896907  [  256/  306]
train() client id: f_00004-0-8 loss: 0.943314  [  288/  306]
train() client id: f_00004-1-0 loss: 0.813180  [   32/  306]
train() client id: f_00004-1-1 loss: 0.910646  [   64/  306]
train() client id: f_00004-1-2 loss: 1.145351  [   96/  306]
train() client id: f_00004-1-3 loss: 1.039556  [  128/  306]
train() client id: f_00004-1-4 loss: 0.861866  [  160/  306]
train() client id: f_00004-1-5 loss: 0.882305  [  192/  306]
train() client id: f_00004-1-6 loss: 0.980803  [  224/  306]
train() client id: f_00004-1-7 loss: 0.996195  [  256/  306]
train() client id: f_00004-1-8 loss: 0.864856  [  288/  306]
train() client id: f_00004-2-0 loss: 0.987633  [   32/  306]
train() client id: f_00004-2-1 loss: 0.867709  [   64/  306]
train() client id: f_00004-2-2 loss: 0.986638  [   96/  306]
train() client id: f_00004-2-3 loss: 1.004637  [  128/  306]
train() client id: f_00004-2-4 loss: 0.810376  [  160/  306]
train() client id: f_00004-2-5 loss: 0.952723  [  192/  306]
train() client id: f_00004-2-6 loss: 1.074116  [  224/  306]
train() client id: f_00004-2-7 loss: 0.939345  [  256/  306]
train() client id: f_00004-2-8 loss: 0.876084  [  288/  306]
train() client id: f_00004-3-0 loss: 0.839425  [   32/  306]
train() client id: f_00004-3-1 loss: 0.964722  [   64/  306]
train() client id: f_00004-3-2 loss: 0.858529  [   96/  306]
train() client id: f_00004-3-3 loss: 0.863404  [  128/  306]
train() client id: f_00004-3-4 loss: 0.943621  [  160/  306]
train() client id: f_00004-3-5 loss: 0.958951  [  192/  306]
train() client id: f_00004-3-6 loss: 0.856414  [  224/  306]
train() client id: f_00004-3-7 loss: 1.132533  [  256/  306]
train() client id: f_00004-3-8 loss: 0.993593  [  288/  306]
train() client id: f_00004-4-0 loss: 1.030894  [   32/  306]
train() client id: f_00004-4-1 loss: 0.841126  [   64/  306]
train() client id: f_00004-4-2 loss: 1.016090  [   96/  306]
train() client id: f_00004-4-3 loss: 0.919063  [  128/  306]
train() client id: f_00004-4-4 loss: 0.911167  [  160/  306]
train() client id: f_00004-4-5 loss: 0.966274  [  192/  306]
train() client id: f_00004-4-6 loss: 0.849537  [  224/  306]
train() client id: f_00004-4-7 loss: 0.824870  [  256/  306]
train() client id: f_00004-4-8 loss: 1.027097  [  288/  306]
train() client id: f_00004-5-0 loss: 0.909948  [   32/  306]
train() client id: f_00004-5-1 loss: 1.030393  [   64/  306]
train() client id: f_00004-5-2 loss: 0.910896  [   96/  306]
train() client id: f_00004-5-3 loss: 0.891124  [  128/  306]
train() client id: f_00004-5-4 loss: 0.913110  [  160/  306]
train() client id: f_00004-5-5 loss: 0.949040  [  192/  306]
train() client id: f_00004-5-6 loss: 0.979686  [  224/  306]
train() client id: f_00004-5-7 loss: 0.896848  [  256/  306]
train() client id: f_00004-5-8 loss: 0.972458  [  288/  306]
train() client id: f_00004-6-0 loss: 0.943832  [   32/  306]
train() client id: f_00004-6-1 loss: 0.824668  [   64/  306]
train() client id: f_00004-6-2 loss: 0.881572  [   96/  306]
train() client id: f_00004-6-3 loss: 0.998955  [  128/  306]
train() client id: f_00004-6-4 loss: 0.883090  [  160/  306]
train() client id: f_00004-6-5 loss: 0.973026  [  192/  306]
train() client id: f_00004-6-6 loss: 1.030620  [  224/  306]
train() client id: f_00004-6-7 loss: 0.960670  [  256/  306]
train() client id: f_00004-6-8 loss: 0.934082  [  288/  306]
train() client id: f_00004-7-0 loss: 0.911523  [   32/  306]
train() client id: f_00004-7-1 loss: 1.005622  [   64/  306]
train() client id: f_00004-7-2 loss: 0.944385  [   96/  306]
train() client id: f_00004-7-3 loss: 0.972736  [  128/  306]
train() client id: f_00004-7-4 loss: 1.011969  [  160/  306]
train() client id: f_00004-7-5 loss: 0.870419  [  192/  306]
train() client id: f_00004-7-6 loss: 0.885415  [  224/  306]
train() client id: f_00004-7-7 loss: 0.837176  [  256/  306]
train() client id: f_00004-7-8 loss: 0.987666  [  288/  306]
train() client id: f_00004-8-0 loss: 0.936509  [   32/  306]
train() client id: f_00004-8-1 loss: 0.931359  [   64/  306]
train() client id: f_00004-8-2 loss: 1.033759  [   96/  306]
train() client id: f_00004-8-3 loss: 0.867069  [  128/  306]
train() client id: f_00004-8-4 loss: 0.931875  [  160/  306]
train() client id: f_00004-8-5 loss: 0.984947  [  192/  306]
train() client id: f_00004-8-6 loss: 0.918501  [  224/  306]
train() client id: f_00004-8-7 loss: 0.842245  [  256/  306]
train() client id: f_00004-8-8 loss: 0.899327  [  288/  306]
train() client id: f_00004-9-0 loss: 0.886273  [   32/  306]
train() client id: f_00004-9-1 loss: 0.930024  [   64/  306]
train() client id: f_00004-9-2 loss: 0.810212  [   96/  306]
train() client id: f_00004-9-3 loss: 1.007889  [  128/  306]
train() client id: f_00004-9-4 loss: 1.127782  [  160/  306]
train() client id: f_00004-9-5 loss: 0.884439  [  192/  306]
train() client id: f_00004-9-6 loss: 0.992671  [  224/  306]
train() client id: f_00004-9-7 loss: 0.925247  [  256/  306]
train() client id: f_00004-9-8 loss: 0.885579  [  288/  306]
train() client id: f_00004-10-0 loss: 0.974505  [   32/  306]
train() client id: f_00004-10-1 loss: 1.032835  [   64/  306]
train() client id: f_00004-10-2 loss: 0.952680  [   96/  306]
train() client id: f_00004-10-3 loss: 0.888044  [  128/  306]
train() client id: f_00004-10-4 loss: 0.940673  [  160/  306]
train() client id: f_00004-10-5 loss: 0.910488  [  192/  306]
train() client id: f_00004-10-6 loss: 0.943804  [  224/  306]
train() client id: f_00004-10-7 loss: 0.881967  [  256/  306]
train() client id: f_00004-10-8 loss: 0.803520  [  288/  306]
train() client id: f_00004-11-0 loss: 0.920023  [   32/  306]
train() client id: f_00004-11-1 loss: 0.986386  [   64/  306]
train() client id: f_00004-11-2 loss: 0.840315  [   96/  306]
train() client id: f_00004-11-3 loss: 0.851874  [  128/  306]
train() client id: f_00004-11-4 loss: 0.887720  [  160/  306]
train() client id: f_00004-11-5 loss: 1.008585  [  192/  306]
train() client id: f_00004-11-6 loss: 0.975139  [  224/  306]
train() client id: f_00004-11-7 loss: 0.946872  [  256/  306]
train() client id: f_00004-11-8 loss: 0.907876  [  288/  306]
train() client id: f_00005-0-0 loss: 0.485258  [   32/  146]
train() client id: f_00005-0-1 loss: 0.442523  [   64/  146]
train() client id: f_00005-0-2 loss: 0.509431  [   96/  146]
train() client id: f_00005-0-3 loss: 0.702489  [  128/  146]
train() client id: f_00005-1-0 loss: 0.806371  [   32/  146]
train() client id: f_00005-1-1 loss: 0.507931  [   64/  146]
train() client id: f_00005-1-2 loss: 0.518955  [   96/  146]
train() client id: f_00005-1-3 loss: 0.644613  [  128/  146]
train() client id: f_00005-2-0 loss: 0.490695  [   32/  146]
train() client id: f_00005-2-1 loss: 0.633359  [   64/  146]
train() client id: f_00005-2-2 loss: 0.788946  [   96/  146]
train() client id: f_00005-2-3 loss: 0.476976  [  128/  146]
train() client id: f_00005-3-0 loss: 0.433925  [   32/  146]
train() client id: f_00005-3-1 loss: 0.812977  [   64/  146]
train() client id: f_00005-3-2 loss: 0.520419  [   96/  146]
train() client id: f_00005-3-3 loss: 0.629893  [  128/  146]
train() client id: f_00005-4-0 loss: 0.753203  [   32/  146]
train() client id: f_00005-4-1 loss: 0.435302  [   64/  146]
train() client id: f_00005-4-2 loss: 0.805879  [   96/  146]
train() client id: f_00005-4-3 loss: 0.422309  [  128/  146]
train() client id: f_00005-5-0 loss: 0.445260  [   32/  146]
train() client id: f_00005-5-1 loss: 0.691455  [   64/  146]
train() client id: f_00005-5-2 loss: 0.493807  [   96/  146]
train() client id: f_00005-5-3 loss: 0.648559  [  128/  146]
train() client id: f_00005-6-0 loss: 0.702611  [   32/  146]
train() client id: f_00005-6-1 loss: 0.602595  [   64/  146]
train() client id: f_00005-6-2 loss: 0.415987  [   96/  146]
train() client id: f_00005-6-3 loss: 0.408159  [  128/  146]
train() client id: f_00005-7-0 loss: 0.620807  [   32/  146]
train() client id: f_00005-7-1 loss: 0.423539  [   64/  146]
train() client id: f_00005-7-2 loss: 0.704899  [   96/  146]
train() client id: f_00005-7-3 loss: 0.689458  [  128/  146]
train() client id: f_00005-8-0 loss: 0.566395  [   32/  146]
train() client id: f_00005-8-1 loss: 0.818100  [   64/  146]
train() client id: f_00005-8-2 loss: 0.561123  [   96/  146]
train() client id: f_00005-8-3 loss: 0.660594  [  128/  146]
train() client id: f_00005-9-0 loss: 0.534959  [   32/  146]
train() client id: f_00005-9-1 loss: 0.367098  [   64/  146]
train() client id: f_00005-9-2 loss: 0.611398  [   96/  146]
train() client id: f_00005-9-3 loss: 0.798252  [  128/  146]
train() client id: f_00005-10-0 loss: 0.807118  [   32/  146]
train() client id: f_00005-10-1 loss: 0.597961  [   64/  146]
train() client id: f_00005-10-2 loss: 0.460316  [   96/  146]
train() client id: f_00005-10-3 loss: 0.442642  [  128/  146]
train() client id: f_00005-11-0 loss: 0.567428  [   32/  146]
train() client id: f_00005-11-1 loss: 0.552086  [   64/  146]
train() client id: f_00005-11-2 loss: 0.650054  [   96/  146]
train() client id: f_00005-11-3 loss: 0.558851  [  128/  146]
train() client id: f_00006-0-0 loss: 0.543251  [   32/   54]
train() client id: f_00006-1-0 loss: 0.539421  [   32/   54]
train() client id: f_00006-2-0 loss: 0.454570  [   32/   54]
train() client id: f_00006-3-0 loss: 0.508656  [   32/   54]
train() client id: f_00006-4-0 loss: 0.568486  [   32/   54]
train() client id: f_00006-5-0 loss: 0.511021  [   32/   54]
train() client id: f_00006-6-0 loss: 0.477991  [   32/   54]
train() client id: f_00006-7-0 loss: 0.502089  [   32/   54]
train() client id: f_00006-8-0 loss: 0.515775  [   32/   54]
train() client id: f_00006-9-0 loss: 0.487974  [   32/   54]
train() client id: f_00006-10-0 loss: 0.489838  [   32/   54]
train() client id: f_00006-11-0 loss: 0.487188  [   32/   54]
train() client id: f_00007-0-0 loss: 0.570065  [   32/  179]
train() client id: f_00007-0-1 loss: 0.768894  [   64/  179]
train() client id: f_00007-0-2 loss: 0.451231  [   96/  179]
train() client id: f_00007-0-3 loss: 0.593700  [  128/  179]
train() client id: f_00007-0-4 loss: 0.492547  [  160/  179]
train() client id: f_00007-1-0 loss: 0.581557  [   32/  179]
train() client id: f_00007-1-1 loss: 0.500210  [   64/  179]
train() client id: f_00007-1-2 loss: 0.505471  [   96/  179]
train() client id: f_00007-1-3 loss: 0.509562  [  128/  179]
train() client id: f_00007-1-4 loss: 0.698538  [  160/  179]
train() client id: f_00007-2-0 loss: 0.763483  [   32/  179]
train() client id: f_00007-2-1 loss: 0.459697  [   64/  179]
train() client id: f_00007-2-2 loss: 0.533374  [   96/  179]
train() client id: f_00007-2-3 loss: 0.688392  [  128/  179]
train() client id: f_00007-2-4 loss: 0.418271  [  160/  179]
train() client id: f_00007-3-0 loss: 0.541296  [   32/  179]
train() client id: f_00007-3-1 loss: 0.468236  [   64/  179]
train() client id: f_00007-3-2 loss: 0.596041  [   96/  179]
train() client id: f_00007-3-3 loss: 0.590328  [  128/  179]
train() client id: f_00007-3-4 loss: 0.520921  [  160/  179]
train() client id: f_00007-4-0 loss: 0.492894  [   32/  179]
train() client id: f_00007-4-1 loss: 0.427368  [   64/  179]
train() client id: f_00007-4-2 loss: 0.670310  [   96/  179]
train() client id: f_00007-4-3 loss: 0.558215  [  128/  179]
train() client id: f_00007-4-4 loss: 0.482244  [  160/  179]
train() client id: f_00007-5-0 loss: 0.506631  [   32/  179]
train() client id: f_00007-5-1 loss: 0.413497  [   64/  179]
train() client id: f_00007-5-2 loss: 0.546040  [   96/  179]
train() client id: f_00007-5-3 loss: 0.540765  [  128/  179]
train() client id: f_00007-5-4 loss: 0.636006  [  160/  179]
train() client id: f_00007-6-0 loss: 0.546489  [   32/  179]
train() client id: f_00007-6-1 loss: 0.495681  [   64/  179]
train() client id: f_00007-6-2 loss: 0.562254  [   96/  179]
train() client id: f_00007-6-3 loss: 0.370883  [  128/  179]
train() client id: f_00007-6-4 loss: 0.633250  [  160/  179]
train() client id: f_00007-7-0 loss: 0.569981  [   32/  179]
train() client id: f_00007-7-1 loss: 0.579558  [   64/  179]
train() client id: f_00007-7-2 loss: 0.452207  [   96/  179]
train() client id: f_00007-7-3 loss: 0.478088  [  128/  179]
train() client id: f_00007-7-4 loss: 0.610560  [  160/  179]
train() client id: f_00007-8-0 loss: 0.589909  [   32/  179]
train() client id: f_00007-8-1 loss: 0.592253  [   64/  179]
train() client id: f_00007-8-2 loss: 0.381611  [   96/  179]
train() client id: f_00007-8-3 loss: 0.539528  [  128/  179]
train() client id: f_00007-8-4 loss: 0.398318  [  160/  179]
train() client id: f_00007-9-0 loss: 0.484619  [   32/  179]
train() client id: f_00007-9-1 loss: 0.363958  [   64/  179]
train() client id: f_00007-9-2 loss: 0.796643  [   96/  179]
train() client id: f_00007-9-3 loss: 0.462625  [  128/  179]
train() client id: f_00007-9-4 loss: 0.556711  [  160/  179]
train() client id: f_00007-10-0 loss: 0.542339  [   32/  179]
train() client id: f_00007-10-1 loss: 0.472286  [   64/  179]
train() client id: f_00007-10-2 loss: 0.484063  [   96/  179]
train() client id: f_00007-10-3 loss: 0.612547  [  128/  179]
train() client id: f_00007-10-4 loss: 0.408912  [  160/  179]
train() client id: f_00007-11-0 loss: 0.355438  [   32/  179]
train() client id: f_00007-11-1 loss: 0.459521  [   64/  179]
train() client id: f_00007-11-2 loss: 0.630675  [   96/  179]
train() client id: f_00007-11-3 loss: 0.635870  [  128/  179]
train() client id: f_00007-11-4 loss: 0.492701  [  160/  179]
train() client id: f_00008-0-0 loss: 0.702208  [   32/  130]
train() client id: f_00008-0-1 loss: 0.635875  [   64/  130]
train() client id: f_00008-0-2 loss: 0.781168  [   96/  130]
train() client id: f_00008-0-3 loss: 0.733271  [  128/  130]
train() client id: f_00008-1-0 loss: 0.752407  [   32/  130]
train() client id: f_00008-1-1 loss: 0.784937  [   64/  130]
train() client id: f_00008-1-2 loss: 0.654017  [   96/  130]
train() client id: f_00008-1-3 loss: 0.668505  [  128/  130]
train() client id: f_00008-2-0 loss: 0.806323  [   32/  130]
train() client id: f_00008-2-1 loss: 0.572872  [   64/  130]
train() client id: f_00008-2-2 loss: 0.791171  [   96/  130]
train() client id: f_00008-2-3 loss: 0.716380  [  128/  130]
train() client id: f_00008-3-0 loss: 0.699218  [   32/  130]
train() client id: f_00008-3-1 loss: 0.730023  [   64/  130]
train() client id: f_00008-3-2 loss: 0.679518  [   96/  130]
train() client id: f_00008-3-3 loss: 0.756557  [  128/  130]
train() client id: f_00008-4-0 loss: 0.646388  [   32/  130]
train() client id: f_00008-4-1 loss: 0.727512  [   64/  130]
train() client id: f_00008-4-2 loss: 0.649471  [   96/  130]
train() client id: f_00008-4-3 loss: 0.792653  [  128/  130]
train() client id: f_00008-5-0 loss: 0.738473  [   32/  130]
train() client id: f_00008-5-1 loss: 0.730811  [   64/  130]
train() client id: f_00008-5-2 loss: 0.635290  [   96/  130]
train() client id: f_00008-5-3 loss: 0.775337  [  128/  130]
train() client id: f_00008-6-0 loss: 0.737099  [   32/  130]
train() client id: f_00008-6-1 loss: 0.752438  [   64/  130]
train() client id: f_00008-6-2 loss: 0.701403  [   96/  130]
train() client id: f_00008-6-3 loss: 0.656638  [  128/  130]
train() client id: f_00008-7-0 loss: 0.662519  [   32/  130]
train() client id: f_00008-7-1 loss: 0.891713  [   64/  130]
train() client id: f_00008-7-2 loss: 0.668705  [   96/  130]
train() client id: f_00008-7-3 loss: 0.668184  [  128/  130]
train() client id: f_00008-8-0 loss: 0.657168  [   32/  130]
train() client id: f_00008-8-1 loss: 0.733728  [   64/  130]
train() client id: f_00008-8-2 loss: 0.802841  [   96/  130]
train() client id: f_00008-8-3 loss: 0.654933  [  128/  130]
train() client id: f_00008-9-0 loss: 0.621510  [   32/  130]
train() client id: f_00008-9-1 loss: 0.711386  [   64/  130]
train() client id: f_00008-9-2 loss: 0.729416  [   96/  130]
train() client id: f_00008-9-3 loss: 0.758422  [  128/  130]
train() client id: f_00008-10-0 loss: 0.746148  [   32/  130]
train() client id: f_00008-10-1 loss: 0.657142  [   64/  130]
train() client id: f_00008-10-2 loss: 0.763498  [   96/  130]
train() client id: f_00008-10-3 loss: 0.723613  [  128/  130]
train() client id: f_00008-11-0 loss: 0.720245  [   32/  130]
train() client id: f_00008-11-1 loss: 0.736018  [   64/  130]
train() client id: f_00008-11-2 loss: 0.765180  [   96/  130]
train() client id: f_00008-11-3 loss: 0.667491  [  128/  130]
train() client id: f_00009-0-0 loss: 1.088268  [   32/  118]
train() client id: f_00009-0-1 loss: 0.958200  [   64/  118]
train() client id: f_00009-0-2 loss: 0.989775  [   96/  118]
train() client id: f_00009-1-0 loss: 1.052211  [   32/  118]
train() client id: f_00009-1-1 loss: 0.941458  [   64/  118]
train() client id: f_00009-1-2 loss: 1.008667  [   96/  118]
train() client id: f_00009-2-0 loss: 0.982008  [   32/  118]
train() client id: f_00009-2-1 loss: 0.929705  [   64/  118]
train() client id: f_00009-2-2 loss: 0.952673  [   96/  118]
train() client id: f_00009-3-0 loss: 0.853642  [   32/  118]
train() client id: f_00009-3-1 loss: 0.848526  [   64/  118]
train() client id: f_00009-3-2 loss: 0.870176  [   96/  118]
train() client id: f_00009-4-0 loss: 0.871627  [   32/  118]
train() client id: f_00009-4-1 loss: 0.909660  [   64/  118]
train() client id: f_00009-4-2 loss: 0.899381  [   96/  118]
train() client id: f_00009-5-0 loss: 0.961748  [   32/  118]
train() client id: f_00009-5-1 loss: 0.821263  [   64/  118]
train() client id: f_00009-5-2 loss: 0.879544  [   96/  118]
train() client id: f_00009-6-0 loss: 0.716517  [   32/  118]
train() client id: f_00009-6-1 loss: 0.855148  [   64/  118]
train() client id: f_00009-6-2 loss: 0.863345  [   96/  118]
train() client id: f_00009-7-0 loss: 0.794025  [   32/  118]
train() client id: f_00009-7-1 loss: 0.803678  [   64/  118]
train() client id: f_00009-7-2 loss: 0.801338  [   96/  118]
train() client id: f_00009-8-0 loss: 0.783292  [   32/  118]
train() client id: f_00009-8-1 loss: 0.727910  [   64/  118]
train() client id: f_00009-8-2 loss: 0.773572  [   96/  118]
train() client id: f_00009-9-0 loss: 0.887192  [   32/  118]
train() client id: f_00009-9-1 loss: 0.716756  [   64/  118]
train() client id: f_00009-9-2 loss: 0.670626  [   96/  118]
train() client id: f_00009-10-0 loss: 0.734633  [   32/  118]
train() client id: f_00009-10-1 loss: 0.807372  [   64/  118]
train() client id: f_00009-10-2 loss: 0.761805  [   96/  118]
train() client id: f_00009-11-0 loss: 0.662717  [   32/  118]
train() client id: f_00009-11-1 loss: 0.805112  [   64/  118]
train() client id: f_00009-11-2 loss: 0.779565  [   96/  118]
At round 30 accuracy: 0.6551724137931034
At round 30 training accuracy: 0.5922199865861838
At round 30 training loss: 0.8266002235872925
update_location
xs = [  -3.9056584     4.20031788  170.00902392   18.81129433    0.97929623
    3.95640986 -132.44319194 -111.32485185  154.66397685  -97.06087855]
ys = [ 162.5879595   145.55583871    1.32061395 -132.45517586  124.35018685
  107.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [190.91908951 176.64694974 197.24302835 167.02765761 159.57420841
 147.10385542 165.97617188 149.64591172 185.01247447 139.41601774]
dists_bs = [171.5439666  181.60317332 386.25866427 363.4439475  183.12322702
 191.15264835 182.65190108 185.45182381 365.32568415 187.92285621]
uav_gains = [1.94386836e-11 2.39101861e-11 1.77469892e-11 2.76188258e-11
 3.10151017e-11 3.80704871e-11 2.80673490e-11 3.64667227e-11
 2.11653608e-11 4.35559512e-11]
bs_gains = [6.12410786e-11 5.22093724e-11 6.31008238e-12 7.48288810e-12
 5.10049673e-11 4.52302711e-11 5.13743493e-11 4.92319435e-11
 7.37546672e-12 4.74407113e-11]
Round 31
-------------------------------
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.90000386 14.30569929  6.7966894   2.4461517  16.50004725  7.94229532
  3.0341758   9.71414062  7.16398278  6.44283898]
obj_prev = 81.24602500345188
eta_min = 3.803699202355281e-14	eta_max = 0.9265448863059464
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 18.86522301759883	eta = 0.9090909090909091
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 33.97406433952712	eta = 0.5048027981544223
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 26.610863860443008	eta = 0.6444812477044536
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.281958371636836	eta = 0.6783573681741374
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.213171638427752	eta = 0.6802080670062467
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.212972778010624	eta = 0.680213431961071
eta = 0.680213431961071
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.03179805 0.06687685 0.03129332 0.01085171 0.07722383 0.03684536
 0.01362773 0.04517342 0.03280751 0.02977912]
ene_total = [2.23637289 4.06508358 2.2200981  1.04612844 4.63524192 2.42743785
 1.19692013 2.90387674 2.44646747 2.03534566]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 0 obj = 4.564704952114099
eta = 0.680213431961071
freqs = [37078600.58661079 74869583.15751041 36668274.98538263 12443483.84142069
 86519118.7997421  41447912.13753933 15615771.9937943  51215637.28090309
 40944429.42168844 33444300.55685099]
eta_min = 0.6802134319610765	eta_max = 0.6802134319610728
af = 0.014970687331339992	bf = 1.4539623964515263	zeta = 0.016467756064473992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [2.70316510e-06 2.31799269e-05 2.60170470e-06 1.03898144e-07
 3.57438195e-05 3.91393429e-06 2.05483065e-07 7.32678673e-06
 3.40085688e-06 2.05959391e-06]
ene_total = [1.72390573 1.38338877 1.76415135 1.58337944 1.39239245 1.42109708
 1.57750334 1.48871744 2.26805167 1.40669455]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 1 obj = 4.564704952114124
eta = 0.6802134319610728
freqs = [37078600.58661079 74869583.15751038 36668274.98538262 12443483.84142068
 86519118.79974204 41447912.1375393  15615771.9937943  51215637.28090306
 40944429.42168847 33444300.55685098]
Done!
At round 31 energy consumption: 4.564704952114099
At round 31 eta: 0.6802134319610728
At round 31 local rounds: 12.618272918924527
At round 31 global rounds: 54.923137353497395
At round 31 a_n: 17.221135753236222
gradient difference: 0.4806233048439026
train() client id: f_00000-0-0 loss: 1.260888  [   32/  126]
train() client id: f_00000-0-1 loss: 1.181813  [   64/  126]
train() client id: f_00000-0-2 loss: 1.019207  [   96/  126]
train() client id: f_00000-1-0 loss: 1.180512  [   32/  126]
train() client id: f_00000-1-1 loss: 1.089867  [   64/  126]
train() client id: f_00000-1-2 loss: 0.985612  [   96/  126]
train() client id: f_00000-2-0 loss: 0.958520  [   32/  126]
train() client id: f_00000-2-1 loss: 1.014914  [   64/  126]
train() client id: f_00000-2-2 loss: 1.081076  [   96/  126]
train() client id: f_00000-3-0 loss: 0.958095  [   32/  126]
train() client id: f_00000-3-1 loss: 1.029450  [   64/  126]
train() client id: f_00000-3-2 loss: 0.909706  [   96/  126]
train() client id: f_00000-4-0 loss: 0.854117  [   32/  126]
train() client id: f_00000-4-1 loss: 1.003177  [   64/  126]
train() client id: f_00000-4-2 loss: 0.950159  [   96/  126]
train() client id: f_00000-5-0 loss: 0.873483  [   32/  126]
train() client id: f_00000-5-1 loss: 0.946164  [   64/  126]
train() client id: f_00000-5-2 loss: 0.902360  [   96/  126]
train() client id: f_00000-6-0 loss: 0.840562  [   32/  126]
train() client id: f_00000-6-1 loss: 0.877837  [   64/  126]
train() client id: f_00000-6-2 loss: 0.889581  [   96/  126]
train() client id: f_00000-7-0 loss: 0.938637  [   32/  126]
train() client id: f_00000-7-1 loss: 0.839049  [   64/  126]
train() client id: f_00000-7-2 loss: 0.829928  [   96/  126]
train() client id: f_00000-8-0 loss: 0.969183  [   32/  126]
train() client id: f_00000-8-1 loss: 0.895245  [   64/  126]
train() client id: f_00000-8-2 loss: 0.865536  [   96/  126]
train() client id: f_00000-9-0 loss: 0.908218  [   32/  126]
train() client id: f_00000-9-1 loss: 0.872620  [   64/  126]
train() client id: f_00000-9-2 loss: 0.902872  [   96/  126]
train() client id: f_00000-10-0 loss: 0.937564  [   32/  126]
train() client id: f_00000-10-1 loss: 0.855961  [   64/  126]
train() client id: f_00000-10-2 loss: 0.828168  [   96/  126]
train() client id: f_00000-11-0 loss: 0.917683  [   32/  126]
train() client id: f_00000-11-1 loss: 0.882383  [   64/  126]
train() client id: f_00000-11-2 loss: 0.816342  [   96/  126]
train() client id: f_00001-0-0 loss: 0.473210  [   32/  265]
train() client id: f_00001-0-1 loss: 0.396994  [   64/  265]
train() client id: f_00001-0-2 loss: 0.464219  [   96/  265]
train() client id: f_00001-0-3 loss: 0.548888  [  128/  265]
train() client id: f_00001-0-4 loss: 0.389065  [  160/  265]
train() client id: f_00001-0-5 loss: 0.476986  [  192/  265]
train() client id: f_00001-0-6 loss: 0.584809  [  224/  265]
train() client id: f_00001-0-7 loss: 0.591506  [  256/  265]
train() client id: f_00001-1-0 loss: 0.482749  [   32/  265]
train() client id: f_00001-1-1 loss: 0.431721  [   64/  265]
train() client id: f_00001-1-2 loss: 0.580058  [   96/  265]
train() client id: f_00001-1-3 loss: 0.508873  [  128/  265]
train() client id: f_00001-1-4 loss: 0.480848  [  160/  265]
train() client id: f_00001-1-5 loss: 0.502531  [  192/  265]
train() client id: f_00001-1-6 loss: 0.394653  [  224/  265]
train() client id: f_00001-1-7 loss: 0.483113  [  256/  265]
train() client id: f_00001-2-0 loss: 0.516034  [   32/  265]
train() client id: f_00001-2-1 loss: 0.452712  [   64/  265]
train() client id: f_00001-2-2 loss: 0.521963  [   96/  265]
train() client id: f_00001-2-3 loss: 0.412534  [  128/  265]
train() client id: f_00001-2-4 loss: 0.589896  [  160/  265]
train() client id: f_00001-2-5 loss: 0.492369  [  192/  265]
train() client id: f_00001-2-6 loss: 0.448159  [  224/  265]
train() client id: f_00001-2-7 loss: 0.369647  [  256/  265]
train() client id: f_00001-3-0 loss: 0.377276  [   32/  265]
train() client id: f_00001-3-1 loss: 0.506032  [   64/  265]
train() client id: f_00001-3-2 loss: 0.483581  [   96/  265]
train() client id: f_00001-3-3 loss: 0.453331  [  128/  265]
train() client id: f_00001-3-4 loss: 0.507789  [  160/  265]
train() client id: f_00001-3-5 loss: 0.520314  [  192/  265]
train() client id: f_00001-3-6 loss: 0.373919  [  224/  265]
train() client id: f_00001-3-7 loss: 0.543083  [  256/  265]
train() client id: f_00001-4-0 loss: 0.409637  [   32/  265]
train() client id: f_00001-4-1 loss: 0.369754  [   64/  265]
train() client id: f_00001-4-2 loss: 0.553729  [   96/  265]
train() client id: f_00001-4-3 loss: 0.492888  [  128/  265]
train() client id: f_00001-4-4 loss: 0.384683  [  160/  265]
train() client id: f_00001-4-5 loss: 0.484297  [  192/  265]
train() client id: f_00001-4-6 loss: 0.622595  [  224/  265]
train() client id: f_00001-4-7 loss: 0.423143  [  256/  265]
train() client id: f_00001-5-0 loss: 0.367176  [   32/  265]
train() client id: f_00001-5-1 loss: 0.367139  [   64/  265]
train() client id: f_00001-5-2 loss: 0.579449  [   96/  265]
train() client id: f_00001-5-3 loss: 0.557352  [  128/  265]
train() client id: f_00001-5-4 loss: 0.505745  [  160/  265]
train() client id: f_00001-5-5 loss: 0.411713  [  192/  265]
train() client id: f_00001-5-6 loss: 0.394953  [  224/  265]
train() client id: f_00001-5-7 loss: 0.533142  [  256/  265]
train() client id: f_00001-6-0 loss: 0.500657  [   32/  265]
train() client id: f_00001-6-1 loss: 0.426068  [   64/  265]
train() client id: f_00001-6-2 loss: 0.352926  [   96/  265]
train() client id: f_00001-6-3 loss: 0.454525  [  128/  265]
train() client id: f_00001-6-4 loss: 0.504808  [  160/  265]
train() client id: f_00001-6-5 loss: 0.537210  [  192/  265]
train() client id: f_00001-6-6 loss: 0.555244  [  224/  265]
train() client id: f_00001-6-7 loss: 0.366053  [  256/  265]
train() client id: f_00001-7-0 loss: 0.566164  [   32/  265]
train() client id: f_00001-7-1 loss: 0.443935  [   64/  265]
train() client id: f_00001-7-2 loss: 0.472961  [   96/  265]
train() client id: f_00001-7-3 loss: 0.503246  [  128/  265]
train() client id: f_00001-7-4 loss: 0.377310  [  160/  265]
train() client id: f_00001-7-5 loss: 0.373474  [  192/  265]
train() client id: f_00001-7-6 loss: 0.416445  [  224/  265]
train() client id: f_00001-7-7 loss: 0.533802  [  256/  265]
train() client id: f_00001-8-0 loss: 0.365368  [   32/  265]
train() client id: f_00001-8-1 loss: 0.453344  [   64/  265]
train() client id: f_00001-8-2 loss: 0.499931  [   96/  265]
train() client id: f_00001-8-3 loss: 0.502960  [  128/  265]
train() client id: f_00001-8-4 loss: 0.545801  [  160/  265]
train() client id: f_00001-8-5 loss: 0.371090  [  192/  265]
train() client id: f_00001-8-6 loss: 0.461108  [  224/  265]
train() client id: f_00001-8-7 loss: 0.362633  [  256/  265]
train() client id: f_00001-9-0 loss: 0.441985  [   32/  265]
train() client id: f_00001-9-1 loss: 0.453304  [   64/  265]
train() client id: f_00001-9-2 loss: 0.558419  [   96/  265]
train() client id: f_00001-9-3 loss: 0.472381  [  128/  265]
train() client id: f_00001-9-4 loss: 0.492148  [  160/  265]
train() client id: f_00001-9-5 loss: 0.518192  [  192/  265]
train() client id: f_00001-9-6 loss: 0.377348  [  224/  265]
train() client id: f_00001-9-7 loss: 0.362732  [  256/  265]
train() client id: f_00001-10-0 loss: 0.573851  [   32/  265]
train() client id: f_00001-10-1 loss: 0.424497  [   64/  265]
train() client id: f_00001-10-2 loss: 0.471943  [   96/  265]
train() client id: f_00001-10-3 loss: 0.503144  [  128/  265]
train() client id: f_00001-10-4 loss: 0.365849  [  160/  265]
train() client id: f_00001-10-5 loss: 0.442951  [  192/  265]
train() client id: f_00001-10-6 loss: 0.513609  [  224/  265]
train() client id: f_00001-10-7 loss: 0.369518  [  256/  265]
train() client id: f_00001-11-0 loss: 0.447836  [   32/  265]
train() client id: f_00001-11-1 loss: 0.424340  [   64/  265]
train() client id: f_00001-11-2 loss: 0.362302  [   96/  265]
train() client id: f_00001-11-3 loss: 0.410817  [  128/  265]
train() client id: f_00001-11-4 loss: 0.544388  [  160/  265]
train() client id: f_00001-11-5 loss: 0.468826  [  192/  265]
train() client id: f_00001-11-6 loss: 0.409011  [  224/  265]
train() client id: f_00001-11-7 loss: 0.609620  [  256/  265]
train() client id: f_00002-0-0 loss: 1.337327  [   32/  124]
train() client id: f_00002-0-1 loss: 1.207884  [   64/  124]
train() client id: f_00002-0-2 loss: 1.327711  [   96/  124]
train() client id: f_00002-1-0 loss: 1.196820  [   32/  124]
train() client id: f_00002-1-1 loss: 1.349864  [   64/  124]
train() client id: f_00002-1-2 loss: 1.222130  [   96/  124]
train() client id: f_00002-2-0 loss: 1.303164  [   32/  124]
train() client id: f_00002-2-1 loss: 1.262296  [   64/  124]
train() client id: f_00002-2-2 loss: 1.089527  [   96/  124]
train() client id: f_00002-3-0 loss: 1.216721  [   32/  124]
train() client id: f_00002-3-1 loss: 1.181607  [   64/  124]
train() client id: f_00002-3-2 loss: 1.231140  [   96/  124]
train() client id: f_00002-4-0 loss: 1.165757  [   32/  124]
train() client id: f_00002-4-1 loss: 1.102070  [   64/  124]
train() client id: f_00002-4-2 loss: 1.257287  [   96/  124]
train() client id: f_00002-5-0 loss: 1.116286  [   32/  124]
train() client id: f_00002-5-1 loss: 1.151230  [   64/  124]
train() client id: f_00002-5-2 loss: 0.946849  [   96/  124]
train() client id: f_00002-6-0 loss: 1.043943  [   32/  124]
train() client id: f_00002-6-1 loss: 1.173292  [   64/  124]
train() client id: f_00002-6-2 loss: 1.142528  [   96/  124]
train() client id: f_00002-7-0 loss: 1.164905  [   32/  124]
train() client id: f_00002-7-1 loss: 1.050921  [   64/  124]
train() client id: f_00002-7-2 loss: 1.064099  [   96/  124]
train() client id: f_00002-8-0 loss: 1.104746  [   32/  124]
train() client id: f_00002-8-1 loss: 0.907536  [   64/  124]
train() client id: f_00002-8-2 loss: 1.226889  [   96/  124]
train() client id: f_00002-9-0 loss: 1.125509  [   32/  124]
train() client id: f_00002-9-1 loss: 1.151968  [   64/  124]
train() client id: f_00002-9-2 loss: 1.007198  [   96/  124]
train() client id: f_00002-10-0 loss: 1.080036  [   32/  124]
train() client id: f_00002-10-1 loss: 1.100692  [   64/  124]
train() client id: f_00002-10-2 loss: 1.138309  [   96/  124]
train() client id: f_00002-11-0 loss: 1.069963  [   32/  124]
train() client id: f_00002-11-1 loss: 1.221435  [   64/  124]
train() client id: f_00002-11-2 loss: 1.005303  [   96/  124]
train() client id: f_00003-0-0 loss: 0.900702  [   32/   43]
train() client id: f_00003-1-0 loss: 0.822804  [   32/   43]
train() client id: f_00003-2-0 loss: 0.602160  [   32/   43]
train() client id: f_00003-3-0 loss: 0.804232  [   32/   43]
train() client id: f_00003-4-0 loss: 0.588069  [   32/   43]
train() client id: f_00003-5-0 loss: 0.696447  [   32/   43]
train() client id: f_00003-6-0 loss: 0.659195  [   32/   43]
train() client id: f_00003-7-0 loss: 0.751667  [   32/   43]
train() client id: f_00003-8-0 loss: 0.739402  [   32/   43]
train() client id: f_00003-9-0 loss: 0.777034  [   32/   43]
train() client id: f_00003-10-0 loss: 0.874796  [   32/   43]
train() client id: f_00003-11-0 loss: 0.784928  [   32/   43]
train() client id: f_00004-0-0 loss: 0.666273  [   32/  306]
train() client id: f_00004-0-1 loss: 0.975082  [   64/  306]
train() client id: f_00004-0-2 loss: 0.699033  [   96/  306]
train() client id: f_00004-0-3 loss: 0.839367  [  128/  306]
train() client id: f_00004-0-4 loss: 0.865617  [  160/  306]
train() client id: f_00004-0-5 loss: 0.935926  [  192/  306]
train() client id: f_00004-0-6 loss: 0.938388  [  224/  306]
train() client id: f_00004-0-7 loss: 0.807367  [  256/  306]
train() client id: f_00004-0-8 loss: 0.824918  [  288/  306]
train() client id: f_00004-1-0 loss: 0.839416  [   32/  306]
train() client id: f_00004-1-1 loss: 0.839101  [   64/  306]
train() client id: f_00004-1-2 loss: 0.819436  [   96/  306]
train() client id: f_00004-1-3 loss: 0.946134  [  128/  306]
train() client id: f_00004-1-4 loss: 0.808852  [  160/  306]
train() client id: f_00004-1-5 loss: 0.968202  [  192/  306]
train() client id: f_00004-1-6 loss: 0.725514  [  224/  306]
train() client id: f_00004-1-7 loss: 0.790075  [  256/  306]
train() client id: f_00004-1-8 loss: 0.825844  [  288/  306]
train() client id: f_00004-2-0 loss: 1.003639  [   32/  306]
train() client id: f_00004-2-1 loss: 0.745451  [   64/  306]
train() client id: f_00004-2-2 loss: 0.941026  [   96/  306]
train() client id: f_00004-2-3 loss: 0.874991  [  128/  306]
train() client id: f_00004-2-4 loss: 0.907630  [  160/  306]
train() client id: f_00004-2-5 loss: 0.841580  [  192/  306]
train() client id: f_00004-2-6 loss: 0.745993  [  224/  306]
train() client id: f_00004-2-7 loss: 0.814739  [  256/  306]
train() client id: f_00004-2-8 loss: 0.695399  [  288/  306]
train() client id: f_00004-3-0 loss: 0.861926  [   32/  306]
train() client id: f_00004-3-1 loss: 0.831494  [   64/  306]
train() client id: f_00004-3-2 loss: 0.789808  [   96/  306]
train() client id: f_00004-3-3 loss: 0.936187  [  128/  306]
train() client id: f_00004-3-4 loss: 0.747839  [  160/  306]
train() client id: f_00004-3-5 loss: 0.871578  [  192/  306]
train() client id: f_00004-3-6 loss: 0.874840  [  224/  306]
train() client id: f_00004-3-7 loss: 0.768050  [  256/  306]
train() client id: f_00004-3-8 loss: 0.853250  [  288/  306]
train() client id: f_00004-4-0 loss: 0.838646  [   32/  306]
train() client id: f_00004-4-1 loss: 0.743519  [   64/  306]
train() client id: f_00004-4-2 loss: 0.956898  [   96/  306]
train() client id: f_00004-4-3 loss: 0.830616  [  128/  306]
train() client id: f_00004-4-4 loss: 0.799791  [  160/  306]
train() client id: f_00004-4-5 loss: 0.908798  [  192/  306]
train() client id: f_00004-4-6 loss: 0.892780  [  224/  306]
train() client id: f_00004-4-7 loss: 0.791376  [  256/  306]
train() client id: f_00004-4-8 loss: 0.871874  [  288/  306]
train() client id: f_00004-5-0 loss: 0.865579  [   32/  306]
train() client id: f_00004-5-1 loss: 0.734209  [   64/  306]
train() client id: f_00004-5-2 loss: 0.773366  [   96/  306]
train() client id: f_00004-5-3 loss: 0.918900  [  128/  306]
train() client id: f_00004-5-4 loss: 0.858437  [  160/  306]
train() client id: f_00004-5-5 loss: 0.722715  [  192/  306]
train() client id: f_00004-5-6 loss: 0.896484  [  224/  306]
train() client id: f_00004-5-7 loss: 0.841954  [  256/  306]
train() client id: f_00004-5-8 loss: 0.935011  [  288/  306]
train() client id: f_00004-6-0 loss: 0.918259  [   32/  306]
train() client id: f_00004-6-1 loss: 0.833470  [   64/  306]
train() client id: f_00004-6-2 loss: 0.899474  [   96/  306]
train() client id: f_00004-6-3 loss: 0.876176  [  128/  306]
train() client id: f_00004-6-4 loss: 0.777581  [  160/  306]
train() client id: f_00004-6-5 loss: 0.846614  [  192/  306]
train() client id: f_00004-6-6 loss: 0.739839  [  224/  306]
train() client id: f_00004-6-7 loss: 0.981164  [  256/  306]
train() client id: f_00004-6-8 loss: 0.700176  [  288/  306]
train() client id: f_00004-7-0 loss: 0.781653  [   32/  306]
train() client id: f_00004-7-1 loss: 0.901694  [   64/  306]
train() client id: f_00004-7-2 loss: 0.903346  [   96/  306]
train() client id: f_00004-7-3 loss: 0.944082  [  128/  306]
train() client id: f_00004-7-4 loss: 0.828372  [  160/  306]
train() client id: f_00004-7-5 loss: 0.732539  [  192/  306]
train() client id: f_00004-7-6 loss: 0.848722  [  224/  306]
train() client id: f_00004-7-7 loss: 0.876045  [  256/  306]
train() client id: f_00004-7-8 loss: 0.786370  [  288/  306]
train() client id: f_00004-8-0 loss: 0.862169  [   32/  306]
train() client id: f_00004-8-1 loss: 0.912446  [   64/  306]
train() client id: f_00004-8-2 loss: 0.801772  [   96/  306]
train() client id: f_00004-8-3 loss: 0.732280  [  128/  306]
train() client id: f_00004-8-4 loss: 0.870160  [  160/  306]
train() client id: f_00004-8-5 loss: 0.898926  [  192/  306]
train() client id: f_00004-8-6 loss: 0.895456  [  224/  306]
train() client id: f_00004-8-7 loss: 0.793300  [  256/  306]
train() client id: f_00004-8-8 loss: 0.858514  [  288/  306]
train() client id: f_00004-9-0 loss: 0.881753  [   32/  306]
train() client id: f_00004-9-1 loss: 0.788318  [   64/  306]
train() client id: f_00004-9-2 loss: 0.723714  [   96/  306]
train() client id: f_00004-9-3 loss: 0.861264  [  128/  306]
train() client id: f_00004-9-4 loss: 0.765329  [  160/  306]
train() client id: f_00004-9-5 loss: 0.932480  [  192/  306]
train() client id: f_00004-9-6 loss: 0.807565  [  224/  306]
train() client id: f_00004-9-7 loss: 0.955533  [  256/  306]
train() client id: f_00004-9-8 loss: 0.853960  [  288/  306]
train() client id: f_00004-10-0 loss: 0.764709  [   32/  306]
train() client id: f_00004-10-1 loss: 0.848238  [   64/  306]
train() client id: f_00004-10-2 loss: 0.896061  [   96/  306]
train() client id: f_00004-10-3 loss: 0.856100  [  128/  306]
train() client id: f_00004-10-4 loss: 0.767028  [  160/  306]
train() client id: f_00004-10-5 loss: 0.710046  [  192/  306]
train() client id: f_00004-10-6 loss: 0.955664  [  224/  306]
train() client id: f_00004-10-7 loss: 0.836915  [  256/  306]
train() client id: f_00004-10-8 loss: 0.904478  [  288/  306]
train() client id: f_00004-11-0 loss: 0.916317  [   32/  306]
train() client id: f_00004-11-1 loss: 0.887783  [   64/  306]
train() client id: f_00004-11-2 loss: 0.709365  [   96/  306]
train() client id: f_00004-11-3 loss: 0.781009  [  128/  306]
train() client id: f_00004-11-4 loss: 0.930573  [  160/  306]
train() client id: f_00004-11-5 loss: 0.895776  [  192/  306]
train() client id: f_00004-11-6 loss: 0.898320  [  224/  306]
train() client id: f_00004-11-7 loss: 0.811503  [  256/  306]
train() client id: f_00004-11-8 loss: 0.721017  [  288/  306]
train() client id: f_00005-0-0 loss: 0.768091  [   32/  146]
train() client id: f_00005-0-1 loss: 0.764465  [   64/  146]
train() client id: f_00005-0-2 loss: 0.651362  [   96/  146]
train() client id: f_00005-0-3 loss: 0.564854  [  128/  146]
train() client id: f_00005-1-0 loss: 0.988861  [   32/  146]
train() client id: f_00005-1-1 loss: 0.787388  [   64/  146]
train() client id: f_00005-1-2 loss: 0.512616  [   96/  146]
train() client id: f_00005-1-3 loss: 0.514827  [  128/  146]
train() client id: f_00005-2-0 loss: 0.587803  [   32/  146]
train() client id: f_00005-2-1 loss: 0.629130  [   64/  146]
train() client id: f_00005-2-2 loss: 0.635569  [   96/  146]
train() client id: f_00005-2-3 loss: 0.765517  [  128/  146]
train() client id: f_00005-3-0 loss: 0.555507  [   32/  146]
train() client id: f_00005-3-1 loss: 0.784562  [   64/  146]
train() client id: f_00005-3-2 loss: 0.786310  [   96/  146]
train() client id: f_00005-3-3 loss: 0.726705  [  128/  146]
train() client id: f_00005-4-0 loss: 0.714345  [   32/  146]
train() client id: f_00005-4-1 loss: 0.750042  [   64/  146]
train() client id: f_00005-4-2 loss: 0.674500  [   96/  146]
train() client id: f_00005-4-3 loss: 0.561055  [  128/  146]
train() client id: f_00005-5-0 loss: 0.774437  [   32/  146]
train() client id: f_00005-5-1 loss: 0.543696  [   64/  146]
train() client id: f_00005-5-2 loss: 0.781991  [   96/  146]
train() client id: f_00005-5-3 loss: 0.463783  [  128/  146]
train() client id: f_00005-6-0 loss: 0.861360  [   32/  146]
train() client id: f_00005-6-1 loss: 0.619074  [   64/  146]
train() client id: f_00005-6-2 loss: 0.691841  [   96/  146]
train() client id: f_00005-6-3 loss: 0.633804  [  128/  146]
train() client id: f_00005-7-0 loss: 0.640455  [   32/  146]
train() client id: f_00005-7-1 loss: 0.529158  [   64/  146]
train() client id: f_00005-7-2 loss: 0.777373  [   96/  146]
train() client id: f_00005-7-3 loss: 0.698698  [  128/  146]
train() client id: f_00005-8-0 loss: 0.767510  [   32/  146]
train() client id: f_00005-8-1 loss: 0.688324  [   64/  146]
train() client id: f_00005-8-2 loss: 0.488658  [   96/  146]
train() client id: f_00005-8-3 loss: 0.725071  [  128/  146]
train() client id: f_00005-9-0 loss: 0.583536  [   32/  146]
train() client id: f_00005-9-1 loss: 0.879811  [   64/  146]
train() client id: f_00005-9-2 loss: 0.564397  [   96/  146]
train() client id: f_00005-9-3 loss: 0.645740  [  128/  146]
train() client id: f_00005-10-0 loss: 0.548394  [   32/  146]
train() client id: f_00005-10-1 loss: 0.879989  [   64/  146]
train() client id: f_00005-10-2 loss: 0.763234  [   96/  146]
train() client id: f_00005-10-3 loss: 0.590066  [  128/  146]
train() client id: f_00005-11-0 loss: 0.548307  [   32/  146]
train() client id: f_00005-11-1 loss: 0.724672  [   64/  146]
train() client id: f_00005-11-2 loss: 0.514686  [   96/  146]
train() client id: f_00005-11-3 loss: 0.972116  [  128/  146]
train() client id: f_00006-0-0 loss: 0.505629  [   32/   54]
train() client id: f_00006-1-0 loss: 0.515329  [   32/   54]
train() client id: f_00006-2-0 loss: 0.513156  [   32/   54]
train() client id: f_00006-3-0 loss: 0.575527  [   32/   54]
train() client id: f_00006-4-0 loss: 0.547117  [   32/   54]
train() client id: f_00006-5-0 loss: 0.545684  [   32/   54]
train() client id: f_00006-6-0 loss: 0.559744  [   32/   54]
train() client id: f_00006-7-0 loss: 0.582397  [   32/   54]
train() client id: f_00006-8-0 loss: 0.518987  [   32/   54]
train() client id: f_00006-9-0 loss: 0.527333  [   32/   54]
train() client id: f_00006-10-0 loss: 0.529296  [   32/   54]
train() client id: f_00006-11-0 loss: 0.539079  [   32/   54]
train() client id: f_00007-0-0 loss: 0.507179  [   32/  179]
train() client id: f_00007-0-1 loss: 0.555571  [   64/  179]
train() client id: f_00007-0-2 loss: 0.618058  [   96/  179]
train() client id: f_00007-0-3 loss: 0.657192  [  128/  179]
train() client id: f_00007-0-4 loss: 0.831965  [  160/  179]
train() client id: f_00007-1-0 loss: 0.562634  [   32/  179]
train() client id: f_00007-1-1 loss: 0.705091  [   64/  179]
train() client id: f_00007-1-2 loss: 0.672767  [   96/  179]
train() client id: f_00007-1-3 loss: 0.640113  [  128/  179]
train() client id: f_00007-1-4 loss: 0.663477  [  160/  179]
train() client id: f_00007-2-0 loss: 0.671187  [   32/  179]
train() client id: f_00007-2-1 loss: 0.683084  [   64/  179]
train() client id: f_00007-2-2 loss: 0.606653  [   96/  179]
train() client id: f_00007-2-3 loss: 0.538767  [  128/  179]
train() client id: f_00007-2-4 loss: 0.682809  [  160/  179]
train() client id: f_00007-3-0 loss: 0.653857  [   32/  179]
train() client id: f_00007-3-1 loss: 0.590093  [   64/  179]
train() client id: f_00007-3-2 loss: 0.572932  [   96/  179]
train() client id: f_00007-3-3 loss: 0.538286  [  128/  179]
train() client id: f_00007-3-4 loss: 0.573494  [  160/  179]
train() client id: f_00007-4-0 loss: 0.745556  [   32/  179]
train() client id: f_00007-4-1 loss: 0.599609  [   64/  179]
train() client id: f_00007-4-2 loss: 0.627520  [   96/  179]
train() client id: f_00007-4-3 loss: 0.567020  [  128/  179]
train() client id: f_00007-4-4 loss: 0.609476  [  160/  179]
train() client id: f_00007-5-0 loss: 0.494099  [   32/  179]
train() client id: f_00007-5-1 loss: 0.745005  [   64/  179]
train() client id: f_00007-5-2 loss: 0.640689  [   96/  179]
train() client id: f_00007-5-3 loss: 0.503279  [  128/  179]
train() client id: f_00007-5-4 loss: 0.803589  [  160/  179]
train() client id: f_00007-6-0 loss: 0.559904  [   32/  179]
train() client id: f_00007-6-1 loss: 0.524450  [   64/  179]
train() client id: f_00007-6-2 loss: 0.594877  [   96/  179]
train() client id: f_00007-6-3 loss: 0.786421  [  128/  179]
train() client id: f_00007-6-4 loss: 0.677086  [  160/  179]
train() client id: f_00007-7-0 loss: 0.563922  [   32/  179]
train() client id: f_00007-7-1 loss: 0.672679  [   64/  179]
train() client id: f_00007-7-2 loss: 0.438418  [   96/  179]
train() client id: f_00007-7-3 loss: 0.682505  [  128/  179]
train() client id: f_00007-7-4 loss: 0.700225  [  160/  179]
train() client id: f_00007-8-0 loss: 0.466596  [   32/  179]
train() client id: f_00007-8-1 loss: 0.479734  [   64/  179]
train() client id: f_00007-8-2 loss: 0.622651  [   96/  179]
train() client id: f_00007-8-3 loss: 0.789552  [  128/  179]
train() client id: f_00007-8-4 loss: 0.674336  [  160/  179]
train() client id: f_00007-9-0 loss: 0.962085  [   32/  179]
train() client id: f_00007-9-1 loss: 0.557176  [   64/  179]
train() client id: f_00007-9-2 loss: 0.538395  [   96/  179]
train() client id: f_00007-9-3 loss: 0.590909  [  128/  179]
train() client id: f_00007-9-4 loss: 0.461129  [  160/  179]
train() client id: f_00007-10-0 loss: 0.640039  [   32/  179]
train() client id: f_00007-10-1 loss: 0.621392  [   64/  179]
train() client id: f_00007-10-2 loss: 0.485431  [   96/  179]
train() client id: f_00007-10-3 loss: 0.782380  [  128/  179]
train() client id: f_00007-10-4 loss: 0.581729  [  160/  179]
train() client id: f_00007-11-0 loss: 0.446426  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582329  [   64/  179]
train() client id: f_00007-11-2 loss: 0.690494  [   96/  179]
train() client id: f_00007-11-3 loss: 0.823741  [  128/  179]
train() client id: f_00007-11-4 loss: 0.492067  [  160/  179]
train() client id: f_00008-0-0 loss: 0.784145  [   32/  130]
train() client id: f_00008-0-1 loss: 0.860505  [   64/  130]
train() client id: f_00008-0-2 loss: 0.802901  [   96/  130]
train() client id: f_00008-0-3 loss: 0.768662  [  128/  130]
train() client id: f_00008-1-0 loss: 0.877840  [   32/  130]
train() client id: f_00008-1-1 loss: 0.692620  [   64/  130]
train() client id: f_00008-1-2 loss: 0.820891  [   96/  130]
train() client id: f_00008-1-3 loss: 0.829362  [  128/  130]
train() client id: f_00008-2-0 loss: 0.823312  [   32/  130]
train() client id: f_00008-2-1 loss: 0.787133  [   64/  130]
train() client id: f_00008-2-2 loss: 0.811904  [   96/  130]
train() client id: f_00008-2-3 loss: 0.791154  [  128/  130]
train() client id: f_00008-3-0 loss: 0.866796  [   32/  130]
train() client id: f_00008-3-1 loss: 0.752186  [   64/  130]
train() client id: f_00008-3-2 loss: 0.743231  [   96/  130]
train() client id: f_00008-3-3 loss: 0.829266  [  128/  130]
train() client id: f_00008-4-0 loss: 0.868258  [   32/  130]
train() client id: f_00008-4-1 loss: 0.715419  [   64/  130]
train() client id: f_00008-4-2 loss: 0.889340  [   96/  130]
train() client id: f_00008-4-3 loss: 0.737649  [  128/  130]
train() client id: f_00008-5-0 loss: 0.957618  [   32/  130]
train() client id: f_00008-5-1 loss: 0.667796  [   64/  130]
train() client id: f_00008-5-2 loss: 0.725978  [   96/  130]
train() client id: f_00008-5-3 loss: 0.811751  [  128/  130]
train() client id: f_00008-6-0 loss: 0.787329  [   32/  130]
train() client id: f_00008-6-1 loss: 0.825948  [   64/  130]
train() client id: f_00008-6-2 loss: 0.807699  [   96/  130]
train() client id: f_00008-6-3 loss: 0.796322  [  128/  130]
train() client id: f_00008-7-0 loss: 0.836376  [   32/  130]
train() client id: f_00008-7-1 loss: 0.745284  [   64/  130]
train() client id: f_00008-7-2 loss: 0.785322  [   96/  130]
train() client id: f_00008-7-3 loss: 0.810708  [  128/  130]
train() client id: f_00008-8-0 loss: 0.912758  [   32/  130]
train() client id: f_00008-8-1 loss: 0.836633  [   64/  130]
train() client id: f_00008-8-2 loss: 0.752674  [   96/  130]
train() client id: f_00008-8-3 loss: 0.704723  [  128/  130]
train() client id: f_00008-9-0 loss: 0.831366  [   32/  130]
train() client id: f_00008-9-1 loss: 0.705883  [   64/  130]
train() client id: f_00008-9-2 loss: 0.873339  [   96/  130]
train() client id: f_00008-9-3 loss: 0.748371  [  128/  130]
train() client id: f_00008-10-0 loss: 0.711900  [   32/  130]
train() client id: f_00008-10-1 loss: 0.753667  [   64/  130]
train() client id: f_00008-10-2 loss: 0.921870  [   96/  130]
train() client id: f_00008-10-3 loss: 0.824229  [  128/  130]
train() client id: f_00008-11-0 loss: 0.740666  [   32/  130]
train() client id: f_00008-11-1 loss: 0.787337  [   64/  130]
train() client id: f_00008-11-2 loss: 0.830711  [   96/  130]
train() client id: f_00008-11-3 loss: 0.836391  [  128/  130]
train() client id: f_00009-0-0 loss: 1.114282  [   32/  118]
train() client id: f_00009-0-1 loss: 0.978037  [   64/  118]
train() client id: f_00009-0-2 loss: 1.024349  [   96/  118]
train() client id: f_00009-1-0 loss: 0.952718  [   32/  118]
train() client id: f_00009-1-1 loss: 1.044772  [   64/  118]
train() client id: f_00009-1-2 loss: 1.086864  [   96/  118]
train() client id: f_00009-2-0 loss: 0.904291  [   32/  118]
train() client id: f_00009-2-1 loss: 1.101197  [   64/  118]
train() client id: f_00009-2-2 loss: 0.942023  [   96/  118]
train() client id: f_00009-3-0 loss: 0.943876  [   32/  118]
train() client id: f_00009-3-1 loss: 0.972333  [   64/  118]
train() client id: f_00009-3-2 loss: 0.940733  [   96/  118]
train() client id: f_00009-4-0 loss: 0.921608  [   32/  118]
train() client id: f_00009-4-1 loss: 0.922817  [   64/  118]
train() client id: f_00009-4-2 loss: 0.832052  [   96/  118]
train() client id: f_00009-5-0 loss: 0.940995  [   32/  118]
train() client id: f_00009-5-1 loss: 0.953233  [   64/  118]
train() client id: f_00009-5-2 loss: 0.813434  [   96/  118]
train() client id: f_00009-6-0 loss: 0.821588  [   32/  118]
train() client id: f_00009-6-1 loss: 0.770197  [   64/  118]
train() client id: f_00009-6-2 loss: 0.953436  [   96/  118]
train() client id: f_00009-7-0 loss: 0.879752  [   32/  118]
train() client id: f_00009-7-1 loss: 0.765798  [   64/  118]
train() client id: f_00009-7-2 loss: 0.890774  [   96/  118]
train() client id: f_00009-8-0 loss: 0.774349  [   32/  118]
train() client id: f_00009-8-1 loss: 0.911502  [   64/  118]
train() client id: f_00009-8-2 loss: 0.824223  [   96/  118]
train() client id: f_00009-9-0 loss: 0.876179  [   32/  118]
train() client id: f_00009-9-1 loss: 0.907781  [   64/  118]
train() client id: f_00009-9-2 loss: 0.731891  [   96/  118]
train() client id: f_00009-10-0 loss: 0.846159  [   32/  118]
train() client id: f_00009-10-1 loss: 0.785477  [   64/  118]
train() client id: f_00009-10-2 loss: 0.791784  [   96/  118]
train() client id: f_00009-11-0 loss: 0.880449  [   32/  118]
train() client id: f_00009-11-1 loss: 0.820164  [   64/  118]
train() client id: f_00009-11-2 loss: 0.699608  [   96/  118]
At round 31 accuracy: 0.6551724137931034
At round 31 training accuracy: 0.5861837692823608
At round 31 training loss: 0.8281308194699434
update_location
xs = [  -3.9056584     4.20031788  175.00902392   18.81129433    0.97929623
    3.95640986 -137.44319194 -116.32485185  159.66397685 -102.06087855]
ys = [ 167.5879595   150.55583871    1.32061395 -137.45517586  129.35018685
  112.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [195.19471903 180.78911261 201.56860488 171.01985313 163.50055002
 150.80678296 169.99271029 153.40191463 189.21219695 142.94206794]
dists_bs = [171.2548162  180.85981021 390.7311684  367.68336205 181.80379023
 189.45309836 181.55205558 183.79506923 369.84387958 185.90483768]
uav_gains = [1.82789532e-11 2.25026314e-11 1.66686654e-11 2.59975805e-11
 2.91614959e-11 3.57650883e-11 2.64028374e-11 3.42619275e-11
 1.99218930e-11 4.09129213e-11]
bs_gains = [6.15310409e-11 5.28124466e-11 6.10992006e-12 7.24380826e-12
 5.20482189e-11 4.63755733e-11 5.22505430e-11 5.04846428e-11
 7.12594497e-12 4.88967675e-11]
Round 32
-------------------------------
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.76786425 14.02640477  6.66672465  2.4004724  16.17773155  7.78673034
  2.97703515  9.52653575  7.02653207  6.31639524]
obj_prev = 79.67242616820144
eta_min = 2.1062834512478834e-14	eta_max = 0.9270669874026624
af = 16.815721006576958	bf = 1.436444871137545	zeta = 18.497293107234654	eta = 0.9090909090909091
af = 16.815721006576958	bf = 1.436444871137545	zeta = 33.430696901951336	eta = 0.5030024069164837
af = 16.815721006576958	bf = 1.436444871137545	zeta = 26.140048449463926	eta = 0.643293413900379
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.823688418099213	eta = 0.677406222772134
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755287540536187	eta = 0.6792779513888344
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755088130913464	eta = 0.6792834231754543
eta = 0.6792834231754543
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.03191095 0.06711429 0.03140442 0.01089024 0.07749801 0.03697618
 0.01367612 0.0453338  0.03292399 0.02988485]
ene_total = [2.19996663 3.98610054 2.18445289 1.03108082 4.54479709 2.37816282
 1.17905352 2.85331091 2.40505659 1.99310631]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 0 obj = 4.495842450611667
eta = 0.6792834231754543
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164858 15329090.24416601 50275192.55652302
 40127838.49551824 32727809.18735842]
eta_min = 0.6792834231754692	eta_max = 0.6792834231754555
af = 0.014083508139639458	bf = 1.436444871137545	zeta = 0.015491858953603405	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [2.60553049e-06 2.22287345e-05 2.50826025e-06 1.00113487e-07
 3.42564517e-05 3.74888670e-06 1.98007609e-07 7.06018162e-06
 3.26655702e-06 1.97229209e-06]
ene_total = [1.71674703 1.35307722 1.75794987 1.57458483 1.35935733 1.38609978
 1.56889819 1.47978715 2.24850168 1.37065582]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 1 obj = 4.495842450611684
eta = 0.6792834231754555
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164856 15329090.24416601 50275192.55652302
 40127838.49551825 32727809.18735842]
Done!
At round 32 energy consumption: 4.495842450611667
At round 32 eta: 0.6792834231754555
At round 32 local rounds: 12.663073636613152
At round 32 global rounds: 53.69580806749958
At round 32 a_n: 16.878589906266903
gradient difference: 0.46302923560142517
train() client id: f_00000-0-0 loss: 1.406591  [   32/  126]
train() client id: f_00000-0-1 loss: 1.347817  [   64/  126]
train() client id: f_00000-0-2 loss: 1.175108  [   96/  126]
train() client id: f_00000-1-0 loss: 1.301406  [   32/  126]
train() client id: f_00000-1-1 loss: 1.028011  [   64/  126]
train() client id: f_00000-1-2 loss: 1.237269  [   96/  126]
train() client id: f_00000-2-0 loss: 1.070071  [   32/  126]
train() client id: f_00000-2-1 loss: 1.034151  [   64/  126]
train() client id: f_00000-2-2 loss: 1.146913  [   96/  126]
train() client id: f_00000-3-0 loss: 0.931621  [   32/  126]
train() client id: f_00000-3-1 loss: 1.097782  [   64/  126]
train() client id: f_00000-3-2 loss: 1.148772  [   96/  126]
train() client id: f_00000-4-0 loss: 0.945195  [   32/  126]
train() client id: f_00000-4-1 loss: 1.130159  [   64/  126]
train() client id: f_00000-4-2 loss: 1.007873  [   96/  126]
train() client id: f_00000-5-0 loss: 0.953359  [   32/  126]
train() client id: f_00000-5-1 loss: 1.013838  [   64/  126]
train() client id: f_00000-5-2 loss: 0.956775  [   96/  126]
train() client id: f_00000-6-0 loss: 0.997701  [   32/  126]
train() client id: f_00000-6-1 loss: 0.907061  [   64/  126]
train() client id: f_00000-6-2 loss: 0.930547  [   96/  126]
train() client id: f_00000-7-0 loss: 1.062457  [   32/  126]
train() client id: f_00000-7-1 loss: 0.880937  [   64/  126]
train() client id: f_00000-7-2 loss: 0.918577  [   96/  126]
train() client id: f_00000-8-0 loss: 1.060235  [   32/  126]
train() client id: f_00000-8-1 loss: 0.864414  [   64/  126]
train() client id: f_00000-8-2 loss: 0.903337  [   96/  126]
train() client id: f_00000-9-0 loss: 1.011535  [   32/  126]
train() client id: f_00000-9-1 loss: 0.878315  [   64/  126]
train() client id: f_00000-9-2 loss: 0.915279  [   96/  126]
train() client id: f_00000-10-0 loss: 0.980626  [   32/  126]
train() client id: f_00000-10-1 loss: 0.960237  [   64/  126]
train() client id: f_00000-10-2 loss: 0.808046  [   96/  126]
train() client id: f_00000-11-0 loss: 0.886952  [   32/  126]
train() client id: f_00000-11-1 loss: 0.954748  [   64/  126]
train() client id: f_00000-11-2 loss: 0.892745  [   96/  126]
train() client id: f_00001-0-0 loss: 0.385416  [   32/  265]
train() client id: f_00001-0-1 loss: 0.229739  [   64/  265]
train() client id: f_00001-0-2 loss: 0.288476  [   96/  265]
train() client id: f_00001-0-3 loss: 0.274258  [  128/  265]
train() client id: f_00001-0-4 loss: 0.281050  [  160/  265]
train() client id: f_00001-0-5 loss: 0.429723  [  192/  265]
train() client id: f_00001-0-6 loss: 0.343315  [  224/  265]
train() client id: f_00001-0-7 loss: 0.230707  [  256/  265]
train() client id: f_00001-1-0 loss: 0.225841  [   32/  265]
train() client id: f_00001-1-1 loss: 0.312248  [   64/  265]
train() client id: f_00001-1-2 loss: 0.227952  [   96/  265]
train() client id: f_00001-1-3 loss: 0.370094  [  128/  265]
train() client id: f_00001-1-4 loss: 0.244131  [  160/  265]
train() client id: f_00001-1-5 loss: 0.288189  [  192/  265]
train() client id: f_00001-1-6 loss: 0.390263  [  224/  265]
train() client id: f_00001-1-7 loss: 0.308209  [  256/  265]
train() client id: f_00001-2-0 loss: 0.368846  [   32/  265]
train() client id: f_00001-2-1 loss: 0.363214  [   64/  265]
train() client id: f_00001-2-2 loss: 0.331411  [   96/  265]
train() client id: f_00001-2-3 loss: 0.203772  [  128/  265]
train() client id: f_00001-2-4 loss: 0.263261  [  160/  265]
train() client id: f_00001-2-5 loss: 0.262517  [  192/  265]
train() client id: f_00001-2-6 loss: 0.198066  [  224/  265]
train() client id: f_00001-2-7 loss: 0.331616  [  256/  265]
train() client id: f_00001-3-0 loss: 0.312185  [   32/  265]
train() client id: f_00001-3-1 loss: 0.406598  [   64/  265]
train() client id: f_00001-3-2 loss: 0.319433  [   96/  265]
train() client id: f_00001-3-3 loss: 0.231145  [  128/  265]
train() client id: f_00001-3-4 loss: 0.207967  [  160/  265]
train() client id: f_00001-3-5 loss: 0.226475  [  192/  265]
train() client id: f_00001-3-6 loss: 0.216675  [  224/  265]
train() client id: f_00001-3-7 loss: 0.300641  [  256/  265]
train() client id: f_00001-4-0 loss: 0.306475  [   32/  265]
train() client id: f_00001-4-1 loss: 0.256129  [   64/  265]
train() client id: f_00001-4-2 loss: 0.296427  [   96/  265]
train() client id: f_00001-4-3 loss: 0.278860  [  128/  265]
train() client id: f_00001-4-4 loss: 0.257915  [  160/  265]
train() client id: f_00001-4-5 loss: 0.176522  [  192/  265]
train() client id: f_00001-4-6 loss: 0.346816  [  224/  265]
train() client id: f_00001-4-7 loss: 0.242576  [  256/  265]
train() client id: f_00001-5-0 loss: 0.197059  [   32/  265]
train() client id: f_00001-5-1 loss: 0.276020  [   64/  265]
train() client id: f_00001-5-2 loss: 0.367399  [   96/  265]
train() client id: f_00001-5-3 loss: 0.305339  [  128/  265]
train() client id: f_00001-5-4 loss: 0.218475  [  160/  265]
train() client id: f_00001-5-5 loss: 0.255280  [  192/  265]
train() client id: f_00001-5-6 loss: 0.279217  [  224/  265]
train() client id: f_00001-5-7 loss: 0.327698  [  256/  265]
train() client id: f_00001-6-0 loss: 0.377194  [   32/  265]
train() client id: f_00001-6-1 loss: 0.211214  [   64/  265]
train() client id: f_00001-6-2 loss: 0.216492  [   96/  265]
train() client id: f_00001-6-3 loss: 0.270284  [  128/  265]
train() client id: f_00001-6-4 loss: 0.342776  [  160/  265]
train() client id: f_00001-6-5 loss: 0.252571  [  192/  265]
train() client id: f_00001-6-6 loss: 0.314148  [  224/  265]
train() client id: f_00001-6-7 loss: 0.201598  [  256/  265]
train() client id: f_00001-7-0 loss: 0.340109  [   32/  265]
train() client id: f_00001-7-1 loss: 0.258724  [   64/  265]
train() client id: f_00001-7-2 loss: 0.179091  [   96/  265]
train() client id: f_00001-7-3 loss: 0.214164  [  128/  265]
train() client id: f_00001-7-4 loss: 0.334661  [  160/  265]
train() client id: f_00001-7-5 loss: 0.188674  [  192/  265]
train() client id: f_00001-7-6 loss: 0.425996  [  224/  265]
train() client id: f_00001-7-7 loss: 0.229447  [  256/  265]
train() client id: f_00001-8-0 loss: 0.176444  [   32/  265]
train() client id: f_00001-8-1 loss: 0.360854  [   64/  265]
train() client id: f_00001-8-2 loss: 0.334793  [   96/  265]
train() client id: f_00001-8-3 loss: 0.263041  [  128/  265]
train() client id: f_00001-8-4 loss: 0.184934  [  160/  265]
train() client id: f_00001-8-5 loss: 0.283854  [  192/  265]
train() client id: f_00001-8-6 loss: 0.237069  [  224/  265]
train() client id: f_00001-8-7 loss: 0.305986  [  256/  265]
train() client id: f_00001-9-0 loss: 0.249182  [   32/  265]
train() client id: f_00001-9-1 loss: 0.326441  [   64/  265]
train() client id: f_00001-9-2 loss: 0.265573  [   96/  265]
train() client id: f_00001-9-3 loss: 0.173166  [  128/  265]
train() client id: f_00001-9-4 loss: 0.364959  [  160/  265]
train() client id: f_00001-9-5 loss: 0.234727  [  192/  265]
train() client id: f_00001-9-6 loss: 0.297444  [  224/  265]
train() client id: f_00001-9-7 loss: 0.215317  [  256/  265]
train() client id: f_00001-10-0 loss: 0.259684  [   32/  265]
train() client id: f_00001-10-1 loss: 0.359105  [   64/  265]
train() client id: f_00001-10-2 loss: 0.321489  [   96/  265]
train() client id: f_00001-10-3 loss: 0.196598  [  128/  265]
train() client id: f_00001-10-4 loss: 0.275154  [  160/  265]
train() client id: f_00001-10-5 loss: 0.253401  [  192/  265]
train() client id: f_00001-10-6 loss: 0.279359  [  224/  265]
train() client id: f_00001-10-7 loss: 0.179633  [  256/  265]
train() client id: f_00001-11-0 loss: 0.225700  [   32/  265]
train() client id: f_00001-11-1 loss: 0.237194  [   64/  265]
train() client id: f_00001-11-2 loss: 0.265187  [   96/  265]
train() client id: f_00001-11-3 loss: 0.236996  [  128/  265]
train() client id: f_00001-11-4 loss: 0.177929  [  160/  265]
train() client id: f_00001-11-5 loss: 0.372471  [  192/  265]
train() client id: f_00001-11-6 loss: 0.361378  [  224/  265]
train() client id: f_00001-11-7 loss: 0.243556  [  256/  265]
train() client id: f_00002-0-0 loss: 1.393220  [   32/  124]
train() client id: f_00002-0-1 loss: 1.271465  [   64/  124]
train() client id: f_00002-0-2 loss: 1.195266  [   96/  124]
train() client id: f_00002-1-0 loss: 1.240555  [   32/  124]
train() client id: f_00002-1-1 loss: 1.285583  [   64/  124]
train() client id: f_00002-1-2 loss: 1.282046  [   96/  124]
train() client id: f_00002-2-0 loss: 1.261312  [   32/  124]
train() client id: f_00002-2-1 loss: 1.170636  [   64/  124]
train() client id: f_00002-2-2 loss: 1.195505  [   96/  124]
train() client id: f_00002-3-0 loss: 1.315126  [   32/  124]
train() client id: f_00002-3-1 loss: 1.143002  [   64/  124]
train() client id: f_00002-3-2 loss: 1.110398  [   96/  124]
train() client id: f_00002-4-0 loss: 1.244273  [   32/  124]
train() client id: f_00002-4-1 loss: 1.158412  [   64/  124]
train() client id: f_00002-4-2 loss: 1.096451  [   96/  124]
train() client id: f_00002-5-0 loss: 1.161034  [   32/  124]
train() client id: f_00002-5-1 loss: 1.140729  [   64/  124]
train() client id: f_00002-5-2 loss: 1.059937  [   96/  124]
train() client id: f_00002-6-0 loss: 1.189864  [   32/  124]
train() client id: f_00002-6-1 loss: 1.201791  [   64/  124]
train() client id: f_00002-6-2 loss: 0.923781  [   96/  124]
train() client id: f_00002-7-0 loss: 1.110707  [   32/  124]
train() client id: f_00002-7-1 loss: 1.084694  [   64/  124]
train() client id: f_00002-7-2 loss: 1.117923  [   96/  124]
train() client id: f_00002-8-0 loss: 1.056117  [   32/  124]
train() client id: f_00002-8-1 loss: 0.997799  [   64/  124]
train() client id: f_00002-8-2 loss: 1.000809  [   96/  124]
train() client id: f_00002-9-0 loss: 1.116993  [   32/  124]
train() client id: f_00002-9-1 loss: 1.143641  [   64/  124]
train() client id: f_00002-9-2 loss: 0.954196  [   96/  124]
train() client id: f_00002-10-0 loss: 1.034568  [   32/  124]
train() client id: f_00002-10-1 loss: 1.020171  [   64/  124]
train() client id: f_00002-10-2 loss: 1.163278  [   96/  124]
train() client id: f_00002-11-0 loss: 1.175578  [   32/  124]
train() client id: f_00002-11-1 loss: 1.021154  [   64/  124]
train() client id: f_00002-11-2 loss: 0.850592  [   96/  124]
train() client id: f_00003-0-0 loss: 0.491473  [   32/   43]
train() client id: f_00003-1-0 loss: 0.489428  [   32/   43]
train() client id: f_00003-2-0 loss: 0.760119  [   32/   43]
train() client id: f_00003-3-0 loss: 0.571946  [   32/   43]
train() client id: f_00003-4-0 loss: 0.486829  [   32/   43]
train() client id: f_00003-5-0 loss: 0.550770  [   32/   43]
train() client id: f_00003-6-0 loss: 0.494411  [   32/   43]
train() client id: f_00003-7-0 loss: 0.755549  [   32/   43]
train() client id: f_00003-8-0 loss: 0.596344  [   32/   43]
train() client id: f_00003-9-0 loss: 0.607947  [   32/   43]
train() client id: f_00003-10-0 loss: 0.646899  [   32/   43]
train() client id: f_00003-11-0 loss: 0.615806  [   32/   43]
train() client id: f_00004-0-0 loss: 0.953712  [   32/  306]
train() client id: f_00004-0-1 loss: 0.947632  [   64/  306]
train() client id: f_00004-0-2 loss: 0.821817  [   96/  306]
train() client id: f_00004-0-3 loss: 1.068204  [  128/  306]
train() client id: f_00004-0-4 loss: 1.070041  [  160/  306]
train() client id: f_00004-0-5 loss: 0.899564  [  192/  306]
train() client id: f_00004-0-6 loss: 0.918985  [  224/  306]
train() client id: f_00004-0-7 loss: 0.810868  [  256/  306]
train() client id: f_00004-0-8 loss: 0.902920  [  288/  306]
train() client id: f_00004-1-0 loss: 1.020350  [   32/  306]
train() client id: f_00004-1-1 loss: 0.842971  [   64/  306]
train() client id: f_00004-1-2 loss: 1.032510  [   96/  306]
train() client id: f_00004-1-3 loss: 0.866407  [  128/  306]
train() client id: f_00004-1-4 loss: 0.902263  [  160/  306]
train() client id: f_00004-1-5 loss: 1.196190  [  192/  306]
train() client id: f_00004-1-6 loss: 0.910146  [  224/  306]
train() client id: f_00004-1-7 loss: 0.870512  [  256/  306]
train() client id: f_00004-1-8 loss: 0.950520  [  288/  306]
train() client id: f_00004-2-0 loss: 0.878500  [   32/  306]
train() client id: f_00004-2-1 loss: 1.000303  [   64/  306]
train() client id: f_00004-2-2 loss: 0.989171  [   96/  306]
train() client id: f_00004-2-3 loss: 0.923451  [  128/  306]
train() client id: f_00004-2-4 loss: 0.824368  [  160/  306]
train() client id: f_00004-2-5 loss: 0.893781  [  192/  306]
train() client id: f_00004-2-6 loss: 1.008840  [  224/  306]
train() client id: f_00004-2-7 loss: 0.896377  [  256/  306]
train() client id: f_00004-2-8 loss: 1.004793  [  288/  306]
train() client id: f_00004-3-0 loss: 0.896162  [   32/  306]
train() client id: f_00004-3-1 loss: 0.864628  [   64/  306]
train() client id: f_00004-3-2 loss: 0.870257  [   96/  306]
train() client id: f_00004-3-3 loss: 0.951583  [  128/  306]
train() client id: f_00004-3-4 loss: 0.838142  [  160/  306]
train() client id: f_00004-3-5 loss: 1.070677  [  192/  306]
train() client id: f_00004-3-6 loss: 1.013924  [  224/  306]
train() client id: f_00004-3-7 loss: 0.943859  [  256/  306]
train() client id: f_00004-3-8 loss: 1.026354  [  288/  306]
train() client id: f_00004-4-0 loss: 0.904866  [   32/  306]
train() client id: f_00004-4-1 loss: 0.897638  [   64/  306]
train() client id: f_00004-4-2 loss: 0.878145  [   96/  306]
train() client id: f_00004-4-3 loss: 0.971946  [  128/  306]
train() client id: f_00004-4-4 loss: 0.889322  [  160/  306]
train() client id: f_00004-4-5 loss: 1.102651  [  192/  306]
train() client id: f_00004-4-6 loss: 0.868644  [  224/  306]
train() client id: f_00004-4-7 loss: 1.053340  [  256/  306]
train() client id: f_00004-4-8 loss: 0.962372  [  288/  306]
train() client id: f_00004-5-0 loss: 0.832405  [   32/  306]
train() client id: f_00004-5-1 loss: 0.888427  [   64/  306]
train() client id: f_00004-5-2 loss: 1.040437  [   96/  306]
train() client id: f_00004-5-3 loss: 0.948001  [  128/  306]
train() client id: f_00004-5-4 loss: 1.072488  [  160/  306]
train() client id: f_00004-5-5 loss: 0.878813  [  192/  306]
train() client id: f_00004-5-6 loss: 0.916180  [  224/  306]
train() client id: f_00004-5-7 loss: 0.976549  [  256/  306]
train() client id: f_00004-5-8 loss: 0.846732  [  288/  306]
train() client id: f_00004-6-0 loss: 0.904021  [   32/  306]
train() client id: f_00004-6-1 loss: 0.926823  [   64/  306]
train() client id: f_00004-6-2 loss: 0.915145  [   96/  306]
train() client id: f_00004-6-3 loss: 0.922190  [  128/  306]
train() client id: f_00004-6-4 loss: 0.994080  [  160/  306]
train() client id: f_00004-6-5 loss: 1.077282  [  192/  306]
train() client id: f_00004-6-6 loss: 0.725494  [  224/  306]
train() client id: f_00004-6-7 loss: 0.975647  [  256/  306]
train() client id: f_00004-6-8 loss: 1.001259  [  288/  306]
train() client id: f_00004-7-0 loss: 0.981313  [   32/  306]
train() client id: f_00004-7-1 loss: 0.921210  [   64/  306]
train() client id: f_00004-7-2 loss: 1.016918  [   96/  306]
train() client id: f_00004-7-3 loss: 0.947471  [  128/  306]
train() client id: f_00004-7-4 loss: 1.037640  [  160/  306]
train() client id: f_00004-7-5 loss: 0.806282  [  192/  306]
train() client id: f_00004-7-6 loss: 0.935597  [  224/  306]
train() client id: f_00004-7-7 loss: 0.848116  [  256/  306]
train() client id: f_00004-7-8 loss: 0.884343  [  288/  306]
train() client id: f_00004-8-0 loss: 1.103511  [   32/  306]
train() client id: f_00004-8-1 loss: 0.933437  [   64/  306]
train() client id: f_00004-8-2 loss: 0.846279  [   96/  306]
train() client id: f_00004-8-3 loss: 0.868392  [  128/  306]
train() client id: f_00004-8-4 loss: 0.966413  [  160/  306]
train() client id: f_00004-8-5 loss: 0.887277  [  192/  306]
train() client id: f_00004-8-6 loss: 0.896076  [  224/  306]
train() client id: f_00004-8-7 loss: 0.844122  [  256/  306]
train() client id: f_00004-8-8 loss: 0.982617  [  288/  306]
train() client id: f_00004-9-0 loss: 0.981796  [   32/  306]
train() client id: f_00004-9-1 loss: 0.930805  [   64/  306]
train() client id: f_00004-9-2 loss: 0.909306  [   96/  306]
train() client id: f_00004-9-3 loss: 0.892638  [  128/  306]
train() client id: f_00004-9-4 loss: 0.927005  [  160/  306]
train() client id: f_00004-9-5 loss: 0.865816  [  192/  306]
train() client id: f_00004-9-6 loss: 0.923371  [  224/  306]
train() client id: f_00004-9-7 loss: 0.956004  [  256/  306]
train() client id: f_00004-9-8 loss: 1.006122  [  288/  306]
train() client id: f_00004-10-0 loss: 0.908336  [   32/  306]
train() client id: f_00004-10-1 loss: 0.952464  [   64/  306]
train() client id: f_00004-10-2 loss: 1.040928  [   96/  306]
train() client id: f_00004-10-3 loss: 0.848888  [  128/  306]
train() client id: f_00004-10-4 loss: 0.938987  [  160/  306]
train() client id: f_00004-10-5 loss: 0.942665  [  192/  306]
train() client id: f_00004-10-6 loss: 0.879534  [  224/  306]
train() client id: f_00004-10-7 loss: 0.840216  [  256/  306]
train() client id: f_00004-10-8 loss: 1.017520  [  288/  306]
train() client id: f_00004-11-0 loss: 0.979436  [   32/  306]
train() client id: f_00004-11-1 loss: 0.905501  [   64/  306]
train() client id: f_00004-11-2 loss: 0.943894  [   96/  306]
train() client id: f_00004-11-3 loss: 0.858637  [  128/  306]
train() client id: f_00004-11-4 loss: 0.823774  [  160/  306]
train() client id: f_00004-11-5 loss: 1.013205  [  192/  306]
train() client id: f_00004-11-6 loss: 0.954038  [  224/  306]
train() client id: f_00004-11-7 loss: 0.878499  [  256/  306]
train() client id: f_00004-11-8 loss: 1.007123  [  288/  306]
train() client id: f_00005-0-0 loss: 0.357411  [   32/  146]
train() client id: f_00005-0-1 loss: 0.234203  [   64/  146]
train() client id: f_00005-0-2 loss: 0.573705  [   96/  146]
train() client id: f_00005-0-3 loss: 0.431771  [  128/  146]
train() client id: f_00005-1-0 loss: 0.621176  [   32/  146]
train() client id: f_00005-1-1 loss: 0.287976  [   64/  146]
train() client id: f_00005-1-2 loss: 0.161680  [   96/  146]
train() client id: f_00005-1-3 loss: 0.442395  [  128/  146]
train() client id: f_00005-2-0 loss: 0.269372  [   32/  146]
train() client id: f_00005-2-1 loss: 0.337033  [   64/  146]
train() client id: f_00005-2-2 loss: 0.574349  [   96/  146]
train() client id: f_00005-2-3 loss: 0.278245  [  128/  146]
train() client id: f_00005-3-0 loss: 0.437762  [   32/  146]
train() client id: f_00005-3-1 loss: 0.398405  [   64/  146]
train() client id: f_00005-3-2 loss: 0.412468  [   96/  146]
train() client id: f_00005-3-3 loss: 0.190365  [  128/  146]
train() client id: f_00005-4-0 loss: 0.391378  [   32/  146]
train() client id: f_00005-4-1 loss: 0.309240  [   64/  146]
train() client id: f_00005-4-2 loss: 0.406855  [   96/  146]
train() client id: f_00005-4-3 loss: 0.266817  [  128/  146]
train() client id: f_00005-5-0 loss: 0.259695  [   32/  146]
train() client id: f_00005-5-1 loss: 0.195790  [   64/  146]
train() client id: f_00005-5-2 loss: 0.427677  [   96/  146]
train() client id: f_00005-5-3 loss: 0.450508  [  128/  146]
train() client id: f_00005-6-0 loss: 0.280899  [   32/  146]
train() client id: f_00005-6-1 loss: 0.346983  [   64/  146]
train() client id: f_00005-6-2 loss: 0.315827  [   96/  146]
train() client id: f_00005-6-3 loss: 0.314154  [  128/  146]
train() client id: f_00005-7-0 loss: 0.342664  [   32/  146]
train() client id: f_00005-7-1 loss: 0.205924  [   64/  146]
train() client id: f_00005-7-2 loss: 0.316332  [   96/  146]
train() client id: f_00005-7-3 loss: 0.628522  [  128/  146]
train() client id: f_00005-8-0 loss: 0.261565  [   32/  146]
train() client id: f_00005-8-1 loss: 0.694778  [   64/  146]
train() client id: f_00005-8-2 loss: 0.118867  [   96/  146]
train() client id: f_00005-8-3 loss: 0.327794  [  128/  146]
train() client id: f_00005-9-0 loss: 0.453181  [   32/  146]
train() client id: f_00005-9-1 loss: 0.326169  [   64/  146]
train() client id: f_00005-9-2 loss: 0.393504  [   96/  146]
train() client id: f_00005-9-3 loss: 0.376923  [  128/  146]
train() client id: f_00005-10-0 loss: 0.554830  [   32/  146]
train() client id: f_00005-10-1 loss: 0.177638  [   64/  146]
train() client id: f_00005-10-2 loss: 0.361693  [   96/  146]
train() client id: f_00005-10-3 loss: 0.332324  [  128/  146]
train() client id: f_00005-11-0 loss: 0.424347  [   32/  146]
train() client id: f_00005-11-1 loss: 0.169872  [   64/  146]
train() client id: f_00005-11-2 loss: 0.301758  [   96/  146]
train() client id: f_00005-11-3 loss: 0.446835  [  128/  146]
train() client id: f_00006-0-0 loss: 0.556124  [   32/   54]
train() client id: f_00006-1-0 loss: 0.511160  [   32/   54]
train() client id: f_00006-2-0 loss: 0.557643  [   32/   54]
train() client id: f_00006-3-0 loss: 0.548433  [   32/   54]
train() client id: f_00006-4-0 loss: 0.477452  [   32/   54]
train() client id: f_00006-5-0 loss: 0.567755  [   32/   54]
train() client id: f_00006-6-0 loss: 0.534741  [   32/   54]
train() client id: f_00006-7-0 loss: 0.534651  [   32/   54]
train() client id: f_00006-8-0 loss: 0.561543  [   32/   54]
train() client id: f_00006-9-0 loss: 0.580397  [   32/   54]
train() client id: f_00006-10-0 loss: 0.581597  [   32/   54]
train() client id: f_00006-11-0 loss: 0.505820  [   32/   54]
train() client id: f_00007-0-0 loss: 0.739601  [   32/  179]
train() client id: f_00007-0-1 loss: 0.776034  [   64/  179]
train() client id: f_00007-0-2 loss: 0.693868  [   96/  179]
train() client id: f_00007-0-3 loss: 0.849239  [  128/  179]
train() client id: f_00007-0-4 loss: 0.672527  [  160/  179]
train() client id: f_00007-1-0 loss: 0.756341  [   32/  179]
train() client id: f_00007-1-1 loss: 0.613639  [   64/  179]
train() client id: f_00007-1-2 loss: 0.776246  [   96/  179]
train() client id: f_00007-1-3 loss: 0.676050  [  128/  179]
train() client id: f_00007-1-4 loss: 0.825186  [  160/  179]
train() client id: f_00007-2-0 loss: 0.813307  [   32/  179]
train() client id: f_00007-2-1 loss: 0.749354  [   64/  179]
train() client id: f_00007-2-2 loss: 0.623767  [   96/  179]
train() client id: f_00007-2-3 loss: 0.716863  [  128/  179]
train() client id: f_00007-2-4 loss: 0.755540  [  160/  179]
train() client id: f_00007-3-0 loss: 0.665500  [   32/  179]
train() client id: f_00007-3-1 loss: 0.817398  [   64/  179]
train() client id: f_00007-3-2 loss: 0.744195  [   96/  179]
train() client id: f_00007-3-3 loss: 0.709547  [  128/  179]
train() client id: f_00007-3-4 loss: 0.688138  [  160/  179]
train() client id: f_00007-4-0 loss: 0.681902  [   32/  179]
train() client id: f_00007-4-1 loss: 0.734711  [   64/  179]
train() client id: f_00007-4-2 loss: 0.647949  [   96/  179]
train() client id: f_00007-4-3 loss: 0.727669  [  128/  179]
train() client id: f_00007-4-4 loss: 0.574063  [  160/  179]
train() client id: f_00007-5-0 loss: 0.674902  [   32/  179]
train() client id: f_00007-5-1 loss: 0.771043  [   64/  179]
train() client id: f_00007-5-2 loss: 0.745822  [   96/  179]
train() client id: f_00007-5-3 loss: 0.541972  [  128/  179]
train() client id: f_00007-5-4 loss: 0.799753  [  160/  179]
train() client id: f_00007-6-0 loss: 0.862776  [   32/  179]
train() client id: f_00007-6-1 loss: 0.658728  [   64/  179]
train() client id: f_00007-6-2 loss: 0.655090  [   96/  179]
train() client id: f_00007-6-3 loss: 0.768620  [  128/  179]
train() client id: f_00007-6-4 loss: 0.641983  [  160/  179]
train() client id: f_00007-7-0 loss: 0.651530  [   32/  179]
train() client id: f_00007-7-1 loss: 0.644934  [   64/  179]
train() client id: f_00007-7-2 loss: 0.754759  [   96/  179]
train() client id: f_00007-7-3 loss: 0.821223  [  128/  179]
train() client id: f_00007-7-4 loss: 0.702442  [  160/  179]
train() client id: f_00007-8-0 loss: 0.590929  [   32/  179]
train() client id: f_00007-8-1 loss: 0.669723  [   64/  179]
train() client id: f_00007-8-2 loss: 0.737801  [   96/  179]
train() client id: f_00007-8-3 loss: 0.823201  [  128/  179]
train() client id: f_00007-8-4 loss: 0.728583  [  160/  179]
train() client id: f_00007-9-0 loss: 0.597274  [   32/  179]
train() client id: f_00007-9-1 loss: 0.727928  [   64/  179]
train() client id: f_00007-9-2 loss: 0.800511  [   96/  179]
train() client id: f_00007-9-3 loss: 0.767937  [  128/  179]
train() client id: f_00007-9-4 loss: 0.678783  [  160/  179]
train() client id: f_00007-10-0 loss: 0.649717  [   32/  179]
train() client id: f_00007-10-1 loss: 0.739387  [   64/  179]
train() client id: f_00007-10-2 loss: 0.650012  [   96/  179]
train() client id: f_00007-10-3 loss: 0.722991  [  128/  179]
train() client id: f_00007-10-4 loss: 0.702565  [  160/  179]
train() client id: f_00007-11-0 loss: 0.723583  [   32/  179]
train() client id: f_00007-11-1 loss: 0.656968  [   64/  179]
train() client id: f_00007-11-2 loss: 0.631017  [   96/  179]
train() client id: f_00007-11-3 loss: 1.010797  [  128/  179]
train() client id: f_00007-11-4 loss: 0.557490  [  160/  179]
train() client id: f_00008-0-0 loss: 0.685853  [   32/  130]
train() client id: f_00008-0-1 loss: 0.704510  [   64/  130]
train() client id: f_00008-0-2 loss: 0.839372  [   96/  130]
train() client id: f_00008-0-3 loss: 0.708695  [  128/  130]
train() client id: f_00008-1-0 loss: 0.842101  [   32/  130]
train() client id: f_00008-1-1 loss: 0.819792  [   64/  130]
train() client id: f_00008-1-2 loss: 0.599679  [   96/  130]
train() client id: f_00008-1-3 loss: 0.681642  [  128/  130]
train() client id: f_00008-2-0 loss: 0.753512  [   32/  130]
train() client id: f_00008-2-1 loss: 0.818684  [   64/  130]
train() client id: f_00008-2-2 loss: 0.753280  [   96/  130]
train() client id: f_00008-2-3 loss: 0.643603  [  128/  130]
train() client id: f_00008-3-0 loss: 0.710527  [   32/  130]
train() client id: f_00008-3-1 loss: 0.718695  [   64/  130]
train() client id: f_00008-3-2 loss: 0.733094  [   96/  130]
train() client id: f_00008-3-3 loss: 0.758844  [  128/  130]
train() client id: f_00008-4-0 loss: 0.756919  [   32/  130]
train() client id: f_00008-4-1 loss: 0.762863  [   64/  130]
train() client id: f_00008-4-2 loss: 0.720798  [   96/  130]
train() client id: f_00008-4-3 loss: 0.684752  [  128/  130]
train() client id: f_00008-5-0 loss: 0.785090  [   32/  130]
train() client id: f_00008-5-1 loss: 0.650835  [   64/  130]
train() client id: f_00008-5-2 loss: 0.771743  [   96/  130]
train() client id: f_00008-5-3 loss: 0.709032  [  128/  130]
train() client id: f_00008-6-0 loss: 0.799091  [   32/  130]
train() client id: f_00008-6-1 loss: 0.830224  [   64/  130]
train() client id: f_00008-6-2 loss: 0.720303  [   96/  130]
train() client id: f_00008-6-3 loss: 0.598630  [  128/  130]
train() client id: f_00008-7-0 loss: 0.711267  [   32/  130]
train() client id: f_00008-7-1 loss: 0.894722  [   64/  130]
train() client id: f_00008-7-2 loss: 0.749029  [   96/  130]
train() client id: f_00008-7-3 loss: 0.592523  [  128/  130]
train() client id: f_00008-8-0 loss: 0.719038  [   32/  130]
train() client id: f_00008-8-1 loss: 0.897200  [   64/  130]
train() client id: f_00008-8-2 loss: 0.622193  [   96/  130]
train() client id: f_00008-8-3 loss: 0.676386  [  128/  130]
train() client id: f_00008-9-0 loss: 0.777887  [   32/  130]
train() client id: f_00008-9-1 loss: 0.717995  [   64/  130]
train() client id: f_00008-9-2 loss: 0.716486  [   96/  130]
train() client id: f_00008-9-3 loss: 0.740766  [  128/  130]
train() client id: f_00008-10-0 loss: 0.625552  [   32/  130]
train() client id: f_00008-10-1 loss: 0.675892  [   64/  130]
train() client id: f_00008-10-2 loss: 0.832597  [   96/  130]
train() client id: f_00008-10-3 loss: 0.820930  [  128/  130]
train() client id: f_00008-11-0 loss: 0.700958  [   32/  130]
train() client id: f_00008-11-1 loss: 0.716097  [   64/  130]
train() client id: f_00008-11-2 loss: 0.715389  [   96/  130]
train() client id: f_00008-11-3 loss: 0.785501  [  128/  130]
train() client id: f_00009-0-0 loss: 1.100408  [   32/  118]
train() client id: f_00009-0-1 loss: 1.232134  [   64/  118]
train() client id: f_00009-0-2 loss: 1.119129  [   96/  118]
train() client id: f_00009-1-0 loss: 1.165439  [   32/  118]
train() client id: f_00009-1-1 loss: 1.075462  [   64/  118]
train() client id: f_00009-1-2 loss: 0.997458  [   96/  118]
train() client id: f_00009-2-0 loss: 1.094310  [   32/  118]
train() client id: f_00009-2-1 loss: 1.027336  [   64/  118]
train() client id: f_00009-2-2 loss: 1.104625  [   96/  118]
train() client id: f_00009-3-0 loss: 1.004573  [   32/  118]
train() client id: f_00009-3-1 loss: 0.965928  [   64/  118]
train() client id: f_00009-3-2 loss: 1.049550  [   96/  118]
train() client id: f_00009-4-0 loss: 1.047805  [   32/  118]
train() client id: f_00009-4-1 loss: 0.989833  [   64/  118]
train() client id: f_00009-4-2 loss: 1.081116  [   96/  118]
train() client id: f_00009-5-0 loss: 0.955193  [   32/  118]
train() client id: f_00009-5-1 loss: 1.033168  [   64/  118]
train() client id: f_00009-5-2 loss: 0.979680  [   96/  118]
train() client id: f_00009-6-0 loss: 1.059417  [   32/  118]
train() client id: f_00009-6-1 loss: 0.979904  [   64/  118]
train() client id: f_00009-6-2 loss: 0.964738  [   96/  118]
train() client id: f_00009-7-0 loss: 0.996135  [   32/  118]
train() client id: f_00009-7-1 loss: 0.892414  [   64/  118]
train() client id: f_00009-7-2 loss: 0.851930  [   96/  118]
train() client id: f_00009-8-0 loss: 0.942621  [   32/  118]
train() client id: f_00009-8-1 loss: 0.894414  [   64/  118]
train() client id: f_00009-8-2 loss: 1.073620  [   96/  118]
train() client id: f_00009-9-0 loss: 0.933602  [   32/  118]
train() client id: f_00009-9-1 loss: 0.935109  [   64/  118]
train() client id: f_00009-9-2 loss: 0.993760  [   96/  118]
train() client id: f_00009-10-0 loss: 1.061314  [   32/  118]
train() client id: f_00009-10-1 loss: 0.991448  [   64/  118]
train() client id: f_00009-10-2 loss: 0.869681  [   96/  118]
train() client id: f_00009-11-0 loss: 0.931486  [   32/  118]
train() client id: f_00009-11-1 loss: 0.983584  [   64/  118]
train() client id: f_00009-11-2 loss: 1.063403  [   96/  118]
At round 32 accuracy: 0.6551724137931034
At round 32 training accuracy: 0.5835010060362174
At round 32 training loss: 0.8321394249038151
update_location
xs = [  -3.9056584     4.20031788  180.00902392   18.81129433    0.97929623
    3.95640986 -142.44319194 -121.32485185  164.66397685 -107.06087855]
ys = [ 172.5879595   155.55583871    1.32061395 -142.45517586  134.35018685
  117.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [199.50402986 184.9736782  205.92472584 175.06382243 167.4841238
 154.58275225 174.06020071 157.22721117 193.44998125 146.55389307]
dists_bs = [171.11134289 180.25212713 395.21631571 371.94167082 180.61317784
 187.87128037 180.58399931 182.26046195 374.37432475 184.00059091]
uav_gains = [1.71760459e-11 2.11772319e-11 1.56382078e-11 2.44758737e-11
 2.74270048e-11 3.36063783e-11 2.48429772e-11 3.21986963e-11
 1.87438290e-11 3.84301675e-11]
bs_gains = [6.16756090e-11 5.33124903e-11 5.91774794e-12 7.01398027e-12
 5.30146221e-11 4.74771870e-11 5.30386105e-11 5.16838882e-11
 6.88711171e-12 5.03269111e-11]
Round 33
-------------------------------
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.63572969 13.74717363  6.53678508  2.35476176 15.85548586  7.6312378
  2.9198615   9.33889895  6.88901514  6.19002745]
obj_prev = 78.09897687730752
eta_min = 1.1396317579661002e-14	eta_max = 0.9276141009437577
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 18.129363196870486	eta = 0.9090909090909091
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 32.88895800241827	eta = 0.5011177085230375
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 25.66977986634642	eta = 0.642048329034932
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.365791295624447	eta = 0.6764089485097873
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.297762088268055	eta = 0.678302767555703
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.29756203224391	eta = 0.678308352418689
eta = 0.678308352418689
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.03202948 0.06736359 0.03152108 0.0109307  0.07778588 0.03711352
 0.01372692 0.0455022  0.03304629 0.02999586]
ene_total = [2.16353862 3.9073382  2.14884473 1.01590035 4.454601   2.32913348
 1.16104971 2.80262517 2.36340733 1.95112344]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 0 obj = 4.427261042788227
eta = 0.678308352418689
freqs = [35725893.75226643 71777219.31597894 35338835.42638909 11985166.67285503
 82896638.99614602 39689662.43089405 15041309.96971984 49331324.0982344
 39308827.86172418 32018442.92741502]
eta_min = 0.6783083524187157	eta_max = 0.6783083524186895
af = 0.013235761297397841	bf = 1.4190753947365375	zeta = 0.014559337427137626	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [2.50952835e-06 2.13046542e-05 2.41647056e-06 9.63855586e-08
 3.28133544e-05 3.58891376e-06 1.90642814e-07 6.79757383e-06
 3.13457667e-06 1.88772088e-06]
ene_total = [1.70964427 1.323468   1.75202375 1.56544914 1.327104   1.35189348
 1.55993376 1.47051205 2.22823206 1.33544537]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 1 obj = 4.427261042788233
eta = 0.6783083524186895
freqs = [35725893.75226644 71777219.31597894 35338835.4263891  11985166.67285503
 82896638.99614602 39689662.43089406 15041309.96971984 49331324.09823441
 39308827.86172419 32018442.92741502]
Done!
At round 33 energy consumption: 4.427261042788227
At round 33 eta: 0.6783083524186895
At round 33 local rounds: 12.71011101057834
At round 33 global rounds: 52.468225498458686
At round 33 a_n: 16.536044059297588
gradient difference: 0.46391814947128296
train() client id: f_00000-0-0 loss: 1.213001  [   32/  126]
train() client id: f_00000-0-1 loss: 0.875095  [   64/  126]
train() client id: f_00000-0-2 loss: 1.126066  [   96/  126]
train() client id: f_00000-1-0 loss: 1.204972  [   32/  126]
train() client id: f_00000-1-1 loss: 0.762070  [   64/  126]
train() client id: f_00000-1-2 loss: 1.076028  [   96/  126]
train() client id: f_00000-2-0 loss: 0.858299  [   32/  126]
train() client id: f_00000-2-1 loss: 0.952162  [   64/  126]
train() client id: f_00000-2-2 loss: 1.179448  [   96/  126]
train() client id: f_00000-3-0 loss: 0.793776  [   32/  126]
train() client id: f_00000-3-1 loss: 0.909726  [   64/  126]
train() client id: f_00000-3-2 loss: 0.989993  [   96/  126]
train() client id: f_00000-4-0 loss: 0.943337  [   32/  126]
train() client id: f_00000-4-1 loss: 0.833857  [   64/  126]
train() client id: f_00000-4-2 loss: 0.962341  [   96/  126]
train() client id: f_00000-5-0 loss: 0.919341  [   32/  126]
train() client id: f_00000-5-1 loss: 0.859503  [   64/  126]
train() client id: f_00000-5-2 loss: 0.915266  [   96/  126]
train() client id: f_00000-6-0 loss: 0.866529  [   32/  126]
train() client id: f_00000-6-1 loss: 0.851894  [   64/  126]
train() client id: f_00000-6-2 loss: 0.879391  [   96/  126]
train() client id: f_00000-7-0 loss: 0.869473  [   32/  126]
train() client id: f_00000-7-1 loss: 0.867842  [   64/  126]
train() client id: f_00000-7-2 loss: 0.903204  [   96/  126]
train() client id: f_00000-8-0 loss: 0.956023  [   32/  126]
train() client id: f_00000-8-1 loss: 0.827444  [   64/  126]
train() client id: f_00000-8-2 loss: 0.867059  [   96/  126]
train() client id: f_00000-9-0 loss: 0.941578  [   32/  126]
train() client id: f_00000-9-1 loss: 0.859966  [   64/  126]
train() client id: f_00000-9-2 loss: 0.820142  [   96/  126]
train() client id: f_00000-10-0 loss: 0.760132  [   32/  126]
train() client id: f_00000-10-1 loss: 1.034997  [   64/  126]
train() client id: f_00000-10-2 loss: 0.940303  [   96/  126]
train() client id: f_00000-11-0 loss: 1.019374  [   32/  126]
train() client id: f_00000-11-1 loss: 0.910690  [   64/  126]
train() client id: f_00000-11-2 loss: 0.838428  [   96/  126]
train() client id: f_00001-0-0 loss: 0.462257  [   32/  265]
train() client id: f_00001-0-1 loss: 0.363562  [   64/  265]
train() client id: f_00001-0-2 loss: 0.438802  [   96/  265]
train() client id: f_00001-0-3 loss: 0.333864  [  128/  265]
train() client id: f_00001-0-4 loss: 0.526897  [  160/  265]
train() client id: f_00001-0-5 loss: 0.512656  [  192/  265]
train() client id: f_00001-0-6 loss: 0.387853  [  224/  265]
train() client id: f_00001-0-7 loss: 0.491512  [  256/  265]
train() client id: f_00001-1-0 loss: 0.441068  [   32/  265]
train() client id: f_00001-1-1 loss: 0.497941  [   64/  265]
train() client id: f_00001-1-2 loss: 0.431602  [   96/  265]
train() client id: f_00001-1-3 loss: 0.352932  [  128/  265]
train() client id: f_00001-1-4 loss: 0.334810  [  160/  265]
train() client id: f_00001-1-5 loss: 0.424891  [  192/  265]
train() client id: f_00001-1-6 loss: 0.407870  [  224/  265]
train() client id: f_00001-1-7 loss: 0.563452  [  256/  265]
train() client id: f_00001-2-0 loss: 0.574031  [   32/  265]
train() client id: f_00001-2-1 loss: 0.408240  [   64/  265]
train() client id: f_00001-2-2 loss: 0.408551  [   96/  265]
train() client id: f_00001-2-3 loss: 0.418945  [  128/  265]
train() client id: f_00001-2-4 loss: 0.395898  [  160/  265]
train() client id: f_00001-2-5 loss: 0.403405  [  192/  265]
train() client id: f_00001-2-6 loss: 0.386692  [  224/  265]
train() client id: f_00001-2-7 loss: 0.414650  [  256/  265]
train() client id: f_00001-3-0 loss: 0.395656  [   32/  265]
train() client id: f_00001-3-1 loss: 0.404770  [   64/  265]
train() client id: f_00001-3-2 loss: 0.421454  [   96/  265]
train() client id: f_00001-3-3 loss: 0.479450  [  128/  265]
train() client id: f_00001-3-4 loss: 0.398266  [  160/  265]
train() client id: f_00001-3-5 loss: 0.460886  [  192/  265]
train() client id: f_00001-3-6 loss: 0.400999  [  224/  265]
train() client id: f_00001-3-7 loss: 0.406298  [  256/  265]
train() client id: f_00001-4-0 loss: 0.364748  [   32/  265]
train() client id: f_00001-4-1 loss: 0.444132  [   64/  265]
train() client id: f_00001-4-2 loss: 0.409842  [   96/  265]
train() client id: f_00001-4-3 loss: 0.474013  [  128/  265]
train() client id: f_00001-4-4 loss: 0.385417  [  160/  265]
train() client id: f_00001-4-5 loss: 0.376933  [  192/  265]
train() client id: f_00001-4-6 loss: 0.482304  [  224/  265]
train() client id: f_00001-4-7 loss: 0.406512  [  256/  265]
train() client id: f_00001-5-0 loss: 0.455737  [   32/  265]
train() client id: f_00001-5-1 loss: 0.393756  [   64/  265]
train() client id: f_00001-5-2 loss: 0.554968  [   96/  265]
train() client id: f_00001-5-3 loss: 0.394581  [  128/  265]
train() client id: f_00001-5-4 loss: 0.321714  [  160/  265]
train() client id: f_00001-5-5 loss: 0.379879  [  192/  265]
train() client id: f_00001-5-6 loss: 0.349189  [  224/  265]
train() client id: f_00001-5-7 loss: 0.418218  [  256/  265]
train() client id: f_00001-6-0 loss: 0.477375  [   32/  265]
train() client id: f_00001-6-1 loss: 0.465733  [   64/  265]
train() client id: f_00001-6-2 loss: 0.317450  [   96/  265]
train() client id: f_00001-6-3 loss: 0.367317  [  128/  265]
train() client id: f_00001-6-4 loss: 0.542380  [  160/  265]
train() client id: f_00001-6-5 loss: 0.322773  [  192/  265]
train() client id: f_00001-6-6 loss: 0.369742  [  224/  265]
train() client id: f_00001-6-7 loss: 0.391251  [  256/  265]
train() client id: f_00001-7-0 loss: 0.376672  [   32/  265]
train() client id: f_00001-7-1 loss: 0.566403  [   64/  265]
train() client id: f_00001-7-2 loss: 0.454786  [   96/  265]
train() client id: f_00001-7-3 loss: 0.364005  [  128/  265]
train() client id: f_00001-7-4 loss: 0.417336  [  160/  265]
train() client id: f_00001-7-5 loss: 0.302470  [  192/  265]
train() client id: f_00001-7-6 loss: 0.413776  [  224/  265]
train() client id: f_00001-7-7 loss: 0.406591  [  256/  265]
train() client id: f_00001-8-0 loss: 0.441442  [   32/  265]
train() client id: f_00001-8-1 loss: 0.397328  [   64/  265]
train() client id: f_00001-8-2 loss: 0.453416  [   96/  265]
train() client id: f_00001-8-3 loss: 0.373268  [  128/  265]
train() client id: f_00001-8-4 loss: 0.393027  [  160/  265]
train() client id: f_00001-8-5 loss: 0.299500  [  192/  265]
train() client id: f_00001-8-6 loss: 0.408100  [  224/  265]
train() client id: f_00001-8-7 loss: 0.530465  [  256/  265]
train() client id: f_00001-9-0 loss: 0.385047  [   32/  265]
train() client id: f_00001-9-1 loss: 0.397585  [   64/  265]
train() client id: f_00001-9-2 loss: 0.364697  [   96/  265]
train() client id: f_00001-9-3 loss: 0.523455  [  128/  265]
train() client id: f_00001-9-4 loss: 0.394020  [  160/  265]
train() client id: f_00001-9-5 loss: 0.410012  [  192/  265]
train() client id: f_00001-9-6 loss: 0.508111  [  224/  265]
train() client id: f_00001-9-7 loss: 0.312755  [  256/  265]
train() client id: f_00001-10-0 loss: 0.421543  [   32/  265]
train() client id: f_00001-10-1 loss: 0.350376  [   64/  265]
train() client id: f_00001-10-2 loss: 0.369058  [   96/  265]
train() client id: f_00001-10-3 loss: 0.644601  [  128/  265]
train() client id: f_00001-10-4 loss: 0.494909  [  160/  265]
train() client id: f_00001-10-5 loss: 0.320428  [  192/  265]
train() client id: f_00001-10-6 loss: 0.302461  [  224/  265]
train() client id: f_00001-10-7 loss: 0.329757  [  256/  265]
train() client id: f_00001-11-0 loss: 0.445418  [   32/  265]
train() client id: f_00001-11-1 loss: 0.493626  [   64/  265]
train() client id: f_00001-11-2 loss: 0.452600  [   96/  265]
train() client id: f_00001-11-3 loss: 0.345930  [  128/  265]
train() client id: f_00001-11-4 loss: 0.392200  [  160/  265]
train() client id: f_00001-11-5 loss: 0.341454  [  192/  265]
train() client id: f_00001-11-6 loss: 0.404096  [  224/  265]
train() client id: f_00001-11-7 loss: 0.426677  [  256/  265]
train() client id: f_00002-0-0 loss: 1.502277  [   32/  124]
train() client id: f_00002-0-1 loss: 1.358006  [   64/  124]
train() client id: f_00002-0-2 loss: 1.471324  [   96/  124]
train() client id: f_00002-1-0 loss: 1.420424  [   32/  124]
train() client id: f_00002-1-1 loss: 1.495257  [   64/  124]
train() client id: f_00002-1-2 loss: 1.249286  [   96/  124]
train() client id: f_00002-2-0 loss: 1.413852  [   32/  124]
train() client id: f_00002-2-1 loss: 1.208400  [   64/  124]
train() client id: f_00002-2-2 loss: 1.504529  [   96/  124]
train() client id: f_00002-3-0 loss: 1.363992  [   32/  124]
train() client id: f_00002-3-1 loss: 1.226568  [   64/  124]
train() client id: f_00002-3-2 loss: 1.359229  [   96/  124]
train() client id: f_00002-4-0 loss: 1.293953  [   32/  124]
train() client id: f_00002-4-1 loss: 1.199754  [   64/  124]
train() client id: f_00002-4-2 loss: 1.184592  [   96/  124]
train() client id: f_00002-5-0 loss: 1.286965  [   32/  124]
train() client id: f_00002-5-1 loss: 1.297649  [   64/  124]
train() client id: f_00002-5-2 loss: 1.216131  [   96/  124]
train() client id: f_00002-6-0 loss: 1.390735  [   32/  124]
train() client id: f_00002-6-1 loss: 1.175439  [   64/  124]
train() client id: f_00002-6-2 loss: 1.162692  [   96/  124]
train() client id: f_00002-7-0 loss: 1.170233  [   32/  124]
train() client id: f_00002-7-1 loss: 1.245941  [   64/  124]
train() client id: f_00002-7-2 loss: 1.172851  [   96/  124]
train() client id: f_00002-8-0 loss: 1.136371  [   32/  124]
train() client id: f_00002-8-1 loss: 1.044922  [   64/  124]
train() client id: f_00002-8-2 loss: 1.298448  [   96/  124]
train() client id: f_00002-9-0 loss: 1.108555  [   32/  124]
train() client id: f_00002-9-1 loss: 1.078666  [   64/  124]
train() client id: f_00002-9-2 loss: 1.377084  [   96/  124]
train() client id: f_00002-10-0 loss: 1.119248  [   32/  124]
train() client id: f_00002-10-1 loss: 1.201092  [   64/  124]
train() client id: f_00002-10-2 loss: 1.208539  [   96/  124]
train() client id: f_00002-11-0 loss: 1.094467  [   32/  124]
train() client id: f_00002-11-1 loss: 1.174407  [   64/  124]
train() client id: f_00002-11-2 loss: 1.308890  [   96/  124]
train() client id: f_00003-0-0 loss: 0.633008  [   32/   43]
train() client id: f_00003-1-0 loss: 0.695692  [   32/   43]
train() client id: f_00003-2-0 loss: 0.742131  [   32/   43]
train() client id: f_00003-3-0 loss: 0.787328  [   32/   43]
train() client id: f_00003-4-0 loss: 0.582139  [   32/   43]
train() client id: f_00003-5-0 loss: 0.667234  [   32/   43]
train() client id: f_00003-6-0 loss: 0.689900  [   32/   43]
train() client id: f_00003-7-0 loss: 0.757830  [   32/   43]
train() client id: f_00003-8-0 loss: 0.824428  [   32/   43]
train() client id: f_00003-9-0 loss: 0.636664  [   32/   43]
train() client id: f_00003-10-0 loss: 0.635886  [   32/   43]
train() client id: f_00003-11-0 loss: 0.708027  [   32/   43]
train() client id: f_00004-0-0 loss: 0.733667  [   32/  306]
train() client id: f_00004-0-1 loss: 0.681931  [   64/  306]
train() client id: f_00004-0-2 loss: 0.828959  [   96/  306]
train() client id: f_00004-0-3 loss: 0.588661  [  128/  306]
train() client id: f_00004-0-4 loss: 0.703131  [  160/  306]
train() client id: f_00004-0-5 loss: 0.724155  [  192/  306]
train() client id: f_00004-0-6 loss: 0.726098  [  224/  306]
train() client id: f_00004-0-7 loss: 0.827672  [  256/  306]
train() client id: f_00004-0-8 loss: 0.592792  [  288/  306]
train() client id: f_00004-1-0 loss: 0.825545  [   32/  306]
train() client id: f_00004-1-1 loss: 0.705812  [   64/  306]
train() client id: f_00004-1-2 loss: 0.763862  [   96/  306]
train() client id: f_00004-1-3 loss: 0.734190  [  128/  306]
train() client id: f_00004-1-4 loss: 0.653242  [  160/  306]
train() client id: f_00004-1-5 loss: 0.556117  [  192/  306]
train() client id: f_00004-1-6 loss: 0.727772  [  224/  306]
train() client id: f_00004-1-7 loss: 0.775147  [  256/  306]
train() client id: f_00004-1-8 loss: 0.717623  [  288/  306]
train() client id: f_00004-2-0 loss: 0.532593  [   32/  306]
train() client id: f_00004-2-1 loss: 0.777998  [   64/  306]
train() client id: f_00004-2-2 loss: 0.789078  [   96/  306]
train() client id: f_00004-2-3 loss: 0.597041  [  128/  306]
train() client id: f_00004-2-4 loss: 0.778464  [  160/  306]
train() client id: f_00004-2-5 loss: 0.805478  [  192/  306]
train() client id: f_00004-2-6 loss: 0.706019  [  224/  306]
train() client id: f_00004-2-7 loss: 0.645863  [  256/  306]
train() client id: f_00004-2-8 loss: 0.780228  [  288/  306]
train() client id: f_00004-3-0 loss: 0.820588  [   32/  306]
train() client id: f_00004-3-1 loss: 0.608469  [   64/  306]
train() client id: f_00004-3-2 loss: 0.667884  [   96/  306]
train() client id: f_00004-3-3 loss: 0.655025  [  128/  306]
train() client id: f_00004-3-4 loss: 0.846094  [  160/  306]
train() client id: f_00004-3-5 loss: 0.694958  [  192/  306]
train() client id: f_00004-3-6 loss: 0.753864  [  224/  306]
train() client id: f_00004-3-7 loss: 0.710029  [  256/  306]
train() client id: f_00004-3-8 loss: 0.677070  [  288/  306]
train() client id: f_00004-4-0 loss: 0.778718  [   32/  306]
train() client id: f_00004-4-1 loss: 0.623097  [   64/  306]
train() client id: f_00004-4-2 loss: 0.664975  [   96/  306]
train() client id: f_00004-4-3 loss: 0.766971  [  128/  306]
train() client id: f_00004-4-4 loss: 0.778747  [  160/  306]
train() client id: f_00004-4-5 loss: 0.744961  [  192/  306]
train() client id: f_00004-4-6 loss: 0.780349  [  224/  306]
train() client id: f_00004-4-7 loss: 0.576008  [  256/  306]
train() client id: f_00004-4-8 loss: 0.755303  [  288/  306]
train() client id: f_00004-5-0 loss: 0.779561  [   32/  306]
train() client id: f_00004-5-1 loss: 0.756367  [   64/  306]
train() client id: f_00004-5-2 loss: 0.659743  [   96/  306]
train() client id: f_00004-5-3 loss: 0.605978  [  128/  306]
train() client id: f_00004-5-4 loss: 0.715671  [  160/  306]
train() client id: f_00004-5-5 loss: 0.684005  [  192/  306]
train() client id: f_00004-5-6 loss: 0.767591  [  224/  306]
train() client id: f_00004-5-7 loss: 0.775783  [  256/  306]
train() client id: f_00004-5-8 loss: 0.705706  [  288/  306]
train() client id: f_00004-6-0 loss: 0.815291  [   32/  306]
train() client id: f_00004-6-1 loss: 0.691225  [   64/  306]
train() client id: f_00004-6-2 loss: 0.723148  [   96/  306]
train() client id: f_00004-6-3 loss: 0.915731  [  128/  306]
train() client id: f_00004-6-4 loss: 0.631644  [  160/  306]
train() client id: f_00004-6-5 loss: 0.763135  [  192/  306]
train() client id: f_00004-6-6 loss: 0.648208  [  224/  306]
train() client id: f_00004-6-7 loss: 0.666931  [  256/  306]
train() client id: f_00004-6-8 loss: 0.704218  [  288/  306]
train() client id: f_00004-7-0 loss: 0.768844  [   32/  306]
train() client id: f_00004-7-1 loss: 0.684524  [   64/  306]
train() client id: f_00004-7-2 loss: 0.725880  [   96/  306]
train() client id: f_00004-7-3 loss: 0.616301  [  128/  306]
train() client id: f_00004-7-4 loss: 0.770803  [  160/  306]
train() client id: f_00004-7-5 loss: 0.732995  [  192/  306]
train() client id: f_00004-7-6 loss: 0.775707  [  224/  306]
train() client id: f_00004-7-7 loss: 0.741267  [  256/  306]
train() client id: f_00004-7-8 loss: 0.734118  [  288/  306]
train() client id: f_00004-8-0 loss: 0.748905  [   32/  306]
train() client id: f_00004-8-1 loss: 0.739747  [   64/  306]
train() client id: f_00004-8-2 loss: 0.756952  [   96/  306]
train() client id: f_00004-8-3 loss: 0.766786  [  128/  306]
train() client id: f_00004-8-4 loss: 0.670031  [  160/  306]
train() client id: f_00004-8-5 loss: 0.762941  [  192/  306]
train() client id: f_00004-8-6 loss: 0.828506  [  224/  306]
train() client id: f_00004-8-7 loss: 0.679392  [  256/  306]
train() client id: f_00004-8-8 loss: 0.646004  [  288/  306]
train() client id: f_00004-9-0 loss: 0.822987  [   32/  306]
train() client id: f_00004-9-1 loss: 0.672614  [   64/  306]
train() client id: f_00004-9-2 loss: 0.643371  [   96/  306]
train() client id: f_00004-9-3 loss: 0.698496  [  128/  306]
train() client id: f_00004-9-4 loss: 0.654061  [  160/  306]
train() client id: f_00004-9-5 loss: 0.755307  [  192/  306]
train() client id: f_00004-9-6 loss: 0.825451  [  224/  306]
train() client id: f_00004-9-7 loss: 0.694460  [  256/  306]
train() client id: f_00004-9-8 loss: 0.887324  [  288/  306]
train() client id: f_00004-10-0 loss: 0.716129  [   32/  306]
train() client id: f_00004-10-1 loss: 0.814373  [   64/  306]
train() client id: f_00004-10-2 loss: 0.692421  [   96/  306]
train() client id: f_00004-10-3 loss: 0.698870  [  128/  306]
train() client id: f_00004-10-4 loss: 0.688081  [  160/  306]
train() client id: f_00004-10-5 loss: 0.720140  [  192/  306]
train() client id: f_00004-10-6 loss: 0.785779  [  224/  306]
train() client id: f_00004-10-7 loss: 0.798596  [  256/  306]
train() client id: f_00004-10-8 loss: 0.756896  [  288/  306]
train() client id: f_00004-11-0 loss: 0.736738  [   32/  306]
train() client id: f_00004-11-1 loss: 0.812507  [   64/  306]
train() client id: f_00004-11-2 loss: 0.688532  [   96/  306]
train() client id: f_00004-11-3 loss: 0.702939  [  128/  306]
train() client id: f_00004-11-4 loss: 0.863895  [  160/  306]
train() client id: f_00004-11-5 loss: 0.723671  [  192/  306]
train() client id: f_00004-11-6 loss: 0.754360  [  224/  306]
train() client id: f_00004-11-7 loss: 0.670414  [  256/  306]
train() client id: f_00004-11-8 loss: 0.689705  [  288/  306]
train() client id: f_00005-0-0 loss: 0.615820  [   32/  146]
train() client id: f_00005-0-1 loss: 0.788639  [   64/  146]
train() client id: f_00005-0-2 loss: 0.344628  [   96/  146]
train() client id: f_00005-0-3 loss: 0.522412  [  128/  146]
train() client id: f_00005-1-0 loss: 0.450342  [   32/  146]
train() client id: f_00005-1-1 loss: 0.381502  [   64/  146]
train() client id: f_00005-1-2 loss: 0.525357  [   96/  146]
train() client id: f_00005-1-3 loss: 0.841877  [  128/  146]
train() client id: f_00005-2-0 loss: 0.550024  [   32/  146]
train() client id: f_00005-2-1 loss: 0.469643  [   64/  146]
train() client id: f_00005-2-2 loss: 0.475602  [   96/  146]
train() client id: f_00005-2-3 loss: 0.574182  [  128/  146]
train() client id: f_00005-3-0 loss: 0.658064  [   32/  146]
train() client id: f_00005-3-1 loss: 0.541686  [   64/  146]
train() client id: f_00005-3-2 loss: 0.498232  [   96/  146]
train() client id: f_00005-3-3 loss: 0.353239  [  128/  146]
train() client id: f_00005-4-0 loss: 0.394676  [   32/  146]
train() client id: f_00005-4-1 loss: 0.647243  [   64/  146]
train() client id: f_00005-4-2 loss: 0.595696  [   96/  146]
train() client id: f_00005-4-3 loss: 0.563711  [  128/  146]
train() client id: f_00005-5-0 loss: 0.462739  [   32/  146]
train() client id: f_00005-5-1 loss: 0.558112  [   64/  146]
train() client id: f_00005-5-2 loss: 0.421890  [   96/  146]
train() client id: f_00005-5-3 loss: 0.501181  [  128/  146]
train() client id: f_00005-6-0 loss: 0.454624  [   32/  146]
train() client id: f_00005-6-1 loss: 0.619296  [   64/  146]
train() client id: f_00005-6-2 loss: 0.400660  [   96/  146]
train() client id: f_00005-6-3 loss: 0.584970  [  128/  146]
train() client id: f_00005-7-0 loss: 0.499826  [   32/  146]
train() client id: f_00005-7-1 loss: 0.533523  [   64/  146]
train() client id: f_00005-7-2 loss: 0.399695  [   96/  146]
train() client id: f_00005-7-3 loss: 0.649139  [  128/  146]
train() client id: f_00005-8-0 loss: 0.497796  [   32/  146]
train() client id: f_00005-8-1 loss: 0.370252  [   64/  146]
train() client id: f_00005-8-2 loss: 0.581720  [   96/  146]
train() client id: f_00005-8-3 loss: 0.605891  [  128/  146]
train() client id: f_00005-9-0 loss: 0.596344  [   32/  146]
train() client id: f_00005-9-1 loss: 0.554138  [   64/  146]
train() client id: f_00005-9-2 loss: 0.283471  [   96/  146]
train() client id: f_00005-9-3 loss: 0.511107  [  128/  146]
train() client id: f_00005-10-0 loss: 0.505687  [   32/  146]
train() client id: f_00005-10-1 loss: 0.590474  [   64/  146]
train() client id: f_00005-10-2 loss: 0.382479  [   96/  146]
train() client id: f_00005-10-3 loss: 0.700093  [  128/  146]
train() client id: f_00005-11-0 loss: 0.330430  [   32/  146]
train() client id: f_00005-11-1 loss: 0.619833  [   64/  146]
train() client id: f_00005-11-2 loss: 0.476982  [   96/  146]
train() client id: f_00005-11-3 loss: 0.607811  [  128/  146]
train() client id: f_00006-0-0 loss: 0.505233  [   32/   54]
train() client id: f_00006-1-0 loss: 0.534206  [   32/   54]
train() client id: f_00006-2-0 loss: 0.547669  [   32/   54]
train() client id: f_00006-3-0 loss: 0.558969  [   32/   54]
train() client id: f_00006-4-0 loss: 0.509485  [   32/   54]
train() client id: f_00006-5-0 loss: 0.469825  [   32/   54]
train() client id: f_00006-6-0 loss: 0.539477  [   32/   54]
train() client id: f_00006-7-0 loss: 0.554811  [   32/   54]
train() client id: f_00006-8-0 loss: 0.579977  [   32/   54]
train() client id: f_00006-9-0 loss: 0.602657  [   32/   54]
train() client id: f_00006-10-0 loss: 0.594250  [   32/   54]
train() client id: f_00006-11-0 loss: 0.538440  [   32/   54]
train() client id: f_00007-0-0 loss: 0.832617  [   32/  179]
train() client id: f_00007-0-1 loss: 0.785968  [   64/  179]
train() client id: f_00007-0-2 loss: 0.651984  [   96/  179]
train() client id: f_00007-0-3 loss: 0.676989  [  128/  179]
train() client id: f_00007-0-4 loss: 0.703580  [  160/  179]
train() client id: f_00007-1-0 loss: 0.603338  [   32/  179]
train() client id: f_00007-1-1 loss: 0.764564  [   64/  179]
train() client id: f_00007-1-2 loss: 0.566393  [   96/  179]
train() client id: f_00007-1-3 loss: 0.686694  [  128/  179]
train() client id: f_00007-1-4 loss: 0.719652  [  160/  179]
train() client id: f_00007-2-0 loss: 0.717668  [   32/  179]
train() client id: f_00007-2-1 loss: 0.671690  [   64/  179]
train() client id: f_00007-2-2 loss: 0.704778  [   96/  179]
train() client id: f_00007-2-3 loss: 0.693851  [  128/  179]
train() client id: f_00007-2-4 loss: 0.747807  [  160/  179]
train() client id: f_00007-3-0 loss: 0.682546  [   32/  179]
train() client id: f_00007-3-1 loss: 0.586850  [   64/  179]
train() client id: f_00007-3-2 loss: 0.833179  [   96/  179]
train() client id: f_00007-3-3 loss: 0.629702  [  128/  179]
train() client id: f_00007-3-4 loss: 0.653079  [  160/  179]
train() client id: f_00007-4-0 loss: 0.725221  [   32/  179]
train() client id: f_00007-4-1 loss: 0.621554  [   64/  179]
train() client id: f_00007-4-2 loss: 0.716502  [   96/  179]
train() client id: f_00007-4-3 loss: 0.731278  [  128/  179]
train() client id: f_00007-4-4 loss: 0.555536  [  160/  179]
train() client id: f_00007-5-0 loss: 0.527053  [   32/  179]
train() client id: f_00007-5-1 loss: 0.792754  [   64/  179]
train() client id: f_00007-5-2 loss: 0.670133  [   96/  179]
train() client id: f_00007-5-3 loss: 0.820176  [  128/  179]
train() client id: f_00007-5-4 loss: 0.666740  [  160/  179]
train() client id: f_00007-6-0 loss: 0.700779  [   32/  179]
train() client id: f_00007-6-1 loss: 0.640185  [   64/  179]
train() client id: f_00007-6-2 loss: 0.911817  [   96/  179]
train() client id: f_00007-6-3 loss: 0.709899  [  128/  179]
train() client id: f_00007-6-4 loss: 0.537668  [  160/  179]
train() client id: f_00007-7-0 loss: 0.800510  [   32/  179]
train() client id: f_00007-7-1 loss: 0.694357  [   64/  179]
train() client id: f_00007-7-2 loss: 0.752145  [   96/  179]
train() client id: f_00007-7-3 loss: 0.512680  [  128/  179]
train() client id: f_00007-7-4 loss: 0.649295  [  160/  179]
train() client id: f_00007-8-0 loss: 0.653095  [   32/  179]
train() client id: f_00007-8-1 loss: 0.625106  [   64/  179]
train() client id: f_00007-8-2 loss: 0.778005  [   96/  179]
train() client id: f_00007-8-3 loss: 0.886293  [  128/  179]
train() client id: f_00007-8-4 loss: 0.544232  [  160/  179]
train() client id: f_00007-9-0 loss: 0.665376  [   32/  179]
train() client id: f_00007-9-1 loss: 0.654853  [   64/  179]
train() client id: f_00007-9-2 loss: 0.715885  [   96/  179]
train() client id: f_00007-9-3 loss: 0.621948  [  128/  179]
train() client id: f_00007-9-4 loss: 0.720338  [  160/  179]
train() client id: f_00007-10-0 loss: 0.919079  [   32/  179]
train() client id: f_00007-10-1 loss: 0.578469  [   64/  179]
train() client id: f_00007-10-2 loss: 0.743476  [   96/  179]
train() client id: f_00007-10-3 loss: 0.515148  [  128/  179]
train() client id: f_00007-10-4 loss: 0.575388  [  160/  179]
train() client id: f_00007-11-0 loss: 0.660746  [   32/  179]
train() client id: f_00007-11-1 loss: 0.723298  [   64/  179]
train() client id: f_00007-11-2 loss: 0.708745  [   96/  179]
train() client id: f_00007-11-3 loss: 0.702472  [  128/  179]
train() client id: f_00007-11-4 loss: 0.538396  [  160/  179]
train() client id: f_00008-0-0 loss: 0.652328  [   32/  130]
train() client id: f_00008-0-1 loss: 0.687393  [   64/  130]
train() client id: f_00008-0-2 loss: 0.932571  [   96/  130]
train() client id: f_00008-0-3 loss: 0.563166  [  128/  130]
train() client id: f_00008-1-0 loss: 0.665495  [   32/  130]
train() client id: f_00008-1-1 loss: 0.760906  [   64/  130]
train() client id: f_00008-1-2 loss: 0.714262  [   96/  130]
train() client id: f_00008-1-3 loss: 0.675130  [  128/  130]
train() client id: f_00008-2-0 loss: 0.749239  [   32/  130]
train() client id: f_00008-2-1 loss: 0.631400  [   64/  130]
train() client id: f_00008-2-2 loss: 0.675265  [   96/  130]
train() client id: f_00008-2-3 loss: 0.735166  [  128/  130]
train() client id: f_00008-3-0 loss: 0.671259  [   32/  130]
train() client id: f_00008-3-1 loss: 0.716505  [   64/  130]
train() client id: f_00008-3-2 loss: 0.730698  [   96/  130]
train() client id: f_00008-3-3 loss: 0.704488  [  128/  130]
train() client id: f_00008-4-0 loss: 0.747407  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671122  [   64/  130]
train() client id: f_00008-4-2 loss: 0.661610  [   96/  130]
train() client id: f_00008-4-3 loss: 0.715605  [  128/  130]
train() client id: f_00008-5-0 loss: 0.703093  [   32/  130]
train() client id: f_00008-5-1 loss: 0.658777  [   64/  130]
train() client id: f_00008-5-2 loss: 0.769650  [   96/  130]
train() client id: f_00008-5-3 loss: 0.685674  [  128/  130]
train() client id: f_00008-6-0 loss: 0.677797  [   32/  130]
train() client id: f_00008-6-1 loss: 0.769721  [   64/  130]
train() client id: f_00008-6-2 loss: 0.689046  [   96/  130]
train() client id: f_00008-6-3 loss: 0.668835  [  128/  130]
train() client id: f_00008-7-0 loss: 0.727919  [   32/  130]
train() client id: f_00008-7-1 loss: 0.529577  [   64/  130]
train() client id: f_00008-7-2 loss: 0.818011  [   96/  130]
train() client id: f_00008-7-3 loss: 0.738338  [  128/  130]
train() client id: f_00008-8-0 loss: 0.721146  [   32/  130]
train() client id: f_00008-8-1 loss: 0.756899  [   64/  130]
train() client id: f_00008-8-2 loss: 0.672416  [   96/  130]
train() client id: f_00008-8-3 loss: 0.613286  [  128/  130]
train() client id: f_00008-9-0 loss: 0.707055  [   32/  130]
train() client id: f_00008-9-1 loss: 0.662300  [   64/  130]
train() client id: f_00008-9-2 loss: 0.682584  [   96/  130]
train() client id: f_00008-9-3 loss: 0.753186  [  128/  130]
train() client id: f_00008-10-0 loss: 0.653428  [   32/  130]
train() client id: f_00008-10-1 loss: 0.720457  [   64/  130]
train() client id: f_00008-10-2 loss: 0.804787  [   96/  130]
train() client id: f_00008-10-3 loss: 0.626332  [  128/  130]
train() client id: f_00008-11-0 loss: 0.614456  [   32/  130]
train() client id: f_00008-11-1 loss: 0.709597  [   64/  130]
train() client id: f_00008-11-2 loss: 0.802016  [   96/  130]
train() client id: f_00008-11-3 loss: 0.681335  [  128/  130]
train() client id: f_00009-0-0 loss: 1.011712  [   32/  118]
train() client id: f_00009-0-1 loss: 1.169623  [   64/  118]
train() client id: f_00009-0-2 loss: 1.026074  [   96/  118]
train() client id: f_00009-1-0 loss: 1.099930  [   32/  118]
train() client id: f_00009-1-1 loss: 0.934638  [   64/  118]
train() client id: f_00009-1-2 loss: 1.035379  [   96/  118]
train() client id: f_00009-2-0 loss: 0.993754  [   32/  118]
train() client id: f_00009-2-1 loss: 0.977159  [   64/  118]
train() client id: f_00009-2-2 loss: 0.912037  [   96/  118]
train() client id: f_00009-3-0 loss: 0.814670  [   32/  118]
train() client id: f_00009-3-1 loss: 1.058305  [   64/  118]
train() client id: f_00009-3-2 loss: 0.894506  [   96/  118]
train() client id: f_00009-4-0 loss: 0.901442  [   32/  118]
train() client id: f_00009-4-1 loss: 0.893972  [   64/  118]
train() client id: f_00009-4-2 loss: 0.867119  [   96/  118]
train() client id: f_00009-5-0 loss: 0.877002  [   32/  118]
train() client id: f_00009-5-1 loss: 0.873493  [   64/  118]
train() client id: f_00009-5-2 loss: 0.731639  [   96/  118]
train() client id: f_00009-6-0 loss: 0.758136  [   32/  118]
train() client id: f_00009-6-1 loss: 0.896087  [   64/  118]
train() client id: f_00009-6-2 loss: 0.836092  [   96/  118]
train() client id: f_00009-7-0 loss: 0.704000  [   32/  118]
train() client id: f_00009-7-1 loss: 0.849139  [   64/  118]
train() client id: f_00009-7-2 loss: 0.828605  [   96/  118]
train() client id: f_00009-8-0 loss: 0.684639  [   32/  118]
train() client id: f_00009-8-1 loss: 0.761175  [   64/  118]
train() client id: f_00009-8-2 loss: 0.943146  [   96/  118]
train() client id: f_00009-9-0 loss: 0.773982  [   32/  118]
train() client id: f_00009-9-1 loss: 0.752825  [   64/  118]
train() client id: f_00009-9-2 loss: 0.786642  [   96/  118]
train() client id: f_00009-10-0 loss: 0.784724  [   32/  118]
train() client id: f_00009-10-1 loss: 0.759209  [   64/  118]
train() client id: f_00009-10-2 loss: 0.819790  [   96/  118]
train() client id: f_00009-11-0 loss: 0.931483  [   32/  118]
train() client id: f_00009-11-1 loss: 0.777725  [   64/  118]
train() client id: f_00009-11-2 loss: 0.632700  [   96/  118]
At round 33 accuracy: 0.6551724137931034
At round 33 training accuracy: 0.5848423876592891
At round 33 training loss: 0.834789837256537
update_location
xs = [  -3.9056584     4.20031788  185.00902392   18.81129433    0.97929623
    3.95640986 -147.44319194 -126.32485185  169.66397685 -112.06087855]
ys = [ 177.5879595   160.55583871    1.32061395 -147.45517586  139.35018685
  122.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [203.84488595 189.197833   210.30949325 179.15605957 171.52094215
 158.42654071 178.17515369 161.11686582 197.72338004 150.24530728]
dists_bs = [171.11391311 179.78149994 399.71368058 376.21823221 179.55395256
 186.4101915  179.74986154 180.85111144 378.91658027 182.21368292]
uav_gains = [1.61237068e-11 1.99260114e-11 1.46504448e-11 2.30455348e-11
 2.58027311e-11 3.15864432e-11 2.33790442e-11 3.02686694e-11
 1.76243014e-11 3.61021161e-11]
bs_gains = [6.16730151e-11 5.37041796e-11 5.73319680e-12 6.79301444e-12
 5.38949620e-11 4.85265100e-11 5.37306513e-11 5.28195595e-11
 6.65843217e-12 5.17210489e-11]
Round 34
-------------------------------
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.50361085 13.46800316  6.40688418  2.3090212  15.53330799  7.47581585
  2.86265615  9.15122779  6.75143071  6.06373403]
obj_prev = 76.52569191747648
eta_min = 6.016067092016877e-15	eta_max = 0.9281867977986356
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 17.761433286506318	eta = 0.9090909090909091
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 32.349008668159414	eta = 0.4991422673511644
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 25.200112188715238	eta = 0.6407414940167677
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.908304317469916	eta = 0.6753618876010812
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.840631284811383	eta = 0.6772789419999331
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.84043047624284	eta = 0.6772846467381501
eta = 0.6772846467381501
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.03215411 0.06762571 0.03164373 0.01097323 0.07808855 0.03725794
 0.01378033 0.04567925 0.03317488 0.03011258]
ene_total = [2.12711913 3.82878864 2.11331163 1.00058971 4.36464773 2.28034368
 1.14291118 2.75181183 2.3215154  1.90939155]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 0 obj = 4.3589825705706815
eta = 0.6772846467381501
freqs = [35048280.46691417 70248512.05463123 34674068.18486581 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
eta_min = 0.677284646738166	eta_max = 0.6772846467381503
af = 0.012426086302136423	bf = 1.4018686060877306	zeta = 0.013668694932350066	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [2.41523466e-06 2.04068274e-05 2.32641206e-06 9.27160332e-08
 3.14130684e-05 3.43384418e-06 1.83392061e-07 6.53901742e-06
 3.00497385e-06 1.80578656e-06]
ene_total = [1.70271381 1.29453122 1.74651965 1.55598755 1.29560796 1.31845798
 1.55062412 1.46086543 2.20722871 1.30104589]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 1 obj = 4.358982570570683
eta = 0.6772846467381503
freqs = [35048280.46691417 70248512.05463123 34674068.18486582 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
Done!
At round 34 energy consumption: 4.3589825705706815
At round 34 eta: 0.6772846467381503
At round 34 local rounds: 12.759567346547202
At round 34 global rounds: 51.24033886878731
At round 34 a_n: 16.193498212328272
gradient difference: 0.4322036802768707
train() client id: f_00000-0-0 loss: 1.208794  [   32/  126]
train() client id: f_00000-0-1 loss: 1.278291  [   64/  126]
train() client id: f_00000-0-2 loss: 1.278628  [   96/  126]
train() client id: f_00000-1-0 loss: 1.243541  [   32/  126]
train() client id: f_00000-1-1 loss: 1.026831  [   64/  126]
train() client id: f_00000-1-2 loss: 1.067027  [   96/  126]
train() client id: f_00000-2-0 loss: 1.138879  [   32/  126]
train() client id: f_00000-2-1 loss: 0.883276  [   64/  126]
train() client id: f_00000-2-2 loss: 1.165958  [   96/  126]
train() client id: f_00000-3-0 loss: 0.926127  [   32/  126]
train() client id: f_00000-3-1 loss: 1.022867  [   64/  126]
train() client id: f_00000-3-2 loss: 0.970874  [   96/  126]
train() client id: f_00000-4-0 loss: 0.976414  [   32/  126]
train() client id: f_00000-4-1 loss: 0.973738  [   64/  126]
train() client id: f_00000-4-2 loss: 1.019415  [   96/  126]
train() client id: f_00000-5-0 loss: 1.031497  [   32/  126]
train() client id: f_00000-5-1 loss: 0.841901  [   64/  126]
train() client id: f_00000-5-2 loss: 0.946911  [   96/  126]
train() client id: f_00000-6-0 loss: 1.045895  [   32/  126]
train() client id: f_00000-6-1 loss: 0.990188  [   64/  126]
train() client id: f_00000-6-2 loss: 0.874748  [   96/  126]
train() client id: f_00000-7-0 loss: 0.837251  [   32/  126]
train() client id: f_00000-7-1 loss: 0.838603  [   64/  126]
train() client id: f_00000-7-2 loss: 1.138314  [   96/  126]
train() client id: f_00000-8-0 loss: 0.869399  [   32/  126]
train() client id: f_00000-8-1 loss: 1.037699  [   64/  126]
train() client id: f_00000-8-2 loss: 0.949175  [   96/  126]
train() client id: f_00000-9-0 loss: 0.990443  [   32/  126]
train() client id: f_00000-9-1 loss: 0.817251  [   64/  126]
train() client id: f_00000-9-2 loss: 0.985514  [   96/  126]
train() client id: f_00000-10-0 loss: 0.975462  [   32/  126]
train() client id: f_00000-10-1 loss: 0.912015  [   64/  126]
train() client id: f_00000-10-2 loss: 0.901239  [   96/  126]
train() client id: f_00000-11-0 loss: 0.871869  [   32/  126]
train() client id: f_00000-11-1 loss: 0.970322  [   64/  126]
train() client id: f_00000-11-2 loss: 1.021387  [   96/  126]
train() client id: f_00001-0-0 loss: 0.430798  [   32/  265]
train() client id: f_00001-0-1 loss: 0.415549  [   64/  265]
train() client id: f_00001-0-2 loss: 0.445733  [   96/  265]
train() client id: f_00001-0-3 loss: 0.473545  [  128/  265]
train() client id: f_00001-0-4 loss: 0.440672  [  160/  265]
train() client id: f_00001-0-5 loss: 0.372619  [  192/  265]
train() client id: f_00001-0-6 loss: 0.426889  [  224/  265]
train() client id: f_00001-0-7 loss: 0.435132  [  256/  265]
train() client id: f_00001-1-0 loss: 0.417343  [   32/  265]
train() client id: f_00001-1-1 loss: 0.395652  [   64/  265]
train() client id: f_00001-1-2 loss: 0.410312  [   96/  265]
train() client id: f_00001-1-3 loss: 0.342214  [  128/  265]
train() client id: f_00001-1-4 loss: 0.345348  [  160/  265]
train() client id: f_00001-1-5 loss: 0.524801  [  192/  265]
train() client id: f_00001-1-6 loss: 0.501467  [  224/  265]
train() client id: f_00001-1-7 loss: 0.327343  [  256/  265]
train() client id: f_00001-2-0 loss: 0.335015  [   32/  265]
train() client id: f_00001-2-1 loss: 0.335922  [   64/  265]
train() client id: f_00001-2-2 loss: 0.392412  [   96/  265]
train() client id: f_00001-2-3 loss: 0.407354  [  128/  265]
train() client id: f_00001-2-4 loss: 0.374180  [  160/  265]
train() client id: f_00001-2-5 loss: 0.497977  [  192/  265]
train() client id: f_00001-2-6 loss: 0.597700  [  224/  265]
train() client id: f_00001-2-7 loss: 0.348840  [  256/  265]
train() client id: f_00001-3-0 loss: 0.435771  [   32/  265]
train() client id: f_00001-3-1 loss: 0.475289  [   64/  265]
train() client id: f_00001-3-2 loss: 0.300795  [   96/  265]
train() client id: f_00001-3-3 loss: 0.481695  [  128/  265]
train() client id: f_00001-3-4 loss: 0.353424  [  160/  265]
train() client id: f_00001-3-5 loss: 0.463497  [  192/  265]
train() client id: f_00001-3-6 loss: 0.411590  [  224/  265]
train() client id: f_00001-3-7 loss: 0.310465  [  256/  265]
train() client id: f_00001-4-0 loss: 0.449018  [   32/  265]
train() client id: f_00001-4-1 loss: 0.480763  [   64/  265]
train() client id: f_00001-4-2 loss: 0.341345  [   96/  265]
train() client id: f_00001-4-3 loss: 0.411889  [  128/  265]
train() client id: f_00001-4-4 loss: 0.306481  [  160/  265]
train() client id: f_00001-4-5 loss: 0.371141  [  192/  265]
train() client id: f_00001-4-6 loss: 0.491185  [  224/  265]
train() client id: f_00001-4-7 loss: 0.343986  [  256/  265]
train() client id: f_00001-5-0 loss: 0.365497  [   32/  265]
train() client id: f_00001-5-1 loss: 0.373197  [   64/  265]
train() client id: f_00001-5-2 loss: 0.323038  [   96/  265]
train() client id: f_00001-5-3 loss: 0.598450  [  128/  265]
train() client id: f_00001-5-4 loss: 0.395197  [  160/  265]
train() client id: f_00001-5-5 loss: 0.419638  [  192/  265]
train() client id: f_00001-5-6 loss: 0.296815  [  224/  265]
train() client id: f_00001-5-7 loss: 0.384209  [  256/  265]
train() client id: f_00001-6-0 loss: 0.407900  [   32/  265]
train() client id: f_00001-6-1 loss: 0.297910  [   64/  265]
train() client id: f_00001-6-2 loss: 0.371224  [   96/  265]
train() client id: f_00001-6-3 loss: 0.475813  [  128/  265]
train() client id: f_00001-6-4 loss: 0.378852  [  160/  265]
train() client id: f_00001-6-5 loss: 0.297443  [  192/  265]
train() client id: f_00001-6-6 loss: 0.432713  [  224/  265]
train() client id: f_00001-6-7 loss: 0.489043  [  256/  265]
train() client id: f_00001-7-0 loss: 0.419273  [   32/  265]
train() client id: f_00001-7-1 loss: 0.413992  [   64/  265]
train() client id: f_00001-7-2 loss: 0.295254  [   96/  265]
train() client id: f_00001-7-3 loss: 0.387243  [  128/  265]
train() client id: f_00001-7-4 loss: 0.438539  [  160/  265]
train() client id: f_00001-7-5 loss: 0.283562  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432984  [  224/  265]
train() client id: f_00001-7-7 loss: 0.446779  [  256/  265]
train() client id: f_00001-8-0 loss: 0.293969  [   32/  265]
train() client id: f_00001-8-1 loss: 0.375982  [   64/  265]
train() client id: f_00001-8-2 loss: 0.459610  [   96/  265]
train() client id: f_00001-8-3 loss: 0.303360  [  128/  265]
train() client id: f_00001-8-4 loss: 0.393639  [  160/  265]
train() client id: f_00001-8-5 loss: 0.389928  [  192/  265]
train() client id: f_00001-8-6 loss: 0.407636  [  224/  265]
train() client id: f_00001-8-7 loss: 0.487864  [  256/  265]
train() client id: f_00001-9-0 loss: 0.291379  [   32/  265]
train() client id: f_00001-9-1 loss: 0.412759  [   64/  265]
train() client id: f_00001-9-2 loss: 0.433895  [   96/  265]
train() client id: f_00001-9-3 loss: 0.291183  [  128/  265]
train() client id: f_00001-9-4 loss: 0.402725  [  160/  265]
train() client id: f_00001-9-5 loss: 0.450662  [  192/  265]
train() client id: f_00001-9-6 loss: 0.321555  [  224/  265]
train() client id: f_00001-9-7 loss: 0.418340  [  256/  265]
train() client id: f_00001-10-0 loss: 0.297521  [   32/  265]
train() client id: f_00001-10-1 loss: 0.486407  [   64/  265]
train() client id: f_00001-10-2 loss: 0.304398  [   96/  265]
train() client id: f_00001-10-3 loss: 0.305331  [  128/  265]
train() client id: f_00001-10-4 loss: 0.401055  [  160/  265]
train() client id: f_00001-10-5 loss: 0.572143  [  192/  265]
train() client id: f_00001-10-6 loss: 0.299902  [  224/  265]
train() client id: f_00001-10-7 loss: 0.335719  [  256/  265]
train() client id: f_00001-11-0 loss: 0.483985  [   32/  265]
train() client id: f_00001-11-1 loss: 0.267127  [   64/  265]
train() client id: f_00001-11-2 loss: 0.294070  [   96/  265]
train() client id: f_00001-11-3 loss: 0.468316  [  128/  265]
train() client id: f_00001-11-4 loss: 0.421571  [  160/  265]
train() client id: f_00001-11-5 loss: 0.286563  [  192/  265]
train() client id: f_00001-11-6 loss: 0.423435  [  224/  265]
train() client id: f_00001-11-7 loss: 0.389273  [  256/  265]
train() client id: f_00002-0-0 loss: 1.102031  [   32/  124]
train() client id: f_00002-0-1 loss: 1.019504  [   64/  124]
train() client id: f_00002-0-2 loss: 1.044068  [   96/  124]
train() client id: f_00002-1-0 loss: 1.033380  [   32/  124]
train() client id: f_00002-1-1 loss: 0.986159  [   64/  124]
train() client id: f_00002-1-2 loss: 0.985342  [   96/  124]
train() client id: f_00002-2-0 loss: 1.048550  [   32/  124]
train() client id: f_00002-2-1 loss: 0.991453  [   64/  124]
train() client id: f_00002-2-2 loss: 0.874271  [   96/  124]
train() client id: f_00002-3-0 loss: 0.840272  [   32/  124]
train() client id: f_00002-3-1 loss: 0.981361  [   64/  124]
train() client id: f_00002-3-2 loss: 0.955529  [   96/  124]
train() client id: f_00002-4-0 loss: 0.864468  [   32/  124]
train() client id: f_00002-4-1 loss: 0.854422  [   64/  124]
train() client id: f_00002-4-2 loss: 0.826880  [   96/  124]
train() client id: f_00002-5-0 loss: 0.838026  [   32/  124]
train() client id: f_00002-5-1 loss: 0.992260  [   64/  124]
train() client id: f_00002-5-2 loss: 0.843601  [   96/  124]
train() client id: f_00002-6-0 loss: 0.969576  [   32/  124]
train() client id: f_00002-6-1 loss: 0.878925  [   64/  124]
train() client id: f_00002-6-2 loss: 0.646184  [   96/  124]
train() client id: f_00002-7-0 loss: 0.868477  [   32/  124]
train() client id: f_00002-7-1 loss: 0.772350  [   64/  124]
train() client id: f_00002-7-2 loss: 0.918490  [   96/  124]
train() client id: f_00002-8-0 loss: 0.795266  [   32/  124]
train() client id: f_00002-8-1 loss: 0.802941  [   64/  124]
train() client id: f_00002-8-2 loss: 0.797460  [   96/  124]
train() client id: f_00002-9-0 loss: 0.872465  [   32/  124]
train() client id: f_00002-9-1 loss: 0.789116  [   64/  124]
train() client id: f_00002-9-2 loss: 0.774771  [   96/  124]
train() client id: f_00002-10-0 loss: 0.846795  [   32/  124]
train() client id: f_00002-10-1 loss: 0.884789  [   64/  124]
train() client id: f_00002-10-2 loss: 0.799875  [   96/  124]
train() client id: f_00002-11-0 loss: 0.832343  [   32/  124]
train() client id: f_00002-11-1 loss: 0.808727  [   64/  124]
train() client id: f_00002-11-2 loss: 0.756245  [   96/  124]
train() client id: f_00003-0-0 loss: 0.533395  [   32/   43]
train() client id: f_00003-1-0 loss: 0.625094  [   32/   43]
train() client id: f_00003-2-0 loss: 0.569540  [   32/   43]
train() client id: f_00003-3-0 loss: 0.499249  [   32/   43]
train() client id: f_00003-4-0 loss: 0.744829  [   32/   43]
train() client id: f_00003-5-0 loss: 0.552165  [   32/   43]
train() client id: f_00003-6-0 loss: 0.584328  [   32/   43]
train() client id: f_00003-7-0 loss: 0.650730  [   32/   43]
train() client id: f_00003-8-0 loss: 0.779982  [   32/   43]
train() client id: f_00003-9-0 loss: 0.488973  [   32/   43]
train() client id: f_00003-10-0 loss: 0.534645  [   32/   43]
train() client id: f_00003-11-0 loss: 0.759086  [   32/   43]
train() client id: f_00004-0-0 loss: 0.751806  [   32/  306]
train() client id: f_00004-0-1 loss: 0.891402  [   64/  306]
train() client id: f_00004-0-2 loss: 0.866197  [   96/  306]
train() client id: f_00004-0-3 loss: 0.865820  [  128/  306]
train() client id: f_00004-0-4 loss: 0.824525  [  160/  306]
train() client id: f_00004-0-5 loss: 1.043613  [  192/  306]
train() client id: f_00004-0-6 loss: 1.035319  [  224/  306]
train() client id: f_00004-0-7 loss: 0.991370  [  256/  306]
train() client id: f_00004-0-8 loss: 0.931517  [  288/  306]
train() client id: f_00004-1-0 loss: 1.076430  [   32/  306]
train() client id: f_00004-1-1 loss: 0.888542  [   64/  306]
train() client id: f_00004-1-2 loss: 0.827493  [   96/  306]
train() client id: f_00004-1-3 loss: 0.874316  [  128/  306]
train() client id: f_00004-1-4 loss: 0.943679  [  160/  306]
train() client id: f_00004-1-5 loss: 0.833834  [  192/  306]
train() client id: f_00004-1-6 loss: 1.046677  [  224/  306]
train() client id: f_00004-1-7 loss: 0.830655  [  256/  306]
train() client id: f_00004-1-8 loss: 0.772942  [  288/  306]
train() client id: f_00004-2-0 loss: 0.815279  [   32/  306]
train() client id: f_00004-2-1 loss: 0.800441  [   64/  306]
train() client id: f_00004-2-2 loss: 0.968599  [   96/  306]
train() client id: f_00004-2-3 loss: 0.869894  [  128/  306]
train() client id: f_00004-2-4 loss: 0.889455  [  160/  306]
train() client id: f_00004-2-5 loss: 0.974815  [  192/  306]
train() client id: f_00004-2-6 loss: 0.957668  [  224/  306]
train() client id: f_00004-2-7 loss: 0.933270  [  256/  306]
train() client id: f_00004-2-8 loss: 0.857762  [  288/  306]
train() client id: f_00004-3-0 loss: 0.999848  [   32/  306]
train() client id: f_00004-3-1 loss: 0.767578  [   64/  306]
train() client id: f_00004-3-2 loss: 0.868446  [   96/  306]
train() client id: f_00004-3-3 loss: 1.000342  [  128/  306]
train() client id: f_00004-3-4 loss: 0.909953  [  160/  306]
train() client id: f_00004-3-5 loss: 0.814298  [  192/  306]
train() client id: f_00004-3-6 loss: 0.947647  [  224/  306]
train() client id: f_00004-3-7 loss: 0.876567  [  256/  306]
train() client id: f_00004-3-8 loss: 0.982589  [  288/  306]
train() client id: f_00004-4-0 loss: 0.894609  [   32/  306]
train() client id: f_00004-4-1 loss: 0.919618  [   64/  306]
train() client id: f_00004-4-2 loss: 0.942166  [   96/  306]
train() client id: f_00004-4-3 loss: 0.861475  [  128/  306]
train() client id: f_00004-4-4 loss: 0.793847  [  160/  306]
train() client id: f_00004-4-5 loss: 0.954301  [  192/  306]
train() client id: f_00004-4-6 loss: 0.958848  [  224/  306]
train() client id: f_00004-4-7 loss: 0.918147  [  256/  306]
train() client id: f_00004-4-8 loss: 0.866669  [  288/  306]
train() client id: f_00004-5-0 loss: 0.945472  [   32/  306]
train() client id: f_00004-5-1 loss: 1.049069  [   64/  306]
train() client id: f_00004-5-2 loss: 0.945259  [   96/  306]
train() client id: f_00004-5-3 loss: 0.735247  [  128/  306]
train() client id: f_00004-5-4 loss: 0.922672  [  160/  306]
train() client id: f_00004-5-5 loss: 0.935748  [  192/  306]
train() client id: f_00004-5-6 loss: 0.897994  [  224/  306]
train() client id: f_00004-5-7 loss: 0.960711  [  256/  306]
train() client id: f_00004-5-8 loss: 0.804638  [  288/  306]
train() client id: f_00004-6-0 loss: 0.900669  [   32/  306]
train() client id: f_00004-6-1 loss: 0.875961  [   64/  306]
train() client id: f_00004-6-2 loss: 0.848836  [   96/  306]
train() client id: f_00004-6-3 loss: 0.924055  [  128/  306]
train() client id: f_00004-6-4 loss: 1.092204  [  160/  306]
train() client id: f_00004-6-5 loss: 0.892109  [  192/  306]
train() client id: f_00004-6-6 loss: 0.971420  [  224/  306]
train() client id: f_00004-6-7 loss: 0.809553  [  256/  306]
train() client id: f_00004-6-8 loss: 0.818789  [  288/  306]
train() client id: f_00004-7-0 loss: 0.976199  [   32/  306]
train() client id: f_00004-7-1 loss: 0.937969  [   64/  306]
train() client id: f_00004-7-2 loss: 0.902768  [   96/  306]
train() client id: f_00004-7-3 loss: 0.841919  [  128/  306]
train() client id: f_00004-7-4 loss: 0.830025  [  160/  306]
train() client id: f_00004-7-5 loss: 0.829658  [  192/  306]
train() client id: f_00004-7-6 loss: 0.853570  [  224/  306]
train() client id: f_00004-7-7 loss: 0.930751  [  256/  306]
train() client id: f_00004-7-8 loss: 0.955833  [  288/  306]
train() client id: f_00004-8-0 loss: 0.842698  [   32/  306]
train() client id: f_00004-8-1 loss: 0.946335  [   64/  306]
train() client id: f_00004-8-2 loss: 0.845504  [   96/  306]
train() client id: f_00004-8-3 loss: 0.984276  [  128/  306]
train() client id: f_00004-8-4 loss: 0.870276  [  160/  306]
train() client id: f_00004-8-5 loss: 0.774626  [  192/  306]
train() client id: f_00004-8-6 loss: 1.025824  [  224/  306]
train() client id: f_00004-8-7 loss: 0.867723  [  256/  306]
train() client id: f_00004-8-8 loss: 0.905320  [  288/  306]
train() client id: f_00004-9-0 loss: 0.926949  [   32/  306]
train() client id: f_00004-9-1 loss: 1.000584  [   64/  306]
train() client id: f_00004-9-2 loss: 0.888894  [   96/  306]
train() client id: f_00004-9-3 loss: 0.827267  [  128/  306]
train() client id: f_00004-9-4 loss: 0.896613  [  160/  306]
train() client id: f_00004-9-5 loss: 0.921891  [  192/  306]
train() client id: f_00004-9-6 loss: 0.901015  [  224/  306]
train() client id: f_00004-9-7 loss: 0.863403  [  256/  306]
train() client id: f_00004-9-8 loss: 0.838057  [  288/  306]
train() client id: f_00004-10-0 loss: 0.852434  [   32/  306]
train() client id: f_00004-10-1 loss: 1.041745  [   64/  306]
train() client id: f_00004-10-2 loss: 0.883850  [   96/  306]
train() client id: f_00004-10-3 loss: 0.867810  [  128/  306]
train() client id: f_00004-10-4 loss: 0.889272  [  160/  306]
train() client id: f_00004-10-5 loss: 0.875427  [  192/  306]
train() client id: f_00004-10-6 loss: 0.845934  [  224/  306]
train() client id: f_00004-10-7 loss: 0.914297  [  256/  306]
train() client id: f_00004-10-8 loss: 0.920536  [  288/  306]
train() client id: f_00004-11-0 loss: 0.964175  [   32/  306]
train() client id: f_00004-11-1 loss: 0.965311  [   64/  306]
train() client id: f_00004-11-2 loss: 0.918867  [   96/  306]
train() client id: f_00004-11-3 loss: 0.873885  [  128/  306]
train() client id: f_00004-11-4 loss: 0.853452  [  160/  306]
train() client id: f_00004-11-5 loss: 0.923304  [  192/  306]
train() client id: f_00004-11-6 loss: 0.867336  [  224/  306]
train() client id: f_00004-11-7 loss: 0.887870  [  256/  306]
train() client id: f_00004-11-8 loss: 0.827912  [  288/  306]
train() client id: f_00005-0-0 loss: 0.718811  [   32/  146]
train() client id: f_00005-0-1 loss: 0.479665  [   64/  146]
train() client id: f_00005-0-2 loss: 0.625808  [   96/  146]
train() client id: f_00005-0-3 loss: 0.348058  [  128/  146]
train() client id: f_00005-1-0 loss: 0.536373  [   32/  146]
train() client id: f_00005-1-1 loss: 0.580607  [   64/  146]
train() client id: f_00005-1-2 loss: 0.451559  [   96/  146]
train() client id: f_00005-1-3 loss: 0.699589  [  128/  146]
train() client id: f_00005-2-0 loss: 0.632084  [   32/  146]
train() client id: f_00005-2-1 loss: 0.436326  [   64/  146]
train() client id: f_00005-2-2 loss: 0.631274  [   96/  146]
train() client id: f_00005-2-3 loss: 0.531590  [  128/  146]
train() client id: f_00005-3-0 loss: 0.453616  [   32/  146]
train() client id: f_00005-3-1 loss: 0.637436  [   64/  146]
train() client id: f_00005-3-2 loss: 0.597842  [   96/  146]
train() client id: f_00005-3-3 loss: 0.347900  [  128/  146]
train() client id: f_00005-4-0 loss: 0.490277  [   32/  146]
train() client id: f_00005-4-1 loss: 0.547039  [   64/  146]
train() client id: f_00005-4-2 loss: 0.558689  [   96/  146]
train() client id: f_00005-4-3 loss: 0.603349  [  128/  146]
train() client id: f_00005-5-0 loss: 0.649870  [   32/  146]
train() client id: f_00005-5-1 loss: 0.437492  [   64/  146]
train() client id: f_00005-5-2 loss: 0.624558  [   96/  146]
train() client id: f_00005-5-3 loss: 0.498481  [  128/  146]
train() client id: f_00005-6-0 loss: 0.576660  [   32/  146]
train() client id: f_00005-6-1 loss: 0.426079  [   64/  146]
train() client id: f_00005-6-2 loss: 0.595573  [   96/  146]
train() client id: f_00005-6-3 loss: 0.486758  [  128/  146]
train() client id: f_00005-7-0 loss: 0.594822  [   32/  146]
train() client id: f_00005-7-1 loss: 0.590513  [   64/  146]
train() client id: f_00005-7-2 loss: 0.305287  [   96/  146]
train() client id: f_00005-7-3 loss: 0.607998  [  128/  146]
train() client id: f_00005-8-0 loss: 0.318695  [   32/  146]
train() client id: f_00005-8-1 loss: 0.632270  [   64/  146]
train() client id: f_00005-8-2 loss: 0.524996  [   96/  146]
train() client id: f_00005-8-3 loss: 0.571450  [  128/  146]
train() client id: f_00005-9-0 loss: 0.417809  [   32/  146]
train() client id: f_00005-9-1 loss: 0.554085  [   64/  146]
train() client id: f_00005-9-2 loss: 0.550198  [   96/  146]
train() client id: f_00005-9-3 loss: 0.377619  [  128/  146]
train() client id: f_00005-10-0 loss: 0.571499  [   32/  146]
train() client id: f_00005-10-1 loss: 0.467810  [   64/  146]
train() client id: f_00005-10-2 loss: 0.430183  [   96/  146]
train() client id: f_00005-10-3 loss: 0.628393  [  128/  146]
train() client id: f_00005-11-0 loss: 0.351181  [   32/  146]
train() client id: f_00005-11-1 loss: 0.591859  [   64/  146]
train() client id: f_00005-11-2 loss: 0.354448  [   96/  146]
train() client id: f_00005-11-3 loss: 0.466999  [  128/  146]
train() client id: f_00006-0-0 loss: 0.541371  [   32/   54]
train() client id: f_00006-1-0 loss: 0.442144  [   32/   54]
train() client id: f_00006-2-0 loss: 0.449132  [   32/   54]
train() client id: f_00006-3-0 loss: 0.455119  [   32/   54]
train() client id: f_00006-4-0 loss: 0.487546  [   32/   54]
train() client id: f_00006-5-0 loss: 0.467982  [   32/   54]
train() client id: f_00006-6-0 loss: 0.542371  [   32/   54]
train() client id: f_00006-7-0 loss: 0.500756  [   32/   54]
train() client id: f_00006-8-0 loss: 0.503852  [   32/   54]
train() client id: f_00006-9-0 loss: 0.553784  [   32/   54]
train() client id: f_00006-10-0 loss: 0.558465  [   32/   54]
train() client id: f_00006-11-0 loss: 0.489871  [   32/   54]
train() client id: f_00007-0-0 loss: 0.610875  [   32/  179]
train() client id: f_00007-0-1 loss: 0.749462  [   64/  179]
train() client id: f_00007-0-2 loss: 0.872778  [   96/  179]
train() client id: f_00007-0-3 loss: 0.843914  [  128/  179]
train() client id: f_00007-0-4 loss: 0.666498  [  160/  179]
train() client id: f_00007-1-0 loss: 0.830735  [   32/  179]
train() client id: f_00007-1-1 loss: 0.581550  [   64/  179]
train() client id: f_00007-1-2 loss: 0.648770  [   96/  179]
train() client id: f_00007-1-3 loss: 0.780233  [  128/  179]
train() client id: f_00007-1-4 loss: 0.733724  [  160/  179]
train() client id: f_00007-2-0 loss: 0.612644  [   32/  179]
train() client id: f_00007-2-1 loss: 0.618936  [   64/  179]
train() client id: f_00007-2-2 loss: 0.754380  [   96/  179]
train() client id: f_00007-2-3 loss: 0.771224  [  128/  179]
train() client id: f_00007-2-4 loss: 0.805387  [  160/  179]
train() client id: f_00007-3-0 loss: 0.677507  [   32/  179]
train() client id: f_00007-3-1 loss: 0.843751  [   64/  179]
train() client id: f_00007-3-2 loss: 0.569823  [   96/  179]
train() client id: f_00007-3-3 loss: 0.610012  [  128/  179]
train() client id: f_00007-3-4 loss: 0.919987  [  160/  179]
train() client id: f_00007-4-0 loss: 0.691708  [   32/  179]
train() client id: f_00007-4-1 loss: 0.798794  [   64/  179]
train() client id: f_00007-4-2 loss: 0.560939  [   96/  179]
train() client id: f_00007-4-3 loss: 0.722230  [  128/  179]
train() client id: f_00007-4-4 loss: 0.781183  [  160/  179]
train() client id: f_00007-5-0 loss: 0.698507  [   32/  179]
train() client id: f_00007-5-1 loss: 0.747363  [   64/  179]
train() client id: f_00007-5-2 loss: 0.733502  [   96/  179]
train() client id: f_00007-5-3 loss: 0.636127  [  128/  179]
train() client id: f_00007-5-4 loss: 0.597844  [  160/  179]
train() client id: f_00007-6-0 loss: 0.902206  [   32/  179]
train() client id: f_00007-6-1 loss: 0.690422  [   64/  179]
train() client id: f_00007-6-2 loss: 0.662940  [   96/  179]
train() client id: f_00007-6-3 loss: 0.821788  [  128/  179]
train() client id: f_00007-6-4 loss: 0.544970  [  160/  179]
train() client id: f_00007-7-0 loss: 0.589337  [   32/  179]
train() client id: f_00007-7-1 loss: 0.684665  [   64/  179]
train() client id: f_00007-7-2 loss: 0.699575  [   96/  179]
train() client id: f_00007-7-3 loss: 0.571309  [  128/  179]
train() client id: f_00007-7-4 loss: 0.699156  [  160/  179]
train() client id: f_00007-8-0 loss: 0.822233  [   32/  179]
train() client id: f_00007-8-1 loss: 0.968879  [   64/  179]
train() client id: f_00007-8-2 loss: 0.579352  [   96/  179]
train() client id: f_00007-8-3 loss: 0.634161  [  128/  179]
train() client id: f_00007-8-4 loss: 0.577673  [  160/  179]
train() client id: f_00007-9-0 loss: 0.725692  [   32/  179]
train() client id: f_00007-9-1 loss: 0.647226  [   64/  179]
train() client id: f_00007-9-2 loss: 0.564586  [   96/  179]
train() client id: f_00007-9-3 loss: 0.642451  [  128/  179]
train() client id: f_00007-9-4 loss: 0.817098  [  160/  179]
train() client id: f_00007-10-0 loss: 0.559908  [   32/  179]
train() client id: f_00007-10-1 loss: 0.784314  [   64/  179]
train() client id: f_00007-10-2 loss: 0.877226  [   96/  179]
train() client id: f_00007-10-3 loss: 0.645090  [  128/  179]
train() client id: f_00007-10-4 loss: 0.648018  [  160/  179]
train() client id: f_00007-11-0 loss: 0.655576  [   32/  179]
train() client id: f_00007-11-1 loss: 0.775403  [   64/  179]
train() client id: f_00007-11-2 loss: 0.730493  [   96/  179]
train() client id: f_00007-11-3 loss: 0.854893  [  128/  179]
train() client id: f_00007-11-4 loss: 0.563049  [  160/  179]
train() client id: f_00008-0-0 loss: 0.763393  [   32/  130]
train() client id: f_00008-0-1 loss: 0.664140  [   64/  130]
train() client id: f_00008-0-2 loss: 0.644959  [   96/  130]
train() client id: f_00008-0-3 loss: 0.767384  [  128/  130]
train() client id: f_00008-1-0 loss: 0.677443  [   32/  130]
train() client id: f_00008-1-1 loss: 0.648207  [   64/  130]
train() client id: f_00008-1-2 loss: 0.794807  [   96/  130]
train() client id: f_00008-1-3 loss: 0.727910  [  128/  130]
train() client id: f_00008-2-0 loss: 0.729365  [   32/  130]
train() client id: f_00008-2-1 loss: 0.613311  [   64/  130]
train() client id: f_00008-2-2 loss: 0.748875  [   96/  130]
train() client id: f_00008-2-3 loss: 0.750501  [  128/  130]
train() client id: f_00008-3-0 loss: 0.794081  [   32/  130]
train() client id: f_00008-3-1 loss: 0.666249  [   64/  130]
train() client id: f_00008-3-2 loss: 0.724666  [   96/  130]
train() client id: f_00008-3-3 loss: 0.688718  [  128/  130]
train() client id: f_00008-4-0 loss: 0.789467  [   32/  130]
train() client id: f_00008-4-1 loss: 0.694921  [   64/  130]
train() client id: f_00008-4-2 loss: 0.776239  [   96/  130]
train() client id: f_00008-4-3 loss: 0.612233  [  128/  130]
train() client id: f_00008-5-0 loss: 0.639842  [   32/  130]
train() client id: f_00008-5-1 loss: 0.674055  [   64/  130]
train() client id: f_00008-5-2 loss: 0.868469  [   96/  130]
train() client id: f_00008-5-3 loss: 0.682513  [  128/  130]
train() client id: f_00008-6-0 loss: 0.628336  [   32/  130]
train() client id: f_00008-6-1 loss: 0.771049  [   64/  130]
train() client id: f_00008-6-2 loss: 0.784158  [   96/  130]
train() client id: f_00008-6-3 loss: 0.682486  [  128/  130]
train() client id: f_00008-7-0 loss: 0.747448  [   32/  130]
train() client id: f_00008-7-1 loss: 0.671553  [   64/  130]
train() client id: f_00008-7-2 loss: 0.705805  [   96/  130]
train() client id: f_00008-7-3 loss: 0.744865  [  128/  130]
train() client id: f_00008-8-0 loss: 0.631410  [   32/  130]
train() client id: f_00008-8-1 loss: 0.858090  [   64/  130]
train() client id: f_00008-8-2 loss: 0.691736  [   96/  130]
train() client id: f_00008-8-3 loss: 0.686676  [  128/  130]
train() client id: f_00008-9-0 loss: 0.650210  [   32/  130]
train() client id: f_00008-9-1 loss: 0.751059  [   64/  130]
train() client id: f_00008-9-2 loss: 0.772282  [   96/  130]
train() client id: f_00008-9-3 loss: 0.676970  [  128/  130]
train() client id: f_00008-10-0 loss: 0.734017  [   32/  130]
train() client id: f_00008-10-1 loss: 0.700058  [   64/  130]
train() client id: f_00008-10-2 loss: 0.671263  [   96/  130]
train() client id: f_00008-10-3 loss: 0.726717  [  128/  130]
train() client id: f_00008-11-0 loss: 0.668882  [   32/  130]
train() client id: f_00008-11-1 loss: 0.641895  [   64/  130]
train() client id: f_00008-11-2 loss: 0.707350  [   96/  130]
train() client id: f_00008-11-3 loss: 0.839645  [  128/  130]
train() client id: f_00009-0-0 loss: 1.226675  [   32/  118]
train() client id: f_00009-0-1 loss: 1.461704  [   64/  118]
train() client id: f_00009-0-2 loss: 1.124875  [   96/  118]
train() client id: f_00009-1-0 loss: 1.295102  [   32/  118]
train() client id: f_00009-1-1 loss: 1.281215  [   64/  118]
train() client id: f_00009-1-2 loss: 1.112729  [   96/  118]
train() client id: f_00009-2-0 loss: 1.168408  [   32/  118]
train() client id: f_00009-2-1 loss: 1.187557  [   64/  118]
train() client id: f_00009-2-2 loss: 1.220541  [   96/  118]
train() client id: f_00009-3-0 loss: 1.232058  [   32/  118]
train() client id: f_00009-3-1 loss: 1.155389  [   64/  118]
train() client id: f_00009-3-2 loss: 1.172796  [   96/  118]
train() client id: f_00009-4-0 loss: 1.155505  [   32/  118]
train() client id: f_00009-4-1 loss: 1.174040  [   64/  118]
train() client id: f_00009-4-2 loss: 1.004143  [   96/  118]
train() client id: f_00009-5-0 loss: 1.022185  [   32/  118]
train() client id: f_00009-5-1 loss: 1.056617  [   64/  118]
train() client id: f_00009-5-2 loss: 1.110979  [   96/  118]
train() client id: f_00009-6-0 loss: 1.220426  [   32/  118]
train() client id: f_00009-6-1 loss: 0.960783  [   64/  118]
train() client id: f_00009-6-2 loss: 1.001531  [   96/  118]
train() client id: f_00009-7-0 loss: 0.987183  [   32/  118]
train() client id: f_00009-7-1 loss: 1.097552  [   64/  118]
train() client id: f_00009-7-2 loss: 1.064854  [   96/  118]
train() client id: f_00009-8-0 loss: 1.126597  [   32/  118]
train() client id: f_00009-8-1 loss: 0.956217  [   64/  118]
train() client id: f_00009-8-2 loss: 1.016596  [   96/  118]
train() client id: f_00009-9-0 loss: 1.041762  [   32/  118]
train() client id: f_00009-9-1 loss: 1.013386  [   64/  118]
train() client id: f_00009-9-2 loss: 1.041402  [   96/  118]
train() client id: f_00009-10-0 loss: 0.972213  [   32/  118]
train() client id: f_00009-10-1 loss: 1.035737  [   64/  118]
train() client id: f_00009-10-2 loss: 1.050799  [   96/  118]
train() client id: f_00009-11-0 loss: 1.007570  [   32/  118]
train() client id: f_00009-11-1 loss: 0.871467  [   64/  118]
train() client id: f_00009-11-2 loss: 1.096946  [   96/  118]
At round 34 accuracy: 0.6551724137931034
At round 34 training accuracy: 0.5848423876592891
At round 34 training loss: 0.8291508132734675
update_location
xs = [  -3.9056584     4.20031788  190.00902392   18.81129433    0.97929623
    3.95640986 -152.44319194 -131.32485185  174.66397685 -117.06087855]
ys = [ 182.5879595   165.55583871    1.32061395 -152.45517586  144.35018685
  127.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [208.21531433 193.45898377 214.72115217 183.29333169 175.60733317
 162.33333086 182.33435582 165.066329   202.03013335 154.01058776]
dists_bs = [171.26252029 179.44900699 404.22285522 380.51243082 178.62845167
 185.07269113 179.05151394 179.56996694 383.47022645 180.54759768]
uav_gains = [1.51163930e-11 1.87414015e-11 1.37010772e-11 2.16985605e-11
 2.42798461e-11 2.96968280e-11 2.20025346e-11 2.84631267e-11
 1.65569782e-11 3.39218710e-11]
bs_gains = [6.15232911e-11 5.39832614e-11 5.55591589e-12 6.58053658e-12
 5.46804790e-11 4.95148575e-11 5.43194911e-11 5.38815050e-11
 6.43940092e-12 5.30685578e-11]
Round 35
-------------------------------
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.3715199  13.18889052  6.27703667  2.2632532  15.21119563  7.32046252
  2.80542146  8.96352054  6.61377748  5.93751329]
obj_prev = 74.95259119677627
eta_min = 3.0936410554122904e-15	eta_max = 0.9287856551033156
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 17.39350337614215	eta = 0.9090909090909091
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 31.811065698035446	eta = 0.4970684084145405
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 24.73112036289277	eta = 0.6393675484357765
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.451279643601314	eta = 0.6742606815832007
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38394553843602	eta = 0.6762022161958229
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38374385979249	eta = 0.6762080482621735
eta = 0.6762080482621735
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.03228538 0.0679018  0.03177292 0.01101803 0.07840736 0.03741005
 0.01383659 0.04586574 0.03331032 0.03023552]
ene_total = [2.0907422  3.75044408 2.07789445 0.9851544  4.27493166 2.23178708
 1.1246433  2.70086556 2.2793763  1.86790483]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 0 obj = 4.291038018675944
eta = 0.6762080482621735
freqs = [34370497.98105174 68730577.98040815 34010002.78410535 11523739.23427672
 79334916.60468721 37963436.80656631 14462760.16316765 47433377.96218494
 37664422.59882781 30619785.51757285]
eta_min = 0.6762080482621772	eta_max = 0.6762080482621733
af = 0.011653184886415587	bf = 1.3848442141784587	zeta = 0.012818503375057147	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [2.32272370e-06 1.95344516e-05 2.23815612e-06 8.91067609e-08
 3.00542243e-05 3.28351691e-06 1.76259075e-07 6.28458343e-06
 2.87780483e-06 1.72640090e-06]
ene_total = [1.69608812 1.26623554 1.74159793 1.54622714 1.26484323 1.28577181
 1.54099513 1.45082842 2.18547748 1.2674389 ]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 1 obj = 4.291038018675939
eta = 0.6762080482621733
freqs = [34370497.98105174 68730577.98040818 34010002.78410535 11523739.23427672
 79334916.60468724 37963436.80656631 14462760.16316766 47433377.96218494
 37664422.59882782 30619785.51757286]
Done!
At round 35 energy consumption: 4.291038018675944
At round 35 eta: 0.6762080482621733
At round 35 local rounds: 12.811659702043181
At round 35 global rounds: 50.012046702878195
At round 35 a_n: 15.850952365358953
gradient difference: 0.516748309135437
train() client id: f_00000-0-0 loss: 1.180104  [   32/  126]
train() client id: f_00000-0-1 loss: 0.952908  [   64/  126]
train() client id: f_00000-0-2 loss: 0.970219  [   96/  126]
train() client id: f_00000-1-0 loss: 1.014087  [   32/  126]
train() client id: f_00000-1-1 loss: 0.778553  [   64/  126]
train() client id: f_00000-1-2 loss: 1.002422  [   96/  126]
train() client id: f_00000-2-0 loss: 0.898844  [   32/  126]
train() client id: f_00000-2-1 loss: 1.016217  [   64/  126]
train() client id: f_00000-2-2 loss: 0.795392  [   96/  126]
train() client id: f_00000-3-0 loss: 0.801656  [   32/  126]
train() client id: f_00000-3-1 loss: 0.891053  [   64/  126]
train() client id: f_00000-3-2 loss: 0.952438  [   96/  126]
train() client id: f_00000-4-0 loss: 0.931539  [   32/  126]
train() client id: f_00000-4-1 loss: 0.765489  [   64/  126]
train() client id: f_00000-4-2 loss: 0.762525  [   96/  126]
train() client id: f_00000-5-0 loss: 0.839581  [   32/  126]
train() client id: f_00000-5-1 loss: 0.689013  [   64/  126]
train() client id: f_00000-5-2 loss: 0.859699  [   96/  126]
train() client id: f_00000-6-0 loss: 0.718032  [   32/  126]
train() client id: f_00000-6-1 loss: 0.761687  [   64/  126]
train() client id: f_00000-6-2 loss: 0.870024  [   96/  126]
train() client id: f_00000-7-0 loss: 0.767058  [   32/  126]
train() client id: f_00000-7-1 loss: 0.813342  [   64/  126]
train() client id: f_00000-7-2 loss: 0.753132  [   96/  126]
train() client id: f_00000-8-0 loss: 0.868146  [   32/  126]
train() client id: f_00000-8-1 loss: 0.687346  [   64/  126]
train() client id: f_00000-8-2 loss: 0.850367  [   96/  126]
train() client id: f_00000-9-0 loss: 0.683540  [   32/  126]
train() client id: f_00000-9-1 loss: 0.863745  [   64/  126]
train() client id: f_00000-9-2 loss: 0.739165  [   96/  126]
train() client id: f_00000-10-0 loss: 0.786096  [   32/  126]
train() client id: f_00000-10-1 loss: 0.902020  [   64/  126]
train() client id: f_00000-10-2 loss: 0.828743  [   96/  126]
train() client id: f_00000-11-0 loss: 0.770434  [   32/  126]
train() client id: f_00000-11-1 loss: 0.697958  [   64/  126]
train() client id: f_00000-11-2 loss: 0.795688  [   96/  126]
train() client id: f_00001-0-0 loss: 0.383589  [   32/  265]
train() client id: f_00001-0-1 loss: 0.514130  [   64/  265]
train() client id: f_00001-0-2 loss: 0.428397  [   96/  265]
train() client id: f_00001-0-3 loss: 0.631360  [  128/  265]
train() client id: f_00001-0-4 loss: 0.411068  [  160/  265]
train() client id: f_00001-0-5 loss: 0.487673  [  192/  265]
train() client id: f_00001-0-6 loss: 0.456096  [  224/  265]
train() client id: f_00001-0-7 loss: 0.547290  [  256/  265]
train() client id: f_00001-1-0 loss: 0.595003  [   32/  265]
train() client id: f_00001-1-1 loss: 0.568990  [   64/  265]
train() client id: f_00001-1-2 loss: 0.435977  [   96/  265]
train() client id: f_00001-1-3 loss: 0.455628  [  128/  265]
train() client id: f_00001-1-4 loss: 0.470981  [  160/  265]
train() client id: f_00001-1-5 loss: 0.381949  [  192/  265]
train() client id: f_00001-1-6 loss: 0.462154  [  224/  265]
train() client id: f_00001-1-7 loss: 0.476516  [  256/  265]
train() client id: f_00001-2-0 loss: 0.453680  [   32/  265]
train() client id: f_00001-2-1 loss: 0.441274  [   64/  265]
train() client id: f_00001-2-2 loss: 0.626674  [   96/  265]
train() client id: f_00001-2-3 loss: 0.396049  [  128/  265]
train() client id: f_00001-2-4 loss: 0.389351  [  160/  265]
train() client id: f_00001-2-5 loss: 0.431530  [  192/  265]
train() client id: f_00001-2-6 loss: 0.501473  [  224/  265]
train() client id: f_00001-2-7 loss: 0.512307  [  256/  265]
train() client id: f_00001-3-0 loss: 0.388766  [   32/  265]
train() client id: f_00001-3-1 loss: 0.485326  [   64/  265]
train() client id: f_00001-3-2 loss: 0.455253  [   96/  265]
train() client id: f_00001-3-3 loss: 0.543897  [  128/  265]
train() client id: f_00001-3-4 loss: 0.605719  [  160/  265]
train() client id: f_00001-3-5 loss: 0.367624  [  192/  265]
train() client id: f_00001-3-6 loss: 0.382311  [  224/  265]
train() client id: f_00001-3-7 loss: 0.545511  [  256/  265]
train() client id: f_00001-4-0 loss: 0.436893  [   32/  265]
train() client id: f_00001-4-1 loss: 0.609929  [   64/  265]
train() client id: f_00001-4-2 loss: 0.383613  [   96/  265]
train() client id: f_00001-4-3 loss: 0.403903  [  128/  265]
train() client id: f_00001-4-4 loss: 0.447446  [  160/  265]
train() client id: f_00001-4-5 loss: 0.391011  [  192/  265]
train() client id: f_00001-4-6 loss: 0.607782  [  224/  265]
train() client id: f_00001-4-7 loss: 0.457789  [  256/  265]
train() client id: f_00001-5-0 loss: 0.424829  [   32/  265]
train() client id: f_00001-5-1 loss: 0.666311  [   64/  265]
train() client id: f_00001-5-2 loss: 0.423581  [   96/  265]
train() client id: f_00001-5-3 loss: 0.368545  [  128/  265]
train() client id: f_00001-5-4 loss: 0.511280  [  160/  265]
train() client id: f_00001-5-5 loss: 0.400515  [  192/  265]
train() client id: f_00001-5-6 loss: 0.538598  [  224/  265]
train() client id: f_00001-5-7 loss: 0.389816  [  256/  265]
train() client id: f_00001-6-0 loss: 0.452919  [   32/  265]
train() client id: f_00001-6-1 loss: 0.478717  [   64/  265]
train() client id: f_00001-6-2 loss: 0.355964  [   96/  265]
train() client id: f_00001-6-3 loss: 0.528477  [  128/  265]
train() client id: f_00001-6-4 loss: 0.418704  [  160/  265]
train() client id: f_00001-6-5 loss: 0.435826  [  192/  265]
train() client id: f_00001-6-6 loss: 0.483955  [  224/  265]
train() client id: f_00001-6-7 loss: 0.572634  [  256/  265]
train() client id: f_00001-7-0 loss: 0.498733  [   32/  265]
train() client id: f_00001-7-1 loss: 0.370908  [   64/  265]
train() client id: f_00001-7-2 loss: 0.417329  [   96/  265]
train() client id: f_00001-7-3 loss: 0.536573  [  128/  265]
train() client id: f_00001-7-4 loss: 0.506526  [  160/  265]
train() client id: f_00001-7-5 loss: 0.407956  [  192/  265]
train() client id: f_00001-7-6 loss: 0.361527  [  224/  265]
train() client id: f_00001-7-7 loss: 0.605652  [  256/  265]
train() client id: f_00001-8-0 loss: 0.384327  [   32/  265]
train() client id: f_00001-8-1 loss: 0.511131  [   64/  265]
train() client id: f_00001-8-2 loss: 0.492717  [   96/  265]
train() client id: f_00001-8-3 loss: 0.498399  [  128/  265]
train() client id: f_00001-8-4 loss: 0.375320  [  160/  265]
train() client id: f_00001-8-5 loss: 0.543509  [  192/  265]
train() client id: f_00001-8-6 loss: 0.454665  [  224/  265]
train() client id: f_00001-8-7 loss: 0.465437  [  256/  265]
train() client id: f_00001-9-0 loss: 0.375478  [   32/  265]
train() client id: f_00001-9-1 loss: 0.547153  [   64/  265]
train() client id: f_00001-9-2 loss: 0.454685  [   96/  265]
train() client id: f_00001-9-3 loss: 0.522256  [  128/  265]
train() client id: f_00001-9-4 loss: 0.446378  [  160/  265]
train() client id: f_00001-9-5 loss: 0.357527  [  192/  265]
train() client id: f_00001-9-6 loss: 0.535249  [  224/  265]
train() client id: f_00001-9-7 loss: 0.437266  [  256/  265]
train() client id: f_00001-10-0 loss: 0.409681  [   32/  265]
train() client id: f_00001-10-1 loss: 0.374780  [   64/  265]
train() client id: f_00001-10-2 loss: 0.420534  [   96/  265]
train() client id: f_00001-10-3 loss: 0.514560  [  128/  265]
train() client id: f_00001-10-4 loss: 0.561098  [  160/  265]
train() client id: f_00001-10-5 loss: 0.526078  [  192/  265]
train() client id: f_00001-10-6 loss: 0.458372  [  224/  265]
train() client id: f_00001-10-7 loss: 0.448695  [  256/  265]
train() client id: f_00001-11-0 loss: 0.486167  [   32/  265]
train() client id: f_00001-11-1 loss: 0.420506  [   64/  265]
train() client id: f_00001-11-2 loss: 0.372179  [   96/  265]
train() client id: f_00001-11-3 loss: 0.460519  [  128/  265]
train() client id: f_00001-11-4 loss: 0.455680  [  160/  265]
train() client id: f_00001-11-5 loss: 0.454244  [  192/  265]
train() client id: f_00001-11-6 loss: 0.577808  [  224/  265]
train() client id: f_00001-11-7 loss: 0.502450  [  256/  265]
train() client id: f_00002-0-0 loss: 1.361513  [   32/  124]
train() client id: f_00002-0-1 loss: 1.176619  [   64/  124]
train() client id: f_00002-0-2 loss: 1.043756  [   96/  124]
train() client id: f_00002-1-0 loss: 1.023271  [   32/  124]
train() client id: f_00002-1-1 loss: 1.360411  [   64/  124]
train() client id: f_00002-1-2 loss: 1.143106  [   96/  124]
train() client id: f_00002-2-0 loss: 1.226286  [   32/  124]
train() client id: f_00002-2-1 loss: 1.130443  [   64/  124]
train() client id: f_00002-2-2 loss: 1.135847  [   96/  124]
train() client id: f_00002-3-0 loss: 1.125197  [   32/  124]
train() client id: f_00002-3-1 loss: 1.208531  [   64/  124]
train() client id: f_00002-3-2 loss: 1.091201  [   96/  124]
train() client id: f_00002-4-0 loss: 1.185971  [   32/  124]
train() client id: f_00002-4-1 loss: 1.133664  [   64/  124]
train() client id: f_00002-4-2 loss: 0.991417  [   96/  124]
train() client id: f_00002-5-0 loss: 1.076467  [   32/  124]
train() client id: f_00002-5-1 loss: 1.067609  [   64/  124]
train() client id: f_00002-5-2 loss: 1.052140  [   96/  124]
train() client id: f_00002-6-0 loss: 1.033141  [   32/  124]
train() client id: f_00002-6-1 loss: 1.215042  [   64/  124]
train() client id: f_00002-6-2 loss: 0.838616  [   96/  124]
train() client id: f_00002-7-0 loss: 0.976695  [   32/  124]
train() client id: f_00002-7-1 loss: 0.920431  [   64/  124]
train() client id: f_00002-7-2 loss: 1.172360  [   96/  124]
train() client id: f_00002-8-0 loss: 0.988103  [   32/  124]
train() client id: f_00002-8-1 loss: 0.996724  [   64/  124]
train() client id: f_00002-8-2 loss: 1.057237  [   96/  124]
train() client id: f_00002-9-0 loss: 0.936675  [   32/  124]
train() client id: f_00002-9-1 loss: 1.184918  [   64/  124]
train() client id: f_00002-9-2 loss: 0.968942  [   96/  124]
train() client id: f_00002-10-0 loss: 1.073362  [   32/  124]
train() client id: f_00002-10-1 loss: 0.880147  [   64/  124]
train() client id: f_00002-10-2 loss: 0.963047  [   96/  124]
train() client id: f_00002-11-0 loss: 0.930126  [   32/  124]
train() client id: f_00002-11-1 loss: 0.999205  [   64/  124]
train() client id: f_00002-11-2 loss: 0.931306  [   96/  124]
train() client id: f_00003-0-0 loss: 0.532091  [   32/   43]
train() client id: f_00003-1-0 loss: 0.475580  [   32/   43]
train() client id: f_00003-2-0 loss: 0.418775  [   32/   43]
train() client id: f_00003-3-0 loss: 0.471596  [   32/   43]
train() client id: f_00003-4-0 loss: 0.424472  [   32/   43]
train() client id: f_00003-5-0 loss: 0.549009  [   32/   43]
train() client id: f_00003-6-0 loss: 0.373153  [   32/   43]
train() client id: f_00003-7-0 loss: 0.739592  [   32/   43]
train() client id: f_00003-8-0 loss: 0.618298  [   32/   43]
train() client id: f_00003-9-0 loss: 0.528448  [   32/   43]
train() client id: f_00003-10-0 loss: 0.554819  [   32/   43]
train() client id: f_00003-11-0 loss: 0.542146  [   32/   43]
train() client id: f_00004-0-0 loss: 0.854264  [   32/  306]
train() client id: f_00004-0-1 loss: 0.886427  [   64/  306]
train() client id: f_00004-0-2 loss: 1.032732  [   96/  306]
train() client id: f_00004-0-3 loss: 0.909393  [  128/  306]
train() client id: f_00004-0-4 loss: 0.828114  [  160/  306]
train() client id: f_00004-0-5 loss: 1.058234  [  192/  306]
train() client id: f_00004-0-6 loss: 0.695706  [  224/  306]
train() client id: f_00004-0-7 loss: 0.845628  [  256/  306]
train() client id: f_00004-0-8 loss: 0.951897  [  288/  306]
train() client id: f_00004-1-0 loss: 0.995360  [   32/  306]
train() client id: f_00004-1-1 loss: 0.917654  [   64/  306]
train() client id: f_00004-1-2 loss: 0.793671  [   96/  306]
train() client id: f_00004-1-3 loss: 0.861490  [  128/  306]
train() client id: f_00004-1-4 loss: 0.872864  [  160/  306]
train() client id: f_00004-1-5 loss: 0.919144  [  192/  306]
train() client id: f_00004-1-6 loss: 0.763020  [  224/  306]
train() client id: f_00004-1-7 loss: 0.987259  [  256/  306]
train() client id: f_00004-1-8 loss: 0.985585  [  288/  306]
train() client id: f_00004-2-0 loss: 1.017423  [   32/  306]
train() client id: f_00004-2-1 loss: 0.801021  [   64/  306]
train() client id: f_00004-2-2 loss: 0.876472  [   96/  306]
train() client id: f_00004-2-3 loss: 0.843240  [  128/  306]
train() client id: f_00004-2-4 loss: 0.977262  [  160/  306]
train() client id: f_00004-2-5 loss: 0.804817  [  192/  306]
train() client id: f_00004-2-6 loss: 0.944186  [  224/  306]
train() client id: f_00004-2-7 loss: 0.979132  [  256/  306]
train() client id: f_00004-2-8 loss: 0.769309  [  288/  306]
train() client id: f_00004-3-0 loss: 0.917248  [   32/  306]
train() client id: f_00004-3-1 loss: 0.847457  [   64/  306]
train() client id: f_00004-3-2 loss: 0.965841  [   96/  306]
train() client id: f_00004-3-3 loss: 0.833337  [  128/  306]
train() client id: f_00004-3-4 loss: 0.863939  [  160/  306]
train() client id: f_00004-3-5 loss: 0.929893  [  192/  306]
train() client id: f_00004-3-6 loss: 0.904618  [  224/  306]
train() client id: f_00004-3-7 loss: 0.826238  [  256/  306]
train() client id: f_00004-3-8 loss: 0.974256  [  288/  306]
train() client id: f_00004-4-0 loss: 0.808399  [   32/  306]
train() client id: f_00004-4-1 loss: 0.818577  [   64/  306]
train() client id: f_00004-4-2 loss: 0.826986  [   96/  306]
train() client id: f_00004-4-3 loss: 0.984688  [  128/  306]
train() client id: f_00004-4-4 loss: 0.836529  [  160/  306]
train() client id: f_00004-4-5 loss: 0.948634  [  192/  306]
train() client id: f_00004-4-6 loss: 0.931533  [  224/  306]
train() client id: f_00004-4-7 loss: 1.016972  [  256/  306]
train() client id: f_00004-4-8 loss: 0.874038  [  288/  306]
train() client id: f_00004-5-0 loss: 0.975333  [   32/  306]
train() client id: f_00004-5-1 loss: 0.946088  [   64/  306]
train() client id: f_00004-5-2 loss: 0.783193  [   96/  306]
train() client id: f_00004-5-3 loss: 0.866103  [  128/  306]
train() client id: f_00004-5-4 loss: 0.905675  [  160/  306]
train() client id: f_00004-5-5 loss: 0.892276  [  192/  306]
train() client id: f_00004-5-6 loss: 0.859562  [  224/  306]
train() client id: f_00004-5-7 loss: 0.864841  [  256/  306]
train() client id: f_00004-5-8 loss: 0.966358  [  288/  306]
train() client id: f_00004-6-0 loss: 0.886588  [   32/  306]
train() client id: f_00004-6-1 loss: 0.831108  [   64/  306]
train() client id: f_00004-6-2 loss: 0.789249  [   96/  306]
train() client id: f_00004-6-3 loss: 0.937311  [  128/  306]
train() client id: f_00004-6-4 loss: 0.959999  [  160/  306]
train() client id: f_00004-6-5 loss: 0.972308  [  192/  306]
train() client id: f_00004-6-6 loss: 0.859039  [  224/  306]
train() client id: f_00004-6-7 loss: 0.963952  [  256/  306]
train() client id: f_00004-6-8 loss: 0.964092  [  288/  306]
train() client id: f_00004-7-0 loss: 0.905728  [   32/  306]
train() client id: f_00004-7-1 loss: 0.794715  [   64/  306]
train() client id: f_00004-7-2 loss: 0.836108  [   96/  306]
train() client id: f_00004-7-3 loss: 0.874212  [  128/  306]
train() client id: f_00004-7-4 loss: 0.807772  [  160/  306]
train() client id: f_00004-7-5 loss: 0.958799  [  192/  306]
train() client id: f_00004-7-6 loss: 0.986149  [  224/  306]
train() client id: f_00004-7-7 loss: 0.922065  [  256/  306]
train() client id: f_00004-7-8 loss: 0.893100  [  288/  306]
train() client id: f_00004-8-0 loss: 0.789952  [   32/  306]
train() client id: f_00004-8-1 loss: 1.006317  [   64/  306]
train() client id: f_00004-8-2 loss: 0.835432  [   96/  306]
train() client id: f_00004-8-3 loss: 0.908274  [  128/  306]
train() client id: f_00004-8-4 loss: 0.839383  [  160/  306]
train() client id: f_00004-8-5 loss: 0.977585  [  192/  306]
train() client id: f_00004-8-6 loss: 0.963475  [  224/  306]
train() client id: f_00004-8-7 loss: 0.880178  [  256/  306]
train() client id: f_00004-8-8 loss: 0.909590  [  288/  306]
train() client id: f_00004-9-0 loss: 0.970226  [   32/  306]
train() client id: f_00004-9-1 loss: 0.791870  [   64/  306]
train() client id: f_00004-9-2 loss: 0.913483  [   96/  306]
train() client id: f_00004-9-3 loss: 0.930401  [  128/  306]
train() client id: f_00004-9-4 loss: 0.879408  [  160/  306]
train() client id: f_00004-9-5 loss: 1.109473  [  192/  306]
train() client id: f_00004-9-6 loss: 0.905961  [  224/  306]
train() client id: f_00004-9-7 loss: 0.801254  [  256/  306]
train() client id: f_00004-9-8 loss: 0.881822  [  288/  306]
train() client id: f_00004-10-0 loss: 0.947699  [   32/  306]
train() client id: f_00004-10-1 loss: 0.991137  [   64/  306]
train() client id: f_00004-10-2 loss: 0.879647  [   96/  306]
train() client id: f_00004-10-3 loss: 0.907061  [  128/  306]
train() client id: f_00004-10-4 loss: 0.963148  [  160/  306]
train() client id: f_00004-10-5 loss: 0.851732  [  192/  306]
train() client id: f_00004-10-6 loss: 0.840886  [  224/  306]
train() client id: f_00004-10-7 loss: 0.900935  [  256/  306]
train() client id: f_00004-10-8 loss: 0.788236  [  288/  306]
train() client id: f_00004-11-0 loss: 0.952536  [   32/  306]
train() client id: f_00004-11-1 loss: 0.796133  [   64/  306]
train() client id: f_00004-11-2 loss: 1.075113  [   96/  306]
train() client id: f_00004-11-3 loss: 0.859561  [  128/  306]
train() client id: f_00004-11-4 loss: 0.815098  [  160/  306]
train() client id: f_00004-11-5 loss: 0.810390  [  192/  306]
train() client id: f_00004-11-6 loss: 0.900864  [  224/  306]
train() client id: f_00004-11-7 loss: 0.935416  [  256/  306]
train() client id: f_00004-11-8 loss: 0.980926  [  288/  306]
train() client id: f_00005-0-0 loss: 0.643301  [   32/  146]
train() client id: f_00005-0-1 loss: 0.608594  [   64/  146]
train() client id: f_00005-0-2 loss: 0.897534  [   96/  146]
train() client id: f_00005-0-3 loss: 0.635146  [  128/  146]
train() client id: f_00005-1-0 loss: 0.752031  [   32/  146]
train() client id: f_00005-1-1 loss: 0.714815  [   64/  146]
train() client id: f_00005-1-2 loss: 0.673710  [   96/  146]
train() client id: f_00005-1-3 loss: 0.542715  [  128/  146]
train() client id: f_00005-2-0 loss: 0.670107  [   32/  146]
train() client id: f_00005-2-1 loss: 0.655646  [   64/  146]
train() client id: f_00005-2-2 loss: 0.633831  [   96/  146]
train() client id: f_00005-2-3 loss: 0.695012  [  128/  146]
train() client id: f_00005-3-0 loss: 0.509442  [   32/  146]
train() client id: f_00005-3-1 loss: 0.586649  [   64/  146]
train() client id: f_00005-3-2 loss: 0.786815  [   96/  146]
train() client id: f_00005-3-3 loss: 0.707154  [  128/  146]
train() client id: f_00005-4-0 loss: 0.535051  [   32/  146]
train() client id: f_00005-4-1 loss: 0.663428  [   64/  146]
train() client id: f_00005-4-2 loss: 0.753526  [   96/  146]
train() client id: f_00005-4-3 loss: 0.554682  [  128/  146]
train() client id: f_00005-5-0 loss: 0.675001  [   32/  146]
train() client id: f_00005-5-1 loss: 0.590790  [   64/  146]
train() client id: f_00005-5-2 loss: 0.548873  [   96/  146]
train() client id: f_00005-5-3 loss: 0.490957  [  128/  146]
train() client id: f_00005-6-0 loss: 0.627228  [   32/  146]
train() client id: f_00005-6-1 loss: 0.629271  [   64/  146]
train() client id: f_00005-6-2 loss: 0.669036  [   96/  146]
train() client id: f_00005-6-3 loss: 0.585037  [  128/  146]
train() client id: f_00005-7-0 loss: 0.588151  [   32/  146]
train() client id: f_00005-7-1 loss: 0.641333  [   64/  146]
train() client id: f_00005-7-2 loss: 0.707649  [   96/  146]
train() client id: f_00005-7-3 loss: 0.848124  [  128/  146]
train() client id: f_00005-8-0 loss: 0.413454  [   32/  146]
train() client id: f_00005-8-1 loss: 0.557431  [   64/  146]
train() client id: f_00005-8-2 loss: 0.624317  [   96/  146]
train() client id: f_00005-8-3 loss: 0.909530  [  128/  146]
train() client id: f_00005-9-0 loss: 0.622350  [   32/  146]
train() client id: f_00005-9-1 loss: 0.496454  [   64/  146]
train() client id: f_00005-9-2 loss: 0.680985  [   96/  146]
train() client id: f_00005-9-3 loss: 0.760006  [  128/  146]
train() client id: f_00005-10-0 loss: 0.773666  [   32/  146]
train() client id: f_00005-10-1 loss: 0.521952  [   64/  146]
train() client id: f_00005-10-2 loss: 0.729785  [   96/  146]
train() client id: f_00005-10-3 loss: 0.608219  [  128/  146]
train() client id: f_00005-11-0 loss: 0.715532  [   32/  146]
train() client id: f_00005-11-1 loss: 0.637537  [   64/  146]
train() client id: f_00005-11-2 loss: 0.689307  [   96/  146]
train() client id: f_00005-11-3 loss: 0.491241  [  128/  146]
train() client id: f_00006-0-0 loss: 0.544337  [   32/   54]
train() client id: f_00006-1-0 loss: 0.524118  [   32/   54]
train() client id: f_00006-2-0 loss: 0.529775  [   32/   54]
train() client id: f_00006-3-0 loss: 0.577001  [   32/   54]
train() client id: f_00006-4-0 loss: 0.537434  [   32/   54]
train() client id: f_00006-5-0 loss: 0.569384  [   32/   54]
train() client id: f_00006-6-0 loss: 0.550076  [   32/   54]
train() client id: f_00006-7-0 loss: 0.569618  [   32/   54]
train() client id: f_00006-8-0 loss: 0.521749  [   32/   54]
train() client id: f_00006-9-0 loss: 0.567563  [   32/   54]
train() client id: f_00006-10-0 loss: 0.530773  [   32/   54]
train() client id: f_00006-11-0 loss: 0.584341  [   32/   54]
train() client id: f_00007-0-0 loss: 0.697950  [   32/  179]
train() client id: f_00007-0-1 loss: 0.796102  [   64/  179]
train() client id: f_00007-0-2 loss: 0.718029  [   96/  179]
train() client id: f_00007-0-3 loss: 0.844301  [  128/  179]
train() client id: f_00007-0-4 loss: 0.584460  [  160/  179]
train() client id: f_00007-1-0 loss: 0.695848  [   32/  179]
train() client id: f_00007-1-1 loss: 0.919996  [   64/  179]
train() client id: f_00007-1-2 loss: 0.616684  [   96/  179]
train() client id: f_00007-1-3 loss: 0.702054  [  128/  179]
train() client id: f_00007-1-4 loss: 0.606274  [  160/  179]
train() client id: f_00007-2-0 loss: 0.681888  [   32/  179]
train() client id: f_00007-2-1 loss: 0.837274  [   64/  179]
train() client id: f_00007-2-2 loss: 0.743427  [   96/  179]
train() client id: f_00007-2-3 loss: 0.626663  [  128/  179]
train() client id: f_00007-2-4 loss: 0.647476  [  160/  179]
train() client id: f_00007-3-0 loss: 0.806276  [   32/  179]
train() client id: f_00007-3-1 loss: 0.579613  [   64/  179]
train() client id: f_00007-3-2 loss: 0.719313  [   96/  179]
train() client id: f_00007-3-3 loss: 0.636791  [  128/  179]
train() client id: f_00007-3-4 loss: 0.698454  [  160/  179]
train() client id: f_00007-4-0 loss: 0.865547  [   32/  179]
train() client id: f_00007-4-1 loss: 0.700833  [   64/  179]
train() client id: f_00007-4-2 loss: 0.682288  [   96/  179]
train() client id: f_00007-4-3 loss: 0.712731  [  128/  179]
train() client id: f_00007-4-4 loss: 0.536797  [  160/  179]
train() client id: f_00007-5-0 loss: 0.693710  [   32/  179]
train() client id: f_00007-5-1 loss: 0.658611  [   64/  179]
train() client id: f_00007-5-2 loss: 0.589968  [   96/  179]
train() client id: f_00007-5-3 loss: 0.876225  [  128/  179]
train() client id: f_00007-5-4 loss: 0.759887  [  160/  179]
train() client id: f_00007-6-0 loss: 0.592351  [   32/  179]
train() client id: f_00007-6-1 loss: 0.731226  [   64/  179]
train() client id: f_00007-6-2 loss: 0.640549  [   96/  179]
train() client id: f_00007-6-3 loss: 0.730461  [  128/  179]
train() client id: f_00007-6-4 loss: 0.708679  [  160/  179]
train() client id: f_00007-7-0 loss: 0.625908  [   32/  179]
train() client id: f_00007-7-1 loss: 0.580912  [   64/  179]
train() client id: f_00007-7-2 loss: 0.637934  [   96/  179]
train() client id: f_00007-7-3 loss: 0.935393  [  128/  179]
train() client id: f_00007-7-4 loss: 0.697159  [  160/  179]
train() client id: f_00007-8-0 loss: 0.629703  [   32/  179]
train() client id: f_00007-8-1 loss: 0.730679  [   64/  179]
train() client id: f_00007-8-2 loss: 0.645289  [   96/  179]
train() client id: f_00007-8-3 loss: 0.709998  [  128/  179]
train() client id: f_00007-8-4 loss: 0.819283  [  160/  179]
train() client id: f_00007-9-0 loss: 0.912196  [   32/  179]
train() client id: f_00007-9-1 loss: 0.654148  [   64/  179]
train() client id: f_00007-9-2 loss: 0.530225  [   96/  179]
train() client id: f_00007-9-3 loss: 0.619024  [  128/  179]
train() client id: f_00007-9-4 loss: 0.765802  [  160/  179]
train() client id: f_00007-10-0 loss: 0.825212  [   32/  179]
train() client id: f_00007-10-1 loss: 0.845571  [   64/  179]
train() client id: f_00007-10-2 loss: 0.662046  [   96/  179]
train() client id: f_00007-10-3 loss: 0.519300  [  128/  179]
train() client id: f_00007-10-4 loss: 0.643860  [  160/  179]
train() client id: f_00007-11-0 loss: 0.647441  [   32/  179]
train() client id: f_00007-11-1 loss: 0.649069  [   64/  179]
train() client id: f_00007-11-2 loss: 0.646655  [   96/  179]
train() client id: f_00007-11-3 loss: 0.781414  [  128/  179]
train() client id: f_00007-11-4 loss: 0.785221  [  160/  179]
train() client id: f_00008-0-0 loss: 0.629288  [   32/  130]
train() client id: f_00008-0-1 loss: 0.683983  [   64/  130]
train() client id: f_00008-0-2 loss: 0.663054  [   96/  130]
train() client id: f_00008-0-3 loss: 0.512905  [  128/  130]
train() client id: f_00008-1-0 loss: 0.586278  [   32/  130]
train() client id: f_00008-1-1 loss: 0.711809  [   64/  130]
train() client id: f_00008-1-2 loss: 0.625680  [   96/  130]
train() client id: f_00008-1-3 loss: 0.578208  [  128/  130]
train() client id: f_00008-2-0 loss: 0.631772  [   32/  130]
train() client id: f_00008-2-1 loss: 0.632782  [   64/  130]
train() client id: f_00008-2-2 loss: 0.616023  [   96/  130]
train() client id: f_00008-2-3 loss: 0.623982  [  128/  130]
train() client id: f_00008-3-0 loss: 0.510297  [   32/  130]
train() client id: f_00008-3-1 loss: 0.610338  [   64/  130]
train() client id: f_00008-3-2 loss: 0.652830  [   96/  130]
train() client id: f_00008-3-3 loss: 0.738053  [  128/  130]
train() client id: f_00008-4-0 loss: 0.694192  [   32/  130]
train() client id: f_00008-4-1 loss: 0.658003  [   64/  130]
train() client id: f_00008-4-2 loss: 0.490534  [   96/  130]
train() client id: f_00008-4-3 loss: 0.639200  [  128/  130]
train() client id: f_00008-5-0 loss: 0.599596  [   32/  130]
train() client id: f_00008-5-1 loss: 0.570483  [   64/  130]
train() client id: f_00008-5-2 loss: 0.648850  [   96/  130]
train() client id: f_00008-5-3 loss: 0.655846  [  128/  130]
train() client id: f_00008-6-0 loss: 0.614959  [   32/  130]
train() client id: f_00008-6-1 loss: 0.610703  [   64/  130]
train() client id: f_00008-6-2 loss: 0.678943  [   96/  130]
train() client id: f_00008-6-3 loss: 0.569445  [  128/  130]
train() client id: f_00008-7-0 loss: 0.651382  [   32/  130]
train() client id: f_00008-7-1 loss: 0.704966  [   64/  130]
train() client id: f_00008-7-2 loss: 0.613689  [   96/  130]
train() client id: f_00008-7-3 loss: 0.542578  [  128/  130]
train() client id: f_00008-8-0 loss: 0.573820  [   32/  130]
train() client id: f_00008-8-1 loss: 0.695237  [   64/  130]
train() client id: f_00008-8-2 loss: 0.554778  [   96/  130]
train() client id: f_00008-8-3 loss: 0.687403  [  128/  130]
train() client id: f_00008-9-0 loss: 0.718025  [   32/  130]
train() client id: f_00008-9-1 loss: 0.607678  [   64/  130]
train() client id: f_00008-9-2 loss: 0.584812  [   96/  130]
train() client id: f_00008-9-3 loss: 0.601157  [  128/  130]
train() client id: f_00008-10-0 loss: 0.649310  [   32/  130]
train() client id: f_00008-10-1 loss: 0.514741  [   64/  130]
train() client id: f_00008-10-2 loss: 0.811305  [   96/  130]
train() client id: f_00008-10-3 loss: 0.537580  [  128/  130]
train() client id: f_00008-11-0 loss: 0.664110  [   32/  130]
train() client id: f_00008-11-1 loss: 0.637441  [   64/  130]
train() client id: f_00008-11-2 loss: 0.581733  [   96/  130]
train() client id: f_00008-11-3 loss: 0.609561  [  128/  130]
train() client id: f_00009-0-0 loss: 0.962364  [   32/  118]
train() client id: f_00009-0-1 loss: 0.895628  [   64/  118]
train() client id: f_00009-0-2 loss: 0.971338  [   96/  118]
train() client id: f_00009-1-0 loss: 0.759607  [   32/  118]
train() client id: f_00009-1-1 loss: 1.013339  [   64/  118]
train() client id: f_00009-1-2 loss: 0.945015  [   96/  118]
train() client id: f_00009-2-0 loss: 0.890007  [   32/  118]
train() client id: f_00009-2-1 loss: 0.800512  [   64/  118]
train() client id: f_00009-2-2 loss: 0.921000  [   96/  118]
train() client id: f_00009-3-0 loss: 0.820407  [   32/  118]
train() client id: f_00009-3-1 loss: 0.952315  [   64/  118]
train() client id: f_00009-3-2 loss: 0.767538  [   96/  118]
train() client id: f_00009-4-0 loss: 0.785922  [   32/  118]
train() client id: f_00009-4-1 loss: 0.842525  [   64/  118]
train() client id: f_00009-4-2 loss: 0.807598  [   96/  118]
train() client id: f_00009-5-0 loss: 0.836201  [   32/  118]
train() client id: f_00009-5-1 loss: 0.769772  [   64/  118]
train() client id: f_00009-5-2 loss: 0.760871  [   96/  118]
train() client id: f_00009-6-0 loss: 0.822854  [   32/  118]
train() client id: f_00009-6-1 loss: 0.706935  [   64/  118]
train() client id: f_00009-6-2 loss: 0.765227  [   96/  118]
train() client id: f_00009-7-0 loss: 0.838196  [   32/  118]
train() client id: f_00009-7-1 loss: 0.684435  [   64/  118]
train() client id: f_00009-7-2 loss: 0.841712  [   96/  118]
train() client id: f_00009-8-0 loss: 0.672069  [   32/  118]
train() client id: f_00009-8-1 loss: 0.666227  [   64/  118]
train() client id: f_00009-8-2 loss: 0.855094  [   96/  118]
train() client id: f_00009-9-0 loss: 0.774105  [   32/  118]
train() client id: f_00009-9-1 loss: 0.673346  [   64/  118]
train() client id: f_00009-9-2 loss: 0.663741  [   96/  118]
train() client id: f_00009-10-0 loss: 0.811646  [   32/  118]
train() client id: f_00009-10-1 loss: 0.754326  [   64/  118]
train() client id: f_00009-10-2 loss: 0.584880  [   96/  118]
train() client id: f_00009-11-0 loss: 0.733694  [   32/  118]
train() client id: f_00009-11-1 loss: 0.890287  [   64/  118]
train() client id: f_00009-11-2 loss: 0.613246  [   96/  118]
At round 35 accuracy: 0.6525198938992043
At round 35 training accuracy: 0.5881958417169685
At round 35 training loss: 0.8290923536149462
update_location
xs = [  -3.9056584     4.20031788  195.00902392   18.81129433    0.97929623
    3.95640986 -157.44319194 -136.32485185  179.66397685 -122.06087855]
ys = [ 187.5879595   170.55583871    1.32061395 -157.45517586  149.35018685
  132.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [212.61349138 197.75473898 219.15807863 187.47265721 179.7399158
 166.29868255 186.53484723 169.07140943 206.36815295 157.84444852]
dists_bs = [171.55678491 179.25541692 408.74344878 384.8236762  177.83876297
 183.86147641 178.49055035 178.41979023 388.03486227 179.00570888]
uav_gains = [1.41493817e-11 1.76163185e-11 1.27867778e-11 2.04271955e-11
 2.28496909e-11 2.79287797e-11 2.07052398e-11 2.67731890e-11
 1.55361629e-11 3.18816260e-11]
bs_gains = [6.12282672e-11 5.41466606e-11 5.38557194e-12 6.37618765e-12
 5.53630598e-11 5.04336040e-11 5.47988501e-11 5.48597238e-11
 6.22954026e-12 5.43584189e-11]
Round 36
-------------------------------
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.2394703  12.90983275  6.14725815  2.21746141 14.88914632  7.16517573
  2.74816098  8.77577622  6.47605413  5.8113634 ]
obj_prev = 73.37969937876653
eta_min = 1.5469793420595954e-15	eta_max = 0.9294112566599255
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 17.025573465777978	eta = 0.9090909090909091
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 31.275397678211032	eta = 0.4948872023642162
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 24.26289815788516	eta = 0.6379202500492736
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.99478274194065	eta = 0.6731002520657826
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927768157082628	eta = 0.6750676277672019
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927565477060664	eta = 0.6750735953751348
eta = 0.6750735953751348
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.03242394 0.0681932  0.03190927 0.01106531 0.07874384 0.03757059
 0.01389597 0.04606258 0.03345327 0.03036527]
ene_total = [2.05444491 3.67229692 2.04263567 0.969603   4.18544742 2.18345709
 1.10625446 2.64978349 2.23698535 1.82665715]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 0 obj = 4.223466627519707
eta = 0.6750735953751348
freqs = [33693065.79010019 67222802.04491435 33347173.22625211 11292050.13707799
 77574909.50601687 37111387.37372465 14172192.11427163 46479487.54198894
 36839485.23777357 29929879.4291095 ]
eta_min = 0.6750735953751431	eta_max = 0.675073595375134
af = 0.01091581578850546	bf = 1.3680266359327833	zeta = 0.012007397367356006	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [2.23206564e-06 1.86867792e-05 2.15176626e-06 8.55597305e-08
 2.87355384e-05 3.13778074e-06 1.69247857e-07 6.03435770e-06
 2.75312427e-06 1.64948093e-06]
ene_total = [1.68991368 1.23854826 1.73742856 1.53620773 1.23478239 1.25381221
 1.53108527 1.44039041 2.16296411 1.23460465]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 1 obj = 4.223466627519696
eta = 0.675073595375134
freqs = [33693065.7901002  67222802.04491436 33347173.22625211 11292050.13707799
 77574909.50601688 37111387.37372466 14172192.11427163 46479487.54198895
 36839485.23777357 29929879.42910951]
Done!
At round 36 energy consumption: 4.223466627519707
At round 36 eta: 0.675073595375134
At round 36 local rounds: 12.866641222936334
At round 36 global rounds: 48.78320795030245
At round 36 a_n: 15.508406518389636
gradient difference: 0.436291366815567
train() client id: f_00000-0-0 loss: 1.216568  [   32/  126]
train() client id: f_00000-0-1 loss: 1.339993  [   64/  126]
train() client id: f_00000-0-2 loss: 1.126849  [   96/  126]
train() client id: f_00000-1-0 loss: 1.259898  [   32/  126]
train() client id: f_00000-1-1 loss: 0.924649  [   64/  126]
train() client id: f_00000-1-2 loss: 1.150199  [   96/  126]
train() client id: f_00000-2-0 loss: 0.974911  [   32/  126]
train() client id: f_00000-2-1 loss: 0.950246  [   64/  126]
train() client id: f_00000-2-2 loss: 1.004280  [   96/  126]
train() client id: f_00000-3-0 loss: 1.011199  [   32/  126]
train() client id: f_00000-3-1 loss: 0.929799  [   64/  126]
train() client id: f_00000-3-2 loss: 1.029673  [   96/  126]
train() client id: f_00000-4-0 loss: 0.888169  [   32/  126]
train() client id: f_00000-4-1 loss: 0.836614  [   64/  126]
train() client id: f_00000-4-2 loss: 1.101307  [   96/  126]
train() client id: f_00000-5-0 loss: 0.971608  [   32/  126]
train() client id: f_00000-5-1 loss: 0.826033  [   64/  126]
train() client id: f_00000-5-2 loss: 0.963747  [   96/  126]
train() client id: f_00000-6-0 loss: 1.000228  [   32/  126]
train() client id: f_00000-6-1 loss: 0.912951  [   64/  126]
train() client id: f_00000-6-2 loss: 0.811252  [   96/  126]
train() client id: f_00000-7-0 loss: 0.872566  [   32/  126]
train() client id: f_00000-7-1 loss: 0.842778  [   64/  126]
train() client id: f_00000-7-2 loss: 0.803410  [   96/  126]
train() client id: f_00000-8-0 loss: 0.776044  [   32/  126]
train() client id: f_00000-8-1 loss: 0.880706  [   64/  126]
train() client id: f_00000-8-2 loss: 0.823909  [   96/  126]
train() client id: f_00000-9-0 loss: 0.875458  [   32/  126]
train() client id: f_00000-9-1 loss: 0.783507  [   64/  126]
train() client id: f_00000-9-2 loss: 0.854859  [   96/  126]
train() client id: f_00000-10-0 loss: 0.812481  [   32/  126]
train() client id: f_00000-10-1 loss: 0.777656  [   64/  126]
train() client id: f_00000-10-2 loss: 0.891271  [   96/  126]
train() client id: f_00000-11-0 loss: 0.858338  [   32/  126]
train() client id: f_00000-11-1 loss: 0.779787  [   64/  126]
train() client id: f_00000-11-2 loss: 0.885522  [   96/  126]
train() client id: f_00001-0-0 loss: 0.509461  [   32/  265]
train() client id: f_00001-0-1 loss: 0.509643  [   64/  265]
train() client id: f_00001-0-2 loss: 0.422532  [   96/  265]
train() client id: f_00001-0-3 loss: 0.357019  [  128/  265]
train() client id: f_00001-0-4 loss: 0.352572  [  160/  265]
train() client id: f_00001-0-5 loss: 0.390400  [  192/  265]
train() client id: f_00001-0-6 loss: 0.402064  [  224/  265]
train() client id: f_00001-0-7 loss: 0.396215  [  256/  265]
train() client id: f_00001-1-0 loss: 0.411035  [   32/  265]
train() client id: f_00001-1-1 loss: 0.385710  [   64/  265]
train() client id: f_00001-1-2 loss: 0.387668  [   96/  265]
train() client id: f_00001-1-3 loss: 0.586094  [  128/  265]
train() client id: f_00001-1-4 loss: 0.383487  [  160/  265]
train() client id: f_00001-1-5 loss: 0.373133  [  192/  265]
train() client id: f_00001-1-6 loss: 0.319882  [  224/  265]
train() client id: f_00001-1-7 loss: 0.435499  [  256/  265]
train() client id: f_00001-2-0 loss: 0.361634  [   32/  265]
train() client id: f_00001-2-1 loss: 0.382185  [   64/  265]
train() client id: f_00001-2-2 loss: 0.364749  [   96/  265]
train() client id: f_00001-2-3 loss: 0.428156  [  128/  265]
train() client id: f_00001-2-4 loss: 0.356737  [  160/  265]
train() client id: f_00001-2-5 loss: 0.561617  [  192/  265]
train() client id: f_00001-2-6 loss: 0.382337  [  224/  265]
train() client id: f_00001-2-7 loss: 0.319166  [  256/  265]
train() client id: f_00001-3-0 loss: 0.390649  [   32/  265]
train() client id: f_00001-3-1 loss: 0.419752  [   64/  265]
train() client id: f_00001-3-2 loss: 0.380417  [   96/  265]
train() client id: f_00001-3-3 loss: 0.483890  [  128/  265]
train() client id: f_00001-3-4 loss: 0.358565  [  160/  265]
train() client id: f_00001-3-5 loss: 0.318400  [  192/  265]
train() client id: f_00001-3-6 loss: 0.414330  [  224/  265]
train() client id: f_00001-3-7 loss: 0.399781  [  256/  265]
train() client id: f_00001-4-0 loss: 0.305219  [   32/  265]
train() client id: f_00001-4-1 loss: 0.361328  [   64/  265]
train() client id: f_00001-4-2 loss: 0.512064  [   96/  265]
train() client id: f_00001-4-3 loss: 0.361730  [  128/  265]
train() client id: f_00001-4-4 loss: 0.415361  [  160/  265]
train() client id: f_00001-4-5 loss: 0.297732  [  192/  265]
train() client id: f_00001-4-6 loss: 0.402437  [  224/  265]
train() client id: f_00001-4-7 loss: 0.433808  [  256/  265]
train() client id: f_00001-5-0 loss: 0.402328  [   32/  265]
train() client id: f_00001-5-1 loss: 0.512761  [   64/  265]
train() client id: f_00001-5-2 loss: 0.286421  [   96/  265]
train() client id: f_00001-5-3 loss: 0.318393  [  128/  265]
train() client id: f_00001-5-4 loss: 0.443473  [  160/  265]
train() client id: f_00001-5-5 loss: 0.354504  [  192/  265]
train() client id: f_00001-5-6 loss: 0.332470  [  224/  265]
train() client id: f_00001-5-7 loss: 0.448240  [  256/  265]
train() client id: f_00001-6-0 loss: 0.414544  [   32/  265]
train() client id: f_00001-6-1 loss: 0.532089  [   64/  265]
train() client id: f_00001-6-2 loss: 0.383383  [   96/  265]
train() client id: f_00001-6-3 loss: 0.289391  [  128/  265]
train() client id: f_00001-6-4 loss: 0.328903  [  160/  265]
train() client id: f_00001-6-5 loss: 0.438084  [  192/  265]
train() client id: f_00001-6-6 loss: 0.339092  [  224/  265]
train() client id: f_00001-6-7 loss: 0.359596  [  256/  265]
train() client id: f_00001-7-0 loss: 0.370872  [   32/  265]
train() client id: f_00001-7-1 loss: 0.286351  [   64/  265]
train() client id: f_00001-7-2 loss: 0.287838  [   96/  265]
train() client id: f_00001-7-3 loss: 0.473343  [  128/  265]
train() client id: f_00001-7-4 loss: 0.387248  [  160/  265]
train() client id: f_00001-7-5 loss: 0.479473  [  192/  265]
train() client id: f_00001-7-6 loss: 0.344888  [  224/  265]
train() client id: f_00001-7-7 loss: 0.371151  [  256/  265]
train() client id: f_00001-8-0 loss: 0.409344  [   32/  265]
train() client id: f_00001-8-1 loss: 0.417013  [   64/  265]
train() client id: f_00001-8-2 loss: 0.466934  [   96/  265]
train() client id: f_00001-8-3 loss: 0.305244  [  128/  265]
train() client id: f_00001-8-4 loss: 0.387690  [  160/  265]
train() client id: f_00001-8-5 loss: 0.381687  [  192/  265]
train() client id: f_00001-8-6 loss: 0.391333  [  224/  265]
train() client id: f_00001-8-7 loss: 0.286149  [  256/  265]
train() client id: f_00001-9-0 loss: 0.399405  [   32/  265]
train() client id: f_00001-9-1 loss: 0.345736  [   64/  265]
train() client id: f_00001-9-2 loss: 0.550382  [   96/  265]
train() client id: f_00001-9-3 loss: 0.498364  [  128/  265]
train() client id: f_00001-9-4 loss: 0.357958  [  160/  265]
train() client id: f_00001-9-5 loss: 0.286311  [  192/  265]
train() client id: f_00001-9-6 loss: 0.285563  [  224/  265]
train() client id: f_00001-9-7 loss: 0.286027  [  256/  265]
train() client id: f_00001-10-0 loss: 0.365812  [   32/  265]
train() client id: f_00001-10-1 loss: 0.397645  [   64/  265]
train() client id: f_00001-10-2 loss: 0.462343  [   96/  265]
train() client id: f_00001-10-3 loss: 0.531775  [  128/  265]
train() client id: f_00001-10-4 loss: 0.299218  [  160/  265]
train() client id: f_00001-10-5 loss: 0.347649  [  192/  265]
train() client id: f_00001-10-6 loss: 0.286915  [  224/  265]
train() client id: f_00001-10-7 loss: 0.349536  [  256/  265]
train() client id: f_00001-11-0 loss: 0.323926  [   32/  265]
train() client id: f_00001-11-1 loss: 0.277031  [   64/  265]
train() client id: f_00001-11-2 loss: 0.301330  [   96/  265]
train() client id: f_00001-11-3 loss: 0.333044  [  128/  265]
train() client id: f_00001-11-4 loss: 0.402273  [  160/  265]
train() client id: f_00001-11-5 loss: 0.279055  [  192/  265]
train() client id: f_00001-11-6 loss: 0.392490  [  224/  265]
train() client id: f_00001-11-7 loss: 0.707829  [  256/  265]
train() client id: f_00002-0-0 loss: 1.220308  [   32/  124]
train() client id: f_00002-0-1 loss: 1.114092  [   64/  124]
train() client id: f_00002-0-2 loss: 1.240533  [   96/  124]
train() client id: f_00002-1-0 loss: 1.108720  [   32/  124]
train() client id: f_00002-1-1 loss: 1.198383  [   64/  124]
train() client id: f_00002-1-2 loss: 1.308671  [   96/  124]
train() client id: f_00002-2-0 loss: 1.233829  [   32/  124]
train() client id: f_00002-2-1 loss: 1.090712  [   64/  124]
train() client id: f_00002-2-2 loss: 1.219366  [   96/  124]
train() client id: f_00002-3-0 loss: 1.125793  [   32/  124]
train() client id: f_00002-3-1 loss: 1.207441  [   64/  124]
train() client id: f_00002-3-2 loss: 1.070643  [   96/  124]
train() client id: f_00002-4-0 loss: 1.197457  [   32/  124]
train() client id: f_00002-4-1 loss: 1.031908  [   64/  124]
train() client id: f_00002-4-2 loss: 1.065464  [   96/  124]
train() client id: f_00002-5-0 loss: 1.011243  [   32/  124]
train() client id: f_00002-5-1 loss: 1.175333  [   64/  124]
train() client id: f_00002-5-2 loss: 1.044037  [   96/  124]
train() client id: f_00002-6-0 loss: 1.148946  [   32/  124]
train() client id: f_00002-6-1 loss: 0.973592  [   64/  124]
train() client id: f_00002-6-2 loss: 1.116671  [   96/  124]
train() client id: f_00002-7-0 loss: 1.107862  [   32/  124]
train() client id: f_00002-7-1 loss: 1.094321  [   64/  124]
train() client id: f_00002-7-2 loss: 1.078570  [   96/  124]
train() client id: f_00002-8-0 loss: 1.186960  [   32/  124]
train() client id: f_00002-8-1 loss: 1.048216  [   64/  124]
train() client id: f_00002-8-2 loss: 0.961622  [   96/  124]
train() client id: f_00002-9-0 loss: 1.206207  [   32/  124]
train() client id: f_00002-9-1 loss: 1.007308  [   64/  124]
train() client id: f_00002-9-2 loss: 1.039409  [   96/  124]
train() client id: f_00002-10-0 loss: 1.064847  [   32/  124]
train() client id: f_00002-10-1 loss: 1.113873  [   64/  124]
train() client id: f_00002-10-2 loss: 1.142666  [   96/  124]
train() client id: f_00002-11-0 loss: 1.184181  [   32/  124]
train() client id: f_00002-11-1 loss: 1.005815  [   64/  124]
train() client id: f_00002-11-2 loss: 1.036192  [   96/  124]
train() client id: f_00003-0-0 loss: 0.773449  [   32/   43]
train() client id: f_00003-1-0 loss: 0.681520  [   32/   43]
train() client id: f_00003-2-0 loss: 0.627434  [   32/   43]
train() client id: f_00003-3-0 loss: 0.715760  [   32/   43]
train() client id: f_00003-4-0 loss: 0.692130  [   32/   43]
train() client id: f_00003-5-0 loss: 0.679633  [   32/   43]
train() client id: f_00003-6-0 loss: 0.686084  [   32/   43]
train() client id: f_00003-7-0 loss: 0.678031  [   32/   43]
train() client id: f_00003-8-0 loss: 0.732913  [   32/   43]
train() client id: f_00003-9-0 loss: 0.856654  [   32/   43]
train() client id: f_00003-10-0 loss: 0.727077  [   32/   43]
train() client id: f_00003-11-0 loss: 0.771284  [   32/   43]
train() client id: f_00004-0-0 loss: 0.592848  [   32/  306]
train() client id: f_00004-0-1 loss: 0.678533  [   64/  306]
train() client id: f_00004-0-2 loss: 0.862135  [   96/  306]
train() client id: f_00004-0-3 loss: 0.894020  [  128/  306]
train() client id: f_00004-0-4 loss: 0.719822  [  160/  306]
train() client id: f_00004-0-5 loss: 0.738235  [  192/  306]
train() client id: f_00004-0-6 loss: 0.785318  [  224/  306]
train() client id: f_00004-0-7 loss: 0.698668  [  256/  306]
train() client id: f_00004-0-8 loss: 0.884008  [  288/  306]
train() client id: f_00004-1-0 loss: 0.775436  [   32/  306]
train() client id: f_00004-1-1 loss: 0.783684  [   64/  306]
train() client id: f_00004-1-2 loss: 0.736730  [   96/  306]
train() client id: f_00004-1-3 loss: 0.712972  [  128/  306]
train() client id: f_00004-1-4 loss: 0.813440  [  160/  306]
train() client id: f_00004-1-5 loss: 0.805852  [  192/  306]
train() client id: f_00004-1-6 loss: 0.804063  [  224/  306]
train() client id: f_00004-1-7 loss: 0.756490  [  256/  306]
train() client id: f_00004-1-8 loss: 0.709271  [  288/  306]
train() client id: f_00004-2-0 loss: 0.505249  [   32/  306]
train() client id: f_00004-2-1 loss: 0.708795  [   64/  306]
train() client id: f_00004-2-2 loss: 0.684937  [   96/  306]
train() client id: f_00004-2-3 loss: 0.745085  [  128/  306]
train() client id: f_00004-2-4 loss: 0.748858  [  160/  306]
train() client id: f_00004-2-5 loss: 1.040757  [  192/  306]
train() client id: f_00004-2-6 loss: 0.844116  [  224/  306]
train() client id: f_00004-2-7 loss: 0.834763  [  256/  306]
train() client id: f_00004-2-8 loss: 0.775932  [  288/  306]
train() client id: f_00004-3-0 loss: 0.648077  [   32/  306]
train() client id: f_00004-3-1 loss: 0.900683  [   64/  306]
train() client id: f_00004-3-2 loss: 0.664512  [   96/  306]
train() client id: f_00004-3-3 loss: 0.868000  [  128/  306]
train() client id: f_00004-3-4 loss: 0.556387  [  160/  306]
train() client id: f_00004-3-5 loss: 0.785136  [  192/  306]
train() client id: f_00004-3-6 loss: 0.840504  [  224/  306]
train() client id: f_00004-3-7 loss: 0.759689  [  256/  306]
train() client id: f_00004-3-8 loss: 0.803466  [  288/  306]
train() client id: f_00004-4-0 loss: 0.813553  [   32/  306]
train() client id: f_00004-4-1 loss: 0.634316  [   64/  306]
train() client id: f_00004-4-2 loss: 0.761535  [   96/  306]
train() client id: f_00004-4-3 loss: 0.871078  [  128/  306]
train() client id: f_00004-4-4 loss: 0.684977  [  160/  306]
train() client id: f_00004-4-5 loss: 0.946494  [  192/  306]
train() client id: f_00004-4-6 loss: 0.701428  [  224/  306]
train() client id: f_00004-4-7 loss: 0.709241  [  256/  306]
train() client id: f_00004-4-8 loss: 0.707030  [  288/  306]
train() client id: f_00004-5-0 loss: 0.665262  [   32/  306]
train() client id: f_00004-5-1 loss: 0.570407  [   64/  306]
train() client id: f_00004-5-2 loss: 0.809990  [   96/  306]
train() client id: f_00004-5-3 loss: 0.914026  [  128/  306]
train() client id: f_00004-5-4 loss: 0.902457  [  160/  306]
train() client id: f_00004-5-5 loss: 0.748589  [  192/  306]
train() client id: f_00004-5-6 loss: 0.684946  [  224/  306]
train() client id: f_00004-5-7 loss: 0.798761  [  256/  306]
train() client id: f_00004-5-8 loss: 0.661931  [  288/  306]
train() client id: f_00004-6-0 loss: 0.783498  [   32/  306]
train() client id: f_00004-6-1 loss: 0.613028  [   64/  306]
train() client id: f_00004-6-2 loss: 0.911774  [   96/  306]
train() client id: f_00004-6-3 loss: 0.806866  [  128/  306]
train() client id: f_00004-6-4 loss: 0.660099  [  160/  306]
train() client id: f_00004-6-5 loss: 0.664219  [  192/  306]
train() client id: f_00004-6-6 loss: 0.879704  [  224/  306]
train() client id: f_00004-6-7 loss: 0.752871  [  256/  306]
train() client id: f_00004-6-8 loss: 0.795876  [  288/  306]
train() client id: f_00004-7-0 loss: 0.813723  [   32/  306]
train() client id: f_00004-7-1 loss: 0.614618  [   64/  306]
train() client id: f_00004-7-2 loss: 0.788481  [   96/  306]
train() client id: f_00004-7-3 loss: 0.693114  [  128/  306]
train() client id: f_00004-7-4 loss: 0.787597  [  160/  306]
train() client id: f_00004-7-5 loss: 0.917692  [  192/  306]
train() client id: f_00004-7-6 loss: 0.854813  [  224/  306]
train() client id: f_00004-7-7 loss: 0.636143  [  256/  306]
train() client id: f_00004-7-8 loss: 0.725169  [  288/  306]
train() client id: f_00004-8-0 loss: 0.705602  [   32/  306]
train() client id: f_00004-8-1 loss: 0.778222  [   64/  306]
train() client id: f_00004-8-2 loss: 0.712638  [   96/  306]
train() client id: f_00004-8-3 loss: 0.775857  [  128/  306]
train() client id: f_00004-8-4 loss: 0.704439  [  160/  306]
train() client id: f_00004-8-5 loss: 0.725823  [  192/  306]
train() client id: f_00004-8-6 loss: 0.859274  [  224/  306]
train() client id: f_00004-8-7 loss: 0.737318  [  256/  306]
train() client id: f_00004-8-8 loss: 0.874964  [  288/  306]
train() client id: f_00004-9-0 loss: 0.885420  [   32/  306]
train() client id: f_00004-9-1 loss: 0.797735  [   64/  306]
train() client id: f_00004-9-2 loss: 0.758180  [   96/  306]
train() client id: f_00004-9-3 loss: 0.686532  [  128/  306]
train() client id: f_00004-9-4 loss: 0.758193  [  160/  306]
train() client id: f_00004-9-5 loss: 0.671457  [  192/  306]
train() client id: f_00004-9-6 loss: 0.761053  [  224/  306]
train() client id: f_00004-9-7 loss: 0.803591  [  256/  306]
train() client id: f_00004-9-8 loss: 0.770284  [  288/  306]
train() client id: f_00004-10-0 loss: 0.613293  [   32/  306]
train() client id: f_00004-10-1 loss: 0.828962  [   64/  306]
train() client id: f_00004-10-2 loss: 0.845073  [   96/  306]
train() client id: f_00004-10-3 loss: 0.712560  [  128/  306]
train() client id: f_00004-10-4 loss: 0.788251  [  160/  306]
train() client id: f_00004-10-5 loss: 0.620649  [  192/  306]
train() client id: f_00004-10-6 loss: 0.770473  [  224/  306]
train() client id: f_00004-10-7 loss: 0.866264  [  256/  306]
train() client id: f_00004-10-8 loss: 0.751028  [  288/  306]
train() client id: f_00004-11-0 loss: 0.762939  [   32/  306]
train() client id: f_00004-11-1 loss: 0.647168  [   64/  306]
train() client id: f_00004-11-2 loss: 0.693398  [   96/  306]
train() client id: f_00004-11-3 loss: 0.723235  [  128/  306]
train() client id: f_00004-11-4 loss: 0.863790  [  160/  306]
train() client id: f_00004-11-5 loss: 0.766074  [  192/  306]
train() client id: f_00004-11-6 loss: 0.925333  [  224/  306]
train() client id: f_00004-11-7 loss: 0.729316  [  256/  306]
train() client id: f_00004-11-8 loss: 0.811994  [  288/  306]
train() client id: f_00005-0-0 loss: 0.682204  [   32/  146]
train() client id: f_00005-0-1 loss: 0.834199  [   64/  146]
train() client id: f_00005-0-2 loss: 0.824804  [   96/  146]
train() client id: f_00005-0-3 loss: 0.874047  [  128/  146]
train() client id: f_00005-1-0 loss: 0.825102  [   32/  146]
train() client id: f_00005-1-1 loss: 0.657889  [   64/  146]
train() client id: f_00005-1-2 loss: 0.801329  [   96/  146]
train() client id: f_00005-1-3 loss: 0.762546  [  128/  146]
train() client id: f_00005-2-0 loss: 0.769392  [   32/  146]
train() client id: f_00005-2-1 loss: 0.733329  [   64/  146]
train() client id: f_00005-2-2 loss: 0.851823  [   96/  146]
train() client id: f_00005-2-3 loss: 0.624714  [  128/  146]
train() client id: f_00005-3-0 loss: 0.628113  [   32/  146]
train() client id: f_00005-3-1 loss: 0.918848  [   64/  146]
train() client id: f_00005-3-2 loss: 0.948637  [   96/  146]
train() client id: f_00005-3-3 loss: 0.652201  [  128/  146]
train() client id: f_00005-4-0 loss: 0.609833  [   32/  146]
train() client id: f_00005-4-1 loss: 0.960499  [   64/  146]
train() client id: f_00005-4-2 loss: 0.598456  [   96/  146]
train() client id: f_00005-4-3 loss: 0.924522  [  128/  146]
train() client id: f_00005-5-0 loss: 0.786967  [   32/  146]
train() client id: f_00005-5-1 loss: 0.876832  [   64/  146]
train() client id: f_00005-5-2 loss: 0.770860  [   96/  146]
train() client id: f_00005-5-3 loss: 0.804777  [  128/  146]
train() client id: f_00005-6-0 loss: 0.720895  [   32/  146]
train() client id: f_00005-6-1 loss: 0.814897  [   64/  146]
train() client id: f_00005-6-2 loss: 0.915519  [   96/  146]
train() client id: f_00005-6-3 loss: 0.707963  [  128/  146]
train() client id: f_00005-7-0 loss: 0.659092  [   32/  146]
train() client id: f_00005-7-1 loss: 0.695294  [   64/  146]
train() client id: f_00005-7-2 loss: 0.982159  [   96/  146]
train() client id: f_00005-7-3 loss: 0.825252  [  128/  146]
train() client id: f_00005-8-0 loss: 0.595731  [   32/  146]
train() client id: f_00005-8-1 loss: 0.664904  [   64/  146]
train() client id: f_00005-8-2 loss: 1.050543  [   96/  146]
train() client id: f_00005-8-3 loss: 0.984161  [  128/  146]
train() client id: f_00005-9-0 loss: 0.951450  [   32/  146]
train() client id: f_00005-9-1 loss: 0.750260  [   64/  146]
train() client id: f_00005-9-2 loss: 0.710910  [   96/  146]
train() client id: f_00005-9-3 loss: 0.899363  [  128/  146]
train() client id: f_00005-10-0 loss: 1.010832  [   32/  146]
train() client id: f_00005-10-1 loss: 0.807752  [   64/  146]
train() client id: f_00005-10-2 loss: 0.584464  [   96/  146]
train() client id: f_00005-10-3 loss: 0.576848  [  128/  146]
train() client id: f_00005-11-0 loss: 0.780195  [   32/  146]
train() client id: f_00005-11-1 loss: 0.751076  [   64/  146]
train() client id: f_00005-11-2 loss: 0.650342  [   96/  146]
train() client id: f_00005-11-3 loss: 0.972189  [  128/  146]
train() client id: f_00006-0-0 loss: 0.491101  [   32/   54]
train() client id: f_00006-1-0 loss: 0.527506  [   32/   54]
train() client id: f_00006-2-0 loss: 0.464158  [   32/   54]
train() client id: f_00006-3-0 loss: 0.415784  [   32/   54]
train() client id: f_00006-4-0 loss: 0.426806  [   32/   54]
train() client id: f_00006-5-0 loss: 0.431678  [   32/   54]
train() client id: f_00006-6-0 loss: 0.473080  [   32/   54]
train() client id: f_00006-7-0 loss: 0.423517  [   32/   54]
train() client id: f_00006-8-0 loss: 0.509365  [   32/   54]
train() client id: f_00006-9-0 loss: 0.510367  [   32/   54]
train() client id: f_00006-10-0 loss: 0.516330  [   32/   54]
train() client id: f_00006-11-0 loss: 0.480538  [   32/   54]
train() client id: f_00007-0-0 loss: 0.527929  [   32/  179]
train() client id: f_00007-0-1 loss: 0.500703  [   64/  179]
train() client id: f_00007-0-2 loss: 0.730353  [   96/  179]
train() client id: f_00007-0-3 loss: 0.608654  [  128/  179]
train() client id: f_00007-0-4 loss: 0.510380  [  160/  179]
train() client id: f_00007-1-0 loss: 0.676127  [   32/  179]
train() client id: f_00007-1-1 loss: 0.497169  [   64/  179]
train() client id: f_00007-1-2 loss: 0.477274  [   96/  179]
train() client id: f_00007-1-3 loss: 0.511957  [  128/  179]
train() client id: f_00007-1-4 loss: 0.561261  [  160/  179]
train() client id: f_00007-2-0 loss: 0.369704  [   32/  179]
train() client id: f_00007-2-1 loss: 0.640340  [   64/  179]
train() client id: f_00007-2-2 loss: 0.729784  [   96/  179]
train() client id: f_00007-2-3 loss: 0.618524  [  128/  179]
train() client id: f_00007-2-4 loss: 0.449177  [  160/  179]
train() client id: f_00007-3-0 loss: 0.403839  [   32/  179]
train() client id: f_00007-3-1 loss: 0.483446  [   64/  179]
train() client id: f_00007-3-2 loss: 0.477196  [   96/  179]
train() client id: f_00007-3-3 loss: 0.461481  [  128/  179]
train() client id: f_00007-3-4 loss: 0.831546  [  160/  179]
train() client id: f_00007-4-0 loss: 0.568092  [   32/  179]
train() client id: f_00007-4-1 loss: 0.392481  [   64/  179]
train() client id: f_00007-4-2 loss: 0.675287  [   96/  179]
train() client id: f_00007-4-3 loss: 0.444225  [  128/  179]
train() client id: f_00007-4-4 loss: 0.527567  [  160/  179]
train() client id: f_00007-5-0 loss: 0.532031  [   32/  179]
train() client id: f_00007-5-1 loss: 0.536446  [   64/  179]
train() client id: f_00007-5-2 loss: 0.510736  [   96/  179]
train() client id: f_00007-5-3 loss: 0.630920  [  128/  179]
train() client id: f_00007-5-4 loss: 0.419848  [  160/  179]
train() client id: f_00007-6-0 loss: 0.712640  [   32/  179]
train() client id: f_00007-6-1 loss: 0.588419  [   64/  179]
train() client id: f_00007-6-2 loss: 0.372729  [   96/  179]
train() client id: f_00007-6-3 loss: 0.480679  [  128/  179]
train() client id: f_00007-6-4 loss: 0.534677  [  160/  179]
train() client id: f_00007-7-0 loss: 0.635501  [   32/  179]
train() client id: f_00007-7-1 loss: 0.559778  [   64/  179]
train() client id: f_00007-7-2 loss: 0.570853  [   96/  179]
train() client id: f_00007-7-3 loss: 0.486767  [  128/  179]
train() client id: f_00007-7-4 loss: 0.446992  [  160/  179]
train() client id: f_00007-8-0 loss: 0.745455  [   32/  179]
train() client id: f_00007-8-1 loss: 0.359989  [   64/  179]
train() client id: f_00007-8-2 loss: 0.566934  [   96/  179]
train() client id: f_00007-8-3 loss: 0.445645  [  128/  179]
train() client id: f_00007-8-4 loss: 0.509026  [  160/  179]
train() client id: f_00007-9-0 loss: 0.516607  [   32/  179]
train() client id: f_00007-9-1 loss: 0.377163  [   64/  179]
train() client id: f_00007-9-2 loss: 0.497655  [   96/  179]
train() client id: f_00007-9-3 loss: 0.728674  [  128/  179]
train() client id: f_00007-9-4 loss: 0.393084  [  160/  179]
train() client id: f_00007-10-0 loss: 0.439473  [   32/  179]
train() client id: f_00007-10-1 loss: 0.619627  [   64/  179]
train() client id: f_00007-10-2 loss: 0.481417  [   96/  179]
train() client id: f_00007-10-3 loss: 0.370897  [  128/  179]
train() client id: f_00007-10-4 loss: 0.673615  [  160/  179]
train() client id: f_00007-11-0 loss: 0.576388  [   32/  179]
train() client id: f_00007-11-1 loss: 0.397099  [   64/  179]
train() client id: f_00007-11-2 loss: 0.477214  [   96/  179]
train() client id: f_00007-11-3 loss: 0.441977  [  128/  179]
train() client id: f_00007-11-4 loss: 0.466187  [  160/  179]
train() client id: f_00008-0-0 loss: 0.737796  [   32/  130]
train() client id: f_00008-0-1 loss: 0.822113  [   64/  130]
train() client id: f_00008-0-2 loss: 0.577138  [   96/  130]
train() client id: f_00008-0-3 loss: 0.731609  [  128/  130]
train() client id: f_00008-1-0 loss: 0.740656  [   32/  130]
train() client id: f_00008-1-1 loss: 0.674479  [   64/  130]
train() client id: f_00008-1-2 loss: 0.791289  [   96/  130]
train() client id: f_00008-1-3 loss: 0.658829  [  128/  130]
train() client id: f_00008-2-0 loss: 0.682471  [   32/  130]
train() client id: f_00008-2-1 loss: 0.780959  [   64/  130]
train() client id: f_00008-2-2 loss: 0.775735  [   96/  130]
train() client id: f_00008-2-3 loss: 0.639618  [  128/  130]
train() client id: f_00008-3-0 loss: 0.686612  [   32/  130]
train() client id: f_00008-3-1 loss: 0.794359  [   64/  130]
train() client id: f_00008-3-2 loss: 0.674944  [   96/  130]
train() client id: f_00008-3-3 loss: 0.717874  [  128/  130]
train() client id: f_00008-4-0 loss: 0.628400  [   32/  130]
train() client id: f_00008-4-1 loss: 0.728459  [   64/  130]
train() client id: f_00008-4-2 loss: 0.752623  [   96/  130]
train() client id: f_00008-4-3 loss: 0.765026  [  128/  130]
train() client id: f_00008-5-0 loss: 0.618152  [   32/  130]
train() client id: f_00008-5-1 loss: 0.725590  [   64/  130]
train() client id: f_00008-5-2 loss: 0.785445  [   96/  130]
train() client id: f_00008-5-3 loss: 0.714246  [  128/  130]
train() client id: f_00008-6-0 loss: 0.820650  [   32/  130]
train() client id: f_00008-6-1 loss: 0.714939  [   64/  130]
train() client id: f_00008-6-2 loss: 0.572576  [   96/  130]
train() client id: f_00008-6-3 loss: 0.761352  [  128/  130]
train() client id: f_00008-7-0 loss: 0.717736  [   32/  130]
train() client id: f_00008-7-1 loss: 0.743436  [   64/  130]
train() client id: f_00008-7-2 loss: 0.717400  [   96/  130]
train() client id: f_00008-7-3 loss: 0.666455  [  128/  130]
train() client id: f_00008-8-0 loss: 0.779361  [   32/  130]
train() client id: f_00008-8-1 loss: 0.661173  [   64/  130]
train() client id: f_00008-8-2 loss: 0.672665  [   96/  130]
train() client id: f_00008-8-3 loss: 0.761095  [  128/  130]
train() client id: f_00008-9-0 loss: 0.652588  [   32/  130]
train() client id: f_00008-9-1 loss: 0.680298  [   64/  130]
train() client id: f_00008-9-2 loss: 0.817223  [   96/  130]
train() client id: f_00008-9-3 loss: 0.712228  [  128/  130]
train() client id: f_00008-10-0 loss: 0.707834  [   32/  130]
train() client id: f_00008-10-1 loss: 0.755265  [   64/  130]
train() client id: f_00008-10-2 loss: 0.680776  [   96/  130]
train() client id: f_00008-10-3 loss: 0.727794  [  128/  130]
train() client id: f_00008-11-0 loss: 0.822302  [   32/  130]
train() client id: f_00008-11-1 loss: 0.680772  [   64/  130]
train() client id: f_00008-11-2 loss: 0.584012  [   96/  130]
train() client id: f_00008-11-3 loss: 0.791970  [  128/  130]
train() client id: f_00009-0-0 loss: 1.298114  [   32/  118]
train() client id: f_00009-0-1 loss: 1.146623  [   64/  118]
train() client id: f_00009-0-2 loss: 1.291594  [   96/  118]
train() client id: f_00009-1-0 loss: 1.235174  [   32/  118]
train() client id: f_00009-1-1 loss: 1.010849  [   64/  118]
train() client id: f_00009-1-2 loss: 1.159141  [   96/  118]
train() client id: f_00009-2-0 loss: 1.147565  [   32/  118]
train() client id: f_00009-2-1 loss: 1.054743  [   64/  118]
train() client id: f_00009-2-2 loss: 1.201733  [   96/  118]
train() client id: f_00009-3-0 loss: 1.158367  [   32/  118]
train() client id: f_00009-3-1 loss: 1.018085  [   64/  118]
train() client id: f_00009-3-2 loss: 1.205758  [   96/  118]
train() client id: f_00009-4-0 loss: 1.033767  [   32/  118]
train() client id: f_00009-4-1 loss: 0.904602  [   64/  118]
train() client id: f_00009-4-2 loss: 1.291268  [   96/  118]
train() client id: f_00009-5-0 loss: 0.903851  [   32/  118]
train() client id: f_00009-5-1 loss: 1.231732  [   64/  118]
train() client id: f_00009-5-2 loss: 1.076969  [   96/  118]
train() client id: f_00009-6-0 loss: 0.942014  [   32/  118]
train() client id: f_00009-6-1 loss: 1.182629  [   64/  118]
train() client id: f_00009-6-2 loss: 0.893743  [   96/  118]
train() client id: f_00009-7-0 loss: 1.026853  [   32/  118]
train() client id: f_00009-7-1 loss: 0.971286  [   64/  118]
train() client id: f_00009-7-2 loss: 0.952043  [   96/  118]
train() client id: f_00009-8-0 loss: 0.918704  [   32/  118]
train() client id: f_00009-8-1 loss: 1.091870  [   64/  118]
train() client id: f_00009-8-2 loss: 0.930783  [   96/  118]
train() client id: f_00009-9-0 loss: 0.851385  [   32/  118]
train() client id: f_00009-9-1 loss: 0.930087  [   64/  118]
train() client id: f_00009-9-2 loss: 0.962155  [   96/  118]
train() client id: f_00009-10-0 loss: 0.833781  [   32/  118]
train() client id: f_00009-10-1 loss: 0.992286  [   64/  118]
train() client id: f_00009-10-2 loss: 0.874433  [   96/  118]
train() client id: f_00009-11-0 loss: 1.025103  [   32/  118]
train() client id: f_00009-11-1 loss: 1.063048  [   64/  118]
train() client id: f_00009-11-2 loss: 0.774732  [   96/  118]
At round 36 accuracy: 0.6525198938992043
At round 36 training accuracy: 0.5915492957746479
At round 36 training loss: 0.8232574211988616
update_location
xs = [  -3.9056584     4.20031788  200.00902392   18.81129433    0.97929623
    3.95640986 -162.44319194 -141.32485185  184.66397685 -127.06087855]
ys = [ 192.5879595   175.55583871    1.32061395 -162.45517586  154.35018685
  137.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [217.03773016 202.08289184 223.6187686  191.69128556 183.91557628
 170.31850552 190.7739006  173.12824728 210.73550797 161.74201283]
dists_bs = [171.99595939 179.20117992 413.27508654 389.1514018  177.18670233
 182.77905793 178.06826916 177.40312868 392.61010444 177.59125147]
uav_gains = [1.32188752e-11 1.65442503e-11 1.19052653e-11 1.92240094e-11
 2.15038573e-11 2.62734338e-11 1.94793158e-11 2.51899720e-11
 1.45569033e-11 2.99729913e-11]
bs_gains = [6.07915198e-11 5.41925596e-11 5.22184833e-12 6.17962338e-12
 5.59354239e-11 5.12743388e-11 5.51634959e-11 5.57445631e-11
 6.02839862e-12 5.55793797e-11]
Round 37
-------------------------------
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.10747648 12.63082676  6.01756463  2.17165072 14.56715747  7.00995328
  2.69087946  8.58799468  6.33825934  5.68528245]
obj_prev = 71.80704525657299
eta_min = 7.508228350489047e-16	eta_max = 0.9300641933191646
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 16.65764355541381	eta = 0.909090909090909
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 30.742318165825036	eta = 0.4925884977645458
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 23.795554951531557	eta = 0.636392483972255
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.538890009475697	eta = 0.6718748046925549
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.472173027112785	eta = 0.6738695143025546
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.47196919842037	eta = 0.6738756265369006
eta = 0.6738756265369006
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.0325705  0.06850145 0.03205351 0.01111533 0.07909978 0.03774042
 0.01395879 0.04627079 0.03360448 0.03050253]
ene_total = [2.01826629 3.59433964 2.00757769 0.95394728 4.09618977 2.13534693
 1.08775634 2.59856536 2.19433777 1.78564212]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 0 obj = 4.156314504851677
eta = 0.6738756265369006
freqs = [33016494.15795401 65724604.40972588 32686070.96869287 11059841.67344831
 75827849.2653193  36266244.90193002 13880926.45922257 45522544.15430705
 36013045.58118701 29245868.53629994]
eta_min = 0.673875626536905	eta_max = 0.6738756265369027
af = 0.01021278956996407	bf = 1.3514443765451492	zeta = 0.011234068526960479	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [2.14332394e-06 1.78631149e-05 2.06729513e-06 8.20770299e-08
 2.74558099e-05 2.99649386e-06 1.62362610e-07 5.78843872e-06
 2.63098523e-06 1.57494870e-06]
ene_total = [1.68434772 1.21143542 1.73418576 1.52598255 1.2053965  1.2225551
 1.52094637 1.42954967 2.1396742  1.20252205]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 1 obj = 4.156314504851704
eta = 0.6738756265369027
freqs = [33016494.15795401 65724604.40972584 32686070.96869286 11059841.67344831
 75827849.26531927 36266244.90193    13880926.45922257 45522544.15430704
 36013045.58118702 29245868.53629993]
Done!
At round 37 energy consumption: 4.156314504851677
At round 37 eta: 0.6738756265369027
At round 37 local rounds: 12.924801449949687
At round 37 global rounds: 47.55365676507614
At round 37 a_n: 15.165860671420319
gradient difference: 0.3969319462776184
train() client id: f_00000-0-0 loss: 1.159499  [   32/  126]
train() client id: f_00000-0-1 loss: 1.266439  [   64/  126]
train() client id: f_00000-0-2 loss: 1.292327  [   96/  126]
train() client id: f_00000-1-0 loss: 1.183578  [   32/  126]
train() client id: f_00000-1-1 loss: 1.068503  [   64/  126]
train() client id: f_00000-1-2 loss: 1.025365  [   96/  126]
train() client id: f_00000-2-0 loss: 1.129179  [   32/  126]
train() client id: f_00000-2-1 loss: 0.993291  [   64/  126]
train() client id: f_00000-2-2 loss: 1.110494  [   96/  126]
train() client id: f_00000-3-0 loss: 0.947264  [   32/  126]
train() client id: f_00000-3-1 loss: 0.950629  [   64/  126]
train() client id: f_00000-3-2 loss: 0.931861  [   96/  126]
train() client id: f_00000-4-0 loss: 0.871666  [   32/  126]
train() client id: f_00000-4-1 loss: 1.057400  [   64/  126]
train() client id: f_00000-4-2 loss: 0.889451  [   96/  126]
train() client id: f_00000-5-0 loss: 0.939881  [   32/  126]
train() client id: f_00000-5-1 loss: 0.877513  [   64/  126]
train() client id: f_00000-5-2 loss: 0.832803  [   96/  126]
train() client id: f_00000-6-0 loss: 0.812598  [   32/  126]
train() client id: f_00000-6-1 loss: 0.927499  [   64/  126]
train() client id: f_00000-6-2 loss: 0.837725  [   96/  126]
train() client id: f_00000-7-0 loss: 0.899308  [   32/  126]
train() client id: f_00000-7-1 loss: 0.865580  [   64/  126]
train() client id: f_00000-7-2 loss: 0.746309  [   96/  126]
train() client id: f_00000-8-0 loss: 0.811435  [   32/  126]
train() client id: f_00000-8-1 loss: 0.898326  [   64/  126]
train() client id: f_00000-8-2 loss: 0.742830  [   96/  126]
train() client id: f_00000-9-0 loss: 0.862934  [   32/  126]
train() client id: f_00000-9-1 loss: 0.730013  [   64/  126]
train() client id: f_00000-9-2 loss: 0.760143  [   96/  126]
train() client id: f_00000-10-0 loss: 0.803108  [   32/  126]
train() client id: f_00000-10-1 loss: 0.867115  [   64/  126]
train() client id: f_00000-10-2 loss: 0.805907  [   96/  126]
train() client id: f_00000-11-0 loss: 0.789076  [   32/  126]
train() client id: f_00000-11-1 loss: 0.742888  [   64/  126]
train() client id: f_00000-11-2 loss: 0.810093  [   96/  126]
train() client id: f_00001-0-0 loss: 0.143927  [   32/  265]
train() client id: f_00001-0-1 loss: 0.284438  [   64/  265]
train() client id: f_00001-0-2 loss: 0.323872  [   96/  265]
train() client id: f_00001-0-3 loss: 0.261915  [  128/  265]
train() client id: f_00001-0-4 loss: 0.149416  [  160/  265]
train() client id: f_00001-0-5 loss: 0.170265  [  192/  265]
train() client id: f_00001-0-6 loss: 0.212840  [  224/  265]
train() client id: f_00001-0-7 loss: 0.164406  [  256/  265]
train() client id: f_00001-1-0 loss: 0.141789  [   32/  265]
train() client id: f_00001-1-1 loss: 0.182138  [   64/  265]
train() client id: f_00001-1-2 loss: 0.193485  [   96/  265]
train() client id: f_00001-1-3 loss: 0.153201  [  128/  265]
train() client id: f_00001-1-4 loss: 0.206357  [  160/  265]
train() client id: f_00001-1-5 loss: 0.211622  [  192/  265]
train() client id: f_00001-1-6 loss: 0.364684  [  224/  265]
train() client id: f_00001-1-7 loss: 0.171201  [  256/  265]
train() client id: f_00001-2-0 loss: 0.111182  [   32/  265]
train() client id: f_00001-2-1 loss: 0.189931  [   64/  265]
train() client id: f_00001-2-2 loss: 0.325176  [   96/  265]
train() client id: f_00001-2-3 loss: 0.085460  [  128/  265]
train() client id: f_00001-2-4 loss: 0.097134  [  160/  265]
train() client id: f_00001-2-5 loss: 0.174961  [  192/  265]
train() client id: f_00001-2-6 loss: 0.230609  [  224/  265]
train() client id: f_00001-2-7 loss: 0.233923  [  256/  265]
train() client id: f_00001-3-0 loss: 0.208815  [   32/  265]
train() client id: f_00001-3-1 loss: 0.279201  [   64/  265]
train() client id: f_00001-3-2 loss: 0.197354  [   96/  265]
train() client id: f_00001-3-3 loss: 0.097128  [  128/  265]
train() client id: f_00001-3-4 loss: 0.229899  [  160/  265]
train() client id: f_00001-3-5 loss: 0.128105  [  192/  265]
train() client id: f_00001-3-6 loss: 0.169753  [  224/  265]
train() client id: f_00001-3-7 loss: 0.150040  [  256/  265]
train() client id: f_00001-4-0 loss: 0.149573  [   32/  265]
train() client id: f_00001-4-1 loss: 0.130065  [   64/  265]
train() client id: f_00001-4-2 loss: 0.224810  [   96/  265]
train() client id: f_00001-4-3 loss: 0.151717  [  128/  265]
train() client id: f_00001-4-4 loss: 0.244001  [  160/  265]
train() client id: f_00001-4-5 loss: 0.095271  [  192/  265]
train() client id: f_00001-4-6 loss: 0.247489  [  224/  265]
train() client id: f_00001-4-7 loss: 0.157466  [  256/  265]
train() client id: f_00001-5-0 loss: 0.266586  [   32/  265]
train() client id: f_00001-5-1 loss: 0.123133  [   64/  265]
train() client id: f_00001-5-2 loss: 0.150573  [   96/  265]
train() client id: f_00001-5-3 loss: 0.081647  [  128/  265]
train() client id: f_00001-5-4 loss: 0.065816  [  160/  265]
train() client id: f_00001-5-5 loss: 0.130962  [  192/  265]
train() client id: f_00001-5-6 loss: 0.272868  [  224/  265]
train() client id: f_00001-5-7 loss: 0.219354  [  256/  265]
train() client id: f_00001-6-0 loss: 0.078885  [   32/  265]
train() client id: f_00001-6-1 loss: 0.168988  [   64/  265]
train() client id: f_00001-6-2 loss: 0.059752  [   96/  265]
train() client id: f_00001-6-3 loss: 0.058191  [  128/  265]
train() client id: f_00001-6-4 loss: 0.135171  [  160/  265]
train() client id: f_00001-6-5 loss: 0.152343  [  192/  265]
train() client id: f_00001-6-6 loss: 0.081943  [  224/  265]
train() client id: f_00001-6-7 loss: 0.475713  [  256/  265]
train() client id: f_00001-7-0 loss: 0.117644  [   32/  265]
train() client id: f_00001-7-1 loss: 0.125353  [   64/  265]
train() client id: f_00001-7-2 loss: 0.149006  [   96/  265]
train() client id: f_00001-7-3 loss: 0.329138  [  128/  265]
train() client id: f_00001-7-4 loss: 0.072797  [  160/  265]
train() client id: f_00001-7-5 loss: 0.080778  [  192/  265]
train() client id: f_00001-7-6 loss: 0.203191  [  224/  265]
train() client id: f_00001-7-7 loss: 0.207294  [  256/  265]
train() client id: f_00001-8-0 loss: 0.124826  [   32/  265]
train() client id: f_00001-8-1 loss: 0.188744  [   64/  265]
train() client id: f_00001-8-2 loss: 0.157947  [   96/  265]
train() client id: f_00001-8-3 loss: 0.070385  [  128/  265]
train() client id: f_00001-8-4 loss: 0.202231  [  160/  265]
train() client id: f_00001-8-5 loss: 0.182226  [  192/  265]
train() client id: f_00001-8-6 loss: 0.105173  [  224/  265]
train() client id: f_00001-8-7 loss: 0.226200  [  256/  265]
train() client id: f_00001-9-0 loss: 0.335863  [   32/  265]
train() client id: f_00001-9-1 loss: 0.132189  [   64/  265]
train() client id: f_00001-9-2 loss: 0.105901  [   96/  265]
train() client id: f_00001-9-3 loss: 0.061417  [  128/  265]
train() client id: f_00001-9-4 loss: 0.183658  [  160/  265]
train() client id: f_00001-9-5 loss: 0.094771  [  192/  265]
train() client id: f_00001-9-6 loss: 0.196224  [  224/  265]
train() client id: f_00001-9-7 loss: 0.125138  [  256/  265]
train() client id: f_00001-10-0 loss: 0.053814  [   32/  265]
train() client id: f_00001-10-1 loss: 0.215475  [   64/  265]
train() client id: f_00001-10-2 loss: 0.179787  [   96/  265]
train() client id: f_00001-10-3 loss: 0.106091  [  128/  265]
train() client id: f_00001-10-4 loss: 0.252648  [  160/  265]
train() client id: f_00001-10-5 loss: 0.053604  [  192/  265]
train() client id: f_00001-10-6 loss: 0.102558  [  224/  265]
train() client id: f_00001-10-7 loss: 0.243045  [  256/  265]
train() client id: f_00001-11-0 loss: 0.170102  [   32/  265]
train() client id: f_00001-11-1 loss: 0.195388  [   64/  265]
train() client id: f_00001-11-2 loss: 0.057903  [   96/  265]
train() client id: f_00001-11-3 loss: 0.162047  [  128/  265]
train() client id: f_00001-11-4 loss: 0.109660  [  160/  265]
train() client id: f_00001-11-5 loss: 0.145443  [  192/  265]
train() client id: f_00001-11-6 loss: 0.105689  [  224/  265]
train() client id: f_00001-11-7 loss: 0.205812  [  256/  265]
train() client id: f_00002-0-0 loss: 1.104465  [   32/  124]
train() client id: f_00002-0-1 loss: 1.169304  [   64/  124]
train() client id: f_00002-0-2 loss: 1.262341  [   96/  124]
train() client id: f_00002-1-0 loss: 1.288348  [   32/  124]
train() client id: f_00002-1-1 loss: 1.057817  [   64/  124]
train() client id: f_00002-1-2 loss: 1.178865  [   96/  124]
train() client id: f_00002-2-0 loss: 1.231643  [   32/  124]
train() client id: f_00002-2-1 loss: 1.075219  [   64/  124]
train() client id: f_00002-2-2 loss: 1.065027  [   96/  124]
train() client id: f_00002-3-0 loss: 1.117740  [   32/  124]
train() client id: f_00002-3-1 loss: 1.210366  [   64/  124]
train() client id: f_00002-3-2 loss: 1.052852  [   96/  124]
train() client id: f_00002-4-0 loss: 1.169229  [   32/  124]
train() client id: f_00002-4-1 loss: 0.996297  [   64/  124]
train() client id: f_00002-4-2 loss: 1.195806  [   96/  124]
train() client id: f_00002-5-0 loss: 1.147753  [   32/  124]
train() client id: f_00002-5-1 loss: 1.084190  [   64/  124]
train() client id: f_00002-5-2 loss: 1.101221  [   96/  124]
train() client id: f_00002-6-0 loss: 1.154624  [   32/  124]
train() client id: f_00002-6-1 loss: 0.942793  [   64/  124]
train() client id: f_00002-6-2 loss: 1.055301  [   96/  124]
train() client id: f_00002-7-0 loss: 0.961731  [   32/  124]
train() client id: f_00002-7-1 loss: 0.968978  [   64/  124]
train() client id: f_00002-7-2 loss: 0.974492  [   96/  124]
train() client id: f_00002-8-0 loss: 1.032757  [   32/  124]
train() client id: f_00002-8-1 loss: 1.047291  [   64/  124]
train() client id: f_00002-8-2 loss: 0.893362  [   96/  124]
train() client id: f_00002-9-0 loss: 1.062275  [   32/  124]
train() client id: f_00002-9-1 loss: 0.906573  [   64/  124]
train() client id: f_00002-9-2 loss: 0.980721  [   96/  124]
train() client id: f_00002-10-0 loss: 1.085351  [   32/  124]
train() client id: f_00002-10-1 loss: 0.844218  [   64/  124]
train() client id: f_00002-10-2 loss: 0.874446  [   96/  124]
train() client id: f_00002-11-0 loss: 0.910093  [   32/  124]
train() client id: f_00002-11-1 loss: 1.054142  [   64/  124]
train() client id: f_00002-11-2 loss: 1.047459  [   96/  124]
train() client id: f_00003-0-0 loss: 0.560190  [   32/   43]
train() client id: f_00003-1-0 loss: 0.501899  [   32/   43]
train() client id: f_00003-2-0 loss: 0.575791  [   32/   43]
train() client id: f_00003-3-0 loss: 0.551190  [   32/   43]
train() client id: f_00003-4-0 loss: 0.425439  [   32/   43]
train() client id: f_00003-5-0 loss: 0.616109  [   32/   43]
train() client id: f_00003-6-0 loss: 0.700329  [   32/   43]
train() client id: f_00003-7-0 loss: 0.603223  [   32/   43]
train() client id: f_00003-8-0 loss: 0.556870  [   32/   43]
train() client id: f_00003-9-0 loss: 0.541266  [   32/   43]
train() client id: f_00003-10-0 loss: 0.772263  [   32/   43]
train() client id: f_00003-11-0 loss: 0.640033  [   32/   43]
train() client id: f_00004-0-0 loss: 0.733011  [   32/  306]
train() client id: f_00004-0-1 loss: 0.754659  [   64/  306]
train() client id: f_00004-0-2 loss: 0.841523  [   96/  306]
train() client id: f_00004-0-3 loss: 0.857356  [  128/  306]
train() client id: f_00004-0-4 loss: 0.830739  [  160/  306]
train() client id: f_00004-0-5 loss: 0.863683  [  192/  306]
train() client id: f_00004-0-6 loss: 0.753589  [  224/  306]
train() client id: f_00004-0-7 loss: 0.846753  [  256/  306]
train() client id: f_00004-0-8 loss: 0.873294  [  288/  306]
train() client id: f_00004-1-0 loss: 0.750755  [   32/  306]
train() client id: f_00004-1-1 loss: 0.832130  [   64/  306]
train() client id: f_00004-1-2 loss: 0.828083  [   96/  306]
train() client id: f_00004-1-3 loss: 0.742674  [  128/  306]
train() client id: f_00004-1-4 loss: 0.887853  [  160/  306]
train() client id: f_00004-1-5 loss: 0.830661  [  192/  306]
train() client id: f_00004-1-6 loss: 0.745129  [  224/  306]
train() client id: f_00004-1-7 loss: 0.933680  [  256/  306]
train() client id: f_00004-1-8 loss: 0.801096  [  288/  306]
train() client id: f_00004-2-0 loss: 0.847666  [   32/  306]
train() client id: f_00004-2-1 loss: 0.826333  [   64/  306]
train() client id: f_00004-2-2 loss: 0.754218  [   96/  306]
train() client id: f_00004-2-3 loss: 0.866231  [  128/  306]
train() client id: f_00004-2-4 loss: 0.708314  [  160/  306]
train() client id: f_00004-2-5 loss: 0.733995  [  192/  306]
train() client id: f_00004-2-6 loss: 0.906825  [  224/  306]
train() client id: f_00004-2-7 loss: 0.813030  [  256/  306]
train() client id: f_00004-2-8 loss: 0.865267  [  288/  306]
train() client id: f_00004-3-0 loss: 0.819885  [   32/  306]
train() client id: f_00004-3-1 loss: 0.744751  [   64/  306]
train() client id: f_00004-3-2 loss: 0.732987  [   96/  306]
train() client id: f_00004-3-3 loss: 0.882638  [  128/  306]
train() client id: f_00004-3-4 loss: 0.868238  [  160/  306]
train() client id: f_00004-3-5 loss: 0.758447  [  192/  306]
train() client id: f_00004-3-6 loss: 0.716835  [  224/  306]
train() client id: f_00004-3-7 loss: 0.852635  [  256/  306]
train() client id: f_00004-3-8 loss: 0.926043  [  288/  306]
train() client id: f_00004-4-0 loss: 0.886133  [   32/  306]
train() client id: f_00004-4-1 loss: 0.792335  [   64/  306]
train() client id: f_00004-4-2 loss: 0.769344  [   96/  306]
train() client id: f_00004-4-3 loss: 0.924488  [  128/  306]
train() client id: f_00004-4-4 loss: 0.685134  [  160/  306]
train() client id: f_00004-4-5 loss: 0.819587  [  192/  306]
train() client id: f_00004-4-6 loss: 0.814705  [  224/  306]
train() client id: f_00004-4-7 loss: 0.913207  [  256/  306]
train() client id: f_00004-4-8 loss: 0.814948  [  288/  306]
train() client id: f_00004-5-0 loss: 0.760749  [   32/  306]
train() client id: f_00004-5-1 loss: 0.842758  [   64/  306]
train() client id: f_00004-5-2 loss: 0.768021  [   96/  306]
train() client id: f_00004-5-3 loss: 0.726900  [  128/  306]
train() client id: f_00004-5-4 loss: 0.944627  [  160/  306]
train() client id: f_00004-5-5 loss: 0.712710  [  192/  306]
train() client id: f_00004-5-6 loss: 1.010557  [  224/  306]
train() client id: f_00004-5-7 loss: 0.762505  [  256/  306]
train() client id: f_00004-5-8 loss: 0.814078  [  288/  306]
train() client id: f_00004-6-0 loss: 0.903230  [   32/  306]
train() client id: f_00004-6-1 loss: 0.830565  [   64/  306]
train() client id: f_00004-6-2 loss: 0.773325  [   96/  306]
train() client id: f_00004-6-3 loss: 0.803784  [  128/  306]
train() client id: f_00004-6-4 loss: 0.772917  [  160/  306]
train() client id: f_00004-6-5 loss: 0.842683  [  192/  306]
train() client id: f_00004-6-6 loss: 0.794577  [  224/  306]
train() client id: f_00004-6-7 loss: 0.870730  [  256/  306]
train() client id: f_00004-6-8 loss: 0.766759  [  288/  306]
train() client id: f_00004-7-0 loss: 0.806581  [   32/  306]
train() client id: f_00004-7-1 loss: 0.829454  [   64/  306]
train() client id: f_00004-7-2 loss: 0.810784  [   96/  306]
train() client id: f_00004-7-3 loss: 0.917669  [  128/  306]
train() client id: f_00004-7-4 loss: 0.761027  [  160/  306]
train() client id: f_00004-7-5 loss: 0.892456  [  192/  306]
train() client id: f_00004-7-6 loss: 0.750451  [  224/  306]
train() client id: f_00004-7-7 loss: 0.840716  [  256/  306]
train() client id: f_00004-7-8 loss: 0.828657  [  288/  306]
train() client id: f_00004-8-0 loss: 0.750657  [   32/  306]
train() client id: f_00004-8-1 loss: 0.862423  [   64/  306]
train() client id: f_00004-8-2 loss: 0.901532  [   96/  306]
train() client id: f_00004-8-3 loss: 0.833896  [  128/  306]
train() client id: f_00004-8-4 loss: 0.754808  [  160/  306]
train() client id: f_00004-8-5 loss: 0.887758  [  192/  306]
train() client id: f_00004-8-6 loss: 0.863236  [  224/  306]
train() client id: f_00004-8-7 loss: 0.769569  [  256/  306]
train() client id: f_00004-8-8 loss: 0.807550  [  288/  306]
train() client id: f_00004-9-0 loss: 0.873363  [   32/  306]
train() client id: f_00004-9-1 loss: 0.745878  [   64/  306]
train() client id: f_00004-9-2 loss: 0.816785  [   96/  306]
train() client id: f_00004-9-3 loss: 0.824929  [  128/  306]
train() client id: f_00004-9-4 loss: 0.746825  [  160/  306]
train() client id: f_00004-9-5 loss: 0.781105  [  192/  306]
train() client id: f_00004-9-6 loss: 0.787401  [  224/  306]
train() client id: f_00004-9-7 loss: 0.916934  [  256/  306]
train() client id: f_00004-9-8 loss: 0.889967  [  288/  306]
train() client id: f_00004-10-0 loss: 0.802111  [   32/  306]
train() client id: f_00004-10-1 loss: 0.887943  [   64/  306]
train() client id: f_00004-10-2 loss: 0.790010  [   96/  306]
train() client id: f_00004-10-3 loss: 0.856833  [  128/  306]
train() client id: f_00004-10-4 loss: 0.910055  [  160/  306]
train() client id: f_00004-10-5 loss: 0.726815  [  192/  306]
train() client id: f_00004-10-6 loss: 0.847416  [  224/  306]
train() client id: f_00004-10-7 loss: 0.817070  [  256/  306]
train() client id: f_00004-10-8 loss: 0.703739  [  288/  306]
train() client id: f_00004-11-0 loss: 0.834131  [   32/  306]
train() client id: f_00004-11-1 loss: 0.879223  [   64/  306]
train() client id: f_00004-11-2 loss: 0.748215  [   96/  306]
train() client id: f_00004-11-3 loss: 0.767802  [  128/  306]
train() client id: f_00004-11-4 loss: 0.819129  [  160/  306]
train() client id: f_00004-11-5 loss: 0.854181  [  192/  306]
train() client id: f_00004-11-6 loss: 0.810926  [  224/  306]
train() client id: f_00004-11-7 loss: 0.817145  [  256/  306]
train() client id: f_00004-11-8 loss: 0.824420  [  288/  306]
train() client id: f_00005-0-0 loss: 0.426077  [   32/  146]
train() client id: f_00005-0-1 loss: 0.462627  [   64/  146]
train() client id: f_00005-0-2 loss: 0.338434  [   96/  146]
train() client id: f_00005-0-3 loss: 0.551834  [  128/  146]
train() client id: f_00005-1-0 loss: 0.575273  [   32/  146]
train() client id: f_00005-1-1 loss: 0.389558  [   64/  146]
train() client id: f_00005-1-2 loss: 0.207975  [   96/  146]
train() client id: f_00005-1-3 loss: 0.397266  [  128/  146]
train() client id: f_00005-2-0 loss: 0.277117  [   32/  146]
train() client id: f_00005-2-1 loss: 0.470729  [   64/  146]
train() client id: f_00005-2-2 loss: 0.539484  [   96/  146]
train() client id: f_00005-2-3 loss: 0.389225  [  128/  146]
train() client id: f_00005-3-0 loss: 0.634124  [   32/  146]
train() client id: f_00005-3-1 loss: 0.077012  [   64/  146]
train() client id: f_00005-3-2 loss: 0.350626  [   96/  146]
train() client id: f_00005-3-3 loss: 0.697766  [  128/  146]
train() client id: f_00005-4-0 loss: 0.219805  [   32/  146]
train() client id: f_00005-4-1 loss: 0.548075  [   64/  146]
train() client id: f_00005-4-2 loss: 0.351408  [   96/  146]
train() client id: f_00005-4-3 loss: 0.157276  [  128/  146]
train() client id: f_00005-5-0 loss: 0.335762  [   32/  146]
train() client id: f_00005-5-1 loss: 0.584927  [   64/  146]
train() client id: f_00005-5-2 loss: 0.460805  [   96/  146]
train() client id: f_00005-5-3 loss: 0.343603  [  128/  146]
train() client id: f_00005-6-0 loss: 0.351600  [   32/  146]
train() client id: f_00005-6-1 loss: 0.271164  [   64/  146]
train() client id: f_00005-6-2 loss: 0.316683  [   96/  146]
train() client id: f_00005-6-3 loss: 0.590779  [  128/  146]
train() client id: f_00005-7-0 loss: 0.295687  [   32/  146]
train() client id: f_00005-7-1 loss: 0.243398  [   64/  146]
train() client id: f_00005-7-2 loss: 0.669204  [   96/  146]
train() client id: f_00005-7-3 loss: 0.325078  [  128/  146]
train() client id: f_00005-8-0 loss: 0.541010  [   32/  146]
train() client id: f_00005-8-1 loss: 0.177971  [   64/  146]
train() client id: f_00005-8-2 loss: 0.483080  [   96/  146]
train() client id: f_00005-8-3 loss: 0.383778  [  128/  146]
train() client id: f_00005-9-0 loss: 0.326930  [   32/  146]
train() client id: f_00005-9-1 loss: 0.673211  [   64/  146]
train() client id: f_00005-9-2 loss: 0.165572  [   96/  146]
train() client id: f_00005-9-3 loss: 0.358131  [  128/  146]
train() client id: f_00005-10-0 loss: 0.599327  [   32/  146]
train() client id: f_00005-10-1 loss: 0.486353  [   64/  146]
train() client id: f_00005-10-2 loss: 0.299442  [   96/  146]
train() client id: f_00005-10-3 loss: 0.254117  [  128/  146]
train() client id: f_00005-11-0 loss: 0.326629  [   32/  146]
train() client id: f_00005-11-1 loss: 0.419808  [   64/  146]
train() client id: f_00005-11-2 loss: 0.326912  [   96/  146]
train() client id: f_00005-11-3 loss: 0.492193  [  128/  146]
train() client id: f_00006-0-0 loss: 0.491535  [   32/   54]
train() client id: f_00006-1-0 loss: 0.572944  [   32/   54]
train() client id: f_00006-2-0 loss: 0.555582  [   32/   54]
train() client id: f_00006-3-0 loss: 0.564222  [   32/   54]
train() client id: f_00006-4-0 loss: 0.522353  [   32/   54]
train() client id: f_00006-5-0 loss: 0.566097  [   32/   54]
train() client id: f_00006-6-0 loss: 0.493806  [   32/   54]
train() client id: f_00006-7-0 loss: 0.502684  [   32/   54]
train() client id: f_00006-8-0 loss: 0.557178  [   32/   54]
train() client id: f_00006-9-0 loss: 0.571251  [   32/   54]
train() client id: f_00006-10-0 loss: 0.566153  [   32/   54]
train() client id: f_00006-11-0 loss: 0.575824  [   32/   54]
train() client id: f_00007-0-0 loss: 0.838022  [   32/  179]
train() client id: f_00007-0-1 loss: 0.588441  [   64/  179]
train() client id: f_00007-0-2 loss: 0.587603  [   96/  179]
train() client id: f_00007-0-3 loss: 0.595024  [  128/  179]
train() client id: f_00007-0-4 loss: 0.695902  [  160/  179]
train() client id: f_00007-1-0 loss: 0.637866  [   32/  179]
train() client id: f_00007-1-1 loss: 0.582059  [   64/  179]
train() client id: f_00007-1-2 loss: 0.786344  [   96/  179]
train() client id: f_00007-1-3 loss: 0.499101  [  128/  179]
train() client id: f_00007-1-4 loss: 0.640880  [  160/  179]
train() client id: f_00007-2-0 loss: 0.530525  [   32/  179]
train() client id: f_00007-2-1 loss: 0.646444  [   64/  179]
train() client id: f_00007-2-2 loss: 0.737863  [   96/  179]
train() client id: f_00007-2-3 loss: 0.773591  [  128/  179]
train() client id: f_00007-2-4 loss: 0.526455  [  160/  179]
train() client id: f_00007-3-0 loss: 0.548846  [   32/  179]
train() client id: f_00007-3-1 loss: 0.829228  [   64/  179]
train() client id: f_00007-3-2 loss: 0.456280  [   96/  179]
train() client id: f_00007-3-3 loss: 0.617655  [  128/  179]
train() client id: f_00007-3-4 loss: 0.609568  [  160/  179]
train() client id: f_00007-4-0 loss: 0.886938  [   32/  179]
train() client id: f_00007-4-1 loss: 0.517232  [   64/  179]
train() client id: f_00007-4-2 loss: 0.496551  [   96/  179]
train() client id: f_00007-4-3 loss: 0.673591  [  128/  179]
train() client id: f_00007-4-4 loss: 0.626782  [  160/  179]
train() client id: f_00007-5-0 loss: 0.584903  [   32/  179]
train() client id: f_00007-5-1 loss: 0.727110  [   64/  179]
train() client id: f_00007-5-2 loss: 0.720555  [   96/  179]
train() client id: f_00007-5-3 loss: 0.584558  [  128/  179]
train() client id: f_00007-5-4 loss: 0.467343  [  160/  179]
train() client id: f_00007-6-0 loss: 0.535375  [   32/  179]
train() client id: f_00007-6-1 loss: 0.713700  [   64/  179]
train() client id: f_00007-6-2 loss: 0.468314  [   96/  179]
train() client id: f_00007-6-3 loss: 0.590012  [  128/  179]
train() client id: f_00007-6-4 loss: 0.741033  [  160/  179]
train() client id: f_00007-7-0 loss: 0.678811  [   32/  179]
train() client id: f_00007-7-1 loss: 0.803894  [   64/  179]
train() client id: f_00007-7-2 loss: 0.475319  [   96/  179]
train() client id: f_00007-7-3 loss: 0.448296  [  128/  179]
train() client id: f_00007-7-4 loss: 0.651963  [  160/  179]
train() client id: f_00007-8-0 loss: 0.773196  [   32/  179]
train() client id: f_00007-8-1 loss: 0.717107  [   64/  179]
train() client id: f_00007-8-2 loss: 0.495356  [   96/  179]
train() client id: f_00007-8-3 loss: 0.521293  [  128/  179]
train() client id: f_00007-8-4 loss: 0.633633  [  160/  179]
train() client id: f_00007-9-0 loss: 0.687233  [   32/  179]
train() client id: f_00007-9-1 loss: 0.494290  [   64/  179]
train() client id: f_00007-9-2 loss: 0.661367  [   96/  179]
train() client id: f_00007-9-3 loss: 0.453874  [  128/  179]
train() client id: f_00007-9-4 loss: 0.752308  [  160/  179]
train() client id: f_00007-10-0 loss: 0.554891  [   32/  179]
train() client id: f_00007-10-1 loss: 0.472128  [   64/  179]
train() client id: f_00007-10-2 loss: 0.458757  [   96/  179]
train() client id: f_00007-10-3 loss: 0.748623  [  128/  179]
train() client id: f_00007-10-4 loss: 0.752313  [  160/  179]
train() client id: f_00007-11-0 loss: 0.441075  [   32/  179]
train() client id: f_00007-11-1 loss: 0.569203  [   64/  179]
train() client id: f_00007-11-2 loss: 0.711197  [   96/  179]
train() client id: f_00007-11-3 loss: 0.549065  [  128/  179]
train() client id: f_00007-11-4 loss: 0.849659  [  160/  179]
train() client id: f_00008-0-0 loss: 0.750668  [   32/  130]
train() client id: f_00008-0-1 loss: 0.678419  [   64/  130]
train() client id: f_00008-0-2 loss: 0.665932  [   96/  130]
train() client id: f_00008-0-3 loss: 0.759541  [  128/  130]
train() client id: f_00008-1-0 loss: 0.741315  [   32/  130]
train() client id: f_00008-1-1 loss: 0.725324  [   64/  130]
train() client id: f_00008-1-2 loss: 0.742692  [   96/  130]
train() client id: f_00008-1-3 loss: 0.637986  [  128/  130]
train() client id: f_00008-2-0 loss: 0.770949  [   32/  130]
train() client id: f_00008-2-1 loss: 0.668181  [   64/  130]
train() client id: f_00008-2-2 loss: 0.780732  [   96/  130]
train() client id: f_00008-2-3 loss: 0.649787  [  128/  130]
train() client id: f_00008-3-0 loss: 0.642831  [   32/  130]
train() client id: f_00008-3-1 loss: 0.593320  [   64/  130]
train() client id: f_00008-3-2 loss: 0.772692  [   96/  130]
train() client id: f_00008-3-3 loss: 0.862814  [  128/  130]
train() client id: f_00008-4-0 loss: 0.670354  [   32/  130]
train() client id: f_00008-4-1 loss: 0.740402  [   64/  130]
train() client id: f_00008-4-2 loss: 0.706716  [   96/  130]
train() client id: f_00008-4-3 loss: 0.758239  [  128/  130]
train() client id: f_00008-5-0 loss: 0.834564  [   32/  130]
train() client id: f_00008-5-1 loss: 0.593753  [   64/  130]
train() client id: f_00008-5-2 loss: 0.768060  [   96/  130]
train() client id: f_00008-5-3 loss: 0.652253  [  128/  130]
train() client id: f_00008-6-0 loss: 0.845346  [   32/  130]
train() client id: f_00008-6-1 loss: 0.639845  [   64/  130]
train() client id: f_00008-6-2 loss: 0.659613  [   96/  130]
train() client id: f_00008-6-3 loss: 0.711173  [  128/  130]
train() client id: f_00008-7-0 loss: 0.691918  [   32/  130]
train() client id: f_00008-7-1 loss: 0.703869  [   64/  130]
train() client id: f_00008-7-2 loss: 0.862228  [   96/  130]
train() client id: f_00008-7-3 loss: 0.559735  [  128/  130]
train() client id: f_00008-8-0 loss: 0.739381  [   32/  130]
train() client id: f_00008-8-1 loss: 0.695996  [   64/  130]
train() client id: f_00008-8-2 loss: 0.719032  [   96/  130]
train() client id: f_00008-8-3 loss: 0.727147  [  128/  130]
train() client id: f_00008-9-0 loss: 0.508367  [   32/  130]
train() client id: f_00008-9-1 loss: 0.705161  [   64/  130]
train() client id: f_00008-9-2 loss: 0.884321  [   96/  130]
train() client id: f_00008-9-3 loss: 0.731093  [  128/  130]
train() client id: f_00008-10-0 loss: 0.674717  [   32/  130]
train() client id: f_00008-10-1 loss: 0.660393  [   64/  130]
train() client id: f_00008-10-2 loss: 0.712641  [   96/  130]
train() client id: f_00008-10-3 loss: 0.796085  [  128/  130]
train() client id: f_00008-11-0 loss: 0.649839  [   32/  130]
train() client id: f_00008-11-1 loss: 0.724184  [   64/  130]
train() client id: f_00008-11-2 loss: 0.689270  [   96/  130]
train() client id: f_00008-11-3 loss: 0.799557  [  128/  130]
train() client id: f_00009-0-0 loss: 1.317345  [   32/  118]
train() client id: f_00009-0-1 loss: 1.310855  [   64/  118]
train() client id: f_00009-0-2 loss: 1.380803  [   96/  118]
train() client id: f_00009-1-0 loss: 1.244849  [   32/  118]
train() client id: f_00009-1-1 loss: 1.417504  [   64/  118]
train() client id: f_00009-1-2 loss: 1.091148  [   96/  118]
train() client id: f_00009-2-0 loss: 1.194526  [   32/  118]
train() client id: f_00009-2-1 loss: 1.190597  [   64/  118]
train() client id: f_00009-2-2 loss: 1.303780  [   96/  118]
train() client id: f_00009-3-0 loss: 1.178418  [   32/  118]
train() client id: f_00009-3-1 loss: 1.243219  [   64/  118]
train() client id: f_00009-3-2 loss: 1.154310  [   96/  118]
train() client id: f_00009-4-0 loss: 1.145680  [   32/  118]
train() client id: f_00009-4-1 loss: 1.205774  [   64/  118]
train() client id: f_00009-4-2 loss: 1.075522  [   96/  118]
train() client id: f_00009-5-0 loss: 0.975008  [   32/  118]
train() client id: f_00009-5-1 loss: 1.190190  [   64/  118]
train() client id: f_00009-5-2 loss: 1.087144  [   96/  118]
train() client id: f_00009-6-0 loss: 1.194678  [   32/  118]
train() client id: f_00009-6-1 loss: 1.028235  [   64/  118]
train() client id: f_00009-6-2 loss: 0.963580  [   96/  118]
train() client id: f_00009-7-0 loss: 1.018945  [   32/  118]
train() client id: f_00009-7-1 loss: 1.044958  [   64/  118]
train() client id: f_00009-7-2 loss: 1.032136  [   96/  118]
train() client id: f_00009-8-0 loss: 1.162038  [   32/  118]
train() client id: f_00009-8-1 loss: 0.999964  [   64/  118]
train() client id: f_00009-8-2 loss: 1.034502  [   96/  118]
train() client id: f_00009-9-0 loss: 1.127839  [   32/  118]
train() client id: f_00009-9-1 loss: 0.966152  [   64/  118]
train() client id: f_00009-9-2 loss: 1.088259  [   96/  118]
train() client id: f_00009-10-0 loss: 1.059222  [   32/  118]
train() client id: f_00009-10-1 loss: 0.999309  [   64/  118]
train() client id: f_00009-10-2 loss: 0.924815  [   96/  118]
train() client id: f_00009-11-0 loss: 0.954404  [   32/  118]
train() client id: f_00009-11-1 loss: 1.116061  [   64/  118]
train() client id: f_00009-11-2 loss: 1.034760  [   96/  118]
At round 37 accuracy: 0.6525198938992043
At round 37 training accuracy: 0.5861837692823608
At round 37 training loss: 0.8298111649689435
update_location
xs = [  -3.9056584     4.20031788  205.00902392   18.81129433    0.97929623
    3.95640986 -167.44319194 -146.32485185  189.66397685 -132.06087855]
ys = [ 197.5879595   180.55583871    1.32061395 -167.45517586  159.35018685
  142.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [221.4864689  206.44140467 228.10182794 195.94667825 188.13144625
 174.389033   195.04900171 177.23328842 215.13041181 165.69878545]
dists_bs = [172.57893742 179.28642244 417.81740916 393.49506386 176.67379362
 181.82773585 177.7856586  176.5222892  397.19558643 176.30729249]
uav_gains = [1.23220895e-11 1.55193540e-11 1.10553362e-11 1.80819760e-11
 2.02342599e-11 2.47219556e-11 1.83173556e-11 2.37047028e-11
 1.36151003e-11 2.81872466e-11]
bs_gains = [6.02182696e-11 5.41204454e-11 5.06444427e-12 5.99051385e-12
 5.63913003e-11 5.20290288e-11 5.54093755e-11 5.65269226e-11
 5.83554908e-12 5.67201424e-11]
Round 38
-------------------------------
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.97555343 12.35186937  5.88797184  2.12582729 14.24522634  6.85479284
  2.63358293  8.40017663  6.2003918   5.55926836]
obj_prev = 70.23466082817666
eta_min = 3.529677745883019e-16	eta_max = 0.9307450633473059
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 16.289713645049638	eta = 0.9090909090909091
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 30.212175618890367	eta = 0.49016101234197246
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 23.329211198156166	eta = 0.6347763094355793
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.083685449611664	eta = 0.6705778625672812
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017241376491434	eta = 0.6726015459057763
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017036233942203	eta = 0.6726078128344527
eta = 0.6726078128344527
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.03272589 0.06882827 0.03220643 0.01116836 0.07947717 0.03792048
 0.01402538 0.04649155 0.03376481 0.03064806]
ene_total = [1.98224589 3.51656478 1.97276077 0.93820235 4.0071535  2.08744961
 1.06916394 2.54721356 2.15142878 1.74485306]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 0 obj = 4.0896326721833045
eta = 0.6726078128344527
freqs = [32341261.25530905 64235440.52933019 32027116.84044752 10827227.71361229
 74093064.8966756  35427679.80345908 13589106.57226709 44562788.60807764
 35185358.32378028 28567479.83964483]
eta_min = 0.6726078128344555	eta_max = 0.6726078128344546
af = 0.00954296352207343	bf = 1.3351291140167194	zeta = 0.010497259874280773	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [2.05655253e-06 1.70628135e-05 1.98478162e-06 7.86607991e-08
 2.62139160e-05 2.85952327e-06 1.55607644e-07 5.54693540e-06
 2.51143918e-06 1.50273092e-06]
ene_total = [1.67955348 1.18486183 1.73204102 1.5156187  1.17665519 1.19197501
 1.51064412 1.41831402 2.11559328 1.17116856]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 1 obj = 4.0896326721833285
eta = 0.6726078128344546
freqs = [32341261.25530905 64235440.52933017 32027116.84044754 10827227.71361229
 74093064.89667559 35427679.80345907 13589106.57226709 44562788.60807764
 35185358.32378031 28567479.83964482]
Done!
At round 38 energy consumption: 4.0896326721833045
At round 38 eta: 0.6726078128344546
At round 38 local rounds: 12.986465296587173
At round 38 global rounds: 46.323221096756726
At round 38 a_n: 14.823314824451002
gradient difference: 0.4750339090824127
train() client id: f_00000-0-0 loss: 0.996166  [   32/  126]
train() client id: f_00000-0-1 loss: 1.188015  [   64/  126]
train() client id: f_00000-0-2 loss: 0.984999  [   96/  126]
train() client id: f_00000-1-0 loss: 1.025887  [   32/  126]
train() client id: f_00000-1-1 loss: 0.799755  [   64/  126]
train() client id: f_00000-1-2 loss: 0.993529  [   96/  126]
train() client id: f_00000-2-0 loss: 0.897555  [   32/  126]
train() client id: f_00000-2-1 loss: 0.978426  [   64/  126]
train() client id: f_00000-2-2 loss: 0.925602  [   96/  126]
train() client id: f_00000-3-0 loss: 0.937977  [   32/  126]
train() client id: f_00000-3-1 loss: 0.829169  [   64/  126]
train() client id: f_00000-3-2 loss: 0.887191  [   96/  126]
train() client id: f_00000-4-0 loss: 0.796250  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896782  [   64/  126]
train() client id: f_00000-4-2 loss: 0.788040  [   96/  126]
train() client id: f_00000-5-0 loss: 0.889620  [   32/  126]
train() client id: f_00000-5-1 loss: 0.787856  [   64/  126]
train() client id: f_00000-5-2 loss: 0.739658  [   96/  126]
train() client id: f_00000-6-0 loss: 0.774253  [   32/  126]
train() client id: f_00000-6-1 loss: 0.979286  [   64/  126]
train() client id: f_00000-6-2 loss: 0.738574  [   96/  126]
train() client id: f_00000-7-0 loss: 0.809675  [   32/  126]
train() client id: f_00000-7-1 loss: 0.851200  [   64/  126]
train() client id: f_00000-7-2 loss: 0.880732  [   96/  126]
train() client id: f_00000-8-0 loss: 0.731889  [   32/  126]
train() client id: f_00000-8-1 loss: 0.802074  [   64/  126]
train() client id: f_00000-8-2 loss: 0.902589  [   96/  126]
train() client id: f_00000-9-0 loss: 0.881268  [   32/  126]
train() client id: f_00000-9-1 loss: 0.898025  [   64/  126]
train() client id: f_00000-9-2 loss: 0.793943  [   96/  126]
train() client id: f_00000-10-0 loss: 0.833494  [   32/  126]
train() client id: f_00000-10-1 loss: 0.824905  [   64/  126]
train() client id: f_00000-10-2 loss: 0.811120  [   96/  126]
train() client id: f_00000-11-0 loss: 0.950361  [   32/  126]
train() client id: f_00000-11-1 loss: 0.677776  [   64/  126]
train() client id: f_00000-11-2 loss: 0.877268  [   96/  126]
train() client id: f_00001-0-0 loss: 0.396834  [   32/  265]
train() client id: f_00001-0-1 loss: 0.456650  [   64/  265]
train() client id: f_00001-0-2 loss: 0.338203  [   96/  265]
train() client id: f_00001-0-3 loss: 0.403431  [  128/  265]
train() client id: f_00001-0-4 loss: 0.417110  [  160/  265]
train() client id: f_00001-0-5 loss: 0.434391  [  192/  265]
train() client id: f_00001-0-6 loss: 0.407888  [  224/  265]
train() client id: f_00001-0-7 loss: 0.432965  [  256/  265]
train() client id: f_00001-1-0 loss: 0.375977  [   32/  265]
train() client id: f_00001-1-1 loss: 0.391329  [   64/  265]
train() client id: f_00001-1-2 loss: 0.473987  [   96/  265]
train() client id: f_00001-1-3 loss: 0.327761  [  128/  265]
train() client id: f_00001-1-4 loss: 0.471212  [  160/  265]
train() client id: f_00001-1-5 loss: 0.339450  [  192/  265]
train() client id: f_00001-1-6 loss: 0.449408  [  224/  265]
train() client id: f_00001-1-7 loss: 0.464214  [  256/  265]
train() client id: f_00001-2-0 loss: 0.467350  [   32/  265]
train() client id: f_00001-2-1 loss: 0.449826  [   64/  265]
train() client id: f_00001-2-2 loss: 0.519850  [   96/  265]
train() client id: f_00001-2-3 loss: 0.426797  [  128/  265]
train() client id: f_00001-2-4 loss: 0.315709  [  160/  265]
train() client id: f_00001-2-5 loss: 0.329273  [  192/  265]
train() client id: f_00001-2-6 loss: 0.355128  [  224/  265]
train() client id: f_00001-2-7 loss: 0.347770  [  256/  265]
train() client id: f_00001-3-0 loss: 0.384635  [   32/  265]
train() client id: f_00001-3-1 loss: 0.309853  [   64/  265]
train() client id: f_00001-3-2 loss: 0.390682  [   96/  265]
train() client id: f_00001-3-3 loss: 0.355218  [  128/  265]
train() client id: f_00001-3-4 loss: 0.549594  [  160/  265]
train() client id: f_00001-3-5 loss: 0.426356  [  192/  265]
train() client id: f_00001-3-6 loss: 0.321230  [  224/  265]
train() client id: f_00001-3-7 loss: 0.363873  [  256/  265]
train() client id: f_00001-4-0 loss: 0.369206  [   32/  265]
train() client id: f_00001-4-1 loss: 0.418950  [   64/  265]
train() client id: f_00001-4-2 loss: 0.340322  [   96/  265]
train() client id: f_00001-4-3 loss: 0.433435  [  128/  265]
train() client id: f_00001-4-4 loss: 0.364130  [  160/  265]
train() client id: f_00001-4-5 loss: 0.354164  [  192/  265]
train() client id: f_00001-4-6 loss: 0.518448  [  224/  265]
train() client id: f_00001-4-7 loss: 0.352559  [  256/  265]
train() client id: f_00001-5-0 loss: 0.376207  [   32/  265]
train() client id: f_00001-5-1 loss: 0.348453  [   64/  265]
train() client id: f_00001-5-2 loss: 0.395826  [   96/  265]
train() client id: f_00001-5-3 loss: 0.316887  [  128/  265]
train() client id: f_00001-5-4 loss: 0.368598  [  160/  265]
train() client id: f_00001-5-5 loss: 0.338670  [  192/  265]
train() client id: f_00001-5-6 loss: 0.449932  [  224/  265]
train() client id: f_00001-5-7 loss: 0.456567  [  256/  265]
train() client id: f_00001-6-0 loss: 0.370924  [   32/  265]
train() client id: f_00001-6-1 loss: 0.353052  [   64/  265]
train() client id: f_00001-6-2 loss: 0.405622  [   96/  265]
train() client id: f_00001-6-3 loss: 0.371174  [  128/  265]
train() client id: f_00001-6-4 loss: 0.393914  [  160/  265]
train() client id: f_00001-6-5 loss: 0.433131  [  192/  265]
train() client id: f_00001-6-6 loss: 0.382530  [  224/  265]
train() client id: f_00001-6-7 loss: 0.326351  [  256/  265]
train() client id: f_00001-7-0 loss: 0.292565  [   32/  265]
train() client id: f_00001-7-1 loss: 0.445156  [   64/  265]
train() client id: f_00001-7-2 loss: 0.401829  [   96/  265]
train() client id: f_00001-7-3 loss: 0.367035  [  128/  265]
train() client id: f_00001-7-4 loss: 0.465515  [  160/  265]
train() client id: f_00001-7-5 loss: 0.374711  [  192/  265]
train() client id: f_00001-7-6 loss: 0.399843  [  224/  265]
train() client id: f_00001-7-7 loss: 0.346063  [  256/  265]
train() client id: f_00001-8-0 loss: 0.271682  [   32/  265]
train() client id: f_00001-8-1 loss: 0.452314  [   64/  265]
train() client id: f_00001-8-2 loss: 0.447501  [   96/  265]
train() client id: f_00001-8-3 loss: 0.347694  [  128/  265]
train() client id: f_00001-8-4 loss: 0.391978  [  160/  265]
train() client id: f_00001-8-5 loss: 0.446132  [  192/  265]
train() client id: f_00001-8-6 loss: 0.329971  [  224/  265]
train() client id: f_00001-8-7 loss: 0.388922  [  256/  265]
train() client id: f_00001-9-0 loss: 0.413054  [   32/  265]
train() client id: f_00001-9-1 loss: 0.389736  [   64/  265]
train() client id: f_00001-9-2 loss: 0.310497  [   96/  265]
train() client id: f_00001-9-3 loss: 0.462631  [  128/  265]
train() client id: f_00001-9-4 loss: 0.461318  [  160/  265]
train() client id: f_00001-9-5 loss: 0.282431  [  192/  265]
train() client id: f_00001-9-6 loss: 0.289904  [  224/  265]
train() client id: f_00001-9-7 loss: 0.460028  [  256/  265]
train() client id: f_00001-10-0 loss: 0.348099  [   32/  265]
train() client id: f_00001-10-1 loss: 0.426344  [   64/  265]
train() client id: f_00001-10-2 loss: 0.302973  [   96/  265]
train() client id: f_00001-10-3 loss: 0.370563  [  128/  265]
train() client id: f_00001-10-4 loss: 0.381117  [  160/  265]
train() client id: f_00001-10-5 loss: 0.291138  [  192/  265]
train() client id: f_00001-10-6 loss: 0.500176  [  224/  265]
train() client id: f_00001-10-7 loss: 0.319451  [  256/  265]
train() client id: f_00001-11-0 loss: 0.288470  [   32/  265]
train() client id: f_00001-11-1 loss: 0.324737  [   64/  265]
train() client id: f_00001-11-2 loss: 0.443383  [   96/  265]
train() client id: f_00001-11-3 loss: 0.452114  [  128/  265]
train() client id: f_00001-11-4 loss: 0.398406  [  160/  265]
train() client id: f_00001-11-5 loss: 0.273522  [  192/  265]
train() client id: f_00001-11-6 loss: 0.348794  [  224/  265]
train() client id: f_00001-11-7 loss: 0.497177  [  256/  265]
train() client id: f_00002-0-0 loss: 1.001383  [   32/  124]
train() client id: f_00002-0-1 loss: 1.177972  [   64/  124]
train() client id: f_00002-0-2 loss: 1.323375  [   96/  124]
train() client id: f_00002-1-0 loss: 1.052703  [   32/  124]
train() client id: f_00002-1-1 loss: 1.040275  [   64/  124]
train() client id: f_00002-1-2 loss: 1.175524  [   96/  124]
train() client id: f_00002-2-0 loss: 1.114349  [   32/  124]
train() client id: f_00002-2-1 loss: 1.009092  [   64/  124]
train() client id: f_00002-2-2 loss: 1.097433  [   96/  124]
train() client id: f_00002-3-0 loss: 1.039352  [   32/  124]
train() client id: f_00002-3-1 loss: 1.027202  [   64/  124]
train() client id: f_00002-3-2 loss: 1.028080  [   96/  124]
train() client id: f_00002-4-0 loss: 1.038466  [   32/  124]
train() client id: f_00002-4-1 loss: 0.850413  [   64/  124]
train() client id: f_00002-4-2 loss: 1.104496  [   96/  124]
train() client id: f_00002-5-0 loss: 0.900420  [   32/  124]
train() client id: f_00002-5-1 loss: 0.962299  [   64/  124]
train() client id: f_00002-5-2 loss: 1.100263  [   96/  124]
train() client id: f_00002-6-0 loss: 1.164634  [   32/  124]
train() client id: f_00002-6-1 loss: 0.938355  [   64/  124]
train() client id: f_00002-6-2 loss: 0.924188  [   96/  124]
train() client id: f_00002-7-0 loss: 0.890188  [   32/  124]
train() client id: f_00002-7-1 loss: 1.032109  [   64/  124]
train() client id: f_00002-7-2 loss: 1.028905  [   96/  124]
train() client id: f_00002-8-0 loss: 1.017502  [   32/  124]
train() client id: f_00002-8-1 loss: 1.005314  [   64/  124]
train() client id: f_00002-8-2 loss: 0.939550  [   96/  124]
train() client id: f_00002-9-0 loss: 1.049609  [   32/  124]
train() client id: f_00002-9-1 loss: 0.901205  [   64/  124]
train() client id: f_00002-9-2 loss: 0.747029  [   96/  124]
train() client id: f_00002-10-0 loss: 0.793210  [   32/  124]
train() client id: f_00002-10-1 loss: 1.007770  [   64/  124]
train() client id: f_00002-10-2 loss: 1.133315  [   96/  124]
train() client id: f_00002-11-0 loss: 0.943620  [   32/  124]
train() client id: f_00002-11-1 loss: 0.812038  [   64/  124]
train() client id: f_00002-11-2 loss: 0.915789  [   96/  124]
train() client id: f_00003-0-0 loss: 0.600605  [   32/   43]
train() client id: f_00003-1-0 loss: 0.712138  [   32/   43]
train() client id: f_00003-2-0 loss: 0.554489  [   32/   43]
train() client id: f_00003-3-0 loss: 0.768552  [   32/   43]
train() client id: f_00003-4-0 loss: 0.586373  [   32/   43]
train() client id: f_00003-5-0 loss: 0.579610  [   32/   43]
train() client id: f_00003-6-0 loss: 0.675762  [   32/   43]
train() client id: f_00003-7-0 loss: 0.480195  [   32/   43]
train() client id: f_00003-8-0 loss: 0.573340  [   32/   43]
train() client id: f_00003-9-0 loss: 0.692401  [   32/   43]
train() client id: f_00003-10-0 loss: 0.694333  [   32/   43]
train() client id: f_00003-11-0 loss: 0.663846  [   32/   43]
train() client id: f_00004-0-0 loss: 0.901989  [   32/  306]
train() client id: f_00004-0-1 loss: 0.763826  [   64/  306]
train() client id: f_00004-0-2 loss: 0.766891  [   96/  306]
train() client id: f_00004-0-3 loss: 0.827893  [  128/  306]
train() client id: f_00004-0-4 loss: 0.853228  [  160/  306]
train() client id: f_00004-0-5 loss: 0.870033  [  192/  306]
train() client id: f_00004-0-6 loss: 0.820942  [  224/  306]
train() client id: f_00004-0-7 loss: 0.902425  [  256/  306]
train() client id: f_00004-0-8 loss: 0.807337  [  288/  306]
train() client id: f_00004-1-0 loss: 0.711866  [   32/  306]
train() client id: f_00004-1-1 loss: 0.815020  [   64/  306]
train() client id: f_00004-1-2 loss: 0.811531  [   96/  306]
train() client id: f_00004-1-3 loss: 0.778233  [  128/  306]
train() client id: f_00004-1-4 loss: 0.762526  [  160/  306]
train() client id: f_00004-1-5 loss: 0.882939  [  192/  306]
train() client id: f_00004-1-6 loss: 0.926430  [  224/  306]
train() client id: f_00004-1-7 loss: 0.805950  [  256/  306]
train() client id: f_00004-1-8 loss: 0.794216  [  288/  306]
train() client id: f_00004-2-0 loss: 0.833849  [   32/  306]
train() client id: f_00004-2-1 loss: 0.863703  [   64/  306]
train() client id: f_00004-2-2 loss: 0.993397  [   96/  306]
train() client id: f_00004-2-3 loss: 0.764922  [  128/  306]
train() client id: f_00004-2-4 loss: 0.782002  [  160/  306]
train() client id: f_00004-2-5 loss: 0.792792  [  192/  306]
train() client id: f_00004-2-6 loss: 0.824751  [  224/  306]
train() client id: f_00004-2-7 loss: 0.907974  [  256/  306]
train() client id: f_00004-2-8 loss: 0.778364  [  288/  306]
train() client id: f_00004-3-0 loss: 0.875007  [   32/  306]
train() client id: f_00004-3-1 loss: 0.909304  [   64/  306]
train() client id: f_00004-3-2 loss: 0.807253  [   96/  306]
train() client id: f_00004-3-3 loss: 0.725748  [  128/  306]
train() client id: f_00004-3-4 loss: 0.729436  [  160/  306]
train() client id: f_00004-3-5 loss: 0.827799  [  192/  306]
train() client id: f_00004-3-6 loss: 0.880095  [  224/  306]
train() client id: f_00004-3-7 loss: 0.869613  [  256/  306]
train() client id: f_00004-3-8 loss: 0.922386  [  288/  306]
train() client id: f_00004-4-0 loss: 0.804289  [   32/  306]
train() client id: f_00004-4-1 loss: 0.893239  [   64/  306]
train() client id: f_00004-4-2 loss: 0.920026  [   96/  306]
train() client id: f_00004-4-3 loss: 0.849981  [  128/  306]
train() client id: f_00004-4-4 loss: 0.875822  [  160/  306]
train() client id: f_00004-4-5 loss: 0.724738  [  192/  306]
train() client id: f_00004-4-6 loss: 0.753309  [  224/  306]
train() client id: f_00004-4-7 loss: 0.892484  [  256/  306]
train() client id: f_00004-4-8 loss: 0.790649  [  288/  306]
train() client id: f_00004-5-0 loss: 0.921864  [   32/  306]
train() client id: f_00004-5-1 loss: 0.766600  [   64/  306]
train() client id: f_00004-5-2 loss: 0.773061  [   96/  306]
train() client id: f_00004-5-3 loss: 0.878222  [  128/  306]
train() client id: f_00004-5-4 loss: 1.001997  [  160/  306]
train() client id: f_00004-5-5 loss: 0.743665  [  192/  306]
train() client id: f_00004-5-6 loss: 0.733230  [  224/  306]
train() client id: f_00004-5-7 loss: 0.818513  [  256/  306]
train() client id: f_00004-5-8 loss: 0.850566  [  288/  306]
train() client id: f_00004-6-0 loss: 0.849400  [   32/  306]
train() client id: f_00004-6-1 loss: 0.846616  [   64/  306]
train() client id: f_00004-6-2 loss: 0.717878  [   96/  306]
train() client id: f_00004-6-3 loss: 0.910416  [  128/  306]
train() client id: f_00004-6-4 loss: 0.732518  [  160/  306]
train() client id: f_00004-6-5 loss: 0.881836  [  192/  306]
train() client id: f_00004-6-6 loss: 0.747574  [  224/  306]
train() client id: f_00004-6-7 loss: 0.796654  [  256/  306]
train() client id: f_00004-6-8 loss: 0.983488  [  288/  306]
train() client id: f_00004-7-0 loss: 0.859574  [   32/  306]
train() client id: f_00004-7-1 loss: 0.741223  [   64/  306]
train() client id: f_00004-7-2 loss: 0.803663  [   96/  306]
train() client id: f_00004-7-3 loss: 0.843109  [  128/  306]
train() client id: f_00004-7-4 loss: 0.841993  [  160/  306]
train() client id: f_00004-7-5 loss: 0.775588  [  192/  306]
train() client id: f_00004-7-6 loss: 0.918318  [  224/  306]
train() client id: f_00004-7-7 loss: 0.784343  [  256/  306]
train() client id: f_00004-7-8 loss: 0.905674  [  288/  306]
train() client id: f_00004-8-0 loss: 0.864460  [   32/  306]
train() client id: f_00004-8-1 loss: 0.899199  [   64/  306]
train() client id: f_00004-8-2 loss: 0.821064  [   96/  306]
train() client id: f_00004-8-3 loss: 0.727448  [  128/  306]
train() client id: f_00004-8-4 loss: 0.775166  [  160/  306]
train() client id: f_00004-8-5 loss: 0.817064  [  192/  306]
train() client id: f_00004-8-6 loss: 0.928402  [  224/  306]
train() client id: f_00004-8-7 loss: 0.910167  [  256/  306]
train() client id: f_00004-8-8 loss: 0.761957  [  288/  306]
train() client id: f_00004-9-0 loss: 0.872733  [   32/  306]
train() client id: f_00004-9-1 loss: 0.795216  [   64/  306]
train() client id: f_00004-9-2 loss: 0.915641  [   96/  306]
train() client id: f_00004-9-3 loss: 0.942454  [  128/  306]
train() client id: f_00004-9-4 loss: 0.704590  [  160/  306]
train() client id: f_00004-9-5 loss: 0.802512  [  192/  306]
train() client id: f_00004-9-6 loss: 0.844973  [  224/  306]
train() client id: f_00004-9-7 loss: 0.889605  [  256/  306]
train() client id: f_00004-9-8 loss: 0.769228  [  288/  306]
train() client id: f_00004-10-0 loss: 0.823502  [   32/  306]
train() client id: f_00004-10-1 loss: 0.850871  [   64/  306]
train() client id: f_00004-10-2 loss: 0.865448  [   96/  306]
train() client id: f_00004-10-3 loss: 0.866823  [  128/  306]
train() client id: f_00004-10-4 loss: 0.839390  [  160/  306]
train() client id: f_00004-10-5 loss: 0.954057  [  192/  306]
train() client id: f_00004-10-6 loss: 0.909844  [  224/  306]
train() client id: f_00004-10-7 loss: 0.750592  [  256/  306]
train() client id: f_00004-10-8 loss: 0.755193  [  288/  306]
train() client id: f_00004-11-0 loss: 0.786315  [   32/  306]
train() client id: f_00004-11-1 loss: 0.767553  [   64/  306]
train() client id: f_00004-11-2 loss: 0.881893  [   96/  306]
train() client id: f_00004-11-3 loss: 0.927210  [  128/  306]
train() client id: f_00004-11-4 loss: 0.823206  [  160/  306]
train() client id: f_00004-11-5 loss: 0.856211  [  192/  306]
train() client id: f_00004-11-6 loss: 0.698660  [  224/  306]
train() client id: f_00004-11-7 loss: 0.841666  [  256/  306]
train() client id: f_00004-11-8 loss: 0.954759  [  288/  306]
train() client id: f_00005-0-0 loss: 0.552543  [   32/  146]
train() client id: f_00005-0-1 loss: 0.586202  [   64/  146]
train() client id: f_00005-0-2 loss: 0.670955  [   96/  146]
train() client id: f_00005-0-3 loss: 0.561022  [  128/  146]
train() client id: f_00005-1-0 loss: 0.309768  [   32/  146]
train() client id: f_00005-1-1 loss: 0.581376  [   64/  146]
train() client id: f_00005-1-2 loss: 0.576489  [   96/  146]
train() client id: f_00005-1-3 loss: 0.777799  [  128/  146]
train() client id: f_00005-2-0 loss: 0.710886  [   32/  146]
train() client id: f_00005-2-1 loss: 0.688746  [   64/  146]
train() client id: f_00005-2-2 loss: 0.436863  [   96/  146]
train() client id: f_00005-2-3 loss: 0.501140  [  128/  146]
train() client id: f_00005-3-0 loss: 0.447061  [   32/  146]
train() client id: f_00005-3-1 loss: 0.780861  [   64/  146]
train() client id: f_00005-3-2 loss: 0.381574  [   96/  146]
train() client id: f_00005-3-3 loss: 0.628849  [  128/  146]
train() client id: f_00005-4-0 loss: 0.298367  [   32/  146]
train() client id: f_00005-4-1 loss: 0.848633  [   64/  146]
train() client id: f_00005-4-2 loss: 0.474598  [   96/  146]
train() client id: f_00005-4-3 loss: 0.679428  [  128/  146]
train() client id: f_00005-5-0 loss: 0.631235  [   32/  146]
train() client id: f_00005-5-1 loss: 0.454362  [   64/  146]
train() client id: f_00005-5-2 loss: 0.501799  [   96/  146]
train() client id: f_00005-5-3 loss: 0.630764  [  128/  146]
train() client id: f_00005-6-0 loss: 0.532957  [   32/  146]
train() client id: f_00005-6-1 loss: 0.630067  [   64/  146]
train() client id: f_00005-6-2 loss: 0.467566  [   96/  146]
train() client id: f_00005-6-3 loss: 0.526416  [  128/  146]
train() client id: f_00005-7-0 loss: 0.418757  [   32/  146]
train() client id: f_00005-7-1 loss: 0.450222  [   64/  146]
train() client id: f_00005-7-2 loss: 0.545976  [   96/  146]
train() client id: f_00005-7-3 loss: 0.793734  [  128/  146]
train() client id: f_00005-8-0 loss: 0.465296  [   32/  146]
train() client id: f_00005-8-1 loss: 0.381162  [   64/  146]
train() client id: f_00005-8-2 loss: 0.434419  [   96/  146]
train() client id: f_00005-8-3 loss: 0.628570  [  128/  146]
train() client id: f_00005-9-0 loss: 0.591778  [   32/  146]
train() client id: f_00005-9-1 loss: 0.388741  [   64/  146]
train() client id: f_00005-9-2 loss: 0.519442  [   96/  146]
train() client id: f_00005-9-3 loss: 0.824039  [  128/  146]
train() client id: f_00005-10-0 loss: 0.513747  [   32/  146]
train() client id: f_00005-10-1 loss: 0.612683  [   64/  146]
train() client id: f_00005-10-2 loss: 0.446096  [   96/  146]
train() client id: f_00005-10-3 loss: 0.639466  [  128/  146]
train() client id: f_00005-11-0 loss: 0.427876  [   32/  146]
train() client id: f_00005-11-1 loss: 0.535462  [   64/  146]
train() client id: f_00005-11-2 loss: 0.729553  [   96/  146]
train() client id: f_00005-11-3 loss: 0.462748  [  128/  146]
train() client id: f_00006-0-0 loss: 0.433031  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505055  [   32/   54]
train() client id: f_00006-2-0 loss: 0.516680  [   32/   54]
train() client id: f_00006-3-0 loss: 0.468620  [   32/   54]
train() client id: f_00006-4-0 loss: 0.488790  [   32/   54]
train() client id: f_00006-5-0 loss: 0.508940  [   32/   54]
train() client id: f_00006-6-0 loss: 0.423710  [   32/   54]
train() client id: f_00006-7-0 loss: 0.421047  [   32/   54]
train() client id: f_00006-8-0 loss: 0.487454  [   32/   54]
train() client id: f_00006-9-0 loss: 0.529867  [   32/   54]
train() client id: f_00006-10-0 loss: 0.486288  [   32/   54]
train() client id: f_00006-11-0 loss: 0.537055  [   32/   54]
train() client id: f_00007-0-0 loss: 0.334842  [   32/  179]
train() client id: f_00007-0-1 loss: 0.388611  [   64/  179]
train() client id: f_00007-0-2 loss: 0.349444  [   96/  179]
train() client id: f_00007-0-3 loss: 0.674276  [  128/  179]
train() client id: f_00007-0-4 loss: 0.347870  [  160/  179]
train() client id: f_00007-1-0 loss: 0.300129  [   32/  179]
train() client id: f_00007-1-1 loss: 0.316060  [   64/  179]
train() client id: f_00007-1-2 loss: 0.538675  [   96/  179]
train() client id: f_00007-1-3 loss: 0.395801  [  128/  179]
train() client id: f_00007-1-4 loss: 0.337761  [  160/  179]
train() client id: f_00007-2-0 loss: 0.275384  [   32/  179]
train() client id: f_00007-2-1 loss: 0.748932  [   64/  179]
train() client id: f_00007-2-2 loss: 0.298530  [   96/  179]
train() client id: f_00007-2-3 loss: 0.312928  [  128/  179]
train() client id: f_00007-2-4 loss: 0.277877  [  160/  179]
train() client id: f_00007-3-0 loss: 0.383538  [   32/  179]
train() client id: f_00007-3-1 loss: 0.356497  [   64/  179]
train() client id: f_00007-3-2 loss: 0.251285  [   96/  179]
train() client id: f_00007-3-3 loss: 0.270843  [  128/  179]
train() client id: f_00007-3-4 loss: 0.524405  [  160/  179]
train() client id: f_00007-4-0 loss: 0.379180  [   32/  179]
train() client id: f_00007-4-1 loss: 0.367596  [   64/  179]
train() client id: f_00007-4-2 loss: 0.367308  [   96/  179]
train() client id: f_00007-4-3 loss: 0.360299  [  128/  179]
train() client id: f_00007-4-4 loss: 0.308354  [  160/  179]
train() client id: f_00007-5-0 loss: 0.239269  [   32/  179]
train() client id: f_00007-5-1 loss: 0.222565  [   64/  179]
train() client id: f_00007-5-2 loss: 0.211857  [   96/  179]
train() client id: f_00007-5-3 loss: 0.211585  [  128/  179]
train() client id: f_00007-5-4 loss: 0.606327  [  160/  179]
train() client id: f_00007-6-0 loss: 0.381374  [   32/  179]
train() client id: f_00007-6-1 loss: 0.282833  [   64/  179]
train() client id: f_00007-6-2 loss: 0.396892  [   96/  179]
train() client id: f_00007-6-3 loss: 0.363455  [  128/  179]
train() client id: f_00007-6-4 loss: 0.373544  [  160/  179]
train() client id: f_00007-7-0 loss: 0.288396  [   32/  179]
train() client id: f_00007-7-1 loss: 0.321212  [   64/  179]
train() client id: f_00007-7-2 loss: 0.278657  [   96/  179]
train() client id: f_00007-7-3 loss: 0.238060  [  128/  179]
train() client id: f_00007-7-4 loss: 0.448528  [  160/  179]
train() client id: f_00007-8-0 loss: 0.182209  [   32/  179]
train() client id: f_00007-8-1 loss: 0.489860  [   64/  179]
train() client id: f_00007-8-2 loss: 0.285250  [   96/  179]
train() client id: f_00007-8-3 loss: 0.487778  [  128/  179]
train() client id: f_00007-8-4 loss: 0.208791  [  160/  179]
train() client id: f_00007-9-0 loss: 0.280167  [   32/  179]
train() client id: f_00007-9-1 loss: 0.152833  [   64/  179]
train() client id: f_00007-9-2 loss: 0.428196  [   96/  179]
train() client id: f_00007-9-3 loss: 0.382604  [  128/  179]
train() client id: f_00007-9-4 loss: 0.480686  [  160/  179]
train() client id: f_00007-10-0 loss: 0.301346  [   32/  179]
train() client id: f_00007-10-1 loss: 0.279723  [   64/  179]
train() client id: f_00007-10-2 loss: 0.450498  [   96/  179]
train() client id: f_00007-10-3 loss: 0.258340  [  128/  179]
train() client id: f_00007-10-4 loss: 0.351328  [  160/  179]
train() client id: f_00007-11-0 loss: 0.264872  [   32/  179]
train() client id: f_00007-11-1 loss: 0.177585  [   64/  179]
train() client id: f_00007-11-2 loss: 0.635205  [   96/  179]
train() client id: f_00007-11-3 loss: 0.295266  [  128/  179]
train() client id: f_00007-11-4 loss: 0.205072  [  160/  179]
train() client id: f_00008-0-0 loss: 0.588098  [   32/  130]
train() client id: f_00008-0-1 loss: 0.589368  [   64/  130]
train() client id: f_00008-0-2 loss: 0.625883  [   96/  130]
train() client id: f_00008-0-3 loss: 0.759701  [  128/  130]
train() client id: f_00008-1-0 loss: 0.580705  [   32/  130]
train() client id: f_00008-1-1 loss: 0.643657  [   64/  130]
train() client id: f_00008-1-2 loss: 0.695714  [   96/  130]
train() client id: f_00008-1-3 loss: 0.624232  [  128/  130]
train() client id: f_00008-2-0 loss: 0.640381  [   32/  130]
train() client id: f_00008-2-1 loss: 0.526354  [   64/  130]
train() client id: f_00008-2-2 loss: 0.744017  [   96/  130]
train() client id: f_00008-2-3 loss: 0.598115  [  128/  130]
train() client id: f_00008-3-0 loss: 0.587450  [   32/  130]
train() client id: f_00008-3-1 loss: 0.602789  [   64/  130]
train() client id: f_00008-3-2 loss: 0.642199  [   96/  130]
train() client id: f_00008-3-3 loss: 0.697258  [  128/  130]
train() client id: f_00008-4-0 loss: 0.531716  [   32/  130]
train() client id: f_00008-4-1 loss: 0.620975  [   64/  130]
train() client id: f_00008-4-2 loss: 0.675505  [   96/  130]
train() client id: f_00008-4-3 loss: 0.687050  [  128/  130]
train() client id: f_00008-5-0 loss: 0.730788  [   32/  130]
train() client id: f_00008-5-1 loss: 0.652253  [   64/  130]
train() client id: f_00008-5-2 loss: 0.636988  [   96/  130]
train() client id: f_00008-5-3 loss: 0.506839  [  128/  130]
train() client id: f_00008-6-0 loss: 0.572541  [   32/  130]
train() client id: f_00008-6-1 loss: 0.633958  [   64/  130]
train() client id: f_00008-6-2 loss: 0.583727  [   96/  130]
train() client id: f_00008-6-3 loss: 0.740205  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708637  [   32/  130]
train() client id: f_00008-7-1 loss: 0.693238  [   64/  130]
train() client id: f_00008-7-2 loss: 0.630539  [   96/  130]
train() client id: f_00008-7-3 loss: 0.522940  [  128/  130]
train() client id: f_00008-8-0 loss: 0.657006  [   32/  130]
train() client id: f_00008-8-1 loss: 0.621129  [   64/  130]
train() client id: f_00008-8-2 loss: 0.612453  [   96/  130]
train() client id: f_00008-8-3 loss: 0.621270  [  128/  130]
train() client id: f_00008-9-0 loss: 0.714897  [   32/  130]
train() client id: f_00008-9-1 loss: 0.558155  [   64/  130]
train() client id: f_00008-9-2 loss: 0.642964  [   96/  130]
train() client id: f_00008-9-3 loss: 0.626220  [  128/  130]
train() client id: f_00008-10-0 loss: 0.529325  [   32/  130]
train() client id: f_00008-10-1 loss: 0.654242  [   64/  130]
train() client id: f_00008-10-2 loss: 0.767845  [   96/  130]
train() client id: f_00008-10-3 loss: 0.599945  [  128/  130]
train() client id: f_00008-11-0 loss: 0.676292  [   32/  130]
train() client id: f_00008-11-1 loss: 0.732823  [   64/  130]
train() client id: f_00008-11-2 loss: 0.580503  [   96/  130]
train() client id: f_00008-11-3 loss: 0.561630  [  128/  130]
train() client id: f_00009-0-0 loss: 0.971801  [   32/  118]
train() client id: f_00009-0-1 loss: 1.048841  [   64/  118]
train() client id: f_00009-0-2 loss: 1.078702  [   96/  118]
train() client id: f_00009-1-0 loss: 0.901244  [   32/  118]
train() client id: f_00009-1-1 loss: 1.016946  [   64/  118]
train() client id: f_00009-1-2 loss: 1.158214  [   96/  118]
train() client id: f_00009-2-0 loss: 1.088945  [   32/  118]
train() client id: f_00009-2-1 loss: 0.853664  [   64/  118]
train() client id: f_00009-2-2 loss: 0.971034  [   96/  118]
train() client id: f_00009-3-0 loss: 0.929829  [   32/  118]
train() client id: f_00009-3-1 loss: 1.047354  [   64/  118]
train() client id: f_00009-3-2 loss: 0.882064  [   96/  118]
train() client id: f_00009-4-0 loss: 0.900458  [   32/  118]
train() client id: f_00009-4-1 loss: 0.940810  [   64/  118]
train() client id: f_00009-4-2 loss: 0.838487  [   96/  118]
train() client id: f_00009-5-0 loss: 1.015030  [   32/  118]
train() client id: f_00009-5-1 loss: 0.826831  [   64/  118]
train() client id: f_00009-5-2 loss: 0.817701  [   96/  118]
train() client id: f_00009-6-0 loss: 0.790858  [   32/  118]
train() client id: f_00009-6-1 loss: 0.906502  [   64/  118]
train() client id: f_00009-6-2 loss: 0.920640  [   96/  118]
train() client id: f_00009-7-0 loss: 0.777467  [   32/  118]
train() client id: f_00009-7-1 loss: 0.830737  [   64/  118]
train() client id: f_00009-7-2 loss: 0.719655  [   96/  118]
train() client id: f_00009-8-0 loss: 0.857820  [   32/  118]
train() client id: f_00009-8-1 loss: 0.717893  [   64/  118]
train() client id: f_00009-8-2 loss: 0.776208  [   96/  118]
train() client id: f_00009-9-0 loss: 1.055166  [   32/  118]
train() client id: f_00009-9-1 loss: 0.706787  [   64/  118]
train() client id: f_00009-9-2 loss: 0.689648  [   96/  118]
train() client id: f_00009-10-0 loss: 0.810717  [   32/  118]
train() client id: f_00009-10-1 loss: 0.665869  [   64/  118]
train() client id: f_00009-10-2 loss: 0.920225  [   96/  118]
train() client id: f_00009-11-0 loss: 0.791252  [   32/  118]
train() client id: f_00009-11-1 loss: 0.854820  [   64/  118]
train() client id: f_00009-11-2 loss: 0.656626  [   96/  118]
At round 38 accuracy: 0.6525198938992043
At round 38 training accuracy: 0.5888665325285044
At round 38 training loss: 0.8318200450125735
update_location
xs = [  -3.9056584     4.20031788  210.00902392   18.81129433    0.97929623
    3.95640986 -172.44319194 -151.32485185  194.66397685 -137.06087855]
ys = [ 202.5879595   185.55583871    1.32061395 -172.45517586  164.35018685
  147.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [225.95826053 210.82839455 232.60596327 200.23649137 192.3848823
 178.50679634 199.35783152 181.38326009 219.5512101  169.71062514]
dists_bs = [173.30426779 179.51094579 422.3700719  397.85414041 176.30125133
 181.00957718 177.64338525 175.77931364 401.79095764 175.15670175]
uav_gains = [1.14573089e-11 1.45365625e-11 1.02368353e-11 1.69945603e-11
 1.90332055e-11 2.32656486e-11 1.72124693e-11 2.23088121e-11
 1.27076078e-11 2.65155347e-11]
bs_gains = [5.95152365e-11 5.39311234e-11 4.91307394e-12 5.80854309e-12
 5.67255845e-11 5.26901863e-11 5.55337207e-11 5.71984612e-11
 5.65058798e-12 5.77695734e-11]
Round 39
-------------------------------
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.84371614 12.07295727  5.75849455  2.07999855 13.92335007  6.69969194
  2.5762787   8.21232374  6.06245015  5.43331894]
obj_prev = 68.66258004963703
eta_min = 1.603615121775097e-16	eta_max = 0.9314544727787696
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 15.921783734685466	eta = 0.909090909090909
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 29.685339818895304	eta = 0.4875924930628838
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 22.863992502600777	eta = 0.6330630509114977
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.629256357976853	eta = 0.6692023345673616
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.563057577188566	eta = 0.6712567917560257
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.562850936268624	eta = 0.6712632245380996
eta = 0.6712632245380996
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.03289102 0.06917555 0.03236894 0.01122471 0.07987818 0.03811181
 0.01409615 0.04672613 0.03393518 0.0308027 ]
ene_total = [1.94642197 3.43896482 1.93822068 0.92238666 3.91833323 2.03975789
 1.05049566 2.49573328 2.10825374 1.704283  ]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 0 obj = 4.023474514862622
eta = 0.6712632245380996
freqs = [31667787.3789384  62754800.17478311 31370631.5489173  10594331.96949695
 72369920.453618   34595378.85521331 13296888.56956221 43600515.27273497
 34356690.70712458 27894452.74815087]
eta_min = 0.6712632245381114	eta_max = 0.6712632245380962
af = 0.008905236727720691	bf = 1.3191144648464428	zeta = 0.009795760400492761	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [1.97179310e-06 1.62852764e-05 1.90424825e-06 7.53131768e-08
 2.50088069e-05 2.72674418e-06 1.48987275e-07 5.30996467e-06
 2.39453594e-06 1.43275870e-06]
ene_total = [1.67569409 1.15879127 1.73115483 1.50519706 1.14852664 1.1620451
 1.50025809 1.40670146 2.09070673 1.14052021]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 1 obj = 4.0234745148625795
eta = 0.6712632245380962
freqs = [31667787.37893841 62754800.17478317 31370631.54891731 10594331.96949696
 72369920.4536181  34595378.85521334 13296888.56956222 43600515.27273501
 34356690.70712457 27894452.7481509 ]
Done!
At round 39 energy consumption: 4.023474514862622
At round 39 eta: 0.6712632245380962
At round 39 local rounds: 13.051990433476096
At round 39 global rounds: 45.091744918477566
At round 39 a_n: 14.480768977481684
gradient difference: 0.4902389645576477
train() client id: f_00000-0-0 loss: 1.257720  [   32/  126]
train() client id: f_00000-0-1 loss: 1.363034  [   64/  126]
train() client id: f_00000-0-2 loss: 1.181563  [   96/  126]
train() client id: f_00000-1-0 loss: 1.250995  [   32/  126]
train() client id: f_00000-1-1 loss: 1.126907  [   64/  126]
train() client id: f_00000-1-2 loss: 1.015171  [   96/  126]
train() client id: f_00000-2-0 loss: 1.151439  [   32/  126]
train() client id: f_00000-2-1 loss: 1.096469  [   64/  126]
train() client id: f_00000-2-2 loss: 0.961539  [   96/  126]
train() client id: f_00000-3-0 loss: 0.940733  [   32/  126]
train() client id: f_00000-3-1 loss: 0.779836  [   64/  126]
train() client id: f_00000-3-2 loss: 1.136546  [   96/  126]
train() client id: f_00000-4-0 loss: 0.952778  [   32/  126]
train() client id: f_00000-4-1 loss: 0.848935  [   64/  126]
train() client id: f_00000-4-2 loss: 1.016508  [   96/  126]
train() client id: f_00000-5-0 loss: 0.831142  [   32/  126]
train() client id: f_00000-5-1 loss: 0.878483  [   64/  126]
train() client id: f_00000-5-2 loss: 0.930292  [   96/  126]
train() client id: f_00000-6-0 loss: 0.840304  [   32/  126]
train() client id: f_00000-6-1 loss: 0.967322  [   64/  126]
train() client id: f_00000-6-2 loss: 0.834585  [   96/  126]
train() client id: f_00000-7-0 loss: 0.838399  [   32/  126]
train() client id: f_00000-7-1 loss: 0.776844  [   64/  126]
train() client id: f_00000-7-2 loss: 0.877152  [   96/  126]
train() client id: f_00000-8-0 loss: 0.882553  [   32/  126]
train() client id: f_00000-8-1 loss: 0.824490  [   64/  126]
train() client id: f_00000-8-2 loss: 0.737821  [   96/  126]
train() client id: f_00000-9-0 loss: 0.816800  [   32/  126]
train() client id: f_00000-9-1 loss: 0.752382  [   64/  126]
train() client id: f_00000-9-2 loss: 0.732983  [   96/  126]
train() client id: f_00000-10-0 loss: 0.873194  [   32/  126]
train() client id: f_00000-10-1 loss: 0.799098  [   64/  126]
train() client id: f_00000-10-2 loss: 0.823751  [   96/  126]
train() client id: f_00000-11-0 loss: 0.684996  [   32/  126]
train() client id: f_00000-11-1 loss: 0.696875  [   64/  126]
train() client id: f_00000-11-2 loss: 0.999646  [   96/  126]
train() client id: f_00000-12-0 loss: 0.717924  [   32/  126]
train() client id: f_00000-12-1 loss: 0.818017  [   64/  126]
train() client id: f_00000-12-2 loss: 0.750805  [   96/  126]
train() client id: f_00001-0-0 loss: 0.284210  [   32/  265]
train() client id: f_00001-0-1 loss: 0.416606  [   64/  265]
train() client id: f_00001-0-2 loss: 0.274642  [   96/  265]
train() client id: f_00001-0-3 loss: 0.372196  [  128/  265]
train() client id: f_00001-0-4 loss: 0.371752  [  160/  265]
train() client id: f_00001-0-5 loss: 0.338344  [  192/  265]
train() client id: f_00001-0-6 loss: 0.318013  [  224/  265]
train() client id: f_00001-0-7 loss: 0.367775  [  256/  265]
train() client id: f_00001-1-0 loss: 0.291843  [   32/  265]
train() client id: f_00001-1-1 loss: 0.435365  [   64/  265]
train() client id: f_00001-1-2 loss: 0.378330  [   96/  265]
train() client id: f_00001-1-3 loss: 0.338582  [  128/  265]
train() client id: f_00001-1-4 loss: 0.275868  [  160/  265]
train() client id: f_00001-1-5 loss: 0.301096  [  192/  265]
train() client id: f_00001-1-6 loss: 0.368172  [  224/  265]
train() client id: f_00001-1-7 loss: 0.256426  [  256/  265]
train() client id: f_00001-2-0 loss: 0.238499  [   32/  265]
train() client id: f_00001-2-1 loss: 0.386847  [   64/  265]
train() client id: f_00001-2-2 loss: 0.611866  [   96/  265]
train() client id: f_00001-2-3 loss: 0.249343  [  128/  265]
train() client id: f_00001-2-4 loss: 0.273324  [  160/  265]
train() client id: f_00001-2-5 loss: 0.274953  [  192/  265]
train() client id: f_00001-2-6 loss: 0.303913  [  224/  265]
train() client id: f_00001-2-7 loss: 0.248231  [  256/  265]
train() client id: f_00001-3-0 loss: 0.240447  [   32/  265]
train() client id: f_00001-3-1 loss: 0.253965  [   64/  265]
train() client id: f_00001-3-2 loss: 0.307510  [   96/  265]
train() client id: f_00001-3-3 loss: 0.317538  [  128/  265]
train() client id: f_00001-3-4 loss: 0.369799  [  160/  265]
train() client id: f_00001-3-5 loss: 0.390776  [  192/  265]
train() client id: f_00001-3-6 loss: 0.402437  [  224/  265]
train() client id: f_00001-3-7 loss: 0.227472  [  256/  265]
train() client id: f_00001-4-0 loss: 0.256595  [   32/  265]
train() client id: f_00001-4-1 loss: 0.229490  [   64/  265]
train() client id: f_00001-4-2 loss: 0.489122  [   96/  265]
train() client id: f_00001-4-3 loss: 0.375280  [  128/  265]
train() client id: f_00001-4-4 loss: 0.234394  [  160/  265]
train() client id: f_00001-4-5 loss: 0.300754  [  192/  265]
train() client id: f_00001-4-6 loss: 0.398545  [  224/  265]
train() client id: f_00001-4-7 loss: 0.261095  [  256/  265]
train() client id: f_00001-5-0 loss: 0.274034  [   32/  265]
train() client id: f_00001-5-1 loss: 0.288038  [   64/  265]
train() client id: f_00001-5-2 loss: 0.352302  [   96/  265]
train() client id: f_00001-5-3 loss: 0.277401  [  128/  265]
train() client id: f_00001-5-4 loss: 0.374905  [  160/  265]
train() client id: f_00001-5-5 loss: 0.393611  [  192/  265]
train() client id: f_00001-5-6 loss: 0.217266  [  224/  265]
train() client id: f_00001-5-7 loss: 0.282864  [  256/  265]
train() client id: f_00001-6-0 loss: 0.301961  [   32/  265]
train() client id: f_00001-6-1 loss: 0.307535  [   64/  265]
train() client id: f_00001-6-2 loss: 0.286567  [   96/  265]
train() client id: f_00001-6-3 loss: 0.339919  [  128/  265]
train() client id: f_00001-6-4 loss: 0.207014  [  160/  265]
train() client id: f_00001-6-5 loss: 0.328777  [  192/  265]
train() client id: f_00001-6-6 loss: 0.329518  [  224/  265]
train() client id: f_00001-6-7 loss: 0.334882  [  256/  265]
train() client id: f_00001-7-0 loss: 0.276755  [   32/  265]
train() client id: f_00001-7-1 loss: 0.227259  [   64/  265]
train() client id: f_00001-7-2 loss: 0.362178  [   96/  265]
train() client id: f_00001-7-3 loss: 0.261229  [  128/  265]
train() client id: f_00001-7-4 loss: 0.410311  [  160/  265]
train() client id: f_00001-7-5 loss: 0.469433  [  192/  265]
train() client id: f_00001-7-6 loss: 0.218337  [  224/  265]
train() client id: f_00001-7-7 loss: 0.225376  [  256/  265]
train() client id: f_00001-8-0 loss: 0.455029  [   32/  265]
train() client id: f_00001-8-1 loss: 0.335775  [   64/  265]
train() client id: f_00001-8-2 loss: 0.349965  [   96/  265]
train() client id: f_00001-8-3 loss: 0.295162  [  128/  265]
train() client id: f_00001-8-4 loss: 0.351805  [  160/  265]
train() client id: f_00001-8-5 loss: 0.213761  [  192/  265]
train() client id: f_00001-8-6 loss: 0.210096  [  224/  265]
train() client id: f_00001-8-7 loss: 0.208334  [  256/  265]
train() client id: f_00001-9-0 loss: 0.342807  [   32/  265]
train() client id: f_00001-9-1 loss: 0.320896  [   64/  265]
train() client id: f_00001-9-2 loss: 0.215539  [   96/  265]
train() client id: f_00001-9-3 loss: 0.451612  [  128/  265]
train() client id: f_00001-9-4 loss: 0.198451  [  160/  265]
train() client id: f_00001-9-5 loss: 0.359124  [  192/  265]
train() client id: f_00001-9-6 loss: 0.264015  [  224/  265]
train() client id: f_00001-9-7 loss: 0.268846  [  256/  265]
train() client id: f_00001-10-0 loss: 0.204898  [   32/  265]
train() client id: f_00001-10-1 loss: 0.314898  [   64/  265]
train() client id: f_00001-10-2 loss: 0.289821  [   96/  265]
train() client id: f_00001-10-3 loss: 0.342211  [  128/  265]
train() client id: f_00001-10-4 loss: 0.379430  [  160/  265]
train() client id: f_00001-10-5 loss: 0.294445  [  192/  265]
train() client id: f_00001-10-6 loss: 0.279053  [  224/  265]
train() client id: f_00001-10-7 loss: 0.293703  [  256/  265]
train() client id: f_00001-11-0 loss: 0.270784  [   32/  265]
train() client id: f_00001-11-1 loss: 0.208107  [   64/  265]
train() client id: f_00001-11-2 loss: 0.199392  [   96/  265]
train() client id: f_00001-11-3 loss: 0.385336  [  128/  265]
train() client id: f_00001-11-4 loss: 0.312706  [  160/  265]
train() client id: f_00001-11-5 loss: 0.296083  [  192/  265]
train() client id: f_00001-11-6 loss: 0.432294  [  224/  265]
train() client id: f_00001-11-7 loss: 0.247236  [  256/  265]
train() client id: f_00001-12-0 loss: 0.229522  [   32/  265]
train() client id: f_00001-12-1 loss: 0.387453  [   64/  265]
train() client id: f_00001-12-2 loss: 0.316358  [   96/  265]
train() client id: f_00001-12-3 loss: 0.295780  [  128/  265]
train() client id: f_00001-12-4 loss: 0.368122  [  160/  265]
train() client id: f_00001-12-5 loss: 0.312999  [  192/  265]
train() client id: f_00001-12-6 loss: 0.220108  [  224/  265]
train() client id: f_00001-12-7 loss: 0.253692  [  256/  265]
train() client id: f_00002-0-0 loss: 1.237216  [   32/  124]
train() client id: f_00002-0-1 loss: 1.072193  [   64/  124]
train() client id: f_00002-0-2 loss: 1.034351  [   96/  124]
train() client id: f_00002-1-0 loss: 1.100558  [   32/  124]
train() client id: f_00002-1-1 loss: 1.116659  [   64/  124]
train() client id: f_00002-1-2 loss: 1.069193  [   96/  124]
train() client id: f_00002-2-0 loss: 0.907708  [   32/  124]
train() client id: f_00002-2-1 loss: 1.241260  [   64/  124]
train() client id: f_00002-2-2 loss: 0.990914  [   96/  124]
train() client id: f_00002-3-0 loss: 0.965758  [   32/  124]
train() client id: f_00002-3-1 loss: 0.948437  [   64/  124]
train() client id: f_00002-3-2 loss: 1.113903  [   96/  124]
train() client id: f_00002-4-0 loss: 1.031362  [   32/  124]
train() client id: f_00002-4-1 loss: 1.150036  [   64/  124]
train() client id: f_00002-4-2 loss: 0.887945  [   96/  124]
train() client id: f_00002-5-0 loss: 1.072619  [   32/  124]
train() client id: f_00002-5-1 loss: 1.017210  [   64/  124]
train() client id: f_00002-5-2 loss: 1.048911  [   96/  124]
train() client id: f_00002-6-0 loss: 0.990899  [   32/  124]
train() client id: f_00002-6-1 loss: 1.151885  [   64/  124]
train() client id: f_00002-6-2 loss: 0.901401  [   96/  124]
train() client id: f_00002-7-0 loss: 0.892771  [   32/  124]
train() client id: f_00002-7-1 loss: 1.120920  [   64/  124]
train() client id: f_00002-7-2 loss: 1.019840  [   96/  124]
train() client id: f_00002-8-0 loss: 1.115998  [   32/  124]
train() client id: f_00002-8-1 loss: 1.057644  [   64/  124]
train() client id: f_00002-8-2 loss: 0.927853  [   96/  124]
train() client id: f_00002-9-0 loss: 1.012845  [   32/  124]
train() client id: f_00002-9-1 loss: 1.035126  [   64/  124]
train() client id: f_00002-9-2 loss: 0.994931  [   96/  124]
train() client id: f_00002-10-0 loss: 0.921528  [   32/  124]
train() client id: f_00002-10-1 loss: 0.989238  [   64/  124]
train() client id: f_00002-10-2 loss: 1.045126  [   96/  124]
train() client id: f_00002-11-0 loss: 0.986418  [   32/  124]
train() client id: f_00002-11-1 loss: 1.058105  [   64/  124]
train() client id: f_00002-11-2 loss: 0.848989  [   96/  124]
train() client id: f_00002-12-0 loss: 0.970117  [   32/  124]
train() client id: f_00002-12-1 loss: 0.903932  [   64/  124]
train() client id: f_00002-12-2 loss: 1.080038  [   96/  124]
train() client id: f_00003-0-0 loss: 0.645562  [   32/   43]
train() client id: f_00003-1-0 loss: 0.525634  [   32/   43]
train() client id: f_00003-2-0 loss: 0.655734  [   32/   43]
train() client id: f_00003-3-0 loss: 0.468287  [   32/   43]
train() client id: f_00003-4-0 loss: 0.638972  [   32/   43]
train() client id: f_00003-5-0 loss: 0.566174  [   32/   43]
train() client id: f_00003-6-0 loss: 0.696947  [   32/   43]
train() client id: f_00003-7-0 loss: 0.525565  [   32/   43]
train() client id: f_00003-8-0 loss: 0.804166  [   32/   43]
train() client id: f_00003-9-0 loss: 0.834982  [   32/   43]
train() client id: f_00003-10-0 loss: 0.526771  [   32/   43]
train() client id: f_00003-11-0 loss: 0.692865  [   32/   43]
train() client id: f_00003-12-0 loss: 0.611065  [   32/   43]
train() client id: f_00004-0-0 loss: 0.785267  [   32/  306]
train() client id: f_00004-0-1 loss: 0.663452  [   64/  306]
train() client id: f_00004-0-2 loss: 0.671968  [   96/  306]
train() client id: f_00004-0-3 loss: 0.778720  [  128/  306]
train() client id: f_00004-0-4 loss: 0.482579  [  160/  306]
train() client id: f_00004-0-5 loss: 0.686092  [  192/  306]
train() client id: f_00004-0-6 loss: 0.766400  [  224/  306]
train() client id: f_00004-0-7 loss: 0.831565  [  256/  306]
train() client id: f_00004-0-8 loss: 0.791403  [  288/  306]
train() client id: f_00004-1-0 loss: 0.628083  [   32/  306]
train() client id: f_00004-1-1 loss: 0.797545  [   64/  306]
train() client id: f_00004-1-2 loss: 0.822734  [   96/  306]
train() client id: f_00004-1-3 loss: 0.666908  [  128/  306]
train() client id: f_00004-1-4 loss: 0.812401  [  160/  306]
train() client id: f_00004-1-5 loss: 0.697846  [  192/  306]
train() client id: f_00004-1-6 loss: 0.686122  [  224/  306]
train() client id: f_00004-1-7 loss: 0.783812  [  256/  306]
train() client id: f_00004-1-8 loss: 0.723295  [  288/  306]
train() client id: f_00004-2-0 loss: 0.693036  [   32/  306]
train() client id: f_00004-2-1 loss: 0.601478  [   64/  306]
train() client id: f_00004-2-2 loss: 0.748746  [   96/  306]
train() client id: f_00004-2-3 loss: 0.632993  [  128/  306]
train() client id: f_00004-2-4 loss: 0.990931  [  160/  306]
train() client id: f_00004-2-5 loss: 0.613622  [  192/  306]
train() client id: f_00004-2-6 loss: 0.691057  [  224/  306]
train() client id: f_00004-2-7 loss: 0.702292  [  256/  306]
train() client id: f_00004-2-8 loss: 0.805802  [  288/  306]
train() client id: f_00004-3-0 loss: 0.745852  [   32/  306]
train() client id: f_00004-3-1 loss: 0.689516  [   64/  306]
train() client id: f_00004-3-2 loss: 0.782055  [   96/  306]
train() client id: f_00004-3-3 loss: 0.773723  [  128/  306]
train() client id: f_00004-3-4 loss: 0.822363  [  160/  306]
train() client id: f_00004-3-5 loss: 0.649354  [  192/  306]
train() client id: f_00004-3-6 loss: 0.768355  [  224/  306]
train() client id: f_00004-3-7 loss: 0.659543  [  256/  306]
train() client id: f_00004-3-8 loss: 0.734897  [  288/  306]
train() client id: f_00004-4-0 loss: 0.768023  [   32/  306]
train() client id: f_00004-4-1 loss: 0.700261  [   64/  306]
train() client id: f_00004-4-2 loss: 0.813048  [   96/  306]
train() client id: f_00004-4-3 loss: 0.730062  [  128/  306]
train() client id: f_00004-4-4 loss: 0.764431  [  160/  306]
train() client id: f_00004-4-5 loss: 0.704824  [  192/  306]
train() client id: f_00004-4-6 loss: 0.773560  [  224/  306]
train() client id: f_00004-4-7 loss: 0.615466  [  256/  306]
train() client id: f_00004-4-8 loss: 0.672171  [  288/  306]
train() client id: f_00004-5-0 loss: 0.846461  [   32/  306]
train() client id: f_00004-5-1 loss: 0.845975  [   64/  306]
train() client id: f_00004-5-2 loss: 0.763487  [   96/  306]
train() client id: f_00004-5-3 loss: 0.687736  [  128/  306]
train() client id: f_00004-5-4 loss: 0.650471  [  160/  306]
train() client id: f_00004-5-5 loss: 0.661827  [  192/  306]
train() client id: f_00004-5-6 loss: 0.692505  [  224/  306]
train() client id: f_00004-5-7 loss: 0.728437  [  256/  306]
train() client id: f_00004-5-8 loss: 0.856210  [  288/  306]
train() client id: f_00004-6-0 loss: 0.756295  [   32/  306]
train() client id: f_00004-6-1 loss: 0.757428  [   64/  306]
train() client id: f_00004-6-2 loss: 0.690018  [   96/  306]
train() client id: f_00004-6-3 loss: 0.766493  [  128/  306]
train() client id: f_00004-6-4 loss: 0.696322  [  160/  306]
train() client id: f_00004-6-5 loss: 0.644029  [  192/  306]
train() client id: f_00004-6-6 loss: 0.843402  [  224/  306]
train() client id: f_00004-6-7 loss: 0.748455  [  256/  306]
train() client id: f_00004-6-8 loss: 0.682890  [  288/  306]
train() client id: f_00004-7-0 loss: 0.659592  [   32/  306]
train() client id: f_00004-7-1 loss: 0.693992  [   64/  306]
train() client id: f_00004-7-2 loss: 0.702251  [   96/  306]
train() client id: f_00004-7-3 loss: 0.695086  [  128/  306]
train() client id: f_00004-7-4 loss: 0.788938  [  160/  306]
train() client id: f_00004-7-5 loss: 0.815756  [  192/  306]
train() client id: f_00004-7-6 loss: 0.895864  [  224/  306]
train() client id: f_00004-7-7 loss: 0.739169  [  256/  306]
train() client id: f_00004-7-8 loss: 0.721403  [  288/  306]
train() client id: f_00004-8-0 loss: 0.702250  [   32/  306]
train() client id: f_00004-8-1 loss: 0.802684  [   64/  306]
train() client id: f_00004-8-2 loss: 0.705452  [   96/  306]
train() client id: f_00004-8-3 loss: 0.671317  [  128/  306]
train() client id: f_00004-8-4 loss: 0.807143  [  160/  306]
train() client id: f_00004-8-5 loss: 0.724897  [  192/  306]
train() client id: f_00004-8-6 loss: 0.732232  [  224/  306]
train() client id: f_00004-8-7 loss: 0.816877  [  256/  306]
train() client id: f_00004-8-8 loss: 0.718803  [  288/  306]
train() client id: f_00004-9-0 loss: 0.847016  [   32/  306]
train() client id: f_00004-9-1 loss: 0.700278  [   64/  306]
train() client id: f_00004-9-2 loss: 0.793762  [   96/  306]
train() client id: f_00004-9-3 loss: 1.015791  [  128/  306]
train() client id: f_00004-9-4 loss: 0.655663  [  160/  306]
train() client id: f_00004-9-5 loss: 0.623362  [  192/  306]
train() client id: f_00004-9-6 loss: 0.769713  [  224/  306]
train() client id: f_00004-9-7 loss: 0.713663  [  256/  306]
train() client id: f_00004-9-8 loss: 0.607786  [  288/  306]
train() client id: f_00004-10-0 loss: 0.679662  [   32/  306]
train() client id: f_00004-10-1 loss: 0.727875  [   64/  306]
train() client id: f_00004-10-2 loss: 0.696433  [   96/  306]
train() client id: f_00004-10-3 loss: 0.800282  [  128/  306]
train() client id: f_00004-10-4 loss: 0.799797  [  160/  306]
train() client id: f_00004-10-5 loss: 0.858762  [  192/  306]
train() client id: f_00004-10-6 loss: 0.786075  [  224/  306]
train() client id: f_00004-10-7 loss: 0.818433  [  256/  306]
train() client id: f_00004-10-8 loss: 0.595044  [  288/  306]
train() client id: f_00004-11-0 loss: 0.775660  [   32/  306]
train() client id: f_00004-11-1 loss: 0.838983  [   64/  306]
train() client id: f_00004-11-2 loss: 0.775876  [   96/  306]
train() client id: f_00004-11-3 loss: 0.746750  [  128/  306]
train() client id: f_00004-11-4 loss: 0.649192  [  160/  306]
train() client id: f_00004-11-5 loss: 0.742807  [  192/  306]
train() client id: f_00004-11-6 loss: 0.779700  [  224/  306]
train() client id: f_00004-11-7 loss: 0.771982  [  256/  306]
train() client id: f_00004-11-8 loss: 0.652328  [  288/  306]
train() client id: f_00004-12-0 loss: 0.720373  [   32/  306]
train() client id: f_00004-12-1 loss: 0.666732  [   64/  306]
train() client id: f_00004-12-2 loss: 0.789480  [   96/  306]
train() client id: f_00004-12-3 loss: 0.879218  [  128/  306]
train() client id: f_00004-12-4 loss: 0.801878  [  160/  306]
train() client id: f_00004-12-5 loss: 0.748687  [  192/  306]
train() client id: f_00004-12-6 loss: 0.753350  [  224/  306]
train() client id: f_00004-12-7 loss: 0.720983  [  256/  306]
train() client id: f_00004-12-8 loss: 0.751163  [  288/  306]
train() client id: f_00005-0-0 loss: 0.611142  [   32/  146]
train() client id: f_00005-0-1 loss: 0.599757  [   64/  146]
train() client id: f_00005-0-2 loss: 0.777651  [   96/  146]
train() client id: f_00005-0-3 loss: 0.679504  [  128/  146]
train() client id: f_00005-1-0 loss: 0.903605  [   32/  146]
train() client id: f_00005-1-1 loss: 0.783134  [   64/  146]
train() client id: f_00005-1-2 loss: 0.659253  [   96/  146]
train() client id: f_00005-1-3 loss: 0.420877  [  128/  146]
train() client id: f_00005-2-0 loss: 0.549166  [   32/  146]
train() client id: f_00005-2-1 loss: 0.906195  [   64/  146]
train() client id: f_00005-2-2 loss: 0.594966  [   96/  146]
train() client id: f_00005-2-3 loss: 0.669999  [  128/  146]
train() client id: f_00005-3-0 loss: 0.528163  [   32/  146]
train() client id: f_00005-3-1 loss: 0.627315  [   64/  146]
train() client id: f_00005-3-2 loss: 0.787881  [   96/  146]
train() client id: f_00005-3-3 loss: 0.642491  [  128/  146]
train() client id: f_00005-4-0 loss: 0.699542  [   32/  146]
train() client id: f_00005-4-1 loss: 0.782560  [   64/  146]
train() client id: f_00005-4-2 loss: 0.851899  [   96/  146]
train() client id: f_00005-4-3 loss: 0.446750  [  128/  146]
train() client id: f_00005-5-0 loss: 0.564941  [   32/  146]
train() client id: f_00005-5-1 loss: 0.815849  [   64/  146]
train() client id: f_00005-5-2 loss: 0.764925  [   96/  146]
train() client id: f_00005-5-3 loss: 0.795342  [  128/  146]
train() client id: f_00005-6-0 loss: 0.896700  [   32/  146]
train() client id: f_00005-6-1 loss: 0.570579  [   64/  146]
train() client id: f_00005-6-2 loss: 0.635934  [   96/  146]
train() client id: f_00005-6-3 loss: 0.510554  [  128/  146]
train() client id: f_00005-7-0 loss: 0.570311  [   32/  146]
train() client id: f_00005-7-1 loss: 0.749224  [   64/  146]
train() client id: f_00005-7-2 loss: 0.489715  [   96/  146]
train() client id: f_00005-7-3 loss: 0.896095  [  128/  146]
train() client id: f_00005-8-0 loss: 0.867700  [   32/  146]
train() client id: f_00005-8-1 loss: 0.834320  [   64/  146]
train() client id: f_00005-8-2 loss: 0.490386  [   96/  146]
train() client id: f_00005-8-3 loss: 0.690952  [  128/  146]
train() client id: f_00005-9-0 loss: 0.820779  [   32/  146]
train() client id: f_00005-9-1 loss: 0.910207  [   64/  146]
train() client id: f_00005-9-2 loss: 0.512399  [   96/  146]
train() client id: f_00005-9-3 loss: 0.703742  [  128/  146]
train() client id: f_00005-10-0 loss: 0.630045  [   32/  146]
train() client id: f_00005-10-1 loss: 0.649961  [   64/  146]
train() client id: f_00005-10-2 loss: 0.800131  [   96/  146]
train() client id: f_00005-10-3 loss: 0.563401  [  128/  146]
train() client id: f_00005-11-0 loss: 0.405838  [   32/  146]
train() client id: f_00005-11-1 loss: 0.812079  [   64/  146]
train() client id: f_00005-11-2 loss: 0.657530  [   96/  146]
train() client id: f_00005-11-3 loss: 0.873992  [  128/  146]
train() client id: f_00005-12-0 loss: 0.846891  [   32/  146]
train() client id: f_00005-12-1 loss: 0.614670  [   64/  146]
train() client id: f_00005-12-2 loss: 0.630403  [   96/  146]
train() client id: f_00005-12-3 loss: 0.705491  [  128/  146]
train() client id: f_00006-0-0 loss: 0.471649  [   32/   54]
train() client id: f_00006-1-0 loss: 0.515710  [   32/   54]
train() client id: f_00006-2-0 loss: 0.521065  [   32/   54]
train() client id: f_00006-3-0 loss: 0.451147  [   32/   54]
train() client id: f_00006-4-0 loss: 0.495196  [   32/   54]
train() client id: f_00006-5-0 loss: 0.475206  [   32/   54]
train() client id: f_00006-6-0 loss: 0.402750  [   32/   54]
train() client id: f_00006-7-0 loss: 0.454222  [   32/   54]
train() client id: f_00006-8-0 loss: 0.471606  [   32/   54]
train() client id: f_00006-9-0 loss: 0.530990  [   32/   54]
train() client id: f_00006-10-0 loss: 0.400713  [   32/   54]
train() client id: f_00006-11-0 loss: 0.518371  [   32/   54]
train() client id: f_00006-12-0 loss: 0.450180  [   32/   54]
train() client id: f_00007-0-0 loss: 0.916457  [   32/  179]
train() client id: f_00007-0-1 loss: 0.702645  [   64/  179]
train() client id: f_00007-0-2 loss: 0.567896  [   96/  179]
train() client id: f_00007-0-3 loss: 0.730445  [  128/  179]
train() client id: f_00007-0-4 loss: 0.758169  [  160/  179]
train() client id: f_00007-1-0 loss: 0.591492  [   32/  179]
train() client id: f_00007-1-1 loss: 0.670304  [   64/  179]
train() client id: f_00007-1-2 loss: 0.565998  [   96/  179]
train() client id: f_00007-1-3 loss: 0.538256  [  128/  179]
train() client id: f_00007-1-4 loss: 1.015901  [  160/  179]
train() client id: f_00007-2-0 loss: 0.648322  [   32/  179]
train() client id: f_00007-2-1 loss: 0.785553  [   64/  179]
train() client id: f_00007-2-2 loss: 0.642963  [   96/  179]
train() client id: f_00007-2-3 loss: 0.769394  [  128/  179]
train() client id: f_00007-2-4 loss: 0.738794  [  160/  179]
train() client id: f_00007-3-0 loss: 0.971275  [   32/  179]
train() client id: f_00007-3-1 loss: 0.620809  [   64/  179]
train() client id: f_00007-3-2 loss: 0.688126  [   96/  179]
train() client id: f_00007-3-3 loss: 0.682706  [  128/  179]
train() client id: f_00007-3-4 loss: 0.658334  [  160/  179]
train() client id: f_00007-4-0 loss: 0.757231  [   32/  179]
train() client id: f_00007-4-1 loss: 0.627250  [   64/  179]
train() client id: f_00007-4-2 loss: 0.698693  [   96/  179]
train() client id: f_00007-4-3 loss: 0.564696  [  128/  179]
train() client id: f_00007-4-4 loss: 0.835730  [  160/  179]
train() client id: f_00007-5-0 loss: 0.824607  [   32/  179]
train() client id: f_00007-5-1 loss: 0.728733  [   64/  179]
train() client id: f_00007-5-2 loss: 0.516959  [   96/  179]
train() client id: f_00007-5-3 loss: 0.791608  [  128/  179]
train() client id: f_00007-5-4 loss: 0.534990  [  160/  179]
train() client id: f_00007-6-0 loss: 0.578902  [   32/  179]
train() client id: f_00007-6-1 loss: 0.901642  [   64/  179]
train() client id: f_00007-6-2 loss: 0.623496  [   96/  179]
train() client id: f_00007-6-3 loss: 0.728724  [  128/  179]
train() client id: f_00007-6-4 loss: 0.668754  [  160/  179]
train() client id: f_00007-7-0 loss: 0.507000  [   32/  179]
train() client id: f_00007-7-1 loss: 0.558772  [   64/  179]
train() client id: f_00007-7-2 loss: 0.647303  [   96/  179]
train() client id: f_00007-7-3 loss: 0.936388  [  128/  179]
train() client id: f_00007-7-4 loss: 0.687409  [  160/  179]
train() client id: f_00007-8-0 loss: 0.663921  [   32/  179]
train() client id: f_00007-8-1 loss: 0.637272  [   64/  179]
train() client id: f_00007-8-2 loss: 0.735551  [   96/  179]
train() client id: f_00007-8-3 loss: 0.773634  [  128/  179]
train() client id: f_00007-8-4 loss: 0.582712  [  160/  179]
train() client id: f_00007-9-0 loss: 0.652205  [   32/  179]
train() client id: f_00007-9-1 loss: 0.695920  [   64/  179]
train() client id: f_00007-9-2 loss: 0.554537  [   96/  179]
train() client id: f_00007-9-3 loss: 0.542771  [  128/  179]
train() client id: f_00007-9-4 loss: 0.972039  [  160/  179]
train() client id: f_00007-10-0 loss: 0.720866  [   32/  179]
train() client id: f_00007-10-1 loss: 0.794561  [   64/  179]
train() client id: f_00007-10-2 loss: 0.633767  [   96/  179]
train() client id: f_00007-10-3 loss: 0.664415  [  128/  179]
train() client id: f_00007-10-4 loss: 0.652462  [  160/  179]
train() client id: f_00007-11-0 loss: 0.638044  [   32/  179]
train() client id: f_00007-11-1 loss: 0.932856  [   64/  179]
train() client id: f_00007-11-2 loss: 0.609705  [   96/  179]
train() client id: f_00007-11-3 loss: 0.643585  [  128/  179]
train() client id: f_00007-11-4 loss: 0.645753  [  160/  179]
train() client id: f_00007-12-0 loss: 0.764812  [   32/  179]
train() client id: f_00007-12-1 loss: 0.643450  [   64/  179]
train() client id: f_00007-12-2 loss: 0.639380  [   96/  179]
train() client id: f_00007-12-3 loss: 0.549853  [  128/  179]
train() client id: f_00007-12-4 loss: 0.805984  [  160/  179]
train() client id: f_00008-0-0 loss: 0.745166  [   32/  130]
train() client id: f_00008-0-1 loss: 0.816922  [   64/  130]
train() client id: f_00008-0-2 loss: 0.818540  [   96/  130]
train() client id: f_00008-0-3 loss: 0.704386  [  128/  130]
train() client id: f_00008-1-0 loss: 0.796272  [   32/  130]
train() client id: f_00008-1-1 loss: 0.929863  [   64/  130]
train() client id: f_00008-1-2 loss: 0.737067  [   96/  130]
train() client id: f_00008-1-3 loss: 0.676949  [  128/  130]
train() client id: f_00008-2-0 loss: 0.728764  [   32/  130]
train() client id: f_00008-2-1 loss: 0.802676  [   64/  130]
train() client id: f_00008-2-2 loss: 0.870254  [   96/  130]
train() client id: f_00008-2-3 loss: 0.728726  [  128/  130]
train() client id: f_00008-3-0 loss: 0.749301  [   32/  130]
train() client id: f_00008-3-1 loss: 0.832858  [   64/  130]
train() client id: f_00008-3-2 loss: 0.737039  [   96/  130]
train() client id: f_00008-3-3 loss: 0.811481  [  128/  130]
train() client id: f_00008-4-0 loss: 0.791165  [   32/  130]
train() client id: f_00008-4-1 loss: 0.778368  [   64/  130]
train() client id: f_00008-4-2 loss: 0.871732  [   96/  130]
train() client id: f_00008-4-3 loss: 0.669642  [  128/  130]
train() client id: f_00008-5-0 loss: 0.748314  [   32/  130]
train() client id: f_00008-5-1 loss: 0.749546  [   64/  130]
train() client id: f_00008-5-2 loss: 0.878770  [   96/  130]
train() client id: f_00008-5-3 loss: 0.758072  [  128/  130]
train() client id: f_00008-6-0 loss: 0.711417  [   32/  130]
train() client id: f_00008-6-1 loss: 0.892585  [   64/  130]
train() client id: f_00008-6-2 loss: 0.891018  [   96/  130]
train() client id: f_00008-6-3 loss: 0.625668  [  128/  130]
train() client id: f_00008-7-0 loss: 0.680290  [   32/  130]
train() client id: f_00008-7-1 loss: 0.852060  [   64/  130]
train() client id: f_00008-7-2 loss: 0.829084  [   96/  130]
train() client id: f_00008-7-3 loss: 0.768994  [  128/  130]
train() client id: f_00008-8-0 loss: 0.816812  [   32/  130]
train() client id: f_00008-8-1 loss: 0.710165  [   64/  130]
train() client id: f_00008-8-2 loss: 0.740031  [   96/  130]
train() client id: f_00008-8-3 loss: 0.861530  [  128/  130]
train() client id: f_00008-9-0 loss: 0.827376  [   32/  130]
train() client id: f_00008-9-1 loss: 0.739539  [   64/  130]
train() client id: f_00008-9-2 loss: 0.837678  [   96/  130]
train() client id: f_00008-9-3 loss: 0.688585  [  128/  130]
train() client id: f_00008-10-0 loss: 0.707769  [   32/  130]
train() client id: f_00008-10-1 loss: 0.720723  [   64/  130]
train() client id: f_00008-10-2 loss: 0.723767  [   96/  130]
train() client id: f_00008-10-3 loss: 0.982215  [  128/  130]
train() client id: f_00008-11-0 loss: 0.772935  [   32/  130]
train() client id: f_00008-11-1 loss: 0.869182  [   64/  130]
train() client id: f_00008-11-2 loss: 0.806162  [   96/  130]
train() client id: f_00008-11-3 loss: 0.680086  [  128/  130]
train() client id: f_00008-12-0 loss: 0.783778  [   32/  130]
train() client id: f_00008-12-1 loss: 0.841887  [   64/  130]
train() client id: f_00008-12-2 loss: 0.677615  [   96/  130]
train() client id: f_00008-12-3 loss: 0.778696  [  128/  130]
train() client id: f_00009-0-0 loss: 1.145144  [   32/  118]
train() client id: f_00009-0-1 loss: 1.284304  [   64/  118]
train() client id: f_00009-0-2 loss: 1.085375  [   96/  118]
train() client id: f_00009-1-0 loss: 1.045634  [   32/  118]
train() client id: f_00009-1-1 loss: 1.096957  [   64/  118]
train() client id: f_00009-1-2 loss: 1.152483  [   96/  118]
train() client id: f_00009-2-0 loss: 1.270916  [   32/  118]
train() client id: f_00009-2-1 loss: 1.025036  [   64/  118]
train() client id: f_00009-2-2 loss: 1.080661  [   96/  118]
train() client id: f_00009-3-0 loss: 1.180060  [   32/  118]
train() client id: f_00009-3-1 loss: 0.949311  [   64/  118]
train() client id: f_00009-3-2 loss: 0.979806  [   96/  118]
train() client id: f_00009-4-0 loss: 1.039845  [   32/  118]
train() client id: f_00009-4-1 loss: 0.987447  [   64/  118]
train() client id: f_00009-4-2 loss: 0.948830  [   96/  118]
train() client id: f_00009-5-0 loss: 1.035902  [   32/  118]
train() client id: f_00009-5-1 loss: 0.846074  [   64/  118]
train() client id: f_00009-5-2 loss: 1.036545  [   96/  118]
train() client id: f_00009-6-0 loss: 0.920216  [   32/  118]
train() client id: f_00009-6-1 loss: 1.096054  [   64/  118]
train() client id: f_00009-6-2 loss: 0.909212  [   96/  118]
train() client id: f_00009-7-0 loss: 0.827618  [   32/  118]
train() client id: f_00009-7-1 loss: 1.083652  [   64/  118]
train() client id: f_00009-7-2 loss: 1.075194  [   96/  118]
train() client id: f_00009-8-0 loss: 0.993631  [   32/  118]
train() client id: f_00009-8-1 loss: 0.943937  [   64/  118]
train() client id: f_00009-8-2 loss: 0.839689  [   96/  118]
train() client id: f_00009-9-0 loss: 1.090620  [   32/  118]
train() client id: f_00009-9-1 loss: 0.860517  [   64/  118]
train() client id: f_00009-9-2 loss: 0.959677  [   96/  118]
train() client id: f_00009-10-0 loss: 0.962132  [   32/  118]
train() client id: f_00009-10-1 loss: 0.980674  [   64/  118]
train() client id: f_00009-10-2 loss: 0.877477  [   96/  118]
train() client id: f_00009-11-0 loss: 0.879543  [   32/  118]
train() client id: f_00009-11-1 loss: 0.937879  [   64/  118]
train() client id: f_00009-11-2 loss: 1.131447  [   96/  118]
train() client id: f_00009-12-0 loss: 0.978626  [   32/  118]
train() client id: f_00009-12-1 loss: 0.892048  [   64/  118]
train() client id: f_00009-12-2 loss: 0.879793  [   96/  118]
At round 39 accuracy: 0.6525198938992043
At round 39 training accuracy: 0.5915492957746479
At round 39 training loss: 0.8268532643964559
update_location
xs = [  -3.9056584     4.20031788  215.00902392   18.81129433    0.97929623
    3.95640986 -177.44319194 -156.32485185  199.66397685 -142.06087855]
ys = [ 207.5879595   190.55583871    1.32061395 -177.45517586  169.35018685
  152.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [230.45176306 215.24212027 237.12997361 204.55855942 196.67344713
 182.66860115 203.69824964 185.57514802 223.99636967 173.77371801]
dists_bs = [174.17017204 179.87422841 426.93274397 402.22813029 176.06996646
 180.32639446 177.6417863  175.17595617 406.39588262 174.14212286]
uav_gains = [1.06238871e-11 1.35916922e-11 9.45055194e-12 1.59558146e-11
 1.78934679e-11 2.18960415e-11 1.61583748e-11 2.09940106e-11
 1.18323052e-11 2.49490095e-11]
bs_gains = [5.86904580e-11 5.36266962e-11 4.76746578e-12 5.63340860e-12
 5.69344720e-11 5.32510343e-11 5.55351203e-11 5.77517960e-11
 5.47313357e-12 5.87169311e-11]
Round 40
-------------------------------
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.71197888 11.79408709  5.62914561  2.03417315 13.60152567  6.544648
  2.51897531  8.02443866  5.92443305  5.30743185]
obj_prev = 67.09083726402451
eta_min = 7.023642441479729e-17	eta_max = 0.9321930357555286
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 15.5538538243213	eta = 0.909090909090909
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 29.16218476187658	eta = 0.4848699515649548
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 22.40002232600407	eta = 0.6312434383873119
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.175688042561685	eta = 0.667740622387296
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.10970401340027	eta = 0.6698278243997874
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.109495669486368	eta = 0.6698344353843775
eta = 0.6698344353843775
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.03306684 0.06954534 0.03254197 0.01128472 0.08030519 0.03831555
 0.0141715  0.04697591 0.03411658 0.03096736]
ene_total = [1.91082934 3.36153203 1.90398611 0.90652186 3.82972324 1.99226433
 1.03177315 2.44413259 2.06480829 1.66392474]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 0 obj = 3.9578926480979075
eta = 0.6698344353843775
freqs = [30996407.38459397 61282205.26212957 30716806.61521623 10361284.76037134
 70657812.12672363 33769043.53321395 13004437.39099001 42636067.25492053
 33527323.09434476 27226537.57500104]
eta_min = 0.6698344353843826	eta_max = 0.6698344353843708
af = 0.008298545367209708	bf = 1.3034344286740114	zeta = 0.00912839990393068	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [1.88907249e-06 1.55299480e-05 1.82569896e-06 7.20362393e-08
 2.38395007e-05 2.59803940e-06 1.42505706e-07 5.07764887e-06
 2.28032353e-06 1.36496714e-06]
ene_total = [1.67292505 1.13318653 1.73166738 1.49481175 1.1209777  1.13273715
 1.48988129 1.3947409  2.06499979 1.11055146]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 1 obj = 3.9578926480978276
eta = 0.6698344353843708
freqs = [30996407.384594   61282205.26212971 30716806.61521625 10361284.76037136
 70657812.12672378 33769043.53321403 13004437.39099002 42636067.2549206
 33527323.09434475 27226537.5750011 ]
Done!
At round 40 energy consumption: 3.9578926480979075
At round 40 eta: 0.6698344353843708
At round 40 local rounds: 13.121762881988987
At round 40 global rounds: 43.859113515789716
At round 40 a_n: 14.138223130512367
gradient difference: 0.4914652705192566
train() client id: f_00000-0-0 loss: 1.195490  [   32/  126]
train() client id: f_00000-0-1 loss: 0.929785  [   64/  126]
train() client id: f_00000-0-2 loss: 0.924886  [   96/  126]
train() client id: f_00000-1-0 loss: 0.785077  [   32/  126]
train() client id: f_00000-1-1 loss: 0.856113  [   64/  126]
train() client id: f_00000-1-2 loss: 0.943663  [   96/  126]
train() client id: f_00000-2-0 loss: 0.889106  [   32/  126]
train() client id: f_00000-2-1 loss: 0.886616  [   64/  126]
train() client id: f_00000-2-2 loss: 1.058198  [   96/  126]
train() client id: f_00000-3-0 loss: 0.805001  [   32/  126]
train() client id: f_00000-3-1 loss: 0.959353  [   64/  126]
train() client id: f_00000-3-2 loss: 0.889159  [   96/  126]
train() client id: f_00000-4-0 loss: 0.803343  [   32/  126]
train() client id: f_00000-4-1 loss: 0.967642  [   64/  126]
train() client id: f_00000-4-2 loss: 0.757089  [   96/  126]
train() client id: f_00000-5-0 loss: 0.857624  [   32/  126]
train() client id: f_00000-5-1 loss: 0.778401  [   64/  126]
train() client id: f_00000-5-2 loss: 0.874539  [   96/  126]
train() client id: f_00000-6-0 loss: 0.927157  [   32/  126]
train() client id: f_00000-6-1 loss: 0.830268  [   64/  126]
train() client id: f_00000-6-2 loss: 0.762164  [   96/  126]
train() client id: f_00000-7-0 loss: 0.878741  [   32/  126]
train() client id: f_00000-7-1 loss: 0.891721  [   64/  126]
train() client id: f_00000-7-2 loss: 0.867405  [   96/  126]
train() client id: f_00000-8-0 loss: 0.909622  [   32/  126]
train() client id: f_00000-8-1 loss: 0.912557  [   64/  126]
train() client id: f_00000-8-2 loss: 0.776260  [   96/  126]
train() client id: f_00000-9-0 loss: 0.842951  [   32/  126]
train() client id: f_00000-9-1 loss: 0.872938  [   64/  126]
train() client id: f_00000-9-2 loss: 0.919485  [   96/  126]
train() client id: f_00000-10-0 loss: 0.866729  [   32/  126]
train() client id: f_00000-10-1 loss: 0.866752  [   64/  126]
train() client id: f_00000-10-2 loss: 0.901829  [   96/  126]
train() client id: f_00000-11-0 loss: 0.861197  [   32/  126]
train() client id: f_00000-11-1 loss: 0.820182  [   64/  126]
train() client id: f_00000-11-2 loss: 1.010933  [   96/  126]
train() client id: f_00000-12-0 loss: 0.907774  [   32/  126]
train() client id: f_00000-12-1 loss: 0.992230  [   64/  126]
train() client id: f_00000-12-2 loss: 0.838480  [   96/  126]
train() client id: f_00001-0-0 loss: 0.442548  [   32/  265]
train() client id: f_00001-0-1 loss: 0.524689  [   64/  265]
train() client id: f_00001-0-2 loss: 0.381898  [   96/  265]
train() client id: f_00001-0-3 loss: 0.434195  [  128/  265]
train() client id: f_00001-0-4 loss: 0.356864  [  160/  265]
train() client id: f_00001-0-5 loss: 0.418901  [  192/  265]
train() client id: f_00001-0-6 loss: 0.513989  [  224/  265]
train() client id: f_00001-0-7 loss: 0.538785  [  256/  265]
train() client id: f_00001-1-0 loss: 0.431152  [   32/  265]
train() client id: f_00001-1-1 loss: 0.493843  [   64/  265]
train() client id: f_00001-1-2 loss: 0.394317  [   96/  265]
train() client id: f_00001-1-3 loss: 0.541119  [  128/  265]
train() client id: f_00001-1-4 loss: 0.399067  [  160/  265]
train() client id: f_00001-1-5 loss: 0.520006  [  192/  265]
train() client id: f_00001-1-6 loss: 0.439807  [  224/  265]
train() client id: f_00001-1-7 loss: 0.352497  [  256/  265]
train() client id: f_00001-2-0 loss: 0.560559  [   32/  265]
train() client id: f_00001-2-1 loss: 0.416418  [   64/  265]
train() client id: f_00001-2-2 loss: 0.432491  [   96/  265]
train() client id: f_00001-2-3 loss: 0.409026  [  128/  265]
train() client id: f_00001-2-4 loss: 0.377753  [  160/  265]
train() client id: f_00001-2-5 loss: 0.370118  [  192/  265]
train() client id: f_00001-2-6 loss: 0.407227  [  224/  265]
train() client id: f_00001-2-7 loss: 0.525074  [  256/  265]
train() client id: f_00001-3-0 loss: 0.397862  [   32/  265]
train() client id: f_00001-3-1 loss: 0.490173  [   64/  265]
train() client id: f_00001-3-2 loss: 0.437674  [   96/  265]
train() client id: f_00001-3-3 loss: 0.422306  [  128/  265]
train() client id: f_00001-3-4 loss: 0.404624  [  160/  265]
train() client id: f_00001-3-5 loss: 0.478835  [  192/  265]
train() client id: f_00001-3-6 loss: 0.404047  [  224/  265]
train() client id: f_00001-3-7 loss: 0.416613  [  256/  265]
train() client id: f_00001-4-0 loss: 0.426586  [   32/  265]
train() client id: f_00001-4-1 loss: 0.323620  [   64/  265]
train() client id: f_00001-4-2 loss: 0.366268  [   96/  265]
train() client id: f_00001-4-3 loss: 0.507699  [  128/  265]
train() client id: f_00001-4-4 loss: 0.369597  [  160/  265]
train() client id: f_00001-4-5 loss: 0.437244  [  192/  265]
train() client id: f_00001-4-6 loss: 0.533753  [  224/  265]
train() client id: f_00001-4-7 loss: 0.469087  [  256/  265]
train() client id: f_00001-5-0 loss: 0.433540  [   32/  265]
train() client id: f_00001-5-1 loss: 0.399123  [   64/  265]
train() client id: f_00001-5-2 loss: 0.351404  [   96/  265]
train() client id: f_00001-5-3 loss: 0.401295  [  128/  265]
train() client id: f_00001-5-4 loss: 0.488334  [  160/  265]
train() client id: f_00001-5-5 loss: 0.544373  [  192/  265]
train() client id: f_00001-5-6 loss: 0.465434  [  224/  265]
train() client id: f_00001-5-7 loss: 0.351484  [  256/  265]
train() client id: f_00001-6-0 loss: 0.390072  [   32/  265]
train() client id: f_00001-6-1 loss: 0.311158  [   64/  265]
train() client id: f_00001-6-2 loss: 0.477608  [   96/  265]
train() client id: f_00001-6-3 loss: 0.496106  [  128/  265]
train() client id: f_00001-6-4 loss: 0.422837  [  160/  265]
train() client id: f_00001-6-5 loss: 0.420641  [  192/  265]
train() client id: f_00001-6-6 loss: 0.415172  [  224/  265]
train() client id: f_00001-6-7 loss: 0.378052  [  256/  265]
train() client id: f_00001-7-0 loss: 0.380261  [   32/  265]
train() client id: f_00001-7-1 loss: 0.436626  [   64/  265]
train() client id: f_00001-7-2 loss: 0.343193  [   96/  265]
train() client id: f_00001-7-3 loss: 0.495100  [  128/  265]
train() client id: f_00001-7-4 loss: 0.498750  [  160/  265]
train() client id: f_00001-7-5 loss: 0.315214  [  192/  265]
train() client id: f_00001-7-6 loss: 0.477059  [  224/  265]
train() client id: f_00001-7-7 loss: 0.439446  [  256/  265]
train() client id: f_00001-8-0 loss: 0.346761  [   32/  265]
train() client id: f_00001-8-1 loss: 0.473645  [   64/  265]
train() client id: f_00001-8-2 loss: 0.391930  [   96/  265]
train() client id: f_00001-8-3 loss: 0.520826  [  128/  265]
train() client id: f_00001-8-4 loss: 0.445060  [  160/  265]
train() client id: f_00001-8-5 loss: 0.496710  [  192/  265]
train() client id: f_00001-8-6 loss: 0.404493  [  224/  265]
train() client id: f_00001-8-7 loss: 0.297895  [  256/  265]
train() client id: f_00001-9-0 loss: 0.467073  [   32/  265]
train() client id: f_00001-9-1 loss: 0.444913  [   64/  265]
train() client id: f_00001-9-2 loss: 0.453734  [   96/  265]
train() client id: f_00001-9-3 loss: 0.474941  [  128/  265]
train() client id: f_00001-9-4 loss: 0.454558  [  160/  265]
train() client id: f_00001-9-5 loss: 0.338541  [  192/  265]
train() client id: f_00001-9-6 loss: 0.315258  [  224/  265]
train() client id: f_00001-9-7 loss: 0.416948  [  256/  265]
train() client id: f_00001-10-0 loss: 0.374212  [   32/  265]
train() client id: f_00001-10-1 loss: 0.331362  [   64/  265]
train() client id: f_00001-10-2 loss: 0.325424  [   96/  265]
train() client id: f_00001-10-3 loss: 0.458012  [  128/  265]
train() client id: f_00001-10-4 loss: 0.408096  [  160/  265]
train() client id: f_00001-10-5 loss: 0.377347  [  192/  265]
train() client id: f_00001-10-6 loss: 0.598304  [  224/  265]
train() client id: f_00001-10-7 loss: 0.430129  [  256/  265]
train() client id: f_00001-11-0 loss: 0.421606  [   32/  265]
train() client id: f_00001-11-1 loss: 0.412426  [   64/  265]
train() client id: f_00001-11-2 loss: 0.464214  [   96/  265]
train() client id: f_00001-11-3 loss: 0.463862  [  128/  265]
train() client id: f_00001-11-4 loss: 0.497277  [  160/  265]
train() client id: f_00001-11-5 loss: 0.384811  [  192/  265]
train() client id: f_00001-11-6 loss: 0.344668  [  224/  265]
train() client id: f_00001-11-7 loss: 0.369588  [  256/  265]
train() client id: f_00001-12-0 loss: 0.483789  [   32/  265]
train() client id: f_00001-12-1 loss: 0.312889  [   64/  265]
train() client id: f_00001-12-2 loss: 0.596378  [   96/  265]
train() client id: f_00001-12-3 loss: 0.422769  [  128/  265]
train() client id: f_00001-12-4 loss: 0.503132  [  160/  265]
train() client id: f_00001-12-5 loss: 0.384513  [  192/  265]
train() client id: f_00001-12-6 loss: 0.315908  [  224/  265]
train() client id: f_00001-12-7 loss: 0.328826  [  256/  265]
train() client id: f_00002-0-0 loss: 1.058383  [   32/  124]
train() client id: f_00002-0-1 loss: 1.144196  [   64/  124]
train() client id: f_00002-0-2 loss: 0.958212  [   96/  124]
train() client id: f_00002-1-0 loss: 0.934044  [   32/  124]
train() client id: f_00002-1-1 loss: 1.013607  [   64/  124]
train() client id: f_00002-1-2 loss: 1.146746  [   96/  124]
train() client id: f_00002-2-0 loss: 1.082130  [   32/  124]
train() client id: f_00002-2-1 loss: 1.219973  [   64/  124]
train() client id: f_00002-2-2 loss: 0.849760  [   96/  124]
train() client id: f_00002-3-0 loss: 1.124030  [   32/  124]
train() client id: f_00002-3-1 loss: 1.040008  [   64/  124]
train() client id: f_00002-3-2 loss: 0.836485  [   96/  124]
train() client id: f_00002-4-0 loss: 0.737872  [   32/  124]
train() client id: f_00002-4-1 loss: 1.013585  [   64/  124]
train() client id: f_00002-4-2 loss: 1.021781  [   96/  124]
train() client id: f_00002-5-0 loss: 1.017285  [   32/  124]
train() client id: f_00002-5-1 loss: 0.862344  [   64/  124]
train() client id: f_00002-5-2 loss: 0.882854  [   96/  124]
train() client id: f_00002-6-0 loss: 0.759265  [   32/  124]
train() client id: f_00002-6-1 loss: 0.940295  [   64/  124]
train() client id: f_00002-6-2 loss: 0.967755  [   96/  124]
train() client id: f_00002-7-0 loss: 0.872387  [   32/  124]
train() client id: f_00002-7-1 loss: 0.841165  [   64/  124]
train() client id: f_00002-7-2 loss: 1.049684  [   96/  124]
train() client id: f_00002-8-0 loss: 0.743331  [   32/  124]
train() client id: f_00002-8-1 loss: 0.997669  [   64/  124]
train() client id: f_00002-8-2 loss: 1.062573  [   96/  124]
train() client id: f_00002-9-0 loss: 0.750199  [   32/  124]
train() client id: f_00002-9-1 loss: 1.085177  [   64/  124]
train() client id: f_00002-9-2 loss: 0.942280  [   96/  124]
train() client id: f_00002-10-0 loss: 0.857393  [   32/  124]
train() client id: f_00002-10-1 loss: 0.992996  [   64/  124]
train() client id: f_00002-10-2 loss: 0.747131  [   96/  124]
train() client id: f_00002-11-0 loss: 0.823798  [   32/  124]
train() client id: f_00002-11-1 loss: 0.778624  [   64/  124]
train() client id: f_00002-11-2 loss: 0.911271  [   96/  124]
train() client id: f_00002-12-0 loss: 0.790364  [   32/  124]
train() client id: f_00002-12-1 loss: 0.896998  [   64/  124]
train() client id: f_00002-12-2 loss: 0.826894  [   96/  124]
train() client id: f_00003-0-0 loss: 0.822552  [   32/   43]
train() client id: f_00003-1-0 loss: 0.647009  [   32/   43]
train() client id: f_00003-2-0 loss: 0.851787  [   32/   43]
train() client id: f_00003-3-0 loss: 0.716628  [   32/   43]
train() client id: f_00003-4-0 loss: 0.677247  [   32/   43]
train() client id: f_00003-5-0 loss: 0.839491  [   32/   43]
train() client id: f_00003-6-0 loss: 0.633918  [   32/   43]
train() client id: f_00003-7-0 loss: 0.862368  [   32/   43]
train() client id: f_00003-8-0 loss: 0.735481  [   32/   43]
train() client id: f_00003-9-0 loss: 0.665148  [   32/   43]
train() client id: f_00003-10-0 loss: 0.680732  [   32/   43]
train() client id: f_00003-11-0 loss: 0.637680  [   32/   43]
train() client id: f_00003-12-0 loss: 0.744757  [   32/   43]
train() client id: f_00004-0-0 loss: 0.856023  [   32/  306]
train() client id: f_00004-0-1 loss: 0.888003  [   64/  306]
train() client id: f_00004-0-2 loss: 0.897002  [   96/  306]
train() client id: f_00004-0-3 loss: 1.000994  [  128/  306]
train() client id: f_00004-0-4 loss: 0.857018  [  160/  306]
train() client id: f_00004-0-5 loss: 0.798455  [  192/  306]
train() client id: f_00004-0-6 loss: 0.911507  [  224/  306]
train() client id: f_00004-0-7 loss: 0.817050  [  256/  306]
train() client id: f_00004-0-8 loss: 0.832215  [  288/  306]
train() client id: f_00004-1-0 loss: 1.010830  [   32/  306]
train() client id: f_00004-1-1 loss: 0.844264  [   64/  306]
train() client id: f_00004-1-2 loss: 1.041172  [   96/  306]
train() client id: f_00004-1-3 loss: 0.832452  [  128/  306]
train() client id: f_00004-1-4 loss: 0.818754  [  160/  306]
train() client id: f_00004-1-5 loss: 0.775640  [  192/  306]
train() client id: f_00004-1-6 loss: 0.851887  [  224/  306]
train() client id: f_00004-1-7 loss: 0.859052  [  256/  306]
train() client id: f_00004-1-8 loss: 0.752682  [  288/  306]
train() client id: f_00004-2-0 loss: 0.887860  [   32/  306]
train() client id: f_00004-2-1 loss: 0.861464  [   64/  306]
train() client id: f_00004-2-2 loss: 0.922586  [   96/  306]
train() client id: f_00004-2-3 loss: 0.855530  [  128/  306]
train() client id: f_00004-2-4 loss: 0.859968  [  160/  306]
train() client id: f_00004-2-5 loss: 0.905370  [  192/  306]
train() client id: f_00004-2-6 loss: 0.800261  [  224/  306]
train() client id: f_00004-2-7 loss: 0.805316  [  256/  306]
train() client id: f_00004-2-8 loss: 0.923210  [  288/  306]
train() client id: f_00004-3-0 loss: 0.856680  [   32/  306]
train() client id: f_00004-3-1 loss: 0.832056  [   64/  306]
train() client id: f_00004-3-2 loss: 0.891163  [   96/  306]
train() client id: f_00004-3-3 loss: 0.936459  [  128/  306]
train() client id: f_00004-3-4 loss: 0.962611  [  160/  306]
train() client id: f_00004-3-5 loss: 0.886777  [  192/  306]
train() client id: f_00004-3-6 loss: 0.728882  [  224/  306]
train() client id: f_00004-3-7 loss: 0.798024  [  256/  306]
train() client id: f_00004-3-8 loss: 0.776585  [  288/  306]
train() client id: f_00004-4-0 loss: 0.897671  [   32/  306]
train() client id: f_00004-4-1 loss: 0.785146  [   64/  306]
train() client id: f_00004-4-2 loss: 0.776119  [   96/  306]
train() client id: f_00004-4-3 loss: 0.834685  [  128/  306]
train() client id: f_00004-4-4 loss: 0.730637  [  160/  306]
train() client id: f_00004-4-5 loss: 0.748734  [  192/  306]
train() client id: f_00004-4-6 loss: 0.959422  [  224/  306]
train() client id: f_00004-4-7 loss: 1.067750  [  256/  306]
train() client id: f_00004-4-8 loss: 0.923544  [  288/  306]
train() client id: f_00004-5-0 loss: 0.879418  [   32/  306]
train() client id: f_00004-5-1 loss: 0.968632  [   64/  306]
train() client id: f_00004-5-2 loss: 0.874252  [   96/  306]
train() client id: f_00004-5-3 loss: 0.757044  [  128/  306]
train() client id: f_00004-5-4 loss: 0.962790  [  160/  306]
train() client id: f_00004-5-5 loss: 0.831756  [  192/  306]
train() client id: f_00004-5-6 loss: 0.766872  [  224/  306]
train() client id: f_00004-5-7 loss: 0.851770  [  256/  306]
train() client id: f_00004-5-8 loss: 0.858492  [  288/  306]
train() client id: f_00004-6-0 loss: 0.871389  [   32/  306]
train() client id: f_00004-6-1 loss: 0.851544  [   64/  306]
train() client id: f_00004-6-2 loss: 0.938369  [   96/  306]
train() client id: f_00004-6-3 loss: 0.769764  [  128/  306]
train() client id: f_00004-6-4 loss: 0.979151  [  160/  306]
train() client id: f_00004-6-5 loss: 0.876320  [  192/  306]
train() client id: f_00004-6-6 loss: 0.751883  [  224/  306]
train() client id: f_00004-6-7 loss: 0.873217  [  256/  306]
train() client id: f_00004-6-8 loss: 0.816139  [  288/  306]
train() client id: f_00004-7-0 loss: 0.780628  [   32/  306]
train() client id: f_00004-7-1 loss: 0.846257  [   64/  306]
train() client id: f_00004-7-2 loss: 0.779863  [   96/  306]
train() client id: f_00004-7-3 loss: 0.888717  [  128/  306]
train() client id: f_00004-7-4 loss: 0.704011  [  160/  306]
train() client id: f_00004-7-5 loss: 0.956420  [  192/  306]
train() client id: f_00004-7-6 loss: 0.891874  [  224/  306]
train() client id: f_00004-7-7 loss: 0.952944  [  256/  306]
train() client id: f_00004-7-8 loss: 0.922444  [  288/  306]
train() client id: f_00004-8-0 loss: 0.753454  [   32/  306]
train() client id: f_00004-8-1 loss: 0.773034  [   64/  306]
train() client id: f_00004-8-2 loss: 0.793124  [   96/  306]
train() client id: f_00004-8-3 loss: 0.831503  [  128/  306]
train() client id: f_00004-8-4 loss: 0.959177  [  160/  306]
train() client id: f_00004-8-5 loss: 0.885350  [  192/  306]
train() client id: f_00004-8-6 loss: 0.924106  [  224/  306]
train() client id: f_00004-8-7 loss: 0.896181  [  256/  306]
train() client id: f_00004-8-8 loss: 0.813651  [  288/  306]
train() client id: f_00004-9-0 loss: 0.945212  [   32/  306]
train() client id: f_00004-9-1 loss: 0.700800  [   64/  306]
train() client id: f_00004-9-2 loss: 0.836174  [   96/  306]
train() client id: f_00004-9-3 loss: 0.956845  [  128/  306]
train() client id: f_00004-9-4 loss: 0.838674  [  160/  306]
train() client id: f_00004-9-5 loss: 0.829418  [  192/  306]
train() client id: f_00004-9-6 loss: 0.933802  [  224/  306]
train() client id: f_00004-9-7 loss: 0.844944  [  256/  306]
train() client id: f_00004-9-8 loss: 0.792703  [  288/  306]
train() client id: f_00004-10-0 loss: 0.883211  [   32/  306]
train() client id: f_00004-10-1 loss: 0.810044  [   64/  306]
train() client id: f_00004-10-2 loss: 0.848652  [   96/  306]
train() client id: f_00004-10-3 loss: 0.770264  [  128/  306]
train() client id: f_00004-10-4 loss: 0.765457  [  160/  306]
train() client id: f_00004-10-5 loss: 0.813449  [  192/  306]
train() client id: f_00004-10-6 loss: 0.903259  [  224/  306]
train() client id: f_00004-10-7 loss: 0.921150  [  256/  306]
train() client id: f_00004-10-8 loss: 1.004874  [  288/  306]
train() client id: f_00004-11-0 loss: 0.845869  [   32/  306]
train() client id: f_00004-11-1 loss: 0.835243  [   64/  306]
train() client id: f_00004-11-2 loss: 0.792623  [   96/  306]
train() client id: f_00004-11-3 loss: 0.936069  [  128/  306]
train() client id: f_00004-11-4 loss: 0.817236  [  160/  306]
train() client id: f_00004-11-5 loss: 0.836064  [  192/  306]
train() client id: f_00004-11-6 loss: 0.903780  [  224/  306]
train() client id: f_00004-11-7 loss: 0.815363  [  256/  306]
train() client id: f_00004-11-8 loss: 0.802918  [  288/  306]
train() client id: f_00004-12-0 loss: 0.850877  [   32/  306]
train() client id: f_00004-12-1 loss: 0.892750  [   64/  306]
train() client id: f_00004-12-2 loss: 0.852802  [   96/  306]
train() client id: f_00004-12-3 loss: 0.859384  [  128/  306]
train() client id: f_00004-12-4 loss: 0.888044  [  160/  306]
train() client id: f_00004-12-5 loss: 0.750996  [  192/  306]
train() client id: f_00004-12-6 loss: 0.836049  [  224/  306]
train() client id: f_00004-12-7 loss: 0.847324  [  256/  306]
train() client id: f_00004-12-8 loss: 0.885696  [  288/  306]
train() client id: f_00005-0-0 loss: 0.382380  [   32/  146]
train() client id: f_00005-0-1 loss: 0.868946  [   64/  146]
train() client id: f_00005-0-2 loss: 0.621422  [   96/  146]
train() client id: f_00005-0-3 loss: 0.420649  [  128/  146]
train() client id: f_00005-1-0 loss: 0.411378  [   32/  146]
train() client id: f_00005-1-1 loss: 0.749869  [   64/  146]
train() client id: f_00005-1-2 loss: 0.732174  [   96/  146]
train() client id: f_00005-1-3 loss: 0.597423  [  128/  146]
train() client id: f_00005-2-0 loss: 0.374023  [   32/  146]
train() client id: f_00005-2-1 loss: 0.686138  [   64/  146]
train() client id: f_00005-2-2 loss: 0.663900  [   96/  146]
train() client id: f_00005-2-3 loss: 0.660317  [  128/  146]
train() client id: f_00005-3-0 loss: 0.545716  [   32/  146]
train() client id: f_00005-3-1 loss: 0.585514  [   64/  146]
train() client id: f_00005-3-2 loss: 0.706175  [   96/  146]
train() client id: f_00005-3-3 loss: 0.498231  [  128/  146]
train() client id: f_00005-4-0 loss: 0.905155  [   32/  146]
train() client id: f_00005-4-1 loss: 0.720777  [   64/  146]
train() client id: f_00005-4-2 loss: 0.492282  [   96/  146]
train() client id: f_00005-4-3 loss: 0.514336  [  128/  146]
train() client id: f_00005-5-0 loss: 0.689875  [   32/  146]
train() client id: f_00005-5-1 loss: 0.585964  [   64/  146]
train() client id: f_00005-5-2 loss: 0.421957  [   96/  146]
train() client id: f_00005-5-3 loss: 0.613462  [  128/  146]
train() client id: f_00005-6-0 loss: 0.364845  [   32/  146]
train() client id: f_00005-6-1 loss: 0.544665  [   64/  146]
train() client id: f_00005-6-2 loss: 0.669988  [   96/  146]
train() client id: f_00005-6-3 loss: 0.643391  [  128/  146]
train() client id: f_00005-7-0 loss: 0.461380  [   32/  146]
train() client id: f_00005-7-1 loss: 0.803859  [   64/  146]
train() client id: f_00005-7-2 loss: 0.502801  [   96/  146]
train() client id: f_00005-7-3 loss: 0.657465  [  128/  146]
train() client id: f_00005-8-0 loss: 0.630355  [   32/  146]
train() client id: f_00005-8-1 loss: 0.644557  [   64/  146]
train() client id: f_00005-8-2 loss: 0.512953  [   96/  146]
train() client id: f_00005-8-3 loss: 0.553888  [  128/  146]
train() client id: f_00005-9-0 loss: 0.608571  [   32/  146]
train() client id: f_00005-9-1 loss: 0.408017  [   64/  146]
train() client id: f_00005-9-2 loss: 0.800799  [   96/  146]
train() client id: f_00005-9-3 loss: 0.774977  [  128/  146]
train() client id: f_00005-10-0 loss: 0.435279  [   32/  146]
train() client id: f_00005-10-1 loss: 0.811445  [   64/  146]
train() client id: f_00005-10-2 loss: 0.717014  [   96/  146]
train() client id: f_00005-10-3 loss: 0.489341  [  128/  146]
train() client id: f_00005-11-0 loss: 0.712389  [   32/  146]
train() client id: f_00005-11-1 loss: 0.544422  [   64/  146]
train() client id: f_00005-11-2 loss: 0.663237  [   96/  146]
train() client id: f_00005-11-3 loss: 0.393791  [  128/  146]
train() client id: f_00005-12-0 loss: 0.490698  [   32/  146]
train() client id: f_00005-12-1 loss: 0.668365  [   64/  146]
train() client id: f_00005-12-2 loss: 0.529138  [   96/  146]
train() client id: f_00005-12-3 loss: 0.663271  [  128/  146]
train() client id: f_00006-0-0 loss: 0.525169  [   32/   54]
train() client id: f_00006-1-0 loss: 0.481982  [   32/   54]
train() client id: f_00006-2-0 loss: 0.514894  [   32/   54]
train() client id: f_00006-3-0 loss: 0.525431  [   32/   54]
train() client id: f_00006-4-0 loss: 0.565652  [   32/   54]
train() client id: f_00006-5-0 loss: 0.576296  [   32/   54]
train() client id: f_00006-6-0 loss: 0.564389  [   32/   54]
train() client id: f_00006-7-0 loss: 0.534238  [   32/   54]
train() client id: f_00006-8-0 loss: 0.508445  [   32/   54]
train() client id: f_00006-9-0 loss: 0.515086  [   32/   54]
train() client id: f_00006-10-0 loss: 0.543793  [   32/   54]
train() client id: f_00006-11-0 loss: 0.566218  [   32/   54]
train() client id: f_00006-12-0 loss: 0.527628  [   32/   54]
train() client id: f_00007-0-0 loss: 0.587021  [   32/  179]
train() client id: f_00007-0-1 loss: 0.468985  [   64/  179]
train() client id: f_00007-0-2 loss: 0.499211  [   96/  179]
train() client id: f_00007-0-3 loss: 0.522240  [  128/  179]
train() client id: f_00007-0-4 loss: 0.399357  [  160/  179]
train() client id: f_00007-1-0 loss: 0.507264  [   32/  179]
train() client id: f_00007-1-1 loss: 0.390275  [   64/  179]
train() client id: f_00007-1-2 loss: 0.349692  [   96/  179]
train() client id: f_00007-1-3 loss: 0.538406  [  128/  179]
train() client id: f_00007-1-4 loss: 0.618664  [  160/  179]
train() client id: f_00007-2-0 loss: 0.486473  [   32/  179]
train() client id: f_00007-2-1 loss: 0.456058  [   64/  179]
train() client id: f_00007-2-2 loss: 0.437709  [   96/  179]
train() client id: f_00007-2-3 loss: 0.422025  [  128/  179]
train() client id: f_00007-2-4 loss: 0.511511  [  160/  179]
train() client id: f_00007-3-0 loss: 0.349658  [   32/  179]
train() client id: f_00007-3-1 loss: 0.301687  [   64/  179]
train() client id: f_00007-3-2 loss: 0.517126  [   96/  179]
train() client id: f_00007-3-3 loss: 0.672967  [  128/  179]
train() client id: f_00007-3-4 loss: 0.497998  [  160/  179]
train() client id: f_00007-4-0 loss: 0.636298  [   32/  179]
train() client id: f_00007-4-1 loss: 0.318656  [   64/  179]
train() client id: f_00007-4-2 loss: 0.501526  [   96/  179]
train() client id: f_00007-4-3 loss: 0.485834  [  128/  179]
train() client id: f_00007-4-4 loss: 0.349485  [  160/  179]
train() client id: f_00007-5-0 loss: 0.468892  [   32/  179]
train() client id: f_00007-5-1 loss: 0.350424  [   64/  179]
train() client id: f_00007-5-2 loss: 0.495576  [   96/  179]
train() client id: f_00007-5-3 loss: 0.415318  [  128/  179]
train() client id: f_00007-5-4 loss: 0.410834  [  160/  179]
train() client id: f_00007-6-0 loss: 0.419057  [   32/  179]
train() client id: f_00007-6-1 loss: 0.375520  [   64/  179]
train() client id: f_00007-6-2 loss: 0.289551  [   96/  179]
train() client id: f_00007-6-3 loss: 0.374720  [  128/  179]
train() client id: f_00007-6-4 loss: 0.595239  [  160/  179]
train() client id: f_00007-7-0 loss: 0.632227  [   32/  179]
train() client id: f_00007-7-1 loss: 0.527461  [   64/  179]
train() client id: f_00007-7-2 loss: 0.345937  [   96/  179]
train() client id: f_00007-7-3 loss: 0.272108  [  128/  179]
train() client id: f_00007-7-4 loss: 0.380930  [  160/  179]
train() client id: f_00007-8-0 loss: 0.462320  [   32/  179]
train() client id: f_00007-8-1 loss: 0.277252  [   64/  179]
train() client id: f_00007-8-2 loss: 0.537531  [   96/  179]
train() client id: f_00007-8-3 loss: 0.338513  [  128/  179]
train() client id: f_00007-8-4 loss: 0.574504  [  160/  179]
train() client id: f_00007-9-0 loss: 0.500609  [   32/  179]
train() client id: f_00007-9-1 loss: 0.270137  [   64/  179]
train() client id: f_00007-9-2 loss: 0.443210  [   96/  179]
train() client id: f_00007-9-3 loss: 0.436321  [  128/  179]
train() client id: f_00007-9-4 loss: 0.454755  [  160/  179]
train() client id: f_00007-10-0 loss: 0.332702  [   32/  179]
train() client id: f_00007-10-1 loss: 0.680589  [   64/  179]
train() client id: f_00007-10-2 loss: 0.443114  [   96/  179]
train() client id: f_00007-10-3 loss: 0.361734  [  128/  179]
train() client id: f_00007-10-4 loss: 0.275916  [  160/  179]
train() client id: f_00007-11-0 loss: 0.501815  [   32/  179]
train() client id: f_00007-11-1 loss: 0.376915  [   64/  179]
train() client id: f_00007-11-2 loss: 0.475009  [   96/  179]
train() client id: f_00007-11-3 loss: 0.287451  [  128/  179]
train() client id: f_00007-11-4 loss: 0.451460  [  160/  179]
train() client id: f_00007-12-0 loss: 0.377248  [   32/  179]
train() client id: f_00007-12-1 loss: 0.344039  [   64/  179]
train() client id: f_00007-12-2 loss: 0.530550  [   96/  179]
train() client id: f_00007-12-3 loss: 0.412695  [  128/  179]
train() client id: f_00007-12-4 loss: 0.325257  [  160/  179]
train() client id: f_00008-0-0 loss: 0.766501  [   32/  130]
train() client id: f_00008-0-1 loss: 0.633291  [   64/  130]
train() client id: f_00008-0-2 loss: 0.778263  [   96/  130]
train() client id: f_00008-0-3 loss: 0.760851  [  128/  130]
train() client id: f_00008-1-0 loss: 0.737495  [   32/  130]
train() client id: f_00008-1-1 loss: 0.679530  [   64/  130]
train() client id: f_00008-1-2 loss: 0.747981  [   96/  130]
train() client id: f_00008-1-3 loss: 0.772152  [  128/  130]
train() client id: f_00008-2-0 loss: 0.660208  [   32/  130]
train() client id: f_00008-2-1 loss: 0.845763  [   64/  130]
train() client id: f_00008-2-2 loss: 0.695924  [   96/  130]
train() client id: f_00008-2-3 loss: 0.736530  [  128/  130]
train() client id: f_00008-3-0 loss: 0.690089  [   32/  130]
train() client id: f_00008-3-1 loss: 0.700219  [   64/  130]
train() client id: f_00008-3-2 loss: 0.669993  [   96/  130]
train() client id: f_00008-3-3 loss: 0.914053  [  128/  130]
train() client id: f_00008-4-0 loss: 0.794014  [   32/  130]
train() client id: f_00008-4-1 loss: 0.713558  [   64/  130]
train() client id: f_00008-4-2 loss: 0.798321  [   96/  130]
train() client id: f_00008-4-3 loss: 0.646381  [  128/  130]
train() client id: f_00008-5-0 loss: 0.723973  [   32/  130]
train() client id: f_00008-5-1 loss: 0.752468  [   64/  130]
train() client id: f_00008-5-2 loss: 0.697981  [   96/  130]
train() client id: f_00008-5-3 loss: 0.753213  [  128/  130]
train() client id: f_00008-6-0 loss: 0.656018  [   32/  130]
train() client id: f_00008-6-1 loss: 0.731962  [   64/  130]
train() client id: f_00008-6-2 loss: 0.750818  [   96/  130]
train() client id: f_00008-6-3 loss: 0.835272  [  128/  130]
train() client id: f_00008-7-0 loss: 0.584298  [   32/  130]
train() client id: f_00008-7-1 loss: 0.907574  [   64/  130]
train() client id: f_00008-7-2 loss: 0.822228  [   96/  130]
train() client id: f_00008-7-3 loss: 0.653311  [  128/  130]
train() client id: f_00008-8-0 loss: 0.712060  [   32/  130]
train() client id: f_00008-8-1 loss: 0.732318  [   64/  130]
train() client id: f_00008-8-2 loss: 0.833853  [   96/  130]
train() client id: f_00008-8-3 loss: 0.660057  [  128/  130]
train() client id: f_00008-9-0 loss: 0.661686  [   32/  130]
train() client id: f_00008-9-1 loss: 0.858158  [   64/  130]
train() client id: f_00008-9-2 loss: 0.670114  [   96/  130]
train() client id: f_00008-9-3 loss: 0.768767  [  128/  130]
train() client id: f_00008-10-0 loss: 0.719086  [   32/  130]
train() client id: f_00008-10-1 loss: 0.799128  [   64/  130]
train() client id: f_00008-10-2 loss: 0.703936  [   96/  130]
train() client id: f_00008-10-3 loss: 0.751363  [  128/  130]
train() client id: f_00008-11-0 loss: 0.771507  [   32/  130]
train() client id: f_00008-11-1 loss: 0.636823  [   64/  130]
train() client id: f_00008-11-2 loss: 0.804871  [   96/  130]
train() client id: f_00008-11-3 loss: 0.752984  [  128/  130]
train() client id: f_00008-12-0 loss: 0.766593  [   32/  130]
train() client id: f_00008-12-1 loss: 0.762851  [   64/  130]
train() client id: f_00008-12-2 loss: 0.695418  [   96/  130]
train() client id: f_00008-12-3 loss: 0.754256  [  128/  130]
train() client id: f_00009-0-0 loss: 1.090202  [   32/  118]
train() client id: f_00009-0-1 loss: 1.222405  [   64/  118]
train() client id: f_00009-0-2 loss: 1.083066  [   96/  118]
train() client id: f_00009-1-0 loss: 1.216101  [   32/  118]
train() client id: f_00009-1-1 loss: 1.048905  [   64/  118]
train() client id: f_00009-1-2 loss: 1.026663  [   96/  118]
train() client id: f_00009-2-0 loss: 1.059637  [   32/  118]
train() client id: f_00009-2-1 loss: 1.064059  [   64/  118]
train() client id: f_00009-2-2 loss: 1.056559  [   96/  118]
train() client id: f_00009-3-0 loss: 0.919526  [   32/  118]
train() client id: f_00009-3-1 loss: 1.068039  [   64/  118]
train() client id: f_00009-3-2 loss: 1.106925  [   96/  118]
train() client id: f_00009-4-0 loss: 1.012158  [   32/  118]
train() client id: f_00009-4-1 loss: 0.897862  [   64/  118]
train() client id: f_00009-4-2 loss: 1.028407  [   96/  118]
train() client id: f_00009-5-0 loss: 0.913226  [   32/  118]
train() client id: f_00009-5-1 loss: 0.883495  [   64/  118]
train() client id: f_00009-5-2 loss: 1.041802  [   96/  118]
train() client id: f_00009-6-0 loss: 1.037474  [   32/  118]
train() client id: f_00009-6-1 loss: 0.848863  [   64/  118]
train() client id: f_00009-6-2 loss: 0.963496  [   96/  118]
train() client id: f_00009-7-0 loss: 0.914751  [   32/  118]
train() client id: f_00009-7-1 loss: 0.899239  [   64/  118]
train() client id: f_00009-7-2 loss: 0.983365  [   96/  118]
train() client id: f_00009-8-0 loss: 1.040291  [   32/  118]
train() client id: f_00009-8-1 loss: 0.886861  [   64/  118]
train() client id: f_00009-8-2 loss: 0.878650  [   96/  118]
train() client id: f_00009-9-0 loss: 0.866230  [   32/  118]
train() client id: f_00009-9-1 loss: 0.876661  [   64/  118]
train() client id: f_00009-9-2 loss: 0.921945  [   96/  118]
train() client id: f_00009-10-0 loss: 0.922312  [   32/  118]
train() client id: f_00009-10-1 loss: 0.867415  [   64/  118]
train() client id: f_00009-10-2 loss: 0.951638  [   96/  118]
train() client id: f_00009-11-0 loss: 1.025579  [   32/  118]
train() client id: f_00009-11-1 loss: 0.691282  [   64/  118]
train() client id: f_00009-11-2 loss: 0.977846  [   96/  118]
train() client id: f_00009-12-0 loss: 0.905427  [   32/  118]
train() client id: f_00009-12-1 loss: 0.747502  [   64/  118]
train() client id: f_00009-12-2 loss: 0.968602  [   96/  118]
At round 40 accuracy: 0.6525198938992043
At round 40 training accuracy: 0.5915492957746479
At round 40 training loss: 0.8276266643114133
update_location
xs = [  -3.9056584     4.20031788  220.00902392   18.81129433    0.97929623
    3.95640986 -182.44319194 -161.32485185  204.66397685 -147.06087855]
ys = [ 212.5879595   195.55583871    1.32061395 -182.45517586  174.35018685
  157.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [234.96573089 219.68097033 241.67274283 208.9108805  200.99489216
 186.87150493 208.06827924 189.80617503 228.46446856 177.88455205]
dists_bs = [175.17456558 180.3754319  431.50510786 406.61655224 175.98049596
 179.77972646 177.78086556 174.71366329 411.01004024 173.26594513]
uav_gains = [9.82218151e-12 1.26815403e-11 8.69804006e-12 1.49604850e-11
 1.68083716e-11 2.06049621e-11 1.51494992e-11 1.97523587e-11
 1.09881270e-11 2.34789484e-11]
bs_gains = [5.77530797e-11 5.32105086e-11 4.62736173e-12 5.46482092e-12
 5.70155582e-11 5.37056630e-11 5.54135584e-11 5.81806876e-11
 5.30282480e-12 5.95521015e-11]
Round 41
-------------------------------
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.58035445 11.51525536  5.49993513  1.98836081 13.27975002  6.38965832
  2.46168242  7.8365251   5.78633913  5.1816046 ]
obj_prev = 65.51946532847218
eta_min = 2.9576516171485466e-17	eta_max = 0.9329613748543103
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 15.185923913957124	eta = 0.9090909090909091
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 28.643068262013102	eta = 0.4819799767971639
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 21.937413464592197	eta = 0.6293077986886223
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.723057688485657	eta = 0.6661847678972268
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65725512436301	eta = 0.668306863289048
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65704485277882	eta = 0.6683136660986402
eta = 0.6683136660986402
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.0332544  0.06993981 0.03272655 0.01134872 0.08076069 0.03853288
 0.01425189 0.04724236 0.0343101  0.03114301]
ene_total = [1.87549698 3.28425833 1.87007596 0.89063245 3.74131718 1.94496118
 1.01302102 2.39242243 2.02108855 1.62377077]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 0 obj = 3.892935261137077
eta = 0.6683136660986402
freqs = [30327342.94858874 59817206.41115162 30065677.93401769 10128218.84644976
 68956163.97393337 32948387.69791197 12711921.71227483 41669829.72547625
 32697548.88052001 26563493.52335148]
eta_min = 0.6683136660986426	eta_max = 0.6683136660986362
af = 0.007721858371378171	bf = 1.2881215340611454	zeta = 0.008494044208515988	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [1.80840054e-06 1.47963121e-05 1.74911774e-06 6.88319331e-08
 2.27050766e-05 2.47329858e-06 1.36166896e-07 4.85011281e-06
 2.16884794e-06 1.29929494e-06]
ene_total = [1.67138562 1.10800963 1.73368868 1.48456874 1.09397392 1.1040216
 1.47961889 1.38247263 2.03845756 1.08123529]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 1 obj = 3.8929352611370294
eta = 0.6683136660986362
freqs = [30327342.94858875 59817206.41115168 30065677.9340177  10128218.84644976
 68956163.97393346 32948387.697912   12711921.71227484 41669829.72547627
 32697548.88052    26563493.52335151]
Done!
At round 41 energy consumption: 3.892935261137077
At round 41 eta: 0.6683136660986362
At round 41 local rounds: 13.19619071883817
At round 41 global rounds: 42.625280831488105
At round 41 a_n: 13.79567728354305
gradient difference: 0.37385934591293335
train() client id: f_00000-0-0 loss: 1.402493  [   32/  126]
train() client id: f_00000-0-1 loss: 1.030796  [   64/  126]
train() client id: f_00000-0-2 loss: 1.129149  [   96/  126]
train() client id: f_00000-1-0 loss: 0.996496  [   32/  126]
train() client id: f_00000-1-1 loss: 1.238516  [   64/  126]
train() client id: f_00000-1-2 loss: 1.086468  [   96/  126]
train() client id: f_00000-2-0 loss: 1.131969  [   32/  126]
train() client id: f_00000-2-1 loss: 0.932369  [   64/  126]
train() client id: f_00000-2-2 loss: 1.127310  [   96/  126]
train() client id: f_00000-3-0 loss: 0.945304  [   32/  126]
train() client id: f_00000-3-1 loss: 0.885694  [   64/  126]
train() client id: f_00000-3-2 loss: 1.047614  [   96/  126]
train() client id: f_00000-4-0 loss: 0.933019  [   32/  126]
train() client id: f_00000-4-1 loss: 0.902249  [   64/  126]
train() client id: f_00000-4-2 loss: 1.033739  [   96/  126]
train() client id: f_00000-5-0 loss: 0.948896  [   32/  126]
train() client id: f_00000-5-1 loss: 0.926202  [   64/  126]
train() client id: f_00000-5-2 loss: 0.924764  [   96/  126]
train() client id: f_00000-6-0 loss: 0.938373  [   32/  126]
train() client id: f_00000-6-1 loss: 0.769868  [   64/  126]
train() client id: f_00000-6-2 loss: 0.894419  [   96/  126]
train() client id: f_00000-7-0 loss: 0.993433  [   32/  126]
train() client id: f_00000-7-1 loss: 0.778337  [   64/  126]
train() client id: f_00000-7-2 loss: 0.864960  [   96/  126]
train() client id: f_00000-8-0 loss: 0.806265  [   32/  126]
train() client id: f_00000-8-1 loss: 0.951116  [   64/  126]
train() client id: f_00000-8-2 loss: 0.939855  [   96/  126]
train() client id: f_00000-9-0 loss: 0.887120  [   32/  126]
train() client id: f_00000-9-1 loss: 0.811614  [   64/  126]
train() client id: f_00000-9-2 loss: 0.902061  [   96/  126]
train() client id: f_00000-10-0 loss: 0.794653  [   32/  126]
train() client id: f_00000-10-1 loss: 1.001044  [   64/  126]
train() client id: f_00000-10-2 loss: 0.795746  [   96/  126]
train() client id: f_00000-11-0 loss: 0.855646  [   32/  126]
train() client id: f_00000-11-1 loss: 0.767829  [   64/  126]
train() client id: f_00000-11-2 loss: 0.960052  [   96/  126]
train() client id: f_00000-12-0 loss: 0.962519  [   32/  126]
train() client id: f_00000-12-1 loss: 0.785750  [   64/  126]
train() client id: f_00000-12-2 loss: 0.842515  [   96/  126]
train() client id: f_00001-0-0 loss: 0.509894  [   32/  265]
train() client id: f_00001-0-1 loss: 0.431412  [   64/  265]
train() client id: f_00001-0-2 loss: 0.521863  [   96/  265]
train() client id: f_00001-0-3 loss: 0.610020  [  128/  265]
train() client id: f_00001-0-4 loss: 0.614854  [  160/  265]
train() client id: f_00001-0-5 loss: 0.545073  [  192/  265]
train() client id: f_00001-0-6 loss: 0.441600  [  224/  265]
train() client id: f_00001-0-7 loss: 0.436276  [  256/  265]
train() client id: f_00001-1-0 loss: 0.522408  [   32/  265]
train() client id: f_00001-1-1 loss: 0.547711  [   64/  265]
train() client id: f_00001-1-2 loss: 0.531192  [   96/  265]
train() client id: f_00001-1-3 loss: 0.414540  [  128/  265]
train() client id: f_00001-1-4 loss: 0.481046  [  160/  265]
train() client id: f_00001-1-5 loss: 0.491246  [  192/  265]
train() client id: f_00001-1-6 loss: 0.621209  [  224/  265]
train() client id: f_00001-1-7 loss: 0.438008  [  256/  265]
train() client id: f_00001-2-0 loss: 0.539981  [   32/  265]
train() client id: f_00001-2-1 loss: 0.462473  [   64/  265]
train() client id: f_00001-2-2 loss: 0.552848  [   96/  265]
train() client id: f_00001-2-3 loss: 0.554503  [  128/  265]
train() client id: f_00001-2-4 loss: 0.395980  [  160/  265]
train() client id: f_00001-2-5 loss: 0.492215  [  192/  265]
train() client id: f_00001-2-6 loss: 0.481528  [  224/  265]
train() client id: f_00001-2-7 loss: 0.536150  [  256/  265]
train() client id: f_00001-3-0 loss: 0.476173  [   32/  265]
train() client id: f_00001-3-1 loss: 0.415583  [   64/  265]
train() client id: f_00001-3-2 loss: 0.479090  [   96/  265]
train() client id: f_00001-3-3 loss: 0.478106  [  128/  265]
train() client id: f_00001-3-4 loss: 0.620090  [  160/  265]
train() client id: f_00001-3-5 loss: 0.430220  [  192/  265]
train() client id: f_00001-3-6 loss: 0.525384  [  224/  265]
train() client id: f_00001-3-7 loss: 0.560663  [  256/  265]
train() client id: f_00001-4-0 loss: 0.605729  [   32/  265]
train() client id: f_00001-4-1 loss: 0.569209  [   64/  265]
train() client id: f_00001-4-2 loss: 0.425379  [   96/  265]
train() client id: f_00001-4-3 loss: 0.462193  [  128/  265]
train() client id: f_00001-4-4 loss: 0.539541  [  160/  265]
train() client id: f_00001-4-5 loss: 0.410092  [  192/  265]
train() client id: f_00001-4-6 loss: 0.541522  [  224/  265]
train() client id: f_00001-4-7 loss: 0.403616  [  256/  265]
train() client id: f_00001-5-0 loss: 0.480108  [   32/  265]
train() client id: f_00001-5-1 loss: 0.583989  [   64/  265]
train() client id: f_00001-5-2 loss: 0.423764  [   96/  265]
train() client id: f_00001-5-3 loss: 0.453559  [  128/  265]
train() client id: f_00001-5-4 loss: 0.608625  [  160/  265]
train() client id: f_00001-5-5 loss: 0.418601  [  192/  265]
train() client id: f_00001-5-6 loss: 0.472740  [  224/  265]
train() client id: f_00001-5-7 loss: 0.501382  [  256/  265]
train() client id: f_00001-6-0 loss: 0.602896  [   32/  265]
train() client id: f_00001-6-1 loss: 0.535068  [   64/  265]
train() client id: f_00001-6-2 loss: 0.491827  [   96/  265]
train() client id: f_00001-6-3 loss: 0.404162  [  128/  265]
train() client id: f_00001-6-4 loss: 0.436779  [  160/  265]
train() client id: f_00001-6-5 loss: 0.482082  [  192/  265]
train() client id: f_00001-6-6 loss: 0.415317  [  224/  265]
train() client id: f_00001-6-7 loss: 0.569214  [  256/  265]
train() client id: f_00001-7-0 loss: 0.521272  [   32/  265]
train() client id: f_00001-7-1 loss: 0.458673  [   64/  265]
train() client id: f_00001-7-2 loss: 0.512742  [   96/  265]
train() client id: f_00001-7-3 loss: 0.480784  [  128/  265]
train() client id: f_00001-7-4 loss: 0.575991  [  160/  265]
train() client id: f_00001-7-5 loss: 0.540105  [  192/  265]
train() client id: f_00001-7-6 loss: 0.447998  [  224/  265]
train() client id: f_00001-7-7 loss: 0.402879  [  256/  265]
train() client id: f_00001-8-0 loss: 0.524037  [   32/  265]
train() client id: f_00001-8-1 loss: 0.617691  [   64/  265]
train() client id: f_00001-8-2 loss: 0.375108  [   96/  265]
train() client id: f_00001-8-3 loss: 0.488239  [  128/  265]
train() client id: f_00001-8-4 loss: 0.501990  [  160/  265]
train() client id: f_00001-8-5 loss: 0.406908  [  192/  265]
train() client id: f_00001-8-6 loss: 0.484481  [  224/  265]
train() client id: f_00001-8-7 loss: 0.465188  [  256/  265]
train() client id: f_00001-9-0 loss: 0.459570  [   32/  265]
train() client id: f_00001-9-1 loss: 0.503120  [   64/  265]
train() client id: f_00001-9-2 loss: 0.450768  [   96/  265]
train() client id: f_00001-9-3 loss: 0.491320  [  128/  265]
train() client id: f_00001-9-4 loss: 0.481442  [  160/  265]
train() client id: f_00001-9-5 loss: 0.488487  [  192/  265]
train() client id: f_00001-9-6 loss: 0.515489  [  224/  265]
train() client id: f_00001-9-7 loss: 0.541055  [  256/  265]
train() client id: f_00001-10-0 loss: 0.471614  [   32/  265]
train() client id: f_00001-10-1 loss: 0.572707  [   64/  265]
train() client id: f_00001-10-2 loss: 0.554381  [   96/  265]
train() client id: f_00001-10-3 loss: 0.399079  [  128/  265]
train() client id: f_00001-10-4 loss: 0.398435  [  160/  265]
train() client id: f_00001-10-5 loss: 0.427234  [  192/  265]
train() client id: f_00001-10-6 loss: 0.627997  [  224/  265]
train() client id: f_00001-10-7 loss: 0.482557  [  256/  265]
train() client id: f_00001-11-0 loss: 0.435067  [   32/  265]
train() client id: f_00001-11-1 loss: 0.582784  [   64/  265]
train() client id: f_00001-11-2 loss: 0.398411  [   96/  265]
train() client id: f_00001-11-3 loss: 0.452175  [  128/  265]
train() client id: f_00001-11-4 loss: 0.572339  [  160/  265]
train() client id: f_00001-11-5 loss: 0.478450  [  192/  265]
train() client id: f_00001-11-6 loss: 0.606640  [  224/  265]
train() client id: f_00001-11-7 loss: 0.410352  [  256/  265]
train() client id: f_00001-12-0 loss: 0.402686  [   32/  265]
train() client id: f_00001-12-1 loss: 0.426070  [   64/  265]
train() client id: f_00001-12-2 loss: 0.493547  [   96/  265]
train() client id: f_00001-12-3 loss: 0.625369  [  128/  265]
train() client id: f_00001-12-4 loss: 0.395420  [  160/  265]
train() client id: f_00001-12-5 loss: 0.604481  [  192/  265]
train() client id: f_00001-12-6 loss: 0.496657  [  224/  265]
train() client id: f_00001-12-7 loss: 0.504058  [  256/  265]
train() client id: f_00002-0-0 loss: 1.246603  [   32/  124]
train() client id: f_00002-0-1 loss: 1.312236  [   64/  124]
train() client id: f_00002-0-2 loss: 1.242297  [   96/  124]
train() client id: f_00002-1-0 loss: 1.380882  [   32/  124]
train() client id: f_00002-1-1 loss: 1.114276  [   64/  124]
train() client id: f_00002-1-2 loss: 1.104879  [   96/  124]
train() client id: f_00002-2-0 loss: 1.212362  [   32/  124]
train() client id: f_00002-2-1 loss: 1.118713  [   64/  124]
train() client id: f_00002-2-2 loss: 1.305004  [   96/  124]
train() client id: f_00002-3-0 loss: 1.121843  [   32/  124]
train() client id: f_00002-3-1 loss: 1.164860  [   64/  124]
train() client id: f_00002-3-2 loss: 1.165555  [   96/  124]
train() client id: f_00002-4-0 loss: 1.092621  [   32/  124]
train() client id: f_00002-4-1 loss: 1.172019  [   64/  124]
train() client id: f_00002-4-2 loss: 1.127154  [   96/  124]
train() client id: f_00002-5-0 loss: 1.264230  [   32/  124]
train() client id: f_00002-5-1 loss: 1.166227  [   64/  124]
train() client id: f_00002-5-2 loss: 1.007672  [   96/  124]
train() client id: f_00002-6-0 loss: 0.936942  [   32/  124]
train() client id: f_00002-6-1 loss: 1.242326  [   64/  124]
train() client id: f_00002-6-2 loss: 1.014577  [   96/  124]
train() client id: f_00002-7-0 loss: 1.130510  [   32/  124]
train() client id: f_00002-7-1 loss: 1.175558  [   64/  124]
train() client id: f_00002-7-2 loss: 1.005301  [   96/  124]
train() client id: f_00002-8-0 loss: 1.027756  [   32/  124]
train() client id: f_00002-8-1 loss: 1.097871  [   64/  124]
train() client id: f_00002-8-2 loss: 0.995066  [   96/  124]
train() client id: f_00002-9-0 loss: 0.872098  [   32/  124]
train() client id: f_00002-9-1 loss: 1.029065  [   64/  124]
train() client id: f_00002-9-2 loss: 1.056866  [   96/  124]
train() client id: f_00002-10-0 loss: 0.903046  [   32/  124]
train() client id: f_00002-10-1 loss: 0.970113  [   64/  124]
train() client id: f_00002-10-2 loss: 1.143605  [   96/  124]
train() client id: f_00002-11-0 loss: 1.068133  [   32/  124]
train() client id: f_00002-11-1 loss: 1.021003  [   64/  124]
train() client id: f_00002-11-2 loss: 1.018638  [   96/  124]
train() client id: f_00002-12-0 loss: 1.069750  [   32/  124]
train() client id: f_00002-12-1 loss: 1.022822  [   64/  124]
train() client id: f_00002-12-2 loss: 1.079870  [   96/  124]
train() client id: f_00003-0-0 loss: 0.565293  [   32/   43]
train() client id: f_00003-1-0 loss: 0.470818  [   32/   43]
train() client id: f_00003-2-0 loss: 0.745525  [   32/   43]
train() client id: f_00003-3-0 loss: 0.540879  [   32/   43]
train() client id: f_00003-4-0 loss: 0.639596  [   32/   43]
train() client id: f_00003-5-0 loss: 0.701224  [   32/   43]
train() client id: f_00003-6-0 loss: 0.624828  [   32/   43]
train() client id: f_00003-7-0 loss: 0.768240  [   32/   43]
train() client id: f_00003-8-0 loss: 0.625865  [   32/   43]
train() client id: f_00003-9-0 loss: 0.850723  [   32/   43]
train() client id: f_00003-10-0 loss: 0.608900  [   32/   43]
train() client id: f_00003-11-0 loss: 0.562650  [   32/   43]
train() client id: f_00003-12-0 loss: 0.617219  [   32/   43]
train() client id: f_00004-0-0 loss: 0.876802  [   32/  306]
train() client id: f_00004-0-1 loss: 0.720818  [   64/  306]
train() client id: f_00004-0-2 loss: 0.859857  [   96/  306]
train() client id: f_00004-0-3 loss: 0.571592  [  128/  306]
train() client id: f_00004-0-4 loss: 0.765518  [  160/  306]
train() client id: f_00004-0-5 loss: 0.851187  [  192/  306]
train() client id: f_00004-0-6 loss: 0.779655  [  224/  306]
train() client id: f_00004-0-7 loss: 0.864096  [  256/  306]
train() client id: f_00004-0-8 loss: 0.812178  [  288/  306]
train() client id: f_00004-1-0 loss: 0.725609  [   32/  306]
train() client id: f_00004-1-1 loss: 0.975555  [   64/  306]
train() client id: f_00004-1-2 loss: 0.879053  [   96/  306]
train() client id: f_00004-1-3 loss: 0.617022  [  128/  306]
train() client id: f_00004-1-4 loss: 0.728214  [  160/  306]
train() client id: f_00004-1-5 loss: 0.736250  [  192/  306]
train() client id: f_00004-1-6 loss: 0.776718  [  224/  306]
train() client id: f_00004-1-7 loss: 0.822153  [  256/  306]
train() client id: f_00004-1-8 loss: 0.756539  [  288/  306]
train() client id: f_00004-2-0 loss: 0.853055  [   32/  306]
train() client id: f_00004-2-1 loss: 0.711447  [   64/  306]
train() client id: f_00004-2-2 loss: 0.628049  [   96/  306]
train() client id: f_00004-2-3 loss: 0.755262  [  128/  306]
train() client id: f_00004-2-4 loss: 0.806384  [  160/  306]
train() client id: f_00004-2-5 loss: 0.839502  [  192/  306]
train() client id: f_00004-2-6 loss: 0.753226  [  224/  306]
train() client id: f_00004-2-7 loss: 0.759794  [  256/  306]
train() client id: f_00004-2-8 loss: 0.856278  [  288/  306]
train() client id: f_00004-3-0 loss: 0.909807  [   32/  306]
train() client id: f_00004-3-1 loss: 0.913947  [   64/  306]
train() client id: f_00004-3-2 loss: 0.761589  [   96/  306]
train() client id: f_00004-3-3 loss: 0.604747  [  128/  306]
train() client id: f_00004-3-4 loss: 0.741013  [  160/  306]
train() client id: f_00004-3-5 loss: 0.788115  [  192/  306]
train() client id: f_00004-3-6 loss: 0.816899  [  224/  306]
train() client id: f_00004-3-7 loss: 0.867279  [  256/  306]
train() client id: f_00004-3-8 loss: 0.608648  [  288/  306]
train() client id: f_00004-4-0 loss: 0.724455  [   32/  306]
train() client id: f_00004-4-1 loss: 0.840031  [   64/  306]
train() client id: f_00004-4-2 loss: 0.857455  [   96/  306]
train() client id: f_00004-4-3 loss: 0.776725  [  128/  306]
train() client id: f_00004-4-4 loss: 0.787422  [  160/  306]
train() client id: f_00004-4-5 loss: 0.722437  [  192/  306]
train() client id: f_00004-4-6 loss: 0.645520  [  224/  306]
train() client id: f_00004-4-7 loss: 0.858297  [  256/  306]
train() client id: f_00004-4-8 loss: 0.775216  [  288/  306]
train() client id: f_00004-5-0 loss: 0.873317  [   32/  306]
train() client id: f_00004-5-1 loss: 0.776617  [   64/  306]
train() client id: f_00004-5-2 loss: 0.677670  [   96/  306]
train() client id: f_00004-5-3 loss: 0.692850  [  128/  306]
train() client id: f_00004-5-4 loss: 0.769843  [  160/  306]
train() client id: f_00004-5-5 loss: 0.751508  [  192/  306]
train() client id: f_00004-5-6 loss: 0.752287  [  224/  306]
train() client id: f_00004-5-7 loss: 0.841583  [  256/  306]
train() client id: f_00004-5-8 loss: 0.731943  [  288/  306]
train() client id: f_00004-6-0 loss: 0.827579  [   32/  306]
train() client id: f_00004-6-1 loss: 0.844759  [   64/  306]
train() client id: f_00004-6-2 loss: 0.758206  [   96/  306]
train() client id: f_00004-6-3 loss: 0.753067  [  128/  306]
train() client id: f_00004-6-4 loss: 0.694915  [  160/  306]
train() client id: f_00004-6-5 loss: 0.700479  [  192/  306]
train() client id: f_00004-6-6 loss: 0.797180  [  224/  306]
train() client id: f_00004-6-7 loss: 0.700467  [  256/  306]
train() client id: f_00004-6-8 loss: 0.832727  [  288/  306]
train() client id: f_00004-7-0 loss: 0.872912  [   32/  306]
train() client id: f_00004-7-1 loss: 0.794464  [   64/  306]
train() client id: f_00004-7-2 loss: 0.898243  [   96/  306]
train() client id: f_00004-7-3 loss: 0.665929  [  128/  306]
train() client id: f_00004-7-4 loss: 0.725112  [  160/  306]
train() client id: f_00004-7-5 loss: 0.593830  [  192/  306]
train() client id: f_00004-7-6 loss: 0.702568  [  224/  306]
train() client id: f_00004-7-7 loss: 0.788122  [  256/  306]
train() client id: f_00004-7-8 loss: 0.862711  [  288/  306]
train() client id: f_00004-8-0 loss: 0.780870  [   32/  306]
train() client id: f_00004-8-1 loss: 0.784846  [   64/  306]
train() client id: f_00004-8-2 loss: 0.871660  [   96/  306]
train() client id: f_00004-8-3 loss: 0.744836  [  128/  306]
train() client id: f_00004-8-4 loss: 0.762962  [  160/  306]
train() client id: f_00004-8-5 loss: 0.679671  [  192/  306]
train() client id: f_00004-8-6 loss: 0.729908  [  224/  306]
train() client id: f_00004-8-7 loss: 0.694041  [  256/  306]
train() client id: f_00004-8-8 loss: 0.735717  [  288/  306]
train() client id: f_00004-9-0 loss: 0.806420  [   32/  306]
train() client id: f_00004-9-1 loss: 0.663012  [   64/  306]
train() client id: f_00004-9-2 loss: 0.782699  [   96/  306]
train() client id: f_00004-9-3 loss: 0.779188  [  128/  306]
train() client id: f_00004-9-4 loss: 0.792607  [  160/  306]
train() client id: f_00004-9-5 loss: 0.798863  [  192/  306]
train() client id: f_00004-9-6 loss: 0.730340  [  224/  306]
train() client id: f_00004-9-7 loss: 0.791580  [  256/  306]
train() client id: f_00004-9-8 loss: 0.707016  [  288/  306]
train() client id: f_00004-10-0 loss: 0.778011  [   32/  306]
train() client id: f_00004-10-1 loss: 0.687547  [   64/  306]
train() client id: f_00004-10-2 loss: 0.650233  [   96/  306]
train() client id: f_00004-10-3 loss: 0.743020  [  128/  306]
train() client id: f_00004-10-4 loss: 0.866607  [  160/  306]
train() client id: f_00004-10-5 loss: 0.756376  [  192/  306]
train() client id: f_00004-10-6 loss: 0.752447  [  224/  306]
train() client id: f_00004-10-7 loss: 0.979766  [  256/  306]
train() client id: f_00004-10-8 loss: 0.698496  [  288/  306]
train() client id: f_00004-11-0 loss: 0.780727  [   32/  306]
train() client id: f_00004-11-1 loss: 0.784382  [   64/  306]
train() client id: f_00004-11-2 loss: 0.653803  [   96/  306]
train() client id: f_00004-11-3 loss: 0.727715  [  128/  306]
train() client id: f_00004-11-4 loss: 0.643104  [  160/  306]
train() client id: f_00004-11-5 loss: 0.796694  [  192/  306]
train() client id: f_00004-11-6 loss: 0.937505  [  224/  306]
train() client id: f_00004-11-7 loss: 0.850810  [  256/  306]
train() client id: f_00004-11-8 loss: 0.754138  [  288/  306]
train() client id: f_00004-12-0 loss: 0.745450  [   32/  306]
train() client id: f_00004-12-1 loss: 0.662790  [   64/  306]
train() client id: f_00004-12-2 loss: 0.703305  [   96/  306]
train() client id: f_00004-12-3 loss: 0.786069  [  128/  306]
train() client id: f_00004-12-4 loss: 0.843297  [  160/  306]
train() client id: f_00004-12-5 loss: 0.810729  [  192/  306]
train() client id: f_00004-12-6 loss: 0.682927  [  224/  306]
train() client id: f_00004-12-7 loss: 0.859711  [  256/  306]
train() client id: f_00004-12-8 loss: 0.697812  [  288/  306]
train() client id: f_00005-0-0 loss: 0.560727  [   32/  146]
train() client id: f_00005-0-1 loss: 0.445846  [   64/  146]
train() client id: f_00005-0-2 loss: 0.457830  [   96/  146]
train() client id: f_00005-0-3 loss: 0.698864  [  128/  146]
train() client id: f_00005-1-0 loss: 0.686881  [   32/  146]
train() client id: f_00005-1-1 loss: 0.493217  [   64/  146]
train() client id: f_00005-1-2 loss: 0.442351  [   96/  146]
train() client id: f_00005-1-3 loss: 0.598225  [  128/  146]
train() client id: f_00005-2-0 loss: 0.343201  [   32/  146]
train() client id: f_00005-2-1 loss: 0.711588  [   64/  146]
train() client id: f_00005-2-2 loss: 0.553017  [   96/  146]
train() client id: f_00005-2-3 loss: 0.738093  [  128/  146]
train() client id: f_00005-3-0 loss: 0.450534  [   32/  146]
train() client id: f_00005-3-1 loss: 0.750837  [   64/  146]
train() client id: f_00005-3-2 loss: 0.410132  [   96/  146]
train() client id: f_00005-3-3 loss: 0.526002  [  128/  146]
train() client id: f_00005-4-0 loss: 0.879621  [   32/  146]
train() client id: f_00005-4-1 loss: 0.503169  [   64/  146]
train() client id: f_00005-4-2 loss: 0.587483  [   96/  146]
train() client id: f_00005-4-3 loss: 0.394687  [  128/  146]
train() client id: f_00005-5-0 loss: 0.549837  [   32/  146]
train() client id: f_00005-5-1 loss: 0.579166  [   64/  146]
train() client id: f_00005-5-2 loss: 0.458683  [   96/  146]
train() client id: f_00005-5-3 loss: 0.547452  [  128/  146]
train() client id: f_00005-6-0 loss: 0.664503  [   32/  146]
train() client id: f_00005-6-1 loss: 0.789923  [   64/  146]
train() client id: f_00005-6-2 loss: 0.374494  [   96/  146]
train() client id: f_00005-6-3 loss: 0.271693  [  128/  146]
train() client id: f_00005-7-0 loss: 0.611010  [   32/  146]
train() client id: f_00005-7-1 loss: 0.521995  [   64/  146]
train() client id: f_00005-7-2 loss: 0.379645  [   96/  146]
train() client id: f_00005-7-3 loss: 0.381210  [  128/  146]
train() client id: f_00005-8-0 loss: 0.655056  [   32/  146]
train() client id: f_00005-8-1 loss: 0.582233  [   64/  146]
train() client id: f_00005-8-2 loss: 0.305012  [   96/  146]
train() client id: f_00005-8-3 loss: 0.636053  [  128/  146]
train() client id: f_00005-9-0 loss: 0.520911  [   32/  146]
train() client id: f_00005-9-1 loss: 0.448954  [   64/  146]
train() client id: f_00005-9-2 loss: 0.404175  [   96/  146]
train() client id: f_00005-9-3 loss: 0.646910  [  128/  146]
train() client id: f_00005-10-0 loss: 0.385915  [   32/  146]
train() client id: f_00005-10-1 loss: 0.649437  [   64/  146]
train() client id: f_00005-10-2 loss: 0.557773  [   96/  146]
train() client id: f_00005-10-3 loss: 0.622632  [  128/  146]
train() client id: f_00005-11-0 loss: 0.837578  [   32/  146]
train() client id: f_00005-11-1 loss: 0.442348  [   64/  146]
train() client id: f_00005-11-2 loss: 0.596579  [   96/  146]
train() client id: f_00005-11-3 loss: 0.381194  [  128/  146]
train() client id: f_00005-12-0 loss: 0.583665  [   32/  146]
train() client id: f_00005-12-1 loss: 0.351288  [   64/  146]
train() client id: f_00005-12-2 loss: 0.471691  [   96/  146]
train() client id: f_00005-12-3 loss: 0.741027  [  128/  146]
train() client id: f_00006-0-0 loss: 0.489751  [   32/   54]
train() client id: f_00006-1-0 loss: 0.540105  [   32/   54]
train() client id: f_00006-2-0 loss: 0.492892  [   32/   54]
train() client id: f_00006-3-0 loss: 0.488223  [   32/   54]
train() client id: f_00006-4-0 loss: 0.540719  [   32/   54]
train() client id: f_00006-5-0 loss: 0.526835  [   32/   54]
train() client id: f_00006-6-0 loss: 0.473396  [   32/   54]
train() client id: f_00006-7-0 loss: 0.446969  [   32/   54]
train() client id: f_00006-8-0 loss: 0.522596  [   32/   54]
train() client id: f_00006-9-0 loss: 0.434044  [   32/   54]
train() client id: f_00006-10-0 loss: 0.435111  [   32/   54]
train() client id: f_00006-11-0 loss: 0.469383  [   32/   54]
train() client id: f_00006-12-0 loss: 0.564191  [   32/   54]
train() client id: f_00007-0-0 loss: 0.733441  [   32/  179]
train() client id: f_00007-0-1 loss: 0.542090  [   64/  179]
train() client id: f_00007-0-2 loss: 0.659338  [   96/  179]
train() client id: f_00007-0-3 loss: 0.702597  [  128/  179]
train() client id: f_00007-0-4 loss: 0.630289  [  160/  179]
train() client id: f_00007-1-0 loss: 0.477056  [   32/  179]
train() client id: f_00007-1-1 loss: 0.694025  [   64/  179]
train() client id: f_00007-1-2 loss: 0.535956  [   96/  179]
train() client id: f_00007-1-3 loss: 0.609805  [  128/  179]
train() client id: f_00007-1-4 loss: 0.726133  [  160/  179]
train() client id: f_00007-2-0 loss: 0.569205  [   32/  179]
train() client id: f_00007-2-1 loss: 0.709798  [   64/  179]
train() client id: f_00007-2-2 loss: 0.496923  [   96/  179]
train() client id: f_00007-2-3 loss: 0.789925  [  128/  179]
train() client id: f_00007-2-4 loss: 0.676698  [  160/  179]
train() client id: f_00007-3-0 loss: 0.628091  [   32/  179]
train() client id: f_00007-3-1 loss: 0.547194  [   64/  179]
train() client id: f_00007-3-2 loss: 0.722564  [   96/  179]
train() client id: f_00007-3-3 loss: 0.794368  [  128/  179]
train() client id: f_00007-3-4 loss: 0.558923  [  160/  179]
train() client id: f_00007-4-0 loss: 0.574301  [   32/  179]
train() client id: f_00007-4-1 loss: 0.640891  [   64/  179]
train() client id: f_00007-4-2 loss: 0.655174  [   96/  179]
train() client id: f_00007-4-3 loss: 0.480321  [  128/  179]
train() client id: f_00007-4-4 loss: 0.847580  [  160/  179]
train() client id: f_00007-5-0 loss: 0.578906  [   32/  179]
train() client id: f_00007-5-1 loss: 0.555212  [   64/  179]
train() client id: f_00007-5-2 loss: 0.482257  [   96/  179]
train() client id: f_00007-5-3 loss: 0.578169  [  128/  179]
train() client id: f_00007-5-4 loss: 0.962707  [  160/  179]
train() client id: f_00007-6-0 loss: 0.658380  [   32/  179]
train() client id: f_00007-6-1 loss: 0.541506  [   64/  179]
train() client id: f_00007-6-2 loss: 0.564379  [   96/  179]
train() client id: f_00007-6-3 loss: 0.631629  [  128/  179]
train() client id: f_00007-6-4 loss: 0.658609  [  160/  179]
train() client id: f_00007-7-0 loss: 0.551557  [   32/  179]
train() client id: f_00007-7-1 loss: 0.779271  [   64/  179]
train() client id: f_00007-7-2 loss: 0.795581  [   96/  179]
train() client id: f_00007-7-3 loss: 0.459310  [  128/  179]
train() client id: f_00007-7-4 loss: 0.509194  [  160/  179]
train() client id: f_00007-8-0 loss: 0.547480  [   32/  179]
train() client id: f_00007-8-1 loss: 0.719616  [   64/  179]
train() client id: f_00007-8-2 loss: 0.496176  [   96/  179]
train() client id: f_00007-8-3 loss: 0.768104  [  128/  179]
train() client id: f_00007-8-4 loss: 0.529563  [  160/  179]
train() client id: f_00007-9-0 loss: 0.474002  [   32/  179]
train() client id: f_00007-9-1 loss: 0.459585  [   64/  179]
train() client id: f_00007-9-2 loss: 0.623931  [   96/  179]
train() client id: f_00007-9-3 loss: 0.769773  [  128/  179]
train() client id: f_00007-9-4 loss: 0.633497  [  160/  179]
train() client id: f_00007-10-0 loss: 0.768596  [   32/  179]
train() client id: f_00007-10-1 loss: 0.691461  [   64/  179]
train() client id: f_00007-10-2 loss: 0.617177  [   96/  179]
train() client id: f_00007-10-3 loss: 0.541416  [  128/  179]
train() client id: f_00007-10-4 loss: 0.461327  [  160/  179]
train() client id: f_00007-11-0 loss: 0.622686  [   32/  179]
train() client id: f_00007-11-1 loss: 0.570394  [   64/  179]
train() client id: f_00007-11-2 loss: 0.597181  [   96/  179]
train() client id: f_00007-11-3 loss: 0.628104  [  128/  179]
train() client id: f_00007-11-4 loss: 0.581651  [  160/  179]
train() client id: f_00007-12-0 loss: 0.629983  [   32/  179]
train() client id: f_00007-12-1 loss: 0.628138  [   64/  179]
train() client id: f_00007-12-2 loss: 0.482566  [   96/  179]
train() client id: f_00007-12-3 loss: 0.617178  [  128/  179]
train() client id: f_00007-12-4 loss: 0.672521  [  160/  179]
train() client id: f_00008-0-0 loss: 0.743325  [   32/  130]
train() client id: f_00008-0-1 loss: 0.750686  [   64/  130]
train() client id: f_00008-0-2 loss: 0.602356  [   96/  130]
train() client id: f_00008-0-3 loss: 0.717883  [  128/  130]
train() client id: f_00008-1-0 loss: 0.785131  [   32/  130]
train() client id: f_00008-1-1 loss: 0.609341  [   64/  130]
train() client id: f_00008-1-2 loss: 0.621587  [   96/  130]
train() client id: f_00008-1-3 loss: 0.765498  [  128/  130]
train() client id: f_00008-2-0 loss: 0.599241  [   32/  130]
train() client id: f_00008-2-1 loss: 0.773286  [   64/  130]
train() client id: f_00008-2-2 loss: 0.629393  [   96/  130]
train() client id: f_00008-2-3 loss: 0.812388  [  128/  130]
train() client id: f_00008-3-0 loss: 0.734855  [   32/  130]
train() client id: f_00008-3-1 loss: 0.794112  [   64/  130]
train() client id: f_00008-3-2 loss: 0.653081  [   96/  130]
train() client id: f_00008-3-3 loss: 0.642568  [  128/  130]
train() client id: f_00008-4-0 loss: 0.710440  [   32/  130]
train() client id: f_00008-4-1 loss: 0.794929  [   64/  130]
train() client id: f_00008-4-2 loss: 0.684925  [   96/  130]
train() client id: f_00008-4-3 loss: 0.630596  [  128/  130]
train() client id: f_00008-5-0 loss: 0.682076  [   32/  130]
train() client id: f_00008-5-1 loss: 0.736989  [   64/  130]
train() client id: f_00008-5-2 loss: 0.701160  [   96/  130]
train() client id: f_00008-5-3 loss: 0.693525  [  128/  130]
train() client id: f_00008-6-0 loss: 0.604893  [   32/  130]
train() client id: f_00008-6-1 loss: 0.731690  [   64/  130]
train() client id: f_00008-6-2 loss: 0.794861  [   96/  130]
train() client id: f_00008-6-3 loss: 0.661159  [  128/  130]
train() client id: f_00008-7-0 loss: 0.786115  [   32/  130]
train() client id: f_00008-7-1 loss: 0.686382  [   64/  130]
train() client id: f_00008-7-2 loss: 0.666607  [   96/  130]
train() client id: f_00008-7-3 loss: 0.636856  [  128/  130]
train() client id: f_00008-8-0 loss: 0.661062  [   32/  130]
train() client id: f_00008-8-1 loss: 0.726885  [   64/  130]
train() client id: f_00008-8-2 loss: 0.802269  [   96/  130]
train() client id: f_00008-8-3 loss: 0.638696  [  128/  130]
train() client id: f_00008-9-0 loss: 0.701026  [   32/  130]
train() client id: f_00008-9-1 loss: 0.686670  [   64/  130]
train() client id: f_00008-9-2 loss: 0.799035  [   96/  130]
train() client id: f_00008-9-3 loss: 0.611529  [  128/  130]
train() client id: f_00008-10-0 loss: 0.649080  [   32/  130]
train() client id: f_00008-10-1 loss: 0.766127  [   64/  130]
train() client id: f_00008-10-2 loss: 0.675100  [   96/  130]
train() client id: f_00008-10-3 loss: 0.721522  [  128/  130]
train() client id: f_00008-11-0 loss: 0.567976  [   32/  130]
train() client id: f_00008-11-1 loss: 0.755613  [   64/  130]
train() client id: f_00008-11-2 loss: 0.746058  [   96/  130]
train() client id: f_00008-11-3 loss: 0.739918  [  128/  130]
train() client id: f_00008-12-0 loss: 0.763736  [   32/  130]
train() client id: f_00008-12-1 loss: 0.657441  [   64/  130]
train() client id: f_00008-12-2 loss: 0.708189  [   96/  130]
train() client id: f_00008-12-3 loss: 0.688184  [  128/  130]
train() client id: f_00009-0-0 loss: 0.990579  [   32/  118]
train() client id: f_00009-0-1 loss: 1.202816  [   64/  118]
train() client id: f_00009-0-2 loss: 1.129629  [   96/  118]
train() client id: f_00009-1-0 loss: 1.125366  [   32/  118]
train() client id: f_00009-1-1 loss: 1.119408  [   64/  118]
train() client id: f_00009-1-2 loss: 1.000870  [   96/  118]
train() client id: f_00009-2-0 loss: 1.048107  [   32/  118]
train() client id: f_00009-2-1 loss: 0.844362  [   64/  118]
train() client id: f_00009-2-2 loss: 0.988405  [   96/  118]
train() client id: f_00009-3-0 loss: 0.852090  [   32/  118]
train() client id: f_00009-3-1 loss: 0.985946  [   64/  118]
train() client id: f_00009-3-2 loss: 1.055728  [   96/  118]
train() client id: f_00009-4-0 loss: 1.007874  [   32/  118]
train() client id: f_00009-4-1 loss: 0.826985  [   64/  118]
train() client id: f_00009-4-2 loss: 0.971122  [   96/  118]
train() client id: f_00009-5-0 loss: 0.830734  [   32/  118]
train() client id: f_00009-5-1 loss: 0.895768  [   64/  118]
train() client id: f_00009-5-2 loss: 0.987141  [   96/  118]
train() client id: f_00009-6-0 loss: 0.883831  [   32/  118]
train() client id: f_00009-6-1 loss: 0.795473  [   64/  118]
train() client id: f_00009-6-2 loss: 0.882854  [   96/  118]
train() client id: f_00009-7-0 loss: 0.985719  [   32/  118]
train() client id: f_00009-7-1 loss: 0.795895  [   64/  118]
train() client id: f_00009-7-2 loss: 0.744809  [   96/  118]
train() client id: f_00009-8-0 loss: 0.753030  [   32/  118]
train() client id: f_00009-8-1 loss: 0.915985  [   64/  118]
train() client id: f_00009-8-2 loss: 0.759767  [   96/  118]
train() client id: f_00009-9-0 loss: 0.864717  [   32/  118]
train() client id: f_00009-9-1 loss: 0.901110  [   64/  118]
train() client id: f_00009-9-2 loss: 0.733498  [   96/  118]
train() client id: f_00009-10-0 loss: 0.742480  [   32/  118]
train() client id: f_00009-10-1 loss: 0.886782  [   64/  118]
train() client id: f_00009-10-2 loss: 0.740604  [   96/  118]
train() client id: f_00009-11-0 loss: 0.752286  [   32/  118]
train() client id: f_00009-11-1 loss: 0.832701  [   64/  118]
train() client id: f_00009-11-2 loss: 0.680994  [   96/  118]
train() client id: f_00009-12-0 loss: 0.796659  [   32/  118]
train() client id: f_00009-12-1 loss: 0.783129  [   64/  118]
train() client id: f_00009-12-2 loss: 0.762019  [   96/  118]
At round 41 accuracy: 0.6525198938992043
At round 41 training accuracy: 0.590878604963112
At round 41 training loss: 0.8350630569369393
update_location
xs = [  -3.9056584     4.20031788  225.00902392   18.81129433    0.97929623
    3.95640986 -187.44319194 -166.32485185  209.66397685 -152.06087855]
ys = [ 217.5879595   200.55583871    1.32061395 -187.45517586  179.35018685
  162.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [239.49900686 224.14345208 246.23323266 213.29160263 205.34714155
 191.11279617 212.46609317 194.07378133 232.95418683 182.039893  ]
dists_bs = [176.31508166 181.0134106  436.08685872 411.01894399 176.03305606
 179.37082135 178.06029338 174.39355681 415.63312302 172.53027713]
uav_gains = [9.05340846e-12 1.18039549e-11 7.98136893e-12 1.40041215e-11
 1.57718869e-11 1.93846067e-11 1.41810871e-11 1.85763382e-11
 1.01750306e-11 2.20968418e-11]
bs_gains = [5.67131265e-11 5.26870620e-11 4.49251656e-12 5.30250316e-12
 5.69679045e-11 5.40491731e-11 5.51704147e-11 5.84802023e-11
 5.13932005e-12 6.02658366e-11]
Round 42
-------------------------------
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.44885328 11.23645857  5.37086954  1.94257213 12.9580199   6.23472006
  2.4044106   7.64858783  5.64816701  5.05583458]
obj_prev = 63.948493486950056
eta_min = 1.1938832184881957e-17	eta_max = 0.9337601214025776
af = 13.47090363962996	bf = 1.273204732538192	zeta = 14.817994003592956	eta = 0.9090909090909091
af = 13.47090363962996	bf = 1.273204732538192	zeta = 28.128308786138636	eta = 0.47890912112953954
af = 13.47090363962996	bf = 1.273204732538192	zeta = 21.476258560043068	eta = 0.6272462962749386
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.271427562519737	eta = 0.6645266396796146
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.2057708129807	eta = 0.666685956418743
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.20555837007598	eta = 0.6666929660097932
eta = 0.6666929660097932
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.03345475 0.07036119 0.03292373 0.0114171  0.08124726 0.03876503
 0.01433775 0.04752699 0.03451681 0.03133064]
ene_total = [1.84044551 3.20713505 1.83649691 0.87474524 3.65310778 1.89784042
 0.99426624 2.34061661 1.97709129 1.58381332]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 0 obj = 3.828642052039432
eta = 0.6666929660097932
freqs = [29660676.66717937 58359378.23819214 29417104.30254062  9895264.30741952
 67264422.3037599  32133134.64789411 12419507.6458075  40702220.97084436
 31867673.54934066 25905086.18443851]
eta_min = 0.6666929660097993	eta_max = 0.6666929660097998
af = 0.007174173529547853	bf = 1.273204732538192	zeta = 0.007891590882502639	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [1.72976861e-06 1.40838874e-05 1.67446812e-06 6.57020031e-08
 2.16046690e-05 2.35241749e-06 1.29974416e-07 4.62748057e-06
 2.06015278e-06 1.23568407e-06]
ene_total = [1.67118944 1.08322193 1.73728876 1.47458345 1.06747966 1.07586755
 1.46958603 1.36994863 2.01106501 1.0525431 ]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 1 obj = 3.8286420520395077
eta = 0.6666929660097998
freqs = [29660676.66717935 58359378.23819202 29417104.30254061  9895264.30741951
 67264422.30375975 32133134.64789404 12419507.64580748 40702220.97084429
 31867673.54934067 25905086.18443845]
Done!
At round 42 energy consumption: 3.828642052039432
At round 42 eta: 0.6666929660097998
At round 42 local rounds: 13.275695913565364
At round 42 global rounds: 41.390297463535276
At round 42 a_n: 13.453131436573733
gradient difference: 0.38261160254478455
train() client id: f_00000-0-0 loss: 1.123900  [   32/  126]
train() client id: f_00000-0-1 loss: 1.392509  [   64/  126]
train() client id: f_00000-0-2 loss: 1.260511  [   96/  126]
train() client id: f_00000-1-0 loss: 1.327138  [   32/  126]
train() client id: f_00000-1-1 loss: 1.175362  [   64/  126]
train() client id: f_00000-1-2 loss: 1.027663  [   96/  126]
train() client id: f_00000-2-0 loss: 1.195504  [   32/  126]
train() client id: f_00000-2-1 loss: 1.027942  [   64/  126]
train() client id: f_00000-2-2 loss: 1.112464  [   96/  126]
train() client id: f_00000-3-0 loss: 0.982614  [   32/  126]
train() client id: f_00000-3-1 loss: 0.969334  [   64/  126]
train() client id: f_00000-3-2 loss: 1.025207  [   96/  126]
train() client id: f_00000-4-0 loss: 0.905508  [   32/  126]
train() client id: f_00000-4-1 loss: 0.875708  [   64/  126]
train() client id: f_00000-4-2 loss: 1.015633  [   96/  126]
train() client id: f_00000-5-0 loss: 0.942840  [   32/  126]
train() client id: f_00000-5-1 loss: 0.865501  [   64/  126]
train() client id: f_00000-5-2 loss: 0.963053  [   96/  126]
train() client id: f_00000-6-0 loss: 0.826196  [   32/  126]
train() client id: f_00000-6-1 loss: 0.863586  [   64/  126]
train() client id: f_00000-6-2 loss: 0.897202  [   96/  126]
train() client id: f_00000-7-0 loss: 0.816743  [   32/  126]
train() client id: f_00000-7-1 loss: 0.867084  [   64/  126]
train() client id: f_00000-7-2 loss: 0.858660  [   96/  126]
train() client id: f_00000-8-0 loss: 0.809245  [   32/  126]
train() client id: f_00000-8-1 loss: 0.976992  [   64/  126]
train() client id: f_00000-8-2 loss: 0.719359  [   96/  126]
train() client id: f_00000-9-0 loss: 0.769409  [   32/  126]
train() client id: f_00000-9-1 loss: 0.851361  [   64/  126]
train() client id: f_00000-9-2 loss: 0.766987  [   96/  126]
train() client id: f_00000-10-0 loss: 0.743125  [   32/  126]
train() client id: f_00000-10-1 loss: 0.832135  [   64/  126]
train() client id: f_00000-10-2 loss: 0.838626  [   96/  126]
train() client id: f_00000-11-0 loss: 0.729179  [   32/  126]
train() client id: f_00000-11-1 loss: 0.653250  [   64/  126]
train() client id: f_00000-11-2 loss: 0.896579  [   96/  126]
train() client id: f_00000-12-0 loss: 0.640436  [   32/  126]
train() client id: f_00000-12-1 loss: 0.771547  [   64/  126]
train() client id: f_00000-12-2 loss: 0.882754  [   96/  126]
train() client id: f_00001-0-0 loss: 0.431646  [   32/  265]
train() client id: f_00001-0-1 loss: 0.490991  [   64/  265]
train() client id: f_00001-0-2 loss: 0.501221  [   96/  265]
train() client id: f_00001-0-3 loss: 0.372415  [  128/  265]
train() client id: f_00001-0-4 loss: 0.405526  [  160/  265]
train() client id: f_00001-0-5 loss: 0.488107  [  192/  265]
train() client id: f_00001-0-6 loss: 0.468286  [  224/  265]
train() client id: f_00001-0-7 loss: 0.471496  [  256/  265]
train() client id: f_00001-1-0 loss: 0.530586  [   32/  265]
train() client id: f_00001-1-1 loss: 0.372167  [   64/  265]
train() client id: f_00001-1-2 loss: 0.458991  [   96/  265]
train() client id: f_00001-1-3 loss: 0.516695  [  128/  265]
train() client id: f_00001-1-4 loss: 0.349343  [  160/  265]
train() client id: f_00001-1-5 loss: 0.367336  [  192/  265]
train() client id: f_00001-1-6 loss: 0.380267  [  224/  265]
train() client id: f_00001-1-7 loss: 0.599627  [  256/  265]
train() client id: f_00001-2-0 loss: 0.418304  [   32/  265]
train() client id: f_00001-2-1 loss: 0.366247  [   64/  265]
train() client id: f_00001-2-2 loss: 0.423923  [   96/  265]
train() client id: f_00001-2-3 loss: 0.429545  [  128/  265]
train() client id: f_00001-2-4 loss: 0.451796  [  160/  265]
train() client id: f_00001-2-5 loss: 0.346198  [  192/  265]
train() client id: f_00001-2-6 loss: 0.485238  [  224/  265]
train() client id: f_00001-2-7 loss: 0.576447  [  256/  265]
train() client id: f_00001-3-0 loss: 0.371175  [   32/  265]
train() client id: f_00001-3-1 loss: 0.513601  [   64/  265]
train() client id: f_00001-3-2 loss: 0.322815  [   96/  265]
train() client id: f_00001-3-3 loss: 0.511170  [  128/  265]
train() client id: f_00001-3-4 loss: 0.428983  [  160/  265]
train() client id: f_00001-3-5 loss: 0.331666  [  192/  265]
train() client id: f_00001-3-6 loss: 0.524382  [  224/  265]
train() client id: f_00001-3-7 loss: 0.449020  [  256/  265]
train() client id: f_00001-4-0 loss: 0.449891  [   32/  265]
train() client id: f_00001-4-1 loss: 0.486942  [   64/  265]
train() client id: f_00001-4-2 loss: 0.556771  [   96/  265]
train() client id: f_00001-4-3 loss: 0.394730  [  128/  265]
train() client id: f_00001-4-4 loss: 0.397884  [  160/  265]
train() client id: f_00001-4-5 loss: 0.394485  [  192/  265]
train() client id: f_00001-4-6 loss: 0.351989  [  224/  265]
train() client id: f_00001-4-7 loss: 0.441856  [  256/  265]
train() client id: f_00001-5-0 loss: 0.331779  [   32/  265]
train() client id: f_00001-5-1 loss: 0.525713  [   64/  265]
train() client id: f_00001-5-2 loss: 0.415545  [   96/  265]
train() client id: f_00001-5-3 loss: 0.372817  [  128/  265]
train() client id: f_00001-5-4 loss: 0.439786  [  160/  265]
train() client id: f_00001-5-5 loss: 0.392532  [  192/  265]
train() client id: f_00001-5-6 loss: 0.551832  [  224/  265]
train() client id: f_00001-5-7 loss: 0.392918  [  256/  265]
train() client id: f_00001-6-0 loss: 0.591715  [   32/  265]
train() client id: f_00001-6-1 loss: 0.417167  [   64/  265]
train() client id: f_00001-6-2 loss: 0.362763  [   96/  265]
train() client id: f_00001-6-3 loss: 0.510999  [  128/  265]
train() client id: f_00001-6-4 loss: 0.401044  [  160/  265]
train() client id: f_00001-6-5 loss: 0.402635  [  192/  265]
train() client id: f_00001-6-6 loss: 0.339145  [  224/  265]
train() client id: f_00001-6-7 loss: 0.430018  [  256/  265]
train() client id: f_00001-7-0 loss: 0.406018  [   32/  265]
train() client id: f_00001-7-1 loss: 0.366440  [   64/  265]
train() client id: f_00001-7-2 loss: 0.413392  [   96/  265]
train() client id: f_00001-7-3 loss: 0.345199  [  128/  265]
train() client id: f_00001-7-4 loss: 0.419960  [  160/  265]
train() client id: f_00001-7-5 loss: 0.490979  [  192/  265]
train() client id: f_00001-7-6 loss: 0.459562  [  224/  265]
train() client id: f_00001-7-7 loss: 0.523108  [  256/  265]
train() client id: f_00001-8-0 loss: 0.388333  [   32/  265]
train() client id: f_00001-8-1 loss: 0.506245  [   64/  265]
train() client id: f_00001-8-2 loss: 0.528975  [   96/  265]
train() client id: f_00001-8-3 loss: 0.327011  [  128/  265]
train() client id: f_00001-8-4 loss: 0.388147  [  160/  265]
train() client id: f_00001-8-5 loss: 0.579158  [  192/  265]
train() client id: f_00001-8-6 loss: 0.401748  [  224/  265]
train() client id: f_00001-8-7 loss: 0.335650  [  256/  265]
train() client id: f_00001-9-0 loss: 0.327334  [   32/  265]
train() client id: f_00001-9-1 loss: 0.621346  [   64/  265]
train() client id: f_00001-9-2 loss: 0.584242  [   96/  265]
train() client id: f_00001-9-3 loss: 0.429271  [  128/  265]
train() client id: f_00001-9-4 loss: 0.338636  [  160/  265]
train() client id: f_00001-9-5 loss: 0.331513  [  192/  265]
train() client id: f_00001-9-6 loss: 0.395206  [  224/  265]
train() client id: f_00001-9-7 loss: 0.420286  [  256/  265]
train() client id: f_00001-10-0 loss: 0.424571  [   32/  265]
train() client id: f_00001-10-1 loss: 0.466656  [   64/  265]
train() client id: f_00001-10-2 loss: 0.517402  [   96/  265]
train() client id: f_00001-10-3 loss: 0.433232  [  128/  265]
train() client id: f_00001-10-4 loss: 0.321103  [  160/  265]
train() client id: f_00001-10-5 loss: 0.357599  [  192/  265]
train() client id: f_00001-10-6 loss: 0.467295  [  224/  265]
train() client id: f_00001-10-7 loss: 0.328356  [  256/  265]
train() client id: f_00001-11-0 loss: 0.528661  [   32/  265]
train() client id: f_00001-11-1 loss: 0.452939  [   64/  265]
train() client id: f_00001-11-2 loss: 0.503599  [   96/  265]
train() client id: f_00001-11-3 loss: 0.337049  [  128/  265]
train() client id: f_00001-11-4 loss: 0.442888  [  160/  265]
train() client id: f_00001-11-5 loss: 0.389124  [  192/  265]
train() client id: f_00001-11-6 loss: 0.376033  [  224/  265]
train() client id: f_00001-11-7 loss: 0.424089  [  256/  265]
train() client id: f_00001-12-0 loss: 0.522933  [   32/  265]
train() client id: f_00001-12-1 loss: 0.587536  [   64/  265]
train() client id: f_00001-12-2 loss: 0.382411  [   96/  265]
train() client id: f_00001-12-3 loss: 0.384833  [  128/  265]
train() client id: f_00001-12-4 loss: 0.384493  [  160/  265]
train() client id: f_00001-12-5 loss: 0.341492  [  192/  265]
train() client id: f_00001-12-6 loss: 0.528287  [  224/  265]
train() client id: f_00001-12-7 loss: 0.338879  [  256/  265]
train() client id: f_00002-0-0 loss: 1.028595  [   32/  124]
train() client id: f_00002-0-1 loss: 1.353594  [   64/  124]
train() client id: f_00002-0-2 loss: 1.078082  [   96/  124]
train() client id: f_00002-1-0 loss: 1.082316  [   32/  124]
train() client id: f_00002-1-1 loss: 1.037264  [   64/  124]
train() client id: f_00002-1-2 loss: 1.129648  [   96/  124]
train() client id: f_00002-2-0 loss: 1.084774  [   32/  124]
train() client id: f_00002-2-1 loss: 1.028628  [   64/  124]
train() client id: f_00002-2-2 loss: 1.140172  [   96/  124]
train() client id: f_00002-3-0 loss: 0.999052  [   32/  124]
train() client id: f_00002-3-1 loss: 1.001847  [   64/  124]
train() client id: f_00002-3-2 loss: 1.029104  [   96/  124]
train() client id: f_00002-4-0 loss: 0.988637  [   32/  124]
train() client id: f_00002-4-1 loss: 1.039914  [   64/  124]
train() client id: f_00002-4-2 loss: 0.910087  [   96/  124]
train() client id: f_00002-5-0 loss: 0.932980  [   32/  124]
train() client id: f_00002-5-1 loss: 1.019057  [   64/  124]
train() client id: f_00002-5-2 loss: 1.028229  [   96/  124]
train() client id: f_00002-6-0 loss: 1.070780  [   32/  124]
train() client id: f_00002-6-1 loss: 1.078958  [   64/  124]
train() client id: f_00002-6-2 loss: 0.929299  [   96/  124]
train() client id: f_00002-7-0 loss: 0.917655  [   32/  124]
train() client id: f_00002-7-1 loss: 0.989077  [   64/  124]
train() client id: f_00002-7-2 loss: 0.944198  [   96/  124]
train() client id: f_00002-8-0 loss: 0.979826  [   32/  124]
train() client id: f_00002-8-1 loss: 0.819073  [   64/  124]
train() client id: f_00002-8-2 loss: 0.954949  [   96/  124]
train() client id: f_00002-9-0 loss: 1.003646  [   32/  124]
train() client id: f_00002-9-1 loss: 0.874317  [   64/  124]
train() client id: f_00002-9-2 loss: 0.851205  [   96/  124]
train() client id: f_00002-10-0 loss: 0.804800  [   32/  124]
train() client id: f_00002-10-1 loss: 0.996366  [   64/  124]
train() client id: f_00002-10-2 loss: 0.863957  [   96/  124]
train() client id: f_00002-11-0 loss: 1.145827  [   32/  124]
train() client id: f_00002-11-1 loss: 0.698281  [   64/  124]
train() client id: f_00002-11-2 loss: 0.765210  [   96/  124]
train() client id: f_00002-12-0 loss: 0.854045  [   32/  124]
train() client id: f_00002-12-1 loss: 0.977770  [   64/  124]
train() client id: f_00002-12-2 loss: 0.988857  [   96/  124]
train() client id: f_00003-0-0 loss: 0.894355  [   32/   43]
train() client id: f_00003-1-0 loss: 0.763873  [   32/   43]
train() client id: f_00003-2-0 loss: 0.692613  [   32/   43]
train() client id: f_00003-3-0 loss: 0.735386  [   32/   43]
train() client id: f_00003-4-0 loss: 0.719522  [   32/   43]
train() client id: f_00003-5-0 loss: 0.765096  [   32/   43]
train() client id: f_00003-6-0 loss: 0.707024  [   32/   43]
train() client id: f_00003-7-0 loss: 0.496175  [   32/   43]
train() client id: f_00003-8-0 loss: 0.620421  [   32/   43]
train() client id: f_00003-9-0 loss: 0.740714  [   32/   43]
train() client id: f_00003-10-0 loss: 0.648527  [   32/   43]
train() client id: f_00003-11-0 loss: 0.665050  [   32/   43]
train() client id: f_00003-12-0 loss: 0.838866  [   32/   43]
train() client id: f_00004-0-0 loss: 0.849714  [   32/  306]
train() client id: f_00004-0-1 loss: 0.929294  [   64/  306]
train() client id: f_00004-0-2 loss: 0.877878  [   96/  306]
train() client id: f_00004-0-3 loss: 0.870791  [  128/  306]
train() client id: f_00004-0-4 loss: 0.837054  [  160/  306]
train() client id: f_00004-0-5 loss: 0.803851  [  192/  306]
train() client id: f_00004-0-6 loss: 0.769195  [  224/  306]
train() client id: f_00004-0-7 loss: 0.904132  [  256/  306]
train() client id: f_00004-0-8 loss: 1.151368  [  288/  306]
train() client id: f_00004-1-0 loss: 0.950091  [   32/  306]
train() client id: f_00004-1-1 loss: 1.012661  [   64/  306]
train() client id: f_00004-1-2 loss: 0.833099  [   96/  306]
train() client id: f_00004-1-3 loss: 0.869731  [  128/  306]
train() client id: f_00004-1-4 loss: 0.981616  [  160/  306]
train() client id: f_00004-1-5 loss: 0.817538  [  192/  306]
train() client id: f_00004-1-6 loss: 0.863823  [  224/  306]
train() client id: f_00004-1-7 loss: 0.966416  [  256/  306]
train() client id: f_00004-1-8 loss: 0.792278  [  288/  306]
train() client id: f_00004-2-0 loss: 0.795245  [   32/  306]
train() client id: f_00004-2-1 loss: 0.940690  [   64/  306]
train() client id: f_00004-2-2 loss: 0.968089  [   96/  306]
train() client id: f_00004-2-3 loss: 0.914576  [  128/  306]
train() client id: f_00004-2-4 loss: 0.924712  [  160/  306]
train() client id: f_00004-2-5 loss: 0.880312  [  192/  306]
train() client id: f_00004-2-6 loss: 0.957447  [  224/  306]
train() client id: f_00004-2-7 loss: 0.848991  [  256/  306]
train() client id: f_00004-2-8 loss: 0.870438  [  288/  306]
train() client id: f_00004-3-0 loss: 0.940982  [   32/  306]
train() client id: f_00004-3-1 loss: 0.945577  [   64/  306]
train() client id: f_00004-3-2 loss: 0.818558  [   96/  306]
train() client id: f_00004-3-3 loss: 0.945702  [  128/  306]
train() client id: f_00004-3-4 loss: 0.961744  [  160/  306]
train() client id: f_00004-3-5 loss: 0.903077  [  192/  306]
train() client id: f_00004-3-6 loss: 0.846036  [  224/  306]
train() client id: f_00004-3-7 loss: 0.967208  [  256/  306]
train() client id: f_00004-3-8 loss: 0.875156  [  288/  306]
train() client id: f_00004-4-0 loss: 0.814515  [   32/  306]
train() client id: f_00004-4-1 loss: 0.844026  [   64/  306]
train() client id: f_00004-4-2 loss: 0.896841  [   96/  306]
train() client id: f_00004-4-3 loss: 0.930589  [  128/  306]
train() client id: f_00004-4-4 loss: 0.993175  [  160/  306]
train() client id: f_00004-4-5 loss: 0.787895  [  192/  306]
train() client id: f_00004-4-6 loss: 0.985210  [  224/  306]
train() client id: f_00004-4-7 loss: 0.909424  [  256/  306]
train() client id: f_00004-4-8 loss: 0.891632  [  288/  306]
train() client id: f_00004-5-0 loss: 0.847488  [   32/  306]
train() client id: f_00004-5-1 loss: 0.856186  [   64/  306]
train() client id: f_00004-5-2 loss: 1.025180  [   96/  306]
train() client id: f_00004-5-3 loss: 0.950400  [  128/  306]
train() client id: f_00004-5-4 loss: 0.849839  [  160/  306]
train() client id: f_00004-5-5 loss: 0.853559  [  192/  306]
train() client id: f_00004-5-6 loss: 0.774297  [  224/  306]
train() client id: f_00004-5-7 loss: 1.057853  [  256/  306]
train() client id: f_00004-5-8 loss: 0.940520  [  288/  306]
train() client id: f_00004-6-0 loss: 0.930000  [   32/  306]
train() client id: f_00004-6-1 loss: 0.746585  [   64/  306]
train() client id: f_00004-6-2 loss: 1.005296  [   96/  306]
train() client id: f_00004-6-3 loss: 0.846728  [  128/  306]
train() client id: f_00004-6-4 loss: 1.008110  [  160/  306]
train() client id: f_00004-6-5 loss: 0.854058  [  192/  306]
train() client id: f_00004-6-6 loss: 0.920649  [  224/  306]
train() client id: f_00004-6-7 loss: 0.835473  [  256/  306]
train() client id: f_00004-6-8 loss: 0.964629  [  288/  306]
train() client id: f_00004-7-0 loss: 0.882309  [   32/  306]
train() client id: f_00004-7-1 loss: 0.971587  [   64/  306]
train() client id: f_00004-7-2 loss: 0.830348  [   96/  306]
train() client id: f_00004-7-3 loss: 0.873450  [  128/  306]
train() client id: f_00004-7-4 loss: 0.779754  [  160/  306]
train() client id: f_00004-7-5 loss: 0.951039  [  192/  306]
train() client id: f_00004-7-6 loss: 0.960270  [  224/  306]
train() client id: f_00004-7-7 loss: 0.930418  [  256/  306]
train() client id: f_00004-7-8 loss: 0.898852  [  288/  306]
train() client id: f_00004-8-0 loss: 0.880702  [   32/  306]
train() client id: f_00004-8-1 loss: 0.705128  [   64/  306]
train() client id: f_00004-8-2 loss: 0.869532  [   96/  306]
train() client id: f_00004-8-3 loss: 0.861625  [  128/  306]
train() client id: f_00004-8-4 loss: 1.031512  [  160/  306]
train() client id: f_00004-8-5 loss: 0.839435  [  192/  306]
train() client id: f_00004-8-6 loss: 0.782140  [  224/  306]
train() client id: f_00004-8-7 loss: 1.095002  [  256/  306]
train() client id: f_00004-8-8 loss: 0.961170  [  288/  306]
train() client id: f_00004-9-0 loss: 0.836803  [   32/  306]
train() client id: f_00004-9-1 loss: 0.914865  [   64/  306]
train() client id: f_00004-9-2 loss: 0.926147  [   96/  306]
train() client id: f_00004-9-3 loss: 0.933878  [  128/  306]
train() client id: f_00004-9-4 loss: 0.905357  [  160/  306]
train() client id: f_00004-9-5 loss: 0.868181  [  192/  306]
train() client id: f_00004-9-6 loss: 0.890345  [  224/  306]
train() client id: f_00004-9-7 loss: 0.983559  [  256/  306]
train() client id: f_00004-9-8 loss: 0.940978  [  288/  306]
train() client id: f_00004-10-0 loss: 1.010743  [   32/  306]
train() client id: f_00004-10-1 loss: 0.906954  [   64/  306]
train() client id: f_00004-10-2 loss: 0.787336  [   96/  306]
train() client id: f_00004-10-3 loss: 0.887453  [  128/  306]
train() client id: f_00004-10-4 loss: 0.948572  [  160/  306]
train() client id: f_00004-10-5 loss: 0.828900  [  192/  306]
train() client id: f_00004-10-6 loss: 0.925733  [  224/  306]
train() client id: f_00004-10-7 loss: 0.834595  [  256/  306]
train() client id: f_00004-10-8 loss: 0.938873  [  288/  306]
train() client id: f_00004-11-0 loss: 0.908010  [   32/  306]
train() client id: f_00004-11-1 loss: 0.797782  [   64/  306]
train() client id: f_00004-11-2 loss: 0.850051  [   96/  306]
train() client id: f_00004-11-3 loss: 0.902307  [  128/  306]
train() client id: f_00004-11-4 loss: 0.916986  [  160/  306]
train() client id: f_00004-11-5 loss: 0.893021  [  192/  306]
train() client id: f_00004-11-6 loss: 0.971296  [  224/  306]
train() client id: f_00004-11-7 loss: 0.981243  [  256/  306]
train() client id: f_00004-11-8 loss: 0.925804  [  288/  306]
train() client id: f_00004-12-0 loss: 0.934287  [   32/  306]
train() client id: f_00004-12-1 loss: 0.809142  [   64/  306]
train() client id: f_00004-12-2 loss: 0.876936  [   96/  306]
train() client id: f_00004-12-3 loss: 0.910102  [  128/  306]
train() client id: f_00004-12-4 loss: 0.882801  [  160/  306]
train() client id: f_00004-12-5 loss: 0.850519  [  192/  306]
train() client id: f_00004-12-6 loss: 0.955476  [  224/  306]
train() client id: f_00004-12-7 loss: 0.852730  [  256/  306]
train() client id: f_00004-12-8 loss: 0.954046  [  288/  306]
train() client id: f_00005-0-0 loss: 0.270051  [   32/  146]
train() client id: f_00005-0-1 loss: 0.381713  [   64/  146]
train() client id: f_00005-0-2 loss: 0.605425  [   96/  146]
train() client id: f_00005-0-3 loss: 0.334763  [  128/  146]
train() client id: f_00005-1-0 loss: 0.421971  [   32/  146]
train() client id: f_00005-1-1 loss: 0.392265  [   64/  146]
train() client id: f_00005-1-2 loss: 0.360737  [   96/  146]
train() client id: f_00005-1-3 loss: 0.463206  [  128/  146]
train() client id: f_00005-2-0 loss: 0.348724  [   32/  146]
train() client id: f_00005-2-1 loss: 0.278133  [   64/  146]
train() client id: f_00005-2-2 loss: 0.435398  [   96/  146]
train() client id: f_00005-2-3 loss: 0.468295  [  128/  146]
train() client id: f_00005-3-0 loss: 0.120174  [   32/  146]
train() client id: f_00005-3-1 loss: 0.445031  [   64/  146]
train() client id: f_00005-3-2 loss: 0.290725  [   96/  146]
train() client id: f_00005-3-3 loss: 0.621446  [  128/  146]
train() client id: f_00005-4-0 loss: 0.296127  [   32/  146]
train() client id: f_00005-4-1 loss: 0.224728  [   64/  146]
train() client id: f_00005-4-2 loss: 0.219131  [   96/  146]
train() client id: f_00005-4-3 loss: 0.612331  [  128/  146]
train() client id: f_00005-5-0 loss: 0.360021  [   32/  146]
train() client id: f_00005-5-1 loss: 0.632958  [   64/  146]
train() client id: f_00005-5-2 loss: 0.245395  [   96/  146]
train() client id: f_00005-5-3 loss: 0.245428  [  128/  146]
train() client id: f_00005-6-0 loss: 0.226850  [   32/  146]
train() client id: f_00005-6-1 loss: 0.617389  [   64/  146]
train() client id: f_00005-6-2 loss: 0.470218  [   96/  146]
train() client id: f_00005-6-3 loss: 0.270596  [  128/  146]
train() client id: f_00005-7-0 loss: 0.537763  [   32/  146]
train() client id: f_00005-7-1 loss: 0.368574  [   64/  146]
train() client id: f_00005-7-2 loss: 0.341558  [   96/  146]
train() client id: f_00005-7-3 loss: 0.378330  [  128/  146]
train() client id: f_00005-8-0 loss: 0.353556  [   32/  146]
train() client id: f_00005-8-1 loss: 0.427192  [   64/  146]
train() client id: f_00005-8-2 loss: 0.232708  [   96/  146]
train() client id: f_00005-8-3 loss: 0.450646  [  128/  146]
train() client id: f_00005-9-0 loss: 0.376713  [   32/  146]
train() client id: f_00005-9-1 loss: 0.248031  [   64/  146]
train() client id: f_00005-9-2 loss: 0.470060  [   96/  146]
train() client id: f_00005-9-3 loss: 0.332085  [  128/  146]
train() client id: f_00005-10-0 loss: 0.373552  [   32/  146]
train() client id: f_00005-10-1 loss: 0.264245  [   64/  146]
train() client id: f_00005-10-2 loss: 0.501879  [   96/  146]
train() client id: f_00005-10-3 loss: 0.337386  [  128/  146]
train() client id: f_00005-11-0 loss: 0.364466  [   32/  146]
train() client id: f_00005-11-1 loss: 0.358235  [   64/  146]
train() client id: f_00005-11-2 loss: 0.272837  [   96/  146]
train() client id: f_00005-11-3 loss: 0.442191  [  128/  146]
train() client id: f_00005-12-0 loss: 0.242488  [   32/  146]
train() client id: f_00005-12-1 loss: 0.650852  [   64/  146]
train() client id: f_00005-12-2 loss: 0.350622  [   96/  146]
train() client id: f_00005-12-3 loss: 0.304728  [  128/  146]
train() client id: f_00006-0-0 loss: 0.532875  [   32/   54]
train() client id: f_00006-1-0 loss: 0.457852  [   32/   54]
train() client id: f_00006-2-0 loss: 0.447821  [   32/   54]
train() client id: f_00006-3-0 loss: 0.441320  [   32/   54]
train() client id: f_00006-4-0 loss: 0.412951  [   32/   54]
train() client id: f_00006-5-0 loss: 0.521348  [   32/   54]
train() client id: f_00006-6-0 loss: 0.451569  [   32/   54]
train() client id: f_00006-7-0 loss: 0.497718  [   32/   54]
train() client id: f_00006-8-0 loss: 0.484799  [   32/   54]
train() client id: f_00006-9-0 loss: 0.469352  [   32/   54]
train() client id: f_00006-10-0 loss: 0.533624  [   32/   54]
train() client id: f_00006-11-0 loss: 0.442396  [   32/   54]
train() client id: f_00006-12-0 loss: 0.522270  [   32/   54]
train() client id: f_00007-0-0 loss: 0.609528  [   32/  179]
train() client id: f_00007-0-1 loss: 0.681044  [   64/  179]
train() client id: f_00007-0-2 loss: 0.448042  [   96/  179]
train() client id: f_00007-0-3 loss: 0.350693  [  128/  179]
train() client id: f_00007-0-4 loss: 0.429581  [  160/  179]
train() client id: f_00007-1-0 loss: 0.579233  [   32/  179]
train() client id: f_00007-1-1 loss: 0.464056  [   64/  179]
train() client id: f_00007-1-2 loss: 0.454251  [   96/  179]
train() client id: f_00007-1-3 loss: 0.522115  [  128/  179]
train() client id: f_00007-1-4 loss: 0.531169  [  160/  179]
train() client id: f_00007-2-0 loss: 0.623097  [   32/  179]
train() client id: f_00007-2-1 loss: 0.372833  [   64/  179]
train() client id: f_00007-2-2 loss: 0.361870  [   96/  179]
train() client id: f_00007-2-3 loss: 0.573583  [  128/  179]
train() client id: f_00007-2-4 loss: 0.529639  [  160/  179]
train() client id: f_00007-3-0 loss: 0.500471  [   32/  179]
train() client id: f_00007-3-1 loss: 0.600500  [   64/  179]
train() client id: f_00007-3-2 loss: 0.438575  [   96/  179]
train() client id: f_00007-3-3 loss: 0.303541  [  128/  179]
train() client id: f_00007-3-4 loss: 0.440752  [  160/  179]
train() client id: f_00007-4-0 loss: 0.560445  [   32/  179]
train() client id: f_00007-4-1 loss: 0.312073  [   64/  179]
train() client id: f_00007-4-2 loss: 0.415680  [   96/  179]
train() client id: f_00007-4-3 loss: 0.527781  [  128/  179]
train() client id: f_00007-4-4 loss: 0.360732  [  160/  179]
train() client id: f_00007-5-0 loss: 0.336541  [   32/  179]
train() client id: f_00007-5-1 loss: 0.430226  [   64/  179]
train() client id: f_00007-5-2 loss: 0.412409  [   96/  179]
train() client id: f_00007-5-3 loss: 0.375852  [  128/  179]
train() client id: f_00007-5-4 loss: 0.466368  [  160/  179]
train() client id: f_00007-6-0 loss: 0.499179  [   32/  179]
train() client id: f_00007-6-1 loss: 0.535197  [   64/  179]
train() client id: f_00007-6-2 loss: 0.403656  [   96/  179]
train() client id: f_00007-6-3 loss: 0.320023  [  128/  179]
train() client id: f_00007-6-4 loss: 0.584602  [  160/  179]
train() client id: f_00007-7-0 loss: 0.266686  [   32/  179]
train() client id: f_00007-7-1 loss: 0.419501  [   64/  179]
train() client id: f_00007-7-2 loss: 0.441741  [   96/  179]
train() client id: f_00007-7-3 loss: 0.411299  [  128/  179]
train() client id: f_00007-7-4 loss: 0.726097  [  160/  179]
train() client id: f_00007-8-0 loss: 0.485474  [   32/  179]
train() client id: f_00007-8-1 loss: 0.570739  [   64/  179]
train() client id: f_00007-8-2 loss: 0.462643  [   96/  179]
train() client id: f_00007-8-3 loss: 0.285265  [  128/  179]
train() client id: f_00007-8-4 loss: 0.434738  [  160/  179]
train() client id: f_00007-9-0 loss: 0.825432  [   32/  179]
train() client id: f_00007-9-1 loss: 0.440991  [   64/  179]
train() client id: f_00007-9-2 loss: 0.291911  [   96/  179]
train() client id: f_00007-9-3 loss: 0.300213  [  128/  179]
train() client id: f_00007-9-4 loss: 0.402865  [  160/  179]
train() client id: f_00007-10-0 loss: 0.395174  [   32/  179]
train() client id: f_00007-10-1 loss: 0.286142  [   64/  179]
train() client id: f_00007-10-2 loss: 0.574314  [   96/  179]
train() client id: f_00007-10-3 loss: 0.380212  [  128/  179]
train() client id: f_00007-10-4 loss: 0.569467  [  160/  179]
train() client id: f_00007-11-0 loss: 0.366404  [   32/  179]
train() client id: f_00007-11-1 loss: 0.451172  [   64/  179]
train() client id: f_00007-11-2 loss: 0.444802  [   96/  179]
train() client id: f_00007-11-3 loss: 0.317099  [  128/  179]
train() client id: f_00007-11-4 loss: 0.717041  [  160/  179]
train() client id: f_00007-12-0 loss: 0.368235  [   32/  179]
train() client id: f_00007-12-1 loss: 0.666303  [   64/  179]
train() client id: f_00007-12-2 loss: 0.483708  [   96/  179]
train() client id: f_00007-12-3 loss: 0.337693  [  128/  179]
train() client id: f_00007-12-4 loss: 0.308317  [  160/  179]
train() client id: f_00008-0-0 loss: 0.717878  [   32/  130]
train() client id: f_00008-0-1 loss: 0.793769  [   64/  130]
train() client id: f_00008-0-2 loss: 0.717949  [   96/  130]
train() client id: f_00008-0-3 loss: 0.777051  [  128/  130]
train() client id: f_00008-1-0 loss: 0.808325  [   32/  130]
train() client id: f_00008-1-1 loss: 0.770861  [   64/  130]
train() client id: f_00008-1-2 loss: 0.822221  [   96/  130]
train() client id: f_00008-1-3 loss: 0.621430  [  128/  130]
train() client id: f_00008-2-0 loss: 0.716267  [   32/  130]
train() client id: f_00008-2-1 loss: 0.783821  [   64/  130]
train() client id: f_00008-2-2 loss: 0.747152  [   96/  130]
train() client id: f_00008-2-3 loss: 0.755660  [  128/  130]
train() client id: f_00008-3-0 loss: 0.748547  [   32/  130]
train() client id: f_00008-3-1 loss: 0.881427  [   64/  130]
train() client id: f_00008-3-2 loss: 0.670523  [   96/  130]
train() client id: f_00008-3-3 loss: 0.713230  [  128/  130]
train() client id: f_00008-4-0 loss: 0.711637  [   32/  130]
train() client id: f_00008-4-1 loss: 0.862990  [   64/  130]
train() client id: f_00008-4-2 loss: 0.631237  [   96/  130]
train() client id: f_00008-4-3 loss: 0.791421  [  128/  130]
train() client id: f_00008-5-0 loss: 0.719210  [   32/  130]
train() client id: f_00008-5-1 loss: 0.805784  [   64/  130]
train() client id: f_00008-5-2 loss: 0.751094  [   96/  130]
train() client id: f_00008-5-3 loss: 0.699472  [  128/  130]
train() client id: f_00008-6-0 loss: 0.799695  [   32/  130]
train() client id: f_00008-6-1 loss: 0.786821  [   64/  130]
train() client id: f_00008-6-2 loss: 0.646854  [   96/  130]
train() client id: f_00008-6-3 loss: 0.778407  [  128/  130]
train() client id: f_00008-7-0 loss: 0.747983  [   32/  130]
train() client id: f_00008-7-1 loss: 0.689327  [   64/  130]
train() client id: f_00008-7-2 loss: 0.944428  [   96/  130]
train() client id: f_00008-7-3 loss: 0.619291  [  128/  130]
train() client id: f_00008-8-0 loss: 0.753151  [   32/  130]
train() client id: f_00008-8-1 loss: 0.695006  [   64/  130]
train() client id: f_00008-8-2 loss: 0.835707  [   96/  130]
train() client id: f_00008-8-3 loss: 0.694830  [  128/  130]
train() client id: f_00008-9-0 loss: 0.752800  [   32/  130]
train() client id: f_00008-9-1 loss: 0.686303  [   64/  130]
train() client id: f_00008-9-2 loss: 0.812564  [   96/  130]
train() client id: f_00008-9-3 loss: 0.723922  [  128/  130]
train() client id: f_00008-10-0 loss: 0.691228  [   32/  130]
train() client id: f_00008-10-1 loss: 0.784196  [   64/  130]
train() client id: f_00008-10-2 loss: 0.820514  [   96/  130]
train() client id: f_00008-10-3 loss: 0.696168  [  128/  130]
train() client id: f_00008-11-0 loss: 0.847372  [   32/  130]
train() client id: f_00008-11-1 loss: 0.704464  [   64/  130]
train() client id: f_00008-11-2 loss: 0.693303  [   96/  130]
train() client id: f_00008-11-3 loss: 0.727748  [  128/  130]
train() client id: f_00008-12-0 loss: 0.699517  [   32/  130]
train() client id: f_00008-12-1 loss: 0.776532  [   64/  130]
train() client id: f_00008-12-2 loss: 0.748845  [   96/  130]
train() client id: f_00008-12-3 loss: 0.776565  [  128/  130]
train() client id: f_00009-0-0 loss: 1.011356  [   32/  118]
train() client id: f_00009-0-1 loss: 1.130561  [   64/  118]
train() client id: f_00009-0-2 loss: 1.174918  [   96/  118]
train() client id: f_00009-1-0 loss: 1.011115  [   32/  118]
train() client id: f_00009-1-1 loss: 0.976538  [   64/  118]
train() client id: f_00009-1-2 loss: 1.030703  [   96/  118]
train() client id: f_00009-2-0 loss: 1.009260  [   32/  118]
train() client id: f_00009-2-1 loss: 0.970723  [   64/  118]
train() client id: f_00009-2-2 loss: 1.046068  [   96/  118]
train() client id: f_00009-3-0 loss: 1.052280  [   32/  118]
train() client id: f_00009-3-1 loss: 0.820367  [   64/  118]
train() client id: f_00009-3-2 loss: 1.010605  [   96/  118]
train() client id: f_00009-4-0 loss: 0.823458  [   32/  118]
train() client id: f_00009-4-1 loss: 1.065702  [   64/  118]
train() client id: f_00009-4-2 loss: 0.861988  [   96/  118]
train() client id: f_00009-5-0 loss: 0.858740  [   32/  118]
train() client id: f_00009-5-1 loss: 0.819445  [   64/  118]
train() client id: f_00009-5-2 loss: 0.867218  [   96/  118]
train() client id: f_00009-6-0 loss: 0.909339  [   32/  118]
train() client id: f_00009-6-1 loss: 0.853074  [   64/  118]
train() client id: f_00009-6-2 loss: 1.028844  [   96/  118]
train() client id: f_00009-7-0 loss: 0.842885  [   32/  118]
train() client id: f_00009-7-1 loss: 0.893541  [   64/  118]
train() client id: f_00009-7-2 loss: 0.964850  [   96/  118]
train() client id: f_00009-8-0 loss: 0.878653  [   32/  118]
train() client id: f_00009-8-1 loss: 0.661214  [   64/  118]
train() client id: f_00009-8-2 loss: 1.124835  [   96/  118]
train() client id: f_00009-9-0 loss: 0.759707  [   32/  118]
train() client id: f_00009-9-1 loss: 0.801414  [   64/  118]
train() client id: f_00009-9-2 loss: 0.900085  [   96/  118]
train() client id: f_00009-10-0 loss: 0.967867  [   32/  118]
train() client id: f_00009-10-1 loss: 0.777556  [   64/  118]
train() client id: f_00009-10-2 loss: 0.776531  [   96/  118]
train() client id: f_00009-11-0 loss: 0.840208  [   32/  118]
train() client id: f_00009-11-1 loss: 0.748837  [   64/  118]
train() client id: f_00009-11-2 loss: 0.922893  [   96/  118]
train() client id: f_00009-12-0 loss: 1.060648  [   32/  118]
train() client id: f_00009-12-1 loss: 0.822282  [   64/  118]
train() client id: f_00009-12-2 loss: 0.760664  [   96/  118]
At round 42 accuracy: 0.6525198938992043
At round 42 training accuracy: 0.5881958417169685
At round 42 training loss: 0.833512517926049
update_location
xs = [  -3.9056584     4.20031788  230.00902392   18.81129433    0.97929623
    3.95640986 -192.44319194 -171.32485185  214.66397685 -157.06087855]
ys = [ 222.5879595   205.55583871    1.32061395 -192.45517586  184.35018685
  167.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [244.05051502 228.62818177 250.81047646 217.69901127 209.72827757
 195.3899751  216.8900013  198.37560615 237.46429822 186.23676175]
dists_bs = [177.58909768 181.7867245  440.67770376 415.43486141 176.22751969
 179.10062273 178.47941057 174.21642051 420.2648364  171.93692248]
uav_gains = [8.31942576e-12 1.09578607e-11 7.30282910e-12 1.30831832e-11
 1.47787344e-11 1.82276138e-11 1.32493055e-11 1.74589294e-11
 9.39389080e-12 2.07944683e-11]
bs_gains = [5.55812671e-11 5.20619013e-11 4.36269716e-12 5.14619051e-12
 5.67920630e-11 5.42777976e-11 5.48084274e-11 5.86468436e-11
 4.98229610e-12 6.08499840e-11]
Round 43
-------------------------------
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.31748257 10.95769314  5.24195078  1.89681827 12.63633198  6.07983027
  2.34717104  7.46063269  5.5099153   4.93011902]
obj_prev = 62.377945058813665
eta_min = 4.6045000649351174e-18	eta_max = 0.9345899157842099
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 14.450064093228786	eta = 0.9090909090909091
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 27.618160282034836	eta = 0.47564435026761315
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 21.01662000340652	eta = 0.6250492182285267
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.820837826510346	eta = 0.6627581547216592
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755289483233774	eta = 0.6649571961010281
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755074608659463	eta = 0.6649644287942615
eta = 0.6649644287942615
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.03366898 0.07081173 0.03313455 0.01149021 0.08176751 0.03901326
 0.01442956 0.04783132 0.03473783 0.03153126]
ene_total = [1.80568477 3.13015273 1.80324111 0.85888841 3.56508653 1.85089364
 0.97553732 2.28873162 1.93281417 1.54404431]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 0 obj = 3.765039909361509
eta = 0.6649644287942615
freqs = [28996330.2200803  56908313.47423983 28770753.14460626  9662542.53169571
 65582048.83208953 31323013.60893005 12127351.29718995 39733680.81777115
 31038012.71809572 25251084.61040274]
eta_min = 0.6649644287942639	eta_max = 0.6649644287943239
af = 0.006654514151181828	bf = 1.2587071102671177	zeta = 0.007319965566300011	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [1.65314891e-06 1.33922227e-05 1.60169383e-06 6.26479190e-08
 2.05374606e-05 2.23529729e-06 1.23931308e-07 4.40987202e-06
 1.95427881e-06 1.17407932e-06]
ene_total = [1.6724149  1.05878427 1.74248852 1.46497738 1.04145823 1.04824288
 1.45990451 1.35723249 1.98280693 1.02444478]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 1 obj = 3.765039909362207
eta = 0.6649644287943239
freqs = [28996330.22008007 56908313.47423852 28770753.14460608  9662542.53169558
 65582048.832088   31323013.60892932 12127351.29718979 39733680.81777052
 31038012.71809573 25251084.61040214]
Done!
At round 43 energy consumption: 3.765039909361509
At round 43 eta: 0.6649644287943239
At round 43 local rounds: 13.360704448743727
At round 43 global rounds: 40.15433760708037
At round 43 a_n: 13.110585589604415
gradient difference: 0.42622071504592896
train() client id: f_00000-0-0 loss: 0.850370  [   32/  126]
train() client id: f_00000-0-1 loss: 1.109528  [   64/  126]
train() client id: f_00000-0-2 loss: 1.200196  [   96/  126]
train() client id: f_00000-1-0 loss: 0.925898  [   32/  126]
train() client id: f_00000-1-1 loss: 1.085296  [   64/  126]
train() client id: f_00000-1-2 loss: 0.964954  [   96/  126]
train() client id: f_00000-2-0 loss: 0.759126  [   32/  126]
train() client id: f_00000-2-1 loss: 1.007359  [   64/  126]
train() client id: f_00000-2-2 loss: 1.124536  [   96/  126]
train() client id: f_00000-3-0 loss: 0.918859  [   32/  126]
train() client id: f_00000-3-1 loss: 0.749323  [   64/  126]
train() client id: f_00000-3-2 loss: 0.959712  [   96/  126]
train() client id: f_00000-4-0 loss: 0.873019  [   32/  126]
train() client id: f_00000-4-1 loss: 0.948533  [   64/  126]
train() client id: f_00000-4-2 loss: 0.717750  [   96/  126]
train() client id: f_00000-5-0 loss: 0.883813  [   32/  126]
train() client id: f_00000-5-1 loss: 0.735851  [   64/  126]
train() client id: f_00000-5-2 loss: 0.865593  [   96/  126]
train() client id: f_00000-6-0 loss: 0.692376  [   32/  126]
train() client id: f_00000-6-1 loss: 0.723881  [   64/  126]
train() client id: f_00000-6-2 loss: 0.791270  [   96/  126]
train() client id: f_00000-7-0 loss: 0.760736  [   32/  126]
train() client id: f_00000-7-1 loss: 0.816396  [   64/  126]
train() client id: f_00000-7-2 loss: 0.789943  [   96/  126]
train() client id: f_00000-8-0 loss: 0.847730  [   32/  126]
train() client id: f_00000-8-1 loss: 0.579762  [   64/  126]
train() client id: f_00000-8-2 loss: 0.812234  [   96/  126]
train() client id: f_00000-9-0 loss: 0.858258  [   32/  126]
train() client id: f_00000-9-1 loss: 0.725259  [   64/  126]
train() client id: f_00000-9-2 loss: 0.668868  [   96/  126]
train() client id: f_00000-10-0 loss: 0.737206  [   32/  126]
train() client id: f_00000-10-1 loss: 0.785399  [   64/  126]
train() client id: f_00000-10-2 loss: 0.790772  [   96/  126]
train() client id: f_00000-11-0 loss: 0.598408  [   32/  126]
train() client id: f_00000-11-1 loss: 0.893286  [   64/  126]
train() client id: f_00000-11-2 loss: 0.769696  [   96/  126]
train() client id: f_00000-12-0 loss: 0.722591  [   32/  126]
train() client id: f_00000-12-1 loss: 0.781413  [   64/  126]
train() client id: f_00000-12-2 loss: 0.745430  [   96/  126]
train() client id: f_00001-0-0 loss: 0.479835  [   32/  265]
train() client id: f_00001-0-1 loss: 0.670818  [   64/  265]
train() client id: f_00001-0-2 loss: 0.415007  [   96/  265]
train() client id: f_00001-0-3 loss: 0.472920  [  128/  265]
train() client id: f_00001-0-4 loss: 0.512741  [  160/  265]
train() client id: f_00001-0-5 loss: 0.561233  [  192/  265]
train() client id: f_00001-0-6 loss: 0.441288  [  224/  265]
train() client id: f_00001-0-7 loss: 0.488335  [  256/  265]
train() client id: f_00001-1-0 loss: 0.485087  [   32/  265]
train() client id: f_00001-1-1 loss: 0.399908  [   64/  265]
train() client id: f_00001-1-2 loss: 0.657157  [   96/  265]
train() client id: f_00001-1-3 loss: 0.567000  [  128/  265]
train() client id: f_00001-1-4 loss: 0.472397  [  160/  265]
train() client id: f_00001-1-5 loss: 0.394087  [  192/  265]
train() client id: f_00001-1-6 loss: 0.488476  [  224/  265]
train() client id: f_00001-1-7 loss: 0.521449  [  256/  265]
train() client id: f_00001-2-0 loss: 0.444058  [   32/  265]
train() client id: f_00001-2-1 loss: 0.465387  [   64/  265]
train() client id: f_00001-2-2 loss: 0.534394  [   96/  265]
train() client id: f_00001-2-3 loss: 0.653241  [  128/  265]
train() client id: f_00001-2-4 loss: 0.467738  [  160/  265]
train() client id: f_00001-2-5 loss: 0.410120  [  192/  265]
train() client id: f_00001-2-6 loss: 0.529877  [  224/  265]
train() client id: f_00001-2-7 loss: 0.450394  [  256/  265]
train() client id: f_00001-3-0 loss: 0.492978  [   32/  265]
train() client id: f_00001-3-1 loss: 0.454168  [   64/  265]
train() client id: f_00001-3-2 loss: 0.444867  [   96/  265]
train() client id: f_00001-3-3 loss: 0.518846  [  128/  265]
train() client id: f_00001-3-4 loss: 0.407619  [  160/  265]
train() client id: f_00001-3-5 loss: 0.446210  [  192/  265]
train() client id: f_00001-3-6 loss: 0.618640  [  224/  265]
train() client id: f_00001-3-7 loss: 0.501445  [  256/  265]
train() client id: f_00001-4-0 loss: 0.557503  [   32/  265]
train() client id: f_00001-4-1 loss: 0.481160  [   64/  265]
train() client id: f_00001-4-2 loss: 0.398966  [   96/  265]
train() client id: f_00001-4-3 loss: 0.442275  [  128/  265]
train() client id: f_00001-4-4 loss: 0.606289  [  160/  265]
train() client id: f_00001-4-5 loss: 0.419096  [  192/  265]
train() client id: f_00001-4-6 loss: 0.651998  [  224/  265]
train() client id: f_00001-4-7 loss: 0.361994  [  256/  265]
train() client id: f_00001-5-0 loss: 0.600303  [   32/  265]
train() client id: f_00001-5-1 loss: 0.393653  [   64/  265]
train() client id: f_00001-5-2 loss: 0.558497  [   96/  265]
train() client id: f_00001-5-3 loss: 0.486923  [  128/  265]
train() client id: f_00001-5-4 loss: 0.512390  [  160/  265]
train() client id: f_00001-5-5 loss: 0.468378  [  192/  265]
train() client id: f_00001-5-6 loss: 0.379823  [  224/  265]
train() client id: f_00001-5-7 loss: 0.505678  [  256/  265]
train() client id: f_00001-6-0 loss: 0.451079  [   32/  265]
train() client id: f_00001-6-1 loss: 0.524260  [   64/  265]
train() client id: f_00001-6-2 loss: 0.478696  [   96/  265]
train() client id: f_00001-6-3 loss: 0.462202  [  128/  265]
train() client id: f_00001-6-4 loss: 0.483111  [  160/  265]
train() client id: f_00001-6-5 loss: 0.468775  [  192/  265]
train() client id: f_00001-6-6 loss: 0.523658  [  224/  265]
train() client id: f_00001-6-7 loss: 0.467677  [  256/  265]
train() client id: f_00001-7-0 loss: 0.459627  [   32/  265]
train() client id: f_00001-7-1 loss: 0.531134  [   64/  265]
train() client id: f_00001-7-2 loss: 0.381151  [   96/  265]
train() client id: f_00001-7-3 loss: 0.392730  [  128/  265]
train() client id: f_00001-7-4 loss: 0.557948  [  160/  265]
train() client id: f_00001-7-5 loss: 0.511517  [  192/  265]
train() client id: f_00001-7-6 loss: 0.494198  [  224/  265]
train() client id: f_00001-7-7 loss: 0.567331  [  256/  265]
train() client id: f_00001-8-0 loss: 0.404170  [   32/  265]
train() client id: f_00001-8-1 loss: 0.430333  [   64/  265]
train() client id: f_00001-8-2 loss: 0.448663  [   96/  265]
train() client id: f_00001-8-3 loss: 0.439019  [  128/  265]
train() client id: f_00001-8-4 loss: 0.608248  [  160/  265]
train() client id: f_00001-8-5 loss: 0.526452  [  192/  265]
train() client id: f_00001-8-6 loss: 0.486715  [  224/  265]
train() client id: f_00001-8-7 loss: 0.490084  [  256/  265]
train() client id: f_00001-9-0 loss: 0.396068  [   32/  265]
train() client id: f_00001-9-1 loss: 0.490395  [   64/  265]
train() client id: f_00001-9-2 loss: 0.471954  [   96/  265]
train() client id: f_00001-9-3 loss: 0.565765  [  128/  265]
train() client id: f_00001-9-4 loss: 0.441188  [  160/  265]
train() client id: f_00001-9-5 loss: 0.555379  [  192/  265]
train() client id: f_00001-9-6 loss: 0.441134  [  224/  265]
train() client id: f_00001-9-7 loss: 0.547046  [  256/  265]
train() client id: f_00001-10-0 loss: 0.466007  [   32/  265]
train() client id: f_00001-10-1 loss: 0.448083  [   64/  265]
train() client id: f_00001-10-2 loss: 0.524594  [   96/  265]
train() client id: f_00001-10-3 loss: 0.432365  [  128/  265]
train() client id: f_00001-10-4 loss: 0.760481  [  160/  265]
train() client id: f_00001-10-5 loss: 0.395243  [  192/  265]
train() client id: f_00001-10-6 loss: 0.376167  [  224/  265]
train() client id: f_00001-10-7 loss: 0.499933  [  256/  265]
train() client id: f_00001-11-0 loss: 0.474379  [   32/  265]
train() client id: f_00001-11-1 loss: 0.536470  [   64/  265]
train() client id: f_00001-11-2 loss: 0.407515  [   96/  265]
train() client id: f_00001-11-3 loss: 0.367479  [  128/  265]
train() client id: f_00001-11-4 loss: 0.546675  [  160/  265]
train() client id: f_00001-11-5 loss: 0.578686  [  192/  265]
train() client id: f_00001-11-6 loss: 0.529461  [  224/  265]
train() client id: f_00001-11-7 loss: 0.463691  [  256/  265]
train() client id: f_00001-12-0 loss: 0.467260  [   32/  265]
train() client id: f_00001-12-1 loss: 0.390590  [   64/  265]
train() client id: f_00001-12-2 loss: 0.521959  [   96/  265]
train() client id: f_00001-12-3 loss: 0.517552  [  128/  265]
train() client id: f_00001-12-4 loss: 0.430158  [  160/  265]
train() client id: f_00001-12-5 loss: 0.499029  [  192/  265]
train() client id: f_00001-12-6 loss: 0.540848  [  224/  265]
train() client id: f_00001-12-7 loss: 0.549314  [  256/  265]
train() client id: f_00002-0-0 loss: 1.275573  [   32/  124]
train() client id: f_00002-0-1 loss: 1.030323  [   64/  124]
train() client id: f_00002-0-2 loss: 0.975455  [   96/  124]
train() client id: f_00002-1-0 loss: 1.027186  [   32/  124]
train() client id: f_00002-1-1 loss: 1.110191  [   64/  124]
train() client id: f_00002-1-2 loss: 0.956722  [   96/  124]
train() client id: f_00002-2-0 loss: 0.883701  [   32/  124]
train() client id: f_00002-2-1 loss: 0.986927  [   64/  124]
train() client id: f_00002-2-2 loss: 1.144139  [   96/  124]
train() client id: f_00002-3-0 loss: 0.928184  [   32/  124]
train() client id: f_00002-3-1 loss: 1.070194  [   64/  124]
train() client id: f_00002-3-2 loss: 0.843198  [   96/  124]
train() client id: f_00002-4-0 loss: 0.799358  [   32/  124]
train() client id: f_00002-4-1 loss: 0.970923  [   64/  124]
train() client id: f_00002-4-2 loss: 1.007519  [   96/  124]
train() client id: f_00002-5-0 loss: 1.016352  [   32/  124]
train() client id: f_00002-5-1 loss: 0.878789  [   64/  124]
train() client id: f_00002-5-2 loss: 0.881777  [   96/  124]
train() client id: f_00002-6-0 loss: 1.001806  [   32/  124]
train() client id: f_00002-6-1 loss: 0.824473  [   64/  124]
train() client id: f_00002-6-2 loss: 0.857362  [   96/  124]
train() client id: f_00002-7-0 loss: 0.958090  [   32/  124]
train() client id: f_00002-7-1 loss: 0.785381  [   64/  124]
train() client id: f_00002-7-2 loss: 0.874841  [   96/  124]
train() client id: f_00002-8-0 loss: 0.742561  [   32/  124]
train() client id: f_00002-8-1 loss: 0.996086  [   64/  124]
train() client id: f_00002-8-2 loss: 0.892680  [   96/  124]
train() client id: f_00002-9-0 loss: 0.775880  [   32/  124]
train() client id: f_00002-9-1 loss: 0.820966  [   64/  124]
train() client id: f_00002-9-2 loss: 0.915872  [   96/  124]
train() client id: f_00002-10-0 loss: 0.767434  [   32/  124]
train() client id: f_00002-10-1 loss: 0.943036  [   64/  124]
train() client id: f_00002-10-2 loss: 0.910456  [   96/  124]
train() client id: f_00002-11-0 loss: 1.085539  [   32/  124]
train() client id: f_00002-11-1 loss: 0.672052  [   64/  124]
train() client id: f_00002-11-2 loss: 0.709294  [   96/  124]
train() client id: f_00002-12-0 loss: 0.958717  [   32/  124]
train() client id: f_00002-12-1 loss: 0.821943  [   64/  124]
train() client id: f_00002-12-2 loss: 0.682428  [   96/  124]
train() client id: f_00003-0-0 loss: 0.462271  [   32/   43]
train() client id: f_00003-1-0 loss: 0.492874  [   32/   43]
train() client id: f_00003-2-0 loss: 0.457258  [   32/   43]
train() client id: f_00003-3-0 loss: 0.500835  [   32/   43]
train() client id: f_00003-4-0 loss: 0.557150  [   32/   43]
train() client id: f_00003-5-0 loss: 0.526720  [   32/   43]
train() client id: f_00003-6-0 loss: 0.525518  [   32/   43]
train() client id: f_00003-7-0 loss: 0.399333  [   32/   43]
train() client id: f_00003-8-0 loss: 0.599419  [   32/   43]
train() client id: f_00003-9-0 loss: 0.493821  [   32/   43]
train() client id: f_00003-10-0 loss: 0.485724  [   32/   43]
train() client id: f_00003-11-0 loss: 0.590362  [   32/   43]
train() client id: f_00003-12-0 loss: 0.437363  [   32/   43]
train() client id: f_00004-0-0 loss: 0.872652  [   32/  306]
train() client id: f_00004-0-1 loss: 0.772406  [   64/  306]
train() client id: f_00004-0-2 loss: 0.830927  [   96/  306]
train() client id: f_00004-0-3 loss: 0.748979  [  128/  306]
train() client id: f_00004-0-4 loss: 0.768401  [  160/  306]
train() client id: f_00004-0-5 loss: 0.818044  [  192/  306]
train() client id: f_00004-0-6 loss: 0.776577  [  224/  306]
train() client id: f_00004-0-7 loss: 0.730164  [  256/  306]
train() client id: f_00004-0-8 loss: 0.668146  [  288/  306]
train() client id: f_00004-1-0 loss: 0.806561  [   32/  306]
train() client id: f_00004-1-1 loss: 0.676255  [   64/  306]
train() client id: f_00004-1-2 loss: 0.697113  [   96/  306]
train() client id: f_00004-1-3 loss: 1.031025  [  128/  306]
train() client id: f_00004-1-4 loss: 0.814041  [  160/  306]
train() client id: f_00004-1-5 loss: 0.756772  [  192/  306]
train() client id: f_00004-1-6 loss: 0.809004  [  224/  306]
train() client id: f_00004-1-7 loss: 0.754865  [  256/  306]
train() client id: f_00004-1-8 loss: 0.681246  [  288/  306]
train() client id: f_00004-2-0 loss: 0.784766  [   32/  306]
train() client id: f_00004-2-1 loss: 0.630030  [   64/  306]
train() client id: f_00004-2-2 loss: 0.905623  [   96/  306]
train() client id: f_00004-2-3 loss: 0.785916  [  128/  306]
train() client id: f_00004-2-4 loss: 0.767360  [  160/  306]
train() client id: f_00004-2-5 loss: 0.866499  [  192/  306]
train() client id: f_00004-2-6 loss: 0.756384  [  224/  306]
train() client id: f_00004-2-7 loss: 0.787567  [  256/  306]
train() client id: f_00004-2-8 loss: 0.814270  [  288/  306]
train() client id: f_00004-3-0 loss: 0.894527  [   32/  306]
train() client id: f_00004-3-1 loss: 0.709140  [   64/  306]
train() client id: f_00004-3-2 loss: 0.886911  [   96/  306]
train() client id: f_00004-3-3 loss: 0.660162  [  128/  306]
train() client id: f_00004-3-4 loss: 0.722713  [  160/  306]
train() client id: f_00004-3-5 loss: 0.845400  [  192/  306]
train() client id: f_00004-3-6 loss: 0.768074  [  224/  306]
train() client id: f_00004-3-7 loss: 0.698197  [  256/  306]
train() client id: f_00004-3-8 loss: 0.746868  [  288/  306]
train() client id: f_00004-4-0 loss: 0.858821  [   32/  306]
train() client id: f_00004-4-1 loss: 0.715822  [   64/  306]
train() client id: f_00004-4-2 loss: 0.770147  [   96/  306]
train() client id: f_00004-4-3 loss: 0.790473  [  128/  306]
train() client id: f_00004-4-4 loss: 0.798029  [  160/  306]
train() client id: f_00004-4-5 loss: 0.739841  [  192/  306]
train() client id: f_00004-4-6 loss: 0.750686  [  224/  306]
train() client id: f_00004-4-7 loss: 0.786221  [  256/  306]
train() client id: f_00004-4-8 loss: 0.693376  [  288/  306]
train() client id: f_00004-5-0 loss: 0.751021  [   32/  306]
train() client id: f_00004-5-1 loss: 0.759650  [   64/  306]
train() client id: f_00004-5-2 loss: 0.736838  [   96/  306]
train() client id: f_00004-5-3 loss: 0.825539  [  128/  306]
train() client id: f_00004-5-4 loss: 0.610985  [  160/  306]
train() client id: f_00004-5-5 loss: 0.832239  [  192/  306]
train() client id: f_00004-5-6 loss: 0.552521  [  224/  306]
train() client id: f_00004-5-7 loss: 0.849460  [  256/  306]
train() client id: f_00004-5-8 loss: 0.931943  [  288/  306]
train() client id: f_00004-6-0 loss: 0.756842  [   32/  306]
train() client id: f_00004-6-1 loss: 0.774324  [   64/  306]
train() client id: f_00004-6-2 loss: 0.691794  [   96/  306]
train() client id: f_00004-6-3 loss: 0.914093  [  128/  306]
train() client id: f_00004-6-4 loss: 0.864821  [  160/  306]
train() client id: f_00004-6-5 loss: 0.789037  [  192/  306]
train() client id: f_00004-6-6 loss: 0.669991  [  224/  306]
train() client id: f_00004-6-7 loss: 0.702885  [  256/  306]
train() client id: f_00004-6-8 loss: 0.788922  [  288/  306]
train() client id: f_00004-7-0 loss: 0.909361  [   32/  306]
train() client id: f_00004-7-1 loss: 0.725004  [   64/  306]
train() client id: f_00004-7-2 loss: 0.767114  [   96/  306]
train() client id: f_00004-7-3 loss: 0.779834  [  128/  306]
train() client id: f_00004-7-4 loss: 0.762122  [  160/  306]
train() client id: f_00004-7-5 loss: 0.707078  [  192/  306]
train() client id: f_00004-7-6 loss: 0.751218  [  224/  306]
train() client id: f_00004-7-7 loss: 0.716976  [  256/  306]
train() client id: f_00004-7-8 loss: 0.768073  [  288/  306]
train() client id: f_00004-8-0 loss: 0.685929  [   32/  306]
train() client id: f_00004-8-1 loss: 0.916560  [   64/  306]
train() client id: f_00004-8-2 loss: 0.777896  [   96/  306]
train() client id: f_00004-8-3 loss: 0.623299  [  128/  306]
train() client id: f_00004-8-4 loss: 0.730504  [  160/  306]
train() client id: f_00004-8-5 loss: 0.778112  [  192/  306]
train() client id: f_00004-8-6 loss: 0.789759  [  224/  306]
train() client id: f_00004-8-7 loss: 0.785561  [  256/  306]
train() client id: f_00004-8-8 loss: 0.767524  [  288/  306]
train() client id: f_00004-9-0 loss: 0.780802  [   32/  306]
train() client id: f_00004-9-1 loss: 0.809106  [   64/  306]
train() client id: f_00004-9-2 loss: 0.973436  [   96/  306]
train() client id: f_00004-9-3 loss: 0.688774  [  128/  306]
train() client id: f_00004-9-4 loss: 0.740696  [  160/  306]
train() client id: f_00004-9-5 loss: 0.779850  [  192/  306]
train() client id: f_00004-9-6 loss: 0.769424  [  224/  306]
train() client id: f_00004-9-7 loss: 0.745855  [  256/  306]
train() client id: f_00004-9-8 loss: 0.688060  [  288/  306]
train() client id: f_00004-10-0 loss: 0.708333  [   32/  306]
train() client id: f_00004-10-1 loss: 0.789539  [   64/  306]
train() client id: f_00004-10-2 loss: 0.775018  [   96/  306]
train() client id: f_00004-10-3 loss: 0.719229  [  128/  306]
train() client id: f_00004-10-4 loss: 0.837154  [  160/  306]
train() client id: f_00004-10-5 loss: 0.733351  [  192/  306]
train() client id: f_00004-10-6 loss: 0.877719  [  224/  306]
train() client id: f_00004-10-7 loss: 0.797668  [  256/  306]
train() client id: f_00004-10-8 loss: 0.639905  [  288/  306]
train() client id: f_00004-11-0 loss: 0.609693  [   32/  306]
train() client id: f_00004-11-1 loss: 0.919644  [   64/  306]
train() client id: f_00004-11-2 loss: 0.765974  [   96/  306]
train() client id: f_00004-11-3 loss: 0.793414  [  128/  306]
train() client id: f_00004-11-4 loss: 0.661241  [  160/  306]
train() client id: f_00004-11-5 loss: 0.707654  [  192/  306]
train() client id: f_00004-11-6 loss: 0.818458  [  224/  306]
train() client id: f_00004-11-7 loss: 0.831816  [  256/  306]
train() client id: f_00004-11-8 loss: 0.683260  [  288/  306]
train() client id: f_00004-12-0 loss: 0.766524  [   32/  306]
train() client id: f_00004-12-1 loss: 0.897806  [   64/  306]
train() client id: f_00004-12-2 loss: 0.687375  [   96/  306]
train() client id: f_00004-12-3 loss: 0.669168  [  128/  306]
train() client id: f_00004-12-4 loss: 0.811728  [  160/  306]
train() client id: f_00004-12-5 loss: 0.739457  [  192/  306]
train() client id: f_00004-12-6 loss: 0.670905  [  224/  306]
train() client id: f_00004-12-7 loss: 0.881778  [  256/  306]
train() client id: f_00004-12-8 loss: 0.806903  [  288/  306]
train() client id: f_00005-0-0 loss: 0.744977  [   32/  146]
train() client id: f_00005-0-1 loss: 0.688044  [   64/  146]
train() client id: f_00005-0-2 loss: 0.504519  [   96/  146]
train() client id: f_00005-0-3 loss: 0.648537  [  128/  146]
train() client id: f_00005-1-0 loss: 0.747953  [   32/  146]
train() client id: f_00005-1-1 loss: 0.634007  [   64/  146]
train() client id: f_00005-1-2 loss: 0.533073  [   96/  146]
train() client id: f_00005-1-3 loss: 0.817634  [  128/  146]
train() client id: f_00005-2-0 loss: 0.469294  [   32/  146]
train() client id: f_00005-2-1 loss: 0.688783  [   64/  146]
train() client id: f_00005-2-2 loss: 0.480603  [   96/  146]
train() client id: f_00005-2-3 loss: 0.941156  [  128/  146]
train() client id: f_00005-3-0 loss: 0.714304  [   32/  146]
train() client id: f_00005-3-1 loss: 0.447898  [   64/  146]
train() client id: f_00005-3-2 loss: 0.512141  [   96/  146]
train() client id: f_00005-3-3 loss: 0.903384  [  128/  146]
train() client id: f_00005-4-0 loss: 0.741763  [   32/  146]
train() client id: f_00005-4-1 loss: 0.752377  [   64/  146]
train() client id: f_00005-4-2 loss: 0.671903  [   96/  146]
train() client id: f_00005-4-3 loss: 0.671185  [  128/  146]
train() client id: f_00005-5-0 loss: 0.453390  [   32/  146]
train() client id: f_00005-5-1 loss: 0.563474  [   64/  146]
train() client id: f_00005-5-2 loss: 0.726279  [   96/  146]
train() client id: f_00005-5-3 loss: 1.003779  [  128/  146]
train() client id: f_00005-6-0 loss: 0.680497  [   32/  146]
train() client id: f_00005-6-1 loss: 0.606083  [   64/  146]
train() client id: f_00005-6-2 loss: 0.598481  [   96/  146]
train() client id: f_00005-6-3 loss: 0.845315  [  128/  146]
train() client id: f_00005-7-0 loss: 0.460629  [   32/  146]
train() client id: f_00005-7-1 loss: 0.775733  [   64/  146]
train() client id: f_00005-7-2 loss: 0.753658  [   96/  146]
train() client id: f_00005-7-3 loss: 0.665179  [  128/  146]
train() client id: f_00005-8-0 loss: 0.669094  [   32/  146]
train() client id: f_00005-8-1 loss: 0.744215  [   64/  146]
train() client id: f_00005-8-2 loss: 0.601363  [   96/  146]
train() client id: f_00005-8-3 loss: 0.666286  [  128/  146]
train() client id: f_00005-9-0 loss: 0.451246  [   32/  146]
train() client id: f_00005-9-1 loss: 0.821875  [   64/  146]
train() client id: f_00005-9-2 loss: 0.867855  [   96/  146]
train() client id: f_00005-9-3 loss: 0.730584  [  128/  146]
train() client id: f_00005-10-0 loss: 0.551669  [   32/  146]
train() client id: f_00005-10-1 loss: 0.725691  [   64/  146]
train() client id: f_00005-10-2 loss: 0.802875  [   96/  146]
train() client id: f_00005-10-3 loss: 0.593220  [  128/  146]
train() client id: f_00005-11-0 loss: 0.786601  [   32/  146]
train() client id: f_00005-11-1 loss: 0.493105  [   64/  146]
train() client id: f_00005-11-2 loss: 0.822462  [   96/  146]
train() client id: f_00005-11-3 loss: 0.556635  [  128/  146]
train() client id: f_00005-12-0 loss: 0.620439  [   32/  146]
train() client id: f_00005-12-1 loss: 0.507975  [   64/  146]
train() client id: f_00005-12-2 loss: 0.655402  [   96/  146]
train() client id: f_00005-12-3 loss: 0.945412  [  128/  146]
train() client id: f_00006-0-0 loss: 0.443624  [   32/   54]
train() client id: f_00006-1-0 loss: 0.507334  [   32/   54]
train() client id: f_00006-2-0 loss: 0.479034  [   32/   54]
train() client id: f_00006-3-0 loss: 0.436223  [   32/   54]
train() client id: f_00006-4-0 loss: 0.437010  [   32/   54]
train() client id: f_00006-5-0 loss: 0.462833  [   32/   54]
train() client id: f_00006-6-0 loss: 0.466462  [   32/   54]
train() client id: f_00006-7-0 loss: 0.512299  [   32/   54]
train() client id: f_00006-8-0 loss: 0.448179  [   32/   54]
train() client id: f_00006-9-0 loss: 0.465595  [   32/   54]
train() client id: f_00006-10-0 loss: 0.528523  [   32/   54]
train() client id: f_00006-11-0 loss: 0.448602  [   32/   54]
train() client id: f_00006-12-0 loss: 0.516991  [   32/   54]
train() client id: f_00007-0-0 loss: 0.712486  [   32/  179]
train() client id: f_00007-0-1 loss: 0.762108  [   64/  179]
train() client id: f_00007-0-2 loss: 0.646370  [   96/  179]
train() client id: f_00007-0-3 loss: 0.639757  [  128/  179]
train() client id: f_00007-0-4 loss: 0.794372  [  160/  179]
train() client id: f_00007-1-0 loss: 0.740247  [   32/  179]
train() client id: f_00007-1-1 loss: 0.927395  [   64/  179]
train() client id: f_00007-1-2 loss: 0.541904  [   96/  179]
train() client id: f_00007-1-3 loss: 0.621295  [  128/  179]
train() client id: f_00007-1-4 loss: 0.550052  [  160/  179]
train() client id: f_00007-2-0 loss: 0.726606  [   32/  179]
train() client id: f_00007-2-1 loss: 0.683542  [   64/  179]
train() client id: f_00007-2-2 loss: 0.810582  [   96/  179]
train() client id: f_00007-2-3 loss: 0.705210  [  128/  179]
train() client id: f_00007-2-4 loss: 0.509951  [  160/  179]
train() client id: f_00007-3-0 loss: 0.530238  [   32/  179]
train() client id: f_00007-3-1 loss: 0.874743  [   64/  179]
train() client id: f_00007-3-2 loss: 0.585311  [   96/  179]
train() client id: f_00007-3-3 loss: 0.609902  [  128/  179]
train() client id: f_00007-3-4 loss: 0.730791  [  160/  179]
train() client id: f_00007-4-0 loss: 0.642404  [   32/  179]
train() client id: f_00007-4-1 loss: 0.695681  [   64/  179]
train() client id: f_00007-4-2 loss: 0.690660  [   96/  179]
train() client id: f_00007-4-3 loss: 0.530582  [  128/  179]
train() client id: f_00007-4-4 loss: 0.748692  [  160/  179]
train() client id: f_00007-5-0 loss: 0.667562  [   32/  179]
train() client id: f_00007-5-1 loss: 0.617573  [   64/  179]
train() client id: f_00007-5-2 loss: 0.620941  [   96/  179]
train() client id: f_00007-5-3 loss: 0.929638  [  128/  179]
train() client id: f_00007-5-4 loss: 0.537701  [  160/  179]
train() client id: f_00007-6-0 loss: 0.790590  [   32/  179]
train() client id: f_00007-6-1 loss: 0.708119  [   64/  179]
train() client id: f_00007-6-2 loss: 0.587198  [   96/  179]
train() client id: f_00007-6-3 loss: 0.742791  [  128/  179]
train() client id: f_00007-6-4 loss: 0.643416  [  160/  179]
train() client id: f_00007-7-0 loss: 0.617948  [   32/  179]
train() client id: f_00007-7-1 loss: 0.677667  [   64/  179]
train() client id: f_00007-7-2 loss: 0.617628  [   96/  179]
train() client id: f_00007-7-3 loss: 0.532518  [  128/  179]
train() client id: f_00007-7-4 loss: 0.936577  [  160/  179]
train() client id: f_00007-8-0 loss: 0.643879  [   32/  179]
train() client id: f_00007-8-1 loss: 0.620143  [   64/  179]
train() client id: f_00007-8-2 loss: 0.817315  [   96/  179]
train() client id: f_00007-8-3 loss: 0.602647  [  128/  179]
train() client id: f_00007-8-4 loss: 0.776141  [  160/  179]
train() client id: f_00007-9-0 loss: 0.768748  [   32/  179]
train() client id: f_00007-9-1 loss: 0.599015  [   64/  179]
train() client id: f_00007-9-2 loss: 0.699750  [   96/  179]
train() client id: f_00007-9-3 loss: 0.488204  [  128/  179]
train() client id: f_00007-9-4 loss: 0.807777  [  160/  179]
train() client id: f_00007-10-0 loss: 0.625579  [   32/  179]
train() client id: f_00007-10-1 loss: 0.599104  [   64/  179]
train() client id: f_00007-10-2 loss: 0.815474  [   96/  179]
train() client id: f_00007-10-3 loss: 0.791191  [  128/  179]
train() client id: f_00007-10-4 loss: 0.662924  [  160/  179]
train() client id: f_00007-11-0 loss: 0.639814  [   32/  179]
train() client id: f_00007-11-1 loss: 0.864273  [   64/  179]
train() client id: f_00007-11-2 loss: 0.539196  [   96/  179]
train() client id: f_00007-11-3 loss: 0.516015  [  128/  179]
train() client id: f_00007-11-4 loss: 0.724967  [  160/  179]
train() client id: f_00007-12-0 loss: 0.629480  [   32/  179]
train() client id: f_00007-12-1 loss: 0.653424  [   64/  179]
train() client id: f_00007-12-2 loss: 0.682937  [   96/  179]
train() client id: f_00007-12-3 loss: 0.825004  [  128/  179]
train() client id: f_00007-12-4 loss: 0.699317  [  160/  179]
train() client id: f_00008-0-0 loss: 0.703063  [   32/  130]
train() client id: f_00008-0-1 loss: 0.767602  [   64/  130]
train() client id: f_00008-0-2 loss: 0.652811  [   96/  130]
train() client id: f_00008-0-3 loss: 0.627625  [  128/  130]
train() client id: f_00008-1-0 loss: 0.699701  [   32/  130]
train() client id: f_00008-1-1 loss: 0.679837  [   64/  130]
train() client id: f_00008-1-2 loss: 0.688297  [   96/  130]
train() client id: f_00008-1-3 loss: 0.712678  [  128/  130]
train() client id: f_00008-2-0 loss: 0.695594  [   32/  130]
train() client id: f_00008-2-1 loss: 0.672418  [   64/  130]
train() client id: f_00008-2-2 loss: 0.598595  [   96/  130]
train() client id: f_00008-2-3 loss: 0.810534  [  128/  130]
train() client id: f_00008-3-0 loss: 0.780237  [   32/  130]
train() client id: f_00008-3-1 loss: 0.703535  [   64/  130]
train() client id: f_00008-3-2 loss: 0.693443  [   96/  130]
train() client id: f_00008-3-3 loss: 0.583630  [  128/  130]
train() client id: f_00008-4-0 loss: 0.669594  [   32/  130]
train() client id: f_00008-4-1 loss: 0.708323  [   64/  130]
train() client id: f_00008-4-2 loss: 0.755492  [   96/  130]
train() client id: f_00008-4-3 loss: 0.642912  [  128/  130]
train() client id: f_00008-5-0 loss: 0.586179  [   32/  130]
train() client id: f_00008-5-1 loss: 0.712795  [   64/  130]
train() client id: f_00008-5-2 loss: 0.736021  [   96/  130]
train() client id: f_00008-5-3 loss: 0.712891  [  128/  130]
train() client id: f_00008-6-0 loss: 0.708102  [   32/  130]
train() client id: f_00008-6-1 loss: 0.770154  [   64/  130]
train() client id: f_00008-6-2 loss: 0.648165  [   96/  130]
train() client id: f_00008-6-3 loss: 0.655943  [  128/  130]
train() client id: f_00008-7-0 loss: 0.625746  [   32/  130]
train() client id: f_00008-7-1 loss: 0.642513  [   64/  130]
train() client id: f_00008-7-2 loss: 0.683139  [   96/  130]
train() client id: f_00008-7-3 loss: 0.828737  [  128/  130]
train() client id: f_00008-8-0 loss: 0.762411  [   32/  130]
train() client id: f_00008-8-1 loss: 0.734992  [   64/  130]
train() client id: f_00008-8-2 loss: 0.545030  [   96/  130]
train() client id: f_00008-8-3 loss: 0.732058  [  128/  130]
train() client id: f_00008-9-0 loss: 0.721953  [   32/  130]
train() client id: f_00008-9-1 loss: 0.664161  [   64/  130]
train() client id: f_00008-9-2 loss: 0.679901  [   96/  130]
train() client id: f_00008-9-3 loss: 0.712210  [  128/  130]
train() client id: f_00008-10-0 loss: 0.646410  [   32/  130]
train() client id: f_00008-10-1 loss: 0.745983  [   64/  130]
train() client id: f_00008-10-2 loss: 0.724419  [   96/  130]
train() client id: f_00008-10-3 loss: 0.667195  [  128/  130]
train() client id: f_00008-11-0 loss: 0.673619  [   32/  130]
train() client id: f_00008-11-1 loss: 0.733492  [   64/  130]
train() client id: f_00008-11-2 loss: 0.726186  [   96/  130]
train() client id: f_00008-11-3 loss: 0.628681  [  128/  130]
train() client id: f_00008-12-0 loss: 0.569172  [   32/  130]
train() client id: f_00008-12-1 loss: 0.668323  [   64/  130]
train() client id: f_00008-12-2 loss: 0.751880  [   96/  130]
train() client id: f_00008-12-3 loss: 0.794637  [  128/  130]
train() client id: f_00009-0-0 loss: 1.279509  [   32/  118]
train() client id: f_00009-0-1 loss: 1.006821  [   64/  118]
train() client id: f_00009-0-2 loss: 1.090338  [   96/  118]
train() client id: f_00009-1-0 loss: 1.095493  [   32/  118]
train() client id: f_00009-1-1 loss: 1.022461  [   64/  118]
train() client id: f_00009-1-2 loss: 1.209596  [   96/  118]
train() client id: f_00009-2-0 loss: 1.035293  [   32/  118]
train() client id: f_00009-2-1 loss: 1.124393  [   64/  118]
train() client id: f_00009-2-2 loss: 1.023775  [   96/  118]
train() client id: f_00009-3-0 loss: 1.034181  [   32/  118]
train() client id: f_00009-3-1 loss: 1.216945  [   64/  118]
train() client id: f_00009-3-2 loss: 0.908585  [   96/  118]
train() client id: f_00009-4-0 loss: 1.147668  [   32/  118]
train() client id: f_00009-4-1 loss: 0.926386  [   64/  118]
train() client id: f_00009-4-2 loss: 0.964834  [   96/  118]
train() client id: f_00009-5-0 loss: 0.976944  [   32/  118]
train() client id: f_00009-5-1 loss: 0.834875  [   64/  118]
train() client id: f_00009-5-2 loss: 1.063039  [   96/  118]
train() client id: f_00009-6-0 loss: 1.031465  [   32/  118]
train() client id: f_00009-6-1 loss: 0.906511  [   64/  118]
train() client id: f_00009-6-2 loss: 0.809348  [   96/  118]
train() client id: f_00009-7-0 loss: 0.983245  [   32/  118]
train() client id: f_00009-7-1 loss: 0.855010  [   64/  118]
train() client id: f_00009-7-2 loss: 0.901434  [   96/  118]
train() client id: f_00009-8-0 loss: 0.866156  [   32/  118]
train() client id: f_00009-8-1 loss: 0.817162  [   64/  118]
train() client id: f_00009-8-2 loss: 1.001621  [   96/  118]
train() client id: f_00009-9-0 loss: 0.951858  [   32/  118]
train() client id: f_00009-9-1 loss: 1.040734  [   64/  118]
train() client id: f_00009-9-2 loss: 0.748603  [   96/  118]
train() client id: f_00009-10-0 loss: 1.092189  [   32/  118]
train() client id: f_00009-10-1 loss: 0.762016  [   64/  118]
train() client id: f_00009-10-2 loss: 0.978424  [   96/  118]
train() client id: f_00009-11-0 loss: 0.894892  [   32/  118]
train() client id: f_00009-11-1 loss: 1.071360  [   64/  118]
train() client id: f_00009-11-2 loss: 0.918437  [   96/  118]
train() client id: f_00009-12-0 loss: 0.929280  [   32/  118]
train() client id: f_00009-12-1 loss: 0.961138  [   64/  118]
train() client id: f_00009-12-2 loss: 0.796685  [   96/  118]
At round 43 accuracy: 0.6525198938992043
At round 43 training accuracy: 0.5902079141515761
At round 43 training loss: 0.8322361160031393
update_location
xs = [  -3.9056584     4.20031788  235.00902392   18.81129433    0.97929623
    3.95640986 -197.44319194 -176.32485185  219.66397685 -162.06087855]
ys = [ 227.5879595   210.55583871    1.32061395 -197.45517586  189.35018685
  172.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [248.61925403 233.13387545 255.40357347 222.13151795 214.1365272
 199.70073579 221.33843901 202.709471   241.99366252 190.47241326]
dists_bs = [178.99376305 182.69365504 445.27736169 419.86387774 176.56341796
 178.96975881 179.03723612 174.18269057 424.90489817 171.48735842]
uav_gains = [7.62245662e-12 1.01432215e-11 6.66462561e-12 1.21951248e-11
 1.38244935e-11 1.71271443e-11 1.23513336e-11 1.63936996e-11
 8.64631835e-12 1.95639647e-11]
bs_gains = [5.43685774e-11 5.13414801e-11 4.23768198e-12 4.99562983e-12
 5.64900616e-11 5.43889979e-11 5.43316212e-11 5.86786482e-11
 4.83144700e-12 6.12976995e-11]
Round 44
-------------------------------
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.18624546 10.67895549  5.11317553  1.85111052 12.31468286  5.9249859
  2.28997512  7.27266655  5.37158258  4.80445503]
obj_prev = 60.80783503027712
eta_min = 1.6905775058448652e-18	eta_max = 0.9354514077356543
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 14.082134182864616	eta = 0.9090909090909091
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 27.112785949573404	eta = 0.4721735416659381
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 20.558519671827195	eta = 0.622707294620243
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.371299283019138	eta = 0.6608715285021035
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305821021572196	eta = 0.6631129622477986
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305603441902093	eta = 0.6631204357204616
eta = 0.6631204357204616
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.03389812 0.07129367 0.03336005 0.01156841 0.08232401 0.03927877
 0.01452777 0.04815685 0.03497425 0.03174586]
ene_total = [1.77121153 3.05330088 1.77028457 0.8430904  3.47724339 1.80411205
 0.95686313 2.2367863  1.88825588 1.50445532]
ti_comp = [0.59818702 0.64270661 0.59304455 0.61337277 0.644077   0.64353969
 0.6137322  0.62114382 0.5788735  0.64520803]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.80350798e-06 5.48286200e-05 6.59759497e-06 2.57188538e-07
 8.40589437e-05 9.14540875e-06 5.08766715e-07 1.80913087e-05
 7.97917407e-06 4.80332220e-06]
ene_total = [0.45223865 0.28084755 0.47224396 0.39288439 0.2766519  0.27582751
 0.39149537 0.36333529 0.52744819 0.26916572]
optimize_network iter = 0 obj = 3.7021385292782734
eta = 0.6631204357204616
freqs = [28334048.79448884 55463616.087887   28126095.25989743  9430159.49291534
 63908512.8336163  30517755.77420575 11835590.3817379  38764656.20847187
 30208889.06833503 24601258.06337142]
eta_min = 0.6631204357204632	eta_max = 0.6660036163441293
af = 0.006161926359054621	bf = 1.2446435035998953	zeta = 0.006778118994960084	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57849488e-06 1.27208928e-05 1.53072061e-06 5.96708039e-08
 1.95026760e-05 2.12184375e-06 1.18039937e-07 4.19739907e-06
 1.85126341e-06 1.11442796e-06]
ene_total = [1.67509603 1.03465718 1.74925201 1.45587345 1.015872   1.02111426
 1.45069832 1.34439885 1.95366795 0.99690874]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 1 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
eta_min = 0.6660036163441323	eta_max = 0.6660036163441293
af = 0.006150348470560822	bf = 1.2446435035998953	zeta = 0.006765383317616905	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57740747e-06 1.26939645e-05 1.52994007e-06 5.95992217e-08
 1.94605963e-05 2.11729946e-06 1.17896944e-07 4.19130791e-06
 1.85126341e-06 1.11198606e-06]
ene_total = [1.67509587 1.0346533  1.74925189 1.45587344 1.01586593 1.0211136
 1.4506983  1.34439798 1.95366795 0.99690839]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 2 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
Done!
At round 44 energy consumption: 3.734002631064773
At round 44 eta: 0.6660036163441293
At round 44 local rounds: 13.309571309784454
At round 44 global rounds: 39.25367528264245
At round 44 a_n: 12.768039742635098
gradient difference: 0.4371423125267029
train() client id: f_00000-0-0 loss: 1.281109  [   32/  126]
train() client id: f_00000-0-1 loss: 1.237890  [   64/  126]
train() client id: f_00000-0-2 loss: 1.018786  [   96/  126]
train() client id: f_00000-1-0 loss: 1.159656  [   32/  126]
train() client id: f_00000-1-1 loss: 1.189116  [   64/  126]
train() client id: f_00000-1-2 loss: 1.116858  [   96/  126]
train() client id: f_00000-2-0 loss: 1.029604  [   32/  126]
train() client id: f_00000-2-1 loss: 1.072912  [   64/  126]
train() client id: f_00000-2-2 loss: 1.049990  [   96/  126]
train() client id: f_00000-3-0 loss: 1.005127  [   32/  126]
train() client id: f_00000-3-1 loss: 1.079730  [   64/  126]
train() client id: f_00000-3-2 loss: 0.936293  [   96/  126]
train() client id: f_00000-4-0 loss: 0.879641  [   32/  126]
train() client id: f_00000-4-1 loss: 1.098789  [   64/  126]
train() client id: f_00000-4-2 loss: 0.911508  [   96/  126]
train() client id: f_00000-5-0 loss: 1.026205  [   32/  126]
train() client id: f_00000-5-1 loss: 0.823936  [   64/  126]
train() client id: f_00000-5-2 loss: 0.825088  [   96/  126]
train() client id: f_00000-6-0 loss: 0.834222  [   32/  126]
train() client id: f_00000-6-1 loss: 0.953905  [   64/  126]
train() client id: f_00000-6-2 loss: 0.905034  [   96/  126]
train() client id: f_00000-7-0 loss: 0.908679  [   32/  126]
train() client id: f_00000-7-1 loss: 0.863078  [   64/  126]
train() client id: f_00000-7-2 loss: 0.775701  [   96/  126]
train() client id: f_00000-8-0 loss: 0.882966  [   32/  126]
train() client id: f_00000-8-1 loss: 0.886725  [   64/  126]
train() client id: f_00000-8-2 loss: 0.777992  [   96/  126]
train() client id: f_00000-9-0 loss: 0.886138  [   32/  126]
train() client id: f_00000-9-1 loss: 0.804356  [   64/  126]
train() client id: f_00000-9-2 loss: 0.812566  [   96/  126]
train() client id: f_00000-10-0 loss: 0.785238  [   32/  126]
train() client id: f_00000-10-1 loss: 0.741158  [   64/  126]
train() client id: f_00000-10-2 loss: 0.811545  [   96/  126]
train() client id: f_00000-11-0 loss: 0.954812  [   32/  126]
train() client id: f_00000-11-1 loss: 0.788983  [   64/  126]
train() client id: f_00000-11-2 loss: 0.777919  [   96/  126]
train() client id: f_00000-12-0 loss: 0.696500  [   32/  126]
train() client id: f_00000-12-1 loss: 0.764694  [   64/  126]
train() client id: f_00000-12-2 loss: 0.972343  [   96/  126]
train() client id: f_00001-0-0 loss: 0.397695  [   32/  265]
train() client id: f_00001-0-1 loss: 0.482458  [   64/  265]
train() client id: f_00001-0-2 loss: 0.591304  [   96/  265]
train() client id: f_00001-0-3 loss: 0.400132  [  128/  265]
train() client id: f_00001-0-4 loss: 0.480196  [  160/  265]
train() client id: f_00001-0-5 loss: 0.365082  [  192/  265]
train() client id: f_00001-0-6 loss: 0.340804  [  224/  265]
train() client id: f_00001-0-7 loss: 0.441256  [  256/  265]
train() client id: f_00001-1-0 loss: 0.472358  [   32/  265]
train() client id: f_00001-1-1 loss: 0.341442  [   64/  265]
train() client id: f_00001-1-2 loss: 0.428426  [   96/  265]
train() client id: f_00001-1-3 loss: 0.372722  [  128/  265]
train() client id: f_00001-1-4 loss: 0.452365  [  160/  265]
train() client id: f_00001-1-5 loss: 0.506735  [  192/  265]
train() client id: f_00001-1-6 loss: 0.468976  [  224/  265]
train() client id: f_00001-1-7 loss: 0.408824  [  256/  265]
train() client id: f_00001-2-0 loss: 0.516418  [   32/  265]
train() client id: f_00001-2-1 loss: 0.313188  [   64/  265]
train() client id: f_00001-2-2 loss: 0.392487  [   96/  265]
train() client id: f_00001-2-3 loss: 0.338815  [  128/  265]
train() client id: f_00001-2-4 loss: 0.354722  [  160/  265]
train() client id: f_00001-2-5 loss: 0.498739  [  192/  265]
train() client id: f_00001-2-6 loss: 0.557318  [  224/  265]
train() client id: f_00001-2-7 loss: 0.427304  [  256/  265]
train() client id: f_00001-3-0 loss: 0.357048  [   32/  265]
train() client id: f_00001-3-1 loss: 0.437747  [   64/  265]
train() client id: f_00001-3-2 loss: 0.389362  [   96/  265]
train() client id: f_00001-3-3 loss: 0.472389  [  128/  265]
train() client id: f_00001-3-4 loss: 0.440675  [  160/  265]
train() client id: f_00001-3-5 loss: 0.370472  [  192/  265]
train() client id: f_00001-3-6 loss: 0.426236  [  224/  265]
train() client id: f_00001-3-7 loss: 0.449194  [  256/  265]
train() client id: f_00001-4-0 loss: 0.460442  [   32/  265]
train() client id: f_00001-4-1 loss: 0.394602  [   64/  265]
train() client id: f_00001-4-2 loss: 0.528948  [   96/  265]
train() client id: f_00001-4-3 loss: 0.312978  [  128/  265]
train() client id: f_00001-4-4 loss: 0.326858  [  160/  265]
train() client id: f_00001-4-5 loss: 0.404981  [  192/  265]
train() client id: f_00001-4-6 loss: 0.439079  [  224/  265]
train() client id: f_00001-4-7 loss: 0.434010  [  256/  265]
train() client id: f_00001-5-0 loss: 0.659632  [   32/  265]
train() client id: f_00001-5-1 loss: 0.323969  [   64/  265]
train() client id: f_00001-5-2 loss: 0.377152  [   96/  265]
train() client id: f_00001-5-3 loss: 0.338119  [  128/  265]
train() client id: f_00001-5-4 loss: 0.468852  [  160/  265]
train() client id: f_00001-5-5 loss: 0.382878  [  192/  265]
train() client id: f_00001-5-6 loss: 0.326033  [  224/  265]
train() client id: f_00001-5-7 loss: 0.429221  [  256/  265]
train() client id: f_00001-6-0 loss: 0.416633  [   32/  265]
train() client id: f_00001-6-1 loss: 0.338326  [   64/  265]
train() client id: f_00001-6-2 loss: 0.531149  [   96/  265]
train() client id: f_00001-6-3 loss: 0.317843  [  128/  265]
train() client id: f_00001-6-4 loss: 0.387861  [  160/  265]
train() client id: f_00001-6-5 loss: 0.435568  [  192/  265]
train() client id: f_00001-6-6 loss: 0.486225  [  224/  265]
train() client id: f_00001-6-7 loss: 0.394519  [  256/  265]
train() client id: f_00001-7-0 loss: 0.361370  [   32/  265]
train() client id: f_00001-7-1 loss: 0.410098  [   64/  265]
train() client id: f_00001-7-2 loss: 0.383652  [   96/  265]
train() client id: f_00001-7-3 loss: 0.354299  [  128/  265]
train() client id: f_00001-7-4 loss: 0.503309  [  160/  265]
train() client id: f_00001-7-5 loss: 0.516057  [  192/  265]
train() client id: f_00001-7-6 loss: 0.359529  [  224/  265]
train() client id: f_00001-7-7 loss: 0.410411  [  256/  265]
train() client id: f_00001-8-0 loss: 0.479352  [   32/  265]
train() client id: f_00001-8-1 loss: 0.382309  [   64/  265]
train() client id: f_00001-8-2 loss: 0.345034  [   96/  265]
train() client id: f_00001-8-3 loss: 0.317894  [  128/  265]
train() client id: f_00001-8-4 loss: 0.373982  [  160/  265]
train() client id: f_00001-8-5 loss: 0.522097  [  192/  265]
train() client id: f_00001-8-6 loss: 0.414879  [  224/  265]
train() client id: f_00001-8-7 loss: 0.360463  [  256/  265]
train() client id: f_00001-9-0 loss: 0.364291  [   32/  265]
train() client id: f_00001-9-1 loss: 0.402616  [   64/  265]
train() client id: f_00001-9-2 loss: 0.401595  [   96/  265]
train() client id: f_00001-9-3 loss: 0.372189  [  128/  265]
train() client id: f_00001-9-4 loss: 0.435853  [  160/  265]
train() client id: f_00001-9-5 loss: 0.438642  [  192/  265]
train() client id: f_00001-9-6 loss: 0.396514  [  224/  265]
train() client id: f_00001-9-7 loss: 0.467714  [  256/  265]
train() client id: f_00001-10-0 loss: 0.380756  [   32/  265]
train() client id: f_00001-10-1 loss: 0.316784  [   64/  265]
train() client id: f_00001-10-2 loss: 0.480273  [   96/  265]
train() client id: f_00001-10-3 loss: 0.309726  [  128/  265]
train() client id: f_00001-10-4 loss: 0.351811  [  160/  265]
train() client id: f_00001-10-5 loss: 0.487179  [  192/  265]
train() client id: f_00001-10-6 loss: 0.426989  [  224/  265]
train() client id: f_00001-10-7 loss: 0.399543  [  256/  265]
train() client id: f_00001-11-0 loss: 0.467373  [   32/  265]
train() client id: f_00001-11-1 loss: 0.390216  [   64/  265]
train() client id: f_00001-11-2 loss: 0.379272  [   96/  265]
train() client id: f_00001-11-3 loss: 0.307412  [  128/  265]
train() client id: f_00001-11-4 loss: 0.463241  [  160/  265]
train() client id: f_00001-11-5 loss: 0.375954  [  192/  265]
train() client id: f_00001-11-6 loss: 0.330165  [  224/  265]
train() client id: f_00001-11-7 loss: 0.494246  [  256/  265]
train() client id: f_00001-12-0 loss: 0.506887  [   32/  265]
train() client id: f_00001-12-1 loss: 0.351104  [   64/  265]
train() client id: f_00001-12-2 loss: 0.314007  [   96/  265]
train() client id: f_00001-12-3 loss: 0.331697  [  128/  265]
train() client id: f_00001-12-4 loss: 0.402091  [  160/  265]
train() client id: f_00001-12-5 loss: 0.446431  [  192/  265]
train() client id: f_00001-12-6 loss: 0.463007  [  224/  265]
train() client id: f_00001-12-7 loss: 0.469190  [  256/  265]
train() client id: f_00002-0-0 loss: 1.266939  [   32/  124]
train() client id: f_00002-0-1 loss: 1.333578  [   64/  124]
train() client id: f_00002-0-2 loss: 1.370945  [   96/  124]
train() client id: f_00002-1-0 loss: 1.560638  [   32/  124]
train() client id: f_00002-1-1 loss: 1.160064  [   64/  124]
train() client id: f_00002-1-2 loss: 1.200649  [   96/  124]
train() client id: f_00002-2-0 loss: 1.316802  [   32/  124]
train() client id: f_00002-2-1 loss: 1.223161  [   64/  124]
train() client id: f_00002-2-2 loss: 1.243519  [   96/  124]
train() client id: f_00002-3-0 loss: 1.392801  [   32/  124]
train() client id: f_00002-3-1 loss: 1.246010  [   64/  124]
train() client id: f_00002-3-2 loss: 1.093266  [   96/  124]
train() client id: f_00002-4-0 loss: 1.183731  [   32/  124]
train() client id: f_00002-4-1 loss: 1.210838  [   64/  124]
train() client id: f_00002-4-2 loss: 1.265812  [   96/  124]
train() client id: f_00002-5-0 loss: 0.877208  [   32/  124]
train() client id: f_00002-5-1 loss: 1.233488  [   64/  124]
train() client id: f_00002-5-2 loss: 1.412982  [   96/  124]
train() client id: f_00002-6-0 loss: 1.087528  [   32/  124]
train() client id: f_00002-6-1 loss: 1.279471  [   64/  124]
train() client id: f_00002-6-2 loss: 1.028437  [   96/  124]
train() client id: f_00002-7-0 loss: 1.182216  [   32/  124]
train() client id: f_00002-7-1 loss: 1.129399  [   64/  124]
train() client id: f_00002-7-2 loss: 1.152698  [   96/  124]
train() client id: f_00002-8-0 loss: 1.071887  [   32/  124]
train() client id: f_00002-8-1 loss: 1.070940  [   64/  124]
train() client id: f_00002-8-2 loss: 1.076916  [   96/  124]
train() client id: f_00002-9-0 loss: 0.966842  [   32/  124]
train() client id: f_00002-9-1 loss: 1.218210  [   64/  124]
train() client id: f_00002-9-2 loss: 1.102073  [   96/  124]
train() client id: f_00002-10-0 loss: 0.995333  [   32/  124]
train() client id: f_00002-10-1 loss: 1.016137  [   64/  124]
train() client id: f_00002-10-2 loss: 1.189561  [   96/  124]
train() client id: f_00002-11-0 loss: 1.125257  [   32/  124]
train() client id: f_00002-11-1 loss: 1.035257  [   64/  124]
train() client id: f_00002-11-2 loss: 1.145571  [   96/  124]
train() client id: f_00002-12-0 loss: 1.077629  [   32/  124]
train() client id: f_00002-12-1 loss: 1.171269  [   64/  124]
train() client id: f_00002-12-2 loss: 1.168841  [   96/  124]
train() client id: f_00003-0-0 loss: 0.793659  [   32/   43]
train() client id: f_00003-1-0 loss: 0.718780  [   32/   43]
train() client id: f_00003-2-0 loss: 0.776887  [   32/   43]
train() client id: f_00003-3-0 loss: 0.660590  [   32/   43]
train() client id: f_00003-4-0 loss: 0.878780  [   32/   43]
train() client id: f_00003-5-0 loss: 0.751418  [   32/   43]
train() client id: f_00003-6-0 loss: 0.750898  [   32/   43]
train() client id: f_00003-7-0 loss: 0.806479  [   32/   43]
train() client id: f_00003-8-0 loss: 0.687786  [   32/   43]
train() client id: f_00003-9-0 loss: 0.835369  [   32/   43]
train() client id: f_00003-10-0 loss: 0.648511  [   32/   43]
train() client id: f_00003-11-0 loss: 0.701107  [   32/   43]
train() client id: f_00003-12-0 loss: 0.738514  [   32/   43]
train() client id: f_00004-0-0 loss: 0.734221  [   32/  306]
train() client id: f_00004-0-1 loss: 0.774386  [   64/  306]
train() client id: f_00004-0-2 loss: 0.848501  [   96/  306]
train() client id: f_00004-0-3 loss: 0.907298  [  128/  306]
train() client id: f_00004-0-4 loss: 0.745733  [  160/  306]
train() client id: f_00004-0-5 loss: 0.817066  [  192/  306]
train() client id: f_00004-0-6 loss: 0.682487  [  224/  306]
train() client id: f_00004-0-7 loss: 0.711862  [  256/  306]
train() client id: f_00004-0-8 loss: 0.750765  [  288/  306]
train() client id: f_00004-1-0 loss: 0.705235  [   32/  306]
train() client id: f_00004-1-1 loss: 0.882159  [   64/  306]
train() client id: f_00004-1-2 loss: 0.853832  [   96/  306]
train() client id: f_00004-1-3 loss: 0.808050  [  128/  306]
train() client id: f_00004-1-4 loss: 0.800357  [  160/  306]
train() client id: f_00004-1-5 loss: 0.821344  [  192/  306]
train() client id: f_00004-1-6 loss: 0.849983  [  224/  306]
train() client id: f_00004-1-7 loss: 0.741172  [  256/  306]
train() client id: f_00004-1-8 loss: 0.665865  [  288/  306]
train() client id: f_00004-2-0 loss: 0.784409  [   32/  306]
train() client id: f_00004-2-1 loss: 0.830213  [   64/  306]
train() client id: f_00004-2-2 loss: 0.605986  [   96/  306]
train() client id: f_00004-2-3 loss: 0.842779  [  128/  306]
train() client id: f_00004-2-4 loss: 0.811211  [  160/  306]
train() client id: f_00004-2-5 loss: 0.793702  [  192/  306]
train() client id: f_00004-2-6 loss: 0.786432  [  224/  306]
train() client id: f_00004-2-7 loss: 0.772052  [  256/  306]
train() client id: f_00004-2-8 loss: 0.844884  [  288/  306]
train() client id: f_00004-3-0 loss: 0.792682  [   32/  306]
train() client id: f_00004-3-1 loss: 0.823154  [   64/  306]
train() client id: f_00004-3-2 loss: 0.718577  [   96/  306]
train() client id: f_00004-3-3 loss: 0.718534  [  128/  306]
train() client id: f_00004-3-4 loss: 0.672629  [  160/  306]
train() client id: f_00004-3-5 loss: 0.921848  [  192/  306]
train() client id: f_00004-3-6 loss: 0.757266  [  224/  306]
train() client id: f_00004-3-7 loss: 0.865131  [  256/  306]
train() client id: f_00004-3-8 loss: 0.777023  [  288/  306]
train() client id: f_00004-4-0 loss: 0.981407  [   32/  306]
train() client id: f_00004-4-1 loss: 0.775216  [   64/  306]
train() client id: f_00004-4-2 loss: 0.770938  [   96/  306]
train() client id: f_00004-4-3 loss: 0.707603  [  128/  306]
train() client id: f_00004-4-4 loss: 0.836766  [  160/  306]
train() client id: f_00004-4-5 loss: 0.687987  [  192/  306]
train() client id: f_00004-4-6 loss: 0.735282  [  224/  306]
train() client id: f_00004-4-7 loss: 0.712358  [  256/  306]
train() client id: f_00004-4-8 loss: 0.833291  [  288/  306]
train() client id: f_00004-5-0 loss: 0.751924  [   32/  306]
train() client id: f_00004-5-1 loss: 0.754919  [   64/  306]
train() client id: f_00004-5-2 loss: 0.841747  [   96/  306]
train() client id: f_00004-5-3 loss: 0.874875  [  128/  306]
train() client id: f_00004-5-4 loss: 0.795766  [  160/  306]
train() client id: f_00004-5-5 loss: 0.830556  [  192/  306]
train() client id: f_00004-5-6 loss: 0.723595  [  224/  306]
train() client id: f_00004-5-7 loss: 0.717458  [  256/  306]
train() client id: f_00004-5-8 loss: 0.795599  [  288/  306]
train() client id: f_00004-6-0 loss: 0.805043  [   32/  306]
train() client id: f_00004-6-1 loss: 0.767652  [   64/  306]
train() client id: f_00004-6-2 loss: 0.724530  [   96/  306]
train() client id: f_00004-6-3 loss: 0.752005  [  128/  306]
train() client id: f_00004-6-4 loss: 0.782911  [  160/  306]
train() client id: f_00004-6-5 loss: 0.784844  [  192/  306]
train() client id: f_00004-6-6 loss: 0.830385  [  224/  306]
train() client id: f_00004-6-7 loss: 0.804536  [  256/  306]
train() client id: f_00004-6-8 loss: 0.747723  [  288/  306]
train() client id: f_00004-7-0 loss: 0.696221  [   32/  306]
train() client id: f_00004-7-1 loss: 0.740096  [   64/  306]
train() client id: f_00004-7-2 loss: 0.860726  [   96/  306]
train() client id: f_00004-7-3 loss: 0.690078  [  128/  306]
train() client id: f_00004-7-4 loss: 0.804505  [  160/  306]
train() client id: f_00004-7-5 loss: 1.000030  [  192/  306]
train() client id: f_00004-7-6 loss: 0.807451  [  224/  306]
train() client id: f_00004-7-7 loss: 0.590372  [  256/  306]
train() client id: f_00004-7-8 loss: 0.882210  [  288/  306]
train() client id: f_00004-8-0 loss: 0.911112  [   32/  306]
train() client id: f_00004-8-1 loss: 0.807305  [   64/  306]
train() client id: f_00004-8-2 loss: 0.831924  [   96/  306]
train() client id: f_00004-8-3 loss: 0.779911  [  128/  306]
train() client id: f_00004-8-4 loss: 0.662253  [  160/  306]
train() client id: f_00004-8-5 loss: 0.851277  [  192/  306]
train() client id: f_00004-8-6 loss: 0.763322  [  224/  306]
train() client id: f_00004-8-7 loss: 0.741305  [  256/  306]
train() client id: f_00004-8-8 loss: 0.788820  [  288/  306]
train() client id: f_00004-9-0 loss: 0.745466  [   32/  306]
train() client id: f_00004-9-1 loss: 0.645174  [   64/  306]
train() client id: f_00004-9-2 loss: 0.797726  [   96/  306]
train() client id: f_00004-9-3 loss: 0.841803  [  128/  306]
train() client id: f_00004-9-4 loss: 0.836402  [  160/  306]
train() client id: f_00004-9-5 loss: 0.866836  [  192/  306]
train() client id: f_00004-9-6 loss: 0.841295  [  224/  306]
train() client id: f_00004-9-7 loss: 0.660448  [  256/  306]
train() client id: f_00004-9-8 loss: 0.792755  [  288/  306]
train() client id: f_00004-10-0 loss: 0.772865  [   32/  306]
train() client id: f_00004-10-1 loss: 0.652631  [   64/  306]
train() client id: f_00004-10-2 loss: 0.882406  [   96/  306]
train() client id: f_00004-10-3 loss: 0.882881  [  128/  306]
train() client id: f_00004-10-4 loss: 0.810395  [  160/  306]
train() client id: f_00004-10-5 loss: 0.816020  [  192/  306]
train() client id: f_00004-10-6 loss: 0.747264  [  224/  306]
train() client id: f_00004-10-7 loss: 0.818517  [  256/  306]
train() client id: f_00004-10-8 loss: 0.822860  [  288/  306]
train() client id: f_00004-11-0 loss: 0.734420  [   32/  306]
train() client id: f_00004-11-1 loss: 0.783660  [   64/  306]
train() client id: f_00004-11-2 loss: 0.843395  [   96/  306]
train() client id: f_00004-11-3 loss: 0.669915  [  128/  306]
train() client id: f_00004-11-4 loss: 0.754030  [  160/  306]
train() client id: f_00004-11-5 loss: 0.771968  [  192/  306]
train() client id: f_00004-11-6 loss: 0.839699  [  224/  306]
train() client id: f_00004-11-7 loss: 0.743994  [  256/  306]
train() client id: f_00004-11-8 loss: 0.911820  [  288/  306]
train() client id: f_00004-12-0 loss: 0.765029  [   32/  306]
train() client id: f_00004-12-1 loss: 0.761190  [   64/  306]
train() client id: f_00004-12-2 loss: 0.884754  [   96/  306]
train() client id: f_00004-12-3 loss: 0.740380  [  128/  306]
train() client id: f_00004-12-4 loss: 0.836660  [  160/  306]
train() client id: f_00004-12-5 loss: 0.749032  [  192/  306]
train() client id: f_00004-12-6 loss: 0.768572  [  224/  306]
train() client id: f_00004-12-7 loss: 0.830643  [  256/  306]
train() client id: f_00004-12-8 loss: 0.750020  [  288/  306]
train() client id: f_00005-0-0 loss: 0.526326  [   32/  146]
train() client id: f_00005-0-1 loss: 0.271460  [   64/  146]
train() client id: f_00005-0-2 loss: 0.131683  [   96/  146]
train() client id: f_00005-0-3 loss: 0.343612  [  128/  146]
train() client id: f_00005-1-0 loss: 0.522927  [   32/  146]
train() client id: f_00005-1-1 loss: 0.276452  [   64/  146]
train() client id: f_00005-1-2 loss: 0.354118  [   96/  146]
train() client id: f_00005-1-3 loss: 0.307305  [  128/  146]
train() client id: f_00005-2-0 loss: 0.159118  [   32/  146]
train() client id: f_00005-2-1 loss: 0.365370  [   64/  146]
train() client id: f_00005-2-2 loss: 0.373926  [   96/  146]
train() client id: f_00005-2-3 loss: 0.396517  [  128/  146]
train() client id: f_00005-3-0 loss: 0.516179  [   32/  146]
train() client id: f_00005-3-1 loss: 0.259477  [   64/  146]
train() client id: f_00005-3-2 loss: 0.281081  [   96/  146]
train() client id: f_00005-3-3 loss: 0.367138  [  128/  146]
train() client id: f_00005-4-0 loss: 0.412937  [   32/  146]
train() client id: f_00005-4-1 loss: 0.332881  [   64/  146]
train() client id: f_00005-4-2 loss: 0.387053  [   96/  146]
train() client id: f_00005-4-3 loss: 0.263522  [  128/  146]
train() client id: f_00005-5-0 loss: 0.261692  [   32/  146]
train() client id: f_00005-5-1 loss: 0.357855  [   64/  146]
train() client id: f_00005-5-2 loss: 0.230575  [   96/  146]
train() client id: f_00005-5-3 loss: 0.460018  [  128/  146]
train() client id: f_00005-6-0 loss: 0.411225  [   32/  146]
train() client id: f_00005-6-1 loss: 0.162096  [   64/  146]
train() client id: f_00005-6-2 loss: 0.656000  [   96/  146]
train() client id: f_00005-6-3 loss: 0.163753  [  128/  146]
train() client id: f_00005-7-0 loss: 0.483838  [   32/  146]
train() client id: f_00005-7-1 loss: 0.448560  [   64/  146]
train() client id: f_00005-7-2 loss: 0.304978  [   96/  146]
train() client id: f_00005-7-3 loss: 0.266691  [  128/  146]
train() client id: f_00005-8-0 loss: 0.419272  [   32/  146]
train() client id: f_00005-8-1 loss: 0.487655  [   64/  146]
train() client id: f_00005-8-2 loss: 0.345915  [   96/  146]
train() client id: f_00005-8-3 loss: 0.150207  [  128/  146]
train() client id: f_00005-9-0 loss: 0.324710  [   32/  146]
train() client id: f_00005-9-1 loss: 0.841130  [   64/  146]
train() client id: f_00005-9-2 loss: 0.119858  [   96/  146]
train() client id: f_00005-9-3 loss: 0.156668  [  128/  146]
train() client id: f_00005-10-0 loss: 0.129749  [   32/  146]
train() client id: f_00005-10-1 loss: 0.463488  [   64/  146]
train() client id: f_00005-10-2 loss: 0.283066  [   96/  146]
train() client id: f_00005-10-3 loss: 0.375308  [  128/  146]
train() client id: f_00005-11-0 loss: 0.352179  [   32/  146]
train() client id: f_00005-11-1 loss: 0.301145  [   64/  146]
train() client id: f_00005-11-2 loss: 0.325229  [   96/  146]
train() client id: f_00005-11-3 loss: 0.252709  [  128/  146]
train() client id: f_00005-12-0 loss: 0.397785  [   32/  146]
train() client id: f_00005-12-1 loss: 0.268079  [   64/  146]
train() client id: f_00005-12-2 loss: 0.517823  [   96/  146]
train() client id: f_00005-12-3 loss: 0.125763  [  128/  146]
train() client id: f_00006-0-0 loss: 0.531309  [   32/   54]
train() client id: f_00006-1-0 loss: 0.456073  [   32/   54]
train() client id: f_00006-2-0 loss: 0.472619  [   32/   54]
train() client id: f_00006-3-0 loss: 0.484268  [   32/   54]
train() client id: f_00006-4-0 loss: 0.487066  [   32/   54]
train() client id: f_00006-5-0 loss: 0.515879  [   32/   54]
train() client id: f_00006-6-0 loss: 0.526357  [   32/   54]
train() client id: f_00006-7-0 loss: 0.460500  [   32/   54]
train() client id: f_00006-8-0 loss: 0.515502  [   32/   54]
train() client id: f_00006-9-0 loss: 0.517717  [   32/   54]
train() client id: f_00006-10-0 loss: 0.501835  [   32/   54]
train() client id: f_00006-11-0 loss: 0.470620  [   32/   54]
train() client id: f_00006-12-0 loss: 0.434193  [   32/   54]
train() client id: f_00007-0-0 loss: 0.670924  [   32/  179]
train() client id: f_00007-0-1 loss: 0.518151  [   64/  179]
train() client id: f_00007-0-2 loss: 0.683746  [   96/  179]
train() client id: f_00007-0-3 loss: 0.492627  [  128/  179]
train() client id: f_00007-0-4 loss: 0.632467  [  160/  179]
train() client id: f_00007-1-0 loss: 0.604304  [   32/  179]
train() client id: f_00007-1-1 loss: 0.534438  [   64/  179]
train() client id: f_00007-1-2 loss: 0.703978  [   96/  179]
train() client id: f_00007-1-3 loss: 0.516984  [  128/  179]
train() client id: f_00007-1-4 loss: 0.521713  [  160/  179]
train() client id: f_00007-2-0 loss: 0.563163  [   32/  179]
train() client id: f_00007-2-1 loss: 0.508036  [   64/  179]
train() client id: f_00007-2-2 loss: 0.642059  [   96/  179]
train() client id: f_00007-2-3 loss: 0.512761  [  128/  179]
train() client id: f_00007-2-4 loss: 0.449695  [  160/  179]
train() client id: f_00007-3-0 loss: 0.500250  [   32/  179]
train() client id: f_00007-3-1 loss: 0.393247  [   64/  179]
train() client id: f_00007-3-2 loss: 0.622885  [   96/  179]
train() client id: f_00007-3-3 loss: 0.583251  [  128/  179]
train() client id: f_00007-3-4 loss: 0.632545  [  160/  179]
train() client id: f_00007-4-0 loss: 0.525934  [   32/  179]
train() client id: f_00007-4-1 loss: 0.544723  [   64/  179]
train() client id: f_00007-4-2 loss: 0.665937  [   96/  179]
train() client id: f_00007-4-3 loss: 0.415539  [  128/  179]
train() client id: f_00007-4-4 loss: 0.574803  [  160/  179]
train() client id: f_00007-5-0 loss: 0.547008  [   32/  179]
train() client id: f_00007-5-1 loss: 0.480666  [   64/  179]
train() client id: f_00007-5-2 loss: 0.538472  [   96/  179]
train() client id: f_00007-5-3 loss: 0.686054  [  128/  179]
train() client id: f_00007-5-4 loss: 0.515752  [  160/  179]
train() client id: f_00007-6-0 loss: 0.518854  [   32/  179]
train() client id: f_00007-6-1 loss: 0.472024  [   64/  179]
train() client id: f_00007-6-2 loss: 0.461601  [   96/  179]
train() client id: f_00007-6-3 loss: 0.572703  [  128/  179]
train() client id: f_00007-6-4 loss: 0.618823  [  160/  179]
train() client id: f_00007-7-0 loss: 0.376487  [   32/  179]
train() client id: f_00007-7-1 loss: 0.585739  [   64/  179]
train() client id: f_00007-7-2 loss: 0.367704  [   96/  179]
train() client id: f_00007-7-3 loss: 0.849156  [  128/  179]
train() client id: f_00007-7-4 loss: 0.554236  [  160/  179]
train() client id: f_00007-8-0 loss: 0.502834  [   32/  179]
train() client id: f_00007-8-1 loss: 0.702233  [   64/  179]
train() client id: f_00007-8-2 loss: 0.550951  [   96/  179]
train() client id: f_00007-8-3 loss: 0.473726  [  128/  179]
train() client id: f_00007-8-4 loss: 0.534807  [  160/  179]
train() client id: f_00007-9-0 loss: 0.493704  [   32/  179]
train() client id: f_00007-9-1 loss: 0.588947  [   64/  179]
train() client id: f_00007-9-2 loss: 0.381813  [   96/  179]
train() client id: f_00007-9-3 loss: 0.550552  [  128/  179]
train() client id: f_00007-9-4 loss: 0.633111  [  160/  179]
train() client id: f_00007-10-0 loss: 0.863543  [   32/  179]
train() client id: f_00007-10-1 loss: 0.403098  [   64/  179]
train() client id: f_00007-10-2 loss: 0.463501  [   96/  179]
train() client id: f_00007-10-3 loss: 0.466770  [  128/  179]
train() client id: f_00007-10-4 loss: 0.461431  [  160/  179]
train() client id: f_00007-11-0 loss: 0.719008  [   32/  179]
train() client id: f_00007-11-1 loss: 0.470168  [   64/  179]
train() client id: f_00007-11-2 loss: 0.571858  [   96/  179]
train() client id: f_00007-11-3 loss: 0.379502  [  128/  179]
train() client id: f_00007-11-4 loss: 0.481309  [  160/  179]
train() client id: f_00007-12-0 loss: 0.667971  [   32/  179]
train() client id: f_00007-12-1 loss: 0.581379  [   64/  179]
train() client id: f_00007-12-2 loss: 0.374787  [   96/  179]
train() client id: f_00007-12-3 loss: 0.713372  [  128/  179]
train() client id: f_00007-12-4 loss: 0.401435  [  160/  179]
train() client id: f_00008-0-0 loss: 0.610556  [   32/  130]
train() client id: f_00008-0-1 loss: 0.542206  [   64/  130]
train() client id: f_00008-0-2 loss: 0.627948  [   96/  130]
train() client id: f_00008-0-3 loss: 0.574644  [  128/  130]
train() client id: f_00008-1-0 loss: 0.590061  [   32/  130]
train() client id: f_00008-1-1 loss: 0.509216  [   64/  130]
train() client id: f_00008-1-2 loss: 0.638350  [   96/  130]
train() client id: f_00008-1-3 loss: 0.649365  [  128/  130]
train() client id: f_00008-2-0 loss: 0.607299  [   32/  130]
train() client id: f_00008-2-1 loss: 0.555689  [   64/  130]
train() client id: f_00008-2-2 loss: 0.547860  [   96/  130]
train() client id: f_00008-2-3 loss: 0.686529  [  128/  130]
train() client id: f_00008-3-0 loss: 0.751164  [   32/  130]
train() client id: f_00008-3-1 loss: 0.554316  [   64/  130]
train() client id: f_00008-3-2 loss: 0.516353  [   96/  130]
train() client id: f_00008-3-3 loss: 0.564926  [  128/  130]
train() client id: f_00008-4-0 loss: 0.589830  [   32/  130]
train() client id: f_00008-4-1 loss: 0.588630  [   64/  130]
train() client id: f_00008-4-2 loss: 0.536456  [   96/  130]
train() client id: f_00008-4-3 loss: 0.648623  [  128/  130]
train() client id: f_00008-5-0 loss: 0.572930  [   32/  130]
train() client id: f_00008-5-1 loss: 0.586733  [   64/  130]
train() client id: f_00008-5-2 loss: 0.605610  [   96/  130]
train() client id: f_00008-5-3 loss: 0.620107  [  128/  130]
train() client id: f_00008-6-0 loss: 0.645787  [   32/  130]
train() client id: f_00008-6-1 loss: 0.686338  [   64/  130]
train() client id: f_00008-6-2 loss: 0.499814  [   96/  130]
train() client id: f_00008-6-3 loss: 0.567886  [  128/  130]
train() client id: f_00008-7-0 loss: 0.539418  [   32/  130]
train() client id: f_00008-7-1 loss: 0.563132  [   64/  130]
train() client id: f_00008-7-2 loss: 0.666742  [   96/  130]
train() client id: f_00008-7-3 loss: 0.615679  [  128/  130]
train() client id: f_00008-8-0 loss: 0.611955  [   32/  130]
train() client id: f_00008-8-1 loss: 0.532728  [   64/  130]
train() client id: f_00008-8-2 loss: 0.649793  [   96/  130]
train() client id: f_00008-8-3 loss: 0.537165  [  128/  130]
train() client id: f_00008-9-0 loss: 0.588702  [   32/  130]
train() client id: f_00008-9-1 loss: 0.534428  [   64/  130]
train() client id: f_00008-9-2 loss: 0.576781  [   96/  130]
train() client id: f_00008-9-3 loss: 0.701487  [  128/  130]
train() client id: f_00008-10-0 loss: 0.585627  [   32/  130]
train() client id: f_00008-10-1 loss: 0.568947  [   64/  130]
train() client id: f_00008-10-2 loss: 0.677315  [   96/  130]
train() client id: f_00008-10-3 loss: 0.564200  [  128/  130]
train() client id: f_00008-11-0 loss: 0.654518  [   32/  130]
train() client id: f_00008-11-1 loss: 0.438144  [   64/  130]
train() client id: f_00008-11-2 loss: 0.723649  [   96/  130]
train() client id: f_00008-11-3 loss: 0.588181  [  128/  130]
train() client id: f_00008-12-0 loss: 0.508957  [   32/  130]
train() client id: f_00008-12-1 loss: 0.629002  [   64/  130]
train() client id: f_00008-12-2 loss: 0.617473  [   96/  130]
train() client id: f_00008-12-3 loss: 0.610436  [  128/  130]
train() client id: f_00009-0-0 loss: 1.191439  [   32/  118]
train() client id: f_00009-0-1 loss: 1.103245  [   64/  118]
train() client id: f_00009-0-2 loss: 0.999976  [   96/  118]
train() client id: f_00009-1-0 loss: 1.097946  [   32/  118]
train() client id: f_00009-1-1 loss: 1.133991  [   64/  118]
train() client id: f_00009-1-2 loss: 0.916009  [   96/  118]
train() client id: f_00009-2-0 loss: 0.982521  [   32/  118]
train() client id: f_00009-2-1 loss: 1.055773  [   64/  118]
train() client id: f_00009-2-2 loss: 1.075849  [   96/  118]
train() client id: f_00009-3-0 loss: 0.878044  [   32/  118]
train() client id: f_00009-3-1 loss: 1.037930  [   64/  118]
train() client id: f_00009-3-2 loss: 1.094529  [   96/  118]
train() client id: f_00009-4-0 loss: 1.070401  [   32/  118]
train() client id: f_00009-4-1 loss: 0.979928  [   64/  118]
train() client id: f_00009-4-2 loss: 0.865043  [   96/  118]
train() client id: f_00009-5-0 loss: 1.027970  [   32/  118]
train() client id: f_00009-5-1 loss: 0.936013  [   64/  118]
train() client id: f_00009-5-2 loss: 0.857650  [   96/  118]
train() client id: f_00009-6-0 loss: 0.967430  [   32/  118]
train() client id: f_00009-6-1 loss: 0.826710  [   64/  118]
train() client id: f_00009-6-2 loss: 1.007575  [   96/  118]
train() client id: f_00009-7-0 loss: 0.829910  [   32/  118]
train() client id: f_00009-7-1 loss: 0.945390  [   64/  118]
train() client id: f_00009-7-2 loss: 0.974336  [   96/  118]
train() client id: f_00009-8-0 loss: 0.960993  [   32/  118]
train() client id: f_00009-8-1 loss: 1.012491  [   64/  118]
train() client id: f_00009-8-2 loss: 0.840258  [   96/  118]
train() client id: f_00009-9-0 loss: 0.931690  [   32/  118]
train() client id: f_00009-9-1 loss: 0.921040  [   64/  118]
train() client id: f_00009-9-2 loss: 0.913262  [   96/  118]
train() client id: f_00009-10-0 loss: 0.929721  [   32/  118]
train() client id: f_00009-10-1 loss: 0.944187  [   64/  118]
train() client id: f_00009-10-2 loss: 0.967946  [   96/  118]
train() client id: f_00009-11-0 loss: 0.998984  [   32/  118]
train() client id: f_00009-11-1 loss: 0.781743  [   64/  118]
train() client id: f_00009-11-2 loss: 1.006460  [   96/  118]
train() client id: f_00009-12-0 loss: 0.908719  [   32/  118]
train() client id: f_00009-12-1 loss: 1.113032  [   64/  118]
train() client id: f_00009-12-2 loss: 0.848895  [   96/  118]
At round 44 accuracy: 0.649867374005305
At round 44 training accuracy: 0.590878604963112
At round 44 training loss: 0.8214419125532867
update_location
xs = [  -3.9056584     4.20031788  240.00902392   18.81129433    0.97929623
    3.95640986 -202.44319194 -181.32485185  224.66397685 -167.06087855]
ys = [ 232.5879595   215.55583871    1.32061395 -202.45517586  194.35018685
  177.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [253.20429118 237.6593408  260.01168355 226.58764977 218.57024991
 204.04294985 225.80995661 207.07336418 246.5412186  194.74431699]
dists_bs = [180.52602805 183.73222358 449.88556219 424.3055828  177.03994586
 178.97853524 179.73247853 174.29245025 429.55303777 171.18271783]
uav_gains = [6.96478461e-12 9.36092802e-12 6.06859569e-12 1.13384472e-11
 1.29057031e-11 1.60769736e-11 1.14854179e-11 1.53749023e-11
 7.93441177e-12 1.83978984e-11]
bs_gains = [5.30863138e-11 5.05330089e-11 4.11726034e-12 4.85057916e-12
 5.60653496e-11 5.43815306e-11 5.37452031e-11 5.85752397e-11
 4.68648315e-12 6.16036319e-11]
Round 45
-------------------------------
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.05514024 10.40024199  4.98453475  1.80545975 11.99306906  5.77018381
  2.2328339   7.08469715  5.23316744  4.67883957]
obj_prev = 59.2381676466099
eta_min = 5.885406011502381e-19	eta_max = 0.9363452566333951
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 13.714204272500444	eta = 0.9090909090909091
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 26.6122320197388	eta = 0.4684860112559707
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 20.101928982645475	eta = 0.6202120423522212
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.922786406509566	eta = 0.6588595443458035
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857340066002127	eta = 0.6611461842396013
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857119499819667	eta = 0.6611539174721296
eta = 0.6611539174721296
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.03414319 0.0718091  0.03360124 0.01165204 0.08291919 0.03956275
 0.0146328  0.04850501 0.0352271  0.03197537]
ene_total = [1.73700778 2.97656775 1.73758614 0.82737842 3.38956642 1.75748632
 0.93827146 2.18480132 1.84341633 1.46503756]
ti_comp = [0.61689568 0.66459269 0.61132264 0.63338719 0.66608943 0.66565651
 0.63376527 0.64167568 0.59949544 0.66739459]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.53685718e-06 5.23971089e-05 6.34460979e-06 2.46460673e-07
 8.03119208e-05 8.73451965e-06 4.87534362e-07 1.73224277e-05
 7.60219691e-06 4.58734577e-06]
ene_total = [0.45070637 0.27270761 0.47169887 0.38832777 0.26811959 0.26705379
 0.38691223 0.35773942 0.51631227 0.26034825]
optimize_network iter = 0 obj = 3.639926174690373
eta = 0.6611539174721296
freqs = [27673393.660334   54024893.67528107 27482409.80128485  9198198.6140707
 62243282.60071961 29717090.05574555 11544335.26103987 37795583.8923452
 29380628.13581736 23955372.57683051]
eta_min = 0.6611539174721324	eta_max = 0.6721599416563945
af = 0.005695477061159853	bf = 1.2310181153532935	zeta = 0.006265024767275839	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50574262e-06 1.20694943e-05 1.46145910e-06 5.67713705e-08
 1.84995754e-05 2.01196663e-06 1.12301867e-07 3.99016180e-06
 1.75113998e-06 1.05667936e-06]
ene_total = [1.67921435 1.010801   1.75748063 1.44739012 0.99068256 0.99444728
 1.44208793 1.33153211 1.92363253 0.96990197]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 1 obj = 3.761787113624325
eta = 0.6721599416563945
freqs = [27640992.43136498 53805865.01782712 27460330.69138759  9177789.36033805
 61985734.40435877 29594844.6264841  11518450.63917791 37692613.12413883
 29380628.13581735 23854513.15446249]
eta_min = 0.6721599416563961	eta_max = 0.6721599416563635
af = 0.0056547282477792775	bf = 1.2310181153532935	zeta = 0.006220201072557206	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50221871e-06 1.19718279e-05 1.45911179e-06 5.65197178e-08
 1.83467983e-05 1.99544766e-06 1.11798827e-07 3.96844972e-06
 1.75113998e-06 1.04780021e-06]
ene_total = [1.67921385 1.01078729 1.7574803  1.44739009 0.9906611  0.99444496
 1.44208786 1.33152906 1.92363253 0.96990072]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 2 obj = 3.761787113623971
eta = 0.6721599416563635
freqs = [27640992.43136505 53805865.01782771 27460330.69138763  9177789.3603381
 61985734.40435947 29594844.62648444 11518450.63917798 37692613.1241391
 29380628.13581733 23854513.15446277]
Done!
At round 45 energy consumption: 3.761787113624325
At round 45 eta: 0.6721599416563635
At round 45 local rounds: 13.008276614299735
At round 45 global rounds: 38.94594152753551
At round 45 a_n: 12.425493895665781
gradient difference: 0.3880879282951355
train() client id: f_00000-0-0 loss: 1.317248  [   32/  126]
train() client id: f_00000-0-1 loss: 1.112730  [   64/  126]
train() client id: f_00000-0-2 loss: 1.195465  [   96/  126]
train() client id: f_00000-1-0 loss: 1.178071  [   32/  126]
train() client id: f_00000-1-1 loss: 1.228373  [   64/  126]
train() client id: f_00000-1-2 loss: 0.907962  [   96/  126]
train() client id: f_00000-2-0 loss: 1.039374  [   32/  126]
train() client id: f_00000-2-1 loss: 1.064324  [   64/  126]
train() client id: f_00000-2-2 loss: 0.798369  [   96/  126]
train() client id: f_00000-3-0 loss: 1.116398  [   32/  126]
train() client id: f_00000-3-1 loss: 0.666169  [   64/  126]
train() client id: f_00000-3-2 loss: 0.854385  [   96/  126]
train() client id: f_00000-4-0 loss: 0.901246  [   32/  126]
train() client id: f_00000-4-1 loss: 0.745081  [   64/  126]
train() client id: f_00000-4-2 loss: 0.799231  [   96/  126]
train() client id: f_00000-5-0 loss: 0.811034  [   32/  126]
train() client id: f_00000-5-1 loss: 0.763734  [   64/  126]
train() client id: f_00000-5-2 loss: 0.781790  [   96/  126]
train() client id: f_00000-6-0 loss: 0.669856  [   32/  126]
train() client id: f_00000-6-1 loss: 0.871671  [   64/  126]
train() client id: f_00000-6-2 loss: 0.714719  [   96/  126]
train() client id: f_00000-7-0 loss: 0.760494  [   32/  126]
train() client id: f_00000-7-1 loss: 0.743462  [   64/  126]
train() client id: f_00000-7-2 loss: 0.720993  [   96/  126]
train() client id: f_00000-8-0 loss: 0.682782  [   32/  126]
train() client id: f_00000-8-1 loss: 0.572758  [   64/  126]
train() client id: f_00000-8-2 loss: 0.802499  [   96/  126]
train() client id: f_00000-9-0 loss: 0.667806  [   32/  126]
train() client id: f_00000-9-1 loss: 0.626233  [   64/  126]
train() client id: f_00000-9-2 loss: 0.725908  [   96/  126]
train() client id: f_00000-10-0 loss: 0.570399  [   32/  126]
train() client id: f_00000-10-1 loss: 0.770353  [   64/  126]
train() client id: f_00000-10-2 loss: 0.650075  [   96/  126]
train() client id: f_00000-11-0 loss: 0.590227  [   32/  126]
train() client id: f_00000-11-1 loss: 0.783101  [   64/  126]
train() client id: f_00000-11-2 loss: 0.615617  [   96/  126]
train() client id: f_00000-12-0 loss: 0.541433  [   32/  126]
train() client id: f_00000-12-1 loss: 0.796833  [   64/  126]
train() client id: f_00000-12-2 loss: 0.602632  [   96/  126]
train() client id: f_00001-0-0 loss: 0.524633  [   32/  265]
train() client id: f_00001-0-1 loss: 0.309160  [   64/  265]
train() client id: f_00001-0-2 loss: 0.373591  [   96/  265]
train() client id: f_00001-0-3 loss: 0.313005  [  128/  265]
train() client id: f_00001-0-4 loss: 0.308266  [  160/  265]
train() client id: f_00001-0-5 loss: 0.533143  [  192/  265]
train() client id: f_00001-0-6 loss: 0.432922  [  224/  265]
train() client id: f_00001-0-7 loss: 0.395898  [  256/  265]
train() client id: f_00001-1-0 loss: 0.437028  [   32/  265]
train() client id: f_00001-1-1 loss: 0.435918  [   64/  265]
train() client id: f_00001-1-2 loss: 0.447084  [   96/  265]
train() client id: f_00001-1-3 loss: 0.296872  [  128/  265]
train() client id: f_00001-1-4 loss: 0.379901  [  160/  265]
train() client id: f_00001-1-5 loss: 0.327829  [  192/  265]
train() client id: f_00001-1-6 loss: 0.318210  [  224/  265]
train() client id: f_00001-1-7 loss: 0.436364  [  256/  265]
train() client id: f_00001-2-0 loss: 0.327408  [   32/  265]
train() client id: f_00001-2-1 loss: 0.298043  [   64/  265]
train() client id: f_00001-2-2 loss: 0.434734  [   96/  265]
train() client id: f_00001-2-3 loss: 0.364608  [  128/  265]
train() client id: f_00001-2-4 loss: 0.374678  [  160/  265]
train() client id: f_00001-2-5 loss: 0.337649  [  192/  265]
train() client id: f_00001-2-6 loss: 0.336320  [  224/  265]
train() client id: f_00001-2-7 loss: 0.524950  [  256/  265]
train() client id: f_00001-3-0 loss: 0.470237  [   32/  265]
train() client id: f_00001-3-1 loss: 0.375974  [   64/  265]
train() client id: f_00001-3-2 loss: 0.339650  [   96/  265]
train() client id: f_00001-3-3 loss: 0.308584  [  128/  265]
train() client id: f_00001-3-4 loss: 0.474296  [  160/  265]
train() client id: f_00001-3-5 loss: 0.299174  [  192/  265]
train() client id: f_00001-3-6 loss: 0.305678  [  224/  265]
train() client id: f_00001-3-7 loss: 0.412858  [  256/  265]
train() client id: f_00001-4-0 loss: 0.313592  [   32/  265]
train() client id: f_00001-4-1 loss: 0.431256  [   64/  265]
train() client id: f_00001-4-2 loss: 0.340142  [   96/  265]
train() client id: f_00001-4-3 loss: 0.425919  [  128/  265]
train() client id: f_00001-4-4 loss: 0.312482  [  160/  265]
train() client id: f_00001-4-5 loss: 0.446980  [  192/  265]
train() client id: f_00001-4-6 loss: 0.316548  [  224/  265]
train() client id: f_00001-4-7 loss: 0.333079  [  256/  265]
train() client id: f_00001-5-0 loss: 0.277467  [   32/  265]
train() client id: f_00001-5-1 loss: 0.386476  [   64/  265]
train() client id: f_00001-5-2 loss: 0.284940  [   96/  265]
train() client id: f_00001-5-3 loss: 0.453193  [  128/  265]
train() client id: f_00001-5-4 loss: 0.345270  [  160/  265]
train() client id: f_00001-5-5 loss: 0.498468  [  192/  265]
train() client id: f_00001-5-6 loss: 0.355586  [  224/  265]
train() client id: f_00001-5-7 loss: 0.379132  [  256/  265]
train() client id: f_00001-6-0 loss: 0.477756  [   32/  265]
train() client id: f_00001-6-1 loss: 0.352709  [   64/  265]
train() client id: f_00001-6-2 loss: 0.345663  [   96/  265]
train() client id: f_00001-6-3 loss: 0.532696  [  128/  265]
train() client id: f_00001-6-4 loss: 0.363040  [  160/  265]
train() client id: f_00001-6-5 loss: 0.276522  [  192/  265]
train() client id: f_00001-6-6 loss: 0.318133  [  224/  265]
train() client id: f_00001-6-7 loss: 0.282313  [  256/  265]
train() client id: f_00001-7-0 loss: 0.417974  [   32/  265]
train() client id: f_00001-7-1 loss: 0.297153  [   64/  265]
train() client id: f_00001-7-2 loss: 0.393053  [   96/  265]
train() client id: f_00001-7-3 loss: 0.355213  [  128/  265]
train() client id: f_00001-7-4 loss: 0.407433  [  160/  265]
train() client id: f_00001-7-5 loss: 0.317149  [  192/  265]
train() client id: f_00001-7-6 loss: 0.407853  [  224/  265]
train() client id: f_00001-7-7 loss: 0.332015  [  256/  265]
train() client id: f_00001-8-0 loss: 0.366812  [   32/  265]
train() client id: f_00001-8-1 loss: 0.519674  [   64/  265]
train() client id: f_00001-8-2 loss: 0.419102  [   96/  265]
train() client id: f_00001-8-3 loss: 0.326570  [  128/  265]
train() client id: f_00001-8-4 loss: 0.265735  [  160/  265]
train() client id: f_00001-8-5 loss: 0.353899  [  192/  265]
train() client id: f_00001-8-6 loss: 0.279531  [  224/  265]
train() client id: f_00001-8-7 loss: 0.343399  [  256/  265]
train() client id: f_00001-9-0 loss: 0.358833  [   32/  265]
train() client id: f_00001-9-1 loss: 0.400464  [   64/  265]
train() client id: f_00001-9-2 loss: 0.469366  [   96/  265]
train() client id: f_00001-9-3 loss: 0.398859  [  128/  265]
train() client id: f_00001-9-4 loss: 0.305689  [  160/  265]
train() client id: f_00001-9-5 loss: 0.368158  [  192/  265]
train() client id: f_00001-9-6 loss: 0.325079  [  224/  265]
train() client id: f_00001-9-7 loss: 0.272895  [  256/  265]
train() client id: f_00001-10-0 loss: 0.308765  [   32/  265]
train() client id: f_00001-10-1 loss: 0.414091  [   64/  265]
train() client id: f_00001-10-2 loss: 0.349629  [   96/  265]
train() client id: f_00001-10-3 loss: 0.324438  [  128/  265]
train() client id: f_00001-10-4 loss: 0.317407  [  160/  265]
train() client id: f_00001-10-5 loss: 0.359759  [  192/  265]
train() client id: f_00001-10-6 loss: 0.311350  [  224/  265]
train() client id: f_00001-10-7 loss: 0.419199  [  256/  265]
train() client id: f_00001-11-0 loss: 0.513381  [   32/  265]
train() client id: f_00001-11-1 loss: 0.424555  [   64/  265]
train() client id: f_00001-11-2 loss: 0.324708  [   96/  265]
train() client id: f_00001-11-3 loss: 0.282619  [  128/  265]
train() client id: f_00001-11-4 loss: 0.304151  [  160/  265]
train() client id: f_00001-11-5 loss: 0.275033  [  192/  265]
train() client id: f_00001-11-6 loss: 0.286615  [  224/  265]
train() client id: f_00001-11-7 loss: 0.480304  [  256/  265]
train() client id: f_00001-12-0 loss: 0.392036  [   32/  265]
train() client id: f_00001-12-1 loss: 0.338133  [   64/  265]
train() client id: f_00001-12-2 loss: 0.312916  [   96/  265]
train() client id: f_00001-12-3 loss: 0.281965  [  128/  265]
train() client id: f_00001-12-4 loss: 0.416681  [  160/  265]
train() client id: f_00001-12-5 loss: 0.382317  [  192/  265]
train() client id: f_00001-12-6 loss: 0.265183  [  224/  265]
train() client id: f_00001-12-7 loss: 0.448409  [  256/  265]
train() client id: f_00002-0-0 loss: 1.181228  [   32/  124]
train() client id: f_00002-0-1 loss: 1.153719  [   64/  124]
train() client id: f_00002-0-2 loss: 1.214452  [   96/  124]
train() client id: f_00002-1-0 loss: 1.186538  [   32/  124]
train() client id: f_00002-1-1 loss: 1.056629  [   64/  124]
train() client id: f_00002-1-2 loss: 1.192292  [   96/  124]
train() client id: f_00002-2-0 loss: 1.167110  [   32/  124]
train() client id: f_00002-2-1 loss: 1.165237  [   64/  124]
train() client id: f_00002-2-2 loss: 1.088886  [   96/  124]
train() client id: f_00002-3-0 loss: 0.959929  [   32/  124]
train() client id: f_00002-3-1 loss: 1.188830  [   64/  124]
train() client id: f_00002-3-2 loss: 0.877295  [   96/  124]
train() client id: f_00002-4-0 loss: 1.020248  [   32/  124]
train() client id: f_00002-4-1 loss: 1.049702  [   64/  124]
train() client id: f_00002-4-2 loss: 1.010478  [   96/  124]
train() client id: f_00002-5-0 loss: 1.160789  [   32/  124]
train() client id: f_00002-5-1 loss: 0.881792  [   64/  124]
train() client id: f_00002-5-2 loss: 0.979848  [   96/  124]
train() client id: f_00002-6-0 loss: 0.917426  [   32/  124]
train() client id: f_00002-6-1 loss: 0.975069  [   64/  124]
train() client id: f_00002-6-2 loss: 0.980973  [   96/  124]
train() client id: f_00002-7-0 loss: 0.926694  [   32/  124]
train() client id: f_00002-7-1 loss: 0.887529  [   64/  124]
train() client id: f_00002-7-2 loss: 0.981928  [   96/  124]
train() client id: f_00002-8-0 loss: 0.823207  [   32/  124]
train() client id: f_00002-8-1 loss: 1.069509  [   64/  124]
train() client id: f_00002-8-2 loss: 1.039974  [   96/  124]
train() client id: f_00002-9-0 loss: 0.978326  [   32/  124]
train() client id: f_00002-9-1 loss: 0.897452  [   64/  124]
train() client id: f_00002-9-2 loss: 0.937111  [   96/  124]
train() client id: f_00002-10-0 loss: 0.850929  [   32/  124]
train() client id: f_00002-10-1 loss: 0.834024  [   64/  124]
train() client id: f_00002-10-2 loss: 1.016489  [   96/  124]
train() client id: f_00002-11-0 loss: 0.805251  [   32/  124]
train() client id: f_00002-11-1 loss: 1.071975  [   64/  124]
train() client id: f_00002-11-2 loss: 0.814862  [   96/  124]
train() client id: f_00002-12-0 loss: 0.947388  [   32/  124]
train() client id: f_00002-12-1 loss: 0.958573  [   64/  124]
train() client id: f_00002-12-2 loss: 1.013233  [   96/  124]
train() client id: f_00003-0-0 loss: 0.729830  [   32/   43]
train() client id: f_00003-1-0 loss: 0.743077  [   32/   43]
train() client id: f_00003-2-0 loss: 0.762509  [   32/   43]
train() client id: f_00003-3-0 loss: 0.688029  [   32/   43]
train() client id: f_00003-4-0 loss: 0.621338  [   32/   43]
train() client id: f_00003-5-0 loss: 0.851594  [   32/   43]
train() client id: f_00003-6-0 loss: 0.533367  [   32/   43]
train() client id: f_00003-7-0 loss: 0.541792  [   32/   43]
train() client id: f_00003-8-0 loss: 0.763658  [   32/   43]
train() client id: f_00003-9-0 loss: 0.659128  [   32/   43]
train() client id: f_00003-10-0 loss: 0.609696  [   32/   43]
train() client id: f_00003-11-0 loss: 0.559219  [   32/   43]
train() client id: f_00003-12-0 loss: 0.656964  [   32/   43]
train() client id: f_00004-0-0 loss: 0.881444  [   32/  306]
train() client id: f_00004-0-1 loss: 0.855650  [   64/  306]
train() client id: f_00004-0-2 loss: 0.951086  [   96/  306]
train() client id: f_00004-0-3 loss: 0.848148  [  128/  306]
train() client id: f_00004-0-4 loss: 0.961627  [  160/  306]
train() client id: f_00004-0-5 loss: 0.891893  [  192/  306]
train() client id: f_00004-0-6 loss: 1.058246  [  224/  306]
train() client id: f_00004-0-7 loss: 0.903899  [  256/  306]
train() client id: f_00004-0-8 loss: 0.806049  [  288/  306]
train() client id: f_00004-1-0 loss: 1.003416  [   32/  306]
train() client id: f_00004-1-1 loss: 0.853985  [   64/  306]
train() client id: f_00004-1-2 loss: 0.852042  [   96/  306]
train() client id: f_00004-1-3 loss: 0.829044  [  128/  306]
train() client id: f_00004-1-4 loss: 0.849782  [  160/  306]
train() client id: f_00004-1-5 loss: 0.927258  [  192/  306]
train() client id: f_00004-1-6 loss: 0.893570  [  224/  306]
train() client id: f_00004-1-7 loss: 0.821032  [  256/  306]
train() client id: f_00004-1-8 loss: 0.937825  [  288/  306]
train() client id: f_00004-2-0 loss: 0.767559  [   32/  306]
train() client id: f_00004-2-1 loss: 0.707566  [   64/  306]
train() client id: f_00004-2-2 loss: 0.856749  [   96/  306]
train() client id: f_00004-2-3 loss: 0.972629  [  128/  306]
train() client id: f_00004-2-4 loss: 1.049520  [  160/  306]
train() client id: f_00004-2-5 loss: 0.878923  [  192/  306]
train() client id: f_00004-2-6 loss: 0.886956  [  224/  306]
train() client id: f_00004-2-7 loss: 0.824382  [  256/  306]
train() client id: f_00004-2-8 loss: 1.034138  [  288/  306]
train() client id: f_00004-3-0 loss: 0.887728  [   32/  306]
train() client id: f_00004-3-1 loss: 0.846091  [   64/  306]
train() client id: f_00004-3-2 loss: 0.831714  [   96/  306]
train() client id: f_00004-3-3 loss: 0.886655  [  128/  306]
train() client id: f_00004-3-4 loss: 0.928936  [  160/  306]
train() client id: f_00004-3-5 loss: 1.108044  [  192/  306]
train() client id: f_00004-3-6 loss: 0.810665  [  224/  306]
train() client id: f_00004-3-7 loss: 0.767491  [  256/  306]
train() client id: f_00004-3-8 loss: 0.869085  [  288/  306]
train() client id: f_00004-4-0 loss: 0.822930  [   32/  306]
train() client id: f_00004-4-1 loss: 0.966034  [   64/  306]
train() client id: f_00004-4-2 loss: 0.819607  [   96/  306]
train() client id: f_00004-4-3 loss: 0.837344  [  128/  306]
train() client id: f_00004-4-4 loss: 0.842107  [  160/  306]
train() client id: f_00004-4-5 loss: 0.865963  [  192/  306]
train() client id: f_00004-4-6 loss: 0.967966  [  224/  306]
train() client id: f_00004-4-7 loss: 0.895854  [  256/  306]
train() client id: f_00004-4-8 loss: 0.889671  [  288/  306]
train() client id: f_00004-5-0 loss: 0.800021  [   32/  306]
train() client id: f_00004-5-1 loss: 0.879133  [   64/  306]
train() client id: f_00004-5-2 loss: 0.903091  [   96/  306]
train() client id: f_00004-5-3 loss: 0.944355  [  128/  306]
train() client id: f_00004-5-4 loss: 0.882394  [  160/  306]
train() client id: f_00004-5-5 loss: 0.768326  [  192/  306]
train() client id: f_00004-5-6 loss: 0.993741  [  224/  306]
train() client id: f_00004-5-7 loss: 0.847423  [  256/  306]
train() client id: f_00004-5-8 loss: 0.869441  [  288/  306]
train() client id: f_00004-6-0 loss: 0.897200  [   32/  306]
train() client id: f_00004-6-1 loss: 0.837595  [   64/  306]
train() client id: f_00004-6-2 loss: 0.864600  [   96/  306]
train() client id: f_00004-6-3 loss: 0.806848  [  128/  306]
train() client id: f_00004-6-4 loss: 0.891320  [  160/  306]
train() client id: f_00004-6-5 loss: 1.076619  [  192/  306]
train() client id: f_00004-6-6 loss: 0.913422  [  224/  306]
train() client id: f_00004-6-7 loss: 0.823638  [  256/  306]
train() client id: f_00004-6-8 loss: 0.856761  [  288/  306]
train() client id: f_00004-7-0 loss: 1.000141  [   32/  306]
train() client id: f_00004-7-1 loss: 0.836663  [   64/  306]
train() client id: f_00004-7-2 loss: 0.906141  [   96/  306]
train() client id: f_00004-7-3 loss: 0.896572  [  128/  306]
train() client id: f_00004-7-4 loss: 0.968983  [  160/  306]
train() client id: f_00004-7-5 loss: 0.818978  [  192/  306]
train() client id: f_00004-7-6 loss: 0.920954  [  224/  306]
train() client id: f_00004-7-7 loss: 0.834630  [  256/  306]
train() client id: f_00004-7-8 loss: 0.730620  [  288/  306]
train() client id: f_00004-8-0 loss: 0.873791  [   32/  306]
train() client id: f_00004-8-1 loss: 0.804968  [   64/  306]
train() client id: f_00004-8-2 loss: 0.935405  [   96/  306]
train() client id: f_00004-8-3 loss: 0.839911  [  128/  306]
train() client id: f_00004-8-4 loss: 0.907865  [  160/  306]
train() client id: f_00004-8-5 loss: 0.913499  [  192/  306]
train() client id: f_00004-8-6 loss: 0.947216  [  224/  306]
train() client id: f_00004-8-7 loss: 0.842244  [  256/  306]
train() client id: f_00004-8-8 loss: 0.867084  [  288/  306]
train() client id: f_00004-9-0 loss: 0.939076  [   32/  306]
train() client id: f_00004-9-1 loss: 0.907290  [   64/  306]
train() client id: f_00004-9-2 loss: 0.897664  [   96/  306]
train() client id: f_00004-9-3 loss: 0.984897  [  128/  306]
train() client id: f_00004-9-4 loss: 0.841201  [  160/  306]
train() client id: f_00004-9-5 loss: 0.917894  [  192/  306]
train() client id: f_00004-9-6 loss: 0.764611  [  224/  306]
train() client id: f_00004-9-7 loss: 0.874039  [  256/  306]
train() client id: f_00004-9-8 loss: 0.829789  [  288/  306]
train() client id: f_00004-10-0 loss: 0.944798  [   32/  306]
train() client id: f_00004-10-1 loss: 0.885675  [   64/  306]
train() client id: f_00004-10-2 loss: 0.782734  [   96/  306]
train() client id: f_00004-10-3 loss: 0.929647  [  128/  306]
train() client id: f_00004-10-4 loss: 0.838619  [  160/  306]
train() client id: f_00004-10-5 loss: 1.015445  [  192/  306]
train() client id: f_00004-10-6 loss: 0.879927  [  224/  306]
train() client id: f_00004-10-7 loss: 0.853208  [  256/  306]
train() client id: f_00004-10-8 loss: 0.741172  [  288/  306]
train() client id: f_00004-11-0 loss: 0.960092  [   32/  306]
train() client id: f_00004-11-1 loss: 0.877933  [   64/  306]
train() client id: f_00004-11-2 loss: 0.801504  [   96/  306]
train() client id: f_00004-11-3 loss: 0.774578  [  128/  306]
train() client id: f_00004-11-4 loss: 0.836118  [  160/  306]
train() client id: f_00004-11-5 loss: 0.873418  [  192/  306]
train() client id: f_00004-11-6 loss: 0.946787  [  224/  306]
train() client id: f_00004-11-7 loss: 0.863902  [  256/  306]
train() client id: f_00004-11-8 loss: 0.833105  [  288/  306]
train() client id: f_00004-12-0 loss: 0.902191  [   32/  306]
train() client id: f_00004-12-1 loss: 1.000152  [   64/  306]
train() client id: f_00004-12-2 loss: 0.805435  [   96/  306]
train() client id: f_00004-12-3 loss: 0.891138  [  128/  306]
train() client id: f_00004-12-4 loss: 0.760418  [  160/  306]
train() client id: f_00004-12-5 loss: 0.966376  [  192/  306]
train() client id: f_00004-12-6 loss: 0.937923  [  224/  306]
train() client id: f_00004-12-7 loss: 0.761639  [  256/  306]
train() client id: f_00004-12-8 loss: 0.861155  [  288/  306]
train() client id: f_00005-0-0 loss: 1.174926  [   32/  146]
train() client id: f_00005-0-1 loss: 0.599030  [   64/  146]
train() client id: f_00005-0-2 loss: 0.572614  [   96/  146]
train() client id: f_00005-0-3 loss: 0.601849  [  128/  146]
train() client id: f_00005-1-0 loss: 0.805109  [   32/  146]
train() client id: f_00005-1-1 loss: 0.765016  [   64/  146]
train() client id: f_00005-1-2 loss: 0.518158  [   96/  146]
train() client id: f_00005-1-3 loss: 1.195056  [  128/  146]
train() client id: f_00005-2-0 loss: 0.768383  [   32/  146]
train() client id: f_00005-2-1 loss: 0.827526  [   64/  146]
train() client id: f_00005-2-2 loss: 0.878820  [   96/  146]
train() client id: f_00005-2-3 loss: 0.698561  [  128/  146]
train() client id: f_00005-3-0 loss: 0.782306  [   32/  146]
train() client id: f_00005-3-1 loss: 0.917960  [   64/  146]
train() client id: f_00005-3-2 loss: 0.533082  [   96/  146]
train() client id: f_00005-3-3 loss: 0.966990  [  128/  146]
train() client id: f_00005-4-0 loss: 0.794129  [   32/  146]
train() client id: f_00005-4-1 loss: 0.916077  [   64/  146]
train() client id: f_00005-4-2 loss: 0.601057  [   96/  146]
train() client id: f_00005-4-3 loss: 0.701988  [  128/  146]
train() client id: f_00005-5-0 loss: 0.638719  [   32/  146]
train() client id: f_00005-5-1 loss: 0.730216  [   64/  146]
train() client id: f_00005-5-2 loss: 0.735364  [   96/  146]
train() client id: f_00005-5-3 loss: 0.890971  [  128/  146]
train() client id: f_00005-6-0 loss: 1.013633  [   32/  146]
train() client id: f_00005-6-1 loss: 0.687750  [   64/  146]
train() client id: f_00005-6-2 loss: 0.839815  [   96/  146]
train() client id: f_00005-6-3 loss: 0.724799  [  128/  146]
train() client id: f_00005-7-0 loss: 0.858355  [   32/  146]
train() client id: f_00005-7-1 loss: 0.803359  [   64/  146]
train() client id: f_00005-7-2 loss: 0.701826  [   96/  146]
train() client id: f_00005-7-3 loss: 0.823440  [  128/  146]
train() client id: f_00005-8-0 loss: 0.707267  [   32/  146]
train() client id: f_00005-8-1 loss: 0.860162  [   64/  146]
train() client id: f_00005-8-2 loss: 0.729222  [   96/  146]
train() client id: f_00005-8-3 loss: 0.762755  [  128/  146]
train() client id: f_00005-9-0 loss: 1.026530  [   32/  146]
train() client id: f_00005-9-1 loss: 0.989706  [   64/  146]
train() client id: f_00005-9-2 loss: 0.700202  [   96/  146]
train() client id: f_00005-9-3 loss: 0.439600  [  128/  146]
train() client id: f_00005-10-0 loss: 0.499904  [   32/  146]
train() client id: f_00005-10-1 loss: 0.659013  [   64/  146]
train() client id: f_00005-10-2 loss: 1.147021  [   96/  146]
train() client id: f_00005-10-3 loss: 0.691045  [  128/  146]
train() client id: f_00005-11-0 loss: 1.008780  [   32/  146]
train() client id: f_00005-11-1 loss: 0.689038  [   64/  146]
train() client id: f_00005-11-2 loss: 0.665890  [   96/  146]
train() client id: f_00005-11-3 loss: 0.666015  [  128/  146]
train() client id: f_00005-12-0 loss: 0.742607  [   32/  146]
train() client id: f_00005-12-1 loss: 1.075506  [   64/  146]
train() client id: f_00005-12-2 loss: 0.526218  [   96/  146]
train() client id: f_00005-12-3 loss: 0.941011  [  128/  146]
train() client id: f_00006-0-0 loss: 0.504536  [   32/   54]
train() client id: f_00006-1-0 loss: 0.531150  [   32/   54]
train() client id: f_00006-2-0 loss: 0.499308  [   32/   54]
train() client id: f_00006-3-0 loss: 0.431769  [   32/   54]
train() client id: f_00006-4-0 loss: 0.501211  [   32/   54]
train() client id: f_00006-5-0 loss: 0.564834  [   32/   54]
train() client id: f_00006-6-0 loss: 0.425411  [   32/   54]
train() client id: f_00006-7-0 loss: 0.508777  [   32/   54]
train() client id: f_00006-8-0 loss: 0.451696  [   32/   54]
train() client id: f_00006-9-0 loss: 0.450876  [   32/   54]
train() client id: f_00006-10-0 loss: 0.498495  [   32/   54]
train() client id: f_00006-11-0 loss: 0.508230  [   32/   54]
train() client id: f_00006-12-0 loss: 0.503016  [   32/   54]
train() client id: f_00007-0-0 loss: 0.919531  [   32/  179]
train() client id: f_00007-0-1 loss: 0.701251  [   64/  179]
train() client id: f_00007-0-2 loss: 0.554058  [   96/  179]
train() client id: f_00007-0-3 loss: 0.561707  [  128/  179]
train() client id: f_00007-0-4 loss: 0.772779  [  160/  179]
train() client id: f_00007-1-0 loss: 0.643621  [   32/  179]
train() client id: f_00007-1-1 loss: 0.524310  [   64/  179]
train() client id: f_00007-1-2 loss: 0.790936  [   96/  179]
train() client id: f_00007-1-3 loss: 0.690534  [  128/  179]
train() client id: f_00007-1-4 loss: 0.742875  [  160/  179]
train() client id: f_00007-2-0 loss: 0.645171  [   32/  179]
train() client id: f_00007-2-1 loss: 0.801944  [   64/  179]
train() client id: f_00007-2-2 loss: 0.673462  [   96/  179]
train() client id: f_00007-2-3 loss: 0.545295  [  128/  179]
train() client id: f_00007-2-4 loss: 0.617657  [  160/  179]
train() client id: f_00007-3-0 loss: 0.873332  [   32/  179]
train() client id: f_00007-3-1 loss: 0.813624  [   64/  179]
train() client id: f_00007-3-2 loss: 0.702559  [   96/  179]
train() client id: f_00007-3-3 loss: 0.575973  [  128/  179]
train() client id: f_00007-3-4 loss: 0.557084  [  160/  179]
train() client id: f_00007-4-0 loss: 0.526719  [   32/  179]
train() client id: f_00007-4-1 loss: 0.773237  [   64/  179]
train() client id: f_00007-4-2 loss: 0.735595  [   96/  179]
train() client id: f_00007-4-3 loss: 0.687540  [  128/  179]
train() client id: f_00007-4-4 loss: 0.670393  [  160/  179]
train() client id: f_00007-5-0 loss: 0.883482  [   32/  179]
train() client id: f_00007-5-1 loss: 0.609611  [   64/  179]
train() client id: f_00007-5-2 loss: 0.583823  [   96/  179]
train() client id: f_00007-5-3 loss: 0.727722  [  128/  179]
train() client id: f_00007-5-4 loss: 0.587282  [  160/  179]
train() client id: f_00007-6-0 loss: 0.707825  [   32/  179]
train() client id: f_00007-6-1 loss: 0.775776  [   64/  179]
train() client id: f_00007-6-2 loss: 0.516733  [   96/  179]
train() client id: f_00007-6-3 loss: 0.635560  [  128/  179]
train() client id: f_00007-6-4 loss: 0.738591  [  160/  179]
train() client id: f_00007-7-0 loss: 0.793134  [   32/  179]
train() client id: f_00007-7-1 loss: 0.545301  [   64/  179]
train() client id: f_00007-7-2 loss: 0.615740  [   96/  179]
train() client id: f_00007-7-3 loss: 0.595750  [  128/  179]
train() client id: f_00007-7-4 loss: 0.636937  [  160/  179]
train() client id: f_00007-8-0 loss: 0.743076  [   32/  179]
train() client id: f_00007-8-1 loss: 0.615640  [   64/  179]
train() client id: f_00007-8-2 loss: 0.726646  [   96/  179]
train() client id: f_00007-8-3 loss: 0.583455  [  128/  179]
train() client id: f_00007-8-4 loss: 0.614178  [  160/  179]
train() client id: f_00007-9-0 loss: 0.626706  [   32/  179]
train() client id: f_00007-9-1 loss: 0.781129  [   64/  179]
train() client id: f_00007-9-2 loss: 0.744098  [   96/  179]
train() client id: f_00007-9-3 loss: 0.536428  [  128/  179]
train() client id: f_00007-9-4 loss: 0.807905  [  160/  179]
train() client id: f_00007-10-0 loss: 0.627452  [   32/  179]
train() client id: f_00007-10-1 loss: 0.781551  [   64/  179]
train() client id: f_00007-10-2 loss: 0.596582  [   96/  179]
train() client id: f_00007-10-3 loss: 0.691188  [  128/  179]
train() client id: f_00007-10-4 loss: 0.758833  [  160/  179]
train() client id: f_00007-11-0 loss: 0.588762  [   32/  179]
train() client id: f_00007-11-1 loss: 0.659173  [   64/  179]
train() client id: f_00007-11-2 loss: 0.785368  [   96/  179]
train() client id: f_00007-11-3 loss: 0.506169  [  128/  179]
train() client id: f_00007-11-4 loss: 0.742543  [  160/  179]
train() client id: f_00007-12-0 loss: 0.498798  [   32/  179]
train() client id: f_00007-12-1 loss: 0.712788  [   64/  179]
train() client id: f_00007-12-2 loss: 0.550650  [   96/  179]
train() client id: f_00007-12-3 loss: 0.860236  [  128/  179]
train() client id: f_00007-12-4 loss: 0.750294  [  160/  179]
train() client id: f_00008-0-0 loss: 0.648668  [   32/  130]
train() client id: f_00008-0-1 loss: 0.755137  [   64/  130]
train() client id: f_00008-0-2 loss: 0.744858  [   96/  130]
train() client id: f_00008-0-3 loss: 0.661174  [  128/  130]
train() client id: f_00008-1-0 loss: 0.693120  [   32/  130]
train() client id: f_00008-1-1 loss: 0.706005  [   64/  130]
train() client id: f_00008-1-2 loss: 0.760461  [   96/  130]
train() client id: f_00008-1-3 loss: 0.690161  [  128/  130]
train() client id: f_00008-2-0 loss: 0.665333  [   32/  130]
train() client id: f_00008-2-1 loss: 0.692184  [   64/  130]
train() client id: f_00008-2-2 loss: 0.749702  [   96/  130]
train() client id: f_00008-2-3 loss: 0.728655  [  128/  130]
train() client id: f_00008-3-0 loss: 0.679582  [   32/  130]
train() client id: f_00008-3-1 loss: 0.717339  [   64/  130]
train() client id: f_00008-3-2 loss: 0.679340  [   96/  130]
train() client id: f_00008-3-3 loss: 0.739328  [  128/  130]
train() client id: f_00008-4-0 loss: 0.694157  [   32/  130]
train() client id: f_00008-4-1 loss: 0.592245  [   64/  130]
train() client id: f_00008-4-2 loss: 0.811326  [   96/  130]
train() client id: f_00008-4-3 loss: 0.737745  [  128/  130]
train() client id: f_00008-5-0 loss: 0.732279  [   32/  130]
train() client id: f_00008-5-1 loss: 0.699549  [   64/  130]
train() client id: f_00008-5-2 loss: 0.708806  [   96/  130]
train() client id: f_00008-5-3 loss: 0.703748  [  128/  130]
train() client id: f_00008-6-0 loss: 0.666302  [   32/  130]
train() client id: f_00008-6-1 loss: 0.709607  [   64/  130]
train() client id: f_00008-6-2 loss: 0.667254  [   96/  130]
train() client id: f_00008-6-3 loss: 0.795528  [  128/  130]
train() client id: f_00008-7-0 loss: 0.690581  [   32/  130]
train() client id: f_00008-7-1 loss: 0.767532  [   64/  130]
train() client id: f_00008-7-2 loss: 0.680644  [   96/  130]
train() client id: f_00008-7-3 loss: 0.705210  [  128/  130]
train() client id: f_00008-8-0 loss: 0.736166  [   32/  130]
train() client id: f_00008-8-1 loss: 0.750828  [   64/  130]
train() client id: f_00008-8-2 loss: 0.634720  [   96/  130]
train() client id: f_00008-8-3 loss: 0.720725  [  128/  130]
train() client id: f_00008-9-0 loss: 0.870071  [   32/  130]
train() client id: f_00008-9-1 loss: 0.686837  [   64/  130]
train() client id: f_00008-9-2 loss: 0.632544  [   96/  130]
train() client id: f_00008-9-3 loss: 0.647652  [  128/  130]
train() client id: f_00008-10-0 loss: 0.756229  [   32/  130]
train() client id: f_00008-10-1 loss: 0.637585  [   64/  130]
train() client id: f_00008-10-2 loss: 0.679558  [   96/  130]
train() client id: f_00008-10-3 loss: 0.724585  [  128/  130]
train() client id: f_00008-11-0 loss: 0.705137  [   32/  130]
train() client id: f_00008-11-1 loss: 0.812036  [   64/  130]
train() client id: f_00008-11-2 loss: 0.709993  [   96/  130]
train() client id: f_00008-11-3 loss: 0.584775  [  128/  130]
train() client id: f_00008-12-0 loss: 0.584444  [   32/  130]
train() client id: f_00008-12-1 loss: 0.767393  [   64/  130]
train() client id: f_00008-12-2 loss: 0.702663  [   96/  130]
train() client id: f_00008-12-3 loss: 0.785531  [  128/  130]
train() client id: f_00009-0-0 loss: 1.164084  [   32/  118]
train() client id: f_00009-0-1 loss: 1.019899  [   64/  118]
train() client id: f_00009-0-2 loss: 1.065241  [   96/  118]
train() client id: f_00009-1-0 loss: 1.014475  [   32/  118]
train() client id: f_00009-1-1 loss: 0.987270  [   64/  118]
train() client id: f_00009-1-2 loss: 1.006857  [   96/  118]
train() client id: f_00009-2-0 loss: 0.961783  [   32/  118]
train() client id: f_00009-2-1 loss: 1.077896  [   64/  118]
train() client id: f_00009-2-2 loss: 0.935777  [   96/  118]
train() client id: f_00009-3-0 loss: 1.024532  [   32/  118]
train() client id: f_00009-3-1 loss: 1.053243  [   64/  118]
train() client id: f_00009-3-2 loss: 0.950102  [   96/  118]
train() client id: f_00009-4-0 loss: 0.907741  [   32/  118]
train() client id: f_00009-4-1 loss: 1.020961  [   64/  118]
train() client id: f_00009-4-2 loss: 0.857438  [   96/  118]
train() client id: f_00009-5-0 loss: 0.758645  [   32/  118]
train() client id: f_00009-5-1 loss: 0.981846  [   64/  118]
train() client id: f_00009-5-2 loss: 0.957779  [   96/  118]
train() client id: f_00009-6-0 loss: 1.039124  [   32/  118]
train() client id: f_00009-6-1 loss: 0.873530  [   64/  118]
train() client id: f_00009-6-2 loss: 0.830524  [   96/  118]
train() client id: f_00009-7-0 loss: 0.983800  [   32/  118]
train() client id: f_00009-7-1 loss: 0.695154  [   64/  118]
train() client id: f_00009-7-2 loss: 0.931203  [   96/  118]
train() client id: f_00009-8-0 loss: 0.859110  [   32/  118]
train() client id: f_00009-8-1 loss: 0.932232  [   64/  118]
train() client id: f_00009-8-2 loss: 0.862441  [   96/  118]
train() client id: f_00009-9-0 loss: 0.832030  [   32/  118]
train() client id: f_00009-9-1 loss: 0.935302  [   64/  118]
train() client id: f_00009-9-2 loss: 0.831020  [   96/  118]
train() client id: f_00009-10-0 loss: 0.819588  [   32/  118]
train() client id: f_00009-10-1 loss: 0.918273  [   64/  118]
train() client id: f_00009-10-2 loss: 0.804841  [   96/  118]
train() client id: f_00009-11-0 loss: 0.909321  [   32/  118]
train() client id: f_00009-11-1 loss: 0.785570  [   64/  118]
train() client id: f_00009-11-2 loss: 0.802963  [   96/  118]
train() client id: f_00009-12-0 loss: 0.909281  [   32/  118]
train() client id: f_00009-12-1 loss: 0.885725  [   64/  118]
train() client id: f_00009-12-2 loss: 0.829398  [   96/  118]
At round 45 accuracy: 0.649867374005305
At round 45 training accuracy: 0.5928906773977196
At round 45 training loss: 0.8242616987920651
update_location
xs = [  -3.9056584     4.20031788  245.00902392   18.81129433    0.97929623
    3.95640986 -207.44319194 -186.32485185  229.66397685 -172.06087855]
ys = [ 237.5879595   220.55583871    1.32061395 -207.45517586  199.35018685
  182.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [257.80475687 242.20346954 264.63402242 231.06603988 223.02792654
 208.41465133 230.30320976 211.46542666 251.1059781  199.05013887]
dists_bs = [182.18267316 184.90021192 454.50204544 428.75958223 177.65597175
 179.12693148 180.56355047 174.54542885 434.20899579 171.02377515]
uav_gains = [6.34845528e-12 8.61260857e-12 5.51598422e-12 1.05126925e-11
 1.20199406e-11 1.50715939e-11 1.06508766e-11 1.43975841e-11
 7.26046655e-12 1.72893462e-11]
bs_gains = [5.17457027e-11 4.96442942e-11 4.00123195e-12 4.71080726e-12
 5.55227049e-11 5.42554792e-11 5.30554308e-11 5.83378396e-11
 4.54713029e-12 6.17640715e-11]
Round 46
-------------------------------
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.92415982 10.12154902  4.85601333  1.75987582 11.67148702  5.61542073
  2.1757575   6.89673295  5.09466844  4.55326949]
obj_prev = 57.668934107649186
eta_min = 1.934063127362615e-19	eta_max = 0.9372721317732582
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 13.346274362136274	eta = 0.9090909090909091
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 26.11640265875819	eta = 0.46457304443429304
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 19.646759764324543	eta = 0.617556118077178
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.475231020287687	eta = 0.6567158310241378
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40977991233988	eta = 0.6590506106332402
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40955607686498	eta = 0.6590586238034545
eta = 0.6590586238034545
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.03440512 0.07235997 0.03385901 0.01174143 0.08355529 0.03986625
 0.01474505 0.04887711 0.03549735 0.03222067]
ene_total = [1.70303953 2.89994011 1.70508731 0.81177669 3.30204155 1.7110066
 0.91978726 2.13279846 1.79829675 1.42578181]
ti_comp = [0.63680768 0.68794036 0.63078466 0.65472313 0.68956148 0.68923291
 0.65512248 0.66360668 0.62159244 0.69103952]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [6.27671845e-06 5.00349702e-05 6.09733991e-06 2.36008472e-07
 7.66753725e-05 8.33612381e-06 4.66846244e-07 1.65720103e-05
 7.23529432e-06 4.37802116e-06]
ene_total = [0.44938525 0.26462869 0.47132941 0.38387283 0.25969147 0.25839833
 0.38242583 0.35209241 0.50487161 0.25166996]
optimize_network iter = 0 obj = 3.578365787451716
eta = 0.6590586238034545
freqs = [27013744.23254671 52591749.45149608 26838799.91285678  8966713.64201406
 60585816.6007772  28920738.74380525 11253659.91650407 36826870.45355151
 28553553.02519245 23313187.49305714]
eta_min = 0.6590586238034565	eta_max = 0.6785433277513145
af = 0.005254252612989919	bf = 1.2178222333661453	zeta = 0.005779677874288912	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.43481355e-06 1.14376411e-05 1.39380888e-06 5.39498714e-08
 1.75274491e-05 1.90557908e-06 1.06717757e-07 3.78824463e-06
 1.65393723e-06 1.00078475e-06]
ene_total = [1.6846925  0.98717606 1.76700979 1.43963457 0.96585086 0.96820657
 1.43418346 1.3187243  1.89268496 0.94339011]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 1 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
eta_min = 0.6785433277513172	eta_max = 0.6785433277513145
af = 0.005187942505206725	bf = 1.2178222333661453	zeta = 0.005706736755727398	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.42967619e-06 1.12736748e-05 1.39076189e-06 5.35419898e-08
 1.72707080e-05 1.87778663e-06 1.05901756e-07 3.75246172e-06
 1.65393723e-06 9.85841782e-07]
ene_total = [1.6846918  0.98715364 1.76700937 1.43963452 0.96581577 0.96820277
 1.43418335 1.31871941 1.89268496 0.94338807]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 2 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
Done!
At round 46 energy consumption: 3.7947078371699785
At round 46 eta: 0.6785433277513145
At round 46 local rounds: 12.69876962905909
At round 46 global rounds: 38.653712827752926
At round 46 a_n: 12.082948048696464
gradient difference: 0.3583540618419647
train() client id: f_00000-0-0 loss: 1.221659  [   32/  126]
train() client id: f_00000-0-1 loss: 1.068039  [   64/  126]
train() client id: f_00000-0-2 loss: 1.178743  [   96/  126]
train() client id: f_00000-1-0 loss: 1.049715  [   32/  126]
train() client id: f_00000-1-1 loss: 1.011867  [   64/  126]
train() client id: f_00000-1-2 loss: 1.056409  [   96/  126]
train() client id: f_00000-2-0 loss: 1.064527  [   32/  126]
train() client id: f_00000-2-1 loss: 1.086345  [   64/  126]
train() client id: f_00000-2-2 loss: 0.945723  [   96/  126]
train() client id: f_00000-3-0 loss: 1.026112  [   32/  126]
train() client id: f_00000-3-1 loss: 0.890043  [   64/  126]
train() client id: f_00000-3-2 loss: 0.966116  [   96/  126]
train() client id: f_00000-4-0 loss: 1.017671  [   32/  126]
train() client id: f_00000-4-1 loss: 0.927628  [   64/  126]
train() client id: f_00000-4-2 loss: 0.886797  [   96/  126]
train() client id: f_00000-5-0 loss: 0.925841  [   32/  126]
train() client id: f_00000-5-1 loss: 0.745832  [   64/  126]
train() client id: f_00000-5-2 loss: 0.966816  [   96/  126]
train() client id: f_00000-6-0 loss: 1.007164  [   32/  126]
train() client id: f_00000-6-1 loss: 0.740357  [   64/  126]
train() client id: f_00000-6-2 loss: 0.822869  [   96/  126]
train() client id: f_00000-7-0 loss: 0.938098  [   32/  126]
train() client id: f_00000-7-1 loss: 0.821684  [   64/  126]
train() client id: f_00000-7-2 loss: 0.792493  [   96/  126]
train() client id: f_00000-8-0 loss: 0.894445  [   32/  126]
train() client id: f_00000-8-1 loss: 0.812597  [   64/  126]
train() client id: f_00000-8-2 loss: 0.823838  [   96/  126]
train() client id: f_00000-9-0 loss: 0.755958  [   32/  126]
train() client id: f_00000-9-1 loss: 0.882157  [   64/  126]
train() client id: f_00000-9-2 loss: 0.888890  [   96/  126]
train() client id: f_00000-10-0 loss: 0.833581  [   32/  126]
train() client id: f_00000-10-1 loss: 0.928833  [   64/  126]
train() client id: f_00000-10-2 loss: 0.833334  [   96/  126]
train() client id: f_00000-11-0 loss: 0.801156  [   32/  126]
train() client id: f_00000-11-1 loss: 0.879543  [   64/  126]
train() client id: f_00000-11-2 loss: 0.803161  [   96/  126]
train() client id: f_00001-0-0 loss: 0.424552  [   32/  265]
train() client id: f_00001-0-1 loss: 0.410963  [   64/  265]
train() client id: f_00001-0-2 loss: 0.507084  [   96/  265]
train() client id: f_00001-0-3 loss: 0.357852  [  128/  265]
train() client id: f_00001-0-4 loss: 0.416083  [  160/  265]
train() client id: f_00001-0-5 loss: 0.293023  [  192/  265]
train() client id: f_00001-0-6 loss: 0.313844  [  224/  265]
train() client id: f_00001-0-7 loss: 0.472841  [  256/  265]
train() client id: f_00001-1-0 loss: 0.361542  [   32/  265]
train() client id: f_00001-1-1 loss: 0.326770  [   64/  265]
train() client id: f_00001-1-2 loss: 0.524302  [   96/  265]
train() client id: f_00001-1-3 loss: 0.350793  [  128/  265]
train() client id: f_00001-1-4 loss: 0.400265  [  160/  265]
train() client id: f_00001-1-5 loss: 0.370295  [  192/  265]
train() client id: f_00001-1-6 loss: 0.363443  [  224/  265]
train() client id: f_00001-1-7 loss: 0.275621  [  256/  265]
train() client id: f_00001-2-0 loss: 0.424905  [   32/  265]
train() client id: f_00001-2-1 loss: 0.400827  [   64/  265]
train() client id: f_00001-2-2 loss: 0.424342  [   96/  265]
train() client id: f_00001-2-3 loss: 0.325548  [  128/  265]
train() client id: f_00001-2-4 loss: 0.326630  [  160/  265]
train() client id: f_00001-2-5 loss: 0.267074  [  192/  265]
train() client id: f_00001-2-6 loss: 0.407007  [  224/  265]
train() client id: f_00001-2-7 loss: 0.400575  [  256/  265]
train() client id: f_00001-3-0 loss: 0.461698  [   32/  265]
train() client id: f_00001-3-1 loss: 0.337269  [   64/  265]
train() client id: f_00001-3-2 loss: 0.298505  [   96/  265]
train() client id: f_00001-3-3 loss: 0.297115  [  128/  265]
train() client id: f_00001-3-4 loss: 0.454156  [  160/  265]
train() client id: f_00001-3-5 loss: 0.397527  [  192/  265]
train() client id: f_00001-3-6 loss: 0.338968  [  224/  265]
train() client id: f_00001-3-7 loss: 0.370785  [  256/  265]
train() client id: f_00001-4-0 loss: 0.348543  [   32/  265]
train() client id: f_00001-4-1 loss: 0.264056  [   64/  265]
train() client id: f_00001-4-2 loss: 0.402661  [   96/  265]
train() client id: f_00001-4-3 loss: 0.423868  [  128/  265]
train() client id: f_00001-4-4 loss: 0.409719  [  160/  265]
train() client id: f_00001-4-5 loss: 0.398292  [  192/  265]
train() client id: f_00001-4-6 loss: 0.366936  [  224/  265]
train() client id: f_00001-4-7 loss: 0.356502  [  256/  265]
train() client id: f_00001-5-0 loss: 0.402490  [   32/  265]
train() client id: f_00001-5-1 loss: 0.375449  [   64/  265]
train() client id: f_00001-5-2 loss: 0.427933  [   96/  265]
train() client id: f_00001-5-3 loss: 0.371405  [  128/  265]
train() client id: f_00001-5-4 loss: 0.265858  [  160/  265]
train() client id: f_00001-5-5 loss: 0.352865  [  192/  265]
train() client id: f_00001-5-6 loss: 0.375550  [  224/  265]
train() client id: f_00001-5-7 loss: 0.355964  [  256/  265]
train() client id: f_00001-6-0 loss: 0.264990  [   32/  265]
train() client id: f_00001-6-1 loss: 0.311459  [   64/  265]
train() client id: f_00001-6-2 loss: 0.494007  [   96/  265]
train() client id: f_00001-6-3 loss: 0.489207  [  128/  265]
train() client id: f_00001-6-4 loss: 0.345786  [  160/  265]
train() client id: f_00001-6-5 loss: 0.353357  [  192/  265]
train() client id: f_00001-6-6 loss: 0.323138  [  224/  265]
train() client id: f_00001-6-7 loss: 0.306786  [  256/  265]
train() client id: f_00001-7-0 loss: 0.401394  [   32/  265]
train() client id: f_00001-7-1 loss: 0.411929  [   64/  265]
train() client id: f_00001-7-2 loss: 0.382552  [   96/  265]
train() client id: f_00001-7-3 loss: 0.270460  [  128/  265]
train() client id: f_00001-7-4 loss: 0.362048  [  160/  265]
train() client id: f_00001-7-5 loss: 0.380124  [  192/  265]
train() client id: f_00001-7-6 loss: 0.329368  [  224/  265]
train() client id: f_00001-7-7 loss: 0.341582  [  256/  265]
train() client id: f_00001-8-0 loss: 0.383205  [   32/  265]
train() client id: f_00001-8-1 loss: 0.265056  [   64/  265]
train() client id: f_00001-8-2 loss: 0.488063  [   96/  265]
train() client id: f_00001-8-3 loss: 0.251903  [  128/  265]
train() client id: f_00001-8-4 loss: 0.333538  [  160/  265]
train() client id: f_00001-8-5 loss: 0.260607  [  192/  265]
train() client id: f_00001-8-6 loss: 0.378980  [  224/  265]
train() client id: f_00001-8-7 loss: 0.416662  [  256/  265]
train() client id: f_00001-9-0 loss: 0.264436  [   32/  265]
train() client id: f_00001-9-1 loss: 0.506575  [   64/  265]
train() client id: f_00001-9-2 loss: 0.424716  [   96/  265]
train() client id: f_00001-9-3 loss: 0.268431  [  128/  265]
train() client id: f_00001-9-4 loss: 0.285144  [  160/  265]
train() client id: f_00001-9-5 loss: 0.258752  [  192/  265]
train() client id: f_00001-9-6 loss: 0.304677  [  224/  265]
train() client id: f_00001-9-7 loss: 0.433460  [  256/  265]
train() client id: f_00001-10-0 loss: 0.318952  [   32/  265]
train() client id: f_00001-10-1 loss: 0.535958  [   64/  265]
train() client id: f_00001-10-2 loss: 0.317846  [   96/  265]
train() client id: f_00001-10-3 loss: 0.332276  [  128/  265]
train() client id: f_00001-10-4 loss: 0.269555  [  160/  265]
train() client id: f_00001-10-5 loss: 0.335767  [  192/  265]
train() client id: f_00001-10-6 loss: 0.277590  [  224/  265]
train() client id: f_00001-10-7 loss: 0.437931  [  256/  265]
train() client id: f_00001-11-0 loss: 0.469362  [   32/  265]
train() client id: f_00001-11-1 loss: 0.316062  [   64/  265]
train() client id: f_00001-11-2 loss: 0.286973  [   96/  265]
train() client id: f_00001-11-3 loss: 0.296228  [  128/  265]
train() client id: f_00001-11-4 loss: 0.379267  [  160/  265]
train() client id: f_00001-11-5 loss: 0.413644  [  192/  265]
train() client id: f_00001-11-6 loss: 0.328637  [  224/  265]
train() client id: f_00001-11-7 loss: 0.325289  [  256/  265]
train() client id: f_00002-0-0 loss: 1.051702  [   32/  124]
train() client id: f_00002-0-1 loss: 1.062015  [   64/  124]
train() client id: f_00002-0-2 loss: 1.012378  [   96/  124]
train() client id: f_00002-1-0 loss: 1.083122  [   32/  124]
train() client id: f_00002-1-1 loss: 0.858024  [   64/  124]
train() client id: f_00002-1-2 loss: 1.041923  [   96/  124]
train() client id: f_00002-2-0 loss: 0.996335  [   32/  124]
train() client id: f_00002-2-1 loss: 1.127431  [   64/  124]
train() client id: f_00002-2-2 loss: 0.945031  [   96/  124]
train() client id: f_00002-3-0 loss: 0.855243  [   32/  124]
train() client id: f_00002-3-1 loss: 0.878823  [   64/  124]
train() client id: f_00002-3-2 loss: 1.066414  [   96/  124]
train() client id: f_00002-4-0 loss: 1.021667  [   32/  124]
train() client id: f_00002-4-1 loss: 0.915291  [   64/  124]
train() client id: f_00002-4-2 loss: 0.891258  [   96/  124]
train() client id: f_00002-5-0 loss: 0.978188  [   32/  124]
train() client id: f_00002-5-1 loss: 0.882129  [   64/  124]
train() client id: f_00002-5-2 loss: 0.728892  [   96/  124]
train() client id: f_00002-6-0 loss: 0.988626  [   32/  124]
train() client id: f_00002-6-1 loss: 0.707460  [   64/  124]
train() client id: f_00002-6-2 loss: 0.814914  [   96/  124]
train() client id: f_00002-7-0 loss: 0.932978  [   32/  124]
train() client id: f_00002-7-1 loss: 0.882652  [   64/  124]
train() client id: f_00002-7-2 loss: 0.876736  [   96/  124]
train() client id: f_00002-8-0 loss: 0.969589  [   32/  124]
train() client id: f_00002-8-1 loss: 0.907675  [   64/  124]
train() client id: f_00002-8-2 loss: 0.827689  [   96/  124]
train() client id: f_00002-9-0 loss: 0.856966  [   32/  124]
train() client id: f_00002-9-1 loss: 0.804677  [   64/  124]
train() client id: f_00002-9-2 loss: 0.970470  [   96/  124]
train() client id: f_00002-10-0 loss: 0.761556  [   32/  124]
train() client id: f_00002-10-1 loss: 0.951006  [   64/  124]
train() client id: f_00002-10-2 loss: 0.733764  [   96/  124]
train() client id: f_00002-11-0 loss: 0.978906  [   32/  124]
train() client id: f_00002-11-1 loss: 0.840530  [   64/  124]
train() client id: f_00002-11-2 loss: 0.688522  [   96/  124]
train() client id: f_00003-0-0 loss: 0.723761  [   32/   43]
train() client id: f_00003-1-0 loss: 0.722246  [   32/   43]
train() client id: f_00003-2-0 loss: 0.725624  [   32/   43]
train() client id: f_00003-3-0 loss: 0.681704  [   32/   43]
train() client id: f_00003-4-0 loss: 0.821273  [   32/   43]
train() client id: f_00003-5-0 loss: 0.645267  [   32/   43]
train() client id: f_00003-6-0 loss: 0.749682  [   32/   43]
train() client id: f_00003-7-0 loss: 0.735292  [   32/   43]
train() client id: f_00003-8-0 loss: 0.647552  [   32/   43]
train() client id: f_00003-9-0 loss: 0.747750  [   32/   43]
train() client id: f_00003-10-0 loss: 0.742777  [   32/   43]
train() client id: f_00003-11-0 loss: 0.594774  [   32/   43]
train() client id: f_00004-0-0 loss: 0.831307  [   32/  306]
train() client id: f_00004-0-1 loss: 0.942951  [   64/  306]
train() client id: f_00004-0-2 loss: 0.903490  [   96/  306]
train() client id: f_00004-0-3 loss: 0.829386  [  128/  306]
train() client id: f_00004-0-4 loss: 0.959568  [  160/  306]
train() client id: f_00004-0-5 loss: 0.864862  [  192/  306]
train() client id: f_00004-0-6 loss: 0.748856  [  224/  306]
train() client id: f_00004-0-7 loss: 0.804692  [  256/  306]
train() client id: f_00004-0-8 loss: 0.903410  [  288/  306]
train() client id: f_00004-1-0 loss: 0.765039  [   32/  306]
train() client id: f_00004-1-1 loss: 0.932977  [   64/  306]
train() client id: f_00004-1-2 loss: 0.845420  [   96/  306]
train() client id: f_00004-1-3 loss: 0.979654  [  128/  306]
train() client id: f_00004-1-4 loss: 0.786164  [  160/  306]
train() client id: f_00004-1-5 loss: 0.759141  [  192/  306]
train() client id: f_00004-1-6 loss: 0.885707  [  224/  306]
train() client id: f_00004-1-7 loss: 0.795691  [  256/  306]
train() client id: f_00004-1-8 loss: 0.942697  [  288/  306]
train() client id: f_00004-2-0 loss: 0.906360  [   32/  306]
train() client id: f_00004-2-1 loss: 0.892840  [   64/  306]
train() client id: f_00004-2-2 loss: 0.787594  [   96/  306]
train() client id: f_00004-2-3 loss: 0.804567  [  128/  306]
train() client id: f_00004-2-4 loss: 0.838222  [  160/  306]
train() client id: f_00004-2-5 loss: 0.827336  [  192/  306]
train() client id: f_00004-2-6 loss: 0.861708  [  224/  306]
train() client id: f_00004-2-7 loss: 0.979484  [  256/  306]
train() client id: f_00004-2-8 loss: 0.807415  [  288/  306]
train() client id: f_00004-3-0 loss: 0.765270  [   32/  306]
train() client id: f_00004-3-1 loss: 0.837536  [   64/  306]
train() client id: f_00004-3-2 loss: 0.813234  [   96/  306]
train() client id: f_00004-3-3 loss: 0.891822  [  128/  306]
train() client id: f_00004-3-4 loss: 0.804374  [  160/  306]
train() client id: f_00004-3-5 loss: 0.848883  [  192/  306]
train() client id: f_00004-3-6 loss: 0.891627  [  224/  306]
train() client id: f_00004-3-7 loss: 0.904259  [  256/  306]
train() client id: f_00004-3-8 loss: 0.864239  [  288/  306]
train() client id: f_00004-4-0 loss: 0.848381  [   32/  306]
train() client id: f_00004-4-1 loss: 0.874457  [   64/  306]
train() client id: f_00004-4-2 loss: 0.934520  [   96/  306]
train() client id: f_00004-4-3 loss: 0.693174  [  128/  306]
train() client id: f_00004-4-4 loss: 0.853360  [  160/  306]
train() client id: f_00004-4-5 loss: 0.841987  [  192/  306]
train() client id: f_00004-4-6 loss: 0.888671  [  224/  306]
train() client id: f_00004-4-7 loss: 0.873594  [  256/  306]
train() client id: f_00004-4-8 loss: 0.794214  [  288/  306]
train() client id: f_00004-5-0 loss: 0.777561  [   32/  306]
train() client id: f_00004-5-1 loss: 0.868090  [   64/  306]
train() client id: f_00004-5-2 loss: 0.868602  [   96/  306]
train() client id: f_00004-5-3 loss: 0.959364  [  128/  306]
train() client id: f_00004-5-4 loss: 0.948428  [  160/  306]
train() client id: f_00004-5-5 loss: 0.938390  [  192/  306]
train() client id: f_00004-5-6 loss: 0.781988  [  224/  306]
train() client id: f_00004-5-7 loss: 0.821032  [  256/  306]
train() client id: f_00004-5-8 loss: 0.802412  [  288/  306]
train() client id: f_00004-6-0 loss: 0.808589  [   32/  306]
train() client id: f_00004-6-1 loss: 0.811590  [   64/  306]
train() client id: f_00004-6-2 loss: 0.740021  [   96/  306]
train() client id: f_00004-6-3 loss: 0.939972  [  128/  306]
train() client id: f_00004-6-4 loss: 0.860857  [  160/  306]
train() client id: f_00004-6-5 loss: 0.844538  [  192/  306]
train() client id: f_00004-6-6 loss: 0.839649  [  224/  306]
train() client id: f_00004-6-7 loss: 1.020491  [  256/  306]
train() client id: f_00004-6-8 loss: 0.856420  [  288/  306]
train() client id: f_00004-7-0 loss: 0.785402  [   32/  306]
train() client id: f_00004-7-1 loss: 0.903044  [   64/  306]
train() client id: f_00004-7-2 loss: 0.863780  [   96/  306]
train() client id: f_00004-7-3 loss: 0.947186  [  128/  306]
train() client id: f_00004-7-4 loss: 0.791318  [  160/  306]
train() client id: f_00004-7-5 loss: 0.944653  [  192/  306]
train() client id: f_00004-7-6 loss: 0.830161  [  224/  306]
train() client id: f_00004-7-7 loss: 0.910399  [  256/  306]
train() client id: f_00004-7-8 loss: 0.735764  [  288/  306]
train() client id: f_00004-8-0 loss: 0.849478  [   32/  306]
train() client id: f_00004-8-1 loss: 0.808252  [   64/  306]
train() client id: f_00004-8-2 loss: 0.770438  [   96/  306]
train() client id: f_00004-8-3 loss: 0.914642  [  128/  306]
train() client id: f_00004-8-4 loss: 0.860675  [  160/  306]
train() client id: f_00004-8-5 loss: 0.742559  [  192/  306]
train() client id: f_00004-8-6 loss: 0.807834  [  224/  306]
train() client id: f_00004-8-7 loss: 0.988857  [  256/  306]
train() client id: f_00004-8-8 loss: 0.915604  [  288/  306]
train() client id: f_00004-9-0 loss: 0.833396  [   32/  306]
train() client id: f_00004-9-1 loss: 0.826151  [   64/  306]
train() client id: f_00004-9-2 loss: 0.816604  [   96/  306]
train() client id: f_00004-9-3 loss: 0.805696  [  128/  306]
train() client id: f_00004-9-4 loss: 0.848727  [  160/  306]
train() client id: f_00004-9-5 loss: 0.948050  [  192/  306]
train() client id: f_00004-9-6 loss: 0.858642  [  224/  306]
train() client id: f_00004-9-7 loss: 0.911232  [  256/  306]
train() client id: f_00004-9-8 loss: 0.905291  [  288/  306]
train() client id: f_00004-10-0 loss: 0.866934  [   32/  306]
train() client id: f_00004-10-1 loss: 0.878967  [   64/  306]
train() client id: f_00004-10-2 loss: 0.845103  [   96/  306]
train() client id: f_00004-10-3 loss: 0.771167  [  128/  306]
train() client id: f_00004-10-4 loss: 0.843206  [  160/  306]
train() client id: f_00004-10-5 loss: 0.881880  [  192/  306]
train() client id: f_00004-10-6 loss: 0.899877  [  224/  306]
train() client id: f_00004-10-7 loss: 0.832673  [  256/  306]
train() client id: f_00004-10-8 loss: 0.790392  [  288/  306]
train() client id: f_00004-11-0 loss: 0.867278  [   32/  306]
train() client id: f_00004-11-1 loss: 0.934988  [   64/  306]
train() client id: f_00004-11-2 loss: 0.852593  [   96/  306]
train() client id: f_00004-11-3 loss: 0.860898  [  128/  306]
train() client id: f_00004-11-4 loss: 0.823349  [  160/  306]
train() client id: f_00004-11-5 loss: 0.785510  [  192/  306]
train() client id: f_00004-11-6 loss: 0.859278  [  224/  306]
train() client id: f_00004-11-7 loss: 0.724383  [  256/  306]
train() client id: f_00004-11-8 loss: 0.868311  [  288/  306]
train() client id: f_00005-0-0 loss: 0.301646  [   32/  146]
train() client id: f_00005-0-1 loss: 0.488139  [   64/  146]
train() client id: f_00005-0-2 loss: 0.553307  [   96/  146]
train() client id: f_00005-0-3 loss: 0.710777  [  128/  146]
train() client id: f_00005-1-0 loss: 0.702644  [   32/  146]
train() client id: f_00005-1-1 loss: 0.308980  [   64/  146]
train() client id: f_00005-1-2 loss: 0.457343  [   96/  146]
train() client id: f_00005-1-3 loss: 0.369197  [  128/  146]
train() client id: f_00005-2-0 loss: 0.363104  [   32/  146]
train() client id: f_00005-2-1 loss: 0.825747  [   64/  146]
train() client id: f_00005-2-2 loss: 0.320282  [   96/  146]
train() client id: f_00005-2-3 loss: 0.463621  [  128/  146]
train() client id: f_00005-3-0 loss: 0.528584  [   32/  146]
train() client id: f_00005-3-1 loss: 0.568604  [   64/  146]
train() client id: f_00005-3-2 loss: 0.366275  [   96/  146]
train() client id: f_00005-3-3 loss: 0.444521  [  128/  146]
train() client id: f_00005-4-0 loss: 0.382952  [   32/  146]
train() client id: f_00005-4-1 loss: 0.773725  [   64/  146]
train() client id: f_00005-4-2 loss: 0.348077  [   96/  146]
train() client id: f_00005-4-3 loss: 0.396057  [  128/  146]
train() client id: f_00005-5-0 loss: 0.454057  [   32/  146]
train() client id: f_00005-5-1 loss: 0.392721  [   64/  146]
train() client id: f_00005-5-2 loss: 0.356406  [   96/  146]
train() client id: f_00005-5-3 loss: 0.480220  [  128/  146]
train() client id: f_00005-6-0 loss: 0.369214  [   32/  146]
train() client id: f_00005-6-1 loss: 0.451741  [   64/  146]
train() client id: f_00005-6-2 loss: 0.468016  [   96/  146]
train() client id: f_00005-6-3 loss: 0.702152  [  128/  146]
train() client id: f_00005-7-0 loss: 0.502518  [   32/  146]
train() client id: f_00005-7-1 loss: 0.296011  [   64/  146]
train() client id: f_00005-7-2 loss: 0.755006  [   96/  146]
train() client id: f_00005-7-3 loss: 0.419075  [  128/  146]
train() client id: f_00005-8-0 loss: 0.259868  [   32/  146]
train() client id: f_00005-8-1 loss: 0.452902  [   64/  146]
train() client id: f_00005-8-2 loss: 0.766804  [   96/  146]
train() client id: f_00005-8-3 loss: 0.498862  [  128/  146]
train() client id: f_00005-9-0 loss: 0.395731  [   32/  146]
train() client id: f_00005-9-1 loss: 0.532386  [   64/  146]
train() client id: f_00005-9-2 loss: 0.328344  [   96/  146]
train() client id: f_00005-9-3 loss: 0.339868  [  128/  146]
train() client id: f_00005-10-0 loss: 0.314672  [   32/  146]
train() client id: f_00005-10-1 loss: 0.420838  [   64/  146]
train() client id: f_00005-10-2 loss: 0.585641  [   96/  146]
train() client id: f_00005-10-3 loss: 0.566794  [  128/  146]
train() client id: f_00005-11-0 loss: 0.709569  [   32/  146]
train() client id: f_00005-11-1 loss: 0.532655  [   64/  146]
train() client id: f_00005-11-2 loss: 0.260588  [   96/  146]
train() client id: f_00005-11-3 loss: 0.477666  [  128/  146]
train() client id: f_00006-0-0 loss: 0.541109  [   32/   54]
train() client id: f_00006-1-0 loss: 0.517321  [   32/   54]
train() client id: f_00006-2-0 loss: 0.479033  [   32/   54]
train() client id: f_00006-3-0 loss: 0.483036  [   32/   54]
train() client id: f_00006-4-0 loss: 0.508012  [   32/   54]
train() client id: f_00006-5-0 loss: 0.450553  [   32/   54]
train() client id: f_00006-6-0 loss: 0.535995  [   32/   54]
train() client id: f_00006-7-0 loss: 0.444135  [   32/   54]
train() client id: f_00006-8-0 loss: 0.432383  [   32/   54]
train() client id: f_00006-9-0 loss: 0.528189  [   32/   54]
train() client id: f_00006-10-0 loss: 0.484401  [   32/   54]
train() client id: f_00006-11-0 loss: 0.488115  [   32/   54]
train() client id: f_00007-0-0 loss: 0.448999  [   32/  179]
train() client id: f_00007-0-1 loss: 0.425627  [   64/  179]
train() client id: f_00007-0-2 loss: 0.687515  [   96/  179]
train() client id: f_00007-0-3 loss: 0.655564  [  128/  179]
train() client id: f_00007-0-4 loss: 0.537818  [  160/  179]
train() client id: f_00007-1-0 loss: 0.553113  [   32/  179]
train() client id: f_00007-1-1 loss: 0.386934  [   64/  179]
train() client id: f_00007-1-2 loss: 0.756857  [   96/  179]
train() client id: f_00007-1-3 loss: 0.556050  [  128/  179]
train() client id: f_00007-1-4 loss: 0.409882  [  160/  179]
train() client id: f_00007-2-0 loss: 0.538651  [   32/  179]
train() client id: f_00007-2-1 loss: 0.675565  [   64/  179]
train() client id: f_00007-2-2 loss: 0.331074  [   96/  179]
train() client id: f_00007-2-3 loss: 0.506654  [  128/  179]
train() client id: f_00007-2-4 loss: 0.379945  [  160/  179]
train() client id: f_00007-3-0 loss: 0.468483  [   32/  179]
train() client id: f_00007-3-1 loss: 0.602230  [   64/  179]
train() client id: f_00007-3-2 loss: 0.388906  [   96/  179]
train() client id: f_00007-3-3 loss: 0.447519  [  128/  179]
train() client id: f_00007-3-4 loss: 0.680634  [  160/  179]
train() client id: f_00007-4-0 loss: 0.635092  [   32/  179]
train() client id: f_00007-4-1 loss: 0.450565  [   64/  179]
train() client id: f_00007-4-2 loss: 0.525213  [   96/  179]
train() client id: f_00007-4-3 loss: 0.397445  [  128/  179]
train() client id: f_00007-4-4 loss: 0.380082  [  160/  179]
train() client id: f_00007-5-0 loss: 0.386089  [   32/  179]
train() client id: f_00007-5-1 loss: 0.395501  [   64/  179]
train() client id: f_00007-5-2 loss: 0.625451  [   96/  179]
train() client id: f_00007-5-3 loss: 0.611290  [  128/  179]
train() client id: f_00007-5-4 loss: 0.444225  [  160/  179]
train() client id: f_00007-6-0 loss: 0.503092  [   32/  179]
train() client id: f_00007-6-1 loss: 0.487870  [   64/  179]
train() client id: f_00007-6-2 loss: 0.493254  [   96/  179]
train() client id: f_00007-6-3 loss: 0.455468  [  128/  179]
train() client id: f_00007-6-4 loss: 0.568589  [  160/  179]
train() client id: f_00007-7-0 loss: 0.455035  [   32/  179]
train() client id: f_00007-7-1 loss: 0.751040  [   64/  179]
train() client id: f_00007-7-2 loss: 0.450762  [   96/  179]
train() client id: f_00007-7-3 loss: 0.506129  [  128/  179]
train() client id: f_00007-7-4 loss: 0.428994  [  160/  179]
train() client id: f_00007-8-0 loss: 0.509755  [   32/  179]
train() client id: f_00007-8-1 loss: 0.730322  [   64/  179]
train() client id: f_00007-8-2 loss: 0.427596  [   96/  179]
train() client id: f_00007-8-3 loss: 0.445416  [  128/  179]
train() client id: f_00007-8-4 loss: 0.464705  [  160/  179]
train() client id: f_00007-9-0 loss: 0.620229  [   32/  179]
train() client id: f_00007-9-1 loss: 0.434943  [   64/  179]
train() client id: f_00007-9-2 loss: 0.522944  [   96/  179]
train() client id: f_00007-9-3 loss: 0.430596  [  128/  179]
train() client id: f_00007-9-4 loss: 0.565166  [  160/  179]
train() client id: f_00007-10-0 loss: 0.544709  [   32/  179]
train() client id: f_00007-10-1 loss: 0.340293  [   64/  179]
train() client id: f_00007-10-2 loss: 0.446773  [   96/  179]
train() client id: f_00007-10-3 loss: 0.593953  [  128/  179]
train() client id: f_00007-10-4 loss: 0.623627  [  160/  179]
train() client id: f_00007-11-0 loss: 0.355505  [   32/  179]
train() client id: f_00007-11-1 loss: 0.482066  [   64/  179]
train() client id: f_00007-11-2 loss: 0.486105  [   96/  179]
train() client id: f_00007-11-3 loss: 0.599413  [  128/  179]
train() client id: f_00007-11-4 loss: 0.536016  [  160/  179]
train() client id: f_00008-0-0 loss: 0.604071  [   32/  130]
train() client id: f_00008-0-1 loss: 0.665417  [   64/  130]
train() client id: f_00008-0-2 loss: 0.771401  [   96/  130]
train() client id: f_00008-0-3 loss: 0.654392  [  128/  130]
train() client id: f_00008-1-0 loss: 0.711614  [   32/  130]
train() client id: f_00008-1-1 loss: 0.724582  [   64/  130]
train() client id: f_00008-1-2 loss: 0.565341  [   96/  130]
train() client id: f_00008-1-3 loss: 0.704923  [  128/  130]
train() client id: f_00008-2-0 loss: 0.715475  [   32/  130]
train() client id: f_00008-2-1 loss: 0.678077  [   64/  130]
train() client id: f_00008-2-2 loss: 0.652346  [   96/  130]
train() client id: f_00008-2-3 loss: 0.668459  [  128/  130]
train() client id: f_00008-3-0 loss: 0.604253  [   32/  130]
train() client id: f_00008-3-1 loss: 0.748314  [   64/  130]
train() client id: f_00008-3-2 loss: 0.668486  [   96/  130]
train() client id: f_00008-3-3 loss: 0.674014  [  128/  130]
train() client id: f_00008-4-0 loss: 0.735068  [   32/  130]
train() client id: f_00008-4-1 loss: 0.603793  [   64/  130]
train() client id: f_00008-4-2 loss: 0.683792  [   96/  130]
train() client id: f_00008-4-3 loss: 0.654888  [  128/  130]
train() client id: f_00008-5-0 loss: 0.708035  [   32/  130]
train() client id: f_00008-5-1 loss: 0.598981  [   64/  130]
train() client id: f_00008-5-2 loss: 0.712812  [   96/  130]
train() client id: f_00008-5-3 loss: 0.673677  [  128/  130]
train() client id: f_00008-6-0 loss: 0.685208  [   32/  130]
train() client id: f_00008-6-1 loss: 0.605709  [   64/  130]
train() client id: f_00008-6-2 loss: 0.778783  [   96/  130]
train() client id: f_00008-6-3 loss: 0.638003  [  128/  130]
train() client id: f_00008-7-0 loss: 0.649300  [   32/  130]
train() client id: f_00008-7-1 loss: 0.728826  [   64/  130]
train() client id: f_00008-7-2 loss: 0.637098  [   96/  130]
train() client id: f_00008-7-3 loss: 0.673619  [  128/  130]
train() client id: f_00008-8-0 loss: 0.711368  [   32/  130]
train() client id: f_00008-8-1 loss: 0.611463  [   64/  130]
train() client id: f_00008-8-2 loss: 0.579455  [   96/  130]
train() client id: f_00008-8-3 loss: 0.813715  [  128/  130]
train() client id: f_00008-9-0 loss: 0.715548  [   32/  130]
train() client id: f_00008-9-1 loss: 0.686605  [   64/  130]
train() client id: f_00008-9-2 loss: 0.617313  [   96/  130]
train() client id: f_00008-9-3 loss: 0.693788  [  128/  130]
train() client id: f_00008-10-0 loss: 0.627358  [   32/  130]
train() client id: f_00008-10-1 loss: 0.668230  [   64/  130]
train() client id: f_00008-10-2 loss: 0.752760  [   96/  130]
train() client id: f_00008-10-3 loss: 0.656124  [  128/  130]
train() client id: f_00008-11-0 loss: 0.692535  [   32/  130]
train() client id: f_00008-11-1 loss: 0.694125  [   64/  130]
train() client id: f_00008-11-2 loss: 0.687891  [   96/  130]
train() client id: f_00008-11-3 loss: 0.594652  [  128/  130]
train() client id: f_00009-0-0 loss: 1.170544  [   32/  118]
train() client id: f_00009-0-1 loss: 1.031751  [   64/  118]
train() client id: f_00009-0-2 loss: 1.229148  [   96/  118]
train() client id: f_00009-1-0 loss: 0.966372  [   32/  118]
train() client id: f_00009-1-1 loss: 1.188915  [   64/  118]
train() client id: f_00009-1-2 loss: 1.211163  [   96/  118]
train() client id: f_00009-2-0 loss: 0.877038  [   32/  118]
train() client id: f_00009-2-1 loss: 1.148581  [   64/  118]
train() client id: f_00009-2-2 loss: 1.078941  [   96/  118]
train() client id: f_00009-3-0 loss: 0.987055  [   32/  118]
train() client id: f_00009-3-1 loss: 0.939344  [   64/  118]
train() client id: f_00009-3-2 loss: 1.143981  [   96/  118]
train() client id: f_00009-4-0 loss: 0.845489  [   32/  118]
train() client id: f_00009-4-1 loss: 0.983994  [   64/  118]
train() client id: f_00009-4-2 loss: 0.929186  [   96/  118]
train() client id: f_00009-5-0 loss: 0.929422  [   32/  118]
train() client id: f_00009-5-1 loss: 0.871717  [   64/  118]
train() client id: f_00009-5-2 loss: 1.015862  [   96/  118]
train() client id: f_00009-6-0 loss: 0.942129  [   32/  118]
train() client id: f_00009-6-1 loss: 0.860787  [   64/  118]
train() client id: f_00009-6-2 loss: 0.813602  [   96/  118]
train() client id: f_00009-7-0 loss: 0.730406  [   32/  118]
train() client id: f_00009-7-1 loss: 1.043829  [   64/  118]
train() client id: f_00009-7-2 loss: 0.809056  [   96/  118]
train() client id: f_00009-8-0 loss: 0.881592  [   32/  118]
train() client id: f_00009-8-1 loss: 0.781972  [   64/  118]
train() client id: f_00009-8-2 loss: 0.866892  [   96/  118]
train() client id: f_00009-9-0 loss: 0.987317  [   32/  118]
train() client id: f_00009-9-1 loss: 0.864956  [   64/  118]
train() client id: f_00009-9-2 loss: 0.725356  [   96/  118]
train() client id: f_00009-10-0 loss: 1.003456  [   32/  118]
train() client id: f_00009-10-1 loss: 0.633962  [   64/  118]
train() client id: f_00009-10-2 loss: 0.803440  [   96/  118]
train() client id: f_00009-11-0 loss: 0.915499  [   32/  118]
train() client id: f_00009-11-1 loss: 0.864439  [   64/  118]
train() client id: f_00009-11-2 loss: 0.729462  [   96/  118]
At round 46 accuracy: 0.6472148541114059
At round 46 training accuracy: 0.5982562038900067
At round 46 training loss: 0.8248992087202477
update_location
xs = [  -3.9056584     4.20031788  250.00902392   18.81129433    0.97929623
    3.95640986 -212.44319194 -191.32485185  234.66397685 -177.06087855]
ys = [ 242.5879595   225.55583871    1.32061395 -212.45517586  204.35018685
  187.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [262.41983969 246.76523063 269.26985732 235.56541882 227.50814905
 212.81402303 234.81695071 215.88393917 255.68701962 203.38772473]
dists_bs = [183.9603381  186.19518456 459.12656158 433.22549684 178.41005064
 179.41460111 181.52858639 174.94100506 438.8725234  171.01093665]
uav_gains = [5.77502052e-12 7.90037418e-12 5.00729947e-12 9.71837173e-12
 1.11658603e-11 1.41063225e-11 9.84803495e-12 1.34576919e-11
 6.62667355e-12 1.62319850e-11]
bs_gains = [5.03577512e-11 4.86835724e-11 3.88940630e-12 4.57609321e-12
 5.48681098e-11 5.40122526e-11 5.22694600e-11 5.79692334e-11
 4.41312865e-12 6.17770556e-11]
Round 47
-------------------------------
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.7932913   9.84287299  4.72759004  1.71436679 11.34993316  5.46069336
  2.11875437  6.70878283  4.95608411  4.42774153]
obj_prev = 56.1001104698108
eta_min = 5.969830954362688e-20	eta_max = 0.9382327126425405
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 12.978344451772108	eta = 0.9090909090909091
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 25.625037120999544	eta = 0.4604284044719558
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 19.192856434060143	eta = 0.6147336638864522
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 18.0285169675076	eta = 0.6544351361468406
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.963027394397052	eta = 0.6568210745943969
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.962800013672382	eta = 0.6568293889135345
eta = 0.6568293889135345
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.0346847  0.07294799 0.03413415 0.01183684 0.08423428 0.04019021
 0.01486487 0.0492743  0.03578581 0.0324825 ]
ene_total = [1.66925633 2.82340307 1.67271292 0.79630461 3.21465228 1.66466235
 0.90143074 2.08079959 1.75289976 1.38667837]
ti_comp = [0.65804615 0.71288711 0.65155892 0.67749658 0.71463044 0.71440598
 0.67791976 0.68705877 0.6453014  0.71627974]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [6.02256253e-06 4.77395682e-05 5.85517725e-06 2.25825789e-07
 7.31448314e-05 7.94970091e-06 4.46690869e-07 1.58399461e-05
 6.87839679e-06 4.17506343e-06]
ene_total = [0.44822894 0.2566042  0.4710644  0.37954037 0.25136048 0.24985529
 0.37805813 0.34642191 0.493133   0.24312494]
optimize_network iter = 0 obj = 3.5173916587449052
eta = 0.6568293889135345
freqs = [26354310.22875877 51163774.23769676 26194218.65211249  8735722.05600123
 58935554.78523942 28128413.29923534 10963593.51009882 35858870.20763274
 27727978.21502854 22674452.1605431 ]
eta_min = 0.6568293889135383	eta_max = 0.6851522482532448
af = 0.0048373581457900995	bf = 1.2050321534900845	zeta = 0.00532109396036911	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.36561793e-06 1.08249620e-05 1.32766327e-06 5.12060679e-08
 1.65856133e-05 1.80259716e-06 1.01287294e-07 3.59171272e-06
 1.55967862e-06 9.46696933e-07]
ene_total = [1.69139003 0.96374279 1.77760827 1.4326949  0.9413374  0.94235589
 1.42707694 1.30607207 1.86080934 0.91733759]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 1 obj = 3.8330520913049613
eta = 0.6851522482532448
freqs = [26297444.52410097 50627871.73999517 26166161.04026021  8689617.22720056
 58304024.4554098  27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656411]
eta_min = 0.6851522482532453	eta_max = 0.6851522482532441
af = 0.0047490407946122245	bf = 1.2050321534900845	zeta = 0.005223944874073448	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.35973100e-06 1.05993827e-05 1.32482057e-06 5.06669901e-08
 1.62320679e-05 1.76428286e-06 1.00207745e-07 3.54345912e-06
 1.55967862e-06 9.26091353e-07]
ene_total = [1.69138925 0.96371281 1.77760789 1.43269482 0.94129041 0.9423508
 1.4270768  1.30606566 1.86080934 0.91733486]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 2 obj = 3.8330520913049524
eta = 0.6851522482532441
freqs = [26297444.52410097 50627871.73999519 26166161.04026021  8689617.22720056
 58304024.45540981 27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656412]
Done!
At round 47 energy consumption: 3.8330520913049613
At round 47 eta: 0.6851522482532441
At round 47 local rounds: 12.381380128537735
At round 47 global rounds: 38.37711395955986
At round 47 a_n: 11.740402201727147
gradient difference: 0.47972774505615234
train() client id: f_00000-0-0 loss: 1.277786  [   32/  126]
train() client id: f_00000-0-1 loss: 1.062275  [   64/  126]
train() client id: f_00000-0-2 loss: 1.108176  [   96/  126]
train() client id: f_00000-1-0 loss: 0.815522  [   32/  126]
train() client id: f_00000-1-1 loss: 0.961325  [   64/  126]
train() client id: f_00000-1-2 loss: 1.171408  [   96/  126]
train() client id: f_00000-2-0 loss: 0.966198  [   32/  126]
train() client id: f_00000-2-1 loss: 1.180908  [   64/  126]
train() client id: f_00000-2-2 loss: 0.995041  [   96/  126]
train() client id: f_00000-3-0 loss: 0.958647  [   32/  126]
train() client id: f_00000-3-1 loss: 0.905871  [   64/  126]
train() client id: f_00000-3-2 loss: 0.968838  [   96/  126]
train() client id: f_00000-4-0 loss: 0.927049  [   32/  126]
train() client id: f_00000-4-1 loss: 1.022460  [   64/  126]
train() client id: f_00000-4-2 loss: 0.955430  [   96/  126]
train() client id: f_00000-5-0 loss: 0.962473  [   32/  126]
train() client id: f_00000-5-1 loss: 0.846360  [   64/  126]
train() client id: f_00000-5-2 loss: 0.974451  [   96/  126]
train() client id: f_00000-6-0 loss: 0.839679  [   32/  126]
train() client id: f_00000-6-1 loss: 0.843974  [   64/  126]
train() client id: f_00000-6-2 loss: 0.885960  [   96/  126]
train() client id: f_00000-7-0 loss: 0.923471  [   32/  126]
train() client id: f_00000-7-1 loss: 0.918676  [   64/  126]
train() client id: f_00000-7-2 loss: 0.924037  [   96/  126]
train() client id: f_00000-8-0 loss: 0.867250  [   32/  126]
train() client id: f_00000-8-1 loss: 0.770639  [   64/  126]
train() client id: f_00000-8-2 loss: 1.038726  [   96/  126]
train() client id: f_00000-9-0 loss: 0.812518  [   32/  126]
train() client id: f_00000-9-1 loss: 0.897013  [   64/  126]
train() client id: f_00000-9-2 loss: 0.901301  [   96/  126]
train() client id: f_00000-10-0 loss: 0.973771  [   32/  126]
train() client id: f_00000-10-1 loss: 0.850717  [   64/  126]
train() client id: f_00000-10-2 loss: 0.807385  [   96/  126]
train() client id: f_00000-11-0 loss: 0.908070  [   32/  126]
train() client id: f_00000-11-1 loss: 0.859341  [   64/  126]
train() client id: f_00000-11-2 loss: 0.792337  [   96/  126]
train() client id: f_00001-0-0 loss: 0.481320  [   32/  265]
train() client id: f_00001-0-1 loss: 0.431106  [   64/  265]
train() client id: f_00001-0-2 loss: 0.426678  [   96/  265]
train() client id: f_00001-0-3 loss: 0.494586  [  128/  265]
train() client id: f_00001-0-4 loss: 0.632516  [  160/  265]
train() client id: f_00001-0-5 loss: 0.516410  [  192/  265]
train() client id: f_00001-0-6 loss: 0.378703  [  224/  265]
train() client id: f_00001-0-7 loss: 0.487729  [  256/  265]
train() client id: f_00001-1-0 loss: 0.470189  [   32/  265]
train() client id: f_00001-1-1 loss: 0.484150  [   64/  265]
train() client id: f_00001-1-2 loss: 0.433174  [   96/  265]
train() client id: f_00001-1-3 loss: 0.480917  [  128/  265]
train() client id: f_00001-1-4 loss: 0.494127  [  160/  265]
train() client id: f_00001-1-5 loss: 0.481024  [  192/  265]
train() client id: f_00001-1-6 loss: 0.460645  [  224/  265]
train() client id: f_00001-1-7 loss: 0.518814  [  256/  265]
train() client id: f_00001-2-0 loss: 0.558595  [   32/  265]
train() client id: f_00001-2-1 loss: 0.547034  [   64/  265]
train() client id: f_00001-2-2 loss: 0.466178  [   96/  265]
train() client id: f_00001-2-3 loss: 0.460106  [  128/  265]
train() client id: f_00001-2-4 loss: 0.396323  [  160/  265]
train() client id: f_00001-2-5 loss: 0.475722  [  192/  265]
train() client id: f_00001-2-6 loss: 0.464571  [  224/  265]
train() client id: f_00001-2-7 loss: 0.412516  [  256/  265]
train() client id: f_00001-3-0 loss: 0.541797  [   32/  265]
train() client id: f_00001-3-1 loss: 0.410823  [   64/  265]
train() client id: f_00001-3-2 loss: 0.481072  [   96/  265]
train() client id: f_00001-3-3 loss: 0.402351  [  128/  265]
train() client id: f_00001-3-4 loss: 0.508156  [  160/  265]
train() client id: f_00001-3-5 loss: 0.458915  [  192/  265]
train() client id: f_00001-3-6 loss: 0.389056  [  224/  265]
train() client id: f_00001-3-7 loss: 0.546735  [  256/  265]
train() client id: f_00001-4-0 loss: 0.560910  [   32/  265]
train() client id: f_00001-4-1 loss: 0.431959  [   64/  265]
train() client id: f_00001-4-2 loss: 0.386269  [   96/  265]
train() client id: f_00001-4-3 loss: 0.369004  [  128/  265]
train() client id: f_00001-4-4 loss: 0.567478  [  160/  265]
train() client id: f_00001-4-5 loss: 0.499882  [  192/  265]
train() client id: f_00001-4-6 loss: 0.375282  [  224/  265]
train() client id: f_00001-4-7 loss: 0.538969  [  256/  265]
train() client id: f_00001-5-0 loss: 0.608572  [   32/  265]
train() client id: f_00001-5-1 loss: 0.368315  [   64/  265]
train() client id: f_00001-5-2 loss: 0.446663  [   96/  265]
train() client id: f_00001-5-3 loss: 0.468135  [  128/  265]
train() client id: f_00001-5-4 loss: 0.449350  [  160/  265]
train() client id: f_00001-5-5 loss: 0.411160  [  192/  265]
train() client id: f_00001-5-6 loss: 0.474515  [  224/  265]
train() client id: f_00001-5-7 loss: 0.492981  [  256/  265]
train() client id: f_00001-6-0 loss: 0.444084  [   32/  265]
train() client id: f_00001-6-1 loss: 0.703070  [   64/  265]
train() client id: f_00001-6-2 loss: 0.446930  [   96/  265]
train() client id: f_00001-6-3 loss: 0.394998  [  128/  265]
train() client id: f_00001-6-4 loss: 0.367304  [  160/  265]
train() client id: f_00001-6-5 loss: 0.433394  [  192/  265]
train() client id: f_00001-6-6 loss: 0.415053  [  224/  265]
train() client id: f_00001-6-7 loss: 0.459278  [  256/  265]
train() client id: f_00001-7-0 loss: 0.436914  [   32/  265]
train() client id: f_00001-7-1 loss: 0.490342  [   64/  265]
train() client id: f_00001-7-2 loss: 0.538519  [   96/  265]
train() client id: f_00001-7-3 loss: 0.407926  [  128/  265]
train() client id: f_00001-7-4 loss: 0.435643  [  160/  265]
train() client id: f_00001-7-5 loss: 0.540774  [  192/  265]
train() client id: f_00001-7-6 loss: 0.395190  [  224/  265]
train() client id: f_00001-7-7 loss: 0.454040  [  256/  265]
train() client id: f_00001-8-0 loss: 0.548531  [   32/  265]
train() client id: f_00001-8-1 loss: 0.488558  [   64/  265]
train() client id: f_00001-8-2 loss: 0.521273  [   96/  265]
train() client id: f_00001-8-3 loss: 0.457024  [  128/  265]
train() client id: f_00001-8-4 loss: 0.439785  [  160/  265]
train() client id: f_00001-8-5 loss: 0.368162  [  192/  265]
train() client id: f_00001-8-6 loss: 0.359555  [  224/  265]
train() client id: f_00001-8-7 loss: 0.358065  [  256/  265]
train() client id: f_00001-9-0 loss: 0.368383  [   32/  265]
train() client id: f_00001-9-1 loss: 0.573839  [   64/  265]
train() client id: f_00001-9-2 loss: 0.367654  [   96/  265]
train() client id: f_00001-9-3 loss: 0.539355  [  128/  265]
train() client id: f_00001-9-4 loss: 0.476221  [  160/  265]
train() client id: f_00001-9-5 loss: 0.403587  [  192/  265]
train() client id: f_00001-9-6 loss: 0.456255  [  224/  265]
train() client id: f_00001-9-7 loss: 0.457535  [  256/  265]
train() client id: f_00001-10-0 loss: 0.429560  [   32/  265]
train() client id: f_00001-10-1 loss: 0.397879  [   64/  265]
train() client id: f_00001-10-2 loss: 0.426478  [   96/  265]
train() client id: f_00001-10-3 loss: 0.469204  [  128/  265]
train() client id: f_00001-10-4 loss: 0.466523  [  160/  265]
train() client id: f_00001-10-5 loss: 0.422882  [  192/  265]
train() client id: f_00001-10-6 loss: 0.540008  [  224/  265]
train() client id: f_00001-10-7 loss: 0.460229  [  256/  265]
train() client id: f_00001-11-0 loss: 0.733402  [   32/  265]
train() client id: f_00001-11-1 loss: 0.451824  [   64/  265]
train() client id: f_00001-11-2 loss: 0.358068  [   96/  265]
train() client id: f_00001-11-3 loss: 0.454963  [  128/  265]
train() client id: f_00001-11-4 loss: 0.395002  [  160/  265]
train() client id: f_00001-11-5 loss: 0.505153  [  192/  265]
train() client id: f_00001-11-6 loss: 0.369612  [  224/  265]
train() client id: f_00001-11-7 loss: 0.433058  [  256/  265]
train() client id: f_00002-0-0 loss: 1.265488  [   32/  124]
train() client id: f_00002-0-1 loss: 1.290158  [   64/  124]
train() client id: f_00002-0-2 loss: 1.352186  [   96/  124]
train() client id: f_00002-1-0 loss: 1.317357  [   32/  124]
train() client id: f_00002-1-1 loss: 1.318879  [   64/  124]
train() client id: f_00002-1-2 loss: 1.128505  [   96/  124]
train() client id: f_00002-2-0 loss: 1.155994  [   32/  124]
train() client id: f_00002-2-1 loss: 1.192004  [   64/  124]
train() client id: f_00002-2-2 loss: 1.327906  [   96/  124]
train() client id: f_00002-3-0 loss: 1.073122  [   32/  124]
train() client id: f_00002-3-1 loss: 1.182247  [   64/  124]
train() client id: f_00002-3-2 loss: 1.239318  [   96/  124]
train() client id: f_00002-4-0 loss: 1.323595  [   32/  124]
train() client id: f_00002-4-1 loss: 1.135366  [   64/  124]
train() client id: f_00002-4-2 loss: 1.135645  [   96/  124]
train() client id: f_00002-5-0 loss: 1.190126  [   32/  124]
train() client id: f_00002-5-1 loss: 1.137177  [   64/  124]
train() client id: f_00002-5-2 loss: 1.132292  [   96/  124]
train() client id: f_00002-6-0 loss: 1.190331  [   32/  124]
train() client id: f_00002-6-1 loss: 1.167548  [   64/  124]
train() client id: f_00002-6-2 loss: 0.937671  [   96/  124]
train() client id: f_00002-7-0 loss: 1.213588  [   32/  124]
train() client id: f_00002-7-1 loss: 1.186744  [   64/  124]
train() client id: f_00002-7-2 loss: 1.061397  [   96/  124]
train() client id: f_00002-8-0 loss: 0.988194  [   32/  124]
train() client id: f_00002-8-1 loss: 1.039773  [   64/  124]
train() client id: f_00002-8-2 loss: 1.067021  [   96/  124]
train() client id: f_00002-9-0 loss: 1.071287  [   32/  124]
train() client id: f_00002-9-1 loss: 0.983196  [   64/  124]
train() client id: f_00002-9-2 loss: 1.239957  [   96/  124]
train() client id: f_00002-10-0 loss: 1.040404  [   32/  124]
train() client id: f_00002-10-1 loss: 1.117604  [   64/  124]
train() client id: f_00002-10-2 loss: 1.224677  [   96/  124]
train() client id: f_00002-11-0 loss: 1.085300  [   32/  124]
train() client id: f_00002-11-1 loss: 0.940605  [   64/  124]
train() client id: f_00002-11-2 loss: 1.158505  [   96/  124]
train() client id: f_00003-0-0 loss: 0.589704  [   32/   43]
train() client id: f_00003-1-0 loss: 0.711702  [   32/   43]
train() client id: f_00003-2-0 loss: 0.771215  [   32/   43]
train() client id: f_00003-3-0 loss: 0.806231  [   32/   43]
train() client id: f_00003-4-0 loss: 0.743182  [   32/   43]
train() client id: f_00003-5-0 loss: 0.818600  [   32/   43]
train() client id: f_00003-6-0 loss: 0.800728  [   32/   43]
train() client id: f_00003-7-0 loss: 0.726855  [   32/   43]
train() client id: f_00003-8-0 loss: 0.550199  [   32/   43]
train() client id: f_00003-9-0 loss: 0.938205  [   32/   43]
train() client id: f_00003-10-0 loss: 0.671822  [   32/   43]
train() client id: f_00003-11-0 loss: 0.854340  [   32/   43]
train() client id: f_00004-0-0 loss: 0.927936  [   32/  306]
train() client id: f_00004-0-1 loss: 0.861102  [   64/  306]
train() client id: f_00004-0-2 loss: 0.722767  [   96/  306]
train() client id: f_00004-0-3 loss: 0.875295  [  128/  306]
train() client id: f_00004-0-4 loss: 0.744980  [  160/  306]
train() client id: f_00004-0-5 loss: 0.811813  [  192/  306]
train() client id: f_00004-0-6 loss: 0.658686  [  224/  306]
train() client id: f_00004-0-7 loss: 0.887001  [  256/  306]
train() client id: f_00004-0-8 loss: 0.764126  [  288/  306]
train() client id: f_00004-1-0 loss: 0.657062  [   32/  306]
train() client id: f_00004-1-1 loss: 0.808055  [   64/  306]
train() client id: f_00004-1-2 loss: 0.858981  [   96/  306]
train() client id: f_00004-1-3 loss: 0.830499  [  128/  306]
train() client id: f_00004-1-4 loss: 0.767598  [  160/  306]
train() client id: f_00004-1-5 loss: 0.956448  [  192/  306]
train() client id: f_00004-1-6 loss: 0.808924  [  224/  306]
train() client id: f_00004-1-7 loss: 0.825159  [  256/  306]
train() client id: f_00004-1-8 loss: 0.819605  [  288/  306]
train() client id: f_00004-2-0 loss: 0.904044  [   32/  306]
train() client id: f_00004-2-1 loss: 0.805557  [   64/  306]
train() client id: f_00004-2-2 loss: 0.799755  [   96/  306]
train() client id: f_00004-2-3 loss: 0.908780  [  128/  306]
train() client id: f_00004-2-4 loss: 0.858718  [  160/  306]
train() client id: f_00004-2-5 loss: 0.665686  [  192/  306]
train() client id: f_00004-2-6 loss: 0.792255  [  224/  306]
train() client id: f_00004-2-7 loss: 0.664306  [  256/  306]
train() client id: f_00004-2-8 loss: 0.875410  [  288/  306]
train() client id: f_00004-3-0 loss: 0.777381  [   32/  306]
train() client id: f_00004-3-1 loss: 0.712527  [   64/  306]
train() client id: f_00004-3-2 loss: 0.862970  [   96/  306]
train() client id: f_00004-3-3 loss: 0.731045  [  128/  306]
train() client id: f_00004-3-4 loss: 0.883646  [  160/  306]
train() client id: f_00004-3-5 loss: 0.952860  [  192/  306]
train() client id: f_00004-3-6 loss: 0.741594  [  224/  306]
train() client id: f_00004-3-7 loss: 0.732116  [  256/  306]
train() client id: f_00004-3-8 loss: 0.863480  [  288/  306]
train() client id: f_00004-4-0 loss: 0.732615  [   32/  306]
train() client id: f_00004-4-1 loss: 0.719842  [   64/  306]
train() client id: f_00004-4-2 loss: 0.811160  [   96/  306]
train() client id: f_00004-4-3 loss: 1.010498  [  128/  306]
train() client id: f_00004-4-4 loss: 0.811260  [  160/  306]
train() client id: f_00004-4-5 loss: 0.846168  [  192/  306]
train() client id: f_00004-4-6 loss: 0.760954  [  224/  306]
train() client id: f_00004-4-7 loss: 0.649211  [  256/  306]
train() client id: f_00004-4-8 loss: 0.924713  [  288/  306]
train() client id: f_00004-5-0 loss: 0.787040  [   32/  306]
train() client id: f_00004-5-1 loss: 0.723357  [   64/  306]
train() client id: f_00004-5-2 loss: 0.770934  [   96/  306]
train() client id: f_00004-5-3 loss: 0.880672  [  128/  306]
train() client id: f_00004-5-4 loss: 0.792276  [  160/  306]
train() client id: f_00004-5-5 loss: 0.952778  [  192/  306]
train() client id: f_00004-5-6 loss: 0.812426  [  224/  306]
train() client id: f_00004-5-7 loss: 0.788674  [  256/  306]
train() client id: f_00004-5-8 loss: 0.783709  [  288/  306]
train() client id: f_00004-6-0 loss: 0.917238  [   32/  306]
train() client id: f_00004-6-1 loss: 0.869510  [   64/  306]
train() client id: f_00004-6-2 loss: 0.783221  [   96/  306]
train() client id: f_00004-6-3 loss: 0.889222  [  128/  306]
train() client id: f_00004-6-4 loss: 0.794752  [  160/  306]
train() client id: f_00004-6-5 loss: 0.813862  [  192/  306]
train() client id: f_00004-6-6 loss: 0.743166  [  224/  306]
train() client id: f_00004-6-7 loss: 0.760891  [  256/  306]
train() client id: f_00004-6-8 loss: 0.757463  [  288/  306]
train() client id: f_00004-7-0 loss: 0.875084  [   32/  306]
train() client id: f_00004-7-1 loss: 0.616289  [   64/  306]
train() client id: f_00004-7-2 loss: 0.706042  [   96/  306]
train() client id: f_00004-7-3 loss: 0.723485  [  128/  306]
train() client id: f_00004-7-4 loss: 0.831991  [  160/  306]
train() client id: f_00004-7-5 loss: 1.054401  [  192/  306]
train() client id: f_00004-7-6 loss: 0.754789  [  224/  306]
train() client id: f_00004-7-7 loss: 0.895654  [  256/  306]
train() client id: f_00004-7-8 loss: 0.762827  [  288/  306]
train() client id: f_00004-8-0 loss: 0.765857  [   32/  306]
train() client id: f_00004-8-1 loss: 0.909542  [   64/  306]
train() client id: f_00004-8-2 loss: 0.783408  [   96/  306]
train() client id: f_00004-8-3 loss: 0.796694  [  128/  306]
train() client id: f_00004-8-4 loss: 0.726346  [  160/  306]
train() client id: f_00004-8-5 loss: 0.814365  [  192/  306]
train() client id: f_00004-8-6 loss: 0.772260  [  224/  306]
train() client id: f_00004-8-7 loss: 0.883602  [  256/  306]
train() client id: f_00004-8-8 loss: 0.911836  [  288/  306]
train() client id: f_00004-9-0 loss: 0.793603  [   32/  306]
train() client id: f_00004-9-1 loss: 0.815666  [   64/  306]
train() client id: f_00004-9-2 loss: 0.891928  [   96/  306]
train() client id: f_00004-9-3 loss: 0.902940  [  128/  306]
train() client id: f_00004-9-4 loss: 0.828077  [  160/  306]
train() client id: f_00004-9-5 loss: 0.752859  [  192/  306]
train() client id: f_00004-9-6 loss: 0.671364  [  224/  306]
train() client id: f_00004-9-7 loss: 0.753028  [  256/  306]
train() client id: f_00004-9-8 loss: 0.777418  [  288/  306]
train() client id: f_00004-10-0 loss: 0.846965  [   32/  306]
train() client id: f_00004-10-1 loss: 0.873717  [   64/  306]
train() client id: f_00004-10-2 loss: 0.691033  [   96/  306]
train() client id: f_00004-10-3 loss: 0.851051  [  128/  306]
train() client id: f_00004-10-4 loss: 0.694216  [  160/  306]
train() client id: f_00004-10-5 loss: 0.842579  [  192/  306]
train() client id: f_00004-10-6 loss: 0.862543  [  224/  306]
train() client id: f_00004-10-7 loss: 0.729511  [  256/  306]
train() client id: f_00004-10-8 loss: 0.821108  [  288/  306]
train() client id: f_00004-11-0 loss: 0.848262  [   32/  306]
train() client id: f_00004-11-1 loss: 0.827739  [   64/  306]
train() client id: f_00004-11-2 loss: 0.744529  [   96/  306]
train() client id: f_00004-11-3 loss: 0.817416  [  128/  306]
train() client id: f_00004-11-4 loss: 0.855989  [  160/  306]
train() client id: f_00004-11-5 loss: 0.850821  [  192/  306]
train() client id: f_00004-11-6 loss: 0.732231  [  224/  306]
train() client id: f_00004-11-7 loss: 0.773201  [  256/  306]
train() client id: f_00004-11-8 loss: 0.841143  [  288/  306]
train() client id: f_00005-0-0 loss: 0.622814  [   32/  146]
train() client id: f_00005-0-1 loss: 0.959021  [   64/  146]
train() client id: f_00005-0-2 loss: 0.576826  [   96/  146]
train() client id: f_00005-0-3 loss: 0.326338  [  128/  146]
train() client id: f_00005-1-0 loss: 0.655029  [   32/  146]
train() client id: f_00005-1-1 loss: 0.803548  [   64/  146]
train() client id: f_00005-1-2 loss: 0.445827  [   96/  146]
train() client id: f_00005-1-3 loss: 0.413248  [  128/  146]
train() client id: f_00005-2-0 loss: 0.654265  [   32/  146]
train() client id: f_00005-2-1 loss: 0.764080  [   64/  146]
train() client id: f_00005-2-2 loss: 0.548266  [   96/  146]
train() client id: f_00005-2-3 loss: 0.545667  [  128/  146]
train() client id: f_00005-3-0 loss: 0.578548  [   32/  146]
train() client id: f_00005-3-1 loss: 0.441117  [   64/  146]
train() client id: f_00005-3-2 loss: 0.822593  [   96/  146]
train() client id: f_00005-3-3 loss: 0.576232  [  128/  146]
train() client id: f_00005-4-0 loss: 0.524382  [   32/  146]
train() client id: f_00005-4-1 loss: 0.583550  [   64/  146]
train() client id: f_00005-4-2 loss: 0.539457  [   96/  146]
train() client id: f_00005-4-3 loss: 0.656659  [  128/  146]
train() client id: f_00005-5-0 loss: 0.618465  [   32/  146]
train() client id: f_00005-5-1 loss: 0.440582  [   64/  146]
train() client id: f_00005-5-2 loss: 0.602895  [   96/  146]
train() client id: f_00005-5-3 loss: 0.606352  [  128/  146]
train() client id: f_00005-6-0 loss: 0.573644  [   32/  146]
train() client id: f_00005-6-1 loss: 0.393378  [   64/  146]
train() client id: f_00005-6-2 loss: 0.666697  [   96/  146]
train() client id: f_00005-6-3 loss: 0.752692  [  128/  146]
train() client id: f_00005-7-0 loss: 0.834193  [   32/  146]
train() client id: f_00005-7-1 loss: 0.618439  [   64/  146]
train() client id: f_00005-7-2 loss: 0.647947  [   96/  146]
train() client id: f_00005-7-3 loss: 0.468135  [  128/  146]
train() client id: f_00005-8-0 loss: 0.735575  [   32/  146]
train() client id: f_00005-8-1 loss: 0.673511  [   64/  146]
train() client id: f_00005-8-2 loss: 0.614808  [   96/  146]
train() client id: f_00005-8-3 loss: 0.416664  [  128/  146]
train() client id: f_00005-9-0 loss: 0.592765  [   32/  146]
train() client id: f_00005-9-1 loss: 0.703918  [   64/  146]
train() client id: f_00005-9-2 loss: 0.662401  [   96/  146]
train() client id: f_00005-9-3 loss: 0.557613  [  128/  146]
train() client id: f_00005-10-0 loss: 0.803048  [   32/  146]
train() client id: f_00005-10-1 loss: 0.377908  [   64/  146]
train() client id: f_00005-10-2 loss: 0.638220  [   96/  146]
train() client id: f_00005-10-3 loss: 0.604945  [  128/  146]
train() client id: f_00005-11-0 loss: 0.469884  [   32/  146]
train() client id: f_00005-11-1 loss: 0.641668  [   64/  146]
train() client id: f_00005-11-2 loss: 0.618707  [   96/  146]
train() client id: f_00005-11-3 loss: 0.814679  [  128/  146]
train() client id: f_00006-0-0 loss: 0.505768  [   32/   54]
train() client id: f_00006-1-0 loss: 0.489112  [   32/   54]
train() client id: f_00006-2-0 loss: 0.514169  [   32/   54]
train() client id: f_00006-3-0 loss: 0.465764  [   32/   54]
train() client id: f_00006-4-0 loss: 0.560808  [   32/   54]
train() client id: f_00006-5-0 loss: 0.582273  [   32/   54]
train() client id: f_00006-6-0 loss: 0.570944  [   32/   54]
train() client id: f_00006-7-0 loss: 0.501624  [   32/   54]
train() client id: f_00006-8-0 loss: 0.457906  [   32/   54]
train() client id: f_00006-9-0 loss: 0.536727  [   32/   54]
train() client id: f_00006-10-0 loss: 0.526918  [   32/   54]
train() client id: f_00006-11-0 loss: 0.584360  [   32/   54]
train() client id: f_00007-0-0 loss: 0.437847  [   32/  179]
train() client id: f_00007-0-1 loss: 0.476295  [   64/  179]
train() client id: f_00007-0-2 loss: 0.536474  [   96/  179]
train() client id: f_00007-0-3 loss: 0.490613  [  128/  179]
train() client id: f_00007-0-4 loss: 0.653501  [  160/  179]
train() client id: f_00007-1-0 loss: 0.532530  [   32/  179]
train() client id: f_00007-1-1 loss: 0.496943  [   64/  179]
train() client id: f_00007-1-2 loss: 0.485320  [   96/  179]
train() client id: f_00007-1-3 loss: 0.695522  [  128/  179]
train() client id: f_00007-1-4 loss: 0.511960  [  160/  179]
train() client id: f_00007-2-0 loss: 0.526929  [   32/  179]
train() client id: f_00007-2-1 loss: 0.487808  [   64/  179]
train() client id: f_00007-2-2 loss: 0.653584  [   96/  179]
train() client id: f_00007-2-3 loss: 0.337917  [  128/  179]
train() client id: f_00007-2-4 loss: 0.563924  [  160/  179]
train() client id: f_00007-3-0 loss: 0.504834  [   32/  179]
train() client id: f_00007-3-1 loss: 0.527984  [   64/  179]
train() client id: f_00007-3-2 loss: 0.599787  [   96/  179]
train() client id: f_00007-3-3 loss: 0.372885  [  128/  179]
train() client id: f_00007-3-4 loss: 0.470773  [  160/  179]
train() client id: f_00007-4-0 loss: 0.413574  [   32/  179]
train() client id: f_00007-4-1 loss: 0.348417  [   64/  179]
train() client id: f_00007-4-2 loss: 0.441292  [   96/  179]
train() client id: f_00007-4-3 loss: 0.645621  [  128/  179]
train() client id: f_00007-4-4 loss: 0.437824  [  160/  179]
train() client id: f_00007-5-0 loss: 0.449783  [   32/  179]
train() client id: f_00007-5-1 loss: 0.478126  [   64/  179]
train() client id: f_00007-5-2 loss: 0.323529  [   96/  179]
train() client id: f_00007-5-3 loss: 0.506629  [  128/  179]
train() client id: f_00007-5-4 loss: 0.543715  [  160/  179]
train() client id: f_00007-6-0 loss: 0.442797  [   32/  179]
train() client id: f_00007-6-1 loss: 0.688928  [   64/  179]
train() client id: f_00007-6-2 loss: 0.450379  [   96/  179]
train() client id: f_00007-6-3 loss: 0.486845  [  128/  179]
train() client id: f_00007-6-4 loss: 0.497399  [  160/  179]
train() client id: f_00007-7-0 loss: 0.553508  [   32/  179]
train() client id: f_00007-7-1 loss: 0.617196  [   64/  179]
train() client id: f_00007-7-2 loss: 0.561681  [   96/  179]
train() client id: f_00007-7-3 loss: 0.439394  [  128/  179]
train() client id: f_00007-7-4 loss: 0.337974  [  160/  179]
train() client id: f_00007-8-0 loss: 0.493532  [   32/  179]
train() client id: f_00007-8-1 loss: 0.528341  [   64/  179]
train() client id: f_00007-8-2 loss: 0.642123  [   96/  179]
train() client id: f_00007-8-3 loss: 0.464027  [  128/  179]
train() client id: f_00007-8-4 loss: 0.367821  [  160/  179]
train() client id: f_00007-9-0 loss: 0.693432  [   32/  179]
train() client id: f_00007-9-1 loss: 0.384260  [   64/  179]
train() client id: f_00007-9-2 loss: 0.535694  [   96/  179]
train() client id: f_00007-9-3 loss: 0.521757  [  128/  179]
train() client id: f_00007-9-4 loss: 0.356702  [  160/  179]
train() client id: f_00007-10-0 loss: 0.410834  [   32/  179]
train() client id: f_00007-10-1 loss: 0.387338  [   64/  179]
train() client id: f_00007-10-2 loss: 0.679634  [   96/  179]
train() client id: f_00007-10-3 loss: 0.430232  [  128/  179]
train() client id: f_00007-10-4 loss: 0.612987  [  160/  179]
train() client id: f_00007-11-0 loss: 0.506981  [   32/  179]
train() client id: f_00007-11-1 loss: 0.509329  [   64/  179]
train() client id: f_00007-11-2 loss: 0.453339  [   96/  179]
train() client id: f_00007-11-3 loss: 0.428295  [  128/  179]
train() client id: f_00007-11-4 loss: 0.591020  [  160/  179]
train() client id: f_00008-0-0 loss: 0.829779  [   32/  130]
train() client id: f_00008-0-1 loss: 0.749503  [   64/  130]
train() client id: f_00008-0-2 loss: 0.773654  [   96/  130]
train() client id: f_00008-0-3 loss: 0.829366  [  128/  130]
train() client id: f_00008-1-0 loss: 0.768303  [   32/  130]
train() client id: f_00008-1-1 loss: 0.899012  [   64/  130]
train() client id: f_00008-1-2 loss: 0.817389  [   96/  130]
train() client id: f_00008-1-3 loss: 0.659875  [  128/  130]
train() client id: f_00008-2-0 loss: 0.820567  [   32/  130]
train() client id: f_00008-2-1 loss: 0.672274  [   64/  130]
train() client id: f_00008-2-2 loss: 0.844084  [   96/  130]
train() client id: f_00008-2-3 loss: 0.829655  [  128/  130]
train() client id: f_00008-3-0 loss: 0.777647  [   32/  130]
train() client id: f_00008-3-1 loss: 0.759787  [   64/  130]
train() client id: f_00008-3-2 loss: 0.856511  [   96/  130]
train() client id: f_00008-3-3 loss: 0.739459  [  128/  130]
train() client id: f_00008-4-0 loss: 0.790274  [   32/  130]
train() client id: f_00008-4-1 loss: 0.822994  [   64/  130]
train() client id: f_00008-4-2 loss: 0.761569  [   96/  130]
train() client id: f_00008-4-3 loss: 0.743393  [  128/  130]
train() client id: f_00008-5-0 loss: 0.690562  [   32/  130]
train() client id: f_00008-5-1 loss: 0.830309  [   64/  130]
train() client id: f_00008-5-2 loss: 0.792503  [   96/  130]
train() client id: f_00008-5-3 loss: 0.842486  [  128/  130]
train() client id: f_00008-6-0 loss: 0.659183  [   32/  130]
train() client id: f_00008-6-1 loss: 0.903918  [   64/  130]
train() client id: f_00008-6-2 loss: 0.785266  [   96/  130]
train() client id: f_00008-6-3 loss: 0.737141  [  128/  130]
train() client id: f_00008-7-0 loss: 0.772759  [   32/  130]
train() client id: f_00008-7-1 loss: 0.767890  [   64/  130]
train() client id: f_00008-7-2 loss: 0.884241  [   96/  130]
train() client id: f_00008-7-3 loss: 0.706298  [  128/  130]
train() client id: f_00008-8-0 loss: 0.840975  [   32/  130]
train() client id: f_00008-8-1 loss: 0.734804  [   64/  130]
train() client id: f_00008-8-2 loss: 0.799250  [   96/  130]
train() client id: f_00008-8-3 loss: 0.776027  [  128/  130]
train() client id: f_00008-9-0 loss: 0.676308  [   32/  130]
train() client id: f_00008-9-1 loss: 0.783468  [   64/  130]
train() client id: f_00008-9-2 loss: 0.833199  [   96/  130]
train() client id: f_00008-9-3 loss: 0.860531  [  128/  130]
train() client id: f_00008-10-0 loss: 0.788936  [   32/  130]
train() client id: f_00008-10-1 loss: 0.836474  [   64/  130]
train() client id: f_00008-10-2 loss: 0.811103  [   96/  130]
train() client id: f_00008-10-3 loss: 0.661887  [  128/  130]
train() client id: f_00008-11-0 loss: 0.775763  [   32/  130]
train() client id: f_00008-11-1 loss: 0.655504  [   64/  130]
train() client id: f_00008-11-2 loss: 0.812741  [   96/  130]
train() client id: f_00008-11-3 loss: 0.908202  [  128/  130]
train() client id: f_00009-0-0 loss: 1.161857  [   32/  118]
train() client id: f_00009-0-1 loss: 1.094700  [   64/  118]
train() client id: f_00009-0-2 loss: 0.932254  [   96/  118]
train() client id: f_00009-1-0 loss: 0.992989  [   32/  118]
train() client id: f_00009-1-1 loss: 0.988374  [   64/  118]
train() client id: f_00009-1-2 loss: 1.064614  [   96/  118]
train() client id: f_00009-2-0 loss: 1.093296  [   32/  118]
train() client id: f_00009-2-1 loss: 0.927583  [   64/  118]
train() client id: f_00009-2-2 loss: 1.006593  [   96/  118]
train() client id: f_00009-3-0 loss: 0.998075  [   32/  118]
train() client id: f_00009-3-1 loss: 0.939418  [   64/  118]
train() client id: f_00009-3-2 loss: 1.019785  [   96/  118]
train() client id: f_00009-4-0 loss: 1.023119  [   32/  118]
train() client id: f_00009-4-1 loss: 0.818131  [   64/  118]
train() client id: f_00009-4-2 loss: 1.004129  [   96/  118]
train() client id: f_00009-5-0 loss: 1.037211  [   32/  118]
train() client id: f_00009-5-1 loss: 0.754241  [   64/  118]
train() client id: f_00009-5-2 loss: 0.996350  [   96/  118]
train() client id: f_00009-6-0 loss: 0.928710  [   32/  118]
train() client id: f_00009-6-1 loss: 0.929369  [   64/  118]
train() client id: f_00009-6-2 loss: 1.002058  [   96/  118]
train() client id: f_00009-7-0 loss: 0.911214  [   32/  118]
train() client id: f_00009-7-1 loss: 0.839311  [   64/  118]
train() client id: f_00009-7-2 loss: 0.966253  [   96/  118]
train() client id: f_00009-8-0 loss: 0.855552  [   32/  118]
train() client id: f_00009-8-1 loss: 0.915483  [   64/  118]
train() client id: f_00009-8-2 loss: 1.013710  [   96/  118]
train() client id: f_00009-9-0 loss: 0.866223  [   32/  118]
train() client id: f_00009-9-1 loss: 0.958460  [   64/  118]
train() client id: f_00009-9-2 loss: 0.813930  [   96/  118]
train() client id: f_00009-10-0 loss: 0.960304  [   32/  118]
train() client id: f_00009-10-1 loss: 0.937996  [   64/  118]
train() client id: f_00009-10-2 loss: 0.930968  [   96/  118]
train() client id: f_00009-11-0 loss: 0.892288  [   32/  118]
train() client id: f_00009-11-1 loss: 0.886961  [   64/  118]
train() client id: f_00009-11-2 loss: 0.876536  [   96/  118]
At round 47 accuracy: 0.6472148541114059
At round 47 training accuracy: 0.5949027498323273
At round 47 training loss: 0.8289393810500789
update_location
xs = [  -3.9056584     4.20031788  255.00902392   18.81129433    0.97929623
    3.95640986 -217.44319194 -196.32485185  239.66397685 -182.06087855]
ys = [ 247.5879595   230.55583871    1.32061395 -217.45517586  209.35018685
  192.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [267.04878179 251.343664   273.91850303 240.08460655 232.00961134
 217.23938387 239.35002039 220.3273104  260.28348348 207.75508503]
dists_bs = [185.85555033 187.61451208 463.75887031 437.70296191 179.3004407
 179.84087577 182.62546262 175.47821451 443.54338182 171.14423519]
uav_gains = [5.24535404e-12 7.22652115e-12 4.54225750e-12 8.95681473e-12
 1.03431723e-11 1.31774062e-11 9.07808384e-12 1.25521670e-11
 6.03484339e-12 1.52201927e-11]
bs_gains = [4.89330838e-11 4.76593468e-11 3.78160221e-12 4.44622592e-12
 5.41085986e-11 5.36545480e-11 5.13951767e-11 5.74736933e-11
 4.28423215e-12 6.16424250e-11]
Round 48
-------------------------------
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.66251584  9.56421031  4.59923771  1.66893827 11.02840386  5.30599829
  2.06183055  6.52085571  4.81741298  4.30225233]
obj_prev = 54.53165585566928
eta_min = 1.7212465954185627e-20	eta_max = 0.9392276891852349
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 12.610414541407936	eta = 0.909090909090909
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 25.13769025130001	eta = 0.4560487898791294
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 18.739989943316214	eta = 0.6117406281506839
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.582476098023903	eta = 0.6520135819067145
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516919047777254	eta = 0.6544537420190021
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516687862363067	eta = 0.6544623795057578
eta = 0.6544623795057578
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.03498261 0.07357454 0.03442733 0.01193851 0.08495777 0.0405354
 0.01499255 0.04969752 0.03609317 0.03276149]
ene_total = [1.63559168 2.74693988 1.64037262 0.78097472 3.12737952 1.61844234
 0.88321539 2.02882541 1.7072293  1.347717  ]
ti_comp = [0.68075038 0.73958322 0.67379048 0.70183595 0.74144644 0.74132564
 0.70228536 0.71216487 0.67077194 0.74326503]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [5.77378821e-06 4.55080801e-05 5.61746411e-06 2.15903231e-07
 6.97155574e-05 7.57470152e-06 4.27050309e-07 1.51259614e-05
 6.53138483e-06 3.97817259e-06]
ene_total = [0.4471766  0.2486291  0.47081906 0.37534488 0.24312091 0.24142
 0.37382507 0.34075668 0.48110627 0.23470829]
optimize_network iter = 0 obj = 3.4569068484449996
eta = 0.6544623795057578
freqs = [25694153.73552614 49740538.88381484 25547504.06148466  8505199.59149158
 57291910.54818947 27339810.52250793 10674113.26419133 34891861.85896511
 26904202.71882671 22038902.98735746]
eta_min = 0.6544623795057585	eta_max = 0.691983191142256
af = 0.004443917503062845	bf = 1.1926074070739463	zeta = 0.00488830925336913	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29805932e-06 1.02310970e-05 1.26291463e-06 4.85392243e-08
 1.56734064e-05 1.70293948e-06 9.60091734e-08 3.40060882e-06
 1.46838170e-06 8.94370180e-07]
ene_total = [1.6991017  0.94046194 1.78898032 1.42663193 0.91710237 0.91685829
 1.42083414 1.2936725  1.8279896  0.89170773]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 1 obj = 3.877075061802886
eta = 0.691983191142256
freqs = [25637255.917499   49049571.69376329 25530186.95114224  8448581.49608057
 56476825.65917366 26951443.47001215 10602081.72934842 34587483.18659778
 26904202.71882672 21718183.03696004]
eta_min = 0.6919831911422635	eta_max = 0.6919831911422551
af = 0.004337069151020109	bf = 1.1926074070739463	zeta = 0.004770776066122121	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29231677e-06 9.94882221e-06 1.26120311e-06 4.78951357e-08
 1.52306115e-05 1.65490197e-06 9.47177586e-08 3.34153728e-06
 1.46838170e-06 8.68529037e-07]
ene_total = [1.69910096 0.94042549 1.7889801  1.42663185 0.91704519 0.91685209
 1.42083397 1.29366487 1.8279896  0.8917044 ]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 2 obj = 3.8770750618028753
eta = 0.6919831911422551
freqs = [25637255.91749901 49049571.69376332 25530186.95114225  8448581.49608057
 56476825.6591737  26951443.47001216 10602081.72934843 34587483.18659779
 26904202.71882672 21718183.03696005]
Done!
At round 48 energy consumption: 3.877075061802886
At round 48 eta: 0.6919831911422551
At round 48 local rounds: 12.056529567513225
At round 48 global rounds: 38.11610880998822
At round 48 a_n: 11.397856354757828
gradient difference: 0.46852317452430725
train() client id: f_00000-0-0 loss: 1.216197  [   32/  126]
train() client id: f_00000-0-1 loss: 1.150630  [   64/  126]
train() client id: f_00000-0-2 loss: 1.269124  [   96/  126]
train() client id: f_00000-1-0 loss: 1.224820  [   32/  126]
train() client id: f_00000-1-1 loss: 1.124799  [   64/  126]
train() client id: f_00000-1-2 loss: 1.194214  [   96/  126]
train() client id: f_00000-2-0 loss: 1.266808  [   32/  126]
train() client id: f_00000-2-1 loss: 1.140491  [   64/  126]
train() client id: f_00000-2-2 loss: 1.048939  [   96/  126]
train() client id: f_00000-3-0 loss: 1.017893  [   32/  126]
train() client id: f_00000-3-1 loss: 0.963820  [   64/  126]
train() client id: f_00000-3-2 loss: 0.961726  [   96/  126]
train() client id: f_00000-4-0 loss: 0.970989  [   32/  126]
train() client id: f_00000-4-1 loss: 0.965475  [   64/  126]
train() client id: f_00000-4-2 loss: 1.177397  [   96/  126]
train() client id: f_00000-5-0 loss: 1.055450  [   32/  126]
train() client id: f_00000-5-1 loss: 0.962245  [   64/  126]
train() client id: f_00000-5-2 loss: 0.892569  [   96/  126]
train() client id: f_00000-6-0 loss: 0.920315  [   32/  126]
train() client id: f_00000-6-1 loss: 1.037431  [   64/  126]
train() client id: f_00000-6-2 loss: 0.967593  [   96/  126]
train() client id: f_00000-7-0 loss: 0.893350  [   32/  126]
train() client id: f_00000-7-1 loss: 0.904998  [   64/  126]
train() client id: f_00000-7-2 loss: 1.083608  [   96/  126]
train() client id: f_00000-8-0 loss: 0.921155  [   32/  126]
train() client id: f_00000-8-1 loss: 0.938740  [   64/  126]
train() client id: f_00000-8-2 loss: 0.928075  [   96/  126]
train() client id: f_00000-9-0 loss: 0.874435  [   32/  126]
train() client id: f_00000-9-1 loss: 0.876539  [   64/  126]
train() client id: f_00000-9-2 loss: 0.964700  [   96/  126]
train() client id: f_00000-10-0 loss: 0.896137  [   32/  126]
train() client id: f_00000-10-1 loss: 0.842646  [   64/  126]
train() client id: f_00000-10-2 loss: 1.026008  [   96/  126]
train() client id: f_00000-11-0 loss: 0.876763  [   32/  126]
train() client id: f_00000-11-1 loss: 0.816258  [   64/  126]
train() client id: f_00000-11-2 loss: 0.991995  [   96/  126]
train() client id: f_00001-0-0 loss: 0.389640  [   32/  265]
train() client id: f_00001-0-1 loss: 0.543490  [   64/  265]
train() client id: f_00001-0-2 loss: 0.446682  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444260  [  128/  265]
train() client id: f_00001-0-4 loss: 0.507838  [  160/  265]
train() client id: f_00001-0-5 loss: 0.580090  [  192/  265]
train() client id: f_00001-0-6 loss: 0.387720  [  224/  265]
train() client id: f_00001-0-7 loss: 0.374411  [  256/  265]
train() client id: f_00001-1-0 loss: 0.451423  [   32/  265]
train() client id: f_00001-1-1 loss: 0.520666  [   64/  265]
train() client id: f_00001-1-2 loss: 0.422913  [   96/  265]
train() client id: f_00001-1-3 loss: 0.367551  [  128/  265]
train() client id: f_00001-1-4 loss: 0.660897  [  160/  265]
train() client id: f_00001-1-5 loss: 0.414668  [  192/  265]
train() client id: f_00001-1-6 loss: 0.448511  [  224/  265]
train() client id: f_00001-1-7 loss: 0.397096  [  256/  265]
train() client id: f_00001-2-0 loss: 0.365048  [   32/  265]
train() client id: f_00001-2-1 loss: 0.497539  [   64/  265]
train() client id: f_00001-2-2 loss: 0.445733  [   96/  265]
train() client id: f_00001-2-3 loss: 0.445086  [  128/  265]
train() client id: f_00001-2-4 loss: 0.568169  [  160/  265]
train() client id: f_00001-2-5 loss: 0.494690  [  192/  265]
train() client id: f_00001-2-6 loss: 0.403971  [  224/  265]
train() client id: f_00001-2-7 loss: 0.391074  [  256/  265]
train() client id: f_00001-3-0 loss: 0.572645  [   32/  265]
train() client id: f_00001-3-1 loss: 0.480804  [   64/  265]
train() client id: f_00001-3-2 loss: 0.433904  [   96/  265]
train() client id: f_00001-3-3 loss: 0.554369  [  128/  265]
train() client id: f_00001-3-4 loss: 0.398359  [  160/  265]
train() client id: f_00001-3-5 loss: 0.353205  [  192/  265]
train() client id: f_00001-3-6 loss: 0.415888  [  224/  265]
train() client id: f_00001-3-7 loss: 0.369024  [  256/  265]
train() client id: f_00001-4-0 loss: 0.606094  [   32/  265]
train() client id: f_00001-4-1 loss: 0.398908  [   64/  265]
train() client id: f_00001-4-2 loss: 0.402232  [   96/  265]
train() client id: f_00001-4-3 loss: 0.365244  [  128/  265]
train() client id: f_00001-4-4 loss: 0.425353  [  160/  265]
train() client id: f_00001-4-5 loss: 0.464469  [  192/  265]
train() client id: f_00001-4-6 loss: 0.533953  [  224/  265]
train() client id: f_00001-4-7 loss: 0.376301  [  256/  265]
train() client id: f_00001-5-0 loss: 0.343732  [   32/  265]
train() client id: f_00001-5-1 loss: 0.553110  [   64/  265]
train() client id: f_00001-5-2 loss: 0.508482  [   96/  265]
train() client id: f_00001-5-3 loss: 0.340062  [  128/  265]
train() client id: f_00001-5-4 loss: 0.587650  [  160/  265]
train() client id: f_00001-5-5 loss: 0.427092  [  192/  265]
train() client id: f_00001-5-6 loss: 0.388534  [  224/  265]
train() client id: f_00001-5-7 loss: 0.389065  [  256/  265]
train() client id: f_00001-6-0 loss: 0.405585  [   32/  265]
train() client id: f_00001-6-1 loss: 0.447806  [   64/  265]
train() client id: f_00001-6-2 loss: 0.523211  [   96/  265]
train() client id: f_00001-6-3 loss: 0.418749  [  128/  265]
train() client id: f_00001-6-4 loss: 0.360716  [  160/  265]
train() client id: f_00001-6-5 loss: 0.342173  [  192/  265]
train() client id: f_00001-6-6 loss: 0.601538  [  224/  265]
train() client id: f_00001-6-7 loss: 0.434919  [  256/  265]
train() client id: f_00001-7-0 loss: 0.406629  [   32/  265]
train() client id: f_00001-7-1 loss: 0.618867  [   64/  265]
train() client id: f_00001-7-2 loss: 0.527930  [   96/  265]
train() client id: f_00001-7-3 loss: 0.344251  [  128/  265]
train() client id: f_00001-7-4 loss: 0.452220  [  160/  265]
train() client id: f_00001-7-5 loss: 0.378939  [  192/  265]
train() client id: f_00001-7-6 loss: 0.462696  [  224/  265]
train() client id: f_00001-7-7 loss: 0.329497  [  256/  265]
train() client id: f_00001-8-0 loss: 0.400004  [   32/  265]
train() client id: f_00001-8-1 loss: 0.403516  [   64/  265]
train() client id: f_00001-8-2 loss: 0.495480  [   96/  265]
train() client id: f_00001-8-3 loss: 0.528295  [  128/  265]
train() client id: f_00001-8-4 loss: 0.327528  [  160/  265]
train() client id: f_00001-8-5 loss: 0.438536  [  192/  265]
train() client id: f_00001-8-6 loss: 0.451790  [  224/  265]
train() client id: f_00001-8-7 loss: 0.471519  [  256/  265]
train() client id: f_00001-9-0 loss: 0.422759  [   32/  265]
train() client id: f_00001-9-1 loss: 0.392774  [   64/  265]
train() client id: f_00001-9-2 loss: 0.371904  [   96/  265]
train() client id: f_00001-9-3 loss: 0.446347  [  128/  265]
train() client id: f_00001-9-4 loss: 0.389294  [  160/  265]
train() client id: f_00001-9-5 loss: 0.450608  [  192/  265]
train() client id: f_00001-9-6 loss: 0.418961  [  224/  265]
train() client id: f_00001-9-7 loss: 0.588253  [  256/  265]
train() client id: f_00001-10-0 loss: 0.477191  [   32/  265]
train() client id: f_00001-10-1 loss: 0.545840  [   64/  265]
train() client id: f_00001-10-2 loss: 0.414558  [   96/  265]
train() client id: f_00001-10-3 loss: 0.483665  [  128/  265]
train() client id: f_00001-10-4 loss: 0.339620  [  160/  265]
train() client id: f_00001-10-5 loss: 0.455005  [  192/  265]
train() client id: f_00001-10-6 loss: 0.363691  [  224/  265]
train() client id: f_00001-10-7 loss: 0.429937  [  256/  265]
train() client id: f_00001-11-0 loss: 0.446589  [   32/  265]
train() client id: f_00001-11-1 loss: 0.364607  [   64/  265]
train() client id: f_00001-11-2 loss: 0.462161  [   96/  265]
train() client id: f_00001-11-3 loss: 0.335739  [  128/  265]
train() client id: f_00001-11-4 loss: 0.401507  [  160/  265]
train() client id: f_00001-11-5 loss: 0.554605  [  192/  265]
train() client id: f_00001-11-6 loss: 0.511252  [  224/  265]
train() client id: f_00001-11-7 loss: 0.435597  [  256/  265]
train() client id: f_00002-0-0 loss: 1.124189  [   32/  124]
train() client id: f_00002-0-1 loss: 0.985954  [   64/  124]
train() client id: f_00002-0-2 loss: 1.060620  [   96/  124]
train() client id: f_00002-1-0 loss: 0.995329  [   32/  124]
train() client id: f_00002-1-1 loss: 1.015970  [   64/  124]
train() client id: f_00002-1-2 loss: 1.144098  [   96/  124]
train() client id: f_00002-2-0 loss: 1.249059  [   32/  124]
train() client id: f_00002-2-1 loss: 0.821324  [   64/  124]
train() client id: f_00002-2-2 loss: 0.926610  [   96/  124]
train() client id: f_00002-3-0 loss: 1.011389  [   32/  124]
train() client id: f_00002-3-1 loss: 1.010851  [   64/  124]
train() client id: f_00002-3-2 loss: 0.950703  [   96/  124]
train() client id: f_00002-4-0 loss: 1.057683  [   32/  124]
train() client id: f_00002-4-1 loss: 0.925618  [   64/  124]
train() client id: f_00002-4-2 loss: 1.077936  [   96/  124]
train() client id: f_00002-5-0 loss: 1.070066  [   32/  124]
train() client id: f_00002-5-1 loss: 0.945932  [   64/  124]
train() client id: f_00002-5-2 loss: 0.832009  [   96/  124]
train() client id: f_00002-6-0 loss: 1.019003  [   32/  124]
train() client id: f_00002-6-1 loss: 1.146858  [   64/  124]
train() client id: f_00002-6-2 loss: 0.753837  [   96/  124]
train() client id: f_00002-7-0 loss: 1.021934  [   32/  124]
train() client id: f_00002-7-1 loss: 0.896801  [   64/  124]
train() client id: f_00002-7-2 loss: 0.877500  [   96/  124]
train() client id: f_00002-8-0 loss: 1.051722  [   32/  124]
train() client id: f_00002-8-1 loss: 0.877404  [   64/  124]
train() client id: f_00002-8-2 loss: 0.977008  [   96/  124]
train() client id: f_00002-9-0 loss: 0.805093  [   32/  124]
train() client id: f_00002-9-1 loss: 0.896492  [   64/  124]
train() client id: f_00002-9-2 loss: 1.060447  [   96/  124]
train() client id: f_00002-10-0 loss: 0.813983  [   32/  124]
train() client id: f_00002-10-1 loss: 0.991958  [   64/  124]
train() client id: f_00002-10-2 loss: 1.031926  [   96/  124]
train() client id: f_00002-11-0 loss: 0.893038  [   32/  124]
train() client id: f_00002-11-1 loss: 0.799940  [   64/  124]
train() client id: f_00002-11-2 loss: 0.995289  [   96/  124]
train() client id: f_00003-0-0 loss: 0.463800  [   32/   43]
train() client id: f_00003-1-0 loss: 0.551123  [   32/   43]
train() client id: f_00003-2-0 loss: 0.517071  [   32/   43]
train() client id: f_00003-3-0 loss: 0.730262  [   32/   43]
train() client id: f_00003-4-0 loss: 0.714869  [   32/   43]
train() client id: f_00003-5-0 loss: 0.503130  [   32/   43]
train() client id: f_00003-6-0 loss: 0.641777  [   32/   43]
train() client id: f_00003-7-0 loss: 0.448452  [   32/   43]
train() client id: f_00003-8-0 loss: 0.558542  [   32/   43]
train() client id: f_00003-9-0 loss: 0.752162  [   32/   43]
train() client id: f_00003-10-0 loss: 0.568632  [   32/   43]
train() client id: f_00003-11-0 loss: 0.320518  [   32/   43]
train() client id: f_00004-0-0 loss: 0.741834  [   32/  306]
train() client id: f_00004-0-1 loss: 0.727816  [   64/  306]
train() client id: f_00004-0-2 loss: 0.738991  [   96/  306]
train() client id: f_00004-0-3 loss: 0.838208  [  128/  306]
train() client id: f_00004-0-4 loss: 0.921241  [  160/  306]
train() client id: f_00004-0-5 loss: 0.816739  [  192/  306]
train() client id: f_00004-0-6 loss: 0.916224  [  224/  306]
train() client id: f_00004-0-7 loss: 0.806708  [  256/  306]
train() client id: f_00004-0-8 loss: 0.749136  [  288/  306]
train() client id: f_00004-1-0 loss: 0.864542  [   32/  306]
train() client id: f_00004-1-1 loss: 0.819587  [   64/  306]
train() client id: f_00004-1-2 loss: 0.773504  [   96/  306]
train() client id: f_00004-1-3 loss: 0.684073  [  128/  306]
train() client id: f_00004-1-4 loss: 0.796520  [  160/  306]
train() client id: f_00004-1-5 loss: 0.761310  [  192/  306]
train() client id: f_00004-1-6 loss: 0.802259  [  224/  306]
train() client id: f_00004-1-7 loss: 0.786504  [  256/  306]
train() client id: f_00004-1-8 loss: 0.877058  [  288/  306]
train() client id: f_00004-2-0 loss: 0.859269  [   32/  306]
train() client id: f_00004-2-1 loss: 0.593937  [   64/  306]
train() client id: f_00004-2-2 loss: 0.928888  [   96/  306]
train() client id: f_00004-2-3 loss: 0.964737  [  128/  306]
train() client id: f_00004-2-4 loss: 0.723207  [  160/  306]
train() client id: f_00004-2-5 loss: 0.689678  [  192/  306]
train() client id: f_00004-2-6 loss: 0.792046  [  224/  306]
train() client id: f_00004-2-7 loss: 0.787482  [  256/  306]
train() client id: f_00004-2-8 loss: 0.788708  [  288/  306]
train() client id: f_00004-3-0 loss: 0.801242  [   32/  306]
train() client id: f_00004-3-1 loss: 0.873328  [   64/  306]
train() client id: f_00004-3-2 loss: 0.638440  [   96/  306]
train() client id: f_00004-3-3 loss: 0.824714  [  128/  306]
train() client id: f_00004-3-4 loss: 0.821319  [  160/  306]
train() client id: f_00004-3-5 loss: 0.683568  [  192/  306]
train() client id: f_00004-3-6 loss: 0.843974  [  224/  306]
train() client id: f_00004-3-7 loss: 0.876773  [  256/  306]
train() client id: f_00004-3-8 loss: 0.876657  [  288/  306]
train() client id: f_00004-4-0 loss: 0.843870  [   32/  306]
train() client id: f_00004-4-1 loss: 0.754802  [   64/  306]
train() client id: f_00004-4-2 loss: 0.772642  [   96/  306]
train() client id: f_00004-4-3 loss: 0.754467  [  128/  306]
train() client id: f_00004-4-4 loss: 0.904503  [  160/  306]
train() client id: f_00004-4-5 loss: 0.781039  [  192/  306]
train() client id: f_00004-4-6 loss: 0.837324  [  224/  306]
train() client id: f_00004-4-7 loss: 0.724741  [  256/  306]
train() client id: f_00004-4-8 loss: 0.803592  [  288/  306]
train() client id: f_00004-5-0 loss: 0.817666  [   32/  306]
train() client id: f_00004-5-1 loss: 0.803267  [   64/  306]
train() client id: f_00004-5-2 loss: 0.854403  [   96/  306]
train() client id: f_00004-5-3 loss: 0.750197  [  128/  306]
train() client id: f_00004-5-4 loss: 0.747338  [  160/  306]
train() client id: f_00004-5-5 loss: 0.771875  [  192/  306]
train() client id: f_00004-5-6 loss: 0.727478  [  224/  306]
train() client id: f_00004-5-7 loss: 0.884816  [  256/  306]
train() client id: f_00004-5-8 loss: 0.872486  [  288/  306]
train() client id: f_00004-6-0 loss: 0.763453  [   32/  306]
train() client id: f_00004-6-1 loss: 0.727472  [   64/  306]
train() client id: f_00004-6-2 loss: 0.871510  [   96/  306]
train() client id: f_00004-6-3 loss: 0.861120  [  128/  306]
train() client id: f_00004-6-4 loss: 0.843029  [  160/  306]
train() client id: f_00004-6-5 loss: 0.723905  [  192/  306]
train() client id: f_00004-6-6 loss: 0.762624  [  224/  306]
train() client id: f_00004-6-7 loss: 0.763587  [  256/  306]
train() client id: f_00004-6-8 loss: 0.832579  [  288/  306]
train() client id: f_00004-7-0 loss: 0.720647  [   32/  306]
train() client id: f_00004-7-1 loss: 0.798889  [   64/  306]
train() client id: f_00004-7-2 loss: 0.694644  [   96/  306]
train() client id: f_00004-7-3 loss: 0.831878  [  128/  306]
train() client id: f_00004-7-4 loss: 0.786355  [  160/  306]
train() client id: f_00004-7-5 loss: 0.900740  [  192/  306]
train() client id: f_00004-7-6 loss: 0.852680  [  224/  306]
train() client id: f_00004-7-7 loss: 0.857073  [  256/  306]
train() client id: f_00004-7-8 loss: 0.759494  [  288/  306]
train() client id: f_00004-8-0 loss: 0.806162  [   32/  306]
train() client id: f_00004-8-1 loss: 0.733103  [   64/  306]
train() client id: f_00004-8-2 loss: 0.758122  [   96/  306]
train() client id: f_00004-8-3 loss: 0.737126  [  128/  306]
train() client id: f_00004-8-4 loss: 0.780136  [  160/  306]
train() client id: f_00004-8-5 loss: 0.884295  [  192/  306]
train() client id: f_00004-8-6 loss: 0.930714  [  224/  306]
train() client id: f_00004-8-7 loss: 0.747556  [  256/  306]
train() client id: f_00004-8-8 loss: 0.828059  [  288/  306]
train() client id: f_00004-9-0 loss: 0.872319  [   32/  306]
train() client id: f_00004-9-1 loss: 0.794469  [   64/  306]
train() client id: f_00004-9-2 loss: 0.844472  [   96/  306]
train() client id: f_00004-9-3 loss: 0.728058  [  128/  306]
train() client id: f_00004-9-4 loss: 0.774493  [  160/  306]
train() client id: f_00004-9-5 loss: 0.802722  [  192/  306]
train() client id: f_00004-9-6 loss: 0.703785  [  224/  306]
train() client id: f_00004-9-7 loss: 0.944473  [  256/  306]
train() client id: f_00004-9-8 loss: 0.774192  [  288/  306]
train() client id: f_00004-10-0 loss: 0.851318  [   32/  306]
train() client id: f_00004-10-1 loss: 0.787329  [   64/  306]
train() client id: f_00004-10-2 loss: 0.811684  [   96/  306]
train() client id: f_00004-10-3 loss: 0.863532  [  128/  306]
train() client id: f_00004-10-4 loss: 0.830554  [  160/  306]
train() client id: f_00004-10-5 loss: 0.719787  [  192/  306]
train() client id: f_00004-10-6 loss: 0.714930  [  224/  306]
train() client id: f_00004-10-7 loss: 0.888204  [  256/  306]
train() client id: f_00004-10-8 loss: 0.716868  [  288/  306]
train() client id: f_00004-11-0 loss: 0.800772  [   32/  306]
train() client id: f_00004-11-1 loss: 0.800935  [   64/  306]
train() client id: f_00004-11-2 loss: 0.883634  [   96/  306]
train() client id: f_00004-11-3 loss: 0.750264  [  128/  306]
train() client id: f_00004-11-4 loss: 0.897551  [  160/  306]
train() client id: f_00004-11-5 loss: 0.790474  [  192/  306]
train() client id: f_00004-11-6 loss: 0.664776  [  224/  306]
train() client id: f_00004-11-7 loss: 0.779893  [  256/  306]
train() client id: f_00004-11-8 loss: 0.764509  [  288/  306]
train() client id: f_00005-0-0 loss: 0.325809  [   32/  146]
train() client id: f_00005-0-1 loss: 0.407301  [   64/  146]
train() client id: f_00005-0-2 loss: 0.295814  [   96/  146]
train() client id: f_00005-0-3 loss: 0.244501  [  128/  146]
train() client id: f_00005-1-0 loss: 0.785523  [   32/  146]
train() client id: f_00005-1-1 loss: 0.152798  [   64/  146]
train() client id: f_00005-1-2 loss: 0.350714  [   96/  146]
train() client id: f_00005-1-3 loss: 0.412640  [  128/  146]
train() client id: f_00005-2-0 loss: 0.315605  [   32/  146]
train() client id: f_00005-2-1 loss: 0.432037  [   64/  146]
train() client id: f_00005-2-2 loss: 0.565915  [   96/  146]
train() client id: f_00005-2-3 loss: 0.383481  [  128/  146]
train() client id: f_00005-3-0 loss: 0.374681  [   32/  146]
train() client id: f_00005-3-1 loss: 0.258493  [   64/  146]
train() client id: f_00005-3-2 loss: 0.466822  [   96/  146]
train() client id: f_00005-3-3 loss: 0.558283  [  128/  146]
train() client id: f_00005-4-0 loss: 0.349593  [   32/  146]
train() client id: f_00005-4-1 loss: 0.413618  [   64/  146]
train() client id: f_00005-4-2 loss: 0.345229  [   96/  146]
train() client id: f_00005-4-3 loss: 0.435964  [  128/  146]
train() client id: f_00005-5-0 loss: 0.465375  [   32/  146]
train() client id: f_00005-5-1 loss: 0.382216  [   64/  146]
train() client id: f_00005-5-2 loss: 0.544983  [   96/  146]
train() client id: f_00005-5-3 loss: 0.226223  [  128/  146]
train() client id: f_00005-6-0 loss: 0.494242  [   32/  146]
train() client id: f_00005-6-1 loss: 0.363373  [   64/  146]
train() client id: f_00005-6-2 loss: 0.142348  [   96/  146]
train() client id: f_00005-6-3 loss: 0.449438  [  128/  146]
train() client id: f_00005-7-0 loss: 0.386218  [   32/  146]
train() client id: f_00005-7-1 loss: 0.337977  [   64/  146]
train() client id: f_00005-7-2 loss: 0.375317  [   96/  146]
train() client id: f_00005-7-3 loss: 0.343644  [  128/  146]
train() client id: f_00005-8-0 loss: 0.206979  [   32/  146]
train() client id: f_00005-8-1 loss: 0.584096  [   64/  146]
train() client id: f_00005-8-2 loss: 0.206344  [   96/  146]
train() client id: f_00005-8-3 loss: 0.461774  [  128/  146]
train() client id: f_00005-9-0 loss: 0.373910  [   32/  146]
train() client id: f_00005-9-1 loss: 0.298907  [   64/  146]
train() client id: f_00005-9-2 loss: 0.250456  [   96/  146]
train() client id: f_00005-9-3 loss: 0.445580  [  128/  146]
train() client id: f_00005-10-0 loss: 0.372165  [   32/  146]
train() client id: f_00005-10-1 loss: 0.551589  [   64/  146]
train() client id: f_00005-10-2 loss: 0.206906  [   96/  146]
train() client id: f_00005-10-3 loss: 0.308095  [  128/  146]
train() client id: f_00005-11-0 loss: 0.467331  [   32/  146]
train() client id: f_00005-11-1 loss: 0.368071  [   64/  146]
train() client id: f_00005-11-2 loss: 0.209762  [   96/  146]
train() client id: f_00005-11-3 loss: 0.476844  [  128/  146]
train() client id: f_00006-0-0 loss: 0.396683  [   32/   54]
train() client id: f_00006-1-0 loss: 0.527131  [   32/   54]
train() client id: f_00006-2-0 loss: 0.505945  [   32/   54]
train() client id: f_00006-3-0 loss: 0.516194  [   32/   54]
train() client id: f_00006-4-0 loss: 0.405612  [   32/   54]
train() client id: f_00006-5-0 loss: 0.400738  [   32/   54]
train() client id: f_00006-6-0 loss: 0.445646  [   32/   54]
train() client id: f_00006-7-0 loss: 0.459757  [   32/   54]
train() client id: f_00006-8-0 loss: 0.450712  [   32/   54]
train() client id: f_00006-9-0 loss: 0.465418  [   32/   54]
train() client id: f_00006-10-0 loss: 0.420735  [   32/   54]
train() client id: f_00006-11-0 loss: 0.437161  [   32/   54]
train() client id: f_00007-0-0 loss: 0.422798  [   32/  179]
train() client id: f_00007-0-1 loss: 0.601219  [   64/  179]
train() client id: f_00007-0-2 loss: 0.537292  [   96/  179]
train() client id: f_00007-0-3 loss: 0.361096  [  128/  179]
train() client id: f_00007-0-4 loss: 0.608932  [  160/  179]
train() client id: f_00007-1-0 loss: 0.383581  [   32/  179]
train() client id: f_00007-1-1 loss: 0.592115  [   64/  179]
train() client id: f_00007-1-2 loss: 0.347605  [   96/  179]
train() client id: f_00007-1-3 loss: 0.313839  [  128/  179]
train() client id: f_00007-1-4 loss: 0.630434  [  160/  179]
train() client id: f_00007-2-0 loss: 0.574993  [   32/  179]
train() client id: f_00007-2-1 loss: 0.312699  [   64/  179]
train() client id: f_00007-2-2 loss: 0.686689  [   96/  179]
train() client id: f_00007-2-3 loss: 0.442666  [  128/  179]
train() client id: f_00007-2-4 loss: 0.394206  [  160/  179]
train() client id: f_00007-3-0 loss: 0.304301  [   32/  179]
train() client id: f_00007-3-1 loss: 0.472149  [   64/  179]
train() client id: f_00007-3-2 loss: 0.493172  [   96/  179]
train() client id: f_00007-3-3 loss: 0.646456  [  128/  179]
train() client id: f_00007-3-4 loss: 0.310975  [  160/  179]
train() client id: f_00007-4-0 loss: 0.376916  [   32/  179]
train() client id: f_00007-4-1 loss: 0.312661  [   64/  179]
train() client id: f_00007-4-2 loss: 0.502084  [   96/  179]
train() client id: f_00007-4-3 loss: 0.595395  [  128/  179]
train() client id: f_00007-4-4 loss: 0.530346  [  160/  179]
train() client id: f_00007-5-0 loss: 0.393607  [   32/  179]
train() client id: f_00007-5-1 loss: 0.282151  [   64/  179]
train() client id: f_00007-5-2 loss: 0.472797  [   96/  179]
train() client id: f_00007-5-3 loss: 0.403282  [  128/  179]
train() client id: f_00007-5-4 loss: 0.604344  [  160/  179]
train() client id: f_00007-6-0 loss: 0.257516  [   32/  179]
train() client id: f_00007-6-1 loss: 0.541428  [   64/  179]
train() client id: f_00007-6-2 loss: 0.295728  [   96/  179]
train() client id: f_00007-6-3 loss: 0.632425  [  128/  179]
train() client id: f_00007-6-4 loss: 0.308658  [  160/  179]
train() client id: f_00007-7-0 loss: 0.483090  [   32/  179]
train() client id: f_00007-7-1 loss: 0.470568  [   64/  179]
train() client id: f_00007-7-2 loss: 0.654363  [   96/  179]
train() client id: f_00007-7-3 loss: 0.298731  [  128/  179]
train() client id: f_00007-7-4 loss: 0.249039  [  160/  179]
train() client id: f_00007-8-0 loss: 0.378663  [   32/  179]
train() client id: f_00007-8-1 loss: 0.658834  [   64/  179]
train() client id: f_00007-8-2 loss: 0.369560  [   96/  179]
train() client id: f_00007-8-3 loss: 0.254820  [  128/  179]
train() client id: f_00007-8-4 loss: 0.342103  [  160/  179]
train() client id: f_00007-9-0 loss: 0.275033  [   32/  179]
train() client id: f_00007-9-1 loss: 0.537579  [   64/  179]
train() client id: f_00007-9-2 loss: 0.267236  [   96/  179]
train() client id: f_00007-9-3 loss: 0.593674  [  128/  179]
train() client id: f_00007-9-4 loss: 0.385080  [  160/  179]
train() client id: f_00007-10-0 loss: 0.458416  [   32/  179]
train() client id: f_00007-10-1 loss: 0.382437  [   64/  179]
train() client id: f_00007-10-2 loss: 0.509397  [   96/  179]
train() client id: f_00007-10-3 loss: 0.318574  [  128/  179]
train() client id: f_00007-10-4 loss: 0.555537  [  160/  179]
train() client id: f_00007-11-0 loss: 0.266788  [   32/  179]
train() client id: f_00007-11-1 loss: 0.250344  [   64/  179]
train() client id: f_00007-11-2 loss: 0.367446  [   96/  179]
train() client id: f_00007-11-3 loss: 0.499147  [  128/  179]
train() client id: f_00007-11-4 loss: 0.723496  [  160/  179]
train() client id: f_00008-0-0 loss: 0.587702  [   32/  130]
train() client id: f_00008-0-1 loss: 0.529613  [   64/  130]
train() client id: f_00008-0-2 loss: 0.653618  [   96/  130]
train() client id: f_00008-0-3 loss: 0.579361  [  128/  130]
train() client id: f_00008-1-0 loss: 0.761365  [   32/  130]
train() client id: f_00008-1-1 loss: 0.679130  [   64/  130]
train() client id: f_00008-1-2 loss: 0.482105  [   96/  130]
train() client id: f_00008-1-3 loss: 0.493199  [  128/  130]
train() client id: f_00008-2-0 loss: 0.567947  [   32/  130]
train() client id: f_00008-2-1 loss: 0.624022  [   64/  130]
train() client id: f_00008-2-2 loss: 0.670300  [   96/  130]
train() client id: f_00008-2-3 loss: 0.566657  [  128/  130]
train() client id: f_00008-3-0 loss: 0.533496  [   32/  130]
train() client id: f_00008-3-1 loss: 0.588265  [   64/  130]
train() client id: f_00008-3-2 loss: 0.729508  [   96/  130]
train() client id: f_00008-3-3 loss: 0.584150  [  128/  130]
train() client id: f_00008-4-0 loss: 0.535141  [   32/  130]
train() client id: f_00008-4-1 loss: 0.467205  [   64/  130]
train() client id: f_00008-4-2 loss: 0.817594  [   96/  130]
train() client id: f_00008-4-3 loss: 0.598851  [  128/  130]
train() client id: f_00008-5-0 loss: 0.631879  [   32/  130]
train() client id: f_00008-5-1 loss: 0.579897  [   64/  130]
train() client id: f_00008-5-2 loss: 0.578931  [   96/  130]
train() client id: f_00008-5-3 loss: 0.646639  [  128/  130]
train() client id: f_00008-6-0 loss: 0.684309  [   32/  130]
train() client id: f_00008-6-1 loss: 0.641691  [   64/  130]
train() client id: f_00008-6-2 loss: 0.577822  [   96/  130]
train() client id: f_00008-6-3 loss: 0.507766  [  128/  130]
train() client id: f_00008-7-0 loss: 0.587561  [   32/  130]
train() client id: f_00008-7-1 loss: 0.558887  [   64/  130]
train() client id: f_00008-7-2 loss: 0.595617  [   96/  130]
train() client id: f_00008-7-3 loss: 0.679164  [  128/  130]
train() client id: f_00008-8-0 loss: 0.646754  [   32/  130]
train() client id: f_00008-8-1 loss: 0.544578  [   64/  130]
train() client id: f_00008-8-2 loss: 0.612924  [   96/  130]
train() client id: f_00008-8-3 loss: 0.613610  [  128/  130]
train() client id: f_00008-9-0 loss: 0.496516  [   32/  130]
train() client id: f_00008-9-1 loss: 0.627687  [   64/  130]
train() client id: f_00008-9-2 loss: 0.506756  [   96/  130]
train() client id: f_00008-9-3 loss: 0.806372  [  128/  130]
train() client id: f_00008-10-0 loss: 0.595653  [   32/  130]
train() client id: f_00008-10-1 loss: 0.723132  [   64/  130]
train() client id: f_00008-10-2 loss: 0.598909  [   96/  130]
train() client id: f_00008-10-3 loss: 0.502847  [  128/  130]
train() client id: f_00008-11-0 loss: 0.472185  [   32/  130]
train() client id: f_00008-11-1 loss: 0.763356  [   64/  130]
train() client id: f_00008-11-2 loss: 0.685288  [   96/  130]
train() client id: f_00008-11-3 loss: 0.526703  [  128/  130]
train() client id: f_00009-0-0 loss: 1.034030  [   32/  118]
train() client id: f_00009-0-1 loss: 1.000942  [   64/  118]
train() client id: f_00009-0-2 loss: 1.138773  [   96/  118]
train() client id: f_00009-1-0 loss: 1.162416  [   32/  118]
train() client id: f_00009-1-1 loss: 0.832363  [   64/  118]
train() client id: f_00009-1-2 loss: 1.025879  [   96/  118]
train() client id: f_00009-2-0 loss: 1.094653  [   32/  118]
train() client id: f_00009-2-1 loss: 0.914507  [   64/  118]
train() client id: f_00009-2-2 loss: 0.951123  [   96/  118]
train() client id: f_00009-3-0 loss: 0.949303  [   32/  118]
train() client id: f_00009-3-1 loss: 0.965283  [   64/  118]
train() client id: f_00009-3-2 loss: 1.022798  [   96/  118]
train() client id: f_00009-4-0 loss: 0.949895  [   32/  118]
train() client id: f_00009-4-1 loss: 0.940737  [   64/  118]
train() client id: f_00009-4-2 loss: 0.837240  [   96/  118]
train() client id: f_00009-5-0 loss: 0.970546  [   32/  118]
train() client id: f_00009-5-1 loss: 0.839280  [   64/  118]
train() client id: f_00009-5-2 loss: 0.940020  [   96/  118]
train() client id: f_00009-6-0 loss: 0.932899  [   32/  118]
train() client id: f_00009-6-1 loss: 0.815742  [   64/  118]
train() client id: f_00009-6-2 loss: 0.982522  [   96/  118]
train() client id: f_00009-7-0 loss: 0.858381  [   32/  118]
train() client id: f_00009-7-1 loss: 0.983169  [   64/  118]
train() client id: f_00009-7-2 loss: 0.865248  [   96/  118]
train() client id: f_00009-8-0 loss: 0.873708  [   32/  118]
train() client id: f_00009-8-1 loss: 0.784922  [   64/  118]
train() client id: f_00009-8-2 loss: 0.922444  [   96/  118]
train() client id: f_00009-9-0 loss: 0.775517  [   32/  118]
train() client id: f_00009-9-1 loss: 0.929027  [   64/  118]
train() client id: f_00009-9-2 loss: 0.793731  [   96/  118]
train() client id: f_00009-10-0 loss: 0.832040  [   32/  118]
train() client id: f_00009-10-1 loss: 0.710122  [   64/  118]
train() client id: f_00009-10-2 loss: 0.941439  [   96/  118]
train() client id: f_00009-11-0 loss: 0.864277  [   32/  118]
train() client id: f_00009-11-1 loss: 0.910938  [   64/  118]
train() client id: f_00009-11-2 loss: 0.927775  [   96/  118]
At round 48 accuracy: 0.6472148541114059
At round 48 training accuracy: 0.5922199865861838
At round 48 training loss: 0.8312609972627557
update_location
xs = [  -3.9056584     4.20031788  260.00902392   18.81129433    0.97929623
    3.95640986 -222.44319194 -201.32485185  244.66397685 -187.06087855]
ys = [ 252.5879595   235.55583871    1.32061395 -222.45517586  214.35018685
  197.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [271.6908748  255.93787492 278.57931822 244.62250522 236.53110075
 221.68917748 243.90134108 224.79406626 264.89456684 212.15038096]
dists_bs = [187.86475237 189.15539519 468.39874042 442.19162659 180.32512278
 180.40477296 183.85181945 176.15576144 448.22134189 171.42332987]
uav_gains = [4.75955329e-12 6.59322551e-12 4.11980872e-12 8.22994915e-12
 9.55255008e-12 1.22821086e-11 8.34286394e-12 1.16790101e-11
 5.48618892e-12 1.42491555e-11]
bs_gains = [4.74818064e-11 4.65802305e-11 3.67764727e-12 4.32100377e-12
 5.32520858e-11 5.31862805e-11 5.04410218e-11 5.68568637e-11
 4.16020754e-12 6.13618281e-11]
Round 49
-------------------------------
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.53180875  9.28555743  4.47092374  1.6235926  10.7068955   5.1513321
  2.0049889   6.33296008  4.67865356  4.17679842]
obj_prev = 52.96351106757935
eta_min = 4.606985121778528e-21	eta_max = 0.9400638118405741
af = 11.129531482767057	bf = 1.180489388448874	zeta = 12.242484631043764	eta = 0.909090909090909
af = 11.129531482767057	bf = 1.180489388448874	zeta = 24.65371738730223	eta = 0.45143421204703454
af = 11.129531482767057	bf = 1.180489388448874	zeta = 18.287853905358034	eta = 0.608575043324591
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.136885852504452	eta = 0.6494488892881634
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.071238826404898	eta = 0.6519463289068677
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.07100360444631	eta = 0.6519553120982448
eta = 0.6519553120982448
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.03529932 0.07424063 0.03473901 0.01204659 0.08572692 0.04090239
 0.01512828 0.05014745 0.03641993 0.03305809]
ene_total = [1.60196419 2.67053184 1.60796312 0.76579092 3.04020143 1.57233453
 0.86514606 1.976894   1.66129063 1.30888688]
ti_comp = [0.70507767 0.76819322 0.69764277 0.72788409 0.77017386 0.77015604
 0.7283619  0.73907045 0.6981679  0.77215943]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [5.52974468e-06 4.33375200e-05 5.38351868e-06 2.06228308e-07
 6.63825764e-05 7.21055148e-06 4.07900477e-07 1.44296163e-05
 6.19409234e-06 3.78703601e-06]
ene_total = [0.44615416 0.2406999  0.47049736 0.37129277 0.23496834 0.23308889
 0.36973465 0.33512519 0.46880422 0.22641604]
optimize_network iter = 0 obj = 3.396781522582919
eta = 0.6519553120982448
freqs = [25032220.23451398 48321587.59240132 24897421.60469212  8275076.45541563
 55654263.85367705 26554609.35010472 10385139.39706882 33926025.18152012
 26082502.95796476 21406261.0487775 ]
eta_min = 0.6519553120982468	eta_max = 0.6979914102198819
af = 0.004073073698070901	bf = 1.180489388448874	zeta = 0.004480381067877992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.23203954e-06 9.65569680e-06 1.19946005e-06 4.59481302e-08
 1.47901871e-05 1.60652707e-06 9.08811424e-08 3.21495093e-06
 1.38005769e-06 8.43760128e-07]
ene_total = [1.70755859 0.91729463 1.80077061 1.42147114 0.89310582 0.89167624
 1.41548626 1.28161791 1.79420944 0.86646288]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 1 obj = 3.9135003002771818
eta = 0.6979914102198819
freqs = [24982229.19460944 47493855.941672   24897421.60469214  8210343.90468077
 54677108.13463603 26088475.84559726 10302681.03588401 33568945.36426646
 26078780.65421337 21021289.55092173]
eta_min = 0.697991410219886	eta_max = 0.6979914102198779
af = 0.003953064498406836	bf = 1.180489388448874	zeta = 0.004348370948247519	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.22712352e-06 9.32773270e-06 1.19946005e-06 4.52320749e-08
 1.42753859e-05 1.55062088e-06 8.94436732e-08 3.14763077e-06
 1.37966381e-06 8.13684556e-07]
ene_total = [1.70755797 0.91725351 1.80077061 1.42147105 0.89304128 0.89166923
 1.41548608 1.28160947 1.79420939 0.86645911]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 2 obj = 3.9135003002771303
eta = 0.6979914102198779
freqs = [24982229.19460944 47493855.94167206 24897421.60469212  8210343.90468077
 54677108.13463611 26088475.8455973  10302681.03588402 33568945.36426648
 26078780.65421337 21021289.55092177]
Done!
At round 49 energy consumption: 3.9135003002771818
At round 49 eta: 0.6979914102198779
At round 49 local rounds: 11.773444044723618
At round 49 global rounds: 37.74017276480797
At round 49 a_n: 11.055310507788512
gradient difference: 0.3314484655857086
train() client id: f_00000-0-0 loss: 0.972724  [   32/  126]
train() client id: f_00000-0-1 loss: 1.418998  [   64/  126]
train() client id: f_00000-0-2 loss: 1.215142  [   96/  126]
train() client id: f_00000-1-0 loss: 1.038403  [   32/  126]
train() client id: f_00000-1-1 loss: 1.078733  [   64/  126]
train() client id: f_00000-1-2 loss: 0.943406  [   96/  126]
train() client id: f_00000-2-0 loss: 1.045940  [   32/  126]
train() client id: f_00000-2-1 loss: 1.045760  [   64/  126]
train() client id: f_00000-2-2 loss: 0.906470  [   96/  126]
train() client id: f_00000-3-0 loss: 0.839177  [   32/  126]
train() client id: f_00000-3-1 loss: 1.009545  [   64/  126]
train() client id: f_00000-3-2 loss: 0.976127  [   96/  126]
train() client id: f_00000-4-0 loss: 0.799521  [   32/  126]
train() client id: f_00000-4-1 loss: 0.871890  [   64/  126]
train() client id: f_00000-4-2 loss: 0.693124  [   96/  126]
train() client id: f_00000-5-0 loss: 0.814198  [   32/  126]
train() client id: f_00000-5-1 loss: 0.874496  [   64/  126]
train() client id: f_00000-5-2 loss: 0.706790  [   96/  126]
train() client id: f_00000-6-0 loss: 0.691994  [   32/  126]
train() client id: f_00000-6-1 loss: 0.826149  [   64/  126]
train() client id: f_00000-6-2 loss: 0.726494  [   96/  126]
train() client id: f_00000-7-0 loss: 0.772066  [   32/  126]
train() client id: f_00000-7-1 loss: 0.706645  [   64/  126]
train() client id: f_00000-7-2 loss: 0.744940  [   96/  126]
train() client id: f_00000-8-0 loss: 0.729139  [   32/  126]
train() client id: f_00000-8-1 loss: 0.690220  [   64/  126]
train() client id: f_00000-8-2 loss: 0.817144  [   96/  126]
train() client id: f_00000-9-0 loss: 0.593558  [   32/  126]
train() client id: f_00000-9-1 loss: 0.655073  [   64/  126]
train() client id: f_00000-9-2 loss: 0.909699  [   96/  126]
train() client id: f_00000-10-0 loss: 0.786317  [   32/  126]
train() client id: f_00000-10-1 loss: 0.620473  [   64/  126]
train() client id: f_00000-10-2 loss: 0.656649  [   96/  126]
train() client id: f_00001-0-0 loss: 0.521497  [   32/  265]
train() client id: f_00001-0-1 loss: 0.550355  [   64/  265]
train() client id: f_00001-0-2 loss: 0.369625  [   96/  265]
train() client id: f_00001-0-3 loss: 0.543910  [  128/  265]
train() client id: f_00001-0-4 loss: 0.442554  [  160/  265]
train() client id: f_00001-0-5 loss: 0.387083  [  192/  265]
train() client id: f_00001-0-6 loss: 0.464587  [  224/  265]
train() client id: f_00001-0-7 loss: 0.494511  [  256/  265]
train() client id: f_00001-1-0 loss: 0.407389  [   32/  265]
train() client id: f_00001-1-1 loss: 0.388143  [   64/  265]
train() client id: f_00001-1-2 loss: 0.428474  [   96/  265]
train() client id: f_00001-1-3 loss: 0.428126  [  128/  265]
train() client id: f_00001-1-4 loss: 0.637126  [  160/  265]
train() client id: f_00001-1-5 loss: 0.503943  [  192/  265]
train() client id: f_00001-1-6 loss: 0.511974  [  224/  265]
train() client id: f_00001-1-7 loss: 0.459831  [  256/  265]
train() client id: f_00001-2-0 loss: 0.464420  [   32/  265]
train() client id: f_00001-2-1 loss: 0.553959  [   64/  265]
train() client id: f_00001-2-2 loss: 0.366941  [   96/  265]
train() client id: f_00001-2-3 loss: 0.360421  [  128/  265]
train() client id: f_00001-2-4 loss: 0.535989  [  160/  265]
train() client id: f_00001-2-5 loss: 0.371570  [  192/  265]
train() client id: f_00001-2-6 loss: 0.588172  [  224/  265]
train() client id: f_00001-2-7 loss: 0.492020  [  256/  265]
train() client id: f_00001-3-0 loss: 0.399033  [   32/  265]
train() client id: f_00001-3-1 loss: 0.472034  [   64/  265]
train() client id: f_00001-3-2 loss: 0.480547  [   96/  265]
train() client id: f_00001-3-3 loss: 0.553578  [  128/  265]
train() client id: f_00001-3-4 loss: 0.503062  [  160/  265]
train() client id: f_00001-3-5 loss: 0.355340  [  192/  265]
train() client id: f_00001-3-6 loss: 0.484529  [  224/  265]
train() client id: f_00001-3-7 loss: 0.418890  [  256/  265]
train() client id: f_00001-4-0 loss: 0.515732  [   32/  265]
train() client id: f_00001-4-1 loss: 0.511766  [   64/  265]
train() client id: f_00001-4-2 loss: 0.360571  [   96/  265]
train() client id: f_00001-4-3 loss: 0.501406  [  128/  265]
train() client id: f_00001-4-4 loss: 0.360648  [  160/  265]
train() client id: f_00001-4-5 loss: 0.559013  [  192/  265]
train() client id: f_00001-4-6 loss: 0.436781  [  224/  265]
train() client id: f_00001-4-7 loss: 0.402130  [  256/  265]
train() client id: f_00001-5-0 loss: 0.364043  [   32/  265]
train() client id: f_00001-5-1 loss: 0.501282  [   64/  265]
train() client id: f_00001-5-2 loss: 0.373433  [   96/  265]
train() client id: f_00001-5-3 loss: 0.497096  [  128/  265]
train() client id: f_00001-5-4 loss: 0.445055  [  160/  265]
train() client id: f_00001-5-5 loss: 0.444004  [  192/  265]
train() client id: f_00001-5-6 loss: 0.522293  [  224/  265]
train() client id: f_00001-5-7 loss: 0.487288  [  256/  265]
train() client id: f_00001-6-0 loss: 0.470867  [   32/  265]
train() client id: f_00001-6-1 loss: 0.469122  [   64/  265]
train() client id: f_00001-6-2 loss: 0.520019  [   96/  265]
train() client id: f_00001-6-3 loss: 0.518482  [  128/  265]
train() client id: f_00001-6-4 loss: 0.446788  [  160/  265]
train() client id: f_00001-6-5 loss: 0.378611  [  192/  265]
train() client id: f_00001-6-6 loss: 0.456947  [  224/  265]
train() client id: f_00001-6-7 loss: 0.361463  [  256/  265]
train() client id: f_00001-7-0 loss: 0.542100  [   32/  265]
train() client id: f_00001-7-1 loss: 0.462789  [   64/  265]
train() client id: f_00001-7-2 loss: 0.399617  [   96/  265]
train() client id: f_00001-7-3 loss: 0.547067  [  128/  265]
train() client id: f_00001-7-4 loss: 0.357874  [  160/  265]
train() client id: f_00001-7-5 loss: 0.441109  [  192/  265]
train() client id: f_00001-7-6 loss: 0.357458  [  224/  265]
train() client id: f_00001-7-7 loss: 0.430925  [  256/  265]
train() client id: f_00001-8-0 loss: 0.395959  [   32/  265]
train() client id: f_00001-8-1 loss: 0.417977  [   64/  265]
train() client id: f_00001-8-2 loss: 0.575587  [   96/  265]
train() client id: f_00001-8-3 loss: 0.367658  [  128/  265]
train() client id: f_00001-8-4 loss: 0.434970  [  160/  265]
train() client id: f_00001-8-5 loss: 0.468078  [  192/  265]
train() client id: f_00001-8-6 loss: 0.454426  [  224/  265]
train() client id: f_00001-8-7 loss: 0.412704  [  256/  265]
train() client id: f_00001-9-0 loss: 0.393024  [   32/  265]
train() client id: f_00001-9-1 loss: 0.387186  [   64/  265]
train() client id: f_00001-9-2 loss: 0.448127  [   96/  265]
train() client id: f_00001-9-3 loss: 0.559657  [  128/  265]
train() client id: f_00001-9-4 loss: 0.483901  [  160/  265]
train() client id: f_00001-9-5 loss: 0.529366  [  192/  265]
train() client id: f_00001-9-6 loss: 0.354079  [  224/  265]
train() client id: f_00001-9-7 loss: 0.450242  [  256/  265]
train() client id: f_00001-10-0 loss: 0.387566  [   32/  265]
train() client id: f_00001-10-1 loss: 0.563446  [   64/  265]
train() client id: f_00001-10-2 loss: 0.451577  [   96/  265]
train() client id: f_00001-10-3 loss: 0.361652  [  128/  265]
train() client id: f_00001-10-4 loss: 0.504205  [  160/  265]
train() client id: f_00001-10-5 loss: 0.378818  [  192/  265]
train() client id: f_00001-10-6 loss: 0.410608  [  224/  265]
train() client id: f_00001-10-7 loss: 0.461738  [  256/  265]
train() client id: f_00002-0-0 loss: 1.230281  [   32/  124]
train() client id: f_00002-0-1 loss: 0.920831  [   64/  124]
train() client id: f_00002-0-2 loss: 1.130923  [   96/  124]
train() client id: f_00002-1-0 loss: 0.932389  [   32/  124]
train() client id: f_00002-1-1 loss: 1.128019  [   64/  124]
train() client id: f_00002-1-2 loss: 1.118709  [   96/  124]
train() client id: f_00002-2-0 loss: 0.994383  [   32/  124]
train() client id: f_00002-2-1 loss: 1.079184  [   64/  124]
train() client id: f_00002-2-2 loss: 0.946495  [   96/  124]
train() client id: f_00002-3-0 loss: 1.073700  [   32/  124]
train() client id: f_00002-3-1 loss: 0.969854  [   64/  124]
train() client id: f_00002-3-2 loss: 0.953012  [   96/  124]
train() client id: f_00002-4-0 loss: 1.030607  [   32/  124]
train() client id: f_00002-4-1 loss: 0.845696  [   64/  124]
train() client id: f_00002-4-2 loss: 0.865747  [   96/  124]
train() client id: f_00002-5-0 loss: 0.805132  [   32/  124]
train() client id: f_00002-5-1 loss: 0.640967  [   64/  124]
train() client id: f_00002-5-2 loss: 1.292420  [   96/  124]
train() client id: f_00002-6-0 loss: 1.010720  [   32/  124]
train() client id: f_00002-6-1 loss: 0.899712  [   64/  124]
train() client id: f_00002-6-2 loss: 0.858978  [   96/  124]
train() client id: f_00002-7-0 loss: 0.907283  [   32/  124]
train() client id: f_00002-7-1 loss: 0.763019  [   64/  124]
train() client id: f_00002-7-2 loss: 0.922409  [   96/  124]
train() client id: f_00002-8-0 loss: 0.923874  [   32/  124]
train() client id: f_00002-8-1 loss: 0.800761  [   64/  124]
train() client id: f_00002-8-2 loss: 0.845836  [   96/  124]
train() client id: f_00002-9-0 loss: 0.698043  [   32/  124]
train() client id: f_00002-9-1 loss: 0.787701  [   64/  124]
train() client id: f_00002-9-2 loss: 1.066129  [   96/  124]
train() client id: f_00002-10-0 loss: 0.843153  [   32/  124]
train() client id: f_00002-10-1 loss: 0.910979  [   64/  124]
train() client id: f_00002-10-2 loss: 0.700256  [   96/  124]
train() client id: f_00003-0-0 loss: 0.921989  [   32/   43]
train() client id: f_00003-1-0 loss: 1.102099  [   32/   43]
train() client id: f_00003-2-0 loss: 0.949094  [   32/   43]
train() client id: f_00003-3-0 loss: 1.048343  [   32/   43]
train() client id: f_00003-4-0 loss: 0.799102  [   32/   43]
train() client id: f_00003-5-0 loss: 0.907258  [   32/   43]
train() client id: f_00003-6-0 loss: 0.821257  [   32/   43]
train() client id: f_00003-7-0 loss: 0.912194  [   32/   43]
train() client id: f_00003-8-0 loss: 0.992452  [   32/   43]
train() client id: f_00003-9-0 loss: 1.101944  [   32/   43]
train() client id: f_00003-10-0 loss: 0.681443  [   32/   43]
train() client id: f_00004-0-0 loss: 0.837380  [   32/  306]
train() client id: f_00004-0-1 loss: 0.744588  [   64/  306]
train() client id: f_00004-0-2 loss: 0.879727  [   96/  306]
train() client id: f_00004-0-3 loss: 0.714704  [  128/  306]
train() client id: f_00004-0-4 loss: 0.733275  [  160/  306]
train() client id: f_00004-0-5 loss: 0.801167  [  192/  306]
train() client id: f_00004-0-6 loss: 0.883171  [  224/  306]
train() client id: f_00004-0-7 loss: 1.031792  [  256/  306]
train() client id: f_00004-0-8 loss: 0.766459  [  288/  306]
train() client id: f_00004-1-0 loss: 0.663183  [   32/  306]
train() client id: f_00004-1-1 loss: 0.768628  [   64/  306]
train() client id: f_00004-1-2 loss: 0.859411  [   96/  306]
train() client id: f_00004-1-3 loss: 0.947219  [  128/  306]
train() client id: f_00004-1-4 loss: 0.920067  [  160/  306]
train() client id: f_00004-1-5 loss: 0.835561  [  192/  306]
train() client id: f_00004-1-6 loss: 0.880092  [  224/  306]
train() client id: f_00004-1-7 loss: 0.751809  [  256/  306]
train() client id: f_00004-1-8 loss: 0.730477  [  288/  306]
train() client id: f_00004-2-0 loss: 0.809790  [   32/  306]
train() client id: f_00004-2-1 loss: 0.914456  [   64/  306]
train() client id: f_00004-2-2 loss: 0.806220  [   96/  306]
train() client id: f_00004-2-3 loss: 0.823113  [  128/  306]
train() client id: f_00004-2-4 loss: 0.928744  [  160/  306]
train() client id: f_00004-2-5 loss: 0.812758  [  192/  306]
train() client id: f_00004-2-6 loss: 0.699084  [  224/  306]
train() client id: f_00004-2-7 loss: 0.765278  [  256/  306]
train() client id: f_00004-2-8 loss: 0.772374  [  288/  306]
train() client id: f_00004-3-0 loss: 0.859441  [   32/  306]
train() client id: f_00004-3-1 loss: 0.910691  [   64/  306]
train() client id: f_00004-3-2 loss: 0.939730  [   96/  306]
train() client id: f_00004-3-3 loss: 0.681419  [  128/  306]
train() client id: f_00004-3-4 loss: 0.918650  [  160/  306]
train() client id: f_00004-3-5 loss: 0.685418  [  192/  306]
train() client id: f_00004-3-6 loss: 0.906846  [  224/  306]
train() client id: f_00004-3-7 loss: 0.758278  [  256/  306]
train() client id: f_00004-3-8 loss: 0.743794  [  288/  306]
train() client id: f_00004-4-0 loss: 0.924702  [   32/  306]
train() client id: f_00004-4-1 loss: 0.732087  [   64/  306]
train() client id: f_00004-4-2 loss: 0.854760  [   96/  306]
train() client id: f_00004-4-3 loss: 0.919008  [  128/  306]
train() client id: f_00004-4-4 loss: 0.818966  [  160/  306]
train() client id: f_00004-4-5 loss: 0.669018  [  192/  306]
train() client id: f_00004-4-6 loss: 0.922834  [  224/  306]
train() client id: f_00004-4-7 loss: 0.725522  [  256/  306]
train() client id: f_00004-4-8 loss: 0.824332  [  288/  306]
train() client id: f_00004-5-0 loss: 0.789018  [   32/  306]
train() client id: f_00004-5-1 loss: 0.886142  [   64/  306]
train() client id: f_00004-5-2 loss: 0.741775  [   96/  306]
train() client id: f_00004-5-3 loss: 0.956806  [  128/  306]
train() client id: f_00004-5-4 loss: 0.807557  [  160/  306]
train() client id: f_00004-5-5 loss: 0.703594  [  192/  306]
train() client id: f_00004-5-6 loss: 0.776692  [  224/  306]
train() client id: f_00004-5-7 loss: 0.943501  [  256/  306]
train() client id: f_00004-5-8 loss: 0.824896  [  288/  306]
train() client id: f_00004-6-0 loss: 0.910863  [   32/  306]
train() client id: f_00004-6-1 loss: 0.959176  [   64/  306]
train() client id: f_00004-6-2 loss: 0.851111  [   96/  306]
train() client id: f_00004-6-3 loss: 0.749961  [  128/  306]
train() client id: f_00004-6-4 loss: 0.800367  [  160/  306]
train() client id: f_00004-6-5 loss: 0.844509  [  192/  306]
train() client id: f_00004-6-6 loss: 0.831725  [  224/  306]
train() client id: f_00004-6-7 loss: 0.797424  [  256/  306]
train() client id: f_00004-6-8 loss: 0.727274  [  288/  306]
train() client id: f_00004-7-0 loss: 0.869443  [   32/  306]
train() client id: f_00004-7-1 loss: 0.760707  [   64/  306]
train() client id: f_00004-7-2 loss: 0.828353  [   96/  306]
train() client id: f_00004-7-3 loss: 0.893377  [  128/  306]
train() client id: f_00004-7-4 loss: 0.954107  [  160/  306]
train() client id: f_00004-7-5 loss: 0.801483  [  192/  306]
train() client id: f_00004-7-6 loss: 0.779531  [  224/  306]
train() client id: f_00004-7-7 loss: 0.761357  [  256/  306]
train() client id: f_00004-7-8 loss: 0.812697  [  288/  306]
train() client id: f_00004-8-0 loss: 0.787815  [   32/  306]
train() client id: f_00004-8-1 loss: 0.731358  [   64/  306]
train() client id: f_00004-8-2 loss: 0.808707  [   96/  306]
train() client id: f_00004-8-3 loss: 0.747394  [  128/  306]
train() client id: f_00004-8-4 loss: 0.803567  [  160/  306]
train() client id: f_00004-8-5 loss: 0.862672  [  192/  306]
train() client id: f_00004-8-6 loss: 0.889991  [  224/  306]
train() client id: f_00004-8-7 loss: 0.841844  [  256/  306]
train() client id: f_00004-8-8 loss: 0.980280  [  288/  306]
train() client id: f_00004-9-0 loss: 0.846731  [   32/  306]
train() client id: f_00004-9-1 loss: 0.882722  [   64/  306]
train() client id: f_00004-9-2 loss: 0.848198  [   96/  306]
train() client id: f_00004-9-3 loss: 0.756879  [  128/  306]
train() client id: f_00004-9-4 loss: 0.793239  [  160/  306]
train() client id: f_00004-9-5 loss: 0.796752  [  192/  306]
train() client id: f_00004-9-6 loss: 1.054221  [  224/  306]
train() client id: f_00004-9-7 loss: 0.725612  [  256/  306]
train() client id: f_00004-9-8 loss: 0.675107  [  288/  306]
train() client id: f_00004-10-0 loss: 0.714148  [   32/  306]
train() client id: f_00004-10-1 loss: 0.734547  [   64/  306]
train() client id: f_00004-10-2 loss: 0.833655  [   96/  306]
train() client id: f_00004-10-3 loss: 0.739943  [  128/  306]
train() client id: f_00004-10-4 loss: 0.766639  [  160/  306]
train() client id: f_00004-10-5 loss: 0.743876  [  192/  306]
train() client id: f_00004-10-6 loss: 0.863598  [  224/  306]
train() client id: f_00004-10-7 loss: 0.896463  [  256/  306]
train() client id: f_00004-10-8 loss: 1.061949  [  288/  306]
train() client id: f_00005-0-0 loss: 0.397377  [   32/  146]
train() client id: f_00005-0-1 loss: 0.480691  [   64/  146]
train() client id: f_00005-0-2 loss: 0.725182  [   96/  146]
train() client id: f_00005-0-3 loss: 0.555423  [  128/  146]
train() client id: f_00005-1-0 loss: 0.358461  [   32/  146]
train() client id: f_00005-1-1 loss: 0.469143  [   64/  146]
train() client id: f_00005-1-2 loss: 0.564675  [   96/  146]
train() client id: f_00005-1-3 loss: 0.829526  [  128/  146]
train() client id: f_00005-2-0 loss: 0.625930  [   32/  146]
train() client id: f_00005-2-1 loss: 0.638537  [   64/  146]
train() client id: f_00005-2-2 loss: 0.341056  [   96/  146]
train() client id: f_00005-2-3 loss: 0.553804  [  128/  146]
train() client id: f_00005-3-0 loss: 0.696120  [   32/  146]
train() client id: f_00005-3-1 loss: 0.689596  [   64/  146]
train() client id: f_00005-3-2 loss: 0.425164  [   96/  146]
train() client id: f_00005-3-3 loss: 0.446679  [  128/  146]
train() client id: f_00005-4-0 loss: 0.429317  [   32/  146]
train() client id: f_00005-4-1 loss: 0.756172  [   64/  146]
train() client id: f_00005-4-2 loss: 0.429950  [   96/  146]
train() client id: f_00005-4-3 loss: 0.481962  [  128/  146]
train() client id: f_00005-5-0 loss: 0.558815  [   32/  146]
train() client id: f_00005-5-1 loss: 0.752701  [   64/  146]
train() client id: f_00005-5-2 loss: 0.474370  [   96/  146]
train() client id: f_00005-5-3 loss: 0.450310  [  128/  146]
train() client id: f_00005-6-0 loss: 0.530413  [   32/  146]
train() client id: f_00005-6-1 loss: 0.583046  [   64/  146]
train() client id: f_00005-6-2 loss: 0.531375  [   96/  146]
train() client id: f_00005-6-3 loss: 0.485038  [  128/  146]
train() client id: f_00005-7-0 loss: 0.373485  [   32/  146]
train() client id: f_00005-7-1 loss: 0.509694  [   64/  146]
train() client id: f_00005-7-2 loss: 0.482869  [   96/  146]
train() client id: f_00005-7-3 loss: 0.897307  [  128/  146]
train() client id: f_00005-8-0 loss: 0.669237  [   32/  146]
train() client id: f_00005-8-1 loss: 0.290149  [   64/  146]
train() client id: f_00005-8-2 loss: 0.683101  [   96/  146]
train() client id: f_00005-8-3 loss: 0.576550  [  128/  146]
train() client id: f_00005-9-0 loss: 0.633480  [   32/  146]
train() client id: f_00005-9-1 loss: 0.468362  [   64/  146]
train() client id: f_00005-9-2 loss: 0.761788  [   96/  146]
train() client id: f_00005-9-3 loss: 0.335331  [  128/  146]
train() client id: f_00005-10-0 loss: 0.545482  [   32/  146]
train() client id: f_00005-10-1 loss: 0.358191  [   64/  146]
train() client id: f_00005-10-2 loss: 0.788822  [   96/  146]
train() client id: f_00005-10-3 loss: 0.624305  [  128/  146]
train() client id: f_00006-0-0 loss: 0.469993  [   32/   54]
train() client id: f_00006-1-0 loss: 0.566945  [   32/   54]
train() client id: f_00006-2-0 loss: 0.560544  [   32/   54]
train() client id: f_00006-3-0 loss: 0.515018  [   32/   54]
train() client id: f_00006-4-0 loss: 0.514831  [   32/   54]
train() client id: f_00006-5-0 loss: 0.453230  [   32/   54]
train() client id: f_00006-6-0 loss: 0.555390  [   32/   54]
train() client id: f_00006-7-0 loss: 0.524163  [   32/   54]
train() client id: f_00006-8-0 loss: 0.570708  [   32/   54]
train() client id: f_00006-9-0 loss: 0.567199  [   32/   54]
train() client id: f_00006-10-0 loss: 0.496945  [   32/   54]
train() client id: f_00007-0-0 loss: 0.513055  [   32/  179]
train() client id: f_00007-0-1 loss: 0.174023  [   64/  179]
train() client id: f_00007-0-2 loss: 0.276421  [   96/  179]
train() client id: f_00007-0-3 loss: 0.280855  [  128/  179]
train() client id: f_00007-0-4 loss: 0.338033  [  160/  179]
train() client id: f_00007-1-0 loss: 0.178620  [   32/  179]
train() client id: f_00007-1-1 loss: 0.597226  [   64/  179]
train() client id: f_00007-1-2 loss: 0.150312  [   96/  179]
train() client id: f_00007-1-3 loss: 0.307144  [  128/  179]
train() client id: f_00007-1-4 loss: 0.291694  [  160/  179]
train() client id: f_00007-2-0 loss: 0.121015  [   32/  179]
train() client id: f_00007-2-1 loss: 0.663682  [   64/  179]
train() client id: f_00007-2-2 loss: 0.200711  [   96/  179]
train() client id: f_00007-2-3 loss: 0.237989  [  128/  179]
train() client id: f_00007-2-4 loss: 0.255179  [  160/  179]
train() client id: f_00007-3-0 loss: 0.301410  [   32/  179]
train() client id: f_00007-3-1 loss: 0.530363  [   64/  179]
train() client id: f_00007-3-2 loss: 0.277555  [   96/  179]
train() client id: f_00007-3-3 loss: 0.237729  [  128/  179]
train() client id: f_00007-3-4 loss: 0.123617  [  160/  179]
train() client id: f_00007-4-0 loss: 0.224123  [   32/  179]
train() client id: f_00007-4-1 loss: 0.307718  [   64/  179]
train() client id: f_00007-4-2 loss: 0.154336  [   96/  179]
train() client id: f_00007-4-3 loss: 0.310418  [  128/  179]
train() client id: f_00007-4-4 loss: 0.204368  [  160/  179]
train() client id: f_00007-5-0 loss: 0.155854  [   32/  179]
train() client id: f_00007-5-1 loss: 0.413576  [   64/  179]
train() client id: f_00007-5-2 loss: 0.236655  [   96/  179]
train() client id: f_00007-5-3 loss: 0.279400  [  128/  179]
train() client id: f_00007-5-4 loss: 0.207384  [  160/  179]
train() client id: f_00007-6-0 loss: 0.239271  [   32/  179]
train() client id: f_00007-6-1 loss: 0.373062  [   64/  179]
train() client id: f_00007-6-2 loss: 0.218316  [   96/  179]
train() client id: f_00007-6-3 loss: 0.378336  [  128/  179]
train() client id: f_00007-6-4 loss: 0.081836  [  160/  179]
train() client id: f_00007-7-0 loss: 0.190641  [   32/  179]
train() client id: f_00007-7-1 loss: 0.188509  [   64/  179]
train() client id: f_00007-7-2 loss: 0.510268  [   96/  179]
train() client id: f_00007-7-3 loss: 0.281708  [  128/  179]
train() client id: f_00007-7-4 loss: 0.156436  [  160/  179]
train() client id: f_00007-8-0 loss: 0.314513  [   32/  179]
train() client id: f_00007-8-1 loss: 0.251899  [   64/  179]
train() client id: f_00007-8-2 loss: 0.358921  [   96/  179]
train() client id: f_00007-8-3 loss: 0.198518  [  128/  179]
train() client id: f_00007-8-4 loss: 0.196672  [  160/  179]
train() client id: f_00007-9-0 loss: 0.151793  [   32/  179]
train() client id: f_00007-9-1 loss: 0.468046  [   64/  179]
train() client id: f_00007-9-2 loss: 0.175730  [   96/  179]
train() client id: f_00007-9-3 loss: 0.176382  [  128/  179]
train() client id: f_00007-9-4 loss: 0.305581  [  160/  179]
train() client id: f_00007-10-0 loss: 0.462695  [   32/  179]
train() client id: f_00007-10-1 loss: 0.254579  [   64/  179]
train() client id: f_00007-10-2 loss: 0.138990  [   96/  179]
train() client id: f_00007-10-3 loss: 0.266750  [  128/  179]
train() client id: f_00007-10-4 loss: 0.180275  [  160/  179]
train() client id: f_00008-0-0 loss: 0.897271  [   32/  130]
train() client id: f_00008-0-1 loss: 0.865943  [   64/  130]
train() client id: f_00008-0-2 loss: 0.737100  [   96/  130]
train() client id: f_00008-0-3 loss: 0.710204  [  128/  130]
train() client id: f_00008-1-0 loss: 0.685549  [   32/  130]
train() client id: f_00008-1-1 loss: 0.728496  [   64/  130]
train() client id: f_00008-1-2 loss: 0.826767  [   96/  130]
train() client id: f_00008-1-3 loss: 0.893951  [  128/  130]
train() client id: f_00008-2-0 loss: 0.787636  [   32/  130]
train() client id: f_00008-2-1 loss: 0.777925  [   64/  130]
train() client id: f_00008-2-2 loss: 0.726687  [   96/  130]
train() client id: f_00008-2-3 loss: 0.922474  [  128/  130]
train() client id: f_00008-3-0 loss: 0.771003  [   32/  130]
train() client id: f_00008-3-1 loss: 0.826623  [   64/  130]
train() client id: f_00008-3-2 loss: 0.761464  [   96/  130]
train() client id: f_00008-3-3 loss: 0.840483  [  128/  130]
train() client id: f_00008-4-0 loss: 0.925336  [   32/  130]
train() client id: f_00008-4-1 loss: 0.676430  [   64/  130]
train() client id: f_00008-4-2 loss: 0.879973  [   96/  130]
train() client id: f_00008-4-3 loss: 0.730480  [  128/  130]
train() client id: f_00008-5-0 loss: 0.824836  [   32/  130]
train() client id: f_00008-5-1 loss: 0.794201  [   64/  130]
train() client id: f_00008-5-2 loss: 0.802636  [   96/  130]
train() client id: f_00008-5-3 loss: 0.767825  [  128/  130]
train() client id: f_00008-6-0 loss: 0.801376  [   32/  130]
train() client id: f_00008-6-1 loss: 0.788885  [   64/  130]
train() client id: f_00008-6-2 loss: 0.804657  [   96/  130]
train() client id: f_00008-6-3 loss: 0.807554  [  128/  130]
train() client id: f_00008-7-0 loss: 0.812950  [   32/  130]
train() client id: f_00008-7-1 loss: 0.783494  [   64/  130]
train() client id: f_00008-7-2 loss: 0.800400  [   96/  130]
train() client id: f_00008-7-3 loss: 0.779601  [  128/  130]
train() client id: f_00008-8-0 loss: 0.799076  [   32/  130]
train() client id: f_00008-8-1 loss: 0.848918  [   64/  130]
train() client id: f_00008-8-2 loss: 0.756719  [   96/  130]
train() client id: f_00008-8-3 loss: 0.762406  [  128/  130]
train() client id: f_00008-9-0 loss: 0.754373  [   32/  130]
train() client id: f_00008-9-1 loss: 0.812474  [   64/  130]
train() client id: f_00008-9-2 loss: 0.823453  [   96/  130]
train() client id: f_00008-9-3 loss: 0.815210  [  128/  130]
train() client id: f_00008-10-0 loss: 0.813114  [   32/  130]
train() client id: f_00008-10-1 loss: 0.916390  [   64/  130]
train() client id: f_00008-10-2 loss: 0.686245  [   96/  130]
train() client id: f_00008-10-3 loss: 0.778507  [  128/  130]
train() client id: f_00009-0-0 loss: 1.108744  [   32/  118]
train() client id: f_00009-0-1 loss: 1.061799  [   64/  118]
train() client id: f_00009-0-2 loss: 1.036044  [   96/  118]
train() client id: f_00009-1-0 loss: 1.207651  [   32/  118]
train() client id: f_00009-1-1 loss: 0.934362  [   64/  118]
train() client id: f_00009-1-2 loss: 0.932431  [   96/  118]
train() client id: f_00009-2-0 loss: 1.007238  [   32/  118]
train() client id: f_00009-2-1 loss: 1.068142  [   64/  118]
train() client id: f_00009-2-2 loss: 0.889936  [   96/  118]
train() client id: f_00009-3-0 loss: 1.104581  [   32/  118]
train() client id: f_00009-3-1 loss: 0.797801  [   64/  118]
train() client id: f_00009-3-2 loss: 0.866058  [   96/  118]
train() client id: f_00009-4-0 loss: 0.823121  [   32/  118]
train() client id: f_00009-4-1 loss: 1.010488  [   64/  118]
train() client id: f_00009-4-2 loss: 0.977790  [   96/  118]
train() client id: f_00009-5-0 loss: 1.020960  [   32/  118]
train() client id: f_00009-5-1 loss: 0.860266  [   64/  118]
train() client id: f_00009-5-2 loss: 0.764208  [   96/  118]
train() client id: f_00009-6-0 loss: 0.853887  [   32/  118]
train() client id: f_00009-6-1 loss: 0.805717  [   64/  118]
train() client id: f_00009-6-2 loss: 0.908688  [   96/  118]
train() client id: f_00009-7-0 loss: 0.723973  [   32/  118]
train() client id: f_00009-7-1 loss: 0.807234  [   64/  118]
train() client id: f_00009-7-2 loss: 0.984896  [   96/  118]
train() client id: f_00009-8-0 loss: 0.746294  [   32/  118]
train() client id: f_00009-8-1 loss: 0.821761  [   64/  118]
train() client id: f_00009-8-2 loss: 0.901209  [   96/  118]
train() client id: f_00009-9-0 loss: 0.666088  [   32/  118]
train() client id: f_00009-9-1 loss: 0.899091  [   64/  118]
train() client id: f_00009-9-2 loss: 0.686922  [   96/  118]
train() client id: f_00009-10-0 loss: 0.657167  [   32/  118]
train() client id: f_00009-10-1 loss: 0.923596  [   64/  118]
train() client id: f_00009-10-2 loss: 0.819452  [   96/  118]
At round 49 accuracy: 0.6472148541114059
At round 49 training accuracy: 0.5922199865861838
At round 49 training loss: 0.8226914130884172
update_location
xs = [  -3.9056584     4.20031788  265.00902392   18.81129433    0.97929623
    3.95640986 -227.44319194 -206.32485185  249.66397685 -192.06087855]
ys = [ 257.5879595   240.55583871    1.32061395 -227.45517586  219.35018685
  202.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [276.34545599 260.54702878 283.25170216 249.17809258 241.07149041
 226.1619617  248.46990985 229.28284006 269.51951935 216.57191167]
dists_bs = [189.98432772 190.81488913 473.04594942 446.69115324 181.48182216
 181.10500714 185.2050848  176.97203396 452.90618355 171.84751034]
uav_gains = [4.31692826e-12 6.00226608e-12 3.73822966e-12 7.54002475e-12
 8.79545935e-12 1.14187632e-11 7.64459119e-12 1.08372987e-11
 4.98118754e-12 1.33149743e-11]
bs_gains = [4.60133985e-11 4.54548008e-11 3.57737746e-12 4.20023415e-12
 5.23071814e-11 5.26124836e-11 4.94158134e-11 5.61256116e-11
 4.04083377e-12 6.09386741e-11]
Round 50
-------------------------------
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.40113982  9.00691085  4.34261068  1.57832816 10.38540444  4.99669129
  1.94822841  6.14510341  4.53980435  4.05137628]
obj_prev = 51.395597693307295
eta_min = 1.1367158248091635e-21	eta_max = 0.9395757044516898
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 11.874554720679596	eta = 0.909090909090909
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 24.172264619780965	eta = 0.44658826617504477
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 17.836063247717608	eta = 0.6052372430028105
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.691468667850707	eta = 0.6467405571604739
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62571758399257	eta = 0.6492982748886553
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62547813342434	eta = 0.6493076264898318
eta = 0.6493076264898318
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.03563512 0.07494687 0.03506948 0.01216119 0.08654243 0.04129149
 0.01527219 0.05062449 0.03676639 0.03337257]
ene_total = [1.56827942 2.59415826 1.57537116 0.75074681 2.95309342 1.52632602
 0.84721729 1.92501916 1.61509008 1.2701765 ]
ti_comp = [0.73120545 0.79889761 0.72329946 0.75580078 0.8009931  0.80107742
 0.7563088  0.76793556 0.72766908 0.80314305]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [5.28975756e-06 4.12247795e-05 5.15266296e-06 1.96785757e-07
 6.31407441e-05 6.85665898e-06 3.89211744e-07 1.37503103e-05
 5.86631230e-06 3.60133211e-06]
ene_total = [0.44507619 0.2328146  0.46999487 0.36738071 0.22689962 0.22485949
 0.36578528 0.32955399 0.45624248 0.21824512]
optimize_network iter = 0 obj = 3.3368523467960083
eta = 0.6493076264898318
freqs = [24367376.98591928 46906432.60757383 24242711.66907599  8045235.73109414
 54021956.04098655 25772468.52031963 10096532.76307652 32961419.31716986
 25263125.77228831 20776230.43734453]
eta_min = 0.6493076264898333	eta_max = 0.6979381775060964
af = 0.0037239897789525765	bf = 1.1686004695034842	zeta = 0.004096388756847835	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16746392e-06 9.09842125e-06 1.13720677e-06 4.34311531e-08
 1.39353344e-05 1.51328333e-06 8.59000931e-08 3.03473097e-06
 1.29471112e-06 7.94823816e-07]
ene_total = [1.71643177 0.89420252 1.81257148 1.41719511 0.86930783 0.86677177
 1.41102229 1.26998969 1.7594524  0.84156459]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 1 obj = 3.8730371713241745
eta = 0.6979381775060964
freqs = [24314579.32952386 46031634.30360291 24242711.66907599  7976349.99245549
 52989718.17503818 25279541.36753655 10008802.12815238 32581091.06223162
 25232695.58126215 20369543.75424322]
eta_min = 0.6979381775061025	eta_max = 0.697938177506091
af = 0.0036043043465234475	bf = 1.1686004695034842	zeta = 0.003964734781175793	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16241023e-06 8.76221731e-06 1.13720677e-06 4.26905959e-08
 1.34078766e-05 1.45595045e-06 8.44137753e-08 2.96510198e-06
 1.29159396e-06 7.64011626e-07]
ene_total = [1.71643115 0.89416163 1.81257148 1.41719502 0.86924368 0.8667648
 1.41102211 1.26998122 1.75945202 0.84156084]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 2 obj = 3.873037171324106
eta = 0.697938177506091
freqs = [24314579.32952385 46031634.30360299 24242711.66907597  7976349.99245549
 52989718.17503828 25279541.3675366  10008802.12815239 32581091.06223165
 25232695.58126213 20369543.75424326]
Done!
At round 50 energy consumption: 3.8730371713241745
At round 50 eta: 0.697938177506091
At round 50 local rounds: 11.775941462244061
At round 50 global rounds: 36.59949614457299
At round 50 a_n: 10.712764660819193
gradient difference: 0.6270567774772644
train() client id: f_00000-0-0 loss: 0.727396  [   32/  126]
train() client id: f_00000-0-1 loss: 0.802179  [   64/  126]
train() client id: f_00000-0-2 loss: 1.132426  [   96/  126]
train() client id: f_00000-1-0 loss: 0.700661  [   32/  126]
train() client id: f_00000-1-1 loss: 0.884781  [   64/  126]
train() client id: f_00000-1-2 loss: 0.996969  [   96/  126]
train() client id: f_00000-2-0 loss: 0.953620  [   32/  126]
train() client id: f_00000-2-1 loss: 0.897213  [   64/  126]
train() client id: f_00000-2-2 loss: 0.842110  [   96/  126]
train() client id: f_00000-3-0 loss: 0.887624  [   32/  126]
train() client id: f_00000-3-1 loss: 0.911332  [   64/  126]
train() client id: f_00000-3-2 loss: 0.894849  [   96/  126]
train() client id: f_00000-4-0 loss: 0.777179  [   32/  126]
train() client id: f_00000-4-1 loss: 0.804326  [   64/  126]
train() client id: f_00000-4-2 loss: 0.820320  [   96/  126]
train() client id: f_00000-5-0 loss: 0.861442  [   32/  126]
train() client id: f_00000-5-1 loss: 0.815981  [   64/  126]
train() client id: f_00000-5-2 loss: 1.020661  [   96/  126]
train() client id: f_00000-6-0 loss: 0.892773  [   32/  126]
train() client id: f_00000-6-1 loss: 0.930340  [   64/  126]
train() client id: f_00000-6-2 loss: 0.786302  [   96/  126]
train() client id: f_00000-7-0 loss: 0.845915  [   32/  126]
train() client id: f_00000-7-1 loss: 0.866455  [   64/  126]
train() client id: f_00000-7-2 loss: 0.937989  [   96/  126]
train() client id: f_00000-8-0 loss: 0.792850  [   32/  126]
train() client id: f_00000-8-1 loss: 1.046255  [   64/  126]
train() client id: f_00000-8-2 loss: 0.985314  [   96/  126]
train() client id: f_00000-9-0 loss: 0.849171  [   32/  126]
train() client id: f_00000-9-1 loss: 1.049696  [   64/  126]
train() client id: f_00000-9-2 loss: 0.877170  [   96/  126]
train() client id: f_00000-10-0 loss: 0.824688  [   32/  126]
train() client id: f_00000-10-1 loss: 1.122280  [   64/  126]
train() client id: f_00000-10-2 loss: 0.889482  [   96/  126]
train() client id: f_00001-0-0 loss: 0.414834  [   32/  265]
train() client id: f_00001-0-1 loss: 0.412149  [   64/  265]
train() client id: f_00001-0-2 loss: 0.565110  [   96/  265]
train() client id: f_00001-0-3 loss: 0.364195  [  128/  265]
train() client id: f_00001-0-4 loss: 0.452886  [  160/  265]
train() client id: f_00001-0-5 loss: 0.429602  [  192/  265]
train() client id: f_00001-0-6 loss: 0.406508  [  224/  265]
train() client id: f_00001-0-7 loss: 0.382305  [  256/  265]
train() client id: f_00001-1-0 loss: 0.495694  [   32/  265]
train() client id: f_00001-1-1 loss: 0.380695  [   64/  265]
train() client id: f_00001-1-2 loss: 0.399840  [   96/  265]
train() client id: f_00001-1-3 loss: 0.329239  [  128/  265]
train() client id: f_00001-1-4 loss: 0.458071  [  160/  265]
train() client id: f_00001-1-5 loss: 0.478562  [  192/  265]
train() client id: f_00001-1-6 loss: 0.416137  [  224/  265]
train() client id: f_00001-1-7 loss: 0.410002  [  256/  265]
train() client id: f_00001-2-0 loss: 0.587742  [   32/  265]
train() client id: f_00001-2-1 loss: 0.331976  [   64/  265]
train() client id: f_00001-2-2 loss: 0.333916  [   96/  265]
train() client id: f_00001-2-3 loss: 0.455065  [  128/  265]
train() client id: f_00001-2-4 loss: 0.439424  [  160/  265]
train() client id: f_00001-2-5 loss: 0.466282  [  192/  265]
train() client id: f_00001-2-6 loss: 0.397358  [  224/  265]
train() client id: f_00001-2-7 loss: 0.305017  [  256/  265]
train() client id: f_00001-3-0 loss: 0.384023  [   32/  265]
train() client id: f_00001-3-1 loss: 0.437773  [   64/  265]
train() client id: f_00001-3-2 loss: 0.338027  [   96/  265]
train() client id: f_00001-3-3 loss: 0.472192  [  128/  265]
train() client id: f_00001-3-4 loss: 0.466618  [  160/  265]
train() client id: f_00001-3-5 loss: 0.320263  [  192/  265]
train() client id: f_00001-3-6 loss: 0.388389  [  224/  265]
train() client id: f_00001-3-7 loss: 0.482778  [  256/  265]
train() client id: f_00001-4-0 loss: 0.449763  [   32/  265]
train() client id: f_00001-4-1 loss: 0.352448  [   64/  265]
train() client id: f_00001-4-2 loss: 0.384456  [   96/  265]
train() client id: f_00001-4-3 loss: 0.539619  [  128/  265]
train() client id: f_00001-4-4 loss: 0.388993  [  160/  265]
train() client id: f_00001-4-5 loss: 0.388405  [  192/  265]
train() client id: f_00001-4-6 loss: 0.316137  [  224/  265]
train() client id: f_00001-4-7 loss: 0.449580  [  256/  265]
train() client id: f_00001-5-0 loss: 0.364721  [   32/  265]
train() client id: f_00001-5-1 loss: 0.358535  [   64/  265]
train() client id: f_00001-5-2 loss: 0.472513  [   96/  265]
train() client id: f_00001-5-3 loss: 0.405338  [  128/  265]
train() client id: f_00001-5-4 loss: 0.310591  [  160/  265]
train() client id: f_00001-5-5 loss: 0.419818  [  192/  265]
train() client id: f_00001-5-6 loss: 0.485244  [  224/  265]
train() client id: f_00001-5-7 loss: 0.370036  [  256/  265]
train() client id: f_00001-6-0 loss: 0.356400  [   32/  265]
train() client id: f_00001-6-1 loss: 0.313666  [   64/  265]
train() client id: f_00001-6-2 loss: 0.412677  [   96/  265]
train() client id: f_00001-6-3 loss: 0.482695  [  128/  265]
train() client id: f_00001-6-4 loss: 0.372807  [  160/  265]
train() client id: f_00001-6-5 loss: 0.459790  [  192/  265]
train() client id: f_00001-6-6 loss: 0.445524  [  224/  265]
train() client id: f_00001-6-7 loss: 0.389637  [  256/  265]
train() client id: f_00001-7-0 loss: 0.403347  [   32/  265]
train() client id: f_00001-7-1 loss: 0.305276  [   64/  265]
train() client id: f_00001-7-2 loss: 0.480291  [   96/  265]
train() client id: f_00001-7-3 loss: 0.442191  [  128/  265]
train() client id: f_00001-7-4 loss: 0.332462  [  160/  265]
train() client id: f_00001-7-5 loss: 0.529800  [  192/  265]
train() client id: f_00001-7-6 loss: 0.305192  [  224/  265]
train() client id: f_00001-7-7 loss: 0.402836  [  256/  265]
train() client id: f_00001-8-0 loss: 0.524792  [   32/  265]
train() client id: f_00001-8-1 loss: 0.407559  [   64/  265]
train() client id: f_00001-8-2 loss: 0.425126  [   96/  265]
train() client id: f_00001-8-3 loss: 0.307373  [  128/  265]
train() client id: f_00001-8-4 loss: 0.427284  [  160/  265]
train() client id: f_00001-8-5 loss: 0.377807  [  192/  265]
train() client id: f_00001-8-6 loss: 0.417017  [  224/  265]
train() client id: f_00001-8-7 loss: 0.331176  [  256/  265]
train() client id: f_00001-9-0 loss: 0.375817  [   32/  265]
train() client id: f_00001-9-1 loss: 0.440955  [   64/  265]
train() client id: f_00001-9-2 loss: 0.444209  [   96/  265]
train() client id: f_00001-9-3 loss: 0.344088  [  128/  265]
train() client id: f_00001-9-4 loss: 0.395977  [  160/  265]
train() client id: f_00001-9-5 loss: 0.447049  [  192/  265]
train() client id: f_00001-9-6 loss: 0.306713  [  224/  265]
train() client id: f_00001-9-7 loss: 0.461402  [  256/  265]
train() client id: f_00001-10-0 loss: 0.294316  [   32/  265]
train() client id: f_00001-10-1 loss: 0.314054  [   64/  265]
train() client id: f_00001-10-2 loss: 0.459235  [   96/  265]
train() client id: f_00001-10-3 loss: 0.392141  [  128/  265]
train() client id: f_00001-10-4 loss: 0.308667  [  160/  265]
train() client id: f_00001-10-5 loss: 0.413338  [  192/  265]
train() client id: f_00001-10-6 loss: 0.572497  [  224/  265]
train() client id: f_00001-10-7 loss: 0.366619  [  256/  265]
train() client id: f_00002-0-0 loss: 1.247656  [   32/  124]
train() client id: f_00002-0-1 loss: 1.195875  [   64/  124]
train() client id: f_00002-0-2 loss: 1.187839  [   96/  124]
train() client id: f_00002-1-0 loss: 1.134490  [   32/  124]
train() client id: f_00002-1-1 loss: 1.121952  [   64/  124]
train() client id: f_00002-1-2 loss: 1.173424  [   96/  124]
train() client id: f_00002-2-0 loss: 1.368577  [   32/  124]
train() client id: f_00002-2-1 loss: 0.991588  [   64/  124]
train() client id: f_00002-2-2 loss: 1.281012  [   96/  124]
train() client id: f_00002-3-0 loss: 1.178054  [   32/  124]
train() client id: f_00002-3-1 loss: 1.140581  [   64/  124]
train() client id: f_00002-3-2 loss: 1.144953  [   96/  124]
train() client id: f_00002-4-0 loss: 1.054205  [   32/  124]
train() client id: f_00002-4-1 loss: 1.173314  [   64/  124]
train() client id: f_00002-4-2 loss: 1.218229  [   96/  124]
train() client id: f_00002-5-0 loss: 1.148126  [   32/  124]
train() client id: f_00002-5-1 loss: 1.214516  [   64/  124]
train() client id: f_00002-5-2 loss: 1.109216  [   96/  124]
train() client id: f_00002-6-0 loss: 1.122067  [   32/  124]
train() client id: f_00002-6-1 loss: 1.056917  [   64/  124]
train() client id: f_00002-6-2 loss: 1.249433  [   96/  124]
train() client id: f_00002-7-0 loss: 0.937472  [   32/  124]
train() client id: f_00002-7-1 loss: 1.101160  [   64/  124]
train() client id: f_00002-7-2 loss: 1.263025  [   96/  124]
train() client id: f_00002-8-0 loss: 1.176089  [   32/  124]
train() client id: f_00002-8-1 loss: 1.102612  [   64/  124]
train() client id: f_00002-8-2 loss: 1.008285  [   96/  124]
train() client id: f_00002-9-0 loss: 1.015423  [   32/  124]
train() client id: f_00002-9-1 loss: 1.327377  [   64/  124]
train() client id: f_00002-9-2 loss: 0.978644  [   96/  124]
train() client id: f_00002-10-0 loss: 1.093855  [   32/  124]
train() client id: f_00002-10-1 loss: 1.010266  [   64/  124]
train() client id: f_00002-10-2 loss: 1.357753  [   96/  124]
train() client id: f_00003-0-0 loss: 1.107261  [   32/   43]
train() client id: f_00003-1-0 loss: 0.636388  [   32/   43]
train() client id: f_00003-2-0 loss: 0.911337  [   32/   43]
train() client id: f_00003-3-0 loss: 0.989579  [   32/   43]
train() client id: f_00003-4-0 loss: 1.110574  [   32/   43]
train() client id: f_00003-5-0 loss: 0.836723  [   32/   43]
train() client id: f_00003-6-0 loss: 0.824423  [   32/   43]
train() client id: f_00003-7-0 loss: 0.944292  [   32/   43]
train() client id: f_00003-8-0 loss: 0.900480  [   32/   43]
train() client id: f_00003-9-0 loss: 0.938085  [   32/   43]
train() client id: f_00003-10-0 loss: 0.980168  [   32/   43]
train() client id: f_00004-0-0 loss: 0.568597  [   32/  306]
train() client id: f_00004-0-1 loss: 0.630116  [   64/  306]
train() client id: f_00004-0-2 loss: 0.658286  [   96/  306]
train() client id: f_00004-0-3 loss: 0.392622  [  128/  306]
train() client id: f_00004-0-4 loss: 0.408773  [  160/  306]
train() client id: f_00004-0-5 loss: 0.566476  [  192/  306]
train() client id: f_00004-0-6 loss: 0.534211  [  224/  306]
train() client id: f_00004-0-7 loss: 0.756155  [  256/  306]
train() client id: f_00004-0-8 loss: 0.518067  [  288/  306]
train() client id: f_00004-1-0 loss: 0.554227  [   32/  306]
train() client id: f_00004-1-1 loss: 0.647989  [   64/  306]
train() client id: f_00004-1-2 loss: 0.683241  [   96/  306]
train() client id: f_00004-1-3 loss: 0.443571  [  128/  306]
train() client id: f_00004-1-4 loss: 0.623101  [  160/  306]
train() client id: f_00004-1-5 loss: 0.493410  [  192/  306]
train() client id: f_00004-1-6 loss: 0.496415  [  224/  306]
train() client id: f_00004-1-7 loss: 0.586839  [  256/  306]
train() client id: f_00004-1-8 loss: 0.629106  [  288/  306]
train() client id: f_00004-2-0 loss: 0.592910  [   32/  306]
train() client id: f_00004-2-1 loss: 0.674628  [   64/  306]
train() client id: f_00004-2-2 loss: 0.483826  [   96/  306]
train() client id: f_00004-2-3 loss: 0.633717  [  128/  306]
train() client id: f_00004-2-4 loss: 0.359845  [  160/  306]
train() client id: f_00004-2-5 loss: 0.791568  [  192/  306]
train() client id: f_00004-2-6 loss: 0.611330  [  224/  306]
train() client id: f_00004-2-7 loss: 0.552572  [  256/  306]
train() client id: f_00004-2-8 loss: 0.490900  [  288/  306]
train() client id: f_00004-3-0 loss: 0.505395  [   32/  306]
train() client id: f_00004-3-1 loss: 0.629426  [   64/  306]
train() client id: f_00004-3-2 loss: 0.603041  [   96/  306]
train() client id: f_00004-3-3 loss: 0.665463  [  128/  306]
train() client id: f_00004-3-4 loss: 0.539649  [  160/  306]
train() client id: f_00004-3-5 loss: 0.533052  [  192/  306]
train() client id: f_00004-3-6 loss: 0.530190  [  224/  306]
train() client id: f_00004-3-7 loss: 0.549468  [  256/  306]
train() client id: f_00004-3-8 loss: 0.812680  [  288/  306]
train() client id: f_00004-4-0 loss: 0.575934  [   32/  306]
train() client id: f_00004-4-1 loss: 0.468128  [   64/  306]
train() client id: f_00004-4-2 loss: 0.553194  [   96/  306]
train() client id: f_00004-4-3 loss: 0.538965  [  128/  306]
train() client id: f_00004-4-4 loss: 0.647040  [  160/  306]
train() client id: f_00004-4-5 loss: 0.638105  [  192/  306]
train() client id: f_00004-4-6 loss: 0.723948  [  224/  306]
train() client id: f_00004-4-7 loss: 0.643921  [  256/  306]
train() client id: f_00004-4-8 loss: 0.591242  [  288/  306]
train() client id: f_00004-5-0 loss: 0.554260  [   32/  306]
train() client id: f_00004-5-1 loss: 0.659963  [   64/  306]
train() client id: f_00004-5-2 loss: 0.542915  [   96/  306]
train() client id: f_00004-5-3 loss: 0.672912  [  128/  306]
train() client id: f_00004-5-4 loss: 0.543433  [  160/  306]
train() client id: f_00004-5-5 loss: 0.515059  [  192/  306]
train() client id: f_00004-5-6 loss: 0.554993  [  224/  306]
train() client id: f_00004-5-7 loss: 0.759192  [  256/  306]
train() client id: f_00004-5-8 loss: 0.536958  [  288/  306]
train() client id: f_00004-6-0 loss: 0.539753  [   32/  306]
train() client id: f_00004-6-1 loss: 0.647412  [   64/  306]
train() client id: f_00004-6-2 loss: 0.547034  [   96/  306]
train() client id: f_00004-6-3 loss: 0.486864  [  128/  306]
train() client id: f_00004-6-4 loss: 0.558654  [  160/  306]
train() client id: f_00004-6-5 loss: 0.700778  [  192/  306]
train() client id: f_00004-6-6 loss: 0.609358  [  224/  306]
train() client id: f_00004-6-7 loss: 0.564232  [  256/  306]
train() client id: f_00004-6-8 loss: 0.738511  [  288/  306]
train() client id: f_00004-7-0 loss: 0.460906  [   32/  306]
train() client id: f_00004-7-1 loss: 0.648380  [   64/  306]
train() client id: f_00004-7-2 loss: 0.626738  [   96/  306]
train() client id: f_00004-7-3 loss: 0.512168  [  128/  306]
train() client id: f_00004-7-4 loss: 0.673937  [  160/  306]
train() client id: f_00004-7-5 loss: 0.609715  [  192/  306]
train() client id: f_00004-7-6 loss: 0.595127  [  224/  306]
train() client id: f_00004-7-7 loss: 0.591462  [  256/  306]
train() client id: f_00004-7-8 loss: 0.726374  [  288/  306]
train() client id: f_00004-8-0 loss: 0.625410  [   32/  306]
train() client id: f_00004-8-1 loss: 0.588716  [   64/  306]
train() client id: f_00004-8-2 loss: 0.572384  [   96/  306]
train() client id: f_00004-8-3 loss: 0.543808  [  128/  306]
train() client id: f_00004-8-4 loss: 0.727895  [  160/  306]
train() client id: f_00004-8-5 loss: 0.583816  [  192/  306]
train() client id: f_00004-8-6 loss: 0.570003  [  224/  306]
train() client id: f_00004-8-7 loss: 0.713626  [  256/  306]
train() client id: f_00004-8-8 loss: 0.559124  [  288/  306]
train() client id: f_00004-9-0 loss: 0.578977  [   32/  306]
train() client id: f_00004-9-1 loss: 0.635369  [   64/  306]
train() client id: f_00004-9-2 loss: 0.677384  [   96/  306]
train() client id: f_00004-9-3 loss: 0.607050  [  128/  306]
train() client id: f_00004-9-4 loss: 0.471365  [  160/  306]
train() client id: f_00004-9-5 loss: 0.547613  [  192/  306]
train() client id: f_00004-9-6 loss: 0.651602  [  224/  306]
train() client id: f_00004-9-7 loss: 0.746898  [  256/  306]
train() client id: f_00004-9-8 loss: 0.619128  [  288/  306]
train() client id: f_00004-10-0 loss: 0.676062  [   32/  306]
train() client id: f_00004-10-1 loss: 0.556278  [   64/  306]
train() client id: f_00004-10-2 loss: 0.755764  [   96/  306]
train() client id: f_00004-10-3 loss: 0.672858  [  128/  306]
train() client id: f_00004-10-4 loss: 0.508702  [  160/  306]
train() client id: f_00004-10-5 loss: 0.537881  [  192/  306]
train() client id: f_00004-10-6 loss: 0.634041  [  224/  306]
train() client id: f_00004-10-7 loss: 0.572383  [  256/  306]
train() client id: f_00004-10-8 loss: 0.592281  [  288/  306]
train() client id: f_00005-0-0 loss: 0.533753  [   32/  146]
train() client id: f_00005-0-1 loss: 0.575017  [   64/  146]
train() client id: f_00005-0-2 loss: 0.587002  [   96/  146]
train() client id: f_00005-0-3 loss: 0.580058  [  128/  146]
train() client id: f_00005-1-0 loss: 0.740942  [   32/  146]
train() client id: f_00005-1-1 loss: 0.515859  [   64/  146]
train() client id: f_00005-1-2 loss: 0.381008  [   96/  146]
train() client id: f_00005-1-3 loss: 0.289109  [  128/  146]
train() client id: f_00005-2-0 loss: 0.497491  [   32/  146]
train() client id: f_00005-2-1 loss: 0.508439  [   64/  146]
train() client id: f_00005-2-2 loss: 0.503460  [   96/  146]
train() client id: f_00005-2-3 loss: 0.663138  [  128/  146]
train() client id: f_00005-3-0 loss: 0.546053  [   32/  146]
train() client id: f_00005-3-1 loss: 0.771609  [   64/  146]
train() client id: f_00005-3-2 loss: 0.420282  [   96/  146]
train() client id: f_00005-3-3 loss: 0.481393  [  128/  146]
train() client id: f_00005-4-0 loss: 0.644884  [   32/  146]
train() client id: f_00005-4-1 loss: 0.616655  [   64/  146]
train() client id: f_00005-4-2 loss: 0.511321  [   96/  146]
train() client id: f_00005-4-3 loss: 0.503083  [  128/  146]
train() client id: f_00005-5-0 loss: 0.594635  [   32/  146]
train() client id: f_00005-5-1 loss: 0.275018  [   64/  146]
train() client id: f_00005-5-2 loss: 0.725025  [   96/  146]
train() client id: f_00005-5-3 loss: 0.492955  [  128/  146]
train() client id: f_00005-6-0 loss: 0.680469  [   32/  146]
train() client id: f_00005-6-1 loss: 0.199147  [   64/  146]
train() client id: f_00005-6-2 loss: 0.500344  [   96/  146]
train() client id: f_00005-6-3 loss: 0.803273  [  128/  146]
train() client id: f_00005-7-0 loss: 0.333876  [   32/  146]
train() client id: f_00005-7-1 loss: 0.549447  [   64/  146]
train() client id: f_00005-7-2 loss: 0.432371  [   96/  146]
train() client id: f_00005-7-3 loss: 0.733615  [  128/  146]
train() client id: f_00005-8-0 loss: 0.774417  [   32/  146]
train() client id: f_00005-8-1 loss: 0.241237  [   64/  146]
train() client id: f_00005-8-2 loss: 0.329394  [   96/  146]
train() client id: f_00005-8-3 loss: 0.488564  [  128/  146]
train() client id: f_00005-9-0 loss: 0.281993  [   32/  146]
train() client id: f_00005-9-1 loss: 0.494125  [   64/  146]
train() client id: f_00005-9-2 loss: 0.695370  [   96/  146]
train() client id: f_00005-9-3 loss: 0.383445  [  128/  146]
train() client id: f_00005-10-0 loss: 0.476353  [   32/  146]
train() client id: f_00005-10-1 loss: 0.554512  [   64/  146]
train() client id: f_00005-10-2 loss: 0.719358  [   96/  146]
train() client id: f_00005-10-3 loss: 0.339738  [  128/  146]
train() client id: f_00006-0-0 loss: 0.510478  [   32/   54]
train() client id: f_00006-1-0 loss: 0.480434  [   32/   54]
train() client id: f_00006-2-0 loss: 0.569664  [   32/   54]
train() client id: f_00006-3-0 loss: 0.506432  [   32/   54]
train() client id: f_00006-4-0 loss: 0.580112  [   32/   54]
train() client id: f_00006-5-0 loss: 0.526203  [   32/   54]
train() client id: f_00006-6-0 loss: 0.465139  [   32/   54]
train() client id: f_00006-7-0 loss: 0.577509  [   32/   54]
train() client id: f_00006-8-0 loss: 0.548651  [   32/   54]
train() client id: f_00006-9-0 loss: 0.477292  [   32/   54]
train() client id: f_00006-10-0 loss: 0.578739  [   32/   54]
train() client id: f_00007-0-0 loss: 0.782886  [   32/  179]
train() client id: f_00007-0-1 loss: 0.701257  [   64/  179]
train() client id: f_00007-0-2 loss: 0.536075  [   96/  179]
train() client id: f_00007-0-3 loss: 0.569915  [  128/  179]
train() client id: f_00007-0-4 loss: 0.816912  [  160/  179]
train() client id: f_00007-1-0 loss: 0.543532  [   32/  179]
train() client id: f_00007-1-1 loss: 1.019403  [   64/  179]
train() client id: f_00007-1-2 loss: 0.572112  [   96/  179]
train() client id: f_00007-1-3 loss: 0.542247  [  128/  179]
train() client id: f_00007-1-4 loss: 0.535370  [  160/  179]
train() client id: f_00007-2-0 loss: 0.554465  [   32/  179]
train() client id: f_00007-2-1 loss: 0.484934  [   64/  179]
train() client id: f_00007-2-2 loss: 0.808508  [   96/  179]
train() client id: f_00007-2-3 loss: 0.453387  [  128/  179]
train() client id: f_00007-2-4 loss: 0.636720  [  160/  179]
train() client id: f_00007-3-0 loss: 0.676123  [   32/  179]
train() client id: f_00007-3-1 loss: 0.733678  [   64/  179]
train() client id: f_00007-3-2 loss: 0.588687  [   96/  179]
train() client id: f_00007-3-3 loss: 0.644710  [  128/  179]
train() client id: f_00007-3-4 loss: 0.488324  [  160/  179]
train() client id: f_00007-4-0 loss: 0.614875  [   32/  179]
train() client id: f_00007-4-1 loss: 0.739696  [   64/  179]
train() client id: f_00007-4-2 loss: 0.571736  [   96/  179]
train() client id: f_00007-4-3 loss: 0.445937  [  128/  179]
train() client id: f_00007-4-4 loss: 0.574936  [  160/  179]
train() client id: f_00007-5-0 loss: 0.468518  [   32/  179]
train() client id: f_00007-5-1 loss: 0.785158  [   64/  179]
train() client id: f_00007-5-2 loss: 0.668132  [   96/  179]
train() client id: f_00007-5-3 loss: 0.766496  [  128/  179]
train() client id: f_00007-5-4 loss: 0.534386  [  160/  179]
train() client id: f_00007-6-0 loss: 0.609018  [   32/  179]
train() client id: f_00007-6-1 loss: 0.748335  [   64/  179]
train() client id: f_00007-6-2 loss: 0.489897  [   96/  179]
train() client id: f_00007-6-3 loss: 0.688645  [  128/  179]
train() client id: f_00007-6-4 loss: 0.681239  [  160/  179]
train() client id: f_00007-7-0 loss: 0.598979  [   32/  179]
train() client id: f_00007-7-1 loss: 0.644671  [   64/  179]
train() client id: f_00007-7-2 loss: 0.764660  [   96/  179]
train() client id: f_00007-7-3 loss: 0.592145  [  128/  179]
train() client id: f_00007-7-4 loss: 0.599981  [  160/  179]
train() client id: f_00007-8-0 loss: 0.577360  [   32/  179]
train() client id: f_00007-8-1 loss: 0.469825  [   64/  179]
train() client id: f_00007-8-2 loss: 0.779061  [   96/  179]
train() client id: f_00007-8-3 loss: 0.447740  [  128/  179]
train() client id: f_00007-8-4 loss: 0.903155  [  160/  179]
train() client id: f_00007-9-0 loss: 0.783083  [   32/  179]
train() client id: f_00007-9-1 loss: 0.622863  [   64/  179]
train() client id: f_00007-9-2 loss: 0.555971  [   96/  179]
train() client id: f_00007-9-3 loss: 0.733802  [  128/  179]
train() client id: f_00007-9-4 loss: 0.477903  [  160/  179]
train() client id: f_00007-10-0 loss: 0.667555  [   32/  179]
train() client id: f_00007-10-1 loss: 0.671052  [   64/  179]
train() client id: f_00007-10-2 loss: 0.588473  [   96/  179]
train() client id: f_00007-10-3 loss: 0.466125  [  128/  179]
train() client id: f_00007-10-4 loss: 0.645498  [  160/  179]
train() client id: f_00008-0-0 loss: 0.792462  [   32/  130]
train() client id: f_00008-0-1 loss: 0.631515  [   64/  130]
train() client id: f_00008-0-2 loss: 0.668196  [   96/  130]
train() client id: f_00008-0-3 loss: 0.818241  [  128/  130]
train() client id: f_00008-1-0 loss: 0.743599  [   32/  130]
train() client id: f_00008-1-1 loss: 0.701163  [   64/  130]
train() client id: f_00008-1-2 loss: 0.772545  [   96/  130]
train() client id: f_00008-1-3 loss: 0.685609  [  128/  130]
train() client id: f_00008-2-0 loss: 0.555503  [   32/  130]
train() client id: f_00008-2-1 loss: 0.691955  [   64/  130]
train() client id: f_00008-2-2 loss: 0.720596  [   96/  130]
train() client id: f_00008-2-3 loss: 0.942446  [  128/  130]
train() client id: f_00008-3-0 loss: 0.736027  [   32/  130]
train() client id: f_00008-3-1 loss: 0.756346  [   64/  130]
train() client id: f_00008-3-2 loss: 0.766926  [   96/  130]
train() client id: f_00008-3-3 loss: 0.628893  [  128/  130]
train() client id: f_00008-4-0 loss: 0.721121  [   32/  130]
train() client id: f_00008-4-1 loss: 0.743550  [   64/  130]
train() client id: f_00008-4-2 loss: 0.709875  [   96/  130]
train() client id: f_00008-4-3 loss: 0.729288  [  128/  130]
train() client id: f_00008-5-0 loss: 0.738184  [   32/  130]
train() client id: f_00008-5-1 loss: 0.730980  [   64/  130]
train() client id: f_00008-5-2 loss: 0.743145  [   96/  130]
train() client id: f_00008-5-3 loss: 0.699368  [  128/  130]
train() client id: f_00008-6-0 loss: 0.712888  [   32/  130]
train() client id: f_00008-6-1 loss: 0.761115  [   64/  130]
train() client id: f_00008-6-2 loss: 0.718254  [   96/  130]
train() client id: f_00008-6-3 loss: 0.689484  [  128/  130]
train() client id: f_00008-7-0 loss: 0.679428  [   32/  130]
train() client id: f_00008-7-1 loss: 0.733314  [   64/  130]
train() client id: f_00008-7-2 loss: 0.806558  [   96/  130]
train() client id: f_00008-7-3 loss: 0.697670  [  128/  130]
train() client id: f_00008-8-0 loss: 0.706861  [   32/  130]
train() client id: f_00008-8-1 loss: 0.779207  [   64/  130]
train() client id: f_00008-8-2 loss: 0.658994  [   96/  130]
train() client id: f_00008-8-3 loss: 0.744481  [  128/  130]
train() client id: f_00008-9-0 loss: 0.642431  [   32/  130]
train() client id: f_00008-9-1 loss: 0.689468  [   64/  130]
train() client id: f_00008-9-2 loss: 0.721488  [   96/  130]
train() client id: f_00008-9-3 loss: 0.845955  [  128/  130]
train() client id: f_00008-10-0 loss: 0.692620  [   32/  130]
train() client id: f_00008-10-1 loss: 0.755395  [   64/  130]
train() client id: f_00008-10-2 loss: 0.702791  [   96/  130]
train() client id: f_00008-10-3 loss: 0.751996  [  128/  130]
train() client id: f_00009-0-0 loss: 1.046243  [   32/  118]
train() client id: f_00009-0-1 loss: 1.136368  [   64/  118]
train() client id: f_00009-0-2 loss: 0.983710  [   96/  118]
train() client id: f_00009-1-0 loss: 0.973522  [   32/  118]
train() client id: f_00009-1-1 loss: 0.935939  [   64/  118]
train() client id: f_00009-1-2 loss: 0.961371  [   96/  118]
train() client id: f_00009-2-0 loss: 0.853235  [   32/  118]
train() client id: f_00009-2-1 loss: 0.937798  [   64/  118]
train() client id: f_00009-2-2 loss: 0.947459  [   96/  118]
train() client id: f_00009-3-0 loss: 0.913361  [   32/  118]
train() client id: f_00009-3-1 loss: 0.787070  [   64/  118]
train() client id: f_00009-3-2 loss: 0.925416  [   96/  118]
train() client id: f_00009-4-0 loss: 0.927058  [   32/  118]
train() client id: f_00009-4-1 loss: 0.860673  [   64/  118]
train() client id: f_00009-4-2 loss: 0.739663  [   96/  118]
train() client id: f_00009-5-0 loss: 0.870068  [   32/  118]
train() client id: f_00009-5-1 loss: 0.732888  [   64/  118]
train() client id: f_00009-5-2 loss: 0.891640  [   96/  118]
train() client id: f_00009-6-0 loss: 0.871100  [   32/  118]
train() client id: f_00009-6-1 loss: 0.743240  [   64/  118]
train() client id: f_00009-6-2 loss: 0.875540  [   96/  118]
train() client id: f_00009-7-0 loss: 0.885664  [   32/  118]
train() client id: f_00009-7-1 loss: 0.704946  [   64/  118]
train() client id: f_00009-7-2 loss: 0.838784  [   96/  118]
train() client id: f_00009-8-0 loss: 0.729982  [   32/  118]
train() client id: f_00009-8-1 loss: 0.714429  [   64/  118]
train() client id: f_00009-8-2 loss: 0.868564  [   96/  118]
train() client id: f_00009-9-0 loss: 0.879908  [   32/  118]
train() client id: f_00009-9-1 loss: 0.909657  [   64/  118]
train() client id: f_00009-9-2 loss: 0.745460  [   96/  118]
train() client id: f_00009-10-0 loss: 0.836338  [   32/  118]
train() client id: f_00009-10-1 loss: 0.802459  [   64/  118]
train() client id: f_00009-10-2 loss: 0.785489  [   96/  118]
At round 50 accuracy: 0.6472148541114059
At round 50 training accuracy: 0.5895372233400402
At round 50 training loss: 0.8356006304255614
update_location
xs = [  -3.9056584     4.20031788  270.00902392   18.81129433    0.97929623
    3.95640986 -232.44319194 -211.32485185  254.66397685 -197.06087855]
ys = [ 262.5879595   245.55583871    1.32061395 -232.45517586  224.35018685
  207.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [281.0119048  265.17034637 287.93509167 253.75041592 245.62973224
 230.65639906 253.05479253 233.79236357 274.1576391  221.01810268]
dists_bs = [192.21062503 192.58992783 477.70028313 451.20121691 182.76803233
 181.94000418 186.68249879 177.9251228  457.59769543 172.41570577]
uav_gains = [3.91606458e-12 5.45480845e-12 3.39525697e-12 6.88931259e-12
 8.07391748e-12 1.05867714e-11 6.98555204e-12 1.00271385e-11
 4.51953049e-12 1.24147554e-11]
bs_gains = [4.45366340e-11 4.42914694e-11 3.48063665e-12 4.08373311e-12
 5.12830001e-11 5.19391842e-11 4.83285750e-11 5.52878520e-11
 3.92590119e-12 6.03780354e-11]
Round 51
-------------------------------
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.27047392  8.72826714  4.21425716  1.53313884 10.06392708  4.84207237
  1.89154359  5.95729153  4.4008638   3.92598233]
obj_prev = 49.827817776908454
eta_min = 2.5652490042097954e-22	eta_max = 0.9391007800776031
af = 10.460568009377656	bf = 1.156843673852587	zeta = 11.506624810315422	eta = 0.9090909090909091
af = 10.460568009377656	bf = 1.156843673852587	zeta = 23.692265208499137	eta = 0.44151827262279386
af = 10.460568009377656	bf = 1.156843673852587	zeta = 17.38415562977596	eta = 0.6017300024316722
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.245893349082294	eta = 0.6438899840474798
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.180034455220028	eta = 0.6465108611683364
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.179790636766835	eta = 0.6465206036478086
eta = 0.6465206036478086
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.03599007 0.0756934  0.0354188  0.01228233 0.08740446 0.04170278
 0.01542432 0.05112875 0.03713261 0.03370499]
ene_total = [1.53443253 2.51779645 1.54247681 0.73582445 2.86602817 1.48040303
 0.82941199 1.87320864 1.56863488 1.23157367]
ti_comp = [0.75933363 0.83189525 0.75096662 0.78576566 0.83410293 0.83428835
 0.78630533 0.79893754 0.75947361 0.83641438]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [5.05315743e-06 3.91666828e-05 4.92425175e-06 1.87558063e-07
 5.99848321e-05 6.51242403e-06 3.70949938e-07 1.30872950e-05
 5.54780444e-06 3.42073551e-06]
ene_total = [0.44384841 0.22497254 0.46920207 0.3635944  0.21891275 0.21673023
 0.36196441 0.32406582 0.44343917 0.21019324]
optimize_network iter = 0 obj = 3.2769230299910435
eta = 0.6465206036478086
freqs = [23698456.65782105 45494550.69374827 23582139.4675787   7815514.32290755
 52394286.76461308 24993025.32151997  9808095.66767819 31997964.51374336
 24446282.03194968 20148497.51725332]
eta_min = 0.6465206036478099	eta_max = 0.6980699554597802
af = 0.0033958499648260683	bf = 1.156843673852587	zeta = 0.003735434961308675	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.10424650e-06 8.55894019e-06 1.07607725e-06 4.09863212e-08
 1.31082480e-05 1.42313425e-06 8.10622219e-08 2.85991478e-06
 1.21233975e-06 7.47519793e-07]
ene_total = [1.7253386  0.8711479  1.82393243 1.41373717 0.84566865 0.84210666
 1.4073824  1.2588513  1.7237018  0.81697375]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 1 obj = 3.8353964400634717
eta = 0.6980699554597802
freqs = [23642854.46224663 44569165.40272366 23582139.46757871  7742332.9461439
 51302927.41865983 24471403.78525164  9714909.2256883  31593097.86188618
 24387978.69085684 19718556.51146085]
eta_min = 0.698069955459783	eta_max = 0.6980699554597762
af = 0.0032766452472554383	bf = 1.156843673852587	zeta = 0.003604309771980982	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.09907093e-06 8.21429384e-06 1.07607725e-06 4.02223555e-08
 1.25678526e-05 1.36435058e-06 7.95291995e-08 2.78800028e-06
 1.20656389e-06 7.15958094e-07]
ene_total = [1.72533799 0.87110728 1.82393243 1.41373708 0.84560497 0.84209973
 1.40738222 1.25884283 1.72370112 0.81697003]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 2 obj = 3.8353964400634206
eta = 0.6980699554597762
freqs = [23642854.46224662 44569165.40272372 23582139.46757869  7742332.9461439
 51302927.41865991 24471403.78525168  9714909.2256883  31593097.8618862
 24387978.69085682 19718556.51146088]
Done!
At round 51 energy consumption: 3.8353964400634717
At round 51 eta: 0.6980699554597762
At round 51 local rounds: 11.76975943561131
At round 51 global rounds: 35.48094949322612
At round 51 a_n: 10.370218813849878
gradient difference: 0.490499883890152
train() client id: f_00000-0-0 loss: 1.120053  [   32/  126]
train() client id: f_00000-0-1 loss: 1.234727  [   64/  126]
train() client id: f_00000-0-2 loss: 1.313625  [   96/  126]
train() client id: f_00000-1-0 loss: 1.085617  [   32/  126]
train() client id: f_00000-1-1 loss: 1.191574  [   64/  126]
train() client id: f_00000-1-2 loss: 1.195245  [   96/  126]
train() client id: f_00000-2-0 loss: 1.117724  [   32/  126]
train() client id: f_00000-2-1 loss: 1.006382  [   64/  126]
train() client id: f_00000-2-2 loss: 1.181440  [   96/  126]
train() client id: f_00000-3-0 loss: 1.001337  [   32/  126]
train() client id: f_00000-3-1 loss: 1.048261  [   64/  126]
train() client id: f_00000-3-2 loss: 0.985819  [   96/  126]
train() client id: f_00000-4-0 loss: 0.913366  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896364  [   64/  126]
train() client id: f_00000-4-2 loss: 0.894322  [   96/  126]
train() client id: f_00000-5-0 loss: 0.916563  [   32/  126]
train() client id: f_00000-5-1 loss: 1.054580  [   64/  126]
train() client id: f_00000-5-2 loss: 0.915048  [   96/  126]
train() client id: f_00000-6-0 loss: 0.924550  [   32/  126]
train() client id: f_00000-6-1 loss: 0.925363  [   64/  126]
train() client id: f_00000-6-2 loss: 0.948070  [   96/  126]
train() client id: f_00000-7-0 loss: 0.842642  [   32/  126]
train() client id: f_00000-7-1 loss: 0.942380  [   64/  126]
train() client id: f_00000-7-2 loss: 0.906531  [   96/  126]
train() client id: f_00000-8-0 loss: 0.923299  [   32/  126]
train() client id: f_00000-8-1 loss: 0.767369  [   64/  126]
train() client id: f_00000-8-2 loss: 0.878762  [   96/  126]
train() client id: f_00000-9-0 loss: 0.838273  [   32/  126]
train() client id: f_00000-9-1 loss: 0.940554  [   64/  126]
train() client id: f_00000-9-2 loss: 0.851997  [   96/  126]
train() client id: f_00000-10-0 loss: 0.846927  [   32/  126]
train() client id: f_00000-10-1 loss: 0.907453  [   64/  126]
train() client id: f_00000-10-2 loss: 0.778471  [   96/  126]
train() client id: f_00001-0-0 loss: 0.378289  [   32/  265]
train() client id: f_00001-0-1 loss: 0.493297  [   64/  265]
train() client id: f_00001-0-2 loss: 0.557966  [   96/  265]
train() client id: f_00001-0-3 loss: 0.695396  [  128/  265]
train() client id: f_00001-0-4 loss: 0.404784  [  160/  265]
train() client id: f_00001-0-5 loss: 0.414503  [  192/  265]
train() client id: f_00001-0-6 loss: 0.388240  [  224/  265]
train() client id: f_00001-0-7 loss: 0.539526  [  256/  265]
train() client id: f_00001-1-0 loss: 0.559891  [   32/  265]
train() client id: f_00001-1-1 loss: 0.375901  [   64/  265]
train() client id: f_00001-1-2 loss: 0.501003  [   96/  265]
train() client id: f_00001-1-3 loss: 0.565629  [  128/  265]
train() client id: f_00001-1-4 loss: 0.390245  [  160/  265]
train() client id: f_00001-1-5 loss: 0.404692  [  192/  265]
train() client id: f_00001-1-6 loss: 0.503554  [  224/  265]
train() client id: f_00001-1-7 loss: 0.538530  [  256/  265]
train() client id: f_00001-2-0 loss: 0.359848  [   32/  265]
train() client id: f_00001-2-1 loss: 0.571677  [   64/  265]
train() client id: f_00001-2-2 loss: 0.685362  [   96/  265]
train() client id: f_00001-2-3 loss: 0.374549  [  128/  265]
train() client id: f_00001-2-4 loss: 0.396965  [  160/  265]
train() client id: f_00001-2-5 loss: 0.447521  [  192/  265]
train() client id: f_00001-2-6 loss: 0.518308  [  224/  265]
train() client id: f_00001-2-7 loss: 0.440498  [  256/  265]
train() client id: f_00001-3-0 loss: 0.387208  [   32/  265]
train() client id: f_00001-3-1 loss: 0.522534  [   64/  265]
train() client id: f_00001-3-2 loss: 0.455161  [   96/  265]
train() client id: f_00001-3-3 loss: 0.464466  [  128/  265]
train() client id: f_00001-3-4 loss: 0.535022  [  160/  265]
train() client id: f_00001-3-5 loss: 0.483064  [  192/  265]
train() client id: f_00001-3-6 loss: 0.432421  [  224/  265]
train() client id: f_00001-3-7 loss: 0.479555  [  256/  265]
train() client id: f_00001-4-0 loss: 0.366555  [   32/  265]
train() client id: f_00001-4-1 loss: 0.386399  [   64/  265]
train() client id: f_00001-4-2 loss: 0.366670  [   96/  265]
train() client id: f_00001-4-3 loss: 0.534425  [  128/  265]
train() client id: f_00001-4-4 loss: 0.486559  [  160/  265]
train() client id: f_00001-4-5 loss: 0.494056  [  192/  265]
train() client id: f_00001-4-6 loss: 0.512941  [  224/  265]
train() client id: f_00001-4-7 loss: 0.469263  [  256/  265]
train() client id: f_00001-5-0 loss: 0.485035  [   32/  265]
train() client id: f_00001-5-1 loss: 0.378098  [   64/  265]
train() client id: f_00001-5-2 loss: 0.481258  [   96/  265]
train() client id: f_00001-5-3 loss: 0.449509  [  128/  265]
train() client id: f_00001-5-4 loss: 0.561692  [  160/  265]
train() client id: f_00001-5-5 loss: 0.452315  [  192/  265]
train() client id: f_00001-5-6 loss: 0.462777  [  224/  265]
train() client id: f_00001-5-7 loss: 0.454164  [  256/  265]
train() client id: f_00001-6-0 loss: 0.373082  [   32/  265]
train() client id: f_00001-6-1 loss: 0.452442  [   64/  265]
train() client id: f_00001-6-2 loss: 0.529721  [   96/  265]
train() client id: f_00001-6-3 loss: 0.378865  [  128/  265]
train() client id: f_00001-6-4 loss: 0.475188  [  160/  265]
train() client id: f_00001-6-5 loss: 0.489153  [  192/  265]
train() client id: f_00001-6-6 loss: 0.374758  [  224/  265]
train() client id: f_00001-6-7 loss: 0.562077  [  256/  265]
train() client id: f_00001-7-0 loss: 0.520814  [   32/  265]
train() client id: f_00001-7-1 loss: 0.550133  [   64/  265]
train() client id: f_00001-7-2 loss: 0.374928  [   96/  265]
train() client id: f_00001-7-3 loss: 0.454696  [  128/  265]
train() client id: f_00001-7-4 loss: 0.367844  [  160/  265]
train() client id: f_00001-7-5 loss: 0.380499  [  192/  265]
train() client id: f_00001-7-6 loss: 0.536028  [  224/  265]
train() client id: f_00001-7-7 loss: 0.532971  [  256/  265]
train() client id: f_00001-8-0 loss: 0.400540  [   32/  265]
train() client id: f_00001-8-1 loss: 0.489520  [   64/  265]
train() client id: f_00001-8-2 loss: 0.461482  [   96/  265]
train() client id: f_00001-8-3 loss: 0.455285  [  128/  265]
train() client id: f_00001-8-4 loss: 0.356425  [  160/  265]
train() client id: f_00001-8-5 loss: 0.488633  [  192/  265]
train() client id: f_00001-8-6 loss: 0.536844  [  224/  265]
train() client id: f_00001-8-7 loss: 0.510323  [  256/  265]
train() client id: f_00001-9-0 loss: 0.358363  [   32/  265]
train() client id: f_00001-9-1 loss: 0.492185  [   64/  265]
train() client id: f_00001-9-2 loss: 0.586143  [   96/  265]
train() client id: f_00001-9-3 loss: 0.558674  [  128/  265]
train() client id: f_00001-9-4 loss: 0.418895  [  160/  265]
train() client id: f_00001-9-5 loss: 0.504355  [  192/  265]
train() client id: f_00001-9-6 loss: 0.355887  [  224/  265]
train() client id: f_00001-9-7 loss: 0.445525  [  256/  265]
train() client id: f_00001-10-0 loss: 0.368061  [   32/  265]
train() client id: f_00001-10-1 loss: 0.452071  [   64/  265]
train() client id: f_00001-10-2 loss: 0.486499  [   96/  265]
train() client id: f_00001-10-3 loss: 0.419530  [  128/  265]
train() client id: f_00001-10-4 loss: 0.512425  [  160/  265]
train() client id: f_00001-10-5 loss: 0.409012  [  192/  265]
train() client id: f_00001-10-6 loss: 0.468806  [  224/  265]
train() client id: f_00001-10-7 loss: 0.508645  [  256/  265]
train() client id: f_00002-0-0 loss: 0.859336  [   32/  124]
train() client id: f_00002-0-1 loss: 1.016561  [   64/  124]
train() client id: f_00002-0-2 loss: 0.816008  [   96/  124]
train() client id: f_00002-1-0 loss: 0.867082  [   32/  124]
train() client id: f_00002-1-1 loss: 0.833855  [   64/  124]
train() client id: f_00002-1-2 loss: 0.787250  [   96/  124]
train() client id: f_00002-2-0 loss: 0.855411  [   32/  124]
train() client id: f_00002-2-1 loss: 0.969180  [   64/  124]
train() client id: f_00002-2-2 loss: 0.534404  [   96/  124]
train() client id: f_00002-3-0 loss: 0.803477  [   32/  124]
train() client id: f_00002-3-1 loss: 0.761641  [   64/  124]
train() client id: f_00002-3-2 loss: 0.831008  [   96/  124]
train() client id: f_00002-4-0 loss: 0.752182  [   32/  124]
train() client id: f_00002-4-1 loss: 0.681290  [   64/  124]
train() client id: f_00002-4-2 loss: 0.722228  [   96/  124]
train() client id: f_00002-5-0 loss: 0.821644  [   32/  124]
train() client id: f_00002-5-1 loss: 0.879425  [   64/  124]
train() client id: f_00002-5-2 loss: 0.609641  [   96/  124]
train() client id: f_00002-6-0 loss: 0.679153  [   32/  124]
train() client id: f_00002-6-1 loss: 0.735448  [   64/  124]
train() client id: f_00002-6-2 loss: 0.727440  [   96/  124]
train() client id: f_00002-7-0 loss: 0.787712  [   32/  124]
train() client id: f_00002-7-1 loss: 0.556113  [   64/  124]
train() client id: f_00002-7-2 loss: 0.599009  [   96/  124]
train() client id: f_00002-8-0 loss: 0.576723  [   32/  124]
train() client id: f_00002-8-1 loss: 0.680485  [   64/  124]
train() client id: f_00002-8-2 loss: 0.805865  [   96/  124]
train() client id: f_00002-9-0 loss: 0.721169  [   32/  124]
train() client id: f_00002-9-1 loss: 0.813592  [   64/  124]
train() client id: f_00002-9-2 loss: 0.666208  [   96/  124]
train() client id: f_00002-10-0 loss: 0.585605  [   32/  124]
train() client id: f_00002-10-1 loss: 0.726733  [   64/  124]
train() client id: f_00002-10-2 loss: 0.585459  [   96/  124]
train() client id: f_00003-0-0 loss: 0.367213  [   32/   43]
train() client id: f_00003-1-0 loss: 0.304803  [   32/   43]
train() client id: f_00003-2-0 loss: 0.422879  [   32/   43]
train() client id: f_00003-3-0 loss: 0.277879  [   32/   43]
train() client id: f_00003-4-0 loss: 0.396925  [   32/   43]
train() client id: f_00003-5-0 loss: 0.491724  [   32/   43]
train() client id: f_00003-6-0 loss: 0.305346  [   32/   43]
train() client id: f_00003-7-0 loss: 0.473020  [   32/   43]
train() client id: f_00003-8-0 loss: 0.649017  [   32/   43]
train() client id: f_00003-9-0 loss: 0.640511  [   32/   43]
train() client id: f_00003-10-0 loss: 0.486326  [   32/   43]
train() client id: f_00004-0-0 loss: 0.854128  [   32/  306]
train() client id: f_00004-0-1 loss: 0.820119  [   64/  306]
train() client id: f_00004-0-2 loss: 0.838876  [   96/  306]
train() client id: f_00004-0-3 loss: 1.027117  [  128/  306]
train() client id: f_00004-0-4 loss: 0.905174  [  160/  306]
train() client id: f_00004-0-5 loss: 0.780282  [  192/  306]
train() client id: f_00004-0-6 loss: 0.791859  [  224/  306]
train() client id: f_00004-0-7 loss: 0.844910  [  256/  306]
train() client id: f_00004-0-8 loss: 0.898854  [  288/  306]
train() client id: f_00004-1-0 loss: 0.966980  [   32/  306]
train() client id: f_00004-1-1 loss: 0.962142  [   64/  306]
train() client id: f_00004-1-2 loss: 0.752679  [   96/  306]
train() client id: f_00004-1-3 loss: 0.877376  [  128/  306]
train() client id: f_00004-1-4 loss: 0.942871  [  160/  306]
train() client id: f_00004-1-5 loss: 0.863635  [  192/  306]
train() client id: f_00004-1-6 loss: 0.894253  [  224/  306]
train() client id: f_00004-1-7 loss: 0.738519  [  256/  306]
train() client id: f_00004-1-8 loss: 0.745851  [  288/  306]
train() client id: f_00004-2-0 loss: 0.890122  [   32/  306]
train() client id: f_00004-2-1 loss: 0.929256  [   64/  306]
train() client id: f_00004-2-2 loss: 1.007611  [   96/  306]
train() client id: f_00004-2-3 loss: 0.850578  [  128/  306]
train() client id: f_00004-2-4 loss: 0.768899  [  160/  306]
train() client id: f_00004-2-5 loss: 0.851127  [  192/  306]
train() client id: f_00004-2-6 loss: 0.835984  [  224/  306]
train() client id: f_00004-2-7 loss: 0.857408  [  256/  306]
train() client id: f_00004-2-8 loss: 0.789567  [  288/  306]
train() client id: f_00004-3-0 loss: 0.871757  [   32/  306]
train() client id: f_00004-3-1 loss: 0.807704  [   64/  306]
train() client id: f_00004-3-2 loss: 0.816835  [   96/  306]
train() client id: f_00004-3-3 loss: 0.876944  [  128/  306]
train() client id: f_00004-3-4 loss: 0.897749  [  160/  306]
train() client id: f_00004-3-5 loss: 0.861642  [  192/  306]
train() client id: f_00004-3-6 loss: 1.023259  [  224/  306]
train() client id: f_00004-3-7 loss: 0.868743  [  256/  306]
train() client id: f_00004-3-8 loss: 0.782893  [  288/  306]
train() client id: f_00004-4-0 loss: 0.912928  [   32/  306]
train() client id: f_00004-4-1 loss: 0.834387  [   64/  306]
train() client id: f_00004-4-2 loss: 0.813918  [   96/  306]
train() client id: f_00004-4-3 loss: 0.864690  [  128/  306]
train() client id: f_00004-4-4 loss: 0.900536  [  160/  306]
train() client id: f_00004-4-5 loss: 0.846174  [  192/  306]
train() client id: f_00004-4-6 loss: 0.898797  [  224/  306]
train() client id: f_00004-4-7 loss: 0.844614  [  256/  306]
train() client id: f_00004-4-8 loss: 0.814586  [  288/  306]
train() client id: f_00004-5-0 loss: 0.702180  [   32/  306]
train() client id: f_00004-5-1 loss: 0.945426  [   64/  306]
train() client id: f_00004-5-2 loss: 0.809207  [   96/  306]
train() client id: f_00004-5-3 loss: 0.853449  [  128/  306]
train() client id: f_00004-5-4 loss: 0.961624  [  160/  306]
train() client id: f_00004-5-5 loss: 0.961581  [  192/  306]
train() client id: f_00004-5-6 loss: 0.925063  [  224/  306]
train() client id: f_00004-5-7 loss: 0.796736  [  256/  306]
train() client id: f_00004-5-8 loss: 0.790992  [  288/  306]
train() client id: f_00004-6-0 loss: 0.813738  [   32/  306]
train() client id: f_00004-6-1 loss: 0.810526  [   64/  306]
train() client id: f_00004-6-2 loss: 0.804385  [   96/  306]
train() client id: f_00004-6-3 loss: 0.873109  [  128/  306]
train() client id: f_00004-6-4 loss: 0.840754  [  160/  306]
train() client id: f_00004-6-5 loss: 0.824317  [  192/  306]
train() client id: f_00004-6-6 loss: 1.065793  [  224/  306]
train() client id: f_00004-6-7 loss: 0.830544  [  256/  306]
train() client id: f_00004-6-8 loss: 0.834241  [  288/  306]
train() client id: f_00004-7-0 loss: 0.844820  [   32/  306]
train() client id: f_00004-7-1 loss: 0.844791  [   64/  306]
train() client id: f_00004-7-2 loss: 0.774289  [   96/  306]
train() client id: f_00004-7-3 loss: 0.907045  [  128/  306]
train() client id: f_00004-7-4 loss: 0.797688  [  160/  306]
train() client id: f_00004-7-5 loss: 0.940554  [  192/  306]
train() client id: f_00004-7-6 loss: 0.874726  [  224/  306]
train() client id: f_00004-7-7 loss: 0.886865  [  256/  306]
train() client id: f_00004-7-8 loss: 0.828024  [  288/  306]
train() client id: f_00004-8-0 loss: 0.785902  [   32/  306]
train() client id: f_00004-8-1 loss: 0.963994  [   64/  306]
train() client id: f_00004-8-2 loss: 0.800159  [   96/  306]
train() client id: f_00004-8-3 loss: 0.802017  [  128/  306]
train() client id: f_00004-8-4 loss: 0.797677  [  160/  306]
train() client id: f_00004-8-5 loss: 0.880248  [  192/  306]
train() client id: f_00004-8-6 loss: 0.777448  [  224/  306]
train() client id: f_00004-8-7 loss: 0.961585  [  256/  306]
train() client id: f_00004-8-8 loss: 0.829775  [  288/  306]
train() client id: f_00004-9-0 loss: 0.761921  [   32/  306]
train() client id: f_00004-9-1 loss: 0.851039  [   64/  306]
train() client id: f_00004-9-2 loss: 0.951366  [   96/  306]
train() client id: f_00004-9-3 loss: 0.773803  [  128/  306]
train() client id: f_00004-9-4 loss: 0.798732  [  160/  306]
train() client id: f_00004-9-5 loss: 0.841205  [  192/  306]
train() client id: f_00004-9-6 loss: 1.004247  [  224/  306]
train() client id: f_00004-9-7 loss: 0.825470  [  256/  306]
train() client id: f_00004-9-8 loss: 0.844788  [  288/  306]
train() client id: f_00004-10-0 loss: 0.710051  [   32/  306]
train() client id: f_00004-10-1 loss: 0.794912  [   64/  306]
train() client id: f_00004-10-2 loss: 0.762995  [   96/  306]
train() client id: f_00004-10-3 loss: 0.848022  [  128/  306]
train() client id: f_00004-10-4 loss: 0.924447  [  160/  306]
train() client id: f_00004-10-5 loss: 0.817639  [  192/  306]
train() client id: f_00004-10-6 loss: 1.052020  [  224/  306]
train() client id: f_00004-10-7 loss: 0.945590  [  256/  306]
train() client id: f_00004-10-8 loss: 0.787377  [  288/  306]
train() client id: f_00005-0-0 loss: 0.549290  [   32/  146]
train() client id: f_00005-0-1 loss: 1.060284  [   64/  146]
train() client id: f_00005-0-2 loss: 0.711232  [   96/  146]
train() client id: f_00005-0-3 loss: 0.827276  [  128/  146]
train() client id: f_00005-1-0 loss: 0.779958  [   32/  146]
train() client id: f_00005-1-1 loss: 0.577311  [   64/  146]
train() client id: f_00005-1-2 loss: 0.768037  [   96/  146]
train() client id: f_00005-1-3 loss: 1.033163  [  128/  146]
train() client id: f_00005-2-0 loss: 0.793016  [   32/  146]
train() client id: f_00005-2-1 loss: 0.532771  [   64/  146]
train() client id: f_00005-2-2 loss: 1.119972  [   96/  146]
train() client id: f_00005-2-3 loss: 0.764106  [  128/  146]
train() client id: f_00005-3-0 loss: 1.038275  [   32/  146]
train() client id: f_00005-3-1 loss: 0.557269  [   64/  146]
train() client id: f_00005-3-2 loss: 0.853786  [   96/  146]
train() client id: f_00005-3-3 loss: 0.856852  [  128/  146]
train() client id: f_00005-4-0 loss: 0.906947  [   32/  146]
train() client id: f_00005-4-1 loss: 0.750189  [   64/  146]
train() client id: f_00005-4-2 loss: 0.638110  [   96/  146]
train() client id: f_00005-4-3 loss: 0.984694  [  128/  146]
train() client id: f_00005-5-0 loss: 0.759515  [   32/  146]
train() client id: f_00005-5-1 loss: 0.512192  [   64/  146]
train() client id: f_00005-5-2 loss: 0.741750  [   96/  146]
train() client id: f_00005-5-3 loss: 0.974270  [  128/  146]
train() client id: f_00005-6-0 loss: 0.514463  [   32/  146]
train() client id: f_00005-6-1 loss: 0.996955  [   64/  146]
train() client id: f_00005-6-2 loss: 0.667080  [   96/  146]
train() client id: f_00005-6-3 loss: 0.840013  [  128/  146]
train() client id: f_00005-7-0 loss: 0.422999  [   32/  146]
train() client id: f_00005-7-1 loss: 1.156223  [   64/  146]
train() client id: f_00005-7-2 loss: 1.068238  [   96/  146]
train() client id: f_00005-7-3 loss: 0.650771  [  128/  146]
train() client id: f_00005-8-0 loss: 0.772550  [   32/  146]
train() client id: f_00005-8-1 loss: 0.765145  [   64/  146]
train() client id: f_00005-8-2 loss: 0.830223  [   96/  146]
train() client id: f_00005-8-3 loss: 0.772023  [  128/  146]
train() client id: f_00005-9-0 loss: 1.119862  [   32/  146]
train() client id: f_00005-9-1 loss: 0.678502  [   64/  146]
train() client id: f_00005-9-2 loss: 0.629064  [   96/  146]
train() client id: f_00005-9-3 loss: 0.726779  [  128/  146]
train() client id: f_00005-10-0 loss: 0.648489  [   32/  146]
train() client id: f_00005-10-1 loss: 0.793721  [   64/  146]
train() client id: f_00005-10-2 loss: 0.965144  [   96/  146]
train() client id: f_00005-10-3 loss: 0.826584  [  128/  146]
train() client id: f_00006-0-0 loss: 0.407765  [   32/   54]
train() client id: f_00006-1-0 loss: 0.523788  [   32/   54]
train() client id: f_00006-2-0 loss: 0.503919  [   32/   54]
train() client id: f_00006-3-0 loss: 0.500278  [   32/   54]
train() client id: f_00006-4-0 loss: 0.454882  [   32/   54]
train() client id: f_00006-5-0 loss: 0.477055  [   32/   54]
train() client id: f_00006-6-0 loss: 0.476250  [   32/   54]
train() client id: f_00006-7-0 loss: 0.403504  [   32/   54]
train() client id: f_00006-8-0 loss: 0.444280  [   32/   54]
train() client id: f_00006-9-0 loss: 0.454913  [   32/   54]
train() client id: f_00006-10-0 loss: 0.399206  [   32/   54]
train() client id: f_00007-0-0 loss: 0.640534  [   32/  179]
train() client id: f_00007-0-1 loss: 0.562843  [   64/  179]
train() client id: f_00007-0-2 loss: 0.408252  [   96/  179]
train() client id: f_00007-0-3 loss: 0.707715  [  128/  179]
train() client id: f_00007-0-4 loss: 0.665696  [  160/  179]
train() client id: f_00007-1-0 loss: 0.567276  [   32/  179]
train() client id: f_00007-1-1 loss: 0.577303  [   64/  179]
train() client id: f_00007-1-2 loss: 0.487484  [   96/  179]
train() client id: f_00007-1-3 loss: 0.515721  [  128/  179]
train() client id: f_00007-1-4 loss: 0.715748  [  160/  179]
train() client id: f_00007-2-0 loss: 0.475120  [   32/  179]
train() client id: f_00007-2-1 loss: 0.576526  [   64/  179]
train() client id: f_00007-2-2 loss: 0.573329  [   96/  179]
train() client id: f_00007-2-3 loss: 0.702683  [  128/  179]
train() client id: f_00007-2-4 loss: 0.388100  [  160/  179]
train() client id: f_00007-3-0 loss: 0.641234  [   32/  179]
train() client id: f_00007-3-1 loss: 0.548090  [   64/  179]
train() client id: f_00007-3-2 loss: 0.591650  [   96/  179]
train() client id: f_00007-3-3 loss: 0.531696  [  128/  179]
train() client id: f_00007-3-4 loss: 0.497782  [  160/  179]
train() client id: f_00007-4-0 loss: 0.558488  [   32/  179]
train() client id: f_00007-4-1 loss: 0.547517  [   64/  179]
train() client id: f_00007-4-2 loss: 0.368942  [   96/  179]
train() client id: f_00007-4-3 loss: 0.721335  [  128/  179]
train() client id: f_00007-4-4 loss: 0.567801  [  160/  179]
train() client id: f_00007-5-0 loss: 0.422254  [   32/  179]
train() client id: f_00007-5-1 loss: 0.557206  [   64/  179]
train() client id: f_00007-5-2 loss: 0.583910  [   96/  179]
train() client id: f_00007-5-3 loss: 0.724159  [  128/  179]
train() client id: f_00007-5-4 loss: 0.461097  [  160/  179]
train() client id: f_00007-6-0 loss: 0.419083  [   32/  179]
train() client id: f_00007-6-1 loss: 0.466833  [   64/  179]
train() client id: f_00007-6-2 loss: 0.575374  [   96/  179]
train() client id: f_00007-6-3 loss: 0.427052  [  128/  179]
train() client id: f_00007-6-4 loss: 0.487291  [  160/  179]
train() client id: f_00007-7-0 loss: 0.365006  [   32/  179]
train() client id: f_00007-7-1 loss: 0.412439  [   64/  179]
train() client id: f_00007-7-2 loss: 0.701449  [   96/  179]
train() client id: f_00007-7-3 loss: 0.531384  [  128/  179]
train() client id: f_00007-7-4 loss: 0.371697  [  160/  179]
train() client id: f_00007-8-0 loss: 0.487540  [   32/  179]
train() client id: f_00007-8-1 loss: 0.551361  [   64/  179]
train() client id: f_00007-8-2 loss: 0.370850  [   96/  179]
train() client id: f_00007-8-3 loss: 0.467258  [  128/  179]
train() client id: f_00007-8-4 loss: 0.733764  [  160/  179]
train() client id: f_00007-9-0 loss: 0.646919  [   32/  179]
train() client id: f_00007-9-1 loss: 0.457050  [   64/  179]
train() client id: f_00007-9-2 loss: 0.462824  [   96/  179]
train() client id: f_00007-9-3 loss: 0.422778  [  128/  179]
train() client id: f_00007-9-4 loss: 0.591977  [  160/  179]
train() client id: f_00007-10-0 loss: 0.512172  [   32/  179]
train() client id: f_00007-10-1 loss: 0.623798  [   64/  179]
train() client id: f_00007-10-2 loss: 0.633343  [   96/  179]
train() client id: f_00007-10-3 loss: 0.386232  [  128/  179]
train() client id: f_00007-10-4 loss: 0.531275  [  160/  179]
train() client id: f_00008-0-0 loss: 0.640350  [   32/  130]
train() client id: f_00008-0-1 loss: 0.610080  [   64/  130]
train() client id: f_00008-0-2 loss: 0.740766  [   96/  130]
train() client id: f_00008-0-3 loss: 0.699708  [  128/  130]
train() client id: f_00008-1-0 loss: 0.680789  [   32/  130]
train() client id: f_00008-1-1 loss: 0.598012  [   64/  130]
train() client id: f_00008-1-2 loss: 0.651022  [   96/  130]
train() client id: f_00008-1-3 loss: 0.737899  [  128/  130]
train() client id: f_00008-2-0 loss: 0.557822  [   32/  130]
train() client id: f_00008-2-1 loss: 0.676462  [   64/  130]
train() client id: f_00008-2-2 loss: 0.690874  [   96/  130]
train() client id: f_00008-2-3 loss: 0.729635  [  128/  130]
train() client id: f_00008-3-0 loss: 0.624533  [   32/  130]
train() client id: f_00008-3-1 loss: 0.653066  [   64/  130]
train() client id: f_00008-3-2 loss: 0.619270  [   96/  130]
train() client id: f_00008-3-3 loss: 0.793930  [  128/  130]
train() client id: f_00008-4-0 loss: 0.751806  [   32/  130]
train() client id: f_00008-4-1 loss: 0.622807  [   64/  130]
train() client id: f_00008-4-2 loss: 0.601402  [   96/  130]
train() client id: f_00008-4-3 loss: 0.710017  [  128/  130]
train() client id: f_00008-5-0 loss: 0.708391  [   32/  130]
train() client id: f_00008-5-1 loss: 0.761988  [   64/  130]
train() client id: f_00008-5-2 loss: 0.677557  [   96/  130]
train() client id: f_00008-5-3 loss: 0.537522  [  128/  130]
train() client id: f_00008-6-0 loss: 0.613908  [   32/  130]
train() client id: f_00008-6-1 loss: 0.624204  [   64/  130]
train() client id: f_00008-6-2 loss: 0.761870  [   96/  130]
train() client id: f_00008-6-3 loss: 0.653678  [  128/  130]
train() client id: f_00008-7-0 loss: 0.731100  [   32/  130]
train() client id: f_00008-7-1 loss: 0.618326  [   64/  130]
train() client id: f_00008-7-2 loss: 0.698993  [   96/  130]
train() client id: f_00008-7-3 loss: 0.629587  [  128/  130]
train() client id: f_00008-8-0 loss: 0.677485  [   32/  130]
train() client id: f_00008-8-1 loss: 0.684610  [   64/  130]
train() client id: f_00008-8-2 loss: 0.661972  [   96/  130]
train() client id: f_00008-8-3 loss: 0.668607  [  128/  130]
train() client id: f_00008-9-0 loss: 0.683278  [   32/  130]
train() client id: f_00008-9-1 loss: 0.601069  [   64/  130]
train() client id: f_00008-9-2 loss: 0.770473  [   96/  130]
train() client id: f_00008-9-3 loss: 0.631965  [  128/  130]
train() client id: f_00008-10-0 loss: 0.660305  [   32/  130]
train() client id: f_00008-10-1 loss: 0.643621  [   64/  130]
train() client id: f_00008-10-2 loss: 0.723236  [   96/  130]
train() client id: f_00008-10-3 loss: 0.640709  [  128/  130]
train() client id: f_00009-0-0 loss: 0.855226  [   32/  118]
train() client id: f_00009-0-1 loss: 0.987997  [   64/  118]
train() client id: f_00009-0-2 loss: 1.009834  [   96/  118]
train() client id: f_00009-1-0 loss: 0.838877  [   32/  118]
train() client id: f_00009-1-1 loss: 0.959970  [   64/  118]
train() client id: f_00009-1-2 loss: 0.867055  [   96/  118]
train() client id: f_00009-2-0 loss: 0.917715  [   32/  118]
train() client id: f_00009-2-1 loss: 0.696071  [   64/  118]
train() client id: f_00009-2-2 loss: 0.975953  [   96/  118]
train() client id: f_00009-3-0 loss: 0.666540  [   32/  118]
train() client id: f_00009-3-1 loss: 1.032777  [   64/  118]
train() client id: f_00009-3-2 loss: 0.865733  [   96/  118]
train() client id: f_00009-4-0 loss: 0.847513  [   32/  118]
train() client id: f_00009-4-1 loss: 0.759233  [   64/  118]
train() client id: f_00009-4-2 loss: 0.934669  [   96/  118]
train() client id: f_00009-5-0 loss: 0.846126  [   32/  118]
train() client id: f_00009-5-1 loss: 0.882668  [   64/  118]
train() client id: f_00009-5-2 loss: 0.752814  [   96/  118]
train() client id: f_00009-6-0 loss: 0.730840  [   32/  118]
train() client id: f_00009-6-1 loss: 0.914182  [   64/  118]
train() client id: f_00009-6-2 loss: 0.953192  [   96/  118]
train() client id: f_00009-7-0 loss: 0.726313  [   32/  118]
train() client id: f_00009-7-1 loss: 0.684087  [   64/  118]
train() client id: f_00009-7-2 loss: 0.938137  [   96/  118]
train() client id: f_00009-8-0 loss: 0.736139  [   32/  118]
train() client id: f_00009-8-1 loss: 0.947272  [   64/  118]
train() client id: f_00009-8-2 loss: 0.888497  [   96/  118]
train() client id: f_00009-9-0 loss: 0.868270  [   32/  118]
train() client id: f_00009-9-1 loss: 0.711553  [   64/  118]
train() client id: f_00009-9-2 loss: 0.796584  [   96/  118]
train() client id: f_00009-10-0 loss: 1.078281  [   32/  118]
train() client id: f_00009-10-1 loss: 0.749991  [   64/  118]
train() client id: f_00009-10-2 loss: 0.711358  [   96/  118]
At round 51 accuracy: 0.6472148541114059
At round 51 training accuracy: 0.596244131455399
At round 51 training loss: 0.8157636283666374
update_location
xs = [  -3.9056584     4.20031788  275.00902392   18.81129433    0.97929623
    3.95640986 -237.44319194 -216.32485185  259.66397685 -202.06087855]
ys = [ 267.5879595   250.55583871    1.32061395 -237.45517586  229.35018685
  212.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [285.68963971 269.80709958 292.62895834 258.33858662 250.20485053
 235.1712481  257.65511821 238.32145892 278.80826897 225.48749521]
dists_bs = [194.53998039 194.4773475  482.36153531 455.72150476 184.18104004
 182.90791844 188.28113892 179.01284267 462.29567447 173.12649821]
uav_gains = [3.55494025e-12 4.95127103e-12 3.08824037e-12 6.27981480e-12
 7.39020348e-12 9.78653336e-12 6.36780404e-12 9.24954034e-12
 4.10015262e-12 1.15466699e-11]
bs_gains = [4.30595271e-11 4.30983677e-11 3.38727622e-12 3.97132493e-12
 5.01889727e-11 5.11732577e-11 4.71883724e-11 5.43523537e-11
 3.81521100e-12 5.96865065e-11]
Round 52
-------------------------------
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.13977177 8.44962292 4.08581891 1.48801359 9.74245982 4.68747181
 1.834924   5.76952797 4.26183039 3.80061293]
obj_prev = 48.26005410532363
eta_min = 5.2478248998997527e-23	eta_max = 0.9386552346452276
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 11.138694899951254	eta = 0.9090909090909091
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 23.212442703241933	eta = 0.43623527270004686
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 16.931595727685714	eta = 0.598058590314986
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.799778450901695	eta = 0.6409005230136667
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733820172197483	eta = 0.6435872637324471
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733571912722388	eta = 0.6435974188731335
eta = 0.6435974188731335
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.03636401 0.07647987 0.03578681 0.01240994 0.0883126  0.04213608
 0.01558458 0.05165999 0.03751843 0.03405519]
ene_total = [1.50031121 2.44142183 1.50915709 0.72099371 2.77897583 1.43455086
 0.8117007  1.8214624  1.52193281 1.19306548]
ti_comp = [0.78968755 0.8674062  0.78087534 0.81798185 0.86972334 0.87000865
 0.81855413 0.83227438 0.79380079 0.87219318]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [4.81930945e-06 3.71600544e-05 4.69770128e-06 1.78526131e-07
 5.69096341e-05 6.17725006e-06 3.53077655e-07 1.24396969e-05
 5.23830472e-06 3.24492313e-06]
ene_total = [0.44237055 0.21717421 0.46800782 0.35990781 0.2110067  0.20870035
 0.35824773 0.31867769 0.43041446 0.20225874]
optimize_network iter = 0 obj = 3.2167660550334363
eta = 0.6435974188731335
freqs = [23024303.73895735 44085381.74515043 22914544.46834145  7585706.59027375
 50770513.42376783 24215895.58340987  9519576.07846752 31035429.18235327
 23632141.3100814  19522731.20042242]
eta_min = 0.6435974188731354	eta_max = 0.698454705717541
af = 0.003087860896766924	bf = 1.1451029605675662	zeta = 0.003396646986443617	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.04231482e-06 8.03693475e-06 1.01601355e-06 3.86114307e-08
 1.23083516e-05 1.33600869e-06 7.63632379e-08 2.69044364e-06
 1.13293465e-06 7.01808322e-07]
ene_total = [1.7338512  0.84809379 1.83437118 1.41097682 0.82214886 0.81764254
 1.40445301 1.24824095 1.68694076 0.79265082]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 1 obj = 3.8009831688814426
eta = 0.698454705717541
freqs = [22965892.7521965  43105303.11724808 22914544.46834147  7508078.8429111
 49615295.72405244 23663342.1290412   9420739.8305015  30604647.12122058
 23544761.66323882 19067718.62692055]
eta_min = 0.698454705717542	eta_max = 0.6984547057175405
af = 0.002969273754740996	bf = 1.1451029605675662	zeta = 0.003266201130215096	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.03703298e-06 7.68356267e-06 1.01601355e-06 3.78252199e-08
 1.17546026e-05 1.27573473e-06 7.47857989e-08 2.61627349e-06
 1.12457210e-06 6.69475724e-07]
ene_total = [1.73385059 0.84805348 1.83437118 1.41097673 0.8220857  0.81763566
 1.40445283 1.24823249 1.68693981 0.79264713]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 2 obj = 3.800983168881437
eta = 0.6984547057175405
freqs = [22965892.7521965  43105303.11724809 22914544.46834146  7508078.8429111
 49615295.72405244 23663342.12904121  9420739.8305015  30604647.12122059
 23544761.66323882 19067718.62692055]
Done!
At round 52 energy consumption: 3.8009831688814426
At round 52 eta: 0.6984547057175405
At round 52 local rounds: 11.751716534008594
At round 52 global rounds: 34.39025251090811
At round 52 a_n: 10.027672966880559
gradient difference: 0.5702829360961914
train() client id: f_00000-0-0 loss: 1.031065  [   32/  126]
train() client id: f_00000-0-1 loss: 1.036789  [   64/  126]
train() client id: f_00000-0-2 loss: 1.142447  [   96/  126]
train() client id: f_00000-1-0 loss: 1.129655  [   32/  126]
train() client id: f_00000-1-1 loss: 1.066830  [   64/  126]
train() client id: f_00000-1-2 loss: 1.006613  [   96/  126]
train() client id: f_00000-2-0 loss: 1.090016  [   32/  126]
train() client id: f_00000-2-1 loss: 0.948797  [   64/  126]
train() client id: f_00000-2-2 loss: 0.931679  [   96/  126]
train() client id: f_00000-3-0 loss: 0.913696  [   32/  126]
train() client id: f_00000-3-1 loss: 1.029057  [   64/  126]
train() client id: f_00000-3-2 loss: 0.793427  [   96/  126]
train() client id: f_00000-4-0 loss: 0.773223  [   32/  126]
train() client id: f_00000-4-1 loss: 0.946782  [   64/  126]
train() client id: f_00000-4-2 loss: 0.926886  [   96/  126]
train() client id: f_00000-5-0 loss: 1.040753  [   32/  126]
train() client id: f_00000-5-1 loss: 0.937177  [   64/  126]
train() client id: f_00000-5-2 loss: 0.918903  [   96/  126]
train() client id: f_00000-6-0 loss: 0.847646  [   32/  126]
train() client id: f_00000-6-1 loss: 0.898949  [   64/  126]
train() client id: f_00000-6-2 loss: 1.021041  [   96/  126]
train() client id: f_00000-7-0 loss: 0.899800  [   32/  126]
train() client id: f_00000-7-1 loss: 0.914112  [   64/  126]
train() client id: f_00000-7-2 loss: 0.779413  [   96/  126]
train() client id: f_00000-8-0 loss: 0.841802  [   32/  126]
train() client id: f_00000-8-1 loss: 0.908718  [   64/  126]
train() client id: f_00000-8-2 loss: 0.953331  [   96/  126]
train() client id: f_00000-9-0 loss: 0.868332  [   32/  126]
train() client id: f_00000-9-1 loss: 0.990955  [   64/  126]
train() client id: f_00000-9-2 loss: 0.938060  [   96/  126]
train() client id: f_00000-10-0 loss: 0.957151  [   32/  126]
train() client id: f_00000-10-1 loss: 0.903084  [   64/  126]
train() client id: f_00000-10-2 loss: 0.983737  [   96/  126]
train() client id: f_00001-0-0 loss: 0.435832  [   32/  265]
train() client id: f_00001-0-1 loss: 0.418928  [   64/  265]
train() client id: f_00001-0-2 loss: 0.472855  [   96/  265]
train() client id: f_00001-0-3 loss: 0.488756  [  128/  265]
train() client id: f_00001-0-4 loss: 0.427680  [  160/  265]
train() client id: f_00001-0-5 loss: 0.332110  [  192/  265]
train() client id: f_00001-0-6 loss: 0.451526  [  224/  265]
train() client id: f_00001-0-7 loss: 0.448507  [  256/  265]
train() client id: f_00001-1-0 loss: 0.615425  [   32/  265]
train() client id: f_00001-1-1 loss: 0.430314  [   64/  265]
train() client id: f_00001-1-2 loss: 0.463352  [   96/  265]
train() client id: f_00001-1-3 loss: 0.387974  [  128/  265]
train() client id: f_00001-1-4 loss: 0.423291  [  160/  265]
train() client id: f_00001-1-5 loss: 0.368145  [  192/  265]
train() client id: f_00001-1-6 loss: 0.361506  [  224/  265]
train() client id: f_00001-1-7 loss: 0.361633  [  256/  265]
train() client id: f_00001-2-0 loss: 0.540183  [   32/  265]
train() client id: f_00001-2-1 loss: 0.434329  [   64/  265]
train() client id: f_00001-2-2 loss: 0.410784  [   96/  265]
train() client id: f_00001-2-3 loss: 0.357654  [  128/  265]
train() client id: f_00001-2-4 loss: 0.428597  [  160/  265]
train() client id: f_00001-2-5 loss: 0.406086  [  192/  265]
train() client id: f_00001-2-6 loss: 0.394785  [  224/  265]
train() client id: f_00001-2-7 loss: 0.400629  [  256/  265]
train() client id: f_00001-3-0 loss: 0.432282  [   32/  265]
train() client id: f_00001-3-1 loss: 0.367208  [   64/  265]
train() client id: f_00001-3-2 loss: 0.517273  [   96/  265]
train() client id: f_00001-3-3 loss: 0.356097  [  128/  265]
train() client id: f_00001-3-4 loss: 0.521793  [  160/  265]
train() client id: f_00001-3-5 loss: 0.428725  [  192/  265]
train() client id: f_00001-3-6 loss: 0.299108  [  224/  265]
train() client id: f_00001-3-7 loss: 0.392028  [  256/  265]
train() client id: f_00001-4-0 loss: 0.347899  [   32/  265]
train() client id: f_00001-4-1 loss: 0.420559  [   64/  265]
train() client id: f_00001-4-2 loss: 0.526586  [   96/  265]
train() client id: f_00001-4-3 loss: 0.389142  [  128/  265]
train() client id: f_00001-4-4 loss: 0.396655  [  160/  265]
train() client id: f_00001-4-5 loss: 0.478939  [  192/  265]
train() client id: f_00001-4-6 loss: 0.340593  [  224/  265]
train() client id: f_00001-4-7 loss: 0.377686  [  256/  265]
train() client id: f_00001-5-0 loss: 0.455030  [   32/  265]
train() client id: f_00001-5-1 loss: 0.480324  [   64/  265]
train() client id: f_00001-5-2 loss: 0.269075  [   96/  265]
train() client id: f_00001-5-3 loss: 0.366527  [  128/  265]
train() client id: f_00001-5-4 loss: 0.439142  [  160/  265]
train() client id: f_00001-5-5 loss: 0.386494  [  192/  265]
train() client id: f_00001-5-6 loss: 0.498930  [  224/  265]
train() client id: f_00001-5-7 loss: 0.318081  [  256/  265]
train() client id: f_00001-6-0 loss: 0.439174  [   32/  265]
train() client id: f_00001-6-1 loss: 0.374714  [   64/  265]
train() client id: f_00001-6-2 loss: 0.447276  [   96/  265]
train() client id: f_00001-6-3 loss: 0.398896  [  128/  265]
train() client id: f_00001-6-4 loss: 0.372610  [  160/  265]
train() client id: f_00001-6-5 loss: 0.371085  [  192/  265]
train() client id: f_00001-6-6 loss: 0.360861  [  224/  265]
train() client id: f_00001-6-7 loss: 0.484451  [  256/  265]
train() client id: f_00001-7-0 loss: 0.481925  [   32/  265]
train() client id: f_00001-7-1 loss: 0.357833  [   64/  265]
train() client id: f_00001-7-2 loss: 0.385835  [   96/  265]
train() client id: f_00001-7-3 loss: 0.305816  [  128/  265]
train() client id: f_00001-7-4 loss: 0.374820  [  160/  265]
train() client id: f_00001-7-5 loss: 0.387579  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432598  [  224/  265]
train() client id: f_00001-7-7 loss: 0.455580  [  256/  265]
train() client id: f_00001-8-0 loss: 0.362318  [   32/  265]
train() client id: f_00001-8-1 loss: 0.364305  [   64/  265]
train() client id: f_00001-8-2 loss: 0.445512  [   96/  265]
train() client id: f_00001-8-3 loss: 0.367080  [  128/  265]
train() client id: f_00001-8-4 loss: 0.528988  [  160/  265]
train() client id: f_00001-8-5 loss: 0.298663  [  192/  265]
train() client id: f_00001-8-6 loss: 0.444828  [  224/  265]
train() client id: f_00001-8-7 loss: 0.400737  [  256/  265]
train() client id: f_00001-9-0 loss: 0.311467  [   32/  265]
train() client id: f_00001-9-1 loss: 0.390042  [   64/  265]
train() client id: f_00001-9-2 loss: 0.413006  [   96/  265]
train() client id: f_00001-9-3 loss: 0.585004  [  128/  265]
train() client id: f_00001-9-4 loss: 0.391258  [  160/  265]
train() client id: f_00001-9-5 loss: 0.334621  [  192/  265]
train() client id: f_00001-9-6 loss: 0.376092  [  224/  265]
train() client id: f_00001-9-7 loss: 0.393851  [  256/  265]
train() client id: f_00001-10-0 loss: 0.426569  [   32/  265]
train() client id: f_00001-10-1 loss: 0.397091  [   64/  265]
train() client id: f_00001-10-2 loss: 0.388851  [   96/  265]
train() client id: f_00001-10-3 loss: 0.337694  [  128/  265]
train() client id: f_00001-10-4 loss: 0.375200  [  160/  265]
train() client id: f_00001-10-5 loss: 0.507191  [  192/  265]
train() client id: f_00001-10-6 loss: 0.317722  [  224/  265]
train() client id: f_00001-10-7 loss: 0.457033  [  256/  265]
train() client id: f_00002-0-0 loss: 1.011026  [   32/  124]
train() client id: f_00002-0-1 loss: 1.393990  [   64/  124]
train() client id: f_00002-0-2 loss: 1.061155  [   96/  124]
train() client id: f_00002-1-0 loss: 1.312083  [   32/  124]
train() client id: f_00002-1-1 loss: 1.153748  [   64/  124]
train() client id: f_00002-1-2 loss: 1.012821  [   96/  124]
train() client id: f_00002-2-0 loss: 1.107516  [   32/  124]
train() client id: f_00002-2-1 loss: 1.101551  [   64/  124]
train() client id: f_00002-2-2 loss: 1.163144  [   96/  124]
train() client id: f_00002-3-0 loss: 1.118341  [   32/  124]
train() client id: f_00002-3-1 loss: 1.210373  [   64/  124]
train() client id: f_00002-3-2 loss: 0.982091  [   96/  124]
train() client id: f_00002-4-0 loss: 1.221823  [   32/  124]
train() client id: f_00002-4-1 loss: 1.029705  [   64/  124]
train() client id: f_00002-4-2 loss: 1.105116  [   96/  124]
train() client id: f_00002-5-0 loss: 1.132622  [   32/  124]
train() client id: f_00002-5-1 loss: 1.078825  [   64/  124]
train() client id: f_00002-5-2 loss: 1.078756  [   96/  124]
train() client id: f_00002-6-0 loss: 1.233550  [   32/  124]
train() client id: f_00002-6-1 loss: 1.078116  [   64/  124]
train() client id: f_00002-6-2 loss: 0.974374  [   96/  124]
train() client id: f_00002-7-0 loss: 0.979765  [   32/  124]
train() client id: f_00002-7-1 loss: 1.033171  [   64/  124]
train() client id: f_00002-7-2 loss: 1.048825  [   96/  124]
train() client id: f_00002-8-0 loss: 1.036467  [   32/  124]
train() client id: f_00002-8-1 loss: 1.223897  [   64/  124]
train() client id: f_00002-8-2 loss: 1.173786  [   96/  124]
train() client id: f_00002-9-0 loss: 1.146958  [   32/  124]
train() client id: f_00002-9-1 loss: 0.879481  [   64/  124]
train() client id: f_00002-9-2 loss: 1.230729  [   96/  124]
train() client id: f_00002-10-0 loss: 1.058323  [   32/  124]
train() client id: f_00002-10-1 loss: 1.009011  [   64/  124]
train() client id: f_00002-10-2 loss: 1.128433  [   96/  124]
train() client id: f_00003-0-0 loss: 0.570911  [   32/   43]
train() client id: f_00003-1-0 loss: 0.681855  [   32/   43]
train() client id: f_00003-2-0 loss: 0.674156  [   32/   43]
train() client id: f_00003-3-0 loss: 0.726032  [   32/   43]
train() client id: f_00003-4-0 loss: 0.751925  [   32/   43]
train() client id: f_00003-5-0 loss: 0.489394  [   32/   43]
train() client id: f_00003-6-0 loss: 0.673713  [   32/   43]
train() client id: f_00003-7-0 loss: 0.586521  [   32/   43]
train() client id: f_00003-8-0 loss: 0.783125  [   32/   43]
train() client id: f_00003-9-0 loss: 0.633479  [   32/   43]
train() client id: f_00003-10-0 loss: 0.833356  [   32/   43]
train() client id: f_00004-0-0 loss: 1.009324  [   32/  306]
train() client id: f_00004-0-1 loss: 0.900035  [   64/  306]
train() client id: f_00004-0-2 loss: 1.046340  [   96/  306]
train() client id: f_00004-0-3 loss: 0.765097  [  128/  306]
train() client id: f_00004-0-4 loss: 0.886385  [  160/  306]
train() client id: f_00004-0-5 loss: 1.051508  [  192/  306]
train() client id: f_00004-0-6 loss: 0.792717  [  224/  306]
train() client id: f_00004-0-7 loss: 0.846676  [  256/  306]
train() client id: f_00004-0-8 loss: 1.019493  [  288/  306]
train() client id: f_00004-1-0 loss: 0.764012  [   32/  306]
train() client id: f_00004-1-1 loss: 0.924577  [   64/  306]
train() client id: f_00004-1-2 loss: 0.840034  [   96/  306]
train() client id: f_00004-1-3 loss: 0.943864  [  128/  306]
train() client id: f_00004-1-4 loss: 0.928466  [  160/  306]
train() client id: f_00004-1-5 loss: 0.999431  [  192/  306]
train() client id: f_00004-1-6 loss: 0.930333  [  224/  306]
train() client id: f_00004-1-7 loss: 0.941589  [  256/  306]
train() client id: f_00004-1-8 loss: 0.985251  [  288/  306]
train() client id: f_00004-2-0 loss: 0.979970  [   32/  306]
train() client id: f_00004-2-1 loss: 0.978403  [   64/  306]
train() client id: f_00004-2-2 loss: 0.909249  [   96/  306]
train() client id: f_00004-2-3 loss: 1.006255  [  128/  306]
train() client id: f_00004-2-4 loss: 0.896407  [  160/  306]
train() client id: f_00004-2-5 loss: 0.832891  [  192/  306]
train() client id: f_00004-2-6 loss: 0.902005  [  224/  306]
train() client id: f_00004-2-7 loss: 0.819931  [  256/  306]
train() client id: f_00004-2-8 loss: 0.885928  [  288/  306]
train() client id: f_00004-3-0 loss: 0.840023  [   32/  306]
train() client id: f_00004-3-1 loss: 0.834229  [   64/  306]
train() client id: f_00004-3-2 loss: 1.017004  [   96/  306]
train() client id: f_00004-3-3 loss: 0.910544  [  128/  306]
train() client id: f_00004-3-4 loss: 0.896564  [  160/  306]
train() client id: f_00004-3-5 loss: 1.032701  [  192/  306]
train() client id: f_00004-3-6 loss: 0.799781  [  224/  306]
train() client id: f_00004-3-7 loss: 0.840343  [  256/  306]
train() client id: f_00004-3-8 loss: 0.830112  [  288/  306]
train() client id: f_00004-4-0 loss: 0.788436  [   32/  306]
train() client id: f_00004-4-1 loss: 0.992832  [   64/  306]
train() client id: f_00004-4-2 loss: 0.790361  [   96/  306]
train() client id: f_00004-4-3 loss: 0.880797  [  128/  306]
train() client id: f_00004-4-4 loss: 0.919932  [  160/  306]
train() client id: f_00004-4-5 loss: 0.991474  [  192/  306]
train() client id: f_00004-4-6 loss: 0.857870  [  224/  306]
train() client id: f_00004-4-7 loss: 0.825309  [  256/  306]
train() client id: f_00004-4-8 loss: 1.125625  [  288/  306]
train() client id: f_00004-5-0 loss: 0.862055  [   32/  306]
train() client id: f_00004-5-1 loss: 0.893628  [   64/  306]
train() client id: f_00004-5-2 loss: 1.001864  [   96/  306]
train() client id: f_00004-5-3 loss: 0.866002  [  128/  306]
train() client id: f_00004-5-4 loss: 0.986365  [  160/  306]
train() client id: f_00004-5-5 loss: 0.835163  [  192/  306]
train() client id: f_00004-5-6 loss: 0.845321  [  224/  306]
train() client id: f_00004-5-7 loss: 0.909375  [  256/  306]
train() client id: f_00004-5-8 loss: 0.922779  [  288/  306]
train() client id: f_00004-6-0 loss: 0.855357  [   32/  306]
train() client id: f_00004-6-1 loss: 0.886442  [   64/  306]
train() client id: f_00004-6-2 loss: 0.887029  [   96/  306]
train() client id: f_00004-6-3 loss: 0.799474  [  128/  306]
train() client id: f_00004-6-4 loss: 0.891570  [  160/  306]
train() client id: f_00004-6-5 loss: 0.903432  [  192/  306]
train() client id: f_00004-6-6 loss: 0.878596  [  224/  306]
train() client id: f_00004-6-7 loss: 0.982694  [  256/  306]
train() client id: f_00004-6-8 loss: 0.992605  [  288/  306]
train() client id: f_00004-7-0 loss: 0.829782  [   32/  306]
train() client id: f_00004-7-1 loss: 0.913722  [   64/  306]
train() client id: f_00004-7-2 loss: 0.681061  [   96/  306]
train() client id: f_00004-7-3 loss: 0.880090  [  128/  306]
train() client id: f_00004-7-4 loss: 0.965078  [  160/  306]
train() client id: f_00004-7-5 loss: 0.848446  [  192/  306]
train() client id: f_00004-7-6 loss: 0.876411  [  224/  306]
train() client id: f_00004-7-7 loss: 1.001345  [  256/  306]
train() client id: f_00004-7-8 loss: 1.042303  [  288/  306]
train() client id: f_00004-8-0 loss: 0.885710  [   32/  306]
train() client id: f_00004-8-1 loss: 0.904159  [   64/  306]
train() client id: f_00004-8-2 loss: 0.847579  [   96/  306]
train() client id: f_00004-8-3 loss: 0.904585  [  128/  306]
train() client id: f_00004-8-4 loss: 0.972691  [  160/  306]
train() client id: f_00004-8-5 loss: 0.847522  [  192/  306]
train() client id: f_00004-8-6 loss: 0.889586  [  224/  306]
train() client id: f_00004-8-7 loss: 0.851994  [  256/  306]
train() client id: f_00004-8-8 loss: 0.915666  [  288/  306]
train() client id: f_00004-9-0 loss: 0.840485  [   32/  306]
train() client id: f_00004-9-1 loss: 1.028747  [   64/  306]
train() client id: f_00004-9-2 loss: 0.806313  [   96/  306]
train() client id: f_00004-9-3 loss: 0.850889  [  128/  306]
train() client id: f_00004-9-4 loss: 1.037056  [  160/  306]
train() client id: f_00004-9-5 loss: 0.810124  [  192/  306]
train() client id: f_00004-9-6 loss: 0.818089  [  224/  306]
train() client id: f_00004-9-7 loss: 0.895655  [  256/  306]
train() client id: f_00004-9-8 loss: 0.921636  [  288/  306]
train() client id: f_00004-10-0 loss: 0.837135  [   32/  306]
train() client id: f_00004-10-1 loss: 0.799720  [   64/  306]
train() client id: f_00004-10-2 loss: 0.833668  [   96/  306]
train() client id: f_00004-10-3 loss: 0.982716  [  128/  306]
train() client id: f_00004-10-4 loss: 0.901794  [  160/  306]
train() client id: f_00004-10-5 loss: 0.727530  [  192/  306]
train() client id: f_00004-10-6 loss: 0.981083  [  224/  306]
train() client id: f_00004-10-7 loss: 1.079719  [  256/  306]
train() client id: f_00004-10-8 loss: 0.801619  [  288/  306]
train() client id: f_00005-0-0 loss: 0.684358  [   32/  146]
train() client id: f_00005-0-1 loss: 0.742901  [   64/  146]
train() client id: f_00005-0-2 loss: 0.388454  [   96/  146]
train() client id: f_00005-0-3 loss: 0.520574  [  128/  146]
train() client id: f_00005-1-0 loss: 0.897167  [   32/  146]
train() client id: f_00005-1-1 loss: 0.564540  [   64/  146]
train() client id: f_00005-1-2 loss: 0.633415  [   96/  146]
train() client id: f_00005-1-3 loss: 0.344538  [  128/  146]
train() client id: f_00005-2-0 loss: 0.753757  [   32/  146]
train() client id: f_00005-2-1 loss: 0.582796  [   64/  146]
train() client id: f_00005-2-2 loss: 0.534928  [   96/  146]
train() client id: f_00005-2-3 loss: 0.542112  [  128/  146]
train() client id: f_00005-3-0 loss: 0.724061  [   32/  146]
train() client id: f_00005-3-1 loss: 0.671830  [   64/  146]
train() client id: f_00005-3-2 loss: 0.722193  [   96/  146]
train() client id: f_00005-3-3 loss: 0.443114  [  128/  146]
train() client id: f_00005-4-0 loss: 0.726803  [   32/  146]
train() client id: f_00005-4-1 loss: 0.755399  [   64/  146]
train() client id: f_00005-4-2 loss: 0.347370  [   96/  146]
train() client id: f_00005-4-3 loss: 0.752286  [  128/  146]
train() client id: f_00005-5-0 loss: 0.693982  [   32/  146]
train() client id: f_00005-5-1 loss: 0.503166  [   64/  146]
train() client id: f_00005-5-2 loss: 0.645927  [   96/  146]
train() client id: f_00005-5-3 loss: 0.821287  [  128/  146]
train() client id: f_00005-6-0 loss: 0.628401  [   32/  146]
train() client id: f_00005-6-1 loss: 0.551552  [   64/  146]
train() client id: f_00005-6-2 loss: 0.531084  [   96/  146]
train() client id: f_00005-6-3 loss: 0.740391  [  128/  146]
train() client id: f_00005-7-0 loss: 0.680104  [   32/  146]
train() client id: f_00005-7-1 loss: 0.522367  [   64/  146]
train() client id: f_00005-7-2 loss: 0.667735  [   96/  146]
train() client id: f_00005-7-3 loss: 0.660204  [  128/  146]
train() client id: f_00005-8-0 loss: 0.568381  [   32/  146]
train() client id: f_00005-8-1 loss: 0.357558  [   64/  146]
train() client id: f_00005-8-2 loss: 0.791128  [   96/  146]
train() client id: f_00005-8-3 loss: 0.767687  [  128/  146]
train() client id: f_00005-9-0 loss: 0.523387  [   32/  146]
train() client id: f_00005-9-1 loss: 0.594624  [   64/  146]
train() client id: f_00005-9-2 loss: 0.567308  [   96/  146]
train() client id: f_00005-9-3 loss: 0.700250  [  128/  146]
train() client id: f_00005-10-0 loss: 0.794059  [   32/  146]
train() client id: f_00005-10-1 loss: 0.760687  [   64/  146]
train() client id: f_00005-10-2 loss: 0.411825  [   96/  146]
train() client id: f_00005-10-3 loss: 0.551398  [  128/  146]
train() client id: f_00006-0-0 loss: 0.569730  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505926  [   32/   54]
train() client id: f_00006-2-0 loss: 0.500944  [   32/   54]
train() client id: f_00006-3-0 loss: 0.522875  [   32/   54]
train() client id: f_00006-4-0 loss: 0.572036  [   32/   54]
train() client id: f_00006-5-0 loss: 0.588280  [   32/   54]
train() client id: f_00006-6-0 loss: 0.580223  [   32/   54]
train() client id: f_00006-7-0 loss: 0.492995  [   32/   54]
train() client id: f_00006-8-0 loss: 0.578250  [   32/   54]
train() client id: f_00006-9-0 loss: 0.494971  [   32/   54]
train() client id: f_00006-10-0 loss: 0.567748  [   32/   54]
train() client id: f_00007-0-0 loss: 0.577416  [   32/  179]
train() client id: f_00007-0-1 loss: 0.651839  [   64/  179]
train() client id: f_00007-0-2 loss: 0.623158  [   96/  179]
train() client id: f_00007-0-3 loss: 0.561461  [  128/  179]
train() client id: f_00007-0-4 loss: 0.560488  [  160/  179]
train() client id: f_00007-1-0 loss: 0.645612  [   32/  179]
train() client id: f_00007-1-1 loss: 0.552687  [   64/  179]
train() client id: f_00007-1-2 loss: 0.541455  [   96/  179]
train() client id: f_00007-1-3 loss: 0.538130  [  128/  179]
train() client id: f_00007-1-4 loss: 0.522699  [  160/  179]
train() client id: f_00007-2-0 loss: 0.502777  [   32/  179]
train() client id: f_00007-2-1 loss: 0.602864  [   64/  179]
train() client id: f_00007-2-2 loss: 0.645682  [   96/  179]
train() client id: f_00007-2-3 loss: 0.445929  [  128/  179]
train() client id: f_00007-2-4 loss: 0.521583  [  160/  179]
train() client id: f_00007-3-0 loss: 0.432968  [   32/  179]
train() client id: f_00007-3-1 loss: 0.584618  [   64/  179]
train() client id: f_00007-3-2 loss: 0.530771  [   96/  179]
train() client id: f_00007-3-3 loss: 0.558282  [  128/  179]
train() client id: f_00007-3-4 loss: 0.629931  [  160/  179]
train() client id: f_00007-4-0 loss: 0.623980  [   32/  179]
train() client id: f_00007-4-1 loss: 0.369331  [   64/  179]
train() client id: f_00007-4-2 loss: 0.463914  [   96/  179]
train() client id: f_00007-4-3 loss: 0.491617  [  128/  179]
train() client id: f_00007-4-4 loss: 0.702654  [  160/  179]
train() client id: f_00007-5-0 loss: 0.738554  [   32/  179]
train() client id: f_00007-5-1 loss: 0.520290  [   64/  179]
train() client id: f_00007-5-2 loss: 0.610421  [   96/  179]
train() client id: f_00007-5-3 loss: 0.428426  [  128/  179]
train() client id: f_00007-5-4 loss: 0.364563  [  160/  179]
train() client id: f_00007-6-0 loss: 0.779039  [   32/  179]
train() client id: f_00007-6-1 loss: 0.364160  [   64/  179]
train() client id: f_00007-6-2 loss: 0.472260  [   96/  179]
train() client id: f_00007-6-3 loss: 0.394065  [  128/  179]
train() client id: f_00007-6-4 loss: 0.474510  [  160/  179]
train() client id: f_00007-7-0 loss: 0.467654  [   32/  179]
train() client id: f_00007-7-1 loss: 0.570750  [   64/  179]
train() client id: f_00007-7-2 loss: 0.588379  [   96/  179]
train() client id: f_00007-7-3 loss: 0.683447  [  128/  179]
train() client id: f_00007-7-4 loss: 0.342755  [  160/  179]
train() client id: f_00007-8-0 loss: 0.359943  [   32/  179]
train() client id: f_00007-8-1 loss: 0.842173  [   64/  179]
train() client id: f_00007-8-2 loss: 0.626740  [   96/  179]
train() client id: f_00007-8-3 loss: 0.412252  [  128/  179]
train() client id: f_00007-8-4 loss: 0.391147  [  160/  179]
train() client id: f_00007-9-0 loss: 0.502460  [   32/  179]
train() client id: f_00007-9-1 loss: 0.316666  [   64/  179]
train() client id: f_00007-9-2 loss: 0.583607  [   96/  179]
train() client id: f_00007-9-3 loss: 0.379769  [  128/  179]
train() client id: f_00007-9-4 loss: 0.730371  [  160/  179]
train() client id: f_00007-10-0 loss: 0.498273  [   32/  179]
train() client id: f_00007-10-1 loss: 0.383434  [   64/  179]
train() client id: f_00007-10-2 loss: 0.418960  [   96/  179]
train() client id: f_00007-10-3 loss: 0.387296  [  128/  179]
train() client id: f_00007-10-4 loss: 0.732498  [  160/  179]
train() client id: f_00008-0-0 loss: 0.742937  [   32/  130]
train() client id: f_00008-0-1 loss: 0.767156  [   64/  130]
train() client id: f_00008-0-2 loss: 0.516388  [   96/  130]
train() client id: f_00008-0-3 loss: 0.722479  [  128/  130]
train() client id: f_00008-1-0 loss: 0.651655  [   32/  130]
train() client id: f_00008-1-1 loss: 0.729915  [   64/  130]
train() client id: f_00008-1-2 loss: 0.693025  [   96/  130]
train() client id: f_00008-1-3 loss: 0.663058  [  128/  130]
train() client id: f_00008-2-0 loss: 0.667308  [   32/  130]
train() client id: f_00008-2-1 loss: 0.637449  [   64/  130]
train() client id: f_00008-2-2 loss: 0.704224  [   96/  130]
train() client id: f_00008-2-3 loss: 0.739413  [  128/  130]
train() client id: f_00008-3-0 loss: 0.736731  [   32/  130]
train() client id: f_00008-3-1 loss: 0.702535  [   64/  130]
train() client id: f_00008-3-2 loss: 0.631213  [   96/  130]
train() client id: f_00008-3-3 loss: 0.683397  [  128/  130]
train() client id: f_00008-4-0 loss: 0.673356  [   32/  130]
train() client id: f_00008-4-1 loss: 0.770845  [   64/  130]
train() client id: f_00008-4-2 loss: 0.662478  [   96/  130]
train() client id: f_00008-4-3 loss: 0.638963  [  128/  130]
train() client id: f_00008-5-0 loss: 0.657630  [   32/  130]
train() client id: f_00008-5-1 loss: 0.793408  [   64/  130]
train() client id: f_00008-5-2 loss: 0.662326  [   96/  130]
train() client id: f_00008-5-3 loss: 0.634719  [  128/  130]
train() client id: f_00008-6-0 loss: 0.722341  [   32/  130]
train() client id: f_00008-6-1 loss: 0.570615  [   64/  130]
train() client id: f_00008-6-2 loss: 0.723832  [   96/  130]
train() client id: f_00008-6-3 loss: 0.730911  [  128/  130]
train() client id: f_00008-7-0 loss: 0.663967  [   32/  130]
train() client id: f_00008-7-1 loss: 0.650593  [   64/  130]
train() client id: f_00008-7-2 loss: 0.680030  [   96/  130]
train() client id: f_00008-7-3 loss: 0.695016  [  128/  130]
train() client id: f_00008-8-0 loss: 0.689982  [   32/  130]
train() client id: f_00008-8-1 loss: 0.729782  [   64/  130]
train() client id: f_00008-8-2 loss: 0.653889  [   96/  130]
train() client id: f_00008-8-3 loss: 0.675326  [  128/  130]
train() client id: f_00008-9-0 loss: 0.651322  [   32/  130]
train() client id: f_00008-9-1 loss: 0.702428  [   64/  130]
train() client id: f_00008-9-2 loss: 0.692411  [   96/  130]
train() client id: f_00008-9-3 loss: 0.703851  [  128/  130]
train() client id: f_00008-10-0 loss: 0.738391  [   32/  130]
train() client id: f_00008-10-1 loss: 0.609171  [   64/  130]
train() client id: f_00008-10-2 loss: 0.725967  [   96/  130]
train() client id: f_00008-10-3 loss: 0.677974  [  128/  130]
train() client id: f_00009-0-0 loss: 1.090647  [   32/  118]
train() client id: f_00009-0-1 loss: 1.176071  [   64/  118]
train() client id: f_00009-0-2 loss: 1.048365  [   96/  118]
train() client id: f_00009-1-0 loss: 1.178534  [   32/  118]
train() client id: f_00009-1-1 loss: 1.093962  [   64/  118]
train() client id: f_00009-1-2 loss: 0.994532  [   96/  118]
train() client id: f_00009-2-0 loss: 1.050478  [   32/  118]
train() client id: f_00009-2-1 loss: 0.998430  [   64/  118]
train() client id: f_00009-2-2 loss: 1.080797  [   96/  118]
train() client id: f_00009-3-0 loss: 1.132195  [   32/  118]
train() client id: f_00009-3-1 loss: 0.991060  [   64/  118]
train() client id: f_00009-3-2 loss: 0.945305  [   96/  118]
train() client id: f_00009-4-0 loss: 0.990284  [   32/  118]
train() client id: f_00009-4-1 loss: 0.991837  [   64/  118]
train() client id: f_00009-4-2 loss: 0.947188  [   96/  118]
train() client id: f_00009-5-0 loss: 0.992978  [   32/  118]
train() client id: f_00009-5-1 loss: 0.852091  [   64/  118]
train() client id: f_00009-5-2 loss: 0.846550  [   96/  118]
train() client id: f_00009-6-0 loss: 0.809746  [   32/  118]
train() client id: f_00009-6-1 loss: 1.080472  [   64/  118]
train() client id: f_00009-6-2 loss: 0.873346  [   96/  118]
train() client id: f_00009-7-0 loss: 1.055080  [   32/  118]
train() client id: f_00009-7-1 loss: 0.942616  [   64/  118]
train() client id: f_00009-7-2 loss: 0.838756  [   96/  118]
train() client id: f_00009-8-0 loss: 0.904074  [   32/  118]
train() client id: f_00009-8-1 loss: 0.993179  [   64/  118]
train() client id: f_00009-8-2 loss: 0.988817  [   96/  118]
train() client id: f_00009-9-0 loss: 0.919570  [   32/  118]
train() client id: f_00009-9-1 loss: 1.080130  [   64/  118]
train() client id: f_00009-9-2 loss: 0.808410  [   96/  118]
train() client id: f_00009-10-0 loss: 0.961114  [   32/  118]
train() client id: f_00009-10-1 loss: 0.937198  [   64/  118]
train() client id: f_00009-10-2 loss: 0.887736  [   96/  118]
At round 52 accuracy: 0.6472148541114059
At round 52 training accuracy: 0.5888665325285044
At round 52 training loss: 0.8400544173559127
update_location
xs = [  -3.9056584     4.20031788  280.00902392   18.81129433    0.97929623
    3.95640986 -242.44319194 -221.32485185  264.66397685 -207.06087855]
ys = [ 272.5879595   255.55583871    1.32061395 -242.45517586  234.35018685
  217.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [290.37811527 274.45660744 297.33280595 262.94177511 254.79593618
 239.70535547 262.27007427 242.86903117 283.47079323 229.97873659]
dists_bs = [196.96873753 196.4739094  487.0295073  460.25171554 185.71795115
 184.00665242 189.99794523 180.23275607 466.99992549 173.97813991]
uav_gains = [3.23107212e-12 4.49127941e-12 2.81429478e-12 5.71301961e-12
 6.74655076e-12 9.01929979e-12 5.79291830e-12 8.50621989e-12
 3.72132531e-12 1.07099613e-11]
bs_gains = [4.15893030e-11 4.18832527e-11 3.29715465e-12 3.86284178e-12
 4.90346648e-11 5.03222692e-11 4.60041651e-11 5.33285356e-11
 3.70857455e-12 5.88720247e-11]
Round 53
-------------------------------
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.00899084 8.17097489 3.95724982 1.44293614 9.42099912 4.53288609
 1.77835404 5.58181324 4.12270253 3.67526442]
obj_prev = 46.692171130454035
eta_min = 9.633908147429921e-24	eta_max = 0.9382560279743308
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 10.770764989587082	eta = 0.9090909090909091
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 22.73132098521351	eta = 0.4307538722609915
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 16.477782321424517	eta = 0.5942307250446651
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.352697592199675	eta = 0.6377774640049659
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286662235887265	eta = 0.6405325364618376
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286409541549396	eta = 0.6405431248831895
eta = 0.6405431248831895
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.03675654 0.07730543 0.03617311 0.0125439  0.08926589 0.04259092
 0.0157528  0.05221763 0.03792342 0.0344228 ]
ene_total = [1.46579906 2.36500809 1.47528965 0.70621205 2.69190422 1.38875393
 0.79404133 1.76977105 1.47499189 1.15463827]
ti_comp = [0.82252146 0.90567545 0.81328504 0.85268029 0.90809929 0.90848315
 0.85328562 0.86816903 0.8308949  0.91072419]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [4.58764259e-06 3.52017978e-05 4.47251598e-06 1.69670107e-07
 5.39100853e-05 5.85055712e-06 3.35555868e-07 1.18065477e-05
 4.93753617e-06 3.07358121e-06]
ene_total = [0.44053948 0.20942103 0.46630285 0.35628294 0.2031812  0.20076965
 0.35459888 0.31339904 0.41719008 0.19444042]
optimize_network iter = 0 obj = 3.1561255646288915
eta = 0.6405431248831895
freqs = [22343821.0822694  42678329.73940749 22238886.46292163  7355570.59611931
 49149853.28596357 23440674.99681275  9230675.16871088 30073423.9891545
 22820828.02431027 18898584.29975834]
eta_min = 0.6405431248831939	eta_max = 0.6991595698398908
af = 0.002799252834533405	bf = 1.1332441370306174	zeta = 0.0030791781179867455	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.81614110e-07 7.53209970e-06 9.56980564e-07 3.63041731e-08
 1.15350966e-05 1.25183889e-06 7.17986128e-08 2.52623730e-06
 1.05648055e-06 6.57651642e-07]
ene_total = [1.74150662 0.82500401 1.84338565 1.40873715 0.79870952 0.79334107
 1.40206398 1.23816416 1.64915218 0.76855593]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 1 obj = 3.770117456851034
eta = 0.6991595698398908
freqs = [22282599.06518497 41639030.99946182 22238886.46292161  7273350.56555898
 47925539.38367743 22854708.69293805  9126001.3904321  29615341.56378339
 22703099.87437893 18416482.47146711]
eta_min = 0.6991595698399091	eta_max = 0.6991595698398846
af = 0.002681430697799304	bf = 1.1332441370306174	zeta = 0.0029495737675792346	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.76242239e-07 7.16972436e-06 9.56980564e-07 3.54970983e-08
 1.09675798e-05 1.19003461e-06 7.01794852e-08 2.44986346e-06
 1.04560832e-06 6.24526300e-07]
ene_total = [1.74150603 0.82496404 1.84338565 1.40873707 0.79864692 0.79333425
 1.4020638  1.23815573 1.64915098 0.76855228]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 2 obj = 3.770117456850956
eta = 0.6991595698398846
freqs = [22282599.06518495 41639030.99946193 22238886.46292159  7273350.56555898
 47925539.38367755 22854708.6929381   9126001.39043211 29615341.56378343
 22703099.87437893 18416482.47146716]
Done!
At round 53 energy consumption: 3.770117456851034
At round 53 eta: 0.6991595698398846
At round 53 local rounds: 11.718687629159392
At round 53 global rounds: 33.33219860622975
At round 53 a_n: 9.685127119911243
gradient difference: 0.5055280923843384
train() client id: f_00000-0-0 loss: 0.993189  [   32/  126]
train() client id: f_00000-0-1 loss: 1.288794  [   64/  126]
train() client id: f_00000-0-2 loss: 0.901794  [   96/  126]
train() client id: f_00000-1-0 loss: 1.050690  [   32/  126]
train() client id: f_00000-1-1 loss: 0.918113  [   64/  126]
train() client id: f_00000-1-2 loss: 0.979315  [   96/  126]
train() client id: f_00000-2-0 loss: 0.893092  [   32/  126]
train() client id: f_00000-2-1 loss: 0.939675  [   64/  126]
train() client id: f_00000-2-2 loss: 0.885842  [   96/  126]
train() client id: f_00000-3-0 loss: 1.000946  [   32/  126]
train() client id: f_00000-3-1 loss: 0.680957  [   64/  126]
train() client id: f_00000-3-2 loss: 0.916338  [   96/  126]
train() client id: f_00000-4-0 loss: 0.821535  [   32/  126]
train() client id: f_00000-4-1 loss: 0.868349  [   64/  126]
train() client id: f_00000-4-2 loss: 0.929920  [   96/  126]
train() client id: f_00000-5-0 loss: 0.771072  [   32/  126]
train() client id: f_00000-5-1 loss: 0.840234  [   64/  126]
train() client id: f_00000-5-2 loss: 0.849495  [   96/  126]
train() client id: f_00000-6-0 loss: 0.819165  [   32/  126]
train() client id: f_00000-6-1 loss: 0.798230  [   64/  126]
train() client id: f_00000-6-2 loss: 0.849882  [   96/  126]
train() client id: f_00000-7-0 loss: 0.772583  [   32/  126]
train() client id: f_00000-7-1 loss: 0.770884  [   64/  126]
train() client id: f_00000-7-2 loss: 0.840176  [   96/  126]
train() client id: f_00000-8-0 loss: 0.781790  [   32/  126]
train() client id: f_00000-8-1 loss: 0.803947  [   64/  126]
train() client id: f_00000-8-2 loss: 0.935449  [   96/  126]
train() client id: f_00000-9-0 loss: 0.704140  [   32/  126]
train() client id: f_00000-9-1 loss: 0.934480  [   64/  126]
train() client id: f_00000-9-2 loss: 0.876404  [   96/  126]
train() client id: f_00000-10-0 loss: 0.723074  [   32/  126]
train() client id: f_00000-10-1 loss: 0.822516  [   64/  126]
train() client id: f_00000-10-2 loss: 0.893865  [   96/  126]
train() client id: f_00001-0-0 loss: 0.344651  [   32/  265]
train() client id: f_00001-0-1 loss: 0.398390  [   64/  265]
train() client id: f_00001-0-2 loss: 0.382711  [   96/  265]
train() client id: f_00001-0-3 loss: 0.466646  [  128/  265]
train() client id: f_00001-0-4 loss: 0.536173  [  160/  265]
train() client id: f_00001-0-5 loss: 0.350732  [  192/  265]
train() client id: f_00001-0-6 loss: 0.532452  [  224/  265]
train() client id: f_00001-0-7 loss: 0.357164  [  256/  265]
train() client id: f_00001-1-0 loss: 0.403350  [   32/  265]
train() client id: f_00001-1-1 loss: 0.540161  [   64/  265]
train() client id: f_00001-1-2 loss: 0.353448  [   96/  265]
train() client id: f_00001-1-3 loss: 0.446577  [  128/  265]
train() client id: f_00001-1-4 loss: 0.457484  [  160/  265]
train() client id: f_00001-1-5 loss: 0.337392  [  192/  265]
train() client id: f_00001-1-6 loss: 0.306700  [  224/  265]
train() client id: f_00001-1-7 loss: 0.510091  [  256/  265]
train() client id: f_00001-2-0 loss: 0.376393  [   32/  265]
train() client id: f_00001-2-1 loss: 0.323481  [   64/  265]
train() client id: f_00001-2-2 loss: 0.414335  [   96/  265]
train() client id: f_00001-2-3 loss: 0.370602  [  128/  265]
train() client id: f_00001-2-4 loss: 0.304115  [  160/  265]
train() client id: f_00001-2-5 loss: 0.516248  [  192/  265]
train() client id: f_00001-2-6 loss: 0.487344  [  224/  265]
train() client id: f_00001-2-7 loss: 0.503290  [  256/  265]
train() client id: f_00001-3-0 loss: 0.516096  [   32/  265]
train() client id: f_00001-3-1 loss: 0.354663  [   64/  265]
train() client id: f_00001-3-2 loss: 0.323296  [   96/  265]
train() client id: f_00001-3-3 loss: 0.373720  [  128/  265]
train() client id: f_00001-3-4 loss: 0.438946  [  160/  265]
train() client id: f_00001-3-5 loss: 0.423785  [  192/  265]
train() client id: f_00001-3-6 loss: 0.398574  [  224/  265]
train() client id: f_00001-3-7 loss: 0.420123  [  256/  265]
train() client id: f_00001-4-0 loss: 0.529929  [   32/  265]
train() client id: f_00001-4-1 loss: 0.472983  [   64/  265]
train() client id: f_00001-4-2 loss: 0.291060  [   96/  265]
train() client id: f_00001-4-3 loss: 0.322581  [  128/  265]
train() client id: f_00001-4-4 loss: 0.328107  [  160/  265]
train() client id: f_00001-4-5 loss: 0.364045  [  192/  265]
train() client id: f_00001-4-6 loss: 0.449584  [  224/  265]
train() client id: f_00001-4-7 loss: 0.405087  [  256/  265]
train() client id: f_00001-5-0 loss: 0.528527  [   32/  265]
train() client id: f_00001-5-1 loss: 0.438517  [   64/  265]
train() client id: f_00001-5-2 loss: 0.404660  [   96/  265]
train() client id: f_00001-5-3 loss: 0.386638  [  128/  265]
train() client id: f_00001-5-4 loss: 0.292050  [  160/  265]
train() client id: f_00001-5-5 loss: 0.316572  [  192/  265]
train() client id: f_00001-5-6 loss: 0.288332  [  224/  265]
train() client id: f_00001-5-7 loss: 0.424783  [  256/  265]
train() client id: f_00001-6-0 loss: 0.566968  [   32/  265]
train() client id: f_00001-6-1 loss: 0.347307  [   64/  265]
train() client id: f_00001-6-2 loss: 0.448954  [   96/  265]
train() client id: f_00001-6-3 loss: 0.358202  [  128/  265]
train() client id: f_00001-6-4 loss: 0.385083  [  160/  265]
train() client id: f_00001-6-5 loss: 0.404004  [  192/  265]
train() client id: f_00001-6-6 loss: 0.301377  [  224/  265]
train() client id: f_00001-6-7 loss: 0.374673  [  256/  265]
train() client id: f_00001-7-0 loss: 0.327333  [   32/  265]
train() client id: f_00001-7-1 loss: 0.388988  [   64/  265]
train() client id: f_00001-7-2 loss: 0.415891  [   96/  265]
train() client id: f_00001-7-3 loss: 0.377054  [  128/  265]
train() client id: f_00001-7-4 loss: 0.360949  [  160/  265]
train() client id: f_00001-7-5 loss: 0.478931  [  192/  265]
train() client id: f_00001-7-6 loss: 0.401559  [  224/  265]
train() client id: f_00001-7-7 loss: 0.442725  [  256/  265]
train() client id: f_00001-8-0 loss: 0.300311  [   32/  265]
train() client id: f_00001-8-1 loss: 0.393072  [   64/  265]
train() client id: f_00001-8-2 loss: 0.476031  [   96/  265]
train() client id: f_00001-8-3 loss: 0.321117  [  128/  265]
train() client id: f_00001-8-4 loss: 0.483338  [  160/  265]
train() client id: f_00001-8-5 loss: 0.330133  [  192/  265]
train() client id: f_00001-8-6 loss: 0.401846  [  224/  265]
train() client id: f_00001-8-7 loss: 0.403975  [  256/  265]
train() client id: f_00001-9-0 loss: 0.553259  [   32/  265]
train() client id: f_00001-9-1 loss: 0.308989  [   64/  265]
train() client id: f_00001-9-2 loss: 0.429288  [   96/  265]
train() client id: f_00001-9-3 loss: 0.301943  [  128/  265]
train() client id: f_00001-9-4 loss: 0.353526  [  160/  265]
train() client id: f_00001-9-5 loss: 0.356521  [  192/  265]
train() client id: f_00001-9-6 loss: 0.375587  [  224/  265]
train() client id: f_00001-9-7 loss: 0.478278  [  256/  265]
train() client id: f_00001-10-0 loss: 0.486050  [   32/  265]
train() client id: f_00001-10-1 loss: 0.439450  [   64/  265]
train() client id: f_00001-10-2 loss: 0.438143  [   96/  265]
train() client id: f_00001-10-3 loss: 0.292969  [  128/  265]
train() client id: f_00001-10-4 loss: 0.415721  [  160/  265]
train() client id: f_00001-10-5 loss: 0.332121  [  192/  265]
train() client id: f_00001-10-6 loss: 0.420655  [  224/  265]
train() client id: f_00001-10-7 loss: 0.343543  [  256/  265]
train() client id: f_00002-0-0 loss: 0.998599  [   32/  124]
train() client id: f_00002-0-1 loss: 1.228435  [   64/  124]
train() client id: f_00002-0-2 loss: 1.044059  [   96/  124]
train() client id: f_00002-1-0 loss: 1.130871  [   32/  124]
train() client id: f_00002-1-1 loss: 1.216758  [   64/  124]
train() client id: f_00002-1-2 loss: 0.982518  [   96/  124]
train() client id: f_00002-2-0 loss: 1.140136  [   32/  124]
train() client id: f_00002-2-1 loss: 0.969080  [   64/  124]
train() client id: f_00002-2-2 loss: 0.887724  [   96/  124]
train() client id: f_00002-3-0 loss: 0.927745  [   32/  124]
train() client id: f_00002-3-1 loss: 1.083139  [   64/  124]
train() client id: f_00002-3-2 loss: 1.083929  [   96/  124]
train() client id: f_00002-4-0 loss: 1.053815  [   32/  124]
train() client id: f_00002-4-1 loss: 1.083807  [   64/  124]
train() client id: f_00002-4-2 loss: 0.845665  [   96/  124]
train() client id: f_00002-5-0 loss: 0.990356  [   32/  124]
train() client id: f_00002-5-1 loss: 0.891493  [   64/  124]
train() client id: f_00002-5-2 loss: 0.918915  [   96/  124]
train() client id: f_00002-6-0 loss: 0.911700  [   32/  124]
train() client id: f_00002-6-1 loss: 1.135135  [   64/  124]
train() client id: f_00002-6-2 loss: 0.893642  [   96/  124]
train() client id: f_00002-7-0 loss: 0.964798  [   32/  124]
train() client id: f_00002-7-1 loss: 1.041471  [   64/  124]
train() client id: f_00002-7-2 loss: 0.838637  [   96/  124]
train() client id: f_00002-8-0 loss: 1.020736  [   32/  124]
train() client id: f_00002-8-1 loss: 0.834325  [   64/  124]
train() client id: f_00002-8-2 loss: 0.897062  [   96/  124]
train() client id: f_00002-9-0 loss: 1.002266  [   32/  124]
train() client id: f_00002-9-1 loss: 0.870414  [   64/  124]
train() client id: f_00002-9-2 loss: 0.771046  [   96/  124]
train() client id: f_00002-10-0 loss: 1.001547  [   32/  124]
train() client id: f_00002-10-1 loss: 0.974401  [   64/  124]
train() client id: f_00002-10-2 loss: 0.842147  [   96/  124]
train() client id: f_00003-0-0 loss: 0.441864  [   32/   43]
train() client id: f_00003-1-0 loss: 0.550582  [   32/   43]
train() client id: f_00003-2-0 loss: 0.352568  [   32/   43]
train() client id: f_00003-3-0 loss: 0.399616  [   32/   43]
train() client id: f_00003-4-0 loss: 0.579010  [   32/   43]
train() client id: f_00003-5-0 loss: 0.509899  [   32/   43]
train() client id: f_00003-6-0 loss: 0.704042  [   32/   43]
train() client id: f_00003-7-0 loss: 0.421168  [   32/   43]
train() client id: f_00003-8-0 loss: 0.273668  [   32/   43]
train() client id: f_00003-9-0 loss: 0.402053  [   32/   43]
train() client id: f_00003-10-0 loss: 0.429124  [   32/   43]
train() client id: f_00004-0-0 loss: 0.834194  [   32/  306]
train() client id: f_00004-0-1 loss: 0.867224  [   64/  306]
train() client id: f_00004-0-2 loss: 0.856220  [   96/  306]
train() client id: f_00004-0-3 loss: 0.785056  [  128/  306]
train() client id: f_00004-0-4 loss: 0.897723  [  160/  306]
train() client id: f_00004-0-5 loss: 0.870888  [  192/  306]
train() client id: f_00004-0-6 loss: 0.939019  [  224/  306]
train() client id: f_00004-0-7 loss: 0.878907  [  256/  306]
train() client id: f_00004-0-8 loss: 0.933132  [  288/  306]
train() client id: f_00004-1-0 loss: 0.838181  [   32/  306]
train() client id: f_00004-1-1 loss: 0.823748  [   64/  306]
train() client id: f_00004-1-2 loss: 0.899577  [   96/  306]
train() client id: f_00004-1-3 loss: 0.911294  [  128/  306]
train() client id: f_00004-1-4 loss: 0.890998  [  160/  306]
train() client id: f_00004-1-5 loss: 0.962401  [  192/  306]
train() client id: f_00004-1-6 loss: 0.810101  [  224/  306]
train() client id: f_00004-1-7 loss: 0.804415  [  256/  306]
train() client id: f_00004-1-8 loss: 0.806577  [  288/  306]
train() client id: f_00004-2-0 loss: 0.822058  [   32/  306]
train() client id: f_00004-2-1 loss: 0.823554  [   64/  306]
train() client id: f_00004-2-2 loss: 0.904138  [   96/  306]
train() client id: f_00004-2-3 loss: 0.974340  [  128/  306]
train() client id: f_00004-2-4 loss: 0.983112  [  160/  306]
train() client id: f_00004-2-5 loss: 0.794852  [  192/  306]
train() client id: f_00004-2-6 loss: 0.817534  [  224/  306]
train() client id: f_00004-2-7 loss: 0.946037  [  256/  306]
train() client id: f_00004-2-8 loss: 0.854472  [  288/  306]
train() client id: f_00004-3-0 loss: 0.847671  [   32/  306]
train() client id: f_00004-3-1 loss: 0.912608  [   64/  306]
train() client id: f_00004-3-2 loss: 0.862382  [   96/  306]
train() client id: f_00004-3-3 loss: 0.903091  [  128/  306]
train() client id: f_00004-3-4 loss: 1.007514  [  160/  306]
train() client id: f_00004-3-5 loss: 0.872611  [  192/  306]
train() client id: f_00004-3-6 loss: 0.817808  [  224/  306]
train() client id: f_00004-3-7 loss: 0.841411  [  256/  306]
train() client id: f_00004-3-8 loss: 0.905567  [  288/  306]
train() client id: f_00004-4-0 loss: 0.887797  [   32/  306]
train() client id: f_00004-4-1 loss: 0.837977  [   64/  306]
train() client id: f_00004-4-2 loss: 0.932858  [   96/  306]
train() client id: f_00004-4-3 loss: 1.020699  [  128/  306]
train() client id: f_00004-4-4 loss: 0.907024  [  160/  306]
train() client id: f_00004-4-5 loss: 0.754583  [  192/  306]
train() client id: f_00004-4-6 loss: 0.912556  [  224/  306]
train() client id: f_00004-4-7 loss: 0.809413  [  256/  306]
train() client id: f_00004-4-8 loss: 0.872342  [  288/  306]
train() client id: f_00004-5-0 loss: 0.787571  [   32/  306]
train() client id: f_00004-5-1 loss: 0.855307  [   64/  306]
train() client id: f_00004-5-2 loss: 0.952575  [   96/  306]
train() client id: f_00004-5-3 loss: 0.870984  [  128/  306]
train() client id: f_00004-5-4 loss: 0.972643  [  160/  306]
train() client id: f_00004-5-5 loss: 0.834801  [  192/  306]
train() client id: f_00004-5-6 loss: 0.853105  [  224/  306]
train() client id: f_00004-5-7 loss: 0.922181  [  256/  306]
train() client id: f_00004-5-8 loss: 0.848076  [  288/  306]
train() client id: f_00004-6-0 loss: 0.776083  [   32/  306]
train() client id: f_00004-6-1 loss: 0.797459  [   64/  306]
train() client id: f_00004-6-2 loss: 0.906419  [   96/  306]
train() client id: f_00004-6-3 loss: 0.910859  [  128/  306]
train() client id: f_00004-6-4 loss: 0.935898  [  160/  306]
train() client id: f_00004-6-5 loss: 0.865673  [  192/  306]
train() client id: f_00004-6-6 loss: 0.975887  [  224/  306]
train() client id: f_00004-6-7 loss: 0.839729  [  256/  306]
train() client id: f_00004-6-8 loss: 0.915008  [  288/  306]
train() client id: f_00004-7-0 loss: 0.932133  [   32/  306]
train() client id: f_00004-7-1 loss: 0.767734  [   64/  306]
train() client id: f_00004-7-2 loss: 0.905655  [   96/  306]
train() client id: f_00004-7-3 loss: 0.980267  [  128/  306]
train() client id: f_00004-7-4 loss: 0.895673  [  160/  306]
train() client id: f_00004-7-5 loss: 0.927383  [  192/  306]
train() client id: f_00004-7-6 loss: 0.819697  [  224/  306]
train() client id: f_00004-7-7 loss: 0.798622  [  256/  306]
train() client id: f_00004-7-8 loss: 0.920828  [  288/  306]
train() client id: f_00004-8-0 loss: 1.087553  [   32/  306]
train() client id: f_00004-8-1 loss: 0.953032  [   64/  306]
train() client id: f_00004-8-2 loss: 0.832262  [   96/  306]
train() client id: f_00004-8-3 loss: 0.912578  [  128/  306]
train() client id: f_00004-8-4 loss: 0.902795  [  160/  306]
train() client id: f_00004-8-5 loss: 0.913125  [  192/  306]
train() client id: f_00004-8-6 loss: 0.787708  [  224/  306]
train() client id: f_00004-8-7 loss: 0.699024  [  256/  306]
train() client id: f_00004-8-8 loss: 0.815490  [  288/  306]
train() client id: f_00004-9-0 loss: 0.852503  [   32/  306]
train() client id: f_00004-9-1 loss: 0.895344  [   64/  306]
train() client id: f_00004-9-2 loss: 0.897423  [   96/  306]
train() client id: f_00004-9-3 loss: 0.808188  [  128/  306]
train() client id: f_00004-9-4 loss: 0.908543  [  160/  306]
train() client id: f_00004-9-5 loss: 0.956074  [  192/  306]
train() client id: f_00004-9-6 loss: 0.846017  [  224/  306]
train() client id: f_00004-9-7 loss: 0.764414  [  256/  306]
train() client id: f_00004-9-8 loss: 0.966283  [  288/  306]
train() client id: f_00004-10-0 loss: 0.777906  [   32/  306]
train() client id: f_00004-10-1 loss: 0.886447  [   64/  306]
train() client id: f_00004-10-2 loss: 0.955734  [   96/  306]
train() client id: f_00004-10-3 loss: 0.861823  [  128/  306]
train() client id: f_00004-10-4 loss: 0.905562  [  160/  306]
train() client id: f_00004-10-5 loss: 0.876314  [  192/  306]
train() client id: f_00004-10-6 loss: 0.760781  [  224/  306]
train() client id: f_00004-10-7 loss: 1.054330  [  256/  306]
train() client id: f_00004-10-8 loss: 0.859639  [  288/  306]
train() client id: f_00005-0-0 loss: 0.500523  [   32/  146]
train() client id: f_00005-0-1 loss: 0.915196  [   64/  146]
train() client id: f_00005-0-2 loss: 0.874296  [   96/  146]
train() client id: f_00005-0-3 loss: 0.591815  [  128/  146]
train() client id: f_00005-1-0 loss: 0.769066  [   32/  146]
train() client id: f_00005-1-1 loss: 0.674527  [   64/  146]
train() client id: f_00005-1-2 loss: 0.468188  [   96/  146]
train() client id: f_00005-1-3 loss: 0.801079  [  128/  146]
train() client id: f_00005-2-0 loss: 0.658004  [   32/  146]
train() client id: f_00005-2-1 loss: 0.756659  [   64/  146]
train() client id: f_00005-2-2 loss: 0.685102  [   96/  146]
train() client id: f_00005-2-3 loss: 0.824101  [  128/  146]
train() client id: f_00005-3-0 loss: 0.659901  [   32/  146]
train() client id: f_00005-3-1 loss: 0.685522  [   64/  146]
train() client id: f_00005-3-2 loss: 0.780817  [   96/  146]
train() client id: f_00005-3-3 loss: 0.620082  [  128/  146]
train() client id: f_00005-4-0 loss: 0.845730  [   32/  146]
train() client id: f_00005-4-1 loss: 0.842246  [   64/  146]
train() client id: f_00005-4-2 loss: 0.505443  [   96/  146]
train() client id: f_00005-4-3 loss: 0.535723  [  128/  146]
train() client id: f_00005-5-0 loss: 0.920626  [   32/  146]
train() client id: f_00005-5-1 loss: 0.611675  [   64/  146]
train() client id: f_00005-5-2 loss: 0.737694  [   96/  146]
train() client id: f_00005-5-3 loss: 0.549779  [  128/  146]
train() client id: f_00005-6-0 loss: 0.820643  [   32/  146]
train() client id: f_00005-6-1 loss: 0.677319  [   64/  146]
train() client id: f_00005-6-2 loss: 0.587704  [   96/  146]
train() client id: f_00005-6-3 loss: 0.422643  [  128/  146]
train() client id: f_00005-7-0 loss: 0.513980  [   32/  146]
train() client id: f_00005-7-1 loss: 0.938501  [   64/  146]
train() client id: f_00005-7-2 loss: 0.812679  [   96/  146]
train() client id: f_00005-7-3 loss: 0.684007  [  128/  146]
train() client id: f_00005-8-0 loss: 0.520558  [   32/  146]
train() client id: f_00005-8-1 loss: 0.773819  [   64/  146]
train() client id: f_00005-8-2 loss: 0.506282  [   96/  146]
train() client id: f_00005-8-3 loss: 0.907578  [  128/  146]
train() client id: f_00005-9-0 loss: 0.753054  [   32/  146]
train() client id: f_00005-9-1 loss: 0.801231  [   64/  146]
train() client id: f_00005-9-2 loss: 0.637020  [   96/  146]
train() client id: f_00005-9-3 loss: 0.482883  [  128/  146]
train() client id: f_00005-10-0 loss: 0.498913  [   32/  146]
train() client id: f_00005-10-1 loss: 0.780129  [   64/  146]
train() client id: f_00005-10-2 loss: 0.709055  [   96/  146]
train() client id: f_00005-10-3 loss: 0.695462  [  128/  146]
train() client id: f_00006-0-0 loss: 0.491162  [   32/   54]
train() client id: f_00006-1-0 loss: 0.450082  [   32/   54]
train() client id: f_00006-2-0 loss: 0.444834  [   32/   54]
train() client id: f_00006-3-0 loss: 0.473743  [   32/   54]
train() client id: f_00006-4-0 loss: 0.512381  [   32/   54]
train() client id: f_00006-5-0 loss: 0.497370  [   32/   54]
train() client id: f_00006-6-0 loss: 0.490316  [   32/   54]
train() client id: f_00006-7-0 loss: 0.426083  [   32/   54]
train() client id: f_00006-8-0 loss: 0.443449  [   32/   54]
train() client id: f_00006-9-0 loss: 0.448450  [   32/   54]
train() client id: f_00006-10-0 loss: 0.509996  [   32/   54]
train() client id: f_00007-0-0 loss: 0.391697  [   32/  179]
train() client id: f_00007-0-1 loss: 0.435817  [   64/  179]
train() client id: f_00007-0-2 loss: 0.481019  [   96/  179]
train() client id: f_00007-0-3 loss: 0.339646  [  128/  179]
train() client id: f_00007-0-4 loss: 0.401506  [  160/  179]
train() client id: f_00007-1-0 loss: 0.312361  [   32/  179]
train() client id: f_00007-1-1 loss: 0.487027  [   64/  179]
train() client id: f_00007-1-2 loss: 0.248109  [   96/  179]
train() client id: f_00007-1-3 loss: 0.330444  [  128/  179]
train() client id: f_00007-1-4 loss: 0.211923  [  160/  179]
train() client id: f_00007-2-0 loss: 0.340209  [   32/  179]
train() client id: f_00007-2-1 loss: 0.718790  [   64/  179]
train() client id: f_00007-2-2 loss: 0.231942  [   96/  179]
train() client id: f_00007-2-3 loss: 0.234536  [  128/  179]
train() client id: f_00007-2-4 loss: 0.291059  [  160/  179]
train() client id: f_00007-3-0 loss: 0.165235  [   32/  179]
train() client id: f_00007-3-1 loss: 0.380071  [   64/  179]
train() client id: f_00007-3-2 loss: 0.497708  [   96/  179]
train() client id: f_00007-3-3 loss: 0.372539  [  128/  179]
train() client id: f_00007-3-4 loss: 0.233735  [  160/  179]
train() client id: f_00007-4-0 loss: 0.384629  [   32/  179]
train() client id: f_00007-4-1 loss: 0.486358  [   64/  179]
train() client id: f_00007-4-2 loss: 0.269166  [   96/  179]
train() client id: f_00007-4-3 loss: 0.203396  [  128/  179]
train() client id: f_00007-4-4 loss: 0.420427  [  160/  179]
train() client id: f_00007-5-0 loss: 0.431094  [   32/  179]
train() client id: f_00007-5-1 loss: 0.378269  [   64/  179]
train() client id: f_00007-5-2 loss: 0.320706  [   96/  179]
train() client id: f_00007-5-3 loss: 0.320746  [  128/  179]
train() client id: f_00007-5-4 loss: 0.196046  [  160/  179]
train() client id: f_00007-6-0 loss: 0.261446  [   32/  179]
train() client id: f_00007-6-1 loss: 0.426699  [   64/  179]
train() client id: f_00007-6-2 loss: 0.278341  [   96/  179]
train() client id: f_00007-6-3 loss: 0.282067  [  128/  179]
train() client id: f_00007-6-4 loss: 0.434562  [  160/  179]
train() client id: f_00007-7-0 loss: 0.271235  [   32/  179]
train() client id: f_00007-7-1 loss: 0.153199  [   64/  179]
train() client id: f_00007-7-2 loss: 0.454360  [   96/  179]
train() client id: f_00007-7-3 loss: 0.577710  [  128/  179]
train() client id: f_00007-7-4 loss: 0.269961  [  160/  179]
train() client id: f_00007-8-0 loss: 0.166602  [   32/  179]
train() client id: f_00007-8-1 loss: 0.386729  [   64/  179]
train() client id: f_00007-8-2 loss: 0.173131  [   96/  179]
train() client id: f_00007-8-3 loss: 0.301193  [  128/  179]
train() client id: f_00007-8-4 loss: 0.532629  [  160/  179]
train() client id: f_00007-9-0 loss: 0.324041  [   32/  179]
train() client id: f_00007-9-1 loss: 0.381327  [   64/  179]
train() client id: f_00007-9-2 loss: 0.471904  [   96/  179]
train() client id: f_00007-9-3 loss: 0.206329  [  128/  179]
train() client id: f_00007-9-4 loss: 0.151399  [  160/  179]
train() client id: f_00007-10-0 loss: 0.154379  [   32/  179]
train() client id: f_00007-10-1 loss: 0.489086  [   64/  179]
train() client id: f_00007-10-2 loss: 0.265792  [   96/  179]
train() client id: f_00007-10-3 loss: 0.203671  [  128/  179]
train() client id: f_00007-10-4 loss: 0.359584  [  160/  179]
train() client id: f_00008-0-0 loss: 0.628014  [   32/  130]
train() client id: f_00008-0-1 loss: 0.794679  [   64/  130]
train() client id: f_00008-0-2 loss: 0.787835  [   96/  130]
train() client id: f_00008-0-3 loss: 0.727250  [  128/  130]
train() client id: f_00008-1-0 loss: 0.743956  [   32/  130]
train() client id: f_00008-1-1 loss: 0.833717  [   64/  130]
train() client id: f_00008-1-2 loss: 0.590611  [   96/  130]
train() client id: f_00008-1-3 loss: 0.749935  [  128/  130]
train() client id: f_00008-2-0 loss: 0.622962  [   32/  130]
train() client id: f_00008-2-1 loss: 0.818788  [   64/  130]
train() client id: f_00008-2-2 loss: 0.760669  [   96/  130]
train() client id: f_00008-2-3 loss: 0.744403  [  128/  130]
train() client id: f_00008-3-0 loss: 0.886835  [   32/  130]
train() client id: f_00008-3-1 loss: 0.709057  [   64/  130]
train() client id: f_00008-3-2 loss: 0.590531  [   96/  130]
train() client id: f_00008-3-3 loss: 0.759935  [  128/  130]
train() client id: f_00008-4-0 loss: 0.760801  [   32/  130]
train() client id: f_00008-4-1 loss: 0.692129  [   64/  130]
train() client id: f_00008-4-2 loss: 0.667608  [   96/  130]
train() client id: f_00008-4-3 loss: 0.795995  [  128/  130]
train() client id: f_00008-5-0 loss: 0.622348  [   32/  130]
train() client id: f_00008-5-1 loss: 0.807169  [   64/  130]
train() client id: f_00008-5-2 loss: 0.674523  [   96/  130]
train() client id: f_00008-5-3 loss: 0.831482  [  128/  130]
train() client id: f_00008-6-0 loss: 0.722988  [   32/  130]
train() client id: f_00008-6-1 loss: 0.737041  [   64/  130]
train() client id: f_00008-6-2 loss: 0.636763  [   96/  130]
train() client id: f_00008-6-3 loss: 0.807563  [  128/  130]
train() client id: f_00008-7-0 loss: 0.670067  [   32/  130]
train() client id: f_00008-7-1 loss: 0.750797  [   64/  130]
train() client id: f_00008-7-2 loss: 0.822522  [   96/  130]
train() client id: f_00008-7-3 loss: 0.684092  [  128/  130]
train() client id: f_00008-8-0 loss: 0.685469  [   32/  130]
train() client id: f_00008-8-1 loss: 0.726921  [   64/  130]
train() client id: f_00008-8-2 loss: 0.693645  [   96/  130]
train() client id: f_00008-8-3 loss: 0.843351  [  128/  130]
train() client id: f_00008-9-0 loss: 0.685199  [   32/  130]
train() client id: f_00008-9-1 loss: 0.710782  [   64/  130]
train() client id: f_00008-9-2 loss: 0.823083  [   96/  130]
train() client id: f_00008-9-3 loss: 0.731787  [  128/  130]
train() client id: f_00008-10-0 loss: 0.734553  [   32/  130]
train() client id: f_00008-10-1 loss: 0.691981  [   64/  130]
train() client id: f_00008-10-2 loss: 0.725001  [   96/  130]
train() client id: f_00008-10-3 loss: 0.722450  [  128/  130]
train() client id: f_00009-0-0 loss: 1.105674  [   32/  118]
train() client id: f_00009-0-1 loss: 0.905810  [   64/  118]
train() client id: f_00009-0-2 loss: 0.926787  [   96/  118]
train() client id: f_00009-1-0 loss: 0.944528  [   32/  118]
train() client id: f_00009-1-1 loss: 0.847801  [   64/  118]
train() client id: f_00009-1-2 loss: 0.968126  [   96/  118]
train() client id: f_00009-2-0 loss: 0.968929  [   32/  118]
train() client id: f_00009-2-1 loss: 0.970973  [   64/  118]
train() client id: f_00009-2-2 loss: 0.842101  [   96/  118]
train() client id: f_00009-3-0 loss: 0.882136  [   32/  118]
train() client id: f_00009-3-1 loss: 0.924825  [   64/  118]
train() client id: f_00009-3-2 loss: 0.755581  [   96/  118]
train() client id: f_00009-4-0 loss: 0.790425  [   32/  118]
train() client id: f_00009-4-1 loss: 0.847989  [   64/  118]
train() client id: f_00009-4-2 loss: 0.972216  [   96/  118]
train() client id: f_00009-5-0 loss: 0.963332  [   32/  118]
train() client id: f_00009-5-1 loss: 0.816484  [   64/  118]
train() client id: f_00009-5-2 loss: 0.921897  [   96/  118]
train() client id: f_00009-6-0 loss: 0.885268  [   32/  118]
train() client id: f_00009-6-1 loss: 0.890642  [   64/  118]
train() client id: f_00009-6-2 loss: 0.854036  [   96/  118]
train() client id: f_00009-7-0 loss: 0.823879  [   32/  118]
train() client id: f_00009-7-1 loss: 0.898991  [   64/  118]
train() client id: f_00009-7-2 loss: 0.877431  [   96/  118]
train() client id: f_00009-8-0 loss: 0.799564  [   32/  118]
train() client id: f_00009-8-1 loss: 0.884346  [   64/  118]
train() client id: f_00009-8-2 loss: 0.912709  [   96/  118]
train() client id: f_00009-9-0 loss: 0.910033  [   32/  118]
train() client id: f_00009-9-1 loss: 0.871282  [   64/  118]
train() client id: f_00009-9-2 loss: 0.826643  [   96/  118]
train() client id: f_00009-10-0 loss: 0.815411  [   32/  118]
train() client id: f_00009-10-1 loss: 0.956038  [   64/  118]
train() client id: f_00009-10-2 loss: 0.838555  [   96/  118]
At round 53 accuracy: 0.6472148541114059
At round 53 training accuracy: 0.5895372233400402
At round 53 training loss: 0.8329294590711289
update_location
xs = [  -3.9056584     4.20031788  285.00902392   18.81129433    0.97929623
    3.95640986 -247.44319194 -226.32485185  269.66397685 -212.06087855]
ys = [ 277.5879595   260.55583871    1.32061395 -247.45517586  239.35018685
  222.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [295.07681953 279.11823257 302.04616822 267.55920626 259.4021414
 244.2576487  266.89890179 247.43406156 288.14463449 234.49057138]
dists_bs = [199.49326595 198.57632151 491.70400773 464.79155911 187.3757168
 185.23387822 191.82974512 181.58219868 471.71026083 174.96857419]
uav_gains = [2.94167056e-12 4.07370156e-12 2.57043737e-12 5.18972845e-12
 6.14485954e-12 8.28695143e-12 5.26179225e-12 7.79933417e-12
 3.38079010e-12 9.90488682e-12]
bs_gains = [4.01323889e-11 4.06534300e-11 3.21013717e-12 3.75812338e-12
 4.78296084e-11 4.93943086e-11 4.47846749e-11 5.22262602e-11
 3.60581283e-12 5.79436618e-11]
Round 54
-------------------------------
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.87808648 7.89231983 3.82850312 1.39788506 9.0995415  4.37831172
 1.72181282 5.39414421 3.98347864 3.54993313]
obj_prev = 45.124016510283404
eta_min = 1.5688388831448793e-24	eta_max = 0.9379204733020944
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 10.402835079222914	eta = 0.9090909090909091
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 22.247241049856704	eta = 0.4250919373822522
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 16.022057938092566	eta = 0.5902564349620266
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.904186500929747	eta = 0.6345279427832983
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.838111741409746	eta = 0.6373535234204303
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.83785471002873	eta = 0.6373645640903601
eta = 0.6373645640903601
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.03716704 0.07816878 0.03657709 0.01268399 0.09026281 0.04306657
 0.01592873 0.0528008  0.03834695 0.03480723]
ene_total = [1.43077897 2.28852743 1.44075621 0.69142509 2.60477925 1.34299583
 0.77637967 1.71811452 1.42782004 1.11627769]
ti_comp = [0.85812284 0.94697766 0.84848758 0.89012496 0.94950542 0.94998635
 0.89076325 0.90687475 0.8710298  0.95228188]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [4.35767693e-06 3.32889784e-05 4.24831188e-06 1.60970289e-07
 5.09813926e-05 5.53179598e-06 3.18345717e-07 1.11868234e-05
 4.64522066e-06 2.90641279e-06]
ene_total = [0.4382524  0.20171506 0.46398296 0.35267029 0.19543653 0.19293825
 0.35096977 0.30823006 0.40378868 0.18673731]
optimize_network iter = 0 obj = 3.094721297443309
eta = 0.6373645640903601
freqs = [21656013.90690634 41272766.08711902 21554285.58755618  7124836.67893045
 47531488.32308537 22666941.75621849  8941057.85172046 29111404.29436724
 22012419.35744325 18275695.9412735 ]
eta_min = 0.6373645640903616	eta_max = 0.700248974656232
af = 0.0025292806273427074	bf = 1.1211163846456331	zeta = 0.002782208690076978	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.22110454e-07 7.04414656e-06 8.98968156e-07 3.40622741e-08
 1.07879670e-05 1.17056105e-06 6.73638541e-08 2.36719862e-06
 9.82956424e-07 6.15014298e-07]
ene_total = [1.74781835 0.80184327 1.85046646 1.40678466 0.77531226 0.76916407
 1.39998808 1.22858694 1.61031875 0.74464912]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 1 obj = 3.7430263059334705
eta = 0.700248974656232
freqs = [21591991.81873228 40169519.43771622 21554285.58755618  7037900.85535273
 46232598.93038776 22044962.3818897   8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
eta_min = 0.7002489746562424	eta_max = 0.7002489746562317
af = 0.0024124103758265034	bf = 1.1211163846456331	zeta = 0.002653651413409154	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.16666407e-07 6.67259097e-06 8.98968156e-07 3.32361034e-08
 1.02064191e-05 1.10720219e-06 6.57065467e-08 2.28871285e-06
 9.69657495e-07 5.81082673e-07]
ene_total = [1.74781777 0.80180369 1.85046646 1.40678457 0.7752503  0.76915732
 1.3999879  1.22857858 1.61031734 0.7446455 ]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 2 obj = 3.743026305933466
eta = 0.7002489746562317
freqs = [21591991.81873227 40169519.4377162  21554285.58755618  7037900.85535273
 46232598.93038776 22044962.38188971  8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
Done!
At round 54 energy consumption: 3.7430263059334705
At round 54 eta: 0.7002489746562317
At round 54 local rounds: 11.66770515392171
At round 54 global rounds: 32.310572111651304
At round 54 a_n: 9.342581272941924
gradient difference: 0.4975607991218567
train() client id: f_00000-0-0 loss: 0.835016  [   32/  126]
train() client id: f_00000-0-1 loss: 0.830361  [   64/  126]
train() client id: f_00000-0-2 loss: 1.086058  [   96/  126]
train() client id: f_00000-1-0 loss: 0.898081  [   32/  126]
train() client id: f_00000-1-1 loss: 0.670959  [   64/  126]
train() client id: f_00000-1-2 loss: 0.879836  [   96/  126]
train() client id: f_00000-2-0 loss: 0.770981  [   32/  126]
train() client id: f_00000-2-1 loss: 0.900691  [   64/  126]
train() client id: f_00000-2-2 loss: 0.809334  [   96/  126]
train() client id: f_00000-3-0 loss: 0.716972  [   32/  126]
train() client id: f_00000-3-1 loss: 0.736187  [   64/  126]
train() client id: f_00000-3-2 loss: 0.889890  [   96/  126]
train() client id: f_00000-4-0 loss: 0.815897  [   32/  126]
train() client id: f_00000-4-1 loss: 0.799787  [   64/  126]
train() client id: f_00000-4-2 loss: 0.711558  [   96/  126]
train() client id: f_00000-5-0 loss: 0.741252  [   32/  126]
train() client id: f_00000-5-1 loss: 0.879801  [   64/  126]
train() client id: f_00000-5-2 loss: 0.829336  [   96/  126]
train() client id: f_00000-6-0 loss: 0.672288  [   32/  126]
train() client id: f_00000-6-1 loss: 0.734041  [   64/  126]
train() client id: f_00000-6-2 loss: 0.724327  [   96/  126]
train() client id: f_00000-7-0 loss: 0.688293  [   32/  126]
train() client id: f_00000-7-1 loss: 0.806741  [   64/  126]
train() client id: f_00000-7-2 loss: 0.841757  [   96/  126]
train() client id: f_00000-8-0 loss: 0.704825  [   32/  126]
train() client id: f_00000-8-1 loss: 0.809164  [   64/  126]
train() client id: f_00000-8-2 loss: 0.751072  [   96/  126]
train() client id: f_00000-9-0 loss: 0.862581  [   32/  126]
train() client id: f_00000-9-1 loss: 0.690166  [   64/  126]
train() client id: f_00000-9-2 loss: 0.809726  [   96/  126]
train() client id: f_00000-10-0 loss: 0.631730  [   32/  126]
train() client id: f_00000-10-1 loss: 0.896168  [   64/  126]
train() client id: f_00000-10-2 loss: 0.775484  [   96/  126]
train() client id: f_00001-0-0 loss: 0.516302  [   32/  265]
train() client id: f_00001-0-1 loss: 0.493305  [   64/  265]
train() client id: f_00001-0-2 loss: 0.502363  [   96/  265]
train() client id: f_00001-0-3 loss: 0.415088  [  128/  265]
train() client id: f_00001-0-4 loss: 0.492766  [  160/  265]
train() client id: f_00001-0-5 loss: 0.429803  [  192/  265]
train() client id: f_00001-0-6 loss: 0.448170  [  224/  265]
train() client id: f_00001-0-7 loss: 0.454137  [  256/  265]
train() client id: f_00001-1-0 loss: 0.372667  [   32/  265]
train() client id: f_00001-1-1 loss: 0.581121  [   64/  265]
train() client id: f_00001-1-2 loss: 0.510961  [   96/  265]
train() client id: f_00001-1-3 loss: 0.406900  [  128/  265]
train() client id: f_00001-1-4 loss: 0.559104  [  160/  265]
train() client id: f_00001-1-5 loss: 0.401194  [  192/  265]
train() client id: f_00001-1-6 loss: 0.444127  [  224/  265]
train() client id: f_00001-1-7 loss: 0.414348  [  256/  265]
train() client id: f_00001-2-0 loss: 0.438099  [   32/  265]
train() client id: f_00001-2-1 loss: 0.417940  [   64/  265]
train() client id: f_00001-2-2 loss: 0.500445  [   96/  265]
train() client id: f_00001-2-3 loss: 0.491157  [  128/  265]
train() client id: f_00001-2-4 loss: 0.420841  [  160/  265]
train() client id: f_00001-2-5 loss: 0.440159  [  192/  265]
train() client id: f_00001-2-6 loss: 0.454978  [  224/  265]
train() client id: f_00001-2-7 loss: 0.432180  [  256/  265]
train() client id: f_00001-3-0 loss: 0.383827  [   32/  265]
train() client id: f_00001-3-1 loss: 0.419558  [   64/  265]
train() client id: f_00001-3-2 loss: 0.517795  [   96/  265]
train() client id: f_00001-3-3 loss: 0.688200  [  128/  265]
train() client id: f_00001-3-4 loss: 0.373088  [  160/  265]
train() client id: f_00001-3-5 loss: 0.444565  [  192/  265]
train() client id: f_00001-3-6 loss: 0.356283  [  224/  265]
train() client id: f_00001-3-7 loss: 0.438773  [  256/  265]
train() client id: f_00001-4-0 loss: 0.502541  [   32/  265]
train() client id: f_00001-4-1 loss: 0.423770  [   64/  265]
train() client id: f_00001-4-2 loss: 0.463632  [   96/  265]
train() client id: f_00001-4-3 loss: 0.348655  [  128/  265]
train() client id: f_00001-4-4 loss: 0.428089  [  160/  265]
train() client id: f_00001-4-5 loss: 0.397526  [  192/  265]
train() client id: f_00001-4-6 loss: 0.367606  [  224/  265]
train() client id: f_00001-4-7 loss: 0.520998  [  256/  265]
train() client id: f_00001-5-0 loss: 0.490848  [   32/  265]
train() client id: f_00001-5-1 loss: 0.468451  [   64/  265]
train() client id: f_00001-5-2 loss: 0.370277  [   96/  265]
train() client id: f_00001-5-3 loss: 0.493636  [  128/  265]
train() client id: f_00001-5-4 loss: 0.465342  [  160/  265]
train() client id: f_00001-5-5 loss: 0.412993  [  192/  265]
train() client id: f_00001-5-6 loss: 0.439137  [  224/  265]
train() client id: f_00001-5-7 loss: 0.429810  [  256/  265]
train() client id: f_00001-6-0 loss: 0.461322  [   32/  265]
train() client id: f_00001-6-1 loss: 0.463049  [   64/  265]
train() client id: f_00001-6-2 loss: 0.360771  [   96/  265]
train() client id: f_00001-6-3 loss: 0.451355  [  128/  265]
train() client id: f_00001-6-4 loss: 0.513973  [  160/  265]
train() client id: f_00001-6-5 loss: 0.423933  [  192/  265]
train() client id: f_00001-6-6 loss: 0.537201  [  224/  265]
train() client id: f_00001-6-7 loss: 0.351799  [  256/  265]
train() client id: f_00001-7-0 loss: 0.351137  [   32/  265]
train() client id: f_00001-7-1 loss: 0.442491  [   64/  265]
train() client id: f_00001-7-2 loss: 0.539364  [   96/  265]
train() client id: f_00001-7-3 loss: 0.349630  [  128/  265]
train() client id: f_00001-7-4 loss: 0.434025  [  160/  265]
train() client id: f_00001-7-5 loss: 0.558996  [  192/  265]
train() client id: f_00001-7-6 loss: 0.398997  [  224/  265]
train() client id: f_00001-7-7 loss: 0.484751  [  256/  265]
train() client id: f_00001-8-0 loss: 0.345452  [   32/  265]
train() client id: f_00001-8-1 loss: 0.486360  [   64/  265]
train() client id: f_00001-8-2 loss: 0.394015  [   96/  265]
train() client id: f_00001-8-3 loss: 0.393559  [  128/  265]
train() client id: f_00001-8-4 loss: 0.566434  [  160/  265]
train() client id: f_00001-8-5 loss: 0.482829  [  192/  265]
train() client id: f_00001-8-6 loss: 0.431756  [  224/  265]
train() client id: f_00001-8-7 loss: 0.442835  [  256/  265]
train() client id: f_00001-9-0 loss: 0.495881  [   32/  265]
train() client id: f_00001-9-1 loss: 0.343270  [   64/  265]
train() client id: f_00001-9-2 loss: 0.505384  [   96/  265]
train() client id: f_00001-9-3 loss: 0.441595  [  128/  265]
train() client id: f_00001-9-4 loss: 0.385778  [  160/  265]
train() client id: f_00001-9-5 loss: 0.454657  [  192/  265]
train() client id: f_00001-9-6 loss: 0.456874  [  224/  265]
train() client id: f_00001-9-7 loss: 0.399780  [  256/  265]
train() client id: f_00001-10-0 loss: 0.453094  [   32/  265]
train() client id: f_00001-10-1 loss: 0.388877  [   64/  265]
train() client id: f_00001-10-2 loss: 0.346543  [   96/  265]
train() client id: f_00001-10-3 loss: 0.570896  [  128/  265]
train() client id: f_00001-10-4 loss: 0.439480  [  160/  265]
train() client id: f_00001-10-5 loss: 0.373710  [  192/  265]
train() client id: f_00001-10-6 loss: 0.491358  [  224/  265]
train() client id: f_00001-10-7 loss: 0.393020  [  256/  265]
train() client id: f_00002-0-0 loss: 1.290310  [   32/  124]
train() client id: f_00002-0-1 loss: 1.411258  [   64/  124]
train() client id: f_00002-0-2 loss: 1.010310  [   96/  124]
train() client id: f_00002-1-0 loss: 1.083819  [   32/  124]
train() client id: f_00002-1-1 loss: 1.227000  [   64/  124]
train() client id: f_00002-1-2 loss: 1.189788  [   96/  124]
train() client id: f_00002-2-0 loss: 1.052074  [   32/  124]
train() client id: f_00002-2-1 loss: 1.275661  [   64/  124]
train() client id: f_00002-2-2 loss: 1.235325  [   96/  124]
train() client id: f_00002-3-0 loss: 1.079990  [   32/  124]
train() client id: f_00002-3-1 loss: 1.101356  [   64/  124]
train() client id: f_00002-3-2 loss: 1.083957  [   96/  124]
train() client id: f_00002-4-0 loss: 1.064344  [   32/  124]
train() client id: f_00002-4-1 loss: 1.022899  [   64/  124]
train() client id: f_00002-4-2 loss: 1.159416  [   96/  124]
train() client id: f_00002-5-0 loss: 1.065917  [   32/  124]
train() client id: f_00002-5-1 loss: 1.161224  [   64/  124]
train() client id: f_00002-5-2 loss: 0.867713  [   96/  124]
train() client id: f_00002-6-0 loss: 0.902257  [   32/  124]
train() client id: f_00002-6-1 loss: 1.121385  [   64/  124]
train() client id: f_00002-6-2 loss: 1.054730  [   96/  124]
train() client id: f_00002-7-0 loss: 1.204804  [   32/  124]
train() client id: f_00002-7-1 loss: 1.114824  [   64/  124]
train() client id: f_00002-7-2 loss: 0.849419  [   96/  124]
train() client id: f_00002-8-0 loss: 1.089325  [   32/  124]
train() client id: f_00002-8-1 loss: 0.887185  [   64/  124]
train() client id: f_00002-8-2 loss: 1.026029  [   96/  124]
train() client id: f_00002-9-0 loss: 1.141017  [   32/  124]
train() client id: f_00002-9-1 loss: 0.894893  [   64/  124]
train() client id: f_00002-9-2 loss: 0.922925  [   96/  124]
train() client id: f_00002-10-0 loss: 0.833312  [   32/  124]
train() client id: f_00002-10-1 loss: 1.041095  [   64/  124]
train() client id: f_00002-10-2 loss: 1.036977  [   96/  124]
train() client id: f_00003-0-0 loss: 0.619764  [   32/   43]
train() client id: f_00003-1-0 loss: 0.730097  [   32/   43]
train() client id: f_00003-2-0 loss: 0.597303  [   32/   43]
train() client id: f_00003-3-0 loss: 0.493733  [   32/   43]
train() client id: f_00003-4-0 loss: 0.665911  [   32/   43]
train() client id: f_00003-5-0 loss: 0.661905  [   32/   43]
train() client id: f_00003-6-0 loss: 0.607074  [   32/   43]
train() client id: f_00003-7-0 loss: 0.637620  [   32/   43]
train() client id: f_00003-8-0 loss: 0.543701  [   32/   43]
train() client id: f_00003-9-0 loss: 0.713198  [   32/   43]
train() client id: f_00003-10-0 loss: 0.434120  [   32/   43]
train() client id: f_00004-0-0 loss: 0.803039  [   32/  306]
train() client id: f_00004-0-1 loss: 0.839631  [   64/  306]
train() client id: f_00004-0-2 loss: 0.820153  [   96/  306]
train() client id: f_00004-0-3 loss: 0.931652  [  128/  306]
train() client id: f_00004-0-4 loss: 0.869851  [  160/  306]
train() client id: f_00004-0-5 loss: 0.950481  [  192/  306]
train() client id: f_00004-0-6 loss: 0.862886  [  224/  306]
train() client id: f_00004-0-7 loss: 0.876779  [  256/  306]
train() client id: f_00004-0-8 loss: 0.744002  [  288/  306]
train() client id: f_00004-1-0 loss: 0.891172  [   32/  306]
train() client id: f_00004-1-1 loss: 0.803244  [   64/  306]
train() client id: f_00004-1-2 loss: 0.928670  [   96/  306]
train() client id: f_00004-1-3 loss: 0.995977  [  128/  306]
train() client id: f_00004-1-4 loss: 0.792124  [  160/  306]
train() client id: f_00004-1-5 loss: 0.703396  [  192/  306]
train() client id: f_00004-1-6 loss: 0.836172  [  224/  306]
train() client id: f_00004-1-7 loss: 0.859224  [  256/  306]
train() client id: f_00004-1-8 loss: 0.922618  [  288/  306]
train() client id: f_00004-2-0 loss: 0.904752  [   32/  306]
train() client id: f_00004-2-1 loss: 0.772747  [   64/  306]
train() client id: f_00004-2-2 loss: 0.950847  [   96/  306]
train() client id: f_00004-2-3 loss: 0.928691  [  128/  306]
train() client id: f_00004-2-4 loss: 0.842718  [  160/  306]
train() client id: f_00004-2-5 loss: 0.821791  [  192/  306]
train() client id: f_00004-2-6 loss: 0.745415  [  224/  306]
train() client id: f_00004-2-7 loss: 0.899321  [  256/  306]
train() client id: f_00004-2-8 loss: 0.789069  [  288/  306]
train() client id: f_00004-3-0 loss: 0.872282  [   32/  306]
train() client id: f_00004-3-1 loss: 0.863696  [   64/  306]
train() client id: f_00004-3-2 loss: 0.671308  [   96/  306]
train() client id: f_00004-3-3 loss: 0.908632  [  128/  306]
train() client id: f_00004-3-4 loss: 0.685699  [  160/  306]
train() client id: f_00004-3-5 loss: 0.900151  [  192/  306]
train() client id: f_00004-3-6 loss: 0.848301  [  224/  306]
train() client id: f_00004-3-7 loss: 0.831601  [  256/  306]
train() client id: f_00004-3-8 loss: 0.988349  [  288/  306]
train() client id: f_00004-4-0 loss: 0.715330  [   32/  306]
train() client id: f_00004-4-1 loss: 0.807523  [   64/  306]
train() client id: f_00004-4-2 loss: 0.776568  [   96/  306]
train() client id: f_00004-4-3 loss: 0.736382  [  128/  306]
train() client id: f_00004-4-4 loss: 0.899615  [  160/  306]
train() client id: f_00004-4-5 loss: 0.862267  [  192/  306]
train() client id: f_00004-4-6 loss: 0.974971  [  224/  306]
train() client id: f_00004-4-7 loss: 1.007196  [  256/  306]
train() client id: f_00004-4-8 loss: 0.917458  [  288/  306]
train() client id: f_00004-5-0 loss: 0.778962  [   32/  306]
train() client id: f_00004-5-1 loss: 0.859720  [   64/  306]
train() client id: f_00004-5-2 loss: 0.854059  [   96/  306]
train() client id: f_00004-5-3 loss: 0.821798  [  128/  306]
train() client id: f_00004-5-4 loss: 0.705692  [  160/  306]
train() client id: f_00004-5-5 loss: 0.994579  [  192/  306]
train() client id: f_00004-5-6 loss: 0.925783  [  224/  306]
train() client id: f_00004-5-7 loss: 0.934798  [  256/  306]
train() client id: f_00004-5-8 loss: 0.829477  [  288/  306]
train() client id: f_00004-6-0 loss: 0.750810  [   32/  306]
train() client id: f_00004-6-1 loss: 0.816618  [   64/  306]
train() client id: f_00004-6-2 loss: 0.892274  [   96/  306]
train() client id: f_00004-6-3 loss: 0.939293  [  128/  306]
train() client id: f_00004-6-4 loss: 0.964116  [  160/  306]
train() client id: f_00004-6-5 loss: 0.809472  [  192/  306]
train() client id: f_00004-6-6 loss: 0.922418  [  224/  306]
train() client id: f_00004-6-7 loss: 0.853253  [  256/  306]
train() client id: f_00004-6-8 loss: 0.757622  [  288/  306]
train() client id: f_00004-7-0 loss: 0.793798  [   32/  306]
train() client id: f_00004-7-1 loss: 1.028011  [   64/  306]
train() client id: f_00004-7-2 loss: 0.813006  [   96/  306]
train() client id: f_00004-7-3 loss: 0.871002  [  128/  306]
train() client id: f_00004-7-4 loss: 0.839745  [  160/  306]
train() client id: f_00004-7-5 loss: 0.688214  [  192/  306]
train() client id: f_00004-7-6 loss: 0.890745  [  224/  306]
train() client id: f_00004-7-7 loss: 0.834678  [  256/  306]
train() client id: f_00004-7-8 loss: 0.859117  [  288/  306]
train() client id: f_00004-8-0 loss: 0.893844  [   32/  306]
train() client id: f_00004-8-1 loss: 0.724739  [   64/  306]
train() client id: f_00004-8-2 loss: 0.927088  [   96/  306]
train() client id: f_00004-8-3 loss: 0.926020  [  128/  306]
train() client id: f_00004-8-4 loss: 0.722058  [  160/  306]
train() client id: f_00004-8-5 loss: 0.973006  [  192/  306]
train() client id: f_00004-8-6 loss: 0.757659  [  224/  306]
train() client id: f_00004-8-7 loss: 0.908189  [  256/  306]
train() client id: f_00004-8-8 loss: 0.899035  [  288/  306]
train() client id: f_00004-9-0 loss: 0.806110  [   32/  306]
train() client id: f_00004-9-1 loss: 0.877941  [   64/  306]
train() client id: f_00004-9-2 loss: 0.936197  [   96/  306]
train() client id: f_00004-9-3 loss: 0.889940  [  128/  306]
train() client id: f_00004-9-4 loss: 0.863897  [  160/  306]
train() client id: f_00004-9-5 loss: 0.891340  [  192/  306]
train() client id: f_00004-9-6 loss: 0.818014  [  224/  306]
train() client id: f_00004-9-7 loss: 0.893064  [  256/  306]
train() client id: f_00004-9-8 loss: 0.785770  [  288/  306]
train() client id: f_00004-10-0 loss: 0.807110  [   32/  306]
train() client id: f_00004-10-1 loss: 0.905318  [   64/  306]
train() client id: f_00004-10-2 loss: 0.866637  [   96/  306]
train() client id: f_00004-10-3 loss: 0.891226  [  128/  306]
train() client id: f_00004-10-4 loss: 0.835527  [  160/  306]
train() client id: f_00004-10-5 loss: 0.908619  [  192/  306]
train() client id: f_00004-10-6 loss: 0.770203  [  224/  306]
train() client id: f_00004-10-7 loss: 0.831981  [  256/  306]
train() client id: f_00004-10-8 loss: 0.907339  [  288/  306]
train() client id: f_00005-0-0 loss: 0.565605  [   32/  146]
train() client id: f_00005-0-1 loss: 0.654168  [   64/  146]
train() client id: f_00005-0-2 loss: 0.957478  [   96/  146]
train() client id: f_00005-0-3 loss: 0.424723  [  128/  146]
train() client id: f_00005-1-0 loss: 0.703810  [   32/  146]
train() client id: f_00005-1-1 loss: 0.510382  [   64/  146]
train() client id: f_00005-1-2 loss: 0.634251  [   96/  146]
train() client id: f_00005-1-3 loss: 0.666907  [  128/  146]
train() client id: f_00005-2-0 loss: 0.770101  [   32/  146]
train() client id: f_00005-2-1 loss: 0.447373  [   64/  146]
train() client id: f_00005-2-2 loss: 0.469878  [   96/  146]
train() client id: f_00005-2-3 loss: 0.839831  [  128/  146]
train() client id: f_00005-3-0 loss: 0.422174  [   32/  146]
train() client id: f_00005-3-1 loss: 0.624061  [   64/  146]
train() client id: f_00005-3-2 loss: 0.922291  [   96/  146]
train() client id: f_00005-3-3 loss: 0.546124  [  128/  146]
train() client id: f_00005-4-0 loss: 0.475059  [   32/  146]
train() client id: f_00005-4-1 loss: 0.466622  [   64/  146]
train() client id: f_00005-4-2 loss: 0.812054  [   96/  146]
train() client id: f_00005-4-3 loss: 0.712313  [  128/  146]
train() client id: f_00005-5-0 loss: 0.457183  [   32/  146]
train() client id: f_00005-5-1 loss: 0.613557  [   64/  146]
train() client id: f_00005-5-2 loss: 0.583011  [   96/  146]
train() client id: f_00005-5-3 loss: 0.695687  [  128/  146]
train() client id: f_00005-6-0 loss: 0.580224  [   32/  146]
train() client id: f_00005-6-1 loss: 0.638512  [   64/  146]
train() client id: f_00005-6-2 loss: 0.365480  [   96/  146]
train() client id: f_00005-6-3 loss: 0.912949  [  128/  146]
train() client id: f_00005-7-0 loss: 0.673300  [   32/  146]
train() client id: f_00005-7-1 loss: 0.709090  [   64/  146]
train() client id: f_00005-7-2 loss: 0.703520  [   96/  146]
train() client id: f_00005-7-3 loss: 0.472384  [  128/  146]
train() client id: f_00005-8-0 loss: 0.557984  [   32/  146]
train() client id: f_00005-8-1 loss: 0.660977  [   64/  146]
train() client id: f_00005-8-2 loss: 0.750422  [   96/  146]
train() client id: f_00005-8-3 loss: 0.533880  [  128/  146]
train() client id: f_00005-9-0 loss: 0.503787  [   32/  146]
train() client id: f_00005-9-1 loss: 0.525714  [   64/  146]
train() client id: f_00005-9-2 loss: 0.610866  [   96/  146]
train() client id: f_00005-9-3 loss: 0.756584  [  128/  146]
train() client id: f_00005-10-0 loss: 0.446046  [   32/  146]
train() client id: f_00005-10-1 loss: 0.624115  [   64/  146]
train() client id: f_00005-10-2 loss: 0.473836  [   96/  146]
train() client id: f_00005-10-3 loss: 0.978281  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497023  [   32/   54]
train() client id: f_00006-1-0 loss: 0.504172  [   32/   54]
train() client id: f_00006-2-0 loss: 0.459671  [   32/   54]
train() client id: f_00006-3-0 loss: 0.397857  [   32/   54]
train() client id: f_00006-4-0 loss: 0.509379  [   32/   54]
train() client id: f_00006-5-0 loss: 0.517810  [   32/   54]
train() client id: f_00006-6-0 loss: 0.443070  [   32/   54]
train() client id: f_00006-7-0 loss: 0.501786  [   32/   54]
train() client id: f_00006-8-0 loss: 0.436864  [   32/   54]
train() client id: f_00006-9-0 loss: 0.451217  [   32/   54]
train() client id: f_00006-10-0 loss: 0.450887  [   32/   54]
train() client id: f_00007-0-0 loss: 0.555366  [   32/  179]
train() client id: f_00007-0-1 loss: 0.427979  [   64/  179]
train() client id: f_00007-0-2 loss: 0.542943  [   96/  179]
train() client id: f_00007-0-3 loss: 0.783209  [  128/  179]
train() client id: f_00007-0-4 loss: 0.656853  [  160/  179]
train() client id: f_00007-1-0 loss: 0.511031  [   32/  179]
train() client id: f_00007-1-1 loss: 0.646451  [   64/  179]
train() client id: f_00007-1-2 loss: 0.585764  [   96/  179]
train() client id: f_00007-1-3 loss: 0.409916  [  128/  179]
train() client id: f_00007-1-4 loss: 0.698426  [  160/  179]
train() client id: f_00007-2-0 loss: 0.408707  [   32/  179]
train() client id: f_00007-2-1 loss: 0.519374  [   64/  179]
train() client id: f_00007-2-2 loss: 0.647914  [   96/  179]
train() client id: f_00007-2-3 loss: 0.603936  [  128/  179]
train() client id: f_00007-2-4 loss: 0.574422  [  160/  179]
train() client id: f_00007-3-0 loss: 0.528908  [   32/  179]
train() client id: f_00007-3-1 loss: 0.461584  [   64/  179]
train() client id: f_00007-3-2 loss: 0.733849  [   96/  179]
train() client id: f_00007-3-3 loss: 0.612340  [  128/  179]
train() client id: f_00007-3-4 loss: 0.464551  [  160/  179]
train() client id: f_00007-4-0 loss: 0.447768  [   32/  179]
train() client id: f_00007-4-1 loss: 0.543565  [   64/  179]
train() client id: f_00007-4-2 loss: 0.527325  [   96/  179]
train() client id: f_00007-4-3 loss: 0.457281  [  128/  179]
train() client id: f_00007-4-4 loss: 0.641463  [  160/  179]
train() client id: f_00007-5-0 loss: 0.495394  [   32/  179]
train() client id: f_00007-5-1 loss: 0.578555  [   64/  179]
train() client id: f_00007-5-2 loss: 0.470754  [   96/  179]
train() client id: f_00007-5-3 loss: 0.559183  [  128/  179]
train() client id: f_00007-5-4 loss: 0.477990  [  160/  179]
train() client id: f_00007-6-0 loss: 0.474589  [   32/  179]
train() client id: f_00007-6-1 loss: 0.531654  [   64/  179]
train() client id: f_00007-6-2 loss: 0.690374  [   96/  179]
train() client id: f_00007-6-3 loss: 0.544017  [  128/  179]
train() client id: f_00007-6-4 loss: 0.458578  [  160/  179]
train() client id: f_00007-7-0 loss: 0.617629  [   32/  179]
train() client id: f_00007-7-1 loss: 0.414157  [   64/  179]
train() client id: f_00007-7-2 loss: 0.540947  [   96/  179]
train() client id: f_00007-7-3 loss: 0.604207  [  128/  179]
train() client id: f_00007-7-4 loss: 0.456360  [  160/  179]
train() client id: f_00007-8-0 loss: 0.481234  [   32/  179]
train() client id: f_00007-8-1 loss: 0.534258  [   64/  179]
train() client id: f_00007-8-2 loss: 0.562103  [   96/  179]
train() client id: f_00007-8-3 loss: 0.421606  [  128/  179]
train() client id: f_00007-8-4 loss: 0.676480  [  160/  179]
train() client id: f_00007-9-0 loss: 0.544969  [   32/  179]
train() client id: f_00007-9-1 loss: 0.606417  [   64/  179]
train() client id: f_00007-9-2 loss: 0.480681  [   96/  179]
train() client id: f_00007-9-3 loss: 0.478838  [  128/  179]
train() client id: f_00007-9-4 loss: 0.332261  [  160/  179]
train() client id: f_00007-10-0 loss: 0.566383  [   32/  179]
train() client id: f_00007-10-1 loss: 0.382908  [   64/  179]
train() client id: f_00007-10-2 loss: 0.433791  [   96/  179]
train() client id: f_00007-10-3 loss: 0.374953  [  128/  179]
train() client id: f_00007-10-4 loss: 0.672191  [  160/  179]
train() client id: f_00008-0-0 loss: 0.717020  [   32/  130]
train() client id: f_00008-0-1 loss: 0.752851  [   64/  130]
train() client id: f_00008-0-2 loss: 0.734277  [   96/  130]
train() client id: f_00008-0-3 loss: 0.782760  [  128/  130]
train() client id: f_00008-1-0 loss: 0.691796  [   32/  130]
train() client id: f_00008-1-1 loss: 0.727864  [   64/  130]
train() client id: f_00008-1-2 loss: 0.583633  [   96/  130]
train() client id: f_00008-1-3 loss: 0.939249  [  128/  130]
train() client id: f_00008-2-0 loss: 0.832458  [   32/  130]
train() client id: f_00008-2-1 loss: 0.789395  [   64/  130]
train() client id: f_00008-2-2 loss: 0.641586  [   96/  130]
train() client id: f_00008-2-3 loss: 0.728727  [  128/  130]
train() client id: f_00008-3-0 loss: 0.734365  [   32/  130]
train() client id: f_00008-3-1 loss: 0.751368  [   64/  130]
train() client id: f_00008-3-2 loss: 0.674051  [   96/  130]
train() client id: f_00008-3-3 loss: 0.835515  [  128/  130]
train() client id: f_00008-4-0 loss: 0.849959  [   32/  130]
train() client id: f_00008-4-1 loss: 0.642434  [   64/  130]
train() client id: f_00008-4-2 loss: 0.688806  [   96/  130]
train() client id: f_00008-4-3 loss: 0.781361  [  128/  130]
train() client id: f_00008-5-0 loss: 0.775164  [   32/  130]
train() client id: f_00008-5-1 loss: 0.642714  [   64/  130]
train() client id: f_00008-5-2 loss: 0.662307  [   96/  130]
train() client id: f_00008-5-3 loss: 0.865976  [  128/  130]
train() client id: f_00008-6-0 loss: 0.747799  [   32/  130]
train() client id: f_00008-6-1 loss: 0.748730  [   64/  130]
train() client id: f_00008-6-2 loss: 0.774373  [   96/  130]
train() client id: f_00008-6-3 loss: 0.688925  [  128/  130]
train() client id: f_00008-7-0 loss: 0.703145  [   32/  130]
train() client id: f_00008-7-1 loss: 0.748770  [   64/  130]
train() client id: f_00008-7-2 loss: 0.684166  [   96/  130]
train() client id: f_00008-7-3 loss: 0.851458  [  128/  130]
train() client id: f_00008-8-0 loss: 0.791684  [   32/  130]
train() client id: f_00008-8-1 loss: 0.759858  [   64/  130]
train() client id: f_00008-8-2 loss: 0.713841  [   96/  130]
train() client id: f_00008-8-3 loss: 0.693180  [  128/  130]
train() client id: f_00008-9-0 loss: 0.917018  [   32/  130]
train() client id: f_00008-9-1 loss: 0.763341  [   64/  130]
train() client id: f_00008-9-2 loss: 0.690139  [   96/  130]
train() client id: f_00008-9-3 loss: 0.620479  [  128/  130]
train() client id: f_00008-10-0 loss: 0.837742  [   32/  130]
train() client id: f_00008-10-1 loss: 0.725802  [   64/  130]
train() client id: f_00008-10-2 loss: 0.659355  [   96/  130]
train() client id: f_00008-10-3 loss: 0.764814  [  128/  130]
train() client id: f_00009-0-0 loss: 1.066914  [   32/  118]
train() client id: f_00009-0-1 loss: 1.091602  [   64/  118]
train() client id: f_00009-0-2 loss: 0.965784  [   96/  118]
train() client id: f_00009-1-0 loss: 0.906108  [   32/  118]
train() client id: f_00009-1-1 loss: 1.043609  [   64/  118]
train() client id: f_00009-1-2 loss: 0.987437  [   96/  118]
train() client id: f_00009-2-0 loss: 0.913750  [   32/  118]
train() client id: f_00009-2-1 loss: 1.014781  [   64/  118]
train() client id: f_00009-2-2 loss: 0.853868  [   96/  118]
train() client id: f_00009-3-0 loss: 0.943069  [   32/  118]
train() client id: f_00009-3-1 loss: 1.001458  [   64/  118]
train() client id: f_00009-3-2 loss: 0.830822  [   96/  118]
train() client id: f_00009-4-0 loss: 0.809472  [   32/  118]
train() client id: f_00009-4-1 loss: 0.915694  [   64/  118]
train() client id: f_00009-4-2 loss: 0.964895  [   96/  118]
train() client id: f_00009-5-0 loss: 0.931663  [   32/  118]
train() client id: f_00009-5-1 loss: 0.781296  [   64/  118]
train() client id: f_00009-5-2 loss: 0.915353  [   96/  118]
train() client id: f_00009-6-0 loss: 0.894647  [   32/  118]
train() client id: f_00009-6-1 loss: 0.857409  [   64/  118]
train() client id: f_00009-6-2 loss: 0.893190  [   96/  118]
train() client id: f_00009-7-0 loss: 0.932738  [   32/  118]
train() client id: f_00009-7-1 loss: 0.879396  [   64/  118]
train() client id: f_00009-7-2 loss: 0.787834  [   96/  118]
train() client id: f_00009-8-0 loss: 0.785199  [   32/  118]
train() client id: f_00009-8-1 loss: 0.866614  [   64/  118]
train() client id: f_00009-8-2 loss: 0.860361  [   96/  118]
train() client id: f_00009-9-0 loss: 0.828597  [   32/  118]
train() client id: f_00009-9-1 loss: 0.776635  [   64/  118]
train() client id: f_00009-9-2 loss: 0.809578  [   96/  118]
train() client id: f_00009-10-0 loss: 0.807222  [   32/  118]
train() client id: f_00009-10-1 loss: 0.871034  [   64/  118]
train() client id: f_00009-10-2 loss: 0.801715  [   96/  118]
At round 54 accuracy: 0.6472148541114059
At round 54 training accuracy: 0.5915492957746479
At round 54 training loss: 0.8253412373698773
update_location
xs = [  -3.9056584     4.20031788  290.00902392   18.81129433    0.97929623
    3.95640986 -252.44319194 -231.32485185  274.66397685 -217.06087855]
ys = [ 282.5879595   265.55583871    1.32061395 -252.45517586  244.35018685
  227.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [299.78527152 283.79137785 306.76860657 272.19015525 264.02267484
 248.82712967 271.54089139 252.01560138 292.82925085 239.02183343]
dists_bs = [202.10997688 200.78125871 496.38485217 469.34075593 189.15115944
 186.58706051 193.77327739 183.05830601 476.42650004 176.09545916]
uav_gains = [2.68378500e-12 3.69674554e-12 2.35370123e-12 4.70996853e-12
 5.58646308e-12 7.59172095e-12 4.77454866e-12 7.13118064e-12
 3.07591018e-12 9.13258179e-12]
bs_gains = [3.86944233e-11 3.94156962e-11 3.12609541e-12 3.65701662e-12
 4.65831512e-11 4.83978224e-11 4.35382740e-11 5.10556319e-11
 3.50675591e-12 5.69113968e-11]
Round 55
-------------------------------
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.74701293 7.6136546  3.69953246 1.35283391 8.77808353 4.22374522
 1.66527442 5.20651362 3.84415712 3.42461541]
obj_prev = 43.55542321522954
eta_min = 2.2364048538474024e-25	eta_max = 0.9376658335407816
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 10.034905168858742	eta = 0.909090909090909
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 21.758383945021716	eta = 0.4192701574551497
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 15.56372063459429	eta = 0.5861478290943805
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.453751471416155	eta = 0.6311607806900414
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387691546337155	eta = 0.6340587044987988
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387430378338086	eta = 0.6340702142568856
eta = 0.6340702142568856
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.03759466 0.07906812 0.03699791 0.01282992 0.09130131 0.04356206
 0.016112   0.05340828 0.03878814 0.03520769]
ene_total = [1.39513647 2.21195089 1.40544566 0.67656761 2.51756531 1.29725943
 0.75865034 1.66646122 1.38042471 1.07796874]
ti_comp = [0.89681784 0.99162304 0.88681254 0.93061922 0.99425194 0.99482833
 0.93128983 0.94868174 0.91451495 0.99717632]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [4.12904738e-06 3.14189100e-05 4.02483533e-06 1.52408097e-07
 4.81191659e-05 5.22046259e-06 3.01410426e-07 1.05794898e-05
 4.36109102e-06 2.74314532e-06]
ene_total = [0.43540987 0.19405873 0.46095183 0.34900978 0.18777323 0.1852064
 0.34730146 0.30316045 0.39023318 0.17914848]
optimize_network iter = 0 obj = 3.032253401302107
eta = 0.6340702142568856
freqs = [20960028.75390409 39868035.25301737 20860054.05324307  6893217.87674232
 45914572.57827595 21894260.42256957  8650365.72831919 28148681.6418485
 21206945.13364052 17653694.93957705]
eta_min = 0.6340702142568873	eta_max = 0.7017829950267004
af = 0.0022772242984259367	bf = 1.1085543441262717	zeta = 0.0025049467282685306	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.63792940e-07 6.57280727e-06 8.41991876e-07 3.18836346e-08
 1.00664856e-05 1.09211601e-06 6.30547859e-08 2.21321959e-06
 9.12336258e-07 5.73863496e-07]
ene_total = [1.75228827 0.77857718 1.85510894 1.40483142 0.75191939 0.74507368
 1.39794292 1.21943002 1.57042295 0.72089048]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 1 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
eta_min = 0.7017829950267082	eta_max = 0.7017829950267004
af = 0.0021615562993590274	bf = 1.1085543441262717	zeta = 0.0023777119292949303	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.58297057e-07 6.19209011e-06 8.41991876e-07 3.10406904e-08
 9.47094369e-06 1.02721042e-06 6.13639241e-08 2.13277124e-06
 8.96703470e-07 5.39129116e-07]
ene_total = [1.75228771 0.77853806 1.85510894 1.40483134 0.75185818 0.74506701
 1.39794275 1.21942176 1.57042134 0.72088691]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 2 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
Done!
At round 55 energy consumption: 3.71984092060831
At round 55 eta: 0.7017829950267004
At round 55 local rounds: 11.596049668428781
At round 55 global rounds: 31.328130579872187
At round 55 a_n: 9.000035425972609
gradient difference: 0.5112971663475037
train() client id: f_00000-0-0 loss: 1.121904  [   32/  126]
train() client id: f_00000-0-1 loss: 0.993937  [   64/  126]
train() client id: f_00000-0-2 loss: 1.073300  [   96/  126]
train() client id: f_00000-1-0 loss: 1.064018  [   32/  126]
train() client id: f_00000-1-1 loss: 0.946979  [   64/  126]
train() client id: f_00000-1-2 loss: 1.075478  [   96/  126]
train() client id: f_00000-2-0 loss: 0.969784  [   32/  126]
train() client id: f_00000-2-1 loss: 1.021680  [   64/  126]
train() client id: f_00000-2-2 loss: 0.919393  [   96/  126]
train() client id: f_00000-3-0 loss: 1.075220  [   32/  126]
train() client id: f_00000-3-1 loss: 0.856916  [   64/  126]
train() client id: f_00000-3-2 loss: 0.887482  [   96/  126]
train() client id: f_00000-4-0 loss: 0.839110  [   32/  126]
train() client id: f_00000-4-1 loss: 0.994761  [   64/  126]
train() client id: f_00000-4-2 loss: 0.925040  [   96/  126]
train() client id: f_00000-5-0 loss: 0.740056  [   32/  126]
train() client id: f_00000-5-1 loss: 0.926943  [   64/  126]
train() client id: f_00000-5-2 loss: 0.901124  [   96/  126]
train() client id: f_00000-6-0 loss: 0.795899  [   32/  126]
train() client id: f_00000-6-1 loss: 0.934031  [   64/  126]
train() client id: f_00000-6-2 loss: 0.829167  [   96/  126]
train() client id: f_00000-7-0 loss: 0.978335  [   32/  126]
train() client id: f_00000-7-1 loss: 0.834432  [   64/  126]
train() client id: f_00000-7-2 loss: 0.782030  [   96/  126]
train() client id: f_00000-8-0 loss: 0.848591  [   32/  126]
train() client id: f_00000-8-1 loss: 0.903618  [   64/  126]
train() client id: f_00000-8-2 loss: 0.848709  [   96/  126]
train() client id: f_00000-9-0 loss: 0.824900  [   32/  126]
train() client id: f_00000-9-1 loss: 0.865213  [   64/  126]
train() client id: f_00000-9-2 loss: 0.948627  [   96/  126]
train() client id: f_00000-10-0 loss: 0.863139  [   32/  126]
train() client id: f_00000-10-1 loss: 0.885183  [   64/  126]
train() client id: f_00000-10-2 loss: 0.869947  [   96/  126]
train() client id: f_00001-0-0 loss: 0.428510  [   32/  265]
train() client id: f_00001-0-1 loss: 0.512040  [   64/  265]
train() client id: f_00001-0-2 loss: 0.511162  [   96/  265]
train() client id: f_00001-0-3 loss: 0.382832  [  128/  265]
train() client id: f_00001-0-4 loss: 0.449251  [  160/  265]
train() client id: f_00001-0-5 loss: 0.497183  [  192/  265]
train() client id: f_00001-0-6 loss: 0.441214  [  224/  265]
train() client id: f_00001-0-7 loss: 0.451126  [  256/  265]
train() client id: f_00001-1-0 loss: 0.397309  [   32/  265]
train() client id: f_00001-1-1 loss: 0.400964  [   64/  265]
train() client id: f_00001-1-2 loss: 0.475921  [   96/  265]
train() client id: f_00001-1-3 loss: 0.458721  [  128/  265]
train() client id: f_00001-1-4 loss: 0.413971  [  160/  265]
train() client id: f_00001-1-5 loss: 0.403327  [  192/  265]
train() client id: f_00001-1-6 loss: 0.574117  [  224/  265]
train() client id: f_00001-1-7 loss: 0.427465  [  256/  265]
train() client id: f_00001-2-0 loss: 0.335236  [   32/  265]
train() client id: f_00001-2-1 loss: 0.435499  [   64/  265]
train() client id: f_00001-2-2 loss: 0.353867  [   96/  265]
train() client id: f_00001-2-3 loss: 0.545047  [  128/  265]
train() client id: f_00001-2-4 loss: 0.478984  [  160/  265]
train() client id: f_00001-2-5 loss: 0.473197  [  192/  265]
train() client id: f_00001-2-6 loss: 0.444045  [  224/  265]
train() client id: f_00001-2-7 loss: 0.514995  [  256/  265]
train() client id: f_00001-3-0 loss: 0.412870  [   32/  265]
train() client id: f_00001-3-1 loss: 0.348035  [   64/  265]
train() client id: f_00001-3-2 loss: 0.385526  [   96/  265]
train() client id: f_00001-3-3 loss: 0.451109  [  128/  265]
train() client id: f_00001-3-4 loss: 0.415783  [  160/  265]
train() client id: f_00001-3-5 loss: 0.610341  [  192/  265]
train() client id: f_00001-3-6 loss: 0.416683  [  224/  265]
train() client id: f_00001-3-7 loss: 0.466474  [  256/  265]
train() client id: f_00001-4-0 loss: 0.357710  [   32/  265]
train() client id: f_00001-4-1 loss: 0.419895  [   64/  265]
train() client id: f_00001-4-2 loss: 0.514780  [   96/  265]
train() client id: f_00001-4-3 loss: 0.406651  [  128/  265]
train() client id: f_00001-4-4 loss: 0.622919  [  160/  265]
train() client id: f_00001-4-5 loss: 0.399060  [  192/  265]
train() client id: f_00001-4-6 loss: 0.395217  [  224/  265]
train() client id: f_00001-4-7 loss: 0.421092  [  256/  265]
train() client id: f_00001-5-0 loss: 0.398606  [   32/  265]
train() client id: f_00001-5-1 loss: 0.443141  [   64/  265]
train() client id: f_00001-5-2 loss: 0.335814  [   96/  265]
train() client id: f_00001-5-3 loss: 0.445106  [  128/  265]
train() client id: f_00001-5-4 loss: 0.534389  [  160/  265]
train() client id: f_00001-5-5 loss: 0.479436  [  192/  265]
train() client id: f_00001-5-6 loss: 0.520426  [  224/  265]
train() client id: f_00001-5-7 loss: 0.356446  [  256/  265]
train() client id: f_00001-6-0 loss: 0.455911  [   32/  265]
train() client id: f_00001-6-1 loss: 0.488519  [   64/  265]
train() client id: f_00001-6-2 loss: 0.381834  [   96/  265]
train() client id: f_00001-6-3 loss: 0.433660  [  128/  265]
train() client id: f_00001-6-4 loss: 0.404596  [  160/  265]
train() client id: f_00001-6-5 loss: 0.521295  [  192/  265]
train() client id: f_00001-6-6 loss: 0.395465  [  224/  265]
train() client id: f_00001-6-7 loss: 0.432363  [  256/  265]
train() client id: f_00001-7-0 loss: 0.422002  [   32/  265]
train() client id: f_00001-7-1 loss: 0.495838  [   64/  265]
train() client id: f_00001-7-2 loss: 0.443367  [   96/  265]
train() client id: f_00001-7-3 loss: 0.418573  [  128/  265]
train() client id: f_00001-7-4 loss: 0.384563  [  160/  265]
train() client id: f_00001-7-5 loss: 0.481092  [  192/  265]
train() client id: f_00001-7-6 loss: 0.504072  [  224/  265]
train() client id: f_00001-7-7 loss: 0.372508  [  256/  265]
train() client id: f_00001-8-0 loss: 0.476004  [   32/  265]
train() client id: f_00001-8-1 loss: 0.386658  [   64/  265]
train() client id: f_00001-8-2 loss: 0.436843  [   96/  265]
train() client id: f_00001-8-3 loss: 0.479696  [  128/  265]
train() client id: f_00001-8-4 loss: 0.417742  [  160/  265]
train() client id: f_00001-8-5 loss: 0.493519  [  192/  265]
train() client id: f_00001-8-6 loss: 0.461119  [  224/  265]
train() client id: f_00001-8-7 loss: 0.369192  [  256/  265]
train() client id: f_00001-9-0 loss: 0.374971  [   32/  265]
train() client id: f_00001-9-1 loss: 0.599252  [   64/  265]
train() client id: f_00001-9-2 loss: 0.437138  [   96/  265]
train() client id: f_00001-9-3 loss: 0.333596  [  128/  265]
train() client id: f_00001-9-4 loss: 0.325460  [  160/  265]
train() client id: f_00001-9-5 loss: 0.500621  [  192/  265]
train() client id: f_00001-9-6 loss: 0.606842  [  224/  265]
train() client id: f_00001-9-7 loss: 0.344721  [  256/  265]
train() client id: f_00001-10-0 loss: 0.484134  [   32/  265]
train() client id: f_00001-10-1 loss: 0.371504  [   64/  265]
train() client id: f_00001-10-2 loss: 0.389374  [   96/  265]
train() client id: f_00001-10-3 loss: 0.486565  [  128/  265]
train() client id: f_00001-10-4 loss: 0.415696  [  160/  265]
train() client id: f_00001-10-5 loss: 0.500024  [  192/  265]
train() client id: f_00001-10-6 loss: 0.415285  [  224/  265]
train() client id: f_00001-10-7 loss: 0.455653  [  256/  265]
train() client id: f_00002-0-0 loss: 1.072005  [   32/  124]
train() client id: f_00002-0-1 loss: 1.166765  [   64/  124]
train() client id: f_00002-0-2 loss: 1.105487  [   96/  124]
train() client id: f_00002-1-0 loss: 1.264564  [   32/  124]
train() client id: f_00002-1-1 loss: 0.881736  [   64/  124]
train() client id: f_00002-1-2 loss: 1.082538  [   96/  124]
train() client id: f_00002-2-0 loss: 1.054677  [   32/  124]
train() client id: f_00002-2-1 loss: 0.963243  [   64/  124]
train() client id: f_00002-2-2 loss: 1.041840  [   96/  124]
train() client id: f_00002-3-0 loss: 1.097020  [   32/  124]
train() client id: f_00002-3-1 loss: 1.153634  [   64/  124]
train() client id: f_00002-3-2 loss: 0.940637  [   96/  124]
train() client id: f_00002-4-0 loss: 1.120522  [   32/  124]
train() client id: f_00002-4-1 loss: 1.071342  [   64/  124]
train() client id: f_00002-4-2 loss: 0.851276  [   96/  124]
train() client id: f_00002-5-0 loss: 0.961965  [   32/  124]
train() client id: f_00002-5-1 loss: 0.985787  [   64/  124]
train() client id: f_00002-5-2 loss: 1.038460  [   96/  124]
train() client id: f_00002-6-0 loss: 0.992830  [   32/  124]
train() client id: f_00002-6-1 loss: 0.930054  [   64/  124]
train() client id: f_00002-6-2 loss: 0.813965  [   96/  124]
train() client id: f_00002-7-0 loss: 0.908521  [   32/  124]
train() client id: f_00002-7-1 loss: 1.217223  [   64/  124]
train() client id: f_00002-7-2 loss: 0.826729  [   96/  124]
train() client id: f_00002-8-0 loss: 0.883989  [   32/  124]
train() client id: f_00002-8-1 loss: 1.014367  [   64/  124]
train() client id: f_00002-8-2 loss: 0.895941  [   96/  124]
train() client id: f_00002-9-0 loss: 1.001222  [   32/  124]
train() client id: f_00002-9-1 loss: 0.871467  [   64/  124]
train() client id: f_00002-9-2 loss: 0.863655  [   96/  124]
train() client id: f_00002-10-0 loss: 1.185235  [   32/  124]
train() client id: f_00002-10-1 loss: 0.858169  [   64/  124]
train() client id: f_00002-10-2 loss: 0.913496  [   96/  124]
train() client id: f_00003-0-0 loss: 0.858350  [   32/   43]
train() client id: f_00003-1-0 loss: 0.818749  [   32/   43]
train() client id: f_00003-2-0 loss: 0.862862  [   32/   43]
train() client id: f_00003-3-0 loss: 0.928254  [   32/   43]
train() client id: f_00003-4-0 loss: 0.812330  [   32/   43]
train() client id: f_00003-5-0 loss: 0.703083  [   32/   43]
train() client id: f_00003-6-0 loss: 0.765746  [   32/   43]
train() client id: f_00003-7-0 loss: 0.784270  [   32/   43]
train() client id: f_00003-8-0 loss: 0.982225  [   32/   43]
train() client id: f_00003-9-0 loss: 0.954860  [   32/   43]
train() client id: f_00003-10-0 loss: 0.743879  [   32/   43]
train() client id: f_00004-0-0 loss: 0.768124  [   32/  306]
train() client id: f_00004-0-1 loss: 0.698567  [   64/  306]
train() client id: f_00004-0-2 loss: 0.642878  [   96/  306]
train() client id: f_00004-0-3 loss: 0.968189  [  128/  306]
train() client id: f_00004-0-4 loss: 0.747242  [  160/  306]
train() client id: f_00004-0-5 loss: 0.783509  [  192/  306]
train() client id: f_00004-0-6 loss: 0.883268  [  224/  306]
train() client id: f_00004-0-7 loss: 0.763814  [  256/  306]
train() client id: f_00004-0-8 loss: 0.878884  [  288/  306]
train() client id: f_00004-1-0 loss: 0.982994  [   32/  306]
train() client id: f_00004-1-1 loss: 0.634978  [   64/  306]
train() client id: f_00004-1-2 loss: 0.839763  [   96/  306]
train() client id: f_00004-1-3 loss: 0.842877  [  128/  306]
train() client id: f_00004-1-4 loss: 0.714276  [  160/  306]
train() client id: f_00004-1-5 loss: 0.757610  [  192/  306]
train() client id: f_00004-1-6 loss: 0.946170  [  224/  306]
train() client id: f_00004-1-7 loss: 0.742966  [  256/  306]
train() client id: f_00004-1-8 loss: 0.741824  [  288/  306]
train() client id: f_00004-2-0 loss: 0.776968  [   32/  306]
train() client id: f_00004-2-1 loss: 0.787388  [   64/  306]
train() client id: f_00004-2-2 loss: 0.754461  [   96/  306]
train() client id: f_00004-2-3 loss: 0.823164  [  128/  306]
train() client id: f_00004-2-4 loss: 0.778084  [  160/  306]
train() client id: f_00004-2-5 loss: 0.805354  [  192/  306]
train() client id: f_00004-2-6 loss: 0.855250  [  224/  306]
train() client id: f_00004-2-7 loss: 0.817735  [  256/  306]
train() client id: f_00004-2-8 loss: 0.874513  [  288/  306]
train() client id: f_00004-3-0 loss: 0.809262  [   32/  306]
train() client id: f_00004-3-1 loss: 0.854809  [   64/  306]
train() client id: f_00004-3-2 loss: 0.761815  [   96/  306]
train() client id: f_00004-3-3 loss: 0.721724  [  128/  306]
train() client id: f_00004-3-4 loss: 0.751467  [  160/  306]
train() client id: f_00004-3-5 loss: 0.812741  [  192/  306]
train() client id: f_00004-3-6 loss: 0.804948  [  224/  306]
train() client id: f_00004-3-7 loss: 0.836904  [  256/  306]
train() client id: f_00004-3-8 loss: 0.896931  [  288/  306]
train() client id: f_00004-4-0 loss: 0.738961  [   32/  306]
train() client id: f_00004-4-1 loss: 0.714985  [   64/  306]
train() client id: f_00004-4-2 loss: 0.704987  [   96/  306]
train() client id: f_00004-4-3 loss: 0.788250  [  128/  306]
train() client id: f_00004-4-4 loss: 1.078348  [  160/  306]
train() client id: f_00004-4-5 loss: 0.800960  [  192/  306]
train() client id: f_00004-4-6 loss: 0.853038  [  224/  306]
train() client id: f_00004-4-7 loss: 0.806722  [  256/  306]
train() client id: f_00004-4-8 loss: 0.799370  [  288/  306]
train() client id: f_00004-5-0 loss: 0.688149  [   32/  306]
train() client id: f_00004-5-1 loss: 0.828916  [   64/  306]
train() client id: f_00004-5-2 loss: 0.829822  [   96/  306]
train() client id: f_00004-5-3 loss: 0.684350  [  128/  306]
train() client id: f_00004-5-4 loss: 0.686467  [  160/  306]
train() client id: f_00004-5-5 loss: 0.888891  [  192/  306]
train() client id: f_00004-5-6 loss: 1.132379  [  224/  306]
train() client id: f_00004-5-7 loss: 0.785205  [  256/  306]
train() client id: f_00004-5-8 loss: 0.744183  [  288/  306]
train() client id: f_00004-6-0 loss: 0.832596  [   32/  306]
train() client id: f_00004-6-1 loss: 0.968177  [   64/  306]
train() client id: f_00004-6-2 loss: 0.793231  [   96/  306]
train() client id: f_00004-6-3 loss: 0.944692  [  128/  306]
train() client id: f_00004-6-4 loss: 0.728495  [  160/  306]
train() client id: f_00004-6-5 loss: 0.855219  [  192/  306]
train() client id: f_00004-6-6 loss: 0.722012  [  224/  306]
train() client id: f_00004-6-7 loss: 0.754812  [  256/  306]
train() client id: f_00004-6-8 loss: 0.705712  [  288/  306]
train() client id: f_00004-7-0 loss: 0.979307  [   32/  306]
train() client id: f_00004-7-1 loss: 0.781544  [   64/  306]
train() client id: f_00004-7-2 loss: 0.759480  [   96/  306]
train() client id: f_00004-7-3 loss: 0.941481  [  128/  306]
train() client id: f_00004-7-4 loss: 0.791015  [  160/  306]
train() client id: f_00004-7-5 loss: 0.822026  [  192/  306]
train() client id: f_00004-7-6 loss: 0.773084  [  224/  306]
train() client id: f_00004-7-7 loss: 0.705172  [  256/  306]
train() client id: f_00004-7-8 loss: 0.722952  [  288/  306]
train() client id: f_00004-8-0 loss: 0.682974  [   32/  306]
train() client id: f_00004-8-1 loss: 0.808631  [   64/  306]
train() client id: f_00004-8-2 loss: 0.930674  [   96/  306]
train() client id: f_00004-8-3 loss: 0.820912  [  128/  306]
train() client id: f_00004-8-4 loss: 0.790793  [  160/  306]
train() client id: f_00004-8-5 loss: 0.765307  [  192/  306]
train() client id: f_00004-8-6 loss: 0.799325  [  224/  306]
train() client id: f_00004-8-7 loss: 0.797910  [  256/  306]
train() client id: f_00004-8-8 loss: 0.788946  [  288/  306]
train() client id: f_00004-9-0 loss: 0.856337  [   32/  306]
train() client id: f_00004-9-1 loss: 0.887697  [   64/  306]
train() client id: f_00004-9-2 loss: 0.834404  [   96/  306]
train() client id: f_00004-9-3 loss: 0.795719  [  128/  306]
train() client id: f_00004-9-4 loss: 0.904089  [  160/  306]
train() client id: f_00004-9-5 loss: 0.691206  [  192/  306]
train() client id: f_00004-9-6 loss: 0.783920  [  224/  306]
train() client id: f_00004-9-7 loss: 0.796313  [  256/  306]
train() client id: f_00004-9-8 loss: 0.720006  [  288/  306]
train() client id: f_00004-10-0 loss: 0.731630  [   32/  306]
train() client id: f_00004-10-1 loss: 0.665301  [   64/  306]
train() client id: f_00004-10-2 loss: 0.934321  [   96/  306]
train() client id: f_00004-10-3 loss: 0.808860  [  128/  306]
train() client id: f_00004-10-4 loss: 0.863563  [  160/  306]
train() client id: f_00004-10-5 loss: 0.874729  [  192/  306]
train() client id: f_00004-10-6 loss: 0.793732  [  224/  306]
train() client id: f_00004-10-7 loss: 0.791729  [  256/  306]
train() client id: f_00004-10-8 loss: 0.725642  [  288/  306]
train() client id: f_00005-0-0 loss: 0.495338  [   32/  146]
train() client id: f_00005-0-1 loss: 0.194115  [   64/  146]
train() client id: f_00005-0-2 loss: 0.175618  [   96/  146]
train() client id: f_00005-0-3 loss: 0.401434  [  128/  146]
train() client id: f_00005-1-0 loss: 0.507653  [   32/  146]
train() client id: f_00005-1-1 loss: 0.273917  [   64/  146]
train() client id: f_00005-1-2 loss: 0.417697  [   96/  146]
train() client id: f_00005-1-3 loss: -0.026597  [  128/  146]
train() client id: f_00005-2-0 loss: 0.187013  [   32/  146]
train() client id: f_00005-2-1 loss: 0.414871  [   64/  146]
train() client id: f_00005-2-2 loss: 0.135809  [   96/  146]
train() client id: f_00005-2-3 loss: 0.422148  [  128/  146]
train() client id: f_00005-3-0 loss: 0.050540  [   32/  146]
train() client id: f_00005-3-1 loss: 0.451558  [   64/  146]
train() client id: f_00005-3-2 loss: 0.156440  [   96/  146]
train() client id: f_00005-3-3 loss: 0.210426  [  128/  146]
train() client id: f_00005-4-0 loss: 0.118186  [   32/  146]
train() client id: f_00005-4-1 loss: 0.591259  [   64/  146]
train() client id: f_00005-4-2 loss: 0.146912  [   96/  146]
train() client id: f_00005-4-3 loss: 0.304312  [  128/  146]
train() client id: f_00005-5-0 loss: 0.028139  [   32/  146]
train() client id: f_00005-5-1 loss: 0.343919  [   64/  146]
train() client id: f_00005-5-2 loss: 0.237753  [   96/  146]
train() client id: f_00005-5-3 loss: 0.197386  [  128/  146]
train() client id: f_00005-6-0 loss: 0.256832  [   32/  146]
train() client id: f_00005-6-1 loss: 0.347191  [   64/  146]
train() client id: f_00005-6-2 loss: 0.359973  [   96/  146]
train() client id: f_00005-6-3 loss: 0.217857  [  128/  146]
train() client id: f_00005-7-0 loss: 0.106210  [   32/  146]
train() client id: f_00005-7-1 loss: 0.298027  [   64/  146]
train() client id: f_00005-7-2 loss: 0.170109  [   96/  146]
train() client id: f_00005-7-3 loss: 0.156496  [  128/  146]
train() client id: f_00005-8-0 loss: 0.460856  [   32/  146]
train() client id: f_00005-8-1 loss: 0.144006  [   64/  146]
train() client id: f_00005-8-2 loss: 0.342736  [   96/  146]
train() client id: f_00005-8-3 loss: 0.105414  [  128/  146]
train() client id: f_00005-9-0 loss: 0.207252  [   32/  146]
train() client id: f_00005-9-1 loss: 0.161460  [   64/  146]
train() client id: f_00005-9-2 loss: 0.304158  [   96/  146]
train() client id: f_00005-9-3 loss: 0.306063  [  128/  146]
train() client id: f_00005-10-0 loss: 0.333677  [   32/  146]
train() client id: f_00005-10-1 loss: 0.131926  [   64/  146]
train() client id: f_00005-10-2 loss: 0.213707  [   96/  146]
train() client id: f_00005-10-3 loss: 0.285134  [  128/  146]
train() client id: f_00006-0-0 loss: 0.521902  [   32/   54]
train() client id: f_00006-1-0 loss: 0.471448  [   32/   54]
train() client id: f_00006-2-0 loss: 0.570435  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532028  [   32/   54]
train() client id: f_00006-4-0 loss: 0.511686  [   32/   54]
train() client id: f_00006-5-0 loss: 0.490816  [   32/   54]
train() client id: f_00006-6-0 loss: 0.474939  [   32/   54]
train() client id: f_00006-7-0 loss: 0.472170  [   32/   54]
train() client id: f_00006-8-0 loss: 0.472341  [   32/   54]
train() client id: f_00006-9-0 loss: 0.497282  [   32/   54]
train() client id: f_00006-10-0 loss: 0.520704  [   32/   54]
train() client id: f_00007-0-0 loss: 0.408582  [   32/  179]
train() client id: f_00007-0-1 loss: 0.627656  [   64/  179]
train() client id: f_00007-0-2 loss: 0.298858  [   96/  179]
train() client id: f_00007-0-3 loss: 0.589832  [  128/  179]
train() client id: f_00007-0-4 loss: 0.291617  [  160/  179]
train() client id: f_00007-1-0 loss: 0.657821  [   32/  179]
train() client id: f_00007-1-1 loss: 0.512862  [   64/  179]
train() client id: f_00007-1-2 loss: 0.395628  [   96/  179]
train() client id: f_00007-1-3 loss: 0.272801  [  128/  179]
train() client id: f_00007-1-4 loss: 0.465222  [  160/  179]
train() client id: f_00007-2-0 loss: 0.499896  [   32/  179]
train() client id: f_00007-2-1 loss: 0.495583  [   64/  179]
train() client id: f_00007-2-2 loss: 0.354502  [   96/  179]
train() client id: f_00007-2-3 loss: 0.588072  [  128/  179]
train() client id: f_00007-2-4 loss: 0.338886  [  160/  179]
train() client id: f_00007-3-0 loss: 0.492594  [   32/  179]
train() client id: f_00007-3-1 loss: 0.413901  [   64/  179]
train() client id: f_00007-3-2 loss: 0.346288  [   96/  179]
train() client id: f_00007-3-3 loss: 0.373129  [  128/  179]
train() client id: f_00007-3-4 loss: 0.575873  [  160/  179]
train() client id: f_00007-4-0 loss: 0.311784  [   32/  179]
train() client id: f_00007-4-1 loss: 0.292810  [   64/  179]
train() client id: f_00007-4-2 loss: 0.474404  [   96/  179]
train() client id: f_00007-4-3 loss: 0.363895  [  128/  179]
train() client id: f_00007-4-4 loss: 0.599218  [  160/  179]
train() client id: f_00007-5-0 loss: 0.302532  [   32/  179]
train() client id: f_00007-5-1 loss: 0.416706  [   64/  179]
train() client id: f_00007-5-2 loss: 0.446313  [   96/  179]
train() client id: f_00007-5-3 loss: 0.424015  [  128/  179]
train() client id: f_00007-5-4 loss: 0.585484  [  160/  179]
train() client id: f_00007-6-0 loss: 0.660791  [   32/  179]
train() client id: f_00007-6-1 loss: 0.492119  [   64/  179]
train() client id: f_00007-6-2 loss: 0.271857  [   96/  179]
train() client id: f_00007-6-3 loss: 0.339445  [  128/  179]
train() client id: f_00007-6-4 loss: 0.332963  [  160/  179]
train() client id: f_00007-7-0 loss: 0.322849  [   32/  179]
train() client id: f_00007-7-1 loss: 0.316371  [   64/  179]
train() client id: f_00007-7-2 loss: 0.569242  [   96/  179]
train() client id: f_00007-7-3 loss: 0.504511  [  128/  179]
train() client id: f_00007-7-4 loss: 0.249296  [  160/  179]
train() client id: f_00007-8-0 loss: 0.520751  [   32/  179]
train() client id: f_00007-8-1 loss: 0.429273  [   64/  179]
train() client id: f_00007-8-2 loss: 0.421794  [   96/  179]
train() client id: f_00007-8-3 loss: 0.249242  [  128/  179]
train() client id: f_00007-8-4 loss: 0.353921  [  160/  179]
train() client id: f_00007-9-0 loss: 0.288372  [   32/  179]
train() client id: f_00007-9-1 loss: 0.430319  [   64/  179]
train() client id: f_00007-9-2 loss: 0.429524  [   96/  179]
train() client id: f_00007-9-3 loss: 0.626415  [  128/  179]
train() client id: f_00007-9-4 loss: 0.343180  [  160/  179]
train() client id: f_00007-10-0 loss: 0.716883  [   32/  179]
train() client id: f_00007-10-1 loss: 0.257250  [   64/  179]
train() client id: f_00007-10-2 loss: 0.319229  [   96/  179]
train() client id: f_00007-10-3 loss: 0.266535  [  128/  179]
train() client id: f_00007-10-4 loss: 0.357434  [  160/  179]
train() client id: f_00008-0-0 loss: 0.732609  [   32/  130]
train() client id: f_00008-0-1 loss: 0.675713  [   64/  130]
train() client id: f_00008-0-2 loss: 0.723168  [   96/  130]
train() client id: f_00008-0-3 loss: 0.697696  [  128/  130]
train() client id: f_00008-1-0 loss: 0.767881  [   32/  130]
train() client id: f_00008-1-1 loss: 0.677379  [   64/  130]
train() client id: f_00008-1-2 loss: 0.739874  [   96/  130]
train() client id: f_00008-1-3 loss: 0.631705  [  128/  130]
train() client id: f_00008-2-0 loss: 0.653758  [   32/  130]
train() client id: f_00008-2-1 loss: 0.800866  [   64/  130]
train() client id: f_00008-2-2 loss: 0.704855  [   96/  130]
train() client id: f_00008-2-3 loss: 0.664164  [  128/  130]
train() client id: f_00008-3-0 loss: 0.582075  [   32/  130]
train() client id: f_00008-3-1 loss: 0.673525  [   64/  130]
train() client id: f_00008-3-2 loss: 0.692051  [   96/  130]
train() client id: f_00008-3-3 loss: 0.854174  [  128/  130]
train() client id: f_00008-4-0 loss: 0.602784  [   32/  130]
train() client id: f_00008-4-1 loss: 0.731618  [   64/  130]
train() client id: f_00008-4-2 loss: 0.760613  [   96/  130]
train() client id: f_00008-4-3 loss: 0.702849  [  128/  130]
train() client id: f_00008-5-0 loss: 0.674054  [   32/  130]
train() client id: f_00008-5-1 loss: 0.717547  [   64/  130]
train() client id: f_00008-5-2 loss: 0.796730  [   96/  130]
train() client id: f_00008-5-3 loss: 0.637023  [  128/  130]
train() client id: f_00008-6-0 loss: 0.678092  [   32/  130]
train() client id: f_00008-6-1 loss: 0.730370  [   64/  130]
train() client id: f_00008-6-2 loss: 0.669103  [   96/  130]
train() client id: f_00008-6-3 loss: 0.712565  [  128/  130]
train() client id: f_00008-7-0 loss: 0.761339  [   32/  130]
train() client id: f_00008-7-1 loss: 0.726363  [   64/  130]
train() client id: f_00008-7-2 loss: 0.604541  [   96/  130]
train() client id: f_00008-7-3 loss: 0.744224  [  128/  130]
train() client id: f_00008-8-0 loss: 0.755977  [   32/  130]
train() client id: f_00008-8-1 loss: 0.636853  [   64/  130]
train() client id: f_00008-8-2 loss: 0.737832  [   96/  130]
train() client id: f_00008-8-3 loss: 0.696826  [  128/  130]
train() client id: f_00008-9-0 loss: 0.630551  [   32/  130]
train() client id: f_00008-9-1 loss: 0.780225  [   64/  130]
train() client id: f_00008-9-2 loss: 0.747992  [   96/  130]
train() client id: f_00008-9-3 loss: 0.656215  [  128/  130]
train() client id: f_00008-10-0 loss: 0.770528  [   32/  130]
train() client id: f_00008-10-1 loss: 0.670824  [   64/  130]
train() client id: f_00008-10-2 loss: 0.698427  [   96/  130]
train() client id: f_00008-10-3 loss: 0.690818  [  128/  130]
train() client id: f_00009-0-0 loss: 1.033214  [   32/  118]
train() client id: f_00009-0-1 loss: 0.920923  [   64/  118]
train() client id: f_00009-0-2 loss: 0.972081  [   96/  118]
train() client id: f_00009-1-0 loss: 0.942174  [   32/  118]
train() client id: f_00009-1-1 loss: 0.926706  [   64/  118]
train() client id: f_00009-1-2 loss: 0.947012  [   96/  118]
train() client id: f_00009-2-0 loss: 0.865254  [   32/  118]
train() client id: f_00009-2-1 loss: 0.954304  [   64/  118]
train() client id: f_00009-2-2 loss: 1.043497  [   96/  118]
train() client id: f_00009-3-0 loss: 0.869927  [   32/  118]
train() client id: f_00009-3-1 loss: 0.852491  [   64/  118]
train() client id: f_00009-3-2 loss: 0.963826  [   96/  118]
train() client id: f_00009-4-0 loss: 0.821187  [   32/  118]
train() client id: f_00009-4-1 loss: 0.753093  [   64/  118]
train() client id: f_00009-4-2 loss: 1.098789  [   96/  118]
train() client id: f_00009-5-0 loss: 0.956294  [   32/  118]
train() client id: f_00009-5-1 loss: 0.780359  [   64/  118]
train() client id: f_00009-5-2 loss: 0.878982  [   96/  118]
train() client id: f_00009-6-0 loss: 0.805689  [   32/  118]
train() client id: f_00009-6-1 loss: 1.024195  [   64/  118]
train() client id: f_00009-6-2 loss: 0.902153  [   96/  118]
train() client id: f_00009-7-0 loss: 0.858194  [   32/  118]
train() client id: f_00009-7-1 loss: 0.861613  [   64/  118]
train() client id: f_00009-7-2 loss: 0.838288  [   96/  118]
train() client id: f_00009-8-0 loss: 0.870340  [   32/  118]
train() client id: f_00009-8-1 loss: 0.656950  [   64/  118]
train() client id: f_00009-8-2 loss: 0.899854  [   96/  118]
train() client id: f_00009-9-0 loss: 0.806876  [   32/  118]
train() client id: f_00009-9-1 loss: 0.916925  [   64/  118]
train() client id: f_00009-9-2 loss: 0.921611  [   96/  118]
train() client id: f_00009-10-0 loss: 0.775425  [   32/  118]
train() client id: f_00009-10-1 loss: 1.018936  [   64/  118]
train() client id: f_00009-10-2 loss: 0.836802  [   96/  118]
At round 55 accuracy: 0.6472148541114059
At round 55 training accuracy: 0.5915492957746479
At round 55 training loss: 0.8191161736935083
update_location
xs = [  -3.9056584     4.20031788  295.00902392   18.81129433    0.97929623
    3.95640986 -257.44319194 -236.32485185  279.66397685 -222.06087855]
ys = [ 287.5879595   270.55583871    1.32061395 -257.45517586  249.35018685
  232.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [304.50301906 288.47548341 311.49970821 276.83394368 268.65679724
 253.41286859 276.19537943 256.61276636 297.52413334 243.57143847]
dists_bs = [204.81533719 203.08538164 501.07186281 473.89903665 191.04099818
 188.06348039 195.82521531 184.65804049 481.14846951 177.35619392]
uav_gains = [2.45442945e-12 3.35809681e-12 2.16122255e-12 4.27299009e-12
 5.07197201e-12 6.93588738e-12 4.33052174e-12 6.50389323e-12
 2.80382037e-12 8.39484877e-12]
bs_gains = [3.72802792e-11 3.81762994e-11 3.04490705e-12 3.55937522e-12
 4.53043251e-11 4.73414518e-11 4.22728929e-11 4.98268075e-11
 3.41124240e-12 5.57858785e-11]
Round 56
-------------------------------
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.61572451 7.33497615 3.57029297 1.30775167 8.45662183 4.06918315
 1.60870824 5.01890964 3.70473634 3.29930761]
obj_prev = 41.986212112103765
eta_min = 2.748442212366637e-26	eta_max = 0.9375089458427088
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 9.666975258494574	eta = 0.909090909090909
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 21.262798910283387	eta = 0.41331150066296846
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 15.102037364580543	eta = 0.5819187910708925
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 14.000878826985186	eta = 0.6276862641626415
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.93490538719279	eta = 0.6306579830804683
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.934640394529897	eta = 0.6306699761950073
eta = 0.6306699761950073
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.03803835 0.0800013  0.03743457 0.01298134 0.09237885 0.04407619
 0.01630215 0.05403861 0.03924592 0.03562322]
ene_total = [1.3587627  2.13524871 1.36925661 0.66156516 2.4302258  1.25152696
 0.7407784  1.61476761 1.33281256 1.03969587]
ti_comp = [0.93897812 1.03996482 0.92863396 0.97451355 1.04269211 1.04336226
 0.97521529 0.99392524 0.96170282 1.04576067]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [3.90152269e-06 2.95892377e-05 3.80197612e-06 1.43967031e-07
 4.53195465e-05 4.91611198e-06 2.84717213e-07 9.98355239e-06
 4.08490276e-06 2.58353803e-06]
ene_total = [0.43191848 0.18645457 0.45712329 0.34523224 0.18019189 0.17757424
 0.34352564 0.29816856 0.37654616 0.17167282]
optimize_network iter = 0 obj = 2.96840789621886
eta = 0.6306699761950073
freqs = [20255185.25779217 38463462.35602392 20155718.00296714  6660421.60236042
 44298241.69414774 21122186.81412296  8358231.69852659 27184444.26259059
 20404389.67271167 17032203.97092095]
eta_min = 0.6306699761950091	eta_max = 0.7038160577019408
af = 0.0020423891099591697	bf = 1.0953806736156944	zeta = 0.0022466280209550867	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.06674524e-07 6.11783813e-06 7.86092385e-07 2.97664645e-08
 9.37021941e-06 1.01644989e-06 5.88678167e-08 2.06418827e-06
 8.44589985e-07 5.34169471e-07]
ene_total = [1.75441871 0.75517231 1.85682433 1.40253966 0.72849401 0.72103243
 1.3955953  1.21056458 1.52944703 0.69724031]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 1 obj = 3.70059882926673
eta = 0.7038160577019408
freqs = [20185710.85684811 37218665.59464449 20155718.00296712  6563875.93252434
 42834368.86978544 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748624 16456368.51125106]
eta_min = 0.7038160577019519	eta_max = 0.703816057701941
af = 0.0019282538903956608	bf = 1.0953806736156944	zeta = 0.0021210792794352272	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.01150298e-07 5.72826139e-06 7.86092385e-07 2.89097636e-08
 8.76115832e-06 9.50052597e-07 5.71494239e-08 1.98200003e-06
 8.26731677e-07 4.98660964e-07]
ene_total = [1.75441816 0.75513375 1.85682433 1.40253957 0.72843371 0.72102585
 1.39559513 1.21055645 1.52944526 0.6972368 ]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 2 obj = 3.7005988292667333
eta = 0.703816057701941
freqs = [20185710.85684812 37218665.5946445  20155718.00296713  6563875.93252435
 42834368.86978545 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748625 16456368.51125106]
Done!
At round 56 energy consumption: 3.70059882926673
At round 56 eta: 0.703816057701941
At round 56 local rounds: 11.50132443724633
At round 56 global rounds: 30.386642017599986
At round 56 a_n: 8.65748957900329
gradient difference: 0.4576411247253418
train() client id: f_00000-0-0 loss: 1.165427  [   32/  126]
train() client id: f_00000-0-1 loss: 1.217833  [   64/  126]
train() client id: f_00000-0-2 loss: 1.161746  [   96/  126]
train() client id: f_00000-1-0 loss: 1.081332  [   32/  126]
train() client id: f_00000-1-1 loss: 1.209928  [   64/  126]
train() client id: f_00000-1-2 loss: 1.032299  [   96/  126]
train() client id: f_00000-2-0 loss: 1.055606  [   32/  126]
train() client id: f_00000-2-1 loss: 0.856533  [   64/  126]
train() client id: f_00000-2-2 loss: 1.141077  [   96/  126]
train() client id: f_00000-3-0 loss: 0.983430  [   32/  126]
train() client id: f_00000-3-1 loss: 0.819229  [   64/  126]
train() client id: f_00000-3-2 loss: 0.919565  [   96/  126]
train() client id: f_00000-4-0 loss: 0.869687  [   32/  126]
train() client id: f_00000-4-1 loss: 0.853287  [   64/  126]
train() client id: f_00000-4-2 loss: 0.949603  [   96/  126]
train() client id: f_00000-5-0 loss: 0.972092  [   32/  126]
train() client id: f_00000-5-1 loss: 0.892471  [   64/  126]
train() client id: f_00000-5-2 loss: 0.801435  [   96/  126]
train() client id: f_00000-6-0 loss: 0.868759  [   32/  126]
train() client id: f_00000-6-1 loss: 0.865720  [   64/  126]
train() client id: f_00000-6-2 loss: 0.825242  [   96/  126]
train() client id: f_00000-7-0 loss: 0.741973  [   32/  126]
train() client id: f_00000-7-1 loss: 0.851992  [   64/  126]
train() client id: f_00000-7-2 loss: 0.777743  [   96/  126]
train() client id: f_00000-8-0 loss: 0.934392  [   32/  126]
train() client id: f_00000-8-1 loss: 0.692230  [   64/  126]
train() client id: f_00000-8-2 loss: 0.842900  [   96/  126]
train() client id: f_00000-9-0 loss: 0.690532  [   32/  126]
train() client id: f_00000-9-1 loss: 0.740098  [   64/  126]
train() client id: f_00000-9-2 loss: 0.957856  [   96/  126]
train() client id: f_00000-10-0 loss: 0.774017  [   32/  126]
train() client id: f_00000-10-1 loss: 0.804320  [   64/  126]
train() client id: f_00000-10-2 loss: 0.796808  [   96/  126]
train() client id: f_00001-0-0 loss: 0.410858  [   32/  265]
train() client id: f_00001-0-1 loss: 0.467737  [   64/  265]
train() client id: f_00001-0-2 loss: 0.367414  [   96/  265]
train() client id: f_00001-0-3 loss: 0.445059  [  128/  265]
train() client id: f_00001-0-4 loss: 0.466602  [  160/  265]
train() client id: f_00001-0-5 loss: 0.526708  [  192/  265]
train() client id: f_00001-0-6 loss: 0.489066  [  224/  265]
train() client id: f_00001-0-7 loss: 0.459518  [  256/  265]
train() client id: f_00001-1-0 loss: 0.479437  [   32/  265]
train() client id: f_00001-1-1 loss: 0.469715  [   64/  265]
train() client id: f_00001-1-2 loss: 0.424018  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482625  [  128/  265]
train() client id: f_00001-1-4 loss: 0.405444  [  160/  265]
train() client id: f_00001-1-5 loss: 0.365334  [  192/  265]
train() client id: f_00001-1-6 loss: 0.492211  [  224/  265]
train() client id: f_00001-1-7 loss: 0.422010  [  256/  265]
train() client id: f_00001-2-0 loss: 0.380418  [   32/  265]
train() client id: f_00001-2-1 loss: 0.502350  [   64/  265]
train() client id: f_00001-2-2 loss: 0.428345  [   96/  265]
train() client id: f_00001-2-3 loss: 0.464026  [  128/  265]
train() client id: f_00001-2-4 loss: 0.523145  [  160/  265]
train() client id: f_00001-2-5 loss: 0.356819  [  192/  265]
train() client id: f_00001-2-6 loss: 0.372336  [  224/  265]
train() client id: f_00001-2-7 loss: 0.412194  [  256/  265]
train() client id: f_00001-3-0 loss: 0.362410  [   32/  265]
train() client id: f_00001-3-1 loss: 0.550847  [   64/  265]
train() client id: f_00001-3-2 loss: 0.355557  [   96/  265]
train() client id: f_00001-3-3 loss: 0.387598  [  128/  265]
train() client id: f_00001-3-4 loss: 0.425461  [  160/  265]
train() client id: f_00001-3-5 loss: 0.546419  [  192/  265]
train() client id: f_00001-3-6 loss: 0.343946  [  224/  265]
train() client id: f_00001-3-7 loss: 0.477102  [  256/  265]
train() client id: f_00001-4-0 loss: 0.378412  [   32/  265]
train() client id: f_00001-4-1 loss: 0.323343  [   64/  265]
train() client id: f_00001-4-2 loss: 0.426942  [   96/  265]
train() client id: f_00001-4-3 loss: 0.422891  [  128/  265]
train() client id: f_00001-4-4 loss: 0.438962  [  160/  265]
train() client id: f_00001-4-5 loss: 0.546691  [  192/  265]
train() client id: f_00001-4-6 loss: 0.421953  [  224/  265]
train() client id: f_00001-4-7 loss: 0.502155  [  256/  265]
train() client id: f_00001-5-0 loss: 0.452514  [   32/  265]
train() client id: f_00001-5-1 loss: 0.439215  [   64/  265]
train() client id: f_00001-5-2 loss: 0.313103  [   96/  265]
train() client id: f_00001-5-3 loss: 0.437704  [  128/  265]
train() client id: f_00001-5-4 loss: 0.405956  [  160/  265]
train() client id: f_00001-5-5 loss: 0.454296  [  192/  265]
train() client id: f_00001-5-6 loss: 0.483073  [  224/  265]
train() client id: f_00001-5-7 loss: 0.360432  [  256/  265]
train() client id: f_00001-6-0 loss: 0.485138  [   32/  265]
train() client id: f_00001-6-1 loss: 0.321693  [   64/  265]
train() client id: f_00001-6-2 loss: 0.374515  [   96/  265]
train() client id: f_00001-6-3 loss: 0.467942  [  128/  265]
train() client id: f_00001-6-4 loss: 0.339719  [  160/  265]
train() client id: f_00001-6-5 loss: 0.406655  [  192/  265]
train() client id: f_00001-6-6 loss: 0.598352  [  224/  265]
train() client id: f_00001-6-7 loss: 0.423408  [  256/  265]
train() client id: f_00001-7-0 loss: 0.388731  [   32/  265]
train() client id: f_00001-7-1 loss: 0.419231  [   64/  265]
train() client id: f_00001-7-2 loss: 0.399978  [   96/  265]
train() client id: f_00001-7-3 loss: 0.440836  [  128/  265]
train() client id: f_00001-7-4 loss: 0.437967  [  160/  265]
train() client id: f_00001-7-5 loss: 0.386571  [  192/  265]
train() client id: f_00001-7-6 loss: 0.481974  [  224/  265]
train() client id: f_00001-7-7 loss: 0.450163  [  256/  265]
train() client id: f_00001-8-0 loss: 0.515029  [   32/  265]
train() client id: f_00001-8-1 loss: 0.366248  [   64/  265]
train() client id: f_00001-8-2 loss: 0.479727  [   96/  265]
train() client id: f_00001-8-3 loss: 0.477217  [  128/  265]
train() client id: f_00001-8-4 loss: 0.368683  [  160/  265]
train() client id: f_00001-8-5 loss: 0.328625  [  192/  265]
train() client id: f_00001-8-6 loss: 0.413363  [  224/  265]
train() client id: f_00001-8-7 loss: 0.392268  [  256/  265]
train() client id: f_00001-9-0 loss: 0.415657  [   32/  265]
train() client id: f_00001-9-1 loss: 0.377784  [   64/  265]
train() client id: f_00001-9-2 loss: 0.349286  [   96/  265]
train() client id: f_00001-9-3 loss: 0.459095  [  128/  265]
train() client id: f_00001-9-4 loss: 0.478689  [  160/  265]
train() client id: f_00001-9-5 loss: 0.380862  [  192/  265]
train() client id: f_00001-9-6 loss: 0.330289  [  224/  265]
train() client id: f_00001-9-7 loss: 0.610833  [  256/  265]
train() client id: f_00001-10-0 loss: 0.475180  [   32/  265]
train() client id: f_00001-10-1 loss: 0.427137  [   64/  265]
train() client id: f_00001-10-2 loss: 0.360770  [   96/  265]
train() client id: f_00001-10-3 loss: 0.343704  [  128/  265]
train() client id: f_00001-10-4 loss: 0.621985  [  160/  265]
train() client id: f_00001-10-5 loss: 0.430783  [  192/  265]
train() client id: f_00001-10-6 loss: 0.329162  [  224/  265]
train() client id: f_00001-10-7 loss: 0.414502  [  256/  265]
train() client id: f_00002-0-0 loss: 1.051666  [   32/  124]
train() client id: f_00002-0-1 loss: 1.127799  [   64/  124]
train() client id: f_00002-0-2 loss: 1.028550  [   96/  124]
train() client id: f_00002-1-0 loss: 1.134918  [   32/  124]
train() client id: f_00002-1-1 loss: 1.212112  [   64/  124]
train() client id: f_00002-1-2 loss: 0.891458  [   96/  124]
train() client id: f_00002-2-0 loss: 0.965168  [   32/  124]
train() client id: f_00002-2-1 loss: 1.086213  [   64/  124]
train() client id: f_00002-2-2 loss: 1.047122  [   96/  124]
train() client id: f_00002-3-0 loss: 0.952538  [   32/  124]
train() client id: f_00002-3-1 loss: 1.083536  [   64/  124]
train() client id: f_00002-3-2 loss: 1.160406  [   96/  124]
train() client id: f_00002-4-0 loss: 0.910060  [   32/  124]
train() client id: f_00002-4-1 loss: 1.091084  [   64/  124]
train() client id: f_00002-4-2 loss: 1.035297  [   96/  124]
train() client id: f_00002-5-0 loss: 1.067527  [   32/  124]
train() client id: f_00002-5-1 loss: 0.890643  [   64/  124]
train() client id: f_00002-5-2 loss: 0.980696  [   96/  124]
train() client id: f_00002-6-0 loss: 1.041103  [   32/  124]
train() client id: f_00002-6-1 loss: 1.073304  [   64/  124]
train() client id: f_00002-6-2 loss: 0.882527  [   96/  124]
train() client id: f_00002-7-0 loss: 0.925526  [   32/  124]
train() client id: f_00002-7-1 loss: 0.952396  [   64/  124]
train() client id: f_00002-7-2 loss: 0.917990  [   96/  124]
train() client id: f_00002-8-0 loss: 1.225869  [   32/  124]
train() client id: f_00002-8-1 loss: 0.869537  [   64/  124]
train() client id: f_00002-8-2 loss: 0.800564  [   96/  124]
train() client id: f_00002-9-0 loss: 0.844653  [   32/  124]
train() client id: f_00002-9-1 loss: 1.133015  [   64/  124]
train() client id: f_00002-9-2 loss: 1.028439  [   96/  124]
train() client id: f_00002-10-0 loss: 1.037772  [   32/  124]
train() client id: f_00002-10-1 loss: 0.918295  [   64/  124]
train() client id: f_00002-10-2 loss: 0.968893  [   96/  124]
train() client id: f_00003-0-0 loss: 0.813193  [   32/   43]
train() client id: f_00003-1-0 loss: 0.818181  [   32/   43]
train() client id: f_00003-2-0 loss: 0.654597  [   32/   43]
train() client id: f_00003-3-0 loss: 0.929260  [   32/   43]
train() client id: f_00003-4-0 loss: 0.629811  [   32/   43]
train() client id: f_00003-5-0 loss: 0.631022  [   32/   43]
train() client id: f_00003-6-0 loss: 0.661432  [   32/   43]
train() client id: f_00003-7-0 loss: 0.647137  [   32/   43]
train() client id: f_00003-8-0 loss: 0.862190  [   32/   43]
train() client id: f_00003-9-0 loss: 0.765300  [   32/   43]
train() client id: f_00003-10-0 loss: 0.724964  [   32/   43]
train() client id: f_00004-0-0 loss: 0.966646  [   32/  306]
train() client id: f_00004-0-1 loss: 0.968498  [   64/  306]
train() client id: f_00004-0-2 loss: 1.033561  [   96/  306]
train() client id: f_00004-0-3 loss: 1.018851  [  128/  306]
train() client id: f_00004-0-4 loss: 0.854344  [  160/  306]
train() client id: f_00004-0-5 loss: 0.979316  [  192/  306]
train() client id: f_00004-0-6 loss: 0.867337  [  224/  306]
train() client id: f_00004-0-7 loss: 1.077136  [  256/  306]
train() client id: f_00004-0-8 loss: 0.980664  [  288/  306]
train() client id: f_00004-1-0 loss: 1.010566  [   32/  306]
train() client id: f_00004-1-1 loss: 0.902701  [   64/  306]
train() client id: f_00004-1-2 loss: 0.957024  [   96/  306]
train() client id: f_00004-1-3 loss: 1.017962  [  128/  306]
train() client id: f_00004-1-4 loss: 0.931433  [  160/  306]
train() client id: f_00004-1-5 loss: 0.949188  [  192/  306]
train() client id: f_00004-1-6 loss: 0.991066  [  224/  306]
train() client id: f_00004-1-7 loss: 0.950296  [  256/  306]
train() client id: f_00004-1-8 loss: 0.969694  [  288/  306]
train() client id: f_00004-2-0 loss: 0.947491  [   32/  306]
train() client id: f_00004-2-1 loss: 0.934045  [   64/  306]
train() client id: f_00004-2-2 loss: 0.830335  [   96/  306]
train() client id: f_00004-2-3 loss: 0.966605  [  128/  306]
train() client id: f_00004-2-4 loss: 1.079060  [  160/  306]
train() client id: f_00004-2-5 loss: 0.970182  [  192/  306]
train() client id: f_00004-2-6 loss: 0.908298  [  224/  306]
train() client id: f_00004-2-7 loss: 0.971197  [  256/  306]
train() client id: f_00004-2-8 loss: 1.083883  [  288/  306]
train() client id: f_00004-3-0 loss: 0.827864  [   32/  306]
train() client id: f_00004-3-1 loss: 0.937322  [   64/  306]
train() client id: f_00004-3-2 loss: 0.848156  [   96/  306]
train() client id: f_00004-3-3 loss: 1.040660  [  128/  306]
train() client id: f_00004-3-4 loss: 1.027542  [  160/  306]
train() client id: f_00004-3-5 loss: 1.059734  [  192/  306]
train() client id: f_00004-3-6 loss: 0.753203  [  224/  306]
train() client id: f_00004-3-7 loss: 1.057454  [  256/  306]
train() client id: f_00004-3-8 loss: 0.995505  [  288/  306]
train() client id: f_00004-4-0 loss: 0.990785  [   32/  306]
train() client id: f_00004-4-1 loss: 0.911986  [   64/  306]
train() client id: f_00004-4-2 loss: 0.988160  [   96/  306]
train() client id: f_00004-4-3 loss: 1.073628  [  128/  306]
train() client id: f_00004-4-4 loss: 1.016745  [  160/  306]
train() client id: f_00004-4-5 loss: 0.903075  [  192/  306]
train() client id: f_00004-4-6 loss: 0.804171  [  224/  306]
train() client id: f_00004-4-7 loss: 0.992194  [  256/  306]
train() client id: f_00004-4-8 loss: 0.901368  [  288/  306]
train() client id: f_00004-5-0 loss: 1.113814  [   32/  306]
train() client id: f_00004-5-1 loss: 1.124442  [   64/  306]
train() client id: f_00004-5-2 loss: 0.848525  [   96/  306]
train() client id: f_00004-5-3 loss: 1.096320  [  128/  306]
train() client id: f_00004-5-4 loss: 0.889291  [  160/  306]
train() client id: f_00004-5-5 loss: 1.017192  [  192/  306]
train() client id: f_00004-5-6 loss: 0.842680  [  224/  306]
train() client id: f_00004-5-7 loss: 0.908666  [  256/  306]
train() client id: f_00004-5-8 loss: 0.903611  [  288/  306]
train() client id: f_00004-6-0 loss: 1.073690  [   32/  306]
train() client id: f_00004-6-1 loss: 0.876012  [   64/  306]
train() client id: f_00004-6-2 loss: 0.913519  [   96/  306]
train() client id: f_00004-6-3 loss: 1.021231  [  128/  306]
train() client id: f_00004-6-4 loss: 1.029353  [  160/  306]
train() client id: f_00004-6-5 loss: 0.937248  [  192/  306]
train() client id: f_00004-6-6 loss: 0.962763  [  224/  306]
train() client id: f_00004-6-7 loss: 0.913251  [  256/  306]
train() client id: f_00004-6-8 loss: 0.901996  [  288/  306]
train() client id: f_00004-7-0 loss: 0.911583  [   32/  306]
train() client id: f_00004-7-1 loss: 0.977852  [   64/  306]
train() client id: f_00004-7-2 loss: 1.117547  [   96/  306]
train() client id: f_00004-7-3 loss: 0.901625  [  128/  306]
train() client id: f_00004-7-4 loss: 0.981427  [  160/  306]
train() client id: f_00004-7-5 loss: 0.925043  [  192/  306]
train() client id: f_00004-7-6 loss: 0.889043  [  224/  306]
train() client id: f_00004-7-7 loss: 0.890560  [  256/  306]
train() client id: f_00004-7-8 loss: 1.081410  [  288/  306]
train() client id: f_00004-8-0 loss: 0.882284  [   32/  306]
train() client id: f_00004-8-1 loss: 1.020651  [   64/  306]
train() client id: f_00004-8-2 loss: 0.883963  [   96/  306]
train() client id: f_00004-8-3 loss: 1.029428  [  128/  306]
train() client id: f_00004-8-4 loss: 1.103069  [  160/  306]
train() client id: f_00004-8-5 loss: 0.908104  [  192/  306]
train() client id: f_00004-8-6 loss: 1.026698  [  224/  306]
train() client id: f_00004-8-7 loss: 0.911294  [  256/  306]
train() client id: f_00004-8-8 loss: 0.861662  [  288/  306]
train() client id: f_00004-9-0 loss: 1.043652  [   32/  306]
train() client id: f_00004-9-1 loss: 0.880587  [   64/  306]
train() client id: f_00004-9-2 loss: 0.925970  [   96/  306]
train() client id: f_00004-9-3 loss: 1.135228  [  128/  306]
train() client id: f_00004-9-4 loss: 0.939229  [  160/  306]
train() client id: f_00004-9-5 loss: 0.855285  [  192/  306]
train() client id: f_00004-9-6 loss: 1.077757  [  224/  306]
train() client id: f_00004-9-7 loss: 0.905045  [  256/  306]
train() client id: f_00004-9-8 loss: 0.861991  [  288/  306]
train() client id: f_00004-10-0 loss: 0.961170  [   32/  306]
train() client id: f_00004-10-1 loss: 0.997200  [   64/  306]
train() client id: f_00004-10-2 loss: 0.982068  [   96/  306]
train() client id: f_00004-10-3 loss: 0.991072  [  128/  306]
train() client id: f_00004-10-4 loss: 0.976156  [  160/  306]
train() client id: f_00004-10-5 loss: 0.964477  [  192/  306]
train() client id: f_00004-10-6 loss: 0.915671  [  224/  306]
train() client id: f_00004-10-7 loss: 0.902346  [  256/  306]
train() client id: f_00004-10-8 loss: 0.872600  [  288/  306]
train() client id: f_00005-0-0 loss: 0.426671  [   32/  146]
train() client id: f_00005-0-1 loss: 0.565925  [   64/  146]
train() client id: f_00005-0-2 loss: 0.718948  [   96/  146]
train() client id: f_00005-0-3 loss: 0.537319  [  128/  146]
train() client id: f_00005-1-0 loss: 0.556921  [   32/  146]
train() client id: f_00005-1-1 loss: 0.532830  [   64/  146]
train() client id: f_00005-1-2 loss: 0.593500  [   96/  146]
train() client id: f_00005-1-3 loss: 0.518572  [  128/  146]
train() client id: f_00005-2-0 loss: 0.692377  [   32/  146]
train() client id: f_00005-2-1 loss: 0.554298  [   64/  146]
train() client id: f_00005-2-2 loss: 0.575540  [   96/  146]
train() client id: f_00005-2-3 loss: 0.513592  [  128/  146]
train() client id: f_00005-3-0 loss: 0.384320  [   32/  146]
train() client id: f_00005-3-1 loss: 0.724389  [   64/  146]
train() client id: f_00005-3-2 loss: 0.571254  [   96/  146]
train() client id: f_00005-3-3 loss: 0.581043  [  128/  146]
train() client id: f_00005-4-0 loss: 0.703280  [   32/  146]
train() client id: f_00005-4-1 loss: 0.289220  [   64/  146]
train() client id: f_00005-4-2 loss: 0.317603  [   96/  146]
train() client id: f_00005-4-3 loss: 1.006988  [  128/  146]
train() client id: f_00005-5-0 loss: 0.710787  [   32/  146]
train() client id: f_00005-5-1 loss: 0.634663  [   64/  146]
train() client id: f_00005-5-2 loss: 0.413191  [   96/  146]
train() client id: f_00005-5-3 loss: 0.545649  [  128/  146]
train() client id: f_00005-6-0 loss: 0.631956  [   32/  146]
train() client id: f_00005-6-1 loss: 0.517402  [   64/  146]
train() client id: f_00005-6-2 loss: 0.588599  [   96/  146]
train() client id: f_00005-6-3 loss: 0.539246  [  128/  146]
train() client id: f_00005-7-0 loss: 0.612290  [   32/  146]
train() client id: f_00005-7-1 loss: 0.492391  [   64/  146]
train() client id: f_00005-7-2 loss: 0.626775  [   96/  146]
train() client id: f_00005-7-3 loss: 0.435223  [  128/  146]
train() client id: f_00005-8-0 loss: 0.663163  [   32/  146]
train() client id: f_00005-8-1 loss: 0.419122  [   64/  146]
train() client id: f_00005-8-2 loss: 0.351154  [   96/  146]
train() client id: f_00005-8-3 loss: 0.642633  [  128/  146]
train() client id: f_00005-9-0 loss: 0.827825  [   32/  146]
train() client id: f_00005-9-1 loss: 0.442503  [   64/  146]
train() client id: f_00005-9-2 loss: 0.504378  [   96/  146]
train() client id: f_00005-9-3 loss: 0.537330  [  128/  146]
train() client id: f_00005-10-0 loss: 0.696619  [   32/  146]
train() client id: f_00005-10-1 loss: 0.326933  [   64/  146]
train() client id: f_00005-10-2 loss: 0.371815  [   96/  146]
train() client id: f_00005-10-3 loss: 0.704046  [  128/  146]
train() client id: f_00006-0-0 loss: 0.560358  [   32/   54]
train() client id: f_00006-1-0 loss: 0.551918  [   32/   54]
train() client id: f_00006-2-0 loss: 0.485696  [   32/   54]
train() client id: f_00006-3-0 loss: 0.541041  [   32/   54]
train() client id: f_00006-4-0 loss: 0.511084  [   32/   54]
train() client id: f_00006-5-0 loss: 0.509795  [   32/   54]
train() client id: f_00006-6-0 loss: 0.536144  [   32/   54]
train() client id: f_00006-7-0 loss: 0.501113  [   32/   54]
train() client id: f_00006-8-0 loss: 0.555289  [   32/   54]
train() client id: f_00006-9-0 loss: 0.520297  [   32/   54]
train() client id: f_00006-10-0 loss: 0.552640  [   32/   54]
train() client id: f_00007-0-0 loss: 0.812811  [   32/  179]
train() client id: f_00007-0-1 loss: 0.712832  [   64/  179]
train() client id: f_00007-0-2 loss: 0.453749  [   96/  179]
train() client id: f_00007-0-3 loss: 0.728226  [  128/  179]
train() client id: f_00007-0-4 loss: 0.490522  [  160/  179]
train() client id: f_00007-1-0 loss: 0.513506  [   32/  179]
train() client id: f_00007-1-1 loss: 0.583384  [   64/  179]
train() client id: f_00007-1-2 loss: 0.477818  [   96/  179]
train() client id: f_00007-1-3 loss: 0.575812  [  128/  179]
train() client id: f_00007-1-4 loss: 0.837931  [  160/  179]
train() client id: f_00007-2-0 loss: 0.481279  [   32/  179]
train() client id: f_00007-2-1 loss: 0.527615  [   64/  179]
train() client id: f_00007-2-2 loss: 0.704327  [   96/  179]
train() client id: f_00007-2-3 loss: 0.553818  [  128/  179]
train() client id: f_00007-2-4 loss: 0.623129  [  160/  179]
train() client id: f_00007-3-0 loss: 0.584223  [   32/  179]
train() client id: f_00007-3-1 loss: 0.433996  [   64/  179]
train() client id: f_00007-3-2 loss: 0.991169  [   96/  179]
train() client id: f_00007-3-3 loss: 0.545805  [  128/  179]
train() client id: f_00007-3-4 loss: 0.485228  [  160/  179]
train() client id: f_00007-4-0 loss: 0.642096  [   32/  179]
train() client id: f_00007-4-1 loss: 0.569564  [   64/  179]
train() client id: f_00007-4-2 loss: 0.483238  [   96/  179]
train() client id: f_00007-4-3 loss: 0.555873  [  128/  179]
train() client id: f_00007-4-4 loss: 0.507311  [  160/  179]
train() client id: f_00007-5-0 loss: 0.543496  [   32/  179]
train() client id: f_00007-5-1 loss: 0.466323  [   64/  179]
train() client id: f_00007-5-2 loss: 0.708414  [   96/  179]
train() client id: f_00007-5-3 loss: 0.423852  [  128/  179]
train() client id: f_00007-5-4 loss: 0.828299  [  160/  179]
train() client id: f_00007-6-0 loss: 0.540165  [   32/  179]
train() client id: f_00007-6-1 loss: 0.548379  [   64/  179]
train() client id: f_00007-6-2 loss: 0.425808  [   96/  179]
train() client id: f_00007-6-3 loss: 0.619386  [  128/  179]
train() client id: f_00007-6-4 loss: 0.557509  [  160/  179]
train() client id: f_00007-7-0 loss: 0.449388  [   32/  179]
train() client id: f_00007-7-1 loss: 0.649031  [   64/  179]
train() client id: f_00007-7-2 loss: 0.880803  [   96/  179]
train() client id: f_00007-7-3 loss: 0.520391  [  128/  179]
train() client id: f_00007-7-4 loss: 0.438254  [  160/  179]
train() client id: f_00007-8-0 loss: 0.601286  [   32/  179]
train() client id: f_00007-8-1 loss: 0.551308  [   64/  179]
train() client id: f_00007-8-2 loss: 0.619018  [   96/  179]
train() client id: f_00007-8-3 loss: 0.633357  [  128/  179]
train() client id: f_00007-8-4 loss: 0.619071  [  160/  179]
train() client id: f_00007-9-0 loss: 0.593422  [   32/  179]
train() client id: f_00007-9-1 loss: 0.460907  [   64/  179]
train() client id: f_00007-9-2 loss: 0.746088  [   96/  179]
train() client id: f_00007-9-3 loss: 0.617844  [  128/  179]
train() client id: f_00007-9-4 loss: 0.551262  [  160/  179]
train() client id: f_00007-10-0 loss: 0.492669  [   32/  179]
train() client id: f_00007-10-1 loss: 0.704216  [   64/  179]
train() client id: f_00007-10-2 loss: 0.552738  [   96/  179]
train() client id: f_00007-10-3 loss: 0.546628  [  128/  179]
train() client id: f_00007-10-4 loss: 0.533573  [  160/  179]
train() client id: f_00008-0-0 loss: 0.687101  [   32/  130]
train() client id: f_00008-0-1 loss: 0.803330  [   64/  130]
train() client id: f_00008-0-2 loss: 0.726212  [   96/  130]
train() client id: f_00008-0-3 loss: 0.780961  [  128/  130]
train() client id: f_00008-1-0 loss: 0.706105  [   32/  130]
train() client id: f_00008-1-1 loss: 0.749262  [   64/  130]
train() client id: f_00008-1-2 loss: 0.759125  [   96/  130]
train() client id: f_00008-1-3 loss: 0.742594  [  128/  130]
train() client id: f_00008-2-0 loss: 0.787204  [   32/  130]
train() client id: f_00008-2-1 loss: 0.809323  [   64/  130]
train() client id: f_00008-2-2 loss: 0.700319  [   96/  130]
train() client id: f_00008-2-3 loss: 0.704791  [  128/  130]
train() client id: f_00008-3-0 loss: 0.695568  [   32/  130]
train() client id: f_00008-3-1 loss: 0.745197  [   64/  130]
train() client id: f_00008-3-2 loss: 0.884872  [   96/  130]
train() client id: f_00008-3-3 loss: 0.675804  [  128/  130]
train() client id: f_00008-4-0 loss: 0.763154  [   32/  130]
train() client id: f_00008-4-1 loss: 0.687911  [   64/  130]
train() client id: f_00008-4-2 loss: 0.706070  [   96/  130]
train() client id: f_00008-4-3 loss: 0.843455  [  128/  130]
train() client id: f_00008-5-0 loss: 0.651867  [   32/  130]
train() client id: f_00008-5-1 loss: 0.892071  [   64/  130]
train() client id: f_00008-5-2 loss: 0.731492  [   96/  130]
train() client id: f_00008-5-3 loss: 0.730686  [  128/  130]
train() client id: f_00008-6-0 loss: 0.577981  [   32/  130]
train() client id: f_00008-6-1 loss: 0.803609  [   64/  130]
train() client id: f_00008-6-2 loss: 0.696338  [   96/  130]
train() client id: f_00008-6-3 loss: 0.881382  [  128/  130]
train() client id: f_00008-7-0 loss: 0.798288  [   32/  130]
train() client id: f_00008-7-1 loss: 0.720121  [   64/  130]
train() client id: f_00008-7-2 loss: 0.715356  [   96/  130]
train() client id: f_00008-7-3 loss: 0.765835  [  128/  130]
train() client id: f_00008-8-0 loss: 0.782668  [   32/  130]
train() client id: f_00008-8-1 loss: 0.818998  [   64/  130]
train() client id: f_00008-8-2 loss: 0.789978  [   96/  130]
train() client id: f_00008-8-3 loss: 0.604071  [  128/  130]
train() client id: f_00008-9-0 loss: 0.687310  [   32/  130]
train() client id: f_00008-9-1 loss: 0.846728  [   64/  130]
train() client id: f_00008-9-2 loss: 0.711955  [   96/  130]
train() client id: f_00008-9-3 loss: 0.750772  [  128/  130]
train() client id: f_00008-10-0 loss: 0.750603  [   32/  130]
train() client id: f_00008-10-1 loss: 0.763058  [   64/  130]
train() client id: f_00008-10-2 loss: 0.664605  [   96/  130]
train() client id: f_00008-10-3 loss: 0.822424  [  128/  130]
train() client id: f_00009-0-0 loss: 0.971988  [   32/  118]
train() client id: f_00009-0-1 loss: 1.018520  [   64/  118]
train() client id: f_00009-0-2 loss: 0.914603  [   96/  118]
train() client id: f_00009-1-0 loss: 1.013114  [   32/  118]
train() client id: f_00009-1-1 loss: 0.960060  [   64/  118]
train() client id: f_00009-1-2 loss: 0.937688  [   96/  118]
train() client id: f_00009-2-0 loss: 0.929454  [   32/  118]
train() client id: f_00009-2-1 loss: 0.931474  [   64/  118]
train() client id: f_00009-2-2 loss: 0.885983  [   96/  118]
train() client id: f_00009-3-0 loss: 0.930079  [   32/  118]
train() client id: f_00009-3-1 loss: 0.700241  [   64/  118]
train() client id: f_00009-3-2 loss: 0.986014  [   96/  118]
train() client id: f_00009-4-0 loss: 0.970258  [   32/  118]
train() client id: f_00009-4-1 loss: 0.924044  [   64/  118]
train() client id: f_00009-4-2 loss: 0.824974  [   96/  118]
train() client id: f_00009-5-0 loss: 0.938939  [   32/  118]
train() client id: f_00009-5-1 loss: 0.839190  [   64/  118]
train() client id: f_00009-5-2 loss: 0.867652  [   96/  118]
train() client id: f_00009-6-0 loss: 0.899751  [   32/  118]
train() client id: f_00009-6-1 loss: 0.817944  [   64/  118]
train() client id: f_00009-6-2 loss: 0.776272  [   96/  118]
train() client id: f_00009-7-0 loss: 0.804947  [   32/  118]
train() client id: f_00009-7-1 loss: 0.761183  [   64/  118]
train() client id: f_00009-7-2 loss: 0.797096  [   96/  118]
train() client id: f_00009-8-0 loss: 0.875310  [   32/  118]
train() client id: f_00009-8-1 loss: 0.901207  [   64/  118]
train() client id: f_00009-8-2 loss: 0.744817  [   96/  118]
train() client id: f_00009-9-0 loss: 0.854731  [   32/  118]
train() client id: f_00009-9-1 loss: 0.934453  [   64/  118]
train() client id: f_00009-9-2 loss: 0.745554  [   96/  118]
train() client id: f_00009-10-0 loss: 0.808513  [   32/  118]
train() client id: f_00009-10-1 loss: 0.834345  [   64/  118]
train() client id: f_00009-10-2 loss: 0.870780  [   96/  118]
At round 56 accuracy: 0.6472148541114059
At round 56 training accuracy: 0.5895372233400402
At round 56 training loss: 0.8233046646096877
update_location
xs = [  -3.9056584     4.20031788  300.00902392   18.81129433    0.97929623
    3.95640986 -262.44319194 -241.32485185  284.66397685 -227.06087855]
ys = [ 292.5879595   275.55583871    1.32061395 -262.45517586  254.35018685
  237.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [309.22963669 293.17002391 316.23908432 281.48993611 273.30381733
 258.0139986  280.86174452 261.22473155 302.22880354 248.13837757]
dists_bs = [207.60588129 205.4853538  505.76486823 478.46614164 193.04187331
 189.66025984 197.98218826 186.37821878 485.87600216 178.74794631]
uav_gains = [2.25068246e-12 3.05507187e-12 1.99030200e-12 3.87733494e-12
 4.60120712e-12 6.32147895e-12 3.92831823e-12 5.91917228e-12
 2.56156197e-12 7.69388670e-12]
bs_gains = [3.58940992e-11 3.69409145e-11 2.96645552e-12 3.46505949e-12
 4.40017369e-11 4.62338788e-11 4.09959494e-11 4.85498225e-11
 3.31911903e-12 5.45781883e-11]
Round 57
-------------------------------
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.48417658 7.05628152 3.44074209 1.26260335 8.13515315 3.91462212
 1.55207962 4.8313157  3.56521466 3.17400612]
obj_prev = 40.416194911724524
eta_min = 2.86092118758865e-27	eta_max = 0.9374658932840896
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 9.299045348130402	eta = 0.909090909090909
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 20.758435476455084	eta = 0.4072405937720066
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 14.636258285641127	eta = 0.5775846137877274
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.545044928479468	eta = 0.6241158766062833
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.479247501397975	eta = 0.6271624278976032
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979113930322	eta = 0.6271749156783474
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979109435532	eta = 0.6271749158874893
eta = 0.6271749158874893
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.03849692 0.08096575 0.03788586 0.01313784 0.09349252 0.04460754
 0.01649868 0.05469007 0.03971905 0.03605267]
ene_total = [1.32155705 2.05839072 1.33209932 0.6463361  2.34272368 1.20578025
 0.72268136 1.56297838 1.28498919 1.00144307]
ti_comp = [0.9850295  1.0924086  0.97437897 1.022215   1.09523156 1.09599371
 1.02294619 1.0429956  1.01299828 1.09844051]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [3.67501858e-06 2.77980148e-05 3.57977419e-06 1.35633586e-07
 4.25793239e-05 4.61837109e-06 2.68239096e-07 9.39810980e-06
 3.81644508e-06 2.42738867e-06]
ene_total = [0.42769316 0.17890492 0.45242289 0.34126134 0.17269288 0.17004157
 0.3395665  0.29322113 0.36274925 0.16430889]
optimize_network iter = 0 obj = 2.90286252803159
eta = 0.6271749158874893
freqs = [19540999.15308214 37058362.31216387 19441028.73626838  6426161.89579366
 42681624.07611285 20350273.65967661  8064295.3890079  26217785.78934446
 19604695.49055661 16410844.32139915]
eta_min = 0.6271749158874926	eta_max = 0.7063960382409014
af = 0.0018241050152146168	bf = 1.0814089668242106	zeta = 0.002006515516736079	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.50791650e-07 5.67902365e-06 7.31333600e-07 2.77094010e-08
 8.69878619e-06 9.43514808e-07 5.48001783e-08 1.91999638e-06
 7.79684519e-07 4.95905832e-07]
ene_total = [1.75372365 0.73159623 1.85514936 1.39952837 0.7050001  0.6970034
 1.39256766 1.20180996 1.48737301 0.67365931]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 1 obj = 3.685249982672195
eta = 0.7063960382409014
freqs = [19468958.52365561 35736956.5886699  19441028.73626838  6324869.42089178
 41128502.02279741 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527822]
eta_min = 0.7063960382409034	eta_max = 0.7063960382409006
af = 0.0017119208968572406	bf = 1.0814089668242106	zeta = 0.0018831129865429649	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.45266057e-07 5.28124555e-06 7.31333600e-07 2.68427459e-08
 8.07723219e-06 8.75740514e-07 5.30618587e-08 1.83637587e-06
 7.59730875e-07 4.59683582e-07]
ene_total = [1.75372312 0.73155835 1.85514936 1.39952829 0.7049409  0.69699695
 1.39256749 1.20180199 1.48737111 0.67365586]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 2 obj = 3.6852499826721847
eta = 0.7063960382409006
freqs = [19468958.52365561 35736956.5886699  19441028.73626836  6324869.42089178
 41128502.02279742 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527823]
Done!
At round 57 energy consumption: 3.685249982672195
At round 57 eta: 0.7063960382409006
At round 57 local rounds: 11.38151016928333
At round 57 global rounds: 29.486964437171718
At round 57 a_n: 8.314943732033974
gradient difference: 0.5021086931228638
train() client id: f_00000-0-0 loss: 1.147393  [   32/  126]
train() client id: f_00000-0-1 loss: 1.180180  [   64/  126]
train() client id: f_00000-0-2 loss: 1.063637  [   96/  126]
train() client id: f_00000-1-0 loss: 0.934255  [   32/  126]
train() client id: f_00000-1-1 loss: 1.211236  [   64/  126]
train() client id: f_00000-1-2 loss: 1.136402  [   96/  126]
train() client id: f_00000-2-0 loss: 1.049908  [   32/  126]
train() client id: f_00000-2-1 loss: 0.920538  [   64/  126]
train() client id: f_00000-2-2 loss: 0.916580  [   96/  126]
train() client id: f_00000-3-0 loss: 0.993118  [   32/  126]
train() client id: f_00000-3-1 loss: 0.865886  [   64/  126]
train() client id: f_00000-3-2 loss: 0.843767  [   96/  126]
train() client id: f_00000-4-0 loss: 0.965164  [   32/  126]
train() client id: f_00000-4-1 loss: 0.827774  [   64/  126]
train() client id: f_00000-4-2 loss: 0.838445  [   96/  126]
train() client id: f_00000-5-0 loss: 0.821065  [   32/  126]
train() client id: f_00000-5-1 loss: 0.899692  [   64/  126]
train() client id: f_00000-5-2 loss: 0.781876  [   96/  126]
train() client id: f_00000-6-0 loss: 0.774280  [   32/  126]
train() client id: f_00000-6-1 loss: 0.848836  [   64/  126]
train() client id: f_00000-6-2 loss: 0.853636  [   96/  126]
train() client id: f_00000-7-0 loss: 0.873016  [   32/  126]
train() client id: f_00000-7-1 loss: 0.863321  [   64/  126]
train() client id: f_00000-7-2 loss: 0.833377  [   96/  126]
train() client id: f_00000-8-0 loss: 0.779828  [   32/  126]
train() client id: f_00000-8-1 loss: 0.758683  [   64/  126]
train() client id: f_00000-8-2 loss: 0.874032  [   96/  126]
train() client id: f_00000-9-0 loss: 0.809901  [   32/  126]
train() client id: f_00000-9-1 loss: 0.781428  [   64/  126]
train() client id: f_00000-9-2 loss: 0.981641  [   96/  126]
train() client id: f_00000-10-0 loss: 0.741060  [   32/  126]
train() client id: f_00000-10-1 loss: 0.801675  [   64/  126]
train() client id: f_00000-10-2 loss: 0.858810  [   96/  126]
train() client id: f_00001-0-0 loss: 0.376483  [   32/  265]
train() client id: f_00001-0-1 loss: 0.391830  [   64/  265]
train() client id: f_00001-0-2 loss: 0.412556  [   96/  265]
train() client id: f_00001-0-3 loss: 0.591196  [  128/  265]
train() client id: f_00001-0-4 loss: 0.529193  [  160/  265]
train() client id: f_00001-0-5 loss: 0.403011  [  192/  265]
train() client id: f_00001-0-6 loss: 0.472254  [  224/  265]
train() client id: f_00001-0-7 loss: 0.440470  [  256/  265]
train() client id: f_00001-1-0 loss: 0.412267  [   32/  265]
train() client id: f_00001-1-1 loss: 0.425706  [   64/  265]
train() client id: f_00001-1-2 loss: 0.430781  [   96/  265]
train() client id: f_00001-1-3 loss: 0.510620  [  128/  265]
train() client id: f_00001-1-4 loss: 0.504818  [  160/  265]
train() client id: f_00001-1-5 loss: 0.470957  [  192/  265]
train() client id: f_00001-1-6 loss: 0.347902  [  224/  265]
train() client id: f_00001-1-7 loss: 0.410264  [  256/  265]
train() client id: f_00001-2-0 loss: 0.493051  [   32/  265]
train() client id: f_00001-2-1 loss: 0.386820  [   64/  265]
train() client id: f_00001-2-2 loss: 0.478380  [   96/  265]
train() client id: f_00001-2-3 loss: 0.409571  [  128/  265]
train() client id: f_00001-2-4 loss: 0.456930  [  160/  265]
train() client id: f_00001-2-5 loss: 0.422864  [  192/  265]
train() client id: f_00001-2-6 loss: 0.426381  [  224/  265]
train() client id: f_00001-2-7 loss: 0.424028  [  256/  265]
train() client id: f_00001-3-0 loss: 0.428001  [   32/  265]
train() client id: f_00001-3-1 loss: 0.430678  [   64/  265]
train() client id: f_00001-3-2 loss: 0.494068  [   96/  265]
train() client id: f_00001-3-3 loss: 0.353050  [  128/  265]
train() client id: f_00001-3-4 loss: 0.382854  [  160/  265]
train() client id: f_00001-3-5 loss: 0.545284  [  192/  265]
train() client id: f_00001-3-6 loss: 0.344152  [  224/  265]
train() client id: f_00001-3-7 loss: 0.449940  [  256/  265]
train() client id: f_00001-4-0 loss: 0.310218  [   32/  265]
train() client id: f_00001-4-1 loss: 0.438309  [   64/  265]
train() client id: f_00001-4-2 loss: 0.394232  [   96/  265]
train() client id: f_00001-4-3 loss: 0.391335  [  128/  265]
train() client id: f_00001-4-4 loss: 0.362109  [  160/  265]
train() client id: f_00001-4-5 loss: 0.488180  [  192/  265]
train() client id: f_00001-4-6 loss: 0.594383  [  224/  265]
train() client id: f_00001-4-7 loss: 0.340403  [  256/  265]
train() client id: f_00001-5-0 loss: 0.376433  [   32/  265]
train() client id: f_00001-5-1 loss: 0.495547  [   64/  265]
train() client id: f_00001-5-2 loss: 0.396491  [   96/  265]
train() client id: f_00001-5-3 loss: 0.357055  [  128/  265]
train() client id: f_00001-5-4 loss: 0.342231  [  160/  265]
train() client id: f_00001-5-5 loss: 0.462299  [  192/  265]
train() client id: f_00001-5-6 loss: 0.532152  [  224/  265]
train() client id: f_00001-5-7 loss: 0.461287  [  256/  265]
train() client id: f_00001-6-0 loss: 0.424666  [   32/  265]
train() client id: f_00001-6-1 loss: 0.446411  [   64/  265]
train() client id: f_00001-6-2 loss: 0.467867  [   96/  265]
train() client id: f_00001-6-3 loss: 0.394718  [  128/  265]
train() client id: f_00001-6-4 loss: 0.502379  [  160/  265]
train() client id: f_00001-6-5 loss: 0.372831  [  192/  265]
train() client id: f_00001-6-6 loss: 0.325028  [  224/  265]
train() client id: f_00001-6-7 loss: 0.475712  [  256/  265]
train() client id: f_00001-7-0 loss: 0.379076  [   32/  265]
train() client id: f_00001-7-1 loss: 0.387030  [   64/  265]
train() client id: f_00001-7-2 loss: 0.365311  [   96/  265]
train() client id: f_00001-7-3 loss: 0.549839  [  128/  265]
train() client id: f_00001-7-4 loss: 0.413658  [  160/  265]
train() client id: f_00001-7-5 loss: 0.424711  [  192/  265]
train() client id: f_00001-7-6 loss: 0.427688  [  224/  265]
train() client id: f_00001-7-7 loss: 0.455895  [  256/  265]
train() client id: f_00001-8-0 loss: 0.446717  [   32/  265]
train() client id: f_00001-8-1 loss: 0.435646  [   64/  265]
train() client id: f_00001-8-2 loss: 0.527260  [   96/  265]
train() client id: f_00001-8-3 loss: 0.420468  [  128/  265]
train() client id: f_00001-8-4 loss: 0.414422  [  160/  265]
train() client id: f_00001-8-5 loss: 0.379372  [  192/  265]
train() client id: f_00001-8-6 loss: 0.446461  [  224/  265]
train() client id: f_00001-8-7 loss: 0.328471  [  256/  265]
train() client id: f_00001-9-0 loss: 0.366809  [   32/  265]
train() client id: f_00001-9-1 loss: 0.379312  [   64/  265]
train() client id: f_00001-9-2 loss: 0.383062  [   96/  265]
train() client id: f_00001-9-3 loss: 0.478445  [  128/  265]
train() client id: f_00001-9-4 loss: 0.365148  [  160/  265]
train() client id: f_00001-9-5 loss: 0.397630  [  192/  265]
train() client id: f_00001-9-6 loss: 0.450277  [  224/  265]
train() client id: f_00001-9-7 loss: 0.578327  [  256/  265]
train() client id: f_00001-10-0 loss: 0.396545  [   32/  265]
train() client id: f_00001-10-1 loss: 0.516941  [   64/  265]
train() client id: f_00001-10-2 loss: 0.413158  [   96/  265]
train() client id: f_00001-10-3 loss: 0.333224  [  128/  265]
train() client id: f_00001-10-4 loss: 0.350549  [  160/  265]
train() client id: f_00001-10-5 loss: 0.414233  [  192/  265]
train() client id: f_00001-10-6 loss: 0.497072  [  224/  265]
train() client id: f_00001-10-7 loss: 0.381544  [  256/  265]
train() client id: f_00002-0-0 loss: 1.222613  [   32/  124]
train() client id: f_00002-0-1 loss: 1.075459  [   64/  124]
train() client id: f_00002-0-2 loss: 1.096320  [   96/  124]
train() client id: f_00002-1-0 loss: 1.257603  [   32/  124]
train() client id: f_00002-1-1 loss: 0.935345  [   64/  124]
train() client id: f_00002-1-2 loss: 0.995483  [   96/  124]
train() client id: f_00002-2-0 loss: 1.128072  [   32/  124]
train() client id: f_00002-2-1 loss: 1.158356  [   64/  124]
train() client id: f_00002-2-2 loss: 0.948781  [   96/  124]
train() client id: f_00002-3-0 loss: 0.984751  [   32/  124]
train() client id: f_00002-3-1 loss: 1.012710  [   64/  124]
train() client id: f_00002-3-2 loss: 1.104041  [   96/  124]
train() client id: f_00002-4-0 loss: 0.951253  [   32/  124]
train() client id: f_00002-4-1 loss: 1.026079  [   64/  124]
train() client id: f_00002-4-2 loss: 0.876486  [   96/  124]
train() client id: f_00002-5-0 loss: 1.114071  [   32/  124]
train() client id: f_00002-5-1 loss: 0.867630  [   64/  124]
train() client id: f_00002-5-2 loss: 1.125526  [   96/  124]
train() client id: f_00002-6-0 loss: 1.115680  [   32/  124]
train() client id: f_00002-6-1 loss: 1.032689  [   64/  124]
train() client id: f_00002-6-2 loss: 0.985322  [   96/  124]
train() client id: f_00002-7-0 loss: 0.833627  [   32/  124]
train() client id: f_00002-7-1 loss: 1.143038  [   64/  124]
train() client id: f_00002-7-2 loss: 1.077135  [   96/  124]
train() client id: f_00002-8-0 loss: 0.918129  [   32/  124]
train() client id: f_00002-8-1 loss: 1.015952  [   64/  124]
train() client id: f_00002-8-2 loss: 0.881196  [   96/  124]
train() client id: f_00002-9-0 loss: 0.983689  [   32/  124]
train() client id: f_00002-9-1 loss: 1.063736  [   64/  124]
train() client id: f_00002-9-2 loss: 0.810057  [   96/  124]
train() client id: f_00002-10-0 loss: 1.087885  [   32/  124]
train() client id: f_00002-10-1 loss: 0.997694  [   64/  124]
train() client id: f_00002-10-2 loss: 0.877096  [   96/  124]
train() client id: f_00003-0-0 loss: 0.498977  [   32/   43]
train() client id: f_00003-1-0 loss: 0.603173  [   32/   43]
train() client id: f_00003-2-0 loss: 0.839771  [   32/   43]
train() client id: f_00003-3-0 loss: 0.544859  [   32/   43]
train() client id: f_00003-4-0 loss: 0.714379  [   32/   43]
train() client id: f_00003-5-0 loss: 0.786141  [   32/   43]
train() client id: f_00003-6-0 loss: 0.577249  [   32/   43]
train() client id: f_00003-7-0 loss: 0.721637  [   32/   43]
train() client id: f_00003-8-0 loss: 0.669201  [   32/   43]
train() client id: f_00003-9-0 loss: 0.641562  [   32/   43]
train() client id: f_00003-10-0 loss: 0.579965  [   32/   43]
train() client id: f_00004-0-0 loss: 0.891156  [   32/  306]
train() client id: f_00004-0-1 loss: 0.884961  [   64/  306]
train() client id: f_00004-0-2 loss: 1.019512  [   96/  306]
train() client id: f_00004-0-3 loss: 0.868252  [  128/  306]
train() client id: f_00004-0-4 loss: 0.881426  [  160/  306]
train() client id: f_00004-0-5 loss: 0.950345  [  192/  306]
train() client id: f_00004-0-6 loss: 1.060377  [  224/  306]
train() client id: f_00004-0-7 loss: 0.885139  [  256/  306]
train() client id: f_00004-0-8 loss: 0.808042  [  288/  306]
train() client id: f_00004-1-0 loss: 0.912703  [   32/  306]
train() client id: f_00004-1-1 loss: 0.889203  [   64/  306]
train() client id: f_00004-1-2 loss: 0.945264  [   96/  306]
train() client id: f_00004-1-3 loss: 0.863206  [  128/  306]
train() client id: f_00004-1-4 loss: 0.841027  [  160/  306]
train() client id: f_00004-1-5 loss: 0.957357  [  192/  306]
train() client id: f_00004-1-6 loss: 1.108807  [  224/  306]
train() client id: f_00004-1-7 loss: 0.914741  [  256/  306]
train() client id: f_00004-1-8 loss: 0.851745  [  288/  306]
train() client id: f_00004-2-0 loss: 0.980585  [   32/  306]
train() client id: f_00004-2-1 loss: 0.905030  [   64/  306]
train() client id: f_00004-2-2 loss: 0.903719  [   96/  306]
train() client id: f_00004-2-3 loss: 0.886168  [  128/  306]
train() client id: f_00004-2-4 loss: 0.953598  [  160/  306]
train() client id: f_00004-2-5 loss: 1.061634  [  192/  306]
train() client id: f_00004-2-6 loss: 0.801983  [  224/  306]
train() client id: f_00004-2-7 loss: 0.933476  [  256/  306]
train() client id: f_00004-2-8 loss: 0.883701  [  288/  306]
train() client id: f_00004-3-0 loss: 0.926833  [   32/  306]
train() client id: f_00004-3-1 loss: 0.875544  [   64/  306]
train() client id: f_00004-3-2 loss: 1.004855  [   96/  306]
train() client id: f_00004-3-3 loss: 0.856053  [  128/  306]
train() client id: f_00004-3-4 loss: 1.002969  [  160/  306]
train() client id: f_00004-3-5 loss: 0.741848  [  192/  306]
train() client id: f_00004-3-6 loss: 0.929338  [  224/  306]
train() client id: f_00004-3-7 loss: 0.850154  [  256/  306]
train() client id: f_00004-3-8 loss: 1.007531  [  288/  306]
train() client id: f_00004-4-0 loss: 0.835390  [   32/  306]
train() client id: f_00004-4-1 loss: 0.943588  [   64/  306]
train() client id: f_00004-4-2 loss: 0.938297  [   96/  306]
train() client id: f_00004-4-3 loss: 0.824215  [  128/  306]
train() client id: f_00004-4-4 loss: 1.076266  [  160/  306]
train() client id: f_00004-4-5 loss: 0.886122  [  192/  306]
train() client id: f_00004-4-6 loss: 0.887282  [  224/  306]
train() client id: f_00004-4-7 loss: 0.823553  [  256/  306]
train() client id: f_00004-4-8 loss: 0.843806  [  288/  306]
train() client id: f_00004-5-0 loss: 1.036981  [   32/  306]
train() client id: f_00004-5-1 loss: 0.959750  [   64/  306]
train() client id: f_00004-5-2 loss: 0.851946  [   96/  306]
train() client id: f_00004-5-3 loss: 0.915269  [  128/  306]
train() client id: f_00004-5-4 loss: 0.898838  [  160/  306]
train() client id: f_00004-5-5 loss: 0.869204  [  192/  306]
train() client id: f_00004-5-6 loss: 0.989949  [  224/  306]
train() client id: f_00004-5-7 loss: 0.827898  [  256/  306]
train() client id: f_00004-5-8 loss: 0.872557  [  288/  306]
train() client id: f_00004-6-0 loss: 0.966389  [   32/  306]
train() client id: f_00004-6-1 loss: 0.771598  [   64/  306]
train() client id: f_00004-6-2 loss: 0.899470  [   96/  306]
train() client id: f_00004-6-3 loss: 0.888245  [  128/  306]
train() client id: f_00004-6-4 loss: 0.921233  [  160/  306]
train() client id: f_00004-6-5 loss: 1.031059  [  192/  306]
train() client id: f_00004-6-6 loss: 0.878292  [  224/  306]
train() client id: f_00004-6-7 loss: 0.920433  [  256/  306]
train() client id: f_00004-6-8 loss: 0.875578  [  288/  306]
train() client id: f_00004-7-0 loss: 0.927723  [   32/  306]
train() client id: f_00004-7-1 loss: 0.833427  [   64/  306]
train() client id: f_00004-7-2 loss: 0.911064  [   96/  306]
train() client id: f_00004-7-3 loss: 0.888399  [  128/  306]
train() client id: f_00004-7-4 loss: 0.980550  [  160/  306]
train() client id: f_00004-7-5 loss: 0.872862  [  192/  306]
train() client id: f_00004-7-6 loss: 0.678495  [  224/  306]
train() client id: f_00004-7-7 loss: 1.030610  [  256/  306]
train() client id: f_00004-7-8 loss: 0.899234  [  288/  306]
train() client id: f_00004-8-0 loss: 1.039101  [   32/  306]
train() client id: f_00004-8-1 loss: 0.853635  [   64/  306]
train() client id: f_00004-8-2 loss: 0.911984  [   96/  306]
train() client id: f_00004-8-3 loss: 0.886518  [  128/  306]
train() client id: f_00004-8-4 loss: 0.930350  [  160/  306]
train() client id: f_00004-8-5 loss: 0.777190  [  192/  306]
train() client id: f_00004-8-6 loss: 0.804412  [  224/  306]
train() client id: f_00004-8-7 loss: 0.915584  [  256/  306]
train() client id: f_00004-8-8 loss: 0.926898  [  288/  306]
train() client id: f_00004-9-0 loss: 0.926929  [   32/  306]
train() client id: f_00004-9-1 loss: 0.871807  [   64/  306]
train() client id: f_00004-9-2 loss: 0.890195  [   96/  306]
train() client id: f_00004-9-3 loss: 0.867175  [  128/  306]
train() client id: f_00004-9-4 loss: 0.985658  [  160/  306]
train() client id: f_00004-9-5 loss: 0.854867  [  192/  306]
train() client id: f_00004-9-6 loss: 0.850403  [  224/  306]
train() client id: f_00004-9-7 loss: 0.901793  [  256/  306]
train() client id: f_00004-9-8 loss: 0.937555  [  288/  306]
train() client id: f_00004-10-0 loss: 0.934965  [   32/  306]
train() client id: f_00004-10-1 loss: 0.893311  [   64/  306]
train() client id: f_00004-10-2 loss: 0.950879  [   96/  306]
train() client id: f_00004-10-3 loss: 0.962837  [  128/  306]
train() client id: f_00004-10-4 loss: 0.859614  [  160/  306]
train() client id: f_00004-10-5 loss: 0.789086  [  192/  306]
train() client id: f_00004-10-6 loss: 0.880607  [  224/  306]
train() client id: f_00004-10-7 loss: 0.911039  [  256/  306]
train() client id: f_00004-10-8 loss: 0.906913  [  288/  306]
train() client id: f_00005-0-0 loss: 0.312247  [   32/  146]
train() client id: f_00005-0-1 loss: 0.630880  [   64/  146]
train() client id: f_00005-0-2 loss: 0.395801  [   96/  146]
train() client id: f_00005-0-3 loss: 0.619576  [  128/  146]
train() client id: f_00005-1-0 loss: 0.488840  [   32/  146]
train() client id: f_00005-1-1 loss: 0.226932  [   64/  146]
train() client id: f_00005-1-2 loss: 0.350487  [   96/  146]
train() client id: f_00005-1-3 loss: 0.878463  [  128/  146]
train() client id: f_00005-2-0 loss: 0.643933  [   32/  146]
train() client id: f_00005-2-1 loss: 0.329349  [   64/  146]
train() client id: f_00005-2-2 loss: 0.387335  [   96/  146]
train() client id: f_00005-2-3 loss: 0.475003  [  128/  146]
train() client id: f_00005-3-0 loss: 0.570596  [   32/  146]
train() client id: f_00005-3-1 loss: 0.458650  [   64/  146]
train() client id: f_00005-3-2 loss: 0.559798  [   96/  146]
train() client id: f_00005-3-3 loss: 0.449759  [  128/  146]
train() client id: f_00005-4-0 loss: 0.614299  [   32/  146]
train() client id: f_00005-4-1 loss: 0.427534  [   64/  146]
train() client id: f_00005-4-2 loss: 0.650325  [   96/  146]
train() client id: f_00005-4-3 loss: 0.302883  [  128/  146]
train() client id: f_00005-5-0 loss: 0.435080  [   32/  146]
train() client id: f_00005-5-1 loss: 0.635716  [   64/  146]
train() client id: f_00005-5-2 loss: 0.398675  [   96/  146]
train() client id: f_00005-5-3 loss: 0.396161  [  128/  146]
train() client id: f_00005-6-0 loss: 0.571957  [   32/  146]
train() client id: f_00005-6-1 loss: 0.480015  [   64/  146]
train() client id: f_00005-6-2 loss: 0.455240  [   96/  146]
train() client id: f_00005-6-3 loss: 0.250646  [  128/  146]
train() client id: f_00005-7-0 loss: 0.391720  [   32/  146]
train() client id: f_00005-7-1 loss: 0.455593  [   64/  146]
train() client id: f_00005-7-2 loss: 0.533213  [   96/  146]
train() client id: f_00005-7-3 loss: 0.434520  [  128/  146]
train() client id: f_00005-8-0 loss: 0.249375  [   32/  146]
train() client id: f_00005-8-1 loss: 0.517579  [   64/  146]
train() client id: f_00005-8-2 loss: 0.696117  [   96/  146]
train() client id: f_00005-8-3 loss: 0.374965  [  128/  146]
train() client id: f_00005-9-0 loss: 0.211480  [   32/  146]
train() client id: f_00005-9-1 loss: 0.748069  [   64/  146]
train() client id: f_00005-9-2 loss: 0.507412  [   96/  146]
train() client id: f_00005-9-3 loss: 0.386933  [  128/  146]
train() client id: f_00005-10-0 loss: 0.119409  [   32/  146]
train() client id: f_00005-10-1 loss: 0.497282  [   64/  146]
train() client id: f_00005-10-2 loss: 0.844543  [   96/  146]
train() client id: f_00005-10-3 loss: 0.462023  [  128/  146]
train() client id: f_00006-0-0 loss: 0.414413  [   32/   54]
train() client id: f_00006-1-0 loss: 0.488160  [   32/   54]
train() client id: f_00006-2-0 loss: 0.449062  [   32/   54]
train() client id: f_00006-3-0 loss: 0.412122  [   32/   54]
train() client id: f_00006-4-0 loss: 0.441787  [   32/   54]
train() client id: f_00006-5-0 loss: 0.439971  [   32/   54]
train() client id: f_00006-6-0 loss: 0.497860  [   32/   54]
train() client id: f_00006-7-0 loss: 0.495658  [   32/   54]
train() client id: f_00006-8-0 loss: 0.485652  [   32/   54]
train() client id: f_00006-9-0 loss: 0.457063  [   32/   54]
train() client id: f_00006-10-0 loss: 0.442649  [   32/   54]
train() client id: f_00007-0-0 loss: 0.527982  [   32/  179]
train() client id: f_00007-0-1 loss: 0.618944  [   64/  179]
train() client id: f_00007-0-2 loss: 0.709261  [   96/  179]
train() client id: f_00007-0-3 loss: 0.690539  [  128/  179]
train() client id: f_00007-0-4 loss: 0.846053  [  160/  179]
train() client id: f_00007-1-0 loss: 0.717620  [   32/  179]
train() client id: f_00007-1-1 loss: 0.573611  [   64/  179]
train() client id: f_00007-1-2 loss: 0.796320  [   96/  179]
train() client id: f_00007-1-3 loss: 0.665310  [  128/  179]
train() client id: f_00007-1-4 loss: 0.552658  [  160/  179]
train() client id: f_00007-2-0 loss: 0.729775  [   32/  179]
train() client id: f_00007-2-1 loss: 0.489167  [   64/  179]
train() client id: f_00007-2-2 loss: 0.629601  [   96/  179]
train() client id: f_00007-2-3 loss: 0.620279  [  128/  179]
train() client id: f_00007-2-4 loss: 0.610264  [  160/  179]
train() client id: f_00007-3-0 loss: 0.540924  [   32/  179]
train() client id: f_00007-3-1 loss: 0.790931  [   64/  179]
train() client id: f_00007-3-2 loss: 0.663103  [   96/  179]
train() client id: f_00007-3-3 loss: 0.474849  [  128/  179]
train() client id: f_00007-3-4 loss: 0.626072  [  160/  179]
train() client id: f_00007-4-0 loss: 0.644542  [   32/  179]
train() client id: f_00007-4-1 loss: 0.612530  [   64/  179]
train() client id: f_00007-4-2 loss: 0.664795  [   96/  179]
train() client id: f_00007-4-3 loss: 0.578743  [  128/  179]
train() client id: f_00007-4-4 loss: 0.573869  [  160/  179]
train() client id: f_00007-5-0 loss: 0.636288  [   32/  179]
train() client id: f_00007-5-1 loss: 0.627144  [   64/  179]
train() client id: f_00007-5-2 loss: 0.593240  [   96/  179]
train() client id: f_00007-5-3 loss: 0.642824  [  128/  179]
train() client id: f_00007-5-4 loss: 0.555361  [  160/  179]
train() client id: f_00007-6-0 loss: 0.573129  [   32/  179]
train() client id: f_00007-6-1 loss: 0.662296  [   64/  179]
train() client id: f_00007-6-2 loss: 0.739348  [   96/  179]
train() client id: f_00007-6-3 loss: 0.565365  [  128/  179]
train() client id: f_00007-6-4 loss: 0.643864  [  160/  179]
train() client id: f_00007-7-0 loss: 0.670623  [   32/  179]
train() client id: f_00007-7-1 loss: 0.554870  [   64/  179]
train() client id: f_00007-7-2 loss: 0.703374  [   96/  179]
train() client id: f_00007-7-3 loss: 0.523731  [  128/  179]
train() client id: f_00007-7-4 loss: 0.565640  [  160/  179]
train() client id: f_00007-8-0 loss: 0.590045  [   32/  179]
train() client id: f_00007-8-1 loss: 0.579482  [   64/  179]
train() client id: f_00007-8-2 loss: 0.822800  [   96/  179]
train() client id: f_00007-8-3 loss: 0.567958  [  128/  179]
train() client id: f_00007-8-4 loss: 0.449575  [  160/  179]
train() client id: f_00007-9-0 loss: 0.472142  [   32/  179]
train() client id: f_00007-9-1 loss: 0.636201  [   64/  179]
train() client id: f_00007-9-2 loss: 0.590953  [   96/  179]
train() client id: f_00007-9-3 loss: 0.644269  [  128/  179]
train() client id: f_00007-9-4 loss: 0.663607  [  160/  179]
train() client id: f_00007-10-0 loss: 0.501822  [   32/  179]
train() client id: f_00007-10-1 loss: 0.538171  [   64/  179]
train() client id: f_00007-10-2 loss: 0.539185  [   96/  179]
train() client id: f_00007-10-3 loss: 0.649518  [  128/  179]
train() client id: f_00007-10-4 loss: 0.923923  [  160/  179]
train() client id: f_00008-0-0 loss: 0.630970  [   32/  130]
train() client id: f_00008-0-1 loss: 0.637636  [   64/  130]
train() client id: f_00008-0-2 loss: 0.609728  [   96/  130]
train() client id: f_00008-0-3 loss: 0.750705  [  128/  130]
train() client id: f_00008-1-0 loss: 0.543490  [   32/  130]
train() client id: f_00008-1-1 loss: 0.802810  [   64/  130]
train() client id: f_00008-1-2 loss: 0.617452  [   96/  130]
train() client id: f_00008-1-3 loss: 0.652237  [  128/  130]
train() client id: f_00008-2-0 loss: 0.766048  [   32/  130]
train() client id: f_00008-2-1 loss: 0.689708  [   64/  130]
train() client id: f_00008-2-2 loss: 0.631902  [   96/  130]
train() client id: f_00008-2-3 loss: 0.549819  [  128/  130]
train() client id: f_00008-3-0 loss: 0.710784  [   32/  130]
train() client id: f_00008-3-1 loss: 0.562113  [   64/  130]
train() client id: f_00008-3-2 loss: 0.618966  [   96/  130]
train() client id: f_00008-3-3 loss: 0.754386  [  128/  130]
train() client id: f_00008-4-0 loss: 0.684487  [   32/  130]
train() client id: f_00008-4-1 loss: 0.670987  [   64/  130]
train() client id: f_00008-4-2 loss: 0.618075  [   96/  130]
train() client id: f_00008-4-3 loss: 0.666117  [  128/  130]
train() client id: f_00008-5-0 loss: 0.728278  [   32/  130]
train() client id: f_00008-5-1 loss: 0.597641  [   64/  130]
train() client id: f_00008-5-2 loss: 0.678785  [   96/  130]
train() client id: f_00008-5-3 loss: 0.637221  [  128/  130]
train() client id: f_00008-6-0 loss: 0.485043  [   32/  130]
train() client id: f_00008-6-1 loss: 0.713023  [   64/  130]
train() client id: f_00008-6-2 loss: 0.782424  [   96/  130]
train() client id: f_00008-6-3 loss: 0.631729  [  128/  130]
train() client id: f_00008-7-0 loss: 0.603722  [   32/  130]
train() client id: f_00008-7-1 loss: 0.615871  [   64/  130]
train() client id: f_00008-7-2 loss: 0.688288  [   96/  130]
train() client id: f_00008-7-3 loss: 0.743731  [  128/  130]
train() client id: f_00008-8-0 loss: 0.643964  [   32/  130]
train() client id: f_00008-8-1 loss: 0.685768  [   64/  130]
train() client id: f_00008-8-2 loss: 0.546408  [   96/  130]
train() client id: f_00008-8-3 loss: 0.774062  [  128/  130]
train() client id: f_00008-9-0 loss: 0.760758  [   32/  130]
train() client id: f_00008-9-1 loss: 0.715308  [   64/  130]
train() client id: f_00008-9-2 loss: 0.610011  [   96/  130]
train() client id: f_00008-9-3 loss: 0.557313  [  128/  130]
train() client id: f_00008-10-0 loss: 0.699776  [   32/  130]
train() client id: f_00008-10-1 loss: 0.624480  [   64/  130]
train() client id: f_00008-10-2 loss: 0.648866  [   96/  130]
train() client id: f_00008-10-3 loss: 0.656071  [  128/  130]
train() client id: f_00009-0-0 loss: 1.055065  [   32/  118]
train() client id: f_00009-0-1 loss: 0.820700  [   64/  118]
train() client id: f_00009-0-2 loss: 0.894201  [   96/  118]
train() client id: f_00009-1-0 loss: 0.985396  [   32/  118]
train() client id: f_00009-1-1 loss: 0.699296  [   64/  118]
train() client id: f_00009-1-2 loss: 0.981362  [   96/  118]
train() client id: f_00009-2-0 loss: 0.814615  [   32/  118]
train() client id: f_00009-2-1 loss: 0.796671  [   64/  118]
train() client id: f_00009-2-2 loss: 1.062557  [   96/  118]
train() client id: f_00009-3-0 loss: 0.846505  [   32/  118]
train() client id: f_00009-3-1 loss: 0.841646  [   64/  118]
train() client id: f_00009-3-2 loss: 0.982784  [   96/  118]
train() client id: f_00009-4-0 loss: 0.921671  [   32/  118]
train() client id: f_00009-4-1 loss: 0.771552  [   64/  118]
train() client id: f_00009-4-2 loss: 0.730406  [   96/  118]
train() client id: f_00009-5-0 loss: 0.845390  [   32/  118]
train() client id: f_00009-5-1 loss: 0.938089  [   64/  118]
train() client id: f_00009-5-2 loss: 0.727515  [   96/  118]
train() client id: f_00009-6-0 loss: 0.812931  [   32/  118]
train() client id: f_00009-6-1 loss: 0.753138  [   64/  118]
train() client id: f_00009-6-2 loss: 0.791331  [   96/  118]
train() client id: f_00009-7-0 loss: 0.802630  [   32/  118]
train() client id: f_00009-7-1 loss: 0.799047  [   64/  118]
train() client id: f_00009-7-2 loss: 0.797930  [   96/  118]
train() client id: f_00009-8-0 loss: 0.805046  [   32/  118]
train() client id: f_00009-8-1 loss: 0.811114  [   64/  118]
train() client id: f_00009-8-2 loss: 0.814319  [   96/  118]
train() client id: f_00009-9-0 loss: 0.642461  [   32/  118]
train() client id: f_00009-9-1 loss: 1.089059  [   64/  118]
train() client id: f_00009-9-2 loss: 0.752106  [   96/  118]
train() client id: f_00009-10-0 loss: 0.919547  [   32/  118]
train() client id: f_00009-10-1 loss: 0.711965  [   64/  118]
train() client id: f_00009-10-2 loss: 0.841550  [   96/  118]
At round 57 accuracy: 0.6472148541114059
At round 57 training accuracy: 0.5915492957746479
At round 57 training loss: 0.8369183203195887
update_location
xs = [  -3.9056584     4.20031788  305.00902392   18.81129433    0.97929623
    3.95640986 -267.44319194 -246.32485185  289.66397685 -232.06087855]
ys = [ 297.5879595   280.55583871    1.32061395 -267.45517586  259.35018685
  242.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [313.96472382 297.87450596 320.98636839 286.15753684 277.96308826
 262.62971077 285.53940438 265.85072671 306.94281138 252.721711  ]
dists_bs = [210.47822106 207.97785702 510.4637031  483.04182061 195.15036951
 191.37438614 200.24080201 188.21553855 490.60893718 180.26768178]
uav_gains = [2.06976062e-12 2.78476917e-12 1.83844324e-12 3.52095540e-12
 4.17321580e-12 5.75002013e-12 3.56593276e-12 5.37807949e-12
 2.34619438e-12 7.03198726e-12]
bs_gains = [3.45393392e-11 3.57146344e-11 2.89062969e-12 3.37393595e-12
 4.26834807e-11 4.50836859e-11 3.97142964e-11 4.72344384e-11
 3.23024015e-12 5.32996109e-11]
Round 58
-------------------------------
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.3523265  6.77756785 3.31084036 1.21735079 7.81367428 3.76005882
 1.49535056 4.64371042 3.4255904  3.04870737]
obj_prev = 38.84517734952248
eta_min = 2.470941254922374e-28	eta_max = 0.9375517380484754
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 8.931115437766236	eta = 0.909090909090909
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 20.2431781023763	eta = 0.40108306173335817
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 14.165631330058979	eta = 0.5731615953668162
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.085726257080514	eta = 0.6204619975235665
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.020212302687447	eta = 0.623583983407007
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.019941071601606	eta = 0.6235969738929089
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.01994106692125	eta = 0.623596974117077
eta = 0.623596974117077
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.03896902 0.08195865 0.03835046 0.01329895 0.09463905 0.04515458
 0.01670101 0.05536075 0.04020613 0.0364948 ]
ene_total = [1.28342906 1.9813468  1.29389683 0.63079383 2.25502199 1.16000083
 0.7042715  1.51102723 1.23695895 0.96319404]
ti_comp = [1.03546301 1.14942413 1.02453888 1.07419893 1.1523401  1.15319241
 1.07495744 1.09635059 1.06887034 1.15568559]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [3.44960441e-06 2.60437692e-05 3.35842002e-06 1.27398049e-07
 3.98960382e-05 4.32694980e-06 2.51956506e-07 8.82240566e-06
 3.55555046e-06 2.27453939e-06]
ene_total = [0.42265881 0.17141173 0.4467888  0.33701571 0.16527621 0.16260767
 0.33534287 0.28827359 0.34886259 0.15705475]
optimize_network iter = 0 obj = 2.83529273119976
eta = 0.623596974117077
freqs = [18817195.6470524  35652050.01875965 18715963.43024579  6190171.56284024
 41063853.09945352 19578076.71969618  7768218.52091367 25247740.71552675
 18807768.58899782 15789240.96805261]
eta_min = 0.6235969741170779	eta_max = 0.7095637697055857
af = 0.0016217254572042322	bf = 1.0664469018281375	zeta = 0.0017838980029246557	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.96202700e-07 5.25618019e-06 6.77799772e-07 2.57116048e-08
 8.05185933e-06 8.73269445e-07 5.08501203e-08 1.78054696e-06
 7.17584839e-07 4.59049871e-07]
ene_total = [1.74973877 0.70781749 1.84965402 1.39538178 0.68140253 0.67295033
 1.38844638 1.19293349 1.4441827  0.65010868]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 1 obj = 3.6736658995998672
eta = 0.7095637697055857
freqs = [18742766.932176   34251295.00082695 18715963.43024578  6084298.06393133
 39418319.48246514 18789072.00779956  7633454.28294466 24640242.31966294
 18518666.61499257 15142443.926058  ]
eta_min = 0.7095637697056919	eta_max = 0.7095637697055788
af = 0.0015119964848909963	bf = 1.0664469018281375	zeta = 0.001663196133380096	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.90706132e-07 4.85126751e-06 6.77799772e-07 2.48396102e-08
 7.41947189e-06 8.04301492e-07 4.91011128e-08 1.69589257e-06
 6.95693805e-07 4.22210775e-07]
ene_total = [1.74973827 0.70778046 1.84965402 1.3953817  0.68134469 0.67294402
 1.38844622 1.19292575 1.44418069 0.65010531]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 2 obj = 3.67366589959978
eta = 0.7095637697055788
freqs = [18742766.93217598 34251295.00082704 18715963.43024575  6084298.06393133
 39418319.48246525 18789072.00779961  7633454.28294467 24640242.31966297
 18518666.61499257 15142443.92605804]
Done!
At round 58 energy consumption: 3.6736658995998672
At round 58 eta: 0.7095637697055788
At round 58 local rounds: 11.234997825322132
At round 58 global rounds: 28.629154577598474
At round 58 a_n: 7.972397885064655
gradient difference: 0.38230738043785095
train() client id: f_00000-0-0 loss: 1.255937  [   32/  126]
train() client id: f_00000-0-1 loss: 1.191579  [   64/  126]
train() client id: f_00000-0-2 loss: 0.944910  [   96/  126]
train() client id: f_00000-1-0 loss: 0.990960  [   32/  126]
train() client id: f_00000-1-1 loss: 1.005724  [   64/  126]
train() client id: f_00000-1-2 loss: 1.185333  [   96/  126]
train() client id: f_00000-2-0 loss: 1.025776  [   32/  126]
train() client id: f_00000-2-1 loss: 0.811524  [   64/  126]
train() client id: f_00000-2-2 loss: 0.931339  [   96/  126]
train() client id: f_00000-3-0 loss: 0.947495  [   32/  126]
train() client id: f_00000-3-1 loss: 0.806632  [   64/  126]
train() client id: f_00000-3-2 loss: 0.822214  [   96/  126]
train() client id: f_00000-4-0 loss: 0.825816  [   32/  126]
train() client id: f_00000-4-1 loss: 0.862862  [   64/  126]
train() client id: f_00000-4-2 loss: 0.934413  [   96/  126]
train() client id: f_00000-5-0 loss: 0.821376  [   32/  126]
train() client id: f_00000-5-1 loss: 0.916304  [   64/  126]
train() client id: f_00000-5-2 loss: 0.830251  [   96/  126]
train() client id: f_00000-6-0 loss: 0.969534  [   32/  126]
train() client id: f_00000-6-1 loss: 0.718844  [   64/  126]
train() client id: f_00000-6-2 loss: 0.755651  [   96/  126]
train() client id: f_00000-7-0 loss: 0.758726  [   32/  126]
train() client id: f_00000-7-1 loss: 0.946024  [   64/  126]
train() client id: f_00000-7-2 loss: 0.760991  [   96/  126]
train() client id: f_00000-8-0 loss: 0.921473  [   32/  126]
train() client id: f_00000-8-1 loss: 0.826705  [   64/  126]
train() client id: f_00000-8-2 loss: 0.716997  [   96/  126]
train() client id: f_00000-9-0 loss: 0.772603  [   32/  126]
train() client id: f_00000-9-1 loss: 0.810376  [   64/  126]
train() client id: f_00000-9-2 loss: 0.823714  [   96/  126]
train() client id: f_00000-10-0 loss: 0.769463  [   32/  126]
train() client id: f_00000-10-1 loss: 0.876087  [   64/  126]
train() client id: f_00000-10-2 loss: 0.757435  [   96/  126]
train() client id: f_00001-0-0 loss: 0.496581  [   32/  265]
train() client id: f_00001-0-1 loss: 0.370676  [   64/  265]
train() client id: f_00001-0-2 loss: 0.437933  [   96/  265]
train() client id: f_00001-0-3 loss: 0.429156  [  128/  265]
train() client id: f_00001-0-4 loss: 0.355132  [  160/  265]
train() client id: f_00001-0-5 loss: 0.550300  [  192/  265]
train() client id: f_00001-0-6 loss: 0.389431  [  224/  265]
train() client id: f_00001-0-7 loss: 0.589395  [  256/  265]
train() client id: f_00001-1-0 loss: 0.478369  [   32/  265]
train() client id: f_00001-1-1 loss: 0.423923  [   64/  265]
train() client id: f_00001-1-2 loss: 0.462146  [   96/  265]
train() client id: f_00001-1-3 loss: 0.644139  [  128/  265]
train() client id: f_00001-1-4 loss: 0.473795  [  160/  265]
train() client id: f_00001-1-5 loss: 0.342994  [  192/  265]
train() client id: f_00001-1-6 loss: 0.404912  [  224/  265]
train() client id: f_00001-1-7 loss: 0.356167  [  256/  265]
train() client id: f_00001-2-0 loss: 0.450062  [   32/  265]
train() client id: f_00001-2-1 loss: 0.446296  [   64/  265]
train() client id: f_00001-2-2 loss: 0.501386  [   96/  265]
train() client id: f_00001-2-3 loss: 0.436822  [  128/  265]
train() client id: f_00001-2-4 loss: 0.458936  [  160/  265]
train() client id: f_00001-2-5 loss: 0.398229  [  192/  265]
train() client id: f_00001-2-6 loss: 0.456896  [  224/  265]
train() client id: f_00001-2-7 loss: 0.391597  [  256/  265]
train() client id: f_00001-3-0 loss: 0.443369  [   32/  265]
train() client id: f_00001-3-1 loss: 0.487891  [   64/  265]
train() client id: f_00001-3-2 loss: 0.464151  [   96/  265]
train() client id: f_00001-3-3 loss: 0.469966  [  128/  265]
train() client id: f_00001-3-4 loss: 0.443128  [  160/  265]
train() client id: f_00001-3-5 loss: 0.412452  [  192/  265]
train() client id: f_00001-3-6 loss: 0.389836  [  224/  265]
train() client id: f_00001-3-7 loss: 0.392818  [  256/  265]
train() client id: f_00001-4-0 loss: 0.495218  [   32/  265]
train() client id: f_00001-4-1 loss: 0.341149  [   64/  265]
train() client id: f_00001-4-2 loss: 0.403286  [   96/  265]
train() client id: f_00001-4-3 loss: 0.396624  [  128/  265]
train() client id: f_00001-4-4 loss: 0.473403  [  160/  265]
train() client id: f_00001-4-5 loss: 0.365999  [  192/  265]
train() client id: f_00001-4-6 loss: 0.407911  [  224/  265]
train() client id: f_00001-4-7 loss: 0.593379  [  256/  265]
train() client id: f_00001-5-0 loss: 0.394512  [   32/  265]
train() client id: f_00001-5-1 loss: 0.350637  [   64/  265]
train() client id: f_00001-5-2 loss: 0.483281  [   96/  265]
train() client id: f_00001-5-3 loss: 0.390776  [  128/  265]
train() client id: f_00001-5-4 loss: 0.363462  [  160/  265]
train() client id: f_00001-5-5 loss: 0.447694  [  192/  265]
train() client id: f_00001-5-6 loss: 0.566260  [  224/  265]
train() client id: f_00001-5-7 loss: 0.372260  [  256/  265]
train() client id: f_00001-6-0 loss: 0.377586  [   32/  265]
train() client id: f_00001-6-1 loss: 0.396689  [   64/  265]
train() client id: f_00001-6-2 loss: 0.634709  [   96/  265]
train() client id: f_00001-6-3 loss: 0.475720  [  128/  265]
train() client id: f_00001-6-4 loss: 0.420127  [  160/  265]
train() client id: f_00001-6-5 loss: 0.320536  [  192/  265]
train() client id: f_00001-6-6 loss: 0.432789  [  224/  265]
train() client id: f_00001-6-7 loss: 0.336461  [  256/  265]
train() client id: f_00001-7-0 loss: 0.413366  [   32/  265]
train() client id: f_00001-7-1 loss: 0.578476  [   64/  265]
train() client id: f_00001-7-2 loss: 0.359552  [   96/  265]
train() client id: f_00001-7-3 loss: 0.408253  [  128/  265]
train() client id: f_00001-7-4 loss: 0.351764  [  160/  265]
train() client id: f_00001-7-5 loss: 0.335365  [  192/  265]
train() client id: f_00001-7-6 loss: 0.521380  [  224/  265]
train() client id: f_00001-7-7 loss: 0.474426  [  256/  265]
train() client id: f_00001-8-0 loss: 0.452313  [   32/  265]
train() client id: f_00001-8-1 loss: 0.345473  [   64/  265]
train() client id: f_00001-8-2 loss: 0.505767  [   96/  265]
train() client id: f_00001-8-3 loss: 0.472414  [  128/  265]
train() client id: f_00001-8-4 loss: 0.403482  [  160/  265]
train() client id: f_00001-8-5 loss: 0.379975  [  192/  265]
train() client id: f_00001-8-6 loss: 0.334944  [  224/  265]
train() client id: f_00001-8-7 loss: 0.549411  [  256/  265]
train() client id: f_00001-9-0 loss: 0.508306  [   32/  265]
train() client id: f_00001-9-1 loss: 0.455761  [   64/  265]
train() client id: f_00001-9-2 loss: 0.388917  [   96/  265]
train() client id: f_00001-9-3 loss: 0.367523  [  128/  265]
train() client id: f_00001-9-4 loss: 0.577057  [  160/  265]
train() client id: f_00001-9-5 loss: 0.408104  [  192/  265]
train() client id: f_00001-9-6 loss: 0.334480  [  224/  265]
train() client id: f_00001-9-7 loss: 0.372176  [  256/  265]
train() client id: f_00001-10-0 loss: 0.397603  [   32/  265]
train() client id: f_00001-10-1 loss: 0.350674  [   64/  265]
train() client id: f_00001-10-2 loss: 0.598527  [   96/  265]
train() client id: f_00001-10-3 loss: 0.524281  [  128/  265]
train() client id: f_00001-10-4 loss: 0.366319  [  160/  265]
train() client id: f_00001-10-5 loss: 0.395024  [  192/  265]
train() client id: f_00001-10-6 loss: 0.340950  [  224/  265]
train() client id: f_00001-10-7 loss: 0.407778  [  256/  265]
train() client id: f_00002-0-0 loss: 1.328671  [   32/  124]
train() client id: f_00002-0-1 loss: 1.160573  [   64/  124]
train() client id: f_00002-0-2 loss: 1.380669  [   96/  124]
train() client id: f_00002-1-0 loss: 1.107695  [   32/  124]
train() client id: f_00002-1-1 loss: 1.160077  [   64/  124]
train() client id: f_00002-1-2 loss: 1.365251  [   96/  124]
train() client id: f_00002-2-0 loss: 1.299555  [   32/  124]
train() client id: f_00002-2-1 loss: 1.364917  [   64/  124]
train() client id: f_00002-2-2 loss: 1.049385  [   96/  124]
train() client id: f_00002-3-0 loss: 1.289354  [   32/  124]
train() client id: f_00002-3-1 loss: 1.015980  [   64/  124]
train() client id: f_00002-3-2 loss: 1.177999  [   96/  124]
train() client id: f_00002-4-0 loss: 1.053872  [   32/  124]
train() client id: f_00002-4-1 loss: 1.120281  [   64/  124]
train() client id: f_00002-4-2 loss: 1.104327  [   96/  124]
train() client id: f_00002-5-0 loss: 1.340547  [   32/  124]
train() client id: f_00002-5-1 loss: 1.060278  [   64/  124]
train() client id: f_00002-5-2 loss: 0.807859  [   96/  124]
train() client id: f_00002-6-0 loss: 1.063552  [   32/  124]
train() client id: f_00002-6-1 loss: 0.971793  [   64/  124]
train() client id: f_00002-6-2 loss: 1.198505  [   96/  124]
train() client id: f_00002-7-0 loss: 0.915034  [   32/  124]
train() client id: f_00002-7-1 loss: 1.109596  [   64/  124]
train() client id: f_00002-7-2 loss: 1.260011  [   96/  124]
train() client id: f_00002-8-0 loss: 1.104045  [   32/  124]
train() client id: f_00002-8-1 loss: 1.010902  [   64/  124]
train() client id: f_00002-8-2 loss: 1.075142  [   96/  124]
train() client id: f_00002-9-0 loss: 0.962777  [   32/  124]
train() client id: f_00002-9-1 loss: 1.113371  [   64/  124]
train() client id: f_00002-9-2 loss: 0.954421  [   96/  124]
train() client id: f_00002-10-0 loss: 1.100288  [   32/  124]
train() client id: f_00002-10-1 loss: 1.024294  [   64/  124]
train() client id: f_00002-10-2 loss: 1.011443  [   96/  124]
train() client id: f_00003-0-0 loss: 0.648288  [   32/   43]
train() client id: f_00003-1-0 loss: 0.916628  [   32/   43]
train() client id: f_00003-2-0 loss: 0.567262  [   32/   43]
train() client id: f_00003-3-0 loss: 0.700516  [   32/   43]
train() client id: f_00003-4-0 loss: 0.823767  [   32/   43]
train() client id: f_00003-5-0 loss: 0.838263  [   32/   43]
train() client id: f_00003-6-0 loss: 0.678117  [   32/   43]
train() client id: f_00003-7-0 loss: 0.834747  [   32/   43]
train() client id: f_00003-8-0 loss: 0.790004  [   32/   43]
train() client id: f_00003-9-0 loss: 0.661940  [   32/   43]
train() client id: f_00003-10-0 loss: 0.874788  [   32/   43]
train() client id: f_00004-0-0 loss: 0.730749  [   32/  306]
train() client id: f_00004-0-1 loss: 0.933456  [   64/  306]
train() client id: f_00004-0-2 loss: 0.892402  [   96/  306]
train() client id: f_00004-0-3 loss: 0.909565  [  128/  306]
train() client id: f_00004-0-4 loss: 0.946201  [  160/  306]
train() client id: f_00004-0-5 loss: 0.953183  [  192/  306]
train() client id: f_00004-0-6 loss: 0.905083  [  224/  306]
train() client id: f_00004-0-7 loss: 0.945725  [  256/  306]
train() client id: f_00004-0-8 loss: 0.823233  [  288/  306]
train() client id: f_00004-1-0 loss: 0.916621  [   32/  306]
train() client id: f_00004-1-1 loss: 0.743151  [   64/  306]
train() client id: f_00004-1-2 loss: 0.885057  [   96/  306]
train() client id: f_00004-1-3 loss: 0.787242  [  128/  306]
train() client id: f_00004-1-4 loss: 0.991595  [  160/  306]
train() client id: f_00004-1-5 loss: 0.845063  [  192/  306]
train() client id: f_00004-1-6 loss: 0.966679  [  224/  306]
train() client id: f_00004-1-7 loss: 0.864517  [  256/  306]
train() client id: f_00004-1-8 loss: 0.912367  [  288/  306]
train() client id: f_00004-2-0 loss: 0.830441  [   32/  306]
train() client id: f_00004-2-1 loss: 0.812288  [   64/  306]
train() client id: f_00004-2-2 loss: 0.989287  [   96/  306]
train() client id: f_00004-2-3 loss: 0.947606  [  128/  306]
train() client id: f_00004-2-4 loss: 0.849343  [  160/  306]
train() client id: f_00004-2-5 loss: 0.811304  [  192/  306]
train() client id: f_00004-2-6 loss: 0.884578  [  224/  306]
train() client id: f_00004-2-7 loss: 0.817834  [  256/  306]
train() client id: f_00004-2-8 loss: 0.929861  [  288/  306]
train() client id: f_00004-3-0 loss: 1.046117  [   32/  306]
train() client id: f_00004-3-1 loss: 0.839582  [   64/  306]
train() client id: f_00004-3-2 loss: 0.843748  [   96/  306]
train() client id: f_00004-3-3 loss: 0.797564  [  128/  306]
train() client id: f_00004-3-4 loss: 0.754442  [  160/  306]
train() client id: f_00004-3-5 loss: 0.918032  [  192/  306]
train() client id: f_00004-3-6 loss: 0.968544  [  224/  306]
train() client id: f_00004-3-7 loss: 0.839554  [  256/  306]
train() client id: f_00004-3-8 loss: 0.897414  [  288/  306]
train() client id: f_00004-4-0 loss: 0.983799  [   32/  306]
train() client id: f_00004-4-1 loss: 0.823506  [   64/  306]
train() client id: f_00004-4-2 loss: 0.768261  [   96/  306]
train() client id: f_00004-4-3 loss: 0.977481  [  128/  306]
train() client id: f_00004-4-4 loss: 0.887025  [  160/  306]
train() client id: f_00004-4-5 loss: 0.795406  [  192/  306]
train() client id: f_00004-4-6 loss: 0.893863  [  224/  306]
train() client id: f_00004-4-7 loss: 0.884353  [  256/  306]
train() client id: f_00004-4-8 loss: 0.820602  [  288/  306]
train() client id: f_00004-5-0 loss: 0.813303  [   32/  306]
train() client id: f_00004-5-1 loss: 0.918115  [   64/  306]
train() client id: f_00004-5-2 loss: 0.821412  [   96/  306]
train() client id: f_00004-5-3 loss: 0.801156  [  128/  306]
train() client id: f_00004-5-4 loss: 0.805512  [  160/  306]
train() client id: f_00004-5-5 loss: 0.927682  [  192/  306]
train() client id: f_00004-5-6 loss: 0.898104  [  224/  306]
train() client id: f_00004-5-7 loss: 0.894138  [  256/  306]
train() client id: f_00004-5-8 loss: 0.901743  [  288/  306]
train() client id: f_00004-6-0 loss: 0.966380  [   32/  306]
train() client id: f_00004-6-1 loss: 0.801229  [   64/  306]
train() client id: f_00004-6-2 loss: 0.993748  [   96/  306]
train() client id: f_00004-6-3 loss: 0.836870  [  128/  306]
train() client id: f_00004-6-4 loss: 0.840971  [  160/  306]
train() client id: f_00004-6-5 loss: 0.843189  [  192/  306]
train() client id: f_00004-6-6 loss: 0.779781  [  224/  306]
train() client id: f_00004-6-7 loss: 0.966350  [  256/  306]
train() client id: f_00004-6-8 loss: 0.863288  [  288/  306]
train() client id: f_00004-7-0 loss: 0.834765  [   32/  306]
train() client id: f_00004-7-1 loss: 0.974998  [   64/  306]
train() client id: f_00004-7-2 loss: 0.931267  [   96/  306]
train() client id: f_00004-7-3 loss: 0.832758  [  128/  306]
train() client id: f_00004-7-4 loss: 0.768526  [  160/  306]
train() client id: f_00004-7-5 loss: 0.792874  [  192/  306]
train() client id: f_00004-7-6 loss: 0.902936  [  224/  306]
train() client id: f_00004-7-7 loss: 0.907457  [  256/  306]
train() client id: f_00004-7-8 loss: 0.890499  [  288/  306]
train() client id: f_00004-8-0 loss: 0.758045  [   32/  306]
train() client id: f_00004-8-1 loss: 1.000957  [   64/  306]
train() client id: f_00004-8-2 loss: 0.888563  [   96/  306]
train() client id: f_00004-8-3 loss: 0.805715  [  128/  306]
train() client id: f_00004-8-4 loss: 0.828275  [  160/  306]
train() client id: f_00004-8-5 loss: 0.816710  [  192/  306]
train() client id: f_00004-8-6 loss: 1.033354  [  224/  306]
train() client id: f_00004-8-7 loss: 0.926042  [  256/  306]
train() client id: f_00004-8-8 loss: 0.831041  [  288/  306]
train() client id: f_00004-9-0 loss: 0.872582  [   32/  306]
train() client id: f_00004-9-1 loss: 0.859070  [   64/  306]
train() client id: f_00004-9-2 loss: 0.843480  [   96/  306]
train() client id: f_00004-9-3 loss: 0.907025  [  128/  306]
train() client id: f_00004-9-4 loss: 0.872927  [  160/  306]
train() client id: f_00004-9-5 loss: 0.837801  [  192/  306]
train() client id: f_00004-9-6 loss: 0.981373  [  224/  306]
train() client id: f_00004-9-7 loss: 0.780271  [  256/  306]
train() client id: f_00004-9-8 loss: 0.885047  [  288/  306]
train() client id: f_00004-10-0 loss: 0.883724  [   32/  306]
train() client id: f_00004-10-1 loss: 0.818893  [   64/  306]
train() client id: f_00004-10-2 loss: 0.940765  [   96/  306]
train() client id: f_00004-10-3 loss: 0.899190  [  128/  306]
train() client id: f_00004-10-4 loss: 0.831092  [  160/  306]
train() client id: f_00004-10-5 loss: 0.744725  [  192/  306]
train() client id: f_00004-10-6 loss: 0.894176  [  224/  306]
train() client id: f_00004-10-7 loss: 0.873478  [  256/  306]
train() client id: f_00004-10-8 loss: 0.942199  [  288/  306]
train() client id: f_00005-0-0 loss: 0.503217  [   32/  146]
train() client id: f_00005-0-1 loss: 0.540592  [   64/  146]
train() client id: f_00005-0-2 loss: 0.670708  [   96/  146]
train() client id: f_00005-0-3 loss: 0.555901  [  128/  146]
train() client id: f_00005-1-0 loss: 0.582197  [   32/  146]
train() client id: f_00005-1-1 loss: 0.644692  [   64/  146]
train() client id: f_00005-1-2 loss: 0.500489  [   96/  146]
train() client id: f_00005-1-3 loss: 0.889128  [  128/  146]
train() client id: f_00005-2-0 loss: 0.413237  [   32/  146]
train() client id: f_00005-2-1 loss: 0.523596  [   64/  146]
train() client id: f_00005-2-2 loss: 0.701406  [   96/  146]
train() client id: f_00005-2-3 loss: 0.770774  [  128/  146]
train() client id: f_00005-3-0 loss: 0.661966  [   32/  146]
train() client id: f_00005-3-1 loss: 0.645547  [   64/  146]
train() client id: f_00005-3-2 loss: 0.642559  [   96/  146]
train() client id: f_00005-3-3 loss: 0.488118  [  128/  146]
train() client id: f_00005-4-0 loss: 0.527560  [   32/  146]
train() client id: f_00005-4-1 loss: 0.548023  [   64/  146]
train() client id: f_00005-4-2 loss: 0.763597  [   96/  146]
train() client id: f_00005-4-3 loss: 0.428555  [  128/  146]
train() client id: f_00005-5-0 loss: 0.654456  [   32/  146]
train() client id: f_00005-5-1 loss: 0.573725  [   64/  146]
train() client id: f_00005-5-2 loss: 0.474457  [   96/  146]
train() client id: f_00005-5-3 loss: 0.793759  [  128/  146]
train() client id: f_00005-6-0 loss: 0.660052  [   32/  146]
train() client id: f_00005-6-1 loss: 0.819880  [   64/  146]
train() client id: f_00005-6-2 loss: 0.429615  [   96/  146]
train() client id: f_00005-6-3 loss: 0.518101  [  128/  146]
train() client id: f_00005-7-0 loss: 0.804477  [   32/  146]
train() client id: f_00005-7-1 loss: 0.653204  [   64/  146]
train() client id: f_00005-7-2 loss: 0.516340  [   96/  146]
train() client id: f_00005-7-3 loss: 0.455927  [  128/  146]
train() client id: f_00005-8-0 loss: 0.522015  [   32/  146]
train() client id: f_00005-8-1 loss: 0.618577  [   64/  146]
train() client id: f_00005-8-2 loss: 0.609804  [   96/  146]
train() client id: f_00005-8-3 loss: 0.613873  [  128/  146]
train() client id: f_00005-9-0 loss: 0.825771  [   32/  146]
train() client id: f_00005-9-1 loss: 0.526400  [   64/  146]
train() client id: f_00005-9-2 loss: 0.306889  [   96/  146]
train() client id: f_00005-9-3 loss: 0.754244  [  128/  146]
train() client id: f_00005-10-0 loss: 0.464103  [   32/  146]
train() client id: f_00005-10-1 loss: 0.812904  [   64/  146]
train() client id: f_00005-10-2 loss: 0.638506  [   96/  146]
train() client id: f_00005-10-3 loss: 0.562354  [  128/  146]
train() client id: f_00006-0-0 loss: 0.536779  [   32/   54]
train() client id: f_00006-1-0 loss: 0.454639  [   32/   54]
train() client id: f_00006-2-0 loss: 0.468856  [   32/   54]
train() client id: f_00006-3-0 loss: 0.513345  [   32/   54]
train() client id: f_00006-4-0 loss: 0.446328  [   32/   54]
train() client id: f_00006-5-0 loss: 0.482779  [   32/   54]
train() client id: f_00006-6-0 loss: 0.483518  [   32/   54]
train() client id: f_00006-7-0 loss: 0.533199  [   32/   54]
train() client id: f_00006-8-0 loss: 0.455054  [   32/   54]
train() client id: f_00006-9-0 loss: 0.532799  [   32/   54]
train() client id: f_00006-10-0 loss: 0.446276  [   32/   54]
train() client id: f_00007-0-0 loss: 0.375253  [   32/  179]
train() client id: f_00007-0-1 loss: 0.251823  [   64/  179]
train() client id: f_00007-0-2 loss: 0.354105  [   96/  179]
train() client id: f_00007-0-3 loss: 0.392222  [  128/  179]
train() client id: f_00007-0-4 loss: 0.217590  [  160/  179]
train() client id: f_00007-1-0 loss: 0.251350  [   32/  179]
train() client id: f_00007-1-1 loss: 0.188487  [   64/  179]
train() client id: f_00007-1-2 loss: 0.479646  [   96/  179]
train() client id: f_00007-1-3 loss: 0.123578  [  128/  179]
train() client id: f_00007-1-4 loss: 0.292867  [  160/  179]
train() client id: f_00007-2-0 loss: 0.216101  [   32/  179]
train() client id: f_00007-2-1 loss: 0.244506  [   64/  179]
train() client id: f_00007-2-2 loss: 0.154105  [   96/  179]
train() client id: f_00007-2-3 loss: 0.149309  [  128/  179]
train() client id: f_00007-2-4 loss: 0.429921  [  160/  179]
train() client id: f_00007-3-0 loss: 0.320047  [   32/  179]
train() client id: f_00007-3-1 loss: 0.309779  [   64/  179]
train() client id: f_00007-3-2 loss: 0.110721  [   96/  179]
train() client id: f_00007-3-3 loss: 0.257361  [  128/  179]
train() client id: f_00007-3-4 loss: 0.141029  [  160/  179]
train() client id: f_00007-4-0 loss: 0.370333  [   32/  179]
train() client id: f_00007-4-1 loss: 0.130768  [   64/  179]
train() client id: f_00007-4-2 loss: 0.513956  [   96/  179]
train() client id: f_00007-4-3 loss: 0.140636  [  128/  179]
train() client id: f_00007-4-4 loss: 0.095486  [  160/  179]
train() client id: f_00007-5-0 loss: 0.436574  [   32/  179]
train() client id: f_00007-5-1 loss: 0.250977  [   64/  179]
train() client id: f_00007-5-2 loss: 0.229399  [   96/  179]
train() client id: f_00007-5-3 loss: 0.148875  [  128/  179]
train() client id: f_00007-5-4 loss: 0.171061  [  160/  179]
train() client id: f_00007-6-0 loss: 0.085101  [   32/  179]
train() client id: f_00007-6-1 loss: 0.188620  [   64/  179]
train() client id: f_00007-6-2 loss: 0.344244  [   96/  179]
train() client id: f_00007-6-3 loss: 0.391757  [  128/  179]
train() client id: f_00007-6-4 loss: 0.066775  [  160/  179]
train() client id: f_00007-7-0 loss: 0.555078  [   32/  179]
train() client id: f_00007-7-1 loss: 0.048928  [   64/  179]
train() client id: f_00007-7-2 loss: 0.089933  [   96/  179]
train() client id: f_00007-7-3 loss: 0.199698  [  128/  179]
train() client id: f_00007-7-4 loss: 0.190673  [  160/  179]
train() client id: f_00007-8-0 loss: 0.050340  [   32/  179]
train() client id: f_00007-8-1 loss: 0.547389  [   64/  179]
train() client id: f_00007-8-2 loss: 0.015629  [   96/  179]
train() client id: f_00007-8-3 loss: 0.262354  [  128/  179]
train() client id: f_00007-8-4 loss: 0.182303  [  160/  179]
train() client id: f_00007-9-0 loss: 0.170971  [   32/  179]
train() client id: f_00007-9-1 loss: 0.175867  [   64/  179]
train() client id: f_00007-9-2 loss: 0.184032  [   96/  179]
train() client id: f_00007-9-3 loss: 0.149872  [  128/  179]
train() client id: f_00007-9-4 loss: 0.405203  [  160/  179]
train() client id: f_00007-10-0 loss: 0.125216  [   32/  179]
train() client id: f_00007-10-1 loss: 0.244193  [   64/  179]
train() client id: f_00007-10-2 loss: 0.243458  [   96/  179]
train() client id: f_00007-10-3 loss: 0.188691  [  128/  179]
train() client id: f_00007-10-4 loss: 0.214248  [  160/  179]
train() client id: f_00008-0-0 loss: 0.750027  [   32/  130]
train() client id: f_00008-0-1 loss: 0.703798  [   64/  130]
train() client id: f_00008-0-2 loss: 0.651991  [   96/  130]
train() client id: f_00008-0-3 loss: 0.700140  [  128/  130]
train() client id: f_00008-1-0 loss: 0.804651  [   32/  130]
train() client id: f_00008-1-1 loss: 0.653472  [   64/  130]
train() client id: f_00008-1-2 loss: 0.583508  [   96/  130]
train() client id: f_00008-1-3 loss: 0.714153  [  128/  130]
train() client id: f_00008-2-0 loss: 0.705610  [   32/  130]
train() client id: f_00008-2-1 loss: 0.682434  [   64/  130]
train() client id: f_00008-2-2 loss: 0.656382  [   96/  130]
train() client id: f_00008-2-3 loss: 0.754062  [  128/  130]
train() client id: f_00008-3-0 loss: 0.720665  [   32/  130]
train() client id: f_00008-3-1 loss: 0.746205  [   64/  130]
train() client id: f_00008-3-2 loss: 0.684708  [   96/  130]
train() client id: f_00008-3-3 loss: 0.643464  [  128/  130]
train() client id: f_00008-4-0 loss: 0.661162  [   32/  130]
train() client id: f_00008-4-1 loss: 0.704337  [   64/  130]
train() client id: f_00008-4-2 loss: 0.783430  [   96/  130]
train() client id: f_00008-4-3 loss: 0.654384  [  128/  130]
train() client id: f_00008-5-0 loss: 0.723071  [   32/  130]
train() client id: f_00008-5-1 loss: 0.614045  [   64/  130]
train() client id: f_00008-5-2 loss: 0.793775  [   96/  130]
train() client id: f_00008-5-3 loss: 0.673917  [  128/  130]
train() client id: f_00008-6-0 loss: 0.694572  [   32/  130]
train() client id: f_00008-6-1 loss: 0.732376  [   64/  130]
train() client id: f_00008-6-2 loss: 0.683273  [   96/  130]
train() client id: f_00008-6-3 loss: 0.652303  [  128/  130]
train() client id: f_00008-7-0 loss: 0.779523  [   32/  130]
train() client id: f_00008-7-1 loss: 0.643417  [   64/  130]
train() client id: f_00008-7-2 loss: 0.691714  [   96/  130]
train() client id: f_00008-7-3 loss: 0.688474  [  128/  130]
train() client id: f_00008-8-0 loss: 0.689501  [   32/  130]
train() client id: f_00008-8-1 loss: 0.718202  [   64/  130]
train() client id: f_00008-8-2 loss: 0.678738  [   96/  130]
train() client id: f_00008-8-3 loss: 0.708237  [  128/  130]
train() client id: f_00008-9-0 loss: 0.828737  [   32/  130]
train() client id: f_00008-9-1 loss: 0.698030  [   64/  130]
train() client id: f_00008-9-2 loss: 0.687022  [   96/  130]
train() client id: f_00008-9-3 loss: 0.592594  [  128/  130]
train() client id: f_00008-10-0 loss: 0.803681  [   32/  130]
train() client id: f_00008-10-1 loss: 0.618479  [   64/  130]
train() client id: f_00008-10-2 loss: 0.591165  [   96/  130]
train() client id: f_00008-10-3 loss: 0.718064  [  128/  130]
train() client id: f_00009-0-0 loss: 1.026023  [   32/  118]
train() client id: f_00009-0-1 loss: 1.040774  [   64/  118]
train() client id: f_00009-0-2 loss: 0.959933  [   96/  118]
train() client id: f_00009-1-0 loss: 0.904249  [   32/  118]
train() client id: f_00009-1-1 loss: 0.935506  [   64/  118]
train() client id: f_00009-1-2 loss: 0.934685  [   96/  118]
train() client id: f_00009-2-0 loss: 1.045974  [   32/  118]
train() client id: f_00009-2-1 loss: 0.890101  [   64/  118]
train() client id: f_00009-2-2 loss: 0.888847  [   96/  118]
train() client id: f_00009-3-0 loss: 0.760846  [   32/  118]
train() client id: f_00009-3-1 loss: 0.918216  [   64/  118]
train() client id: f_00009-3-2 loss: 0.937793  [   96/  118]
train() client id: f_00009-4-0 loss: 0.926586  [   32/  118]
train() client id: f_00009-4-1 loss: 0.902717  [   64/  118]
train() client id: f_00009-4-2 loss: 0.800568  [   96/  118]
train() client id: f_00009-5-0 loss: 0.658949  [   32/  118]
train() client id: f_00009-5-1 loss: 0.909570  [   64/  118]
train() client id: f_00009-5-2 loss: 0.938437  [   96/  118]
train() client id: f_00009-6-0 loss: 1.023989  [   32/  118]
train() client id: f_00009-6-1 loss: 0.798983  [   64/  118]
train() client id: f_00009-6-2 loss: 0.692248  [   96/  118]
train() client id: f_00009-7-0 loss: 0.745339  [   32/  118]
train() client id: f_00009-7-1 loss: 0.819398  [   64/  118]
train() client id: f_00009-7-2 loss: 0.917441  [   96/  118]
train() client id: f_00009-8-0 loss: 0.896203  [   32/  118]
train() client id: f_00009-8-1 loss: 0.774359  [   64/  118]
train() client id: f_00009-8-2 loss: 0.753602  [   96/  118]
train() client id: f_00009-9-0 loss: 0.963654  [   32/  118]
train() client id: f_00009-9-1 loss: 0.718075  [   64/  118]
train() client id: f_00009-9-2 loss: 0.721193  [   96/  118]
train() client id: f_00009-10-0 loss: 0.867358  [   32/  118]
train() client id: f_00009-10-1 loss: 0.757237  [   64/  118]
train() client id: f_00009-10-2 loss: 0.723672  [   96/  118]
At round 58 accuracy: 0.6472148541114059
At round 58 training accuracy: 0.5902079141515761
At round 58 training loss: 0.8281467757694357
update_location
xs = [  -3.9056584     4.20031788  310.00902392   18.81129433    0.97929623
    3.95640986 -272.44319194 -251.32485185  294.66397685 -237.06087855]
ys = [ 302.5879595   285.55583871    1.32061395 -272.45517586  264.35018685
  247.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [318.70790295 302.58846589 325.74121466 290.83618696 282.63400416
 267.25924958 290.22781289 270.490032   311.66573316 257.32056272]
dists_bs = [213.4290541  210.55960534 515.16820789 487.62583218 197.36303755
 193.20273595 202.59765721 190.16660451 495.34711972 181.91219277]
uav_gains = [1.90906750e-12 2.54420374e-12 1.70337274e-12 3.20136059e-12
 3.78635653e-12 5.22235086e-12 3.24089386e-12 4.88091679e-12
 2.15488058e-12 6.41123405e-12]
bs_gains = [3.32188179e-11 3.45019724e-11 2.81732361e-12 3.28587708e-12
 4.13570723e-11 4.38992321e-11 3.84341882e-11 4.58900129e-11
 3.14446733e-12 5.19614211e-11]
Round 59
-------------------------------
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.22013439 6.49883235 3.18055184 1.17195349 7.49218211 3.60548997
 1.43848068 4.45606788 3.28586187 2.92340788]
obj_prev = 37.272962461679306
eta_min = 1.7285175423879787e-29	eta_max = 0.9377803249311158
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 8.56318552740206	eta = 0.909090909090909
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 19.714881852065698	eta = 0.3948648627079844
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 13.689416383079331	eta = 0.5686666179167633
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.622409124377576	eta = 0.6167375846489944
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557303683729339	eta = 0.6199351637809642
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030281762911	eta = 0.6199486615180114
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030276907934	eta = 0.6199486617577047
eta = 0.6199486617577047
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.0394532  0.08297697 0.03882696 0.01346419 0.09581492 0.04571561
 0.01690851 0.0560486  0.04070568 0.03694824]
ene_total = [1.24429988 1.90408729 1.25458552 0.61484934 2.16708445 1.11417022
 0.68545833 1.4588381  1.18872479 0.92493235]
ti_comp = [1.09084903 1.21156017 1.07968336 1.13102359 1.21456653 1.21550713
 1.13180693 1.15453068 1.12986699 1.21804473]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [3.22550318e-06 2.43255565e-05 3.13824884e-06 1.19255159e-07
 3.72680608e-05 4.04164983e-06 2.35858602e-07 8.25587810e-06
 3.30210249e-06 2.12488134e-06]
ene_total = [0.4167514  0.16397633 0.44017198 0.33241142 0.15794132 0.15527123
 0.33077065 0.2832709  0.3349044  0.14990784]
optimize_network iter = 0 obj = 2.7653774631169483
eta = 0.6199486617577047
freqs = [18083713.04857494 34243850.94632931 17980716.29606591  5952213.52469169
 39444079.62545728 18805161.01539245  7469699.35866398 24273324.57197437
 18013484.95120145 15167027.70060071]
eta_min = 0.6199486617577048	eta_max = 0.7133529451356779
af = 0.0014346255173217418	bf = 1.0502994844473539	zeta = 0.001578088069053916	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.42985407e-07 4.84915903e-06 6.25591760e-07 2.37728263e-08
 7.42917241e-06 8.05679522e-07 4.70170484e-08 1.64576156e-06
 6.58255038e-07 4.23582807e-07]
ene_total = [1.74202974 0.68380564 1.83994709 1.38965906 0.65766716 0.64883763
 1.38279153 1.18365265 1.39985765 0.62655025]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 1 obj = 3.6656508882711805
eta = 0.7133529451356779
freqs = [18007133.55773868 32762205.35417381 17980716.2960659   5842034.71544074
 37704373.23913116 17970837.74832523  7329456.31607533 23636668.595279
 17687093.31024747 14483474.75860257]
eta_min = 0.7133529451356788	eta_max = 0.7133529451356644
af = 0.0013279300383138625	bf = 1.0502994844473539	zeta = 0.0014607230421452488	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.37551209e-07 4.43861511e-06 6.25591760e-07 2.29008752e-08
 6.78828762e-06 7.35774715e-07 4.52681388e-08 1.56056161e-06
 6.34616915e-07 3.86262812e-07]
ene_total = [1.74202926 0.68376964 1.83994709 1.38965899 0.65761096 0.6488315
 1.38279138 1.18364518 1.39985558 0.62654698]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 2 obj = 3.6656508882710073
eta = 0.7133529451356644
freqs = [18007133.55773862 32762205.35417396 17980716.29606583  5842034.71544073
 37704373.23913135 17970837.74832532  7329456.31607533 23636668.59527903
 17687093.31024747 14483474.75860265]
Done!
At round 59 energy consumption: 3.6656508882711805
At round 59 eta: 0.7133529451356644
At round 59 local rounds: 11.060599497015481
At round 59 global rounds: 27.812593047006303
At round 59 a_n: 7.62985203809534
gradient difference: 0.44443121552467346
train() client id: f_00000-0-0 loss: 1.258124  [   32/  126]
train() client id: f_00000-0-1 loss: 1.270484  [   64/  126]
train() client id: f_00000-0-2 loss: 1.275640  [   96/  126]
train() client id: f_00000-1-0 loss: 1.332611  [   32/  126]
train() client id: f_00000-1-1 loss: 1.181378  [   64/  126]
train() client id: f_00000-1-2 loss: 0.934395  [   96/  126]
train() client id: f_00000-2-0 loss: 1.104058  [   32/  126]
train() client id: f_00000-2-1 loss: 1.081663  [   64/  126]
train() client id: f_00000-2-2 loss: 1.016729  [   96/  126]
train() client id: f_00000-3-0 loss: 1.028298  [   32/  126]
train() client id: f_00000-3-1 loss: 0.927144  [   64/  126]
train() client id: f_00000-3-2 loss: 0.981381  [   96/  126]
train() client id: f_00000-4-0 loss: 1.034721  [   32/  126]
train() client id: f_00000-4-1 loss: 0.839111  [   64/  126]
train() client id: f_00000-4-2 loss: 0.981977  [   96/  126]
train() client id: f_00000-5-0 loss: 0.861515  [   32/  126]
train() client id: f_00000-5-1 loss: 0.800181  [   64/  126]
train() client id: f_00000-5-2 loss: 1.007396  [   96/  126]
train() client id: f_00000-6-0 loss: 0.792372  [   32/  126]
train() client id: f_00000-6-1 loss: 0.995939  [   64/  126]
train() client id: f_00000-6-2 loss: 0.903132  [   96/  126]
train() client id: f_00000-7-0 loss: 0.869860  [   32/  126]
train() client id: f_00000-7-1 loss: 0.817938  [   64/  126]
train() client id: f_00000-7-2 loss: 0.884440  [   96/  126]
train() client id: f_00000-8-0 loss: 0.776818  [   32/  126]
train() client id: f_00000-8-1 loss: 0.871809  [   64/  126]
train() client id: f_00000-8-2 loss: 0.854140  [   96/  126]
train() client id: f_00000-9-0 loss: 0.908302  [   32/  126]
train() client id: f_00000-9-1 loss: 0.776513  [   64/  126]
train() client id: f_00000-9-2 loss: 0.896551  [   96/  126]
train() client id: f_00000-10-0 loss: 0.853243  [   32/  126]
train() client id: f_00000-10-1 loss: 0.866195  [   64/  126]
train() client id: f_00000-10-2 loss: 0.953658  [   96/  126]
train() client id: f_00001-0-0 loss: 0.464287  [   32/  265]
train() client id: f_00001-0-1 loss: 0.465762  [   64/  265]
train() client id: f_00001-0-2 loss: 0.471355  [   96/  265]
train() client id: f_00001-0-3 loss: 0.410293  [  128/  265]
train() client id: f_00001-0-4 loss: 0.577780  [  160/  265]
train() client id: f_00001-0-5 loss: 0.428325  [  192/  265]
train() client id: f_00001-0-6 loss: 0.409428  [  224/  265]
train() client id: f_00001-0-7 loss: 0.564938  [  256/  265]
train() client id: f_00001-1-0 loss: 0.510060  [   32/  265]
train() client id: f_00001-1-1 loss: 0.376894  [   64/  265]
train() client id: f_00001-1-2 loss: 0.464145  [   96/  265]
train() client id: f_00001-1-3 loss: 0.454048  [  128/  265]
train() client id: f_00001-1-4 loss: 0.425639  [  160/  265]
train() client id: f_00001-1-5 loss: 0.426905  [  192/  265]
train() client id: f_00001-1-6 loss: 0.464082  [  224/  265]
train() client id: f_00001-1-7 loss: 0.531764  [  256/  265]
train() client id: f_00001-2-0 loss: 0.397705  [   32/  265]
train() client id: f_00001-2-1 loss: 0.357560  [   64/  265]
train() client id: f_00001-2-2 loss: 0.399016  [   96/  265]
train() client id: f_00001-2-3 loss: 0.517608  [  128/  265]
train() client id: f_00001-2-4 loss: 0.614800  [  160/  265]
train() client id: f_00001-2-5 loss: 0.414716  [  192/  265]
train() client id: f_00001-2-6 loss: 0.441511  [  224/  265]
train() client id: f_00001-2-7 loss: 0.489677  [  256/  265]
train() client id: f_00001-3-0 loss: 0.447662  [   32/  265]
train() client id: f_00001-3-1 loss: 0.398008  [   64/  265]
train() client id: f_00001-3-2 loss: 0.502746  [   96/  265]
train() client id: f_00001-3-3 loss: 0.438914  [  128/  265]
train() client id: f_00001-3-4 loss: 0.458396  [  160/  265]
train() client id: f_00001-3-5 loss: 0.544135  [  192/  265]
train() client id: f_00001-3-6 loss: 0.370352  [  224/  265]
train() client id: f_00001-3-7 loss: 0.468622  [  256/  265]
train() client id: f_00001-4-0 loss: 0.352409  [   32/  265]
train() client id: f_00001-4-1 loss: 0.576730  [   64/  265]
train() client id: f_00001-4-2 loss: 0.435699  [   96/  265]
train() client id: f_00001-4-3 loss: 0.559387  [  128/  265]
train() client id: f_00001-4-4 loss: 0.374067  [  160/  265]
train() client id: f_00001-4-5 loss: 0.377197  [  192/  265]
train() client id: f_00001-4-6 loss: 0.502717  [  224/  265]
train() client id: f_00001-4-7 loss: 0.486814  [  256/  265]
train() client id: f_00001-5-0 loss: 0.337741  [   32/  265]
train() client id: f_00001-5-1 loss: 0.451265  [   64/  265]
train() client id: f_00001-5-2 loss: 0.537075  [   96/  265]
train() client id: f_00001-5-3 loss: 0.510692  [  128/  265]
train() client id: f_00001-5-4 loss: 0.508612  [  160/  265]
train() client id: f_00001-5-5 loss: 0.408164  [  192/  265]
train() client id: f_00001-5-6 loss: 0.448395  [  224/  265]
train() client id: f_00001-5-7 loss: 0.424266  [  256/  265]
train() client id: f_00001-6-0 loss: 0.353266  [   32/  265]
train() client id: f_00001-6-1 loss: 0.375552  [   64/  265]
train() client id: f_00001-6-2 loss: 0.512814  [   96/  265]
train() client id: f_00001-6-3 loss: 0.430226  [  128/  265]
train() client id: f_00001-6-4 loss: 0.417972  [  160/  265]
train() client id: f_00001-6-5 loss: 0.435032  [  192/  265]
train() client id: f_00001-6-6 loss: 0.510648  [  224/  265]
train() client id: f_00001-6-7 loss: 0.570298  [  256/  265]
train() client id: f_00001-7-0 loss: 0.522205  [   32/  265]
train() client id: f_00001-7-1 loss: 0.416404  [   64/  265]
train() client id: f_00001-7-2 loss: 0.459948  [   96/  265]
train() client id: f_00001-7-3 loss: 0.482292  [  128/  265]
train() client id: f_00001-7-4 loss: 0.408925  [  160/  265]
train() client id: f_00001-7-5 loss: 0.462060  [  192/  265]
train() client id: f_00001-7-6 loss: 0.369219  [  224/  265]
train() client id: f_00001-7-7 loss: 0.349013  [  256/  265]
train() client id: f_00001-8-0 loss: 0.461601  [   32/  265]
train() client id: f_00001-8-1 loss: 0.530924  [   64/  265]
train() client id: f_00001-8-2 loss: 0.446930  [   96/  265]
train() client id: f_00001-8-3 loss: 0.433502  [  128/  265]
train() client id: f_00001-8-4 loss: 0.394983  [  160/  265]
train() client id: f_00001-8-5 loss: 0.527297  [  192/  265]
train() client id: f_00001-8-6 loss: 0.465505  [  224/  265]
train() client id: f_00001-8-7 loss: 0.372298  [  256/  265]
train() client id: f_00001-9-0 loss: 0.485900  [   32/  265]
train() client id: f_00001-9-1 loss: 0.452664  [   64/  265]
train() client id: f_00001-9-2 loss: 0.567501  [   96/  265]
train() client id: f_00001-9-3 loss: 0.420205  [  128/  265]
train() client id: f_00001-9-4 loss: 0.466010  [  160/  265]
train() client id: f_00001-9-5 loss: 0.446385  [  192/  265]
train() client id: f_00001-9-6 loss: 0.448940  [  224/  265]
train() client id: f_00001-9-7 loss: 0.349800  [  256/  265]
train() client id: f_00001-10-0 loss: 0.410551  [   32/  265]
train() client id: f_00001-10-1 loss: 0.417930  [   64/  265]
train() client id: f_00001-10-2 loss: 0.451908  [   96/  265]
train() client id: f_00001-10-3 loss: 0.556579  [  128/  265]
train() client id: f_00001-10-4 loss: 0.465248  [  160/  265]
train() client id: f_00001-10-5 loss: 0.577848  [  192/  265]
train() client id: f_00001-10-6 loss: 0.355419  [  224/  265]
train() client id: f_00001-10-7 loss: 0.358617  [  256/  265]
train() client id: f_00002-0-0 loss: 1.296106  [   32/  124]
train() client id: f_00002-0-1 loss: 1.054804  [   64/  124]
train() client id: f_00002-0-2 loss: 1.219062  [   96/  124]
train() client id: f_00002-1-0 loss: 1.078516  [   32/  124]
train() client id: f_00002-1-1 loss: 1.182012  [   64/  124]
train() client id: f_00002-1-2 loss: 1.047150  [   96/  124]
train() client id: f_00002-2-0 loss: 1.076838  [   32/  124]
train() client id: f_00002-2-1 loss: 1.045687  [   64/  124]
train() client id: f_00002-2-2 loss: 1.080520  [   96/  124]
train() client id: f_00002-3-0 loss: 1.029862  [   32/  124]
train() client id: f_00002-3-1 loss: 1.025477  [   64/  124]
train() client id: f_00002-3-2 loss: 1.047544  [   96/  124]
train() client id: f_00002-4-0 loss: 1.275877  [   32/  124]
train() client id: f_00002-4-1 loss: 0.914187  [   64/  124]
train() client id: f_00002-4-2 loss: 0.845063  [   96/  124]
train() client id: f_00002-5-0 loss: 1.083494  [   32/  124]
train() client id: f_00002-5-1 loss: 0.977455  [   64/  124]
train() client id: f_00002-5-2 loss: 1.028154  [   96/  124]
train() client id: f_00002-6-0 loss: 0.901197  [   32/  124]
train() client id: f_00002-6-1 loss: 1.168104  [   64/  124]
train() client id: f_00002-6-2 loss: 0.959021  [   96/  124]
train() client id: f_00002-7-0 loss: 1.056067  [   32/  124]
train() client id: f_00002-7-1 loss: 0.841544  [   64/  124]
train() client id: f_00002-7-2 loss: 1.090107  [   96/  124]
train() client id: f_00002-8-0 loss: 1.089294  [   32/  124]
train() client id: f_00002-8-1 loss: 0.926247  [   64/  124]
train() client id: f_00002-8-2 loss: 0.998720  [   96/  124]
train() client id: f_00002-9-0 loss: 0.914267  [   32/  124]
train() client id: f_00002-9-1 loss: 1.085044  [   64/  124]
train() client id: f_00002-9-2 loss: 1.057998  [   96/  124]
train() client id: f_00002-10-0 loss: 1.090263  [   32/  124]
train() client id: f_00002-10-1 loss: 0.864616  [   64/  124]
train() client id: f_00002-10-2 loss: 1.115131  [   96/  124]
train() client id: f_00003-0-0 loss: 0.324062  [   32/   43]
train() client id: f_00003-1-0 loss: 0.499043  [   32/   43]
train() client id: f_00003-2-0 loss: 0.500058  [   32/   43]
train() client id: f_00003-3-0 loss: 0.552541  [   32/   43]
train() client id: f_00003-4-0 loss: 0.583261  [   32/   43]
train() client id: f_00003-5-0 loss: 0.528127  [   32/   43]
train() client id: f_00003-6-0 loss: 0.543817  [   32/   43]
train() client id: f_00003-7-0 loss: 0.426973  [   32/   43]
train() client id: f_00003-8-0 loss: 0.532199  [   32/   43]
train() client id: f_00003-9-0 loss: 0.350701  [   32/   43]
train() client id: f_00003-10-0 loss: 0.246090  [   32/   43]
train() client id: f_00004-0-0 loss: 0.718876  [   32/  306]
train() client id: f_00004-0-1 loss: 0.673170  [   64/  306]
train() client id: f_00004-0-2 loss: 0.841025  [   96/  306]
train() client id: f_00004-0-3 loss: 0.642859  [  128/  306]
train() client id: f_00004-0-4 loss: 0.686029  [  160/  306]
train() client id: f_00004-0-5 loss: 0.709201  [  192/  306]
train() client id: f_00004-0-6 loss: 0.717523  [  224/  306]
train() client id: f_00004-0-7 loss: 0.667715  [  256/  306]
train() client id: f_00004-0-8 loss: 0.824312  [  288/  306]
train() client id: f_00004-1-0 loss: 0.658546  [   32/  306]
train() client id: f_00004-1-1 loss: 0.704452  [   64/  306]
train() client id: f_00004-1-2 loss: 0.806251  [   96/  306]
train() client id: f_00004-1-3 loss: 0.722234  [  128/  306]
train() client id: f_00004-1-4 loss: 0.718934  [  160/  306]
train() client id: f_00004-1-5 loss: 0.757321  [  192/  306]
train() client id: f_00004-1-6 loss: 0.943123  [  224/  306]
train() client id: f_00004-1-7 loss: 0.608370  [  256/  306]
train() client id: f_00004-1-8 loss: 0.736460  [  288/  306]
train() client id: f_00004-2-0 loss: 0.760462  [   32/  306]
train() client id: f_00004-2-1 loss: 0.771034  [   64/  306]
train() client id: f_00004-2-2 loss: 0.771486  [   96/  306]
train() client id: f_00004-2-3 loss: 0.684323  [  128/  306]
train() client id: f_00004-2-4 loss: 0.578774  [  160/  306]
train() client id: f_00004-2-5 loss: 0.772089  [  192/  306]
train() client id: f_00004-2-6 loss: 0.723721  [  224/  306]
train() client id: f_00004-2-7 loss: 0.707711  [  256/  306]
train() client id: f_00004-2-8 loss: 0.730563  [  288/  306]
train() client id: f_00004-3-0 loss: 0.679212  [   32/  306]
train() client id: f_00004-3-1 loss: 0.696758  [   64/  306]
train() client id: f_00004-3-2 loss: 0.730736  [   96/  306]
train() client id: f_00004-3-3 loss: 0.708666  [  128/  306]
train() client id: f_00004-3-4 loss: 0.798157  [  160/  306]
train() client id: f_00004-3-5 loss: 0.723807  [  192/  306]
train() client id: f_00004-3-6 loss: 0.708869  [  224/  306]
train() client id: f_00004-3-7 loss: 0.771791  [  256/  306]
train() client id: f_00004-3-8 loss: 0.818304  [  288/  306]
train() client id: f_00004-4-0 loss: 0.672249  [   32/  306]
train() client id: f_00004-4-1 loss: 0.690549  [   64/  306]
train() client id: f_00004-4-2 loss: 0.900371  [   96/  306]
train() client id: f_00004-4-3 loss: 0.750542  [  128/  306]
train() client id: f_00004-4-4 loss: 0.766453  [  160/  306]
train() client id: f_00004-4-5 loss: 0.701035  [  192/  306]
train() client id: f_00004-4-6 loss: 0.650345  [  224/  306]
train() client id: f_00004-4-7 loss: 0.759143  [  256/  306]
train() client id: f_00004-4-8 loss: 0.661789  [  288/  306]
train() client id: f_00004-5-0 loss: 0.730839  [   32/  306]
train() client id: f_00004-5-1 loss: 0.838865  [   64/  306]
train() client id: f_00004-5-2 loss: 0.781090  [   96/  306]
train() client id: f_00004-5-3 loss: 0.823316  [  128/  306]
train() client id: f_00004-5-4 loss: 0.793278  [  160/  306]
train() client id: f_00004-5-5 loss: 0.653996  [  192/  306]
train() client id: f_00004-5-6 loss: 0.771145  [  224/  306]
train() client id: f_00004-5-7 loss: 0.637543  [  256/  306]
train() client id: f_00004-5-8 loss: 0.634083  [  288/  306]
train() client id: f_00004-6-0 loss: 0.685779  [   32/  306]
train() client id: f_00004-6-1 loss: 0.721432  [   64/  306]
train() client id: f_00004-6-2 loss: 0.747122  [   96/  306]
train() client id: f_00004-6-3 loss: 0.790872  [  128/  306]
train() client id: f_00004-6-4 loss: 0.835689  [  160/  306]
train() client id: f_00004-6-5 loss: 0.668864  [  192/  306]
train() client id: f_00004-6-6 loss: 0.710505  [  224/  306]
train() client id: f_00004-6-7 loss: 0.776776  [  256/  306]
train() client id: f_00004-6-8 loss: 0.739006  [  288/  306]
train() client id: f_00004-7-0 loss: 0.595130  [   32/  306]
train() client id: f_00004-7-1 loss: 0.736899  [   64/  306]
train() client id: f_00004-7-2 loss: 0.750316  [   96/  306]
train() client id: f_00004-7-3 loss: 0.773254  [  128/  306]
train() client id: f_00004-7-4 loss: 0.659441  [  160/  306]
train() client id: f_00004-7-5 loss: 0.701704  [  192/  306]
train() client id: f_00004-7-6 loss: 0.738212  [  224/  306]
train() client id: f_00004-7-7 loss: 0.769632  [  256/  306]
train() client id: f_00004-7-8 loss: 0.874981  [  288/  306]
train() client id: f_00004-8-0 loss: 0.726749  [   32/  306]
train() client id: f_00004-8-1 loss: 0.757467  [   64/  306]
train() client id: f_00004-8-2 loss: 0.643817  [   96/  306]
train() client id: f_00004-8-3 loss: 0.781909  [  128/  306]
train() client id: f_00004-8-4 loss: 0.620053  [  160/  306]
train() client id: f_00004-8-5 loss: 0.755686  [  192/  306]
train() client id: f_00004-8-6 loss: 0.805151  [  224/  306]
train() client id: f_00004-8-7 loss: 0.769304  [  256/  306]
train() client id: f_00004-8-8 loss: 0.725451  [  288/  306]
train() client id: f_00004-9-0 loss: 0.863176  [   32/  306]
train() client id: f_00004-9-1 loss: 0.726952  [   64/  306]
train() client id: f_00004-9-2 loss: 0.664573  [   96/  306]
train() client id: f_00004-9-3 loss: 0.675709  [  128/  306]
train() client id: f_00004-9-4 loss: 0.732428  [  160/  306]
train() client id: f_00004-9-5 loss: 0.747408  [  192/  306]
train() client id: f_00004-9-6 loss: 0.711861  [  224/  306]
train() client id: f_00004-9-7 loss: 0.728886  [  256/  306]
train() client id: f_00004-9-8 loss: 0.837918  [  288/  306]
train() client id: f_00004-10-0 loss: 0.768615  [   32/  306]
train() client id: f_00004-10-1 loss: 0.755652  [   64/  306]
train() client id: f_00004-10-2 loss: 0.712066  [   96/  306]
train() client id: f_00004-10-3 loss: 0.871011  [  128/  306]
train() client id: f_00004-10-4 loss: 0.813329  [  160/  306]
train() client id: f_00004-10-5 loss: 0.810873  [  192/  306]
train() client id: f_00004-10-6 loss: 0.675576  [  224/  306]
train() client id: f_00004-10-7 loss: 0.687315  [  256/  306]
train() client id: f_00004-10-8 loss: 0.689656  [  288/  306]
train() client id: f_00005-0-0 loss: 0.860500  [   32/  146]
train() client id: f_00005-0-1 loss: 0.626991  [   64/  146]
train() client id: f_00005-0-2 loss: 0.411467  [   96/  146]
train() client id: f_00005-0-3 loss: 0.676763  [  128/  146]
train() client id: f_00005-1-0 loss: 0.626945  [   32/  146]
train() client id: f_00005-1-1 loss: 0.666581  [   64/  146]
train() client id: f_00005-1-2 loss: 0.688192  [   96/  146]
train() client id: f_00005-1-3 loss: 0.651049  [  128/  146]
train() client id: f_00005-2-0 loss: 0.613431  [   32/  146]
train() client id: f_00005-2-1 loss: 0.688537  [   64/  146]
train() client id: f_00005-2-2 loss: 0.738112  [   96/  146]
train() client id: f_00005-2-3 loss: 0.625468  [  128/  146]
train() client id: f_00005-3-0 loss: 0.384508  [   32/  146]
train() client id: f_00005-3-1 loss: 0.535906  [   64/  146]
train() client id: f_00005-3-2 loss: 0.886928  [   96/  146]
train() client id: f_00005-3-3 loss: 0.839010  [  128/  146]
train() client id: f_00005-4-0 loss: 0.664293  [   32/  146]
train() client id: f_00005-4-1 loss: 0.871530  [   64/  146]
train() client id: f_00005-4-2 loss: 0.354726  [   96/  146]
train() client id: f_00005-4-3 loss: 0.614476  [  128/  146]
train() client id: f_00005-5-0 loss: 0.702689  [   32/  146]
train() client id: f_00005-5-1 loss: 0.649139  [   64/  146]
train() client id: f_00005-5-2 loss: 0.593373  [   96/  146]
train() client id: f_00005-5-3 loss: 0.758729  [  128/  146]
train() client id: f_00005-6-0 loss: 0.642466  [   32/  146]
train() client id: f_00005-6-1 loss: 1.021219  [   64/  146]
train() client id: f_00005-6-2 loss: 0.668119  [   96/  146]
train() client id: f_00005-6-3 loss: 0.324577  [  128/  146]
train() client id: f_00005-7-0 loss: 0.633211  [   32/  146]
train() client id: f_00005-7-1 loss: 0.816772  [   64/  146]
train() client id: f_00005-7-2 loss: 0.540927  [   96/  146]
train() client id: f_00005-7-3 loss: 0.685078  [  128/  146]
train() client id: f_00005-8-0 loss: 0.760931  [   32/  146]
train() client id: f_00005-8-1 loss: 0.474951  [   64/  146]
train() client id: f_00005-8-2 loss: 0.747855  [   96/  146]
train() client id: f_00005-8-3 loss: 0.522493  [  128/  146]
train() client id: f_00005-9-0 loss: 0.761009  [   32/  146]
train() client id: f_00005-9-1 loss: 0.394511  [   64/  146]
train() client id: f_00005-9-2 loss: 0.710571  [   96/  146]
train() client id: f_00005-9-3 loss: 0.848155  [  128/  146]
train() client id: f_00005-10-0 loss: 0.613026  [   32/  146]
train() client id: f_00005-10-1 loss: 0.603021  [   64/  146]
train() client id: f_00005-10-2 loss: 0.882451  [   96/  146]
train() client id: f_00005-10-3 loss: 0.619280  [  128/  146]
train() client id: f_00006-0-0 loss: 0.495137  [   32/   54]
train() client id: f_00006-1-0 loss: 0.569420  [   32/   54]
train() client id: f_00006-2-0 loss: 0.528623  [   32/   54]
train() client id: f_00006-3-0 loss: 0.557318  [   32/   54]
train() client id: f_00006-4-0 loss: 0.485266  [   32/   54]
train() client id: f_00006-5-0 loss: 0.509785  [   32/   54]
train() client id: f_00006-6-0 loss: 0.575026  [   32/   54]
train() client id: f_00006-7-0 loss: 0.567498  [   32/   54]
train() client id: f_00006-8-0 loss: 0.514956  [   32/   54]
train() client id: f_00006-9-0 loss: 0.465058  [   32/   54]
train() client id: f_00006-10-0 loss: 0.514065  [   32/   54]
train() client id: f_00007-0-0 loss: 0.447086  [   32/  179]
train() client id: f_00007-0-1 loss: 0.465612  [   64/  179]
train() client id: f_00007-0-2 loss: 0.638038  [   96/  179]
train() client id: f_00007-0-3 loss: 0.748317  [  128/  179]
train() client id: f_00007-0-4 loss: 0.597385  [  160/  179]
train() client id: f_00007-1-0 loss: 0.549323  [   32/  179]
train() client id: f_00007-1-1 loss: 0.636442  [   64/  179]
train() client id: f_00007-1-2 loss: 0.580950  [   96/  179]
train() client id: f_00007-1-3 loss: 0.603869  [  128/  179]
train() client id: f_00007-1-4 loss: 0.519866  [  160/  179]
train() client id: f_00007-2-0 loss: 0.556033  [   32/  179]
train() client id: f_00007-2-1 loss: 0.726746  [   64/  179]
train() client id: f_00007-2-2 loss: 0.587402  [   96/  179]
train() client id: f_00007-2-3 loss: 0.471849  [  128/  179]
train() client id: f_00007-2-4 loss: 0.460756  [  160/  179]
train() client id: f_00007-3-0 loss: 0.475349  [   32/  179]
train() client id: f_00007-3-1 loss: 0.569166  [   64/  179]
train() client id: f_00007-3-2 loss: 0.552583  [   96/  179]
train() client id: f_00007-3-3 loss: 0.682286  [  128/  179]
train() client id: f_00007-3-4 loss: 0.491199  [  160/  179]
train() client id: f_00007-4-0 loss: 0.497311  [   32/  179]
train() client id: f_00007-4-1 loss: 0.774532  [   64/  179]
train() client id: f_00007-4-2 loss: 0.534959  [   96/  179]
train() client id: f_00007-4-3 loss: 0.432268  [  128/  179]
train() client id: f_00007-4-4 loss: 0.417196  [  160/  179]
train() client id: f_00007-5-0 loss: 0.856746  [   32/  179]
train() client id: f_00007-5-1 loss: 0.527056  [   64/  179]
train() client id: f_00007-5-2 loss: 0.384718  [   96/  179]
train() client id: f_00007-5-3 loss: 0.395088  [  128/  179]
train() client id: f_00007-5-4 loss: 0.457453  [  160/  179]
train() client id: f_00007-6-0 loss: 0.490265  [   32/  179]
train() client id: f_00007-6-1 loss: 0.407739  [   64/  179]
train() client id: f_00007-6-2 loss: 0.555337  [   96/  179]
train() client id: f_00007-6-3 loss: 0.554259  [  128/  179]
train() client id: f_00007-6-4 loss: 0.619925  [  160/  179]
train() client id: f_00007-7-0 loss: 0.354289  [   32/  179]
train() client id: f_00007-7-1 loss: 0.515400  [   64/  179]
train() client id: f_00007-7-2 loss: 0.657738  [   96/  179]
train() client id: f_00007-7-3 loss: 0.509481  [  128/  179]
train() client id: f_00007-7-4 loss: 0.647837  [  160/  179]
train() client id: f_00007-8-0 loss: 0.780732  [   32/  179]
train() client id: f_00007-8-1 loss: 0.508330  [   64/  179]
train() client id: f_00007-8-2 loss: 0.763438  [   96/  179]
train() client id: f_00007-8-3 loss: 0.342172  [  128/  179]
train() client id: f_00007-8-4 loss: 0.382078  [  160/  179]
train() client id: f_00007-9-0 loss: 0.703617  [   32/  179]
train() client id: f_00007-9-1 loss: 0.457049  [   64/  179]
train() client id: f_00007-9-2 loss: 0.369036  [   96/  179]
train() client id: f_00007-9-3 loss: 0.616287  [  128/  179]
train() client id: f_00007-9-4 loss: 0.622801  [  160/  179]
train() client id: f_00007-10-0 loss: 0.395546  [   32/  179]
train() client id: f_00007-10-1 loss: 0.474425  [   64/  179]
train() client id: f_00007-10-2 loss: 0.356300  [   96/  179]
train() client id: f_00007-10-3 loss: 0.613912  [  128/  179]
train() client id: f_00007-10-4 loss: 0.598051  [  160/  179]
train() client id: f_00008-0-0 loss: 0.716025  [   32/  130]
train() client id: f_00008-0-1 loss: 0.747527  [   64/  130]
train() client id: f_00008-0-2 loss: 0.675085  [   96/  130]
train() client id: f_00008-0-3 loss: 0.775101  [  128/  130]
train() client id: f_00008-1-0 loss: 0.761712  [   32/  130]
train() client id: f_00008-1-1 loss: 0.715893  [   64/  130]
train() client id: f_00008-1-2 loss: 0.807083  [   96/  130]
train() client id: f_00008-1-3 loss: 0.594265  [  128/  130]
train() client id: f_00008-2-0 loss: 0.770173  [   32/  130]
train() client id: f_00008-2-1 loss: 0.767794  [   64/  130]
train() client id: f_00008-2-2 loss: 0.622872  [   96/  130]
train() client id: f_00008-2-3 loss: 0.743721  [  128/  130]
train() client id: f_00008-3-0 loss: 0.720547  [   32/  130]
train() client id: f_00008-3-1 loss: 0.805091  [   64/  130]
train() client id: f_00008-3-2 loss: 0.599761  [   96/  130]
train() client id: f_00008-3-3 loss: 0.776646  [  128/  130]
train() client id: f_00008-4-0 loss: 0.823760  [   32/  130]
train() client id: f_00008-4-1 loss: 0.715003  [   64/  130]
train() client id: f_00008-4-2 loss: 0.673235  [   96/  130]
train() client id: f_00008-4-3 loss: 0.658646  [  128/  130]
train() client id: f_00008-5-0 loss: 0.721967  [   32/  130]
train() client id: f_00008-5-1 loss: 0.711957  [   64/  130]
train() client id: f_00008-5-2 loss: 0.754440  [   96/  130]
train() client id: f_00008-5-3 loss: 0.699279  [  128/  130]
train() client id: f_00008-6-0 loss: 0.647050  [   32/  130]
train() client id: f_00008-6-1 loss: 0.724088  [   64/  130]
train() client id: f_00008-6-2 loss: 0.705911  [   96/  130]
train() client id: f_00008-6-3 loss: 0.821967  [  128/  130]
train() client id: f_00008-7-0 loss: 0.660278  [   32/  130]
train() client id: f_00008-7-1 loss: 0.705371  [   64/  130]
train() client id: f_00008-7-2 loss: 0.842578  [   96/  130]
train() client id: f_00008-7-3 loss: 0.657231  [  128/  130]
train() client id: f_00008-8-0 loss: 0.742832  [   32/  130]
train() client id: f_00008-8-1 loss: 0.705471  [   64/  130]
train() client id: f_00008-8-2 loss: 0.710896  [   96/  130]
train() client id: f_00008-8-3 loss: 0.729614  [  128/  130]
train() client id: f_00008-9-0 loss: 0.750688  [   32/  130]
train() client id: f_00008-9-1 loss: 0.632665  [   64/  130]
train() client id: f_00008-9-2 loss: 0.814841  [   96/  130]
train() client id: f_00008-9-3 loss: 0.646445  [  128/  130]
train() client id: f_00008-10-0 loss: 0.655643  [   32/  130]
train() client id: f_00008-10-1 loss: 0.627503  [   64/  130]
train() client id: f_00008-10-2 loss: 0.747415  [   96/  130]
train() client id: f_00008-10-3 loss: 0.855359  [  128/  130]
train() client id: f_00009-0-0 loss: 1.088708  [   32/  118]
train() client id: f_00009-0-1 loss: 1.205306  [   64/  118]
train() client id: f_00009-0-2 loss: 1.025708  [   96/  118]
train() client id: f_00009-1-0 loss: 1.135029  [   32/  118]
train() client id: f_00009-1-1 loss: 0.980805  [   64/  118]
train() client id: f_00009-1-2 loss: 1.209288  [   96/  118]
train() client id: f_00009-2-0 loss: 1.069625  [   32/  118]
train() client id: f_00009-2-1 loss: 0.991404  [   64/  118]
train() client id: f_00009-2-2 loss: 1.064445  [   96/  118]
train() client id: f_00009-3-0 loss: 1.028824  [   32/  118]
train() client id: f_00009-3-1 loss: 1.027866  [   64/  118]
train() client id: f_00009-3-2 loss: 0.958609  [   96/  118]
train() client id: f_00009-4-0 loss: 0.958107  [   32/  118]
train() client id: f_00009-4-1 loss: 0.845057  [   64/  118]
train() client id: f_00009-4-2 loss: 1.098286  [   96/  118]
train() client id: f_00009-5-0 loss: 1.027409  [   32/  118]
train() client id: f_00009-5-1 loss: 0.896044  [   64/  118]
train() client id: f_00009-5-2 loss: 1.011215  [   96/  118]
train() client id: f_00009-6-0 loss: 0.875300  [   32/  118]
train() client id: f_00009-6-1 loss: 0.848613  [   64/  118]
train() client id: f_00009-6-2 loss: 1.134208  [   96/  118]
train() client id: f_00009-7-0 loss: 0.956160  [   32/  118]
train() client id: f_00009-7-1 loss: 0.973676  [   64/  118]
train() client id: f_00009-7-2 loss: 0.883628  [   96/  118]
train() client id: f_00009-8-0 loss: 0.887791  [   32/  118]
train() client id: f_00009-8-1 loss: 0.854227  [   64/  118]
train() client id: f_00009-8-2 loss: 0.960528  [   96/  118]
train() client id: f_00009-9-0 loss: 0.933328  [   32/  118]
train() client id: f_00009-9-1 loss: 1.022189  [   64/  118]
train() client id: f_00009-9-2 loss: 0.752739  [   96/  118]
train() client id: f_00009-10-0 loss: 0.961739  [   32/  118]
train() client id: f_00009-10-1 loss: 0.739001  [   64/  118]
train() client id: f_00009-10-2 loss: 0.907414  [   96/  118]
At round 59 accuracy: 0.6472148541114059
At round 59 training accuracy: 0.5922199865861838
At round 59 training loss: 0.8307286442470463
update_location
xs = [  -3.9056584     4.20031788  315.00902392   18.81129433    0.97929623
    3.95640986 -277.44319194 -256.32485185  299.66397685 -242.06087855]
ys = [ 307.5879595   290.55583871    1.32061395 -277.45517586  269.35018685
  252.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [323.45881808 307.31146753 330.50329676 295.5253617  287.31599707
 271.90190877 294.92645743 275.14197413 316.3971697  261.93411534]
dists_bs = [216.45517025 213.22735703 519.87822868 492.21794357 199.67641438
 195.1420987  205.04936632 192.2279532  500.09040061 183.67812789]
uav_gains = [1.76622191e-12 2.33041781e-12 1.58304528e-12 2.91576877e-12
 3.43842860e-12 4.73853249e-12 2.95041862e-12 4.42719338e-12
 1.98494725e-12 5.83324094e-12]
bs_gains = [3.19347696e-11 3.33068759e-11 2.74643623e-12 3.20076101e-12
 4.00294047e-11 4.26885463e-11 3.71612640e-11 4.45253937e-11
 3.06166898e-12 5.05746914e-11]
Round 60
-------------------------------
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.0875637  6.22007234 3.04984448 1.12636964 7.17067363 3.45091238
 1.38142813 4.26835796 3.14602737 2.7981042 ]
obj_prev = 35.69935382138897
eta_min = 9.518668131305417e-31	eta_max = 0.9381641580347129
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 8.195255617037894	eta = 0.909090909090909
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 19.17140763760524	eta = 0.38861165126506014
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 13.206898476870052	eta = 0.5641167297661407
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.154598615650826	eta = 0.6129558543819038
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.090043572188366	eta = 0.6162287451356823
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768790589424	eta = 0.6162427510544748
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768785575135	eta = 0.6162427513100645
eta = 0.6162427513100645
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.03994795 0.08401752 0.03931386 0.01363303 0.09701646 0.0462889
 0.01712055 0.05675146 0.04121614 0.03741158]
ene_total = [1.20410292 1.8265834  1.21411492 0.59841362 2.07887597 1.06827008
 0.66615118 1.4063269  1.14028819 0.8866416 ]
ti_comp = [1.15185544 1.27946326 1.14047878 1.19334864 1.28255751 1.28358448
 1.19415401 1.21817826 1.19663411 1.28616459]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [3.00308521e-06 2.26429994e-05 2.91972948e-06 1.11204574e-07
 3.46946537e-05 3.76237107e-06 2.19944230e-07 7.69820333e-06
 3.05604171e-06 1.97835807e-06]
ene_total = [0.40991842 0.15659935 0.43253582 0.32736424 0.15068698 0.14803017
 0.32576518 0.27814891 0.32089064 0.14286493]
optimize_network iter = 0 obj = 2.6928046428038024
eta = 0.6162427513100645
freqs = [17340697.3279414  32833111.65400368 17235681.79847541  5712090.78528441
 37821484.27558679 18031106.89702021  7168485.47995044 23293576.56497024
 17221697.8440477  14543852.06878811]
eta_min = 0.6162427513100649	eta_max = 0.7177903732762834
af = 0.0012621994794922648	bf = 1.0327722521438556	zeta = 0.0013884194274414914	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.91233479e-07 4.45784864e-06 5.74822789e-07 2.18934405e-08
 6.83052241e-06 7.40718154e-07 4.33015993e-08 1.51558655e-06
 6.01659308e-07 3.89489955e-07]
ene_total = [1.73019841 0.65953123 1.82567951 1.3819049  0.63376085 0.62463054
 1.37514743 1.17363935 1.35437921 0.60294663]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 1 obj = 3.6609545155387906
eta = 0.7177903732762834
freqs = [17262263.50383285 31270462.24446457 17235681.79847541  5597996.92086191
 35987511.86898271 17151433.11384352  7023260.57451295 22629570.05842017
 16857767.37652736 13823527.31845897]
eta_min = 0.7177903732762844	eta_max = 0.7177903732762764
af = 0.001159170611255296	bf = 1.0327722521438556	zeta = 0.0012750876723808255	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.85897150e-07 4.04361532e-06 5.74822789e-07 2.10275717e-08
 6.18415585e-06 6.70207126e-07 4.15648943e-08 1.43041150e-06
 5.76499351e-07 3.51864226e-07]
ene_total = [1.73019796 0.65949647 1.82567951 1.38190483 0.6337066  0.62462463
 1.37514729 1.1736322  1.3543771  0.60294347]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 2 obj = 3.6609545155387
eta = 0.7177903732762764
freqs = [17262263.50383282 31270462.24446465 17235681.79847537  5597996.92086191
 35987511.8689828  17151433.11384356  7023260.57451294 22629570.05842018
 16857767.37652736 13823527.31845901]
Done!
At round 60 energy consumption: 3.6609545155387906
At round 60 eta: 0.7177903732762764
At round 60 local rounds: 10.857538900724403
At round 60 global rounds: 27.03611541063686
At round 60 a_n: 7.287306191126021
gradient difference: 0.46964430809020996
train() client id: f_00000-0-0 loss: 0.851511  [   32/  126]
train() client id: f_00000-0-1 loss: 0.832598  [   64/  126]
train() client id: f_00000-0-2 loss: 1.332357  [   96/  126]
train() client id: f_00000-1-0 loss: 1.080301  [   32/  126]
train() client id: f_00000-1-1 loss: 0.867943  [   64/  126]
train() client id: f_00000-1-2 loss: 0.785764  [   96/  126]
train() client id: f_00000-2-0 loss: 0.985563  [   32/  126]
train() client id: f_00000-2-1 loss: 0.801104  [   64/  126]
train() client id: f_00000-2-2 loss: 0.692606  [   96/  126]
train() client id: f_00000-3-0 loss: 0.661996  [   32/  126]
train() client id: f_00000-3-1 loss: 0.888237  [   64/  126]
train() client id: f_00000-3-2 loss: 0.812331  [   96/  126]
train() client id: f_00000-4-0 loss: 0.718937  [   32/  126]
train() client id: f_00000-4-1 loss: 0.674172  [   64/  126]
train() client id: f_00000-4-2 loss: 0.782301  [   96/  126]
train() client id: f_00000-5-0 loss: 0.741967  [   32/  126]
train() client id: f_00000-5-1 loss: 0.775459  [   64/  126]
train() client id: f_00000-5-2 loss: 0.736453  [   96/  126]
train() client id: f_00000-6-0 loss: 0.812597  [   32/  126]
train() client id: f_00000-6-1 loss: 0.688985  [   64/  126]
train() client id: f_00000-6-2 loss: 0.642098  [   96/  126]
train() client id: f_00000-7-0 loss: 0.751269  [   32/  126]
train() client id: f_00000-7-1 loss: 0.725231  [   64/  126]
train() client id: f_00000-7-2 loss: 0.735311  [   96/  126]
train() client id: f_00000-8-0 loss: 0.726385  [   32/  126]
train() client id: f_00000-8-1 loss: 0.778993  [   64/  126]
train() client id: f_00000-8-2 loss: 0.571214  [   96/  126]
train() client id: f_00000-9-0 loss: 0.809214  [   32/  126]
train() client id: f_00000-9-1 loss: 0.634258  [   64/  126]
train() client id: f_00000-9-2 loss: 0.682783  [   96/  126]
train() client id: f_00001-0-0 loss: 0.555239  [   32/  265]
train() client id: f_00001-0-1 loss: 0.562140  [   64/  265]
train() client id: f_00001-0-2 loss: 0.433210  [   96/  265]
train() client id: f_00001-0-3 loss: 0.472893  [  128/  265]
train() client id: f_00001-0-4 loss: 0.645684  [  160/  265]
train() client id: f_00001-0-5 loss: 0.487228  [  192/  265]
train() client id: f_00001-0-6 loss: 0.582164  [  224/  265]
train() client id: f_00001-0-7 loss: 0.435676  [  256/  265]
train() client id: f_00001-1-0 loss: 0.548196  [   32/  265]
train() client id: f_00001-1-1 loss: 0.449111  [   64/  265]
train() client id: f_00001-1-2 loss: 0.440075  [   96/  265]
train() client id: f_00001-1-3 loss: 0.676865  [  128/  265]
train() client id: f_00001-1-4 loss: 0.551954  [  160/  265]
train() client id: f_00001-1-5 loss: 0.529032  [  192/  265]
train() client id: f_00001-1-6 loss: 0.443923  [  224/  265]
train() client id: f_00001-1-7 loss: 0.441985  [  256/  265]
train() client id: f_00001-2-0 loss: 0.410976  [   32/  265]
train() client id: f_00001-2-1 loss: 0.574045  [   64/  265]
train() client id: f_00001-2-2 loss: 0.526993  [   96/  265]
train() client id: f_00001-2-3 loss: 0.444741  [  128/  265]
train() client id: f_00001-2-4 loss: 0.499616  [  160/  265]
train() client id: f_00001-2-5 loss: 0.556991  [  192/  265]
train() client id: f_00001-2-6 loss: 0.513879  [  224/  265]
train() client id: f_00001-2-7 loss: 0.568972  [  256/  265]
train() client id: f_00001-3-0 loss: 0.520880  [   32/  265]
train() client id: f_00001-3-1 loss: 0.421330  [   64/  265]
train() client id: f_00001-3-2 loss: 0.453755  [   96/  265]
train() client id: f_00001-3-3 loss: 0.515097  [  128/  265]
train() client id: f_00001-3-4 loss: 0.609855  [  160/  265]
train() client id: f_00001-3-5 loss: 0.440505  [  192/  265]
train() client id: f_00001-3-6 loss: 0.446108  [  224/  265]
train() client id: f_00001-3-7 loss: 0.623158  [  256/  265]
train() client id: f_00001-4-0 loss: 0.508382  [   32/  265]
train() client id: f_00001-4-1 loss: 0.473574  [   64/  265]
train() client id: f_00001-4-2 loss: 0.639649  [   96/  265]
train() client id: f_00001-4-3 loss: 0.463533  [  128/  265]
train() client id: f_00001-4-4 loss: 0.514095  [  160/  265]
train() client id: f_00001-4-5 loss: 0.449945  [  192/  265]
train() client id: f_00001-4-6 loss: 0.519551  [  224/  265]
train() client id: f_00001-4-7 loss: 0.503361  [  256/  265]
train() client id: f_00001-5-0 loss: 0.483081  [   32/  265]
train() client id: f_00001-5-1 loss: 0.412817  [   64/  265]
train() client id: f_00001-5-2 loss: 0.504241  [   96/  265]
train() client id: f_00001-5-3 loss: 0.532845  [  128/  265]
train() client id: f_00001-5-4 loss: 0.571415  [  160/  265]
train() client id: f_00001-5-5 loss: 0.597972  [  192/  265]
train() client id: f_00001-5-6 loss: 0.493529  [  224/  265]
train() client id: f_00001-5-7 loss: 0.487562  [  256/  265]
train() client id: f_00001-6-0 loss: 0.533015  [   32/  265]
train() client id: f_00001-6-1 loss: 0.452727  [   64/  265]
train() client id: f_00001-6-2 loss: 0.603763  [   96/  265]
train() client id: f_00001-6-3 loss: 0.414619  [  128/  265]
train() client id: f_00001-6-4 loss: 0.504322  [  160/  265]
train() client id: f_00001-6-5 loss: 0.526708  [  192/  265]
train() client id: f_00001-6-6 loss: 0.530454  [  224/  265]
train() client id: f_00001-6-7 loss: 0.505503  [  256/  265]
train() client id: f_00001-7-0 loss: 0.496103  [   32/  265]
train() client id: f_00001-7-1 loss: 0.659469  [   64/  265]
train() client id: f_00001-7-2 loss: 0.445532  [   96/  265]
train() client id: f_00001-7-3 loss: 0.521431  [  128/  265]
train() client id: f_00001-7-4 loss: 0.470650  [  160/  265]
train() client id: f_00001-7-5 loss: 0.472984  [  192/  265]
train() client id: f_00001-7-6 loss: 0.500009  [  224/  265]
train() client id: f_00001-7-7 loss: 0.505396  [  256/  265]
train() client id: f_00001-8-0 loss: 0.568230  [   32/  265]
train() client id: f_00001-8-1 loss: 0.431085  [   64/  265]
train() client id: f_00001-8-2 loss: 0.476632  [   96/  265]
train() client id: f_00001-8-3 loss: 0.515797  [  128/  265]
train() client id: f_00001-8-4 loss: 0.493977  [  160/  265]
train() client id: f_00001-8-5 loss: 0.456317  [  192/  265]
train() client id: f_00001-8-6 loss: 0.574870  [  224/  265]
train() client id: f_00001-8-7 loss: 0.552456  [  256/  265]
train() client id: f_00001-9-0 loss: 0.483547  [   32/  265]
train() client id: f_00001-9-1 loss: 0.459110  [   64/  265]
train() client id: f_00001-9-2 loss: 0.506869  [   96/  265]
train() client id: f_00001-9-3 loss: 0.573157  [  128/  265]
train() client id: f_00001-9-4 loss: 0.567032  [  160/  265]
train() client id: f_00001-9-5 loss: 0.448571  [  192/  265]
train() client id: f_00001-9-6 loss: 0.573510  [  224/  265]
train() client id: f_00001-9-7 loss: 0.466725  [  256/  265]
train() client id: f_00002-0-0 loss: 0.926202  [   32/  124]
train() client id: f_00002-0-1 loss: 1.118633  [   64/  124]
train() client id: f_00002-0-2 loss: 0.951838  [   96/  124]
train() client id: f_00002-1-0 loss: 0.872675  [   32/  124]
train() client id: f_00002-1-1 loss: 0.954180  [   64/  124]
train() client id: f_00002-1-2 loss: 1.055331  [   96/  124]
train() client id: f_00002-2-0 loss: 0.847878  [   32/  124]
train() client id: f_00002-2-1 loss: 0.949668  [   64/  124]
train() client id: f_00002-2-2 loss: 0.983699  [   96/  124]
train() client id: f_00002-3-0 loss: 0.901938  [   32/  124]
train() client id: f_00002-3-1 loss: 0.917847  [   64/  124]
train() client id: f_00002-3-2 loss: 0.806120  [   96/  124]
train() client id: f_00002-4-0 loss: 0.876832  [   32/  124]
train() client id: f_00002-4-1 loss: 0.748470  [   64/  124]
train() client id: f_00002-4-2 loss: 0.941494  [   96/  124]
train() client id: f_00002-5-0 loss: 0.685522  [   32/  124]
train() client id: f_00002-5-1 loss: 0.794238  [   64/  124]
train() client id: f_00002-5-2 loss: 0.758379  [   96/  124]
train() client id: f_00002-6-0 loss: 0.848923  [   32/  124]
train() client id: f_00002-6-1 loss: 0.895687  [   64/  124]
train() client id: f_00002-6-2 loss: 0.753432  [   96/  124]
train() client id: f_00002-7-0 loss: 0.873815  [   32/  124]
train() client id: f_00002-7-1 loss: 0.693574  [   64/  124]
train() client id: f_00002-7-2 loss: 0.986914  [   96/  124]
train() client id: f_00002-8-0 loss: 0.767676  [   32/  124]
train() client id: f_00002-8-1 loss: 0.952406  [   64/  124]
train() client id: f_00002-8-2 loss: 0.748430  [   96/  124]
train() client id: f_00002-9-0 loss: 0.952507  [   32/  124]
train() client id: f_00002-9-1 loss: 0.730817  [   64/  124]
train() client id: f_00002-9-2 loss: 0.654436  [   96/  124]
train() client id: f_00003-0-0 loss: 0.961039  [   32/   43]
train() client id: f_00003-1-0 loss: 0.957371  [   32/   43]
train() client id: f_00003-2-0 loss: 0.980360  [   32/   43]
train() client id: f_00003-3-0 loss: 0.968650  [   32/   43]
train() client id: f_00003-4-0 loss: 0.701939  [   32/   43]
train() client id: f_00003-5-0 loss: 1.076383  [   32/   43]
train() client id: f_00003-6-0 loss: 0.775903  [   32/   43]
train() client id: f_00003-7-0 loss: 0.773671  [   32/   43]
train() client id: f_00003-8-0 loss: 0.782599  [   32/   43]
train() client id: f_00003-9-0 loss: 0.731450  [   32/   43]
train() client id: f_00004-0-0 loss: 0.906418  [   32/  306]
train() client id: f_00004-0-1 loss: 0.902765  [   64/  306]
train() client id: f_00004-0-2 loss: 0.871415  [   96/  306]
train() client id: f_00004-0-3 loss: 0.811456  [  128/  306]
train() client id: f_00004-0-4 loss: 0.934831  [  160/  306]
train() client id: f_00004-0-5 loss: 0.801533  [  192/  306]
train() client id: f_00004-0-6 loss: 0.902103  [  224/  306]
train() client id: f_00004-0-7 loss: 0.746748  [  256/  306]
train() client id: f_00004-0-8 loss: 0.779800  [  288/  306]
train() client id: f_00004-1-0 loss: 0.786633  [   32/  306]
train() client id: f_00004-1-1 loss: 0.908156  [   64/  306]
train() client id: f_00004-1-2 loss: 0.866741  [   96/  306]
train() client id: f_00004-1-3 loss: 0.983075  [  128/  306]
train() client id: f_00004-1-4 loss: 0.930948  [  160/  306]
train() client id: f_00004-1-5 loss: 0.696583  [  192/  306]
train() client id: f_00004-1-6 loss: 0.814823  [  224/  306]
train() client id: f_00004-1-7 loss: 0.941553  [  256/  306]
train() client id: f_00004-1-8 loss: 0.744535  [  288/  306]
train() client id: f_00004-2-0 loss: 0.998303  [   32/  306]
train() client id: f_00004-2-1 loss: 0.828525  [   64/  306]
train() client id: f_00004-2-2 loss: 0.709980  [   96/  306]
train() client id: f_00004-2-3 loss: 0.923963  [  128/  306]
train() client id: f_00004-2-4 loss: 0.797360  [  160/  306]
train() client id: f_00004-2-5 loss: 0.795772  [  192/  306]
train() client id: f_00004-2-6 loss: 0.776401  [  224/  306]
train() client id: f_00004-2-7 loss: 0.861237  [  256/  306]
train() client id: f_00004-2-8 loss: 0.903955  [  288/  306]
train() client id: f_00004-3-0 loss: 0.998569  [   32/  306]
train() client id: f_00004-3-1 loss: 0.868219  [   64/  306]
train() client id: f_00004-3-2 loss: 0.826660  [   96/  306]
train() client id: f_00004-3-3 loss: 0.842752  [  128/  306]
train() client id: f_00004-3-4 loss: 0.833914  [  160/  306]
train() client id: f_00004-3-5 loss: 0.817126  [  192/  306]
train() client id: f_00004-3-6 loss: 0.711304  [  224/  306]
train() client id: f_00004-3-7 loss: 0.771907  [  256/  306]
train() client id: f_00004-3-8 loss: 0.884629  [  288/  306]
train() client id: f_00004-4-0 loss: 0.856484  [   32/  306]
train() client id: f_00004-4-1 loss: 0.897311  [   64/  306]
train() client id: f_00004-4-2 loss: 0.725754  [   96/  306]
train() client id: f_00004-4-3 loss: 0.842980  [  128/  306]
train() client id: f_00004-4-4 loss: 0.899960  [  160/  306]
train() client id: f_00004-4-5 loss: 0.816203  [  192/  306]
train() client id: f_00004-4-6 loss: 0.980141  [  224/  306]
train() client id: f_00004-4-7 loss: 0.929986  [  256/  306]
train() client id: f_00004-4-8 loss: 0.780261  [  288/  306]
train() client id: f_00004-5-0 loss: 0.791058  [   32/  306]
train() client id: f_00004-5-1 loss: 0.708977  [   64/  306]
train() client id: f_00004-5-2 loss: 0.760549  [   96/  306]
train() client id: f_00004-5-3 loss: 0.866619  [  128/  306]
train() client id: f_00004-5-4 loss: 0.806162  [  160/  306]
train() client id: f_00004-5-5 loss: 0.889489  [  192/  306]
train() client id: f_00004-5-6 loss: 1.012577  [  224/  306]
train() client id: f_00004-5-7 loss: 0.890002  [  256/  306]
train() client id: f_00004-5-8 loss: 0.853842  [  288/  306]
train() client id: f_00004-6-0 loss: 0.773523  [   32/  306]
train() client id: f_00004-6-1 loss: 0.744617  [   64/  306]
train() client id: f_00004-6-2 loss: 0.914375  [   96/  306]
train() client id: f_00004-6-3 loss: 0.785226  [  128/  306]
train() client id: f_00004-6-4 loss: 0.730102  [  160/  306]
train() client id: f_00004-6-5 loss: 0.859544  [  192/  306]
train() client id: f_00004-6-6 loss: 0.955571  [  224/  306]
train() client id: f_00004-6-7 loss: 0.904280  [  256/  306]
train() client id: f_00004-6-8 loss: 0.860264  [  288/  306]
train() client id: f_00004-7-0 loss: 0.812487  [   32/  306]
train() client id: f_00004-7-1 loss: 0.827116  [   64/  306]
train() client id: f_00004-7-2 loss: 0.859089  [   96/  306]
train() client id: f_00004-7-3 loss: 0.779335  [  128/  306]
train() client id: f_00004-7-4 loss: 0.901439  [  160/  306]
train() client id: f_00004-7-5 loss: 0.809110  [  192/  306]
train() client id: f_00004-7-6 loss: 0.857716  [  224/  306]
train() client id: f_00004-7-7 loss: 0.952964  [  256/  306]
train() client id: f_00004-7-8 loss: 0.865519  [  288/  306]
train() client id: f_00004-8-0 loss: 0.837989  [   32/  306]
train() client id: f_00004-8-1 loss: 0.830284  [   64/  306]
train() client id: f_00004-8-2 loss: 0.948873  [   96/  306]
train() client id: f_00004-8-3 loss: 0.731667  [  128/  306]
train() client id: f_00004-8-4 loss: 0.930923  [  160/  306]
train() client id: f_00004-8-5 loss: 0.898184  [  192/  306]
train() client id: f_00004-8-6 loss: 0.872351  [  224/  306]
train() client id: f_00004-8-7 loss: 0.846172  [  256/  306]
train() client id: f_00004-8-8 loss: 0.761145  [  288/  306]
train() client id: f_00004-9-0 loss: 0.821370  [   32/  306]
train() client id: f_00004-9-1 loss: 0.871587  [   64/  306]
train() client id: f_00004-9-2 loss: 0.962898  [   96/  306]
train() client id: f_00004-9-3 loss: 0.784467  [  128/  306]
train() client id: f_00004-9-4 loss: 0.861369  [  160/  306]
train() client id: f_00004-9-5 loss: 0.743881  [  192/  306]
train() client id: f_00004-9-6 loss: 0.780526  [  224/  306]
train() client id: f_00004-9-7 loss: 0.890218  [  256/  306]
train() client id: f_00004-9-8 loss: 0.866113  [  288/  306]
train() client id: f_00005-0-0 loss: 0.519542  [   32/  146]
train() client id: f_00005-0-1 loss: 0.719650  [   64/  146]
train() client id: f_00005-0-2 loss: 0.605372  [   96/  146]
train() client id: f_00005-0-3 loss: 0.829210  [  128/  146]
train() client id: f_00005-1-0 loss: 0.540811  [   32/  146]
train() client id: f_00005-1-1 loss: 0.682182  [   64/  146]
train() client id: f_00005-1-2 loss: 0.623654  [   96/  146]
train() client id: f_00005-1-3 loss: 0.826861  [  128/  146]
train() client id: f_00005-2-0 loss: 0.611707  [   32/  146]
train() client id: f_00005-2-1 loss: 0.404805  [   64/  146]
train() client id: f_00005-2-2 loss: 0.837778  [   96/  146]
train() client id: f_00005-2-3 loss: 0.567852  [  128/  146]
train() client id: f_00005-3-0 loss: 0.755806  [   32/  146]
train() client id: f_00005-3-1 loss: 0.617421  [   64/  146]
train() client id: f_00005-3-2 loss: 0.681958  [   96/  146]
train() client id: f_00005-3-3 loss: 0.590213  [  128/  146]
train() client id: f_00005-4-0 loss: 0.665489  [   32/  146]
train() client id: f_00005-4-1 loss: 0.622546  [   64/  146]
train() client id: f_00005-4-2 loss: 0.490398  [   96/  146]
train() client id: f_00005-4-3 loss: 0.841286  [  128/  146]
train() client id: f_00005-5-0 loss: 0.835240  [   32/  146]
train() client id: f_00005-5-1 loss: 0.547695  [   64/  146]
train() client id: f_00005-5-2 loss: 0.659634  [   96/  146]
train() client id: f_00005-5-3 loss: 0.504155  [  128/  146]
train() client id: f_00005-6-0 loss: 0.594363  [   32/  146]
train() client id: f_00005-6-1 loss: 0.422827  [   64/  146]
train() client id: f_00005-6-2 loss: 0.828662  [   96/  146]
train() client id: f_00005-6-3 loss: 0.898047  [  128/  146]
train() client id: f_00005-7-0 loss: 0.512915  [   32/  146]
train() client id: f_00005-7-1 loss: 0.645121  [   64/  146]
train() client id: f_00005-7-2 loss: 0.680413  [   96/  146]
train() client id: f_00005-7-3 loss: 0.690840  [  128/  146]
train() client id: f_00005-8-0 loss: 0.660441  [   32/  146]
train() client id: f_00005-8-1 loss: 0.819069  [   64/  146]
train() client id: f_00005-8-2 loss: 0.819533  [   96/  146]
train() client id: f_00005-8-3 loss: 0.398824  [  128/  146]
train() client id: f_00005-9-0 loss: 0.523254  [   32/  146]
train() client id: f_00005-9-1 loss: 0.730631  [   64/  146]
train() client id: f_00005-9-2 loss: 0.550574  [   96/  146]
train() client id: f_00005-9-3 loss: 0.946330  [  128/  146]
train() client id: f_00006-0-0 loss: 0.564883  [   32/   54]
train() client id: f_00006-1-0 loss: 0.507765  [   32/   54]
train() client id: f_00006-2-0 loss: 0.479562  [   32/   54]
train() client id: f_00006-3-0 loss: 0.570478  [   32/   54]
train() client id: f_00006-4-0 loss: 0.465086  [   32/   54]
train() client id: f_00006-5-0 loss: 0.494338  [   32/   54]
train() client id: f_00006-6-0 loss: 0.507796  [   32/   54]
train() client id: f_00006-7-0 loss: 0.578527  [   32/   54]
train() client id: f_00006-8-0 loss: 0.462227  [   32/   54]
train() client id: f_00006-9-0 loss: 0.560642  [   32/   54]
train() client id: f_00007-0-0 loss: 0.346324  [   32/  179]
train() client id: f_00007-0-1 loss: 0.380599  [   64/  179]
train() client id: f_00007-0-2 loss: 0.558027  [   96/  179]
train() client id: f_00007-0-3 loss: 0.677881  [  128/  179]
train() client id: f_00007-0-4 loss: 0.746404  [  160/  179]
train() client id: f_00007-1-0 loss: 0.659039  [   32/  179]
train() client id: f_00007-1-1 loss: 0.584501  [   64/  179]
train() client id: f_00007-1-2 loss: 0.634437  [   96/  179]
train() client id: f_00007-1-3 loss: 0.427860  [  128/  179]
train() client id: f_00007-1-4 loss: 0.411461  [  160/  179]
train() client id: f_00007-2-0 loss: 0.548935  [   32/  179]
train() client id: f_00007-2-1 loss: 0.618249  [   64/  179]
train() client id: f_00007-2-2 loss: 0.343469  [   96/  179]
train() client id: f_00007-2-3 loss: 0.476474  [  128/  179]
train() client id: f_00007-2-4 loss: 0.359539  [  160/  179]
train() client id: f_00007-3-0 loss: 0.616405  [   32/  179]
train() client id: f_00007-3-1 loss: 0.368646  [   64/  179]
train() client id: f_00007-3-2 loss: 0.529825  [   96/  179]
train() client id: f_00007-3-3 loss: 0.528068  [  128/  179]
train() client id: f_00007-3-4 loss: 0.575053  [  160/  179]
train() client id: f_00007-4-0 loss: 0.615156  [   32/  179]
train() client id: f_00007-4-1 loss: 0.506049  [   64/  179]
train() client id: f_00007-4-2 loss: 0.376049  [   96/  179]
train() client id: f_00007-4-3 loss: 0.569202  [  128/  179]
train() client id: f_00007-4-4 loss: 0.531119  [  160/  179]
train() client id: f_00007-5-0 loss: 0.584907  [   32/  179]
train() client id: f_00007-5-1 loss: 0.528804  [   64/  179]
train() client id: f_00007-5-2 loss: 0.472745  [   96/  179]
train() client id: f_00007-5-3 loss: 0.548794  [  128/  179]
train() client id: f_00007-5-4 loss: 0.391977  [  160/  179]
train() client id: f_00007-6-0 loss: 0.310541  [   32/  179]
train() client id: f_00007-6-1 loss: 0.646450  [   64/  179]
train() client id: f_00007-6-2 loss: 0.602034  [   96/  179]
train() client id: f_00007-6-3 loss: 0.453090  [  128/  179]
train() client id: f_00007-6-4 loss: 0.542418  [  160/  179]
train() client id: f_00007-7-0 loss: 0.528691  [   32/  179]
train() client id: f_00007-7-1 loss: 0.544632  [   64/  179]
train() client id: f_00007-7-2 loss: 0.478171  [   96/  179]
train() client id: f_00007-7-3 loss: 0.459169  [  128/  179]
train() client id: f_00007-7-4 loss: 0.399667  [  160/  179]
train() client id: f_00007-8-0 loss: 0.372040  [   32/  179]
train() client id: f_00007-8-1 loss: 0.430624  [   64/  179]
train() client id: f_00007-8-2 loss: 0.470290  [   96/  179]
train() client id: f_00007-8-3 loss: 0.629514  [  128/  179]
train() client id: f_00007-8-4 loss: 0.649222  [  160/  179]
train() client id: f_00007-9-0 loss: 0.613464  [   32/  179]
train() client id: f_00007-9-1 loss: 0.526858  [   64/  179]
train() client id: f_00007-9-2 loss: 0.513651  [   96/  179]
train() client id: f_00007-9-3 loss: 0.578026  [  128/  179]
train() client id: f_00007-9-4 loss: 0.440018  [  160/  179]
train() client id: f_00008-0-0 loss: 0.577874  [   32/  130]
train() client id: f_00008-0-1 loss: 0.661451  [   64/  130]
train() client id: f_00008-0-2 loss: 0.572513  [   96/  130]
train() client id: f_00008-0-3 loss: 0.633610  [  128/  130]
train() client id: f_00008-1-0 loss: 0.536703  [   32/  130]
train() client id: f_00008-1-1 loss: 0.683181  [   64/  130]
train() client id: f_00008-1-2 loss: 0.579210  [   96/  130]
train() client id: f_00008-1-3 loss: 0.678251  [  128/  130]
train() client id: f_00008-2-0 loss: 0.545829  [   32/  130]
train() client id: f_00008-2-1 loss: 0.649594  [   64/  130]
train() client id: f_00008-2-2 loss: 0.714768  [   96/  130]
train() client id: f_00008-2-3 loss: 0.573251  [  128/  130]
train() client id: f_00008-3-0 loss: 0.635769  [   32/  130]
train() client id: f_00008-3-1 loss: 0.568793  [   64/  130]
train() client id: f_00008-3-2 loss: 0.661732  [   96/  130]
train() client id: f_00008-3-3 loss: 0.616405  [  128/  130]
train() client id: f_00008-4-0 loss: 0.666773  [   32/  130]
train() client id: f_00008-4-1 loss: 0.537676  [   64/  130]
train() client id: f_00008-4-2 loss: 0.768378  [   96/  130]
train() client id: f_00008-4-3 loss: 0.446602  [  128/  130]
train() client id: f_00008-5-0 loss: 0.702587  [   32/  130]
train() client id: f_00008-5-1 loss: 0.561826  [   64/  130]
train() client id: f_00008-5-2 loss: 0.667069  [   96/  130]
train() client id: f_00008-5-3 loss: 0.555867  [  128/  130]
train() client id: f_00008-6-0 loss: 0.596452  [   32/  130]
train() client id: f_00008-6-1 loss: 0.565971  [   64/  130]
train() client id: f_00008-6-2 loss: 0.571785  [   96/  130]
train() client id: f_00008-6-3 loss: 0.710710  [  128/  130]
train() client id: f_00008-7-0 loss: 0.555720  [   32/  130]
train() client id: f_00008-7-1 loss: 0.607596  [   64/  130]
train() client id: f_00008-7-2 loss: 0.628291  [   96/  130]
train() client id: f_00008-7-3 loss: 0.679593  [  128/  130]
train() client id: f_00008-8-0 loss: 0.603457  [   32/  130]
train() client id: f_00008-8-1 loss: 0.745316  [   64/  130]
train() client id: f_00008-8-2 loss: 0.563843  [   96/  130]
train() client id: f_00008-8-3 loss: 0.569334  [  128/  130]
train() client id: f_00008-9-0 loss: 0.580057  [   32/  130]
train() client id: f_00008-9-1 loss: 0.608501  [   64/  130]
train() client id: f_00008-9-2 loss: 0.629829  [   96/  130]
train() client id: f_00008-9-3 loss: 0.675212  [  128/  130]
train() client id: f_00009-0-0 loss: 0.916794  [   32/  118]
train() client id: f_00009-0-1 loss: 0.857112  [   64/  118]
train() client id: f_00009-0-2 loss: 0.992592  [   96/  118]
train() client id: f_00009-1-0 loss: 0.996357  [   32/  118]
train() client id: f_00009-1-1 loss: 0.989309  [   64/  118]
train() client id: f_00009-1-2 loss: 0.795620  [   96/  118]
train() client id: f_00009-2-0 loss: 0.957498  [   32/  118]
train() client id: f_00009-2-1 loss: 0.860190  [   64/  118]
train() client id: f_00009-2-2 loss: 0.821920  [   96/  118]
train() client id: f_00009-3-0 loss: 0.847737  [   32/  118]
train() client id: f_00009-3-1 loss: 0.947842  [   64/  118]
train() client id: f_00009-3-2 loss: 0.868984  [   96/  118]
train() client id: f_00009-4-0 loss: 0.821587  [   32/  118]
train() client id: f_00009-4-1 loss: 0.886217  [   64/  118]
train() client id: f_00009-4-2 loss: 0.825573  [   96/  118]
train() client id: f_00009-5-0 loss: 0.874081  [   32/  118]
train() client id: f_00009-5-1 loss: 0.703221  [   64/  118]
train() client id: f_00009-5-2 loss: 0.842226  [   96/  118]
train() client id: f_00009-6-0 loss: 0.832179  [   32/  118]
train() client id: f_00009-6-1 loss: 0.904766  [   64/  118]
train() client id: f_00009-6-2 loss: 0.766862  [   96/  118]
train() client id: f_00009-7-0 loss: 0.852098  [   32/  118]
train() client id: f_00009-7-1 loss: 0.807463  [   64/  118]
train() client id: f_00009-7-2 loss: 0.741950  [   96/  118]
train() client id: f_00009-8-0 loss: 0.888262  [   32/  118]
train() client id: f_00009-8-1 loss: 0.680282  [   64/  118]
train() client id: f_00009-8-2 loss: 0.783667  [   96/  118]
train() client id: f_00009-9-0 loss: 0.814767  [   32/  118]
train() client id: f_00009-9-1 loss: 0.726036  [   64/  118]
train() client id: f_00009-9-2 loss: 0.855922  [   96/  118]
At round 60 accuracy: 0.6472148541114059
At round 60 training accuracy: 0.5888665325285044
At round 60 training loss: 0.8265239606465892
update_location
xs = [  -3.9056584     4.20031788  320.00902392   18.81129433    0.97929623
    3.95640986 -282.44319194 -261.32485185  304.66397685 -247.06087855]
ys = [ 312.5879595   295.55583871    1.32061395 -282.45517586  274.35018685
  257.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [328.2171333  312.04310033 335.27230636 300.22456789 292.0085342
 276.55702758 299.63485647 279.80592283 321.13674465 266.56160558]
dists_bs = [219.55345664 215.9779252  524.5936169  496.81793016 202.08704146
 197.18919898 207.59256862 194.39607637 504.83863615 185.5620205 ]
uav_gains = [1.63906976e-12 2.14056491e-12 1.47563907e-12 2.66124979e-12
 3.12682385e-12 4.29784027e-12 2.69155883e-12 4.01567151e-12
 1.83392238e-12 5.29895911e-12]
bs_gains = [3.06888987e-11 3.21327477e-11 2.67787117e-12 3.11847132e-12
 3.87067209e-11 4.14592405e-11 3.59005447e-11 4.31488375e-11
 2.98171993e-12 4.91501249e-11]
Round 61
-------------------------------
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.95458156 5.94128521 2.91869021 1.0805571  6.84914591 3.29632295
 1.32415061 4.08054691 3.00608516 2.67279298]
obj_prev = 34.12415860899734
eta_min = 3.98916673942854e-32	eta_max = 0.9387143479457988
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 7.8273257066737205	eta = 0.9090909090909091
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 18.61065565296208	eta = 0.3823481974584871
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 12.717399503431304	eta = 0.5595287496088128
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.681826444107466	eta = 0.60912997436458
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617979436161402	eta = 0.6124774692131586
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704178505617	eta = 0.6124919806097139
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704173351733	eta = 0.6124919808814296
eta = 0.6124919808814296
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.04045173 0.08507706 0.03980964 0.01380496 0.09823992 0.04687264
 0.01733646 0.05746714 0.04173591 0.03788337]
ene_total = [1.16278399 1.74880761 1.17244705 0.58140006 1.9903631  1.02228243
 0.6462616  1.35340359 1.09164915 0.84830559]
ti_comp = [1.21927116 1.35390191 1.20771185 1.26195871 1.35708161 1.35819303
 1.2627831  1.28806187 1.26993947 1.36081379]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [2.78285633e-06 2.09963099e-05 2.70344885e-06 1.03251135e-07
 3.21760040e-05 3.48911535e-06 2.04222466e-07 7.14933098e-06
 2.81736924e-06 1.83496748e-06]
ene_total = [0.40211875 0.14928058 0.4238552  0.32179204 0.14351122 0.14088165
 0.32024363 0.27283609 0.3068348  0.13592205]
optimize_network iter = 0 obj = 2.617276020137961
eta = 0.6124919808814296
freqs = [16588488.88317586 31419209.64704492 16481431.96089219  5469654.50966602
 36195288.80600585 17255515.63488949  6864384.21294392 22307602.23984509
 16432245.4685148  13919379.90342778]
eta_min = 0.6124919808814304	eta_max = 0.7228965206792303
af = 0.0011038579011319113	bf = 1.0136743134601105	zeta = 0.0012142436912451024	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.41052682e-07 4.08217617e-06 5.25614002e-07 2.00744476e-08
 6.25577148e-06 6.78366036e-07 3.97056477e-08 1.38999799e-06
 5.47762803e-07 3.56760810e-07]
ene_total = [1.71388696 0.63496578 1.80654562 1.37166028 0.60965148 0.60029516
 1.36505352 1.16252622 1.30772847 0.57926126]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 1 obj = 3.659284534384905
eta = 0.7228965206792303
freqs = [16508552.89543737 29777055.50547944 16481431.96089219  5352148.75441332
 34268839.33358462 16331361.10328626  6714818.53144301 21618818.75231183
 16031043.09450184 13163002.59895364]
eta_min = 0.7228965206792295	eta_max = 0.7228965206792275
af = 0.0010051578008740583	bf = 1.0136743134601105	zeta = 0.0011056735809614642	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.35850837e-07 3.66661011e-06 5.25614002e-07 1.92211852e-08
 5.60758124e-06 6.07649264e-07 3.79942326e-08 1.30548627e-06
 5.21341470e-07 3.19041588e-07]
ene_total = [1.71388654 0.63493247 1.80654562 1.37166021 0.60959952 0.60028949
 1.36505338 1.16251944 1.30772635 0.57925823]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 2 obj = 3.6592845343848683
eta = 0.7228965206792275
freqs = [16508552.89543735 29777055.50547947 16481431.96089218  5352148.75441332
 34268839.33358465 16331361.10328627  6714818.531443   21618818.75231183
 16031043.09450183 13163002.59895365]
Done!
At round 61 energy consumption: 3.659284534384905
At round 61 eta: 0.7228965206792275
At round 61 local rounds: 10.625424748609518
At round 61 global rounds: 26.298140351714242
At round 61 a_n: 6.944760344156705
gradient difference: 0.5798643827438354
train() client id: f_00000-0-0 loss: 0.873967  [   32/  126]
train() client id: f_00000-0-1 loss: 0.915862  [   64/  126]
train() client id: f_00000-0-2 loss: 0.903197  [   96/  126]
train() client id: f_00000-1-0 loss: 0.866066  [   32/  126]
train() client id: f_00000-1-1 loss: 1.012088  [   64/  126]
train() client id: f_00000-1-2 loss: 0.978871  [   96/  126]
train() client id: f_00000-2-0 loss: 0.713418  [   32/  126]
train() client id: f_00000-2-1 loss: 0.940606  [   64/  126]
train() client id: f_00000-2-2 loss: 0.806846  [   96/  126]
train() client id: f_00000-3-0 loss: 0.863991  [   32/  126]
train() client id: f_00000-3-1 loss: 0.902270  [   64/  126]
train() client id: f_00000-3-2 loss: 0.803203  [   96/  126]
train() client id: f_00000-4-0 loss: 0.917766  [   32/  126]
train() client id: f_00000-4-1 loss: 0.823000  [   64/  126]
train() client id: f_00000-4-2 loss: 0.896916  [   96/  126]
train() client id: f_00000-5-0 loss: 0.807763  [   32/  126]
train() client id: f_00000-5-1 loss: 0.875650  [   64/  126]
train() client id: f_00000-5-2 loss: 0.784437  [   96/  126]
train() client id: f_00000-6-0 loss: 0.804502  [   32/  126]
train() client id: f_00000-6-1 loss: 0.928618  [   64/  126]
train() client id: f_00000-6-2 loss: 0.916587  [   96/  126]
train() client id: f_00000-7-0 loss: 0.843908  [   32/  126]
train() client id: f_00000-7-1 loss: 0.846400  [   64/  126]
train() client id: f_00000-7-2 loss: 0.852244  [   96/  126]
train() client id: f_00000-8-0 loss: 0.901402  [   32/  126]
train() client id: f_00000-8-1 loss: 0.762072  [   64/  126]
train() client id: f_00000-8-2 loss: 0.909873  [   96/  126]
train() client id: f_00000-9-0 loss: 0.792623  [   32/  126]
train() client id: f_00000-9-1 loss: 0.892849  [   64/  126]
train() client id: f_00000-9-2 loss: 0.836317  [   96/  126]
train() client id: f_00001-0-0 loss: 0.510314  [   32/  265]
train() client id: f_00001-0-1 loss: 0.434208  [   64/  265]
train() client id: f_00001-0-2 loss: 0.500796  [   96/  265]
train() client id: f_00001-0-3 loss: 0.611244  [  128/  265]
train() client id: f_00001-0-4 loss: 0.484740  [  160/  265]
train() client id: f_00001-0-5 loss: 0.456021  [  192/  265]
train() client id: f_00001-0-6 loss: 0.541137  [  224/  265]
train() client id: f_00001-0-7 loss: 0.529386  [  256/  265]
train() client id: f_00001-1-0 loss: 0.562198  [   32/  265]
train() client id: f_00001-1-1 loss: 0.432338  [   64/  265]
train() client id: f_00001-1-2 loss: 0.567343  [   96/  265]
train() client id: f_00001-1-3 loss: 0.499906  [  128/  265]
train() client id: f_00001-1-4 loss: 0.500735  [  160/  265]
train() client id: f_00001-1-5 loss: 0.492275  [  192/  265]
train() client id: f_00001-1-6 loss: 0.567247  [  224/  265]
train() client id: f_00001-1-7 loss: 0.458510  [  256/  265]
train() client id: f_00001-2-0 loss: 0.457720  [   32/  265]
train() client id: f_00001-2-1 loss: 0.514685  [   64/  265]
train() client id: f_00001-2-2 loss: 0.461931  [   96/  265]
train() client id: f_00001-2-3 loss: 0.559704  [  128/  265]
train() client id: f_00001-2-4 loss: 0.404317  [  160/  265]
train() client id: f_00001-2-5 loss: 0.521253  [  192/  265]
train() client id: f_00001-2-6 loss: 0.500565  [  224/  265]
train() client id: f_00001-2-7 loss: 0.584068  [  256/  265]
train() client id: f_00001-3-0 loss: 0.422759  [   32/  265]
train() client id: f_00001-3-1 loss: 0.530682  [   64/  265]
train() client id: f_00001-3-2 loss: 0.513064  [   96/  265]
train() client id: f_00001-3-3 loss: 0.516037  [  128/  265]
train() client id: f_00001-3-4 loss: 0.597856  [  160/  265]
train() client id: f_00001-3-5 loss: 0.400924  [  192/  265]
train() client id: f_00001-3-6 loss: 0.623954  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411099  [  256/  265]
train() client id: f_00001-4-0 loss: 0.593941  [   32/  265]
train() client id: f_00001-4-1 loss: 0.468350  [   64/  265]
train() client id: f_00001-4-2 loss: 0.607674  [   96/  265]
train() client id: f_00001-4-3 loss: 0.412992  [  128/  265]
train() client id: f_00001-4-4 loss: 0.521429  [  160/  265]
train() client id: f_00001-4-5 loss: 0.447570  [  192/  265]
train() client id: f_00001-4-6 loss: 0.414904  [  224/  265]
train() client id: f_00001-4-7 loss: 0.487025  [  256/  265]
train() client id: f_00001-5-0 loss: 0.578256  [   32/  265]
train() client id: f_00001-5-1 loss: 0.469455  [   64/  265]
train() client id: f_00001-5-2 loss: 0.402411  [   96/  265]
train() client id: f_00001-5-3 loss: 0.413698  [  128/  265]
train() client id: f_00001-5-4 loss: 0.600721  [  160/  265]
train() client id: f_00001-5-5 loss: 0.578118  [  192/  265]
train() client id: f_00001-5-6 loss: 0.459106  [  224/  265]
train() client id: f_00001-5-7 loss: 0.492957  [  256/  265]
train() client id: f_00001-6-0 loss: 0.491683  [   32/  265]
train() client id: f_00001-6-1 loss: 0.400415  [   64/  265]
train() client id: f_00001-6-2 loss: 0.554745  [   96/  265]
train() client id: f_00001-6-3 loss: 0.454727  [  128/  265]
train() client id: f_00001-6-4 loss: 0.562107  [  160/  265]
train() client id: f_00001-6-5 loss: 0.449010  [  192/  265]
train() client id: f_00001-6-6 loss: 0.520737  [  224/  265]
train() client id: f_00001-6-7 loss: 0.561410  [  256/  265]
train() client id: f_00001-7-0 loss: 0.491099  [   32/  265]
train() client id: f_00001-7-1 loss: 0.649804  [   64/  265]
train() client id: f_00001-7-2 loss: 0.467484  [   96/  265]
train() client id: f_00001-7-3 loss: 0.488433  [  128/  265]
train() client id: f_00001-7-4 loss: 0.463799  [  160/  265]
train() client id: f_00001-7-5 loss: 0.404678  [  192/  265]
train() client id: f_00001-7-6 loss: 0.403948  [  224/  265]
train() client id: f_00001-7-7 loss: 0.616042  [  256/  265]
train() client id: f_00001-8-0 loss: 0.430338  [   32/  265]
train() client id: f_00001-8-1 loss: 0.500936  [   64/  265]
train() client id: f_00001-8-2 loss: 0.639372  [   96/  265]
train() client id: f_00001-8-3 loss: 0.456553  [  128/  265]
train() client id: f_00001-8-4 loss: 0.460022  [  160/  265]
train() client id: f_00001-8-5 loss: 0.484088  [  192/  265]
train() client id: f_00001-8-6 loss: 0.545761  [  224/  265]
train() client id: f_00001-8-7 loss: 0.476854  [  256/  265]
train() client id: f_00001-9-0 loss: 0.492637  [   32/  265]
train() client id: f_00001-9-1 loss: 0.511614  [   64/  265]
train() client id: f_00001-9-2 loss: 0.488431  [   96/  265]
train() client id: f_00001-9-3 loss: 0.500058  [  128/  265]
train() client id: f_00001-9-4 loss: 0.539704  [  160/  265]
train() client id: f_00001-9-5 loss: 0.555365  [  192/  265]
train() client id: f_00001-9-6 loss: 0.498296  [  224/  265]
train() client id: f_00001-9-7 loss: 0.390104  [  256/  265]
train() client id: f_00002-0-0 loss: 1.356231  [   32/  124]
train() client id: f_00002-0-1 loss: 1.187407  [   64/  124]
train() client id: f_00002-0-2 loss: 1.376908  [   96/  124]
train() client id: f_00002-1-0 loss: 1.306098  [   32/  124]
train() client id: f_00002-1-1 loss: 1.214308  [   64/  124]
train() client id: f_00002-1-2 loss: 1.364116  [   96/  124]
train() client id: f_00002-2-0 loss: 1.417268  [   32/  124]
train() client id: f_00002-2-1 loss: 1.163698  [   64/  124]
train() client id: f_00002-2-2 loss: 1.163028  [   96/  124]
train() client id: f_00002-3-0 loss: 1.074425  [   32/  124]
train() client id: f_00002-3-1 loss: 1.316886  [   64/  124]
train() client id: f_00002-3-2 loss: 1.248240  [   96/  124]
train() client id: f_00002-4-0 loss: 1.199062  [   32/  124]
train() client id: f_00002-4-1 loss: 1.157930  [   64/  124]
train() client id: f_00002-4-2 loss: 1.135337  [   96/  124]
train() client id: f_00002-5-0 loss: 1.236770  [   32/  124]
train() client id: f_00002-5-1 loss: 1.047763  [   64/  124]
train() client id: f_00002-5-2 loss: 1.093493  [   96/  124]
train() client id: f_00002-6-0 loss: 1.184124  [   32/  124]
train() client id: f_00002-6-1 loss: 1.190412  [   64/  124]
train() client id: f_00002-6-2 loss: 1.037563  [   96/  124]
train() client id: f_00002-7-0 loss: 1.196625  [   32/  124]
train() client id: f_00002-7-1 loss: 1.069567  [   64/  124]
train() client id: f_00002-7-2 loss: 1.070010  [   96/  124]
train() client id: f_00002-8-0 loss: 1.108915  [   32/  124]
train() client id: f_00002-8-1 loss: 1.126735  [   64/  124]
train() client id: f_00002-8-2 loss: 1.242079  [   96/  124]
train() client id: f_00002-9-0 loss: 1.276548  [   32/  124]
train() client id: f_00002-9-1 loss: 1.080190  [   64/  124]
train() client id: f_00002-9-2 loss: 1.134406  [   96/  124]
train() client id: f_00003-0-0 loss: 0.574504  [   32/   43]
train() client id: f_00003-1-0 loss: 0.715650  [   32/   43]
train() client id: f_00003-2-0 loss: 0.911763  [   32/   43]
train() client id: f_00003-3-0 loss: 0.684676  [   32/   43]
train() client id: f_00003-4-0 loss: 0.897727  [   32/   43]
train() client id: f_00003-5-0 loss: 0.614547  [   32/   43]
train() client id: f_00003-6-0 loss: 0.668896  [   32/   43]
train() client id: f_00003-7-0 loss: 0.752610  [   32/   43]
train() client id: f_00003-8-0 loss: 0.769708  [   32/   43]
train() client id: f_00003-9-0 loss: 0.798690  [   32/   43]
train() client id: f_00004-0-0 loss: 0.810367  [   32/  306]
train() client id: f_00004-0-1 loss: 0.768564  [   64/  306]
train() client id: f_00004-0-2 loss: 0.716379  [   96/  306]
train() client id: f_00004-0-3 loss: 0.806107  [  128/  306]
train() client id: f_00004-0-4 loss: 0.953980  [  160/  306]
train() client id: f_00004-0-5 loss: 0.745974  [  192/  306]
train() client id: f_00004-0-6 loss: 0.733298  [  224/  306]
train() client id: f_00004-0-7 loss: 0.749602  [  256/  306]
train() client id: f_00004-0-8 loss: 0.836483  [  288/  306]
train() client id: f_00004-1-0 loss: 0.832924  [   32/  306]
train() client id: f_00004-1-1 loss: 0.780572  [   64/  306]
train() client id: f_00004-1-2 loss: 0.796115  [   96/  306]
train() client id: f_00004-1-3 loss: 0.786775  [  128/  306]
train() client id: f_00004-1-4 loss: 0.670605  [  160/  306]
train() client id: f_00004-1-5 loss: 0.789007  [  192/  306]
train() client id: f_00004-1-6 loss: 0.702742  [  224/  306]
train() client id: f_00004-1-7 loss: 0.850407  [  256/  306]
train() client id: f_00004-1-8 loss: 0.828668  [  288/  306]
train() client id: f_00004-2-0 loss: 0.802531  [   32/  306]
train() client id: f_00004-2-1 loss: 0.914101  [   64/  306]
train() client id: f_00004-2-2 loss: 0.843457  [   96/  306]
train() client id: f_00004-2-3 loss: 0.730927  [  128/  306]
train() client id: f_00004-2-4 loss: 0.765675  [  160/  306]
train() client id: f_00004-2-5 loss: 0.838872  [  192/  306]
train() client id: f_00004-2-6 loss: 0.688326  [  224/  306]
train() client id: f_00004-2-7 loss: 0.676268  [  256/  306]
train() client id: f_00004-2-8 loss: 0.822587  [  288/  306]
train() client id: f_00004-3-0 loss: 0.710725  [   32/  306]
train() client id: f_00004-3-1 loss: 0.919089  [   64/  306]
train() client id: f_00004-3-2 loss: 0.776372  [   96/  306]
train() client id: f_00004-3-3 loss: 0.832943  [  128/  306]
train() client id: f_00004-3-4 loss: 0.753577  [  160/  306]
train() client id: f_00004-3-5 loss: 0.717862  [  192/  306]
train() client id: f_00004-3-6 loss: 0.880517  [  224/  306]
train() client id: f_00004-3-7 loss: 0.707681  [  256/  306]
train() client id: f_00004-3-8 loss: 0.708067  [  288/  306]
train() client id: f_00004-4-0 loss: 0.879045  [   32/  306]
train() client id: f_00004-4-1 loss: 0.641546  [   64/  306]
train() client id: f_00004-4-2 loss: 0.768405  [   96/  306]
train() client id: f_00004-4-3 loss: 0.724805  [  128/  306]
train() client id: f_00004-4-4 loss: 0.677723  [  160/  306]
train() client id: f_00004-4-5 loss: 0.916825  [  192/  306]
train() client id: f_00004-4-6 loss: 0.862434  [  224/  306]
train() client id: f_00004-4-7 loss: 0.695653  [  256/  306]
train() client id: f_00004-4-8 loss: 0.826933  [  288/  306]
train() client id: f_00004-5-0 loss: 0.739113  [   32/  306]
train() client id: f_00004-5-1 loss: 0.774833  [   64/  306]
train() client id: f_00004-5-2 loss: 0.895134  [   96/  306]
train() client id: f_00004-5-3 loss: 0.707772  [  128/  306]
train() client id: f_00004-5-4 loss: 0.688974  [  160/  306]
train() client id: f_00004-5-5 loss: 0.857160  [  192/  306]
train() client id: f_00004-5-6 loss: 0.767929  [  224/  306]
train() client id: f_00004-5-7 loss: 0.774562  [  256/  306]
train() client id: f_00004-5-8 loss: 0.747249  [  288/  306]
train() client id: f_00004-6-0 loss: 0.869986  [   32/  306]
train() client id: f_00004-6-1 loss: 0.701676  [   64/  306]
train() client id: f_00004-6-2 loss: 0.880576  [   96/  306]
train() client id: f_00004-6-3 loss: 0.652702  [  128/  306]
train() client id: f_00004-6-4 loss: 0.889209  [  160/  306]
train() client id: f_00004-6-5 loss: 0.822481  [  192/  306]
train() client id: f_00004-6-6 loss: 0.766585  [  224/  306]
train() client id: f_00004-6-7 loss: 0.787185  [  256/  306]
train() client id: f_00004-6-8 loss: 0.720842  [  288/  306]
train() client id: f_00004-7-0 loss: 0.728337  [   32/  306]
train() client id: f_00004-7-1 loss: 0.752636  [   64/  306]
train() client id: f_00004-7-2 loss: 0.692819  [   96/  306]
train() client id: f_00004-7-3 loss: 0.712414  [  128/  306]
train() client id: f_00004-7-4 loss: 0.862194  [  160/  306]
train() client id: f_00004-7-5 loss: 0.782811  [  192/  306]
train() client id: f_00004-7-6 loss: 0.779750  [  224/  306]
train() client id: f_00004-7-7 loss: 0.863609  [  256/  306]
train() client id: f_00004-7-8 loss: 0.849548  [  288/  306]
train() client id: f_00004-8-0 loss: 0.805484  [   32/  306]
train() client id: f_00004-8-1 loss: 0.724331  [   64/  306]
train() client id: f_00004-8-2 loss: 0.784804  [   96/  306]
train() client id: f_00004-8-3 loss: 0.763290  [  128/  306]
train() client id: f_00004-8-4 loss: 0.605037  [  160/  306]
train() client id: f_00004-8-5 loss: 0.836942  [  192/  306]
train() client id: f_00004-8-6 loss: 0.842375  [  224/  306]
train() client id: f_00004-8-7 loss: 0.819697  [  256/  306]
train() client id: f_00004-8-8 loss: 0.876689  [  288/  306]
train() client id: f_00004-9-0 loss: 0.687003  [   32/  306]
train() client id: f_00004-9-1 loss: 0.863302  [   64/  306]
train() client id: f_00004-9-2 loss: 0.823522  [   96/  306]
train() client id: f_00004-9-3 loss: 0.702736  [  128/  306]
train() client id: f_00004-9-4 loss: 0.647337  [  160/  306]
train() client id: f_00004-9-5 loss: 0.788072  [  192/  306]
train() client id: f_00004-9-6 loss: 0.830858  [  224/  306]
train() client id: f_00004-9-7 loss: 0.797405  [  256/  306]
train() client id: f_00004-9-8 loss: 0.835336  [  288/  306]
train() client id: f_00005-0-0 loss: 0.497553  [   32/  146]
train() client id: f_00005-0-1 loss: 0.263808  [   64/  146]
train() client id: f_00005-0-2 loss: 0.401522  [   96/  146]
train() client id: f_00005-0-3 loss: 0.252525  [  128/  146]
train() client id: f_00005-1-0 loss: 0.601506  [   32/  146]
train() client id: f_00005-1-1 loss: 0.353595  [   64/  146]
train() client id: f_00005-1-2 loss: 0.229321  [   96/  146]
train() client id: f_00005-1-3 loss: 0.183816  [  128/  146]
train() client id: f_00005-2-0 loss: 0.122386  [   32/  146]
train() client id: f_00005-2-1 loss: 0.343452  [   64/  146]
train() client id: f_00005-2-2 loss: 0.334549  [   96/  146]
train() client id: f_00005-2-3 loss: 0.534333  [  128/  146]
train() client id: f_00005-3-0 loss: 0.235563  [   32/  146]
train() client id: f_00005-3-1 loss: 0.333882  [   64/  146]
train() client id: f_00005-3-2 loss: 0.280397  [   96/  146]
train() client id: f_00005-3-3 loss: 0.366461  [  128/  146]
train() client id: f_00005-4-0 loss: 0.303202  [   32/  146]
train() client id: f_00005-4-1 loss: 0.320709  [   64/  146]
train() client id: f_00005-4-2 loss: 0.145418  [   96/  146]
train() client id: f_00005-4-3 loss: 0.528702  [  128/  146]
train() client id: f_00005-5-0 loss: 0.397033  [   32/  146]
train() client id: f_00005-5-1 loss: 0.355700  [   64/  146]
train() client id: f_00005-5-2 loss: 0.170091  [   96/  146]
train() client id: f_00005-5-3 loss: 0.280828  [  128/  146]
train() client id: f_00005-6-0 loss: 0.254782  [   32/  146]
train() client id: f_00005-6-1 loss: 0.479076  [   64/  146]
train() client id: f_00005-6-2 loss: 0.077777  [   96/  146]
train() client id: f_00005-6-3 loss: 0.480158  [  128/  146]
train() client id: f_00005-7-0 loss: 0.364979  [   32/  146]
train() client id: f_00005-7-1 loss: 0.248736  [   64/  146]
train() client id: f_00005-7-2 loss: 0.285605  [   96/  146]
train() client id: f_00005-7-3 loss: 0.412221  [  128/  146]
train() client id: f_00005-8-0 loss: 0.233227  [   32/  146]
train() client id: f_00005-8-1 loss: 0.281899  [   64/  146]
train() client id: f_00005-8-2 loss: 0.178501  [   96/  146]
train() client id: f_00005-8-3 loss: 0.525683  [  128/  146]
train() client id: f_00005-9-0 loss: 0.257839  [   32/  146]
train() client id: f_00005-9-1 loss: 0.203248  [   64/  146]
train() client id: f_00005-9-2 loss: -0.067075  [   96/  146]
train() client id: f_00005-9-3 loss: 0.589059  [  128/  146]
train() client id: f_00006-0-0 loss: 0.486146  [   32/   54]
train() client id: f_00006-1-0 loss: 0.426355  [   32/   54]
train() client id: f_00006-2-0 loss: 0.400976  [   32/   54]
train() client id: f_00006-3-0 loss: 0.486169  [   32/   54]
train() client id: f_00006-4-0 loss: 0.403998  [   32/   54]
train() client id: f_00006-5-0 loss: 0.386036  [   32/   54]
train() client id: f_00006-6-0 loss: 0.400684  [   32/   54]
train() client id: f_00006-7-0 loss: 0.446367  [   32/   54]
train() client id: f_00006-8-0 loss: 0.402517  [   32/   54]
train() client id: f_00006-9-0 loss: 0.500033  [   32/   54]
train() client id: f_00007-0-0 loss: 0.597419  [   32/  179]
train() client id: f_00007-0-1 loss: 0.525772  [   64/  179]
train() client id: f_00007-0-2 loss: 0.707736  [   96/  179]
train() client id: f_00007-0-3 loss: 0.734993  [  128/  179]
train() client id: f_00007-0-4 loss: 0.892328  [  160/  179]
train() client id: f_00007-1-0 loss: 0.647875  [   32/  179]
train() client id: f_00007-1-1 loss: 0.617819  [   64/  179]
train() client id: f_00007-1-2 loss: 0.534753  [   96/  179]
train() client id: f_00007-1-3 loss: 0.615909  [  128/  179]
train() client id: f_00007-1-4 loss: 0.706467  [  160/  179]
train() client id: f_00007-2-0 loss: 0.870402  [   32/  179]
train() client id: f_00007-2-1 loss: 0.608902  [   64/  179]
train() client id: f_00007-2-2 loss: 0.541091  [   96/  179]
train() client id: f_00007-2-3 loss: 0.512070  [  128/  179]
train() client id: f_00007-2-4 loss: 0.716823  [  160/  179]
train() client id: f_00007-3-0 loss: 0.810559  [   32/  179]
train() client id: f_00007-3-1 loss: 0.570910  [   64/  179]
train() client id: f_00007-3-2 loss: 0.616612  [   96/  179]
train() client id: f_00007-3-3 loss: 0.492851  [  128/  179]
train() client id: f_00007-3-4 loss: 0.653457  [  160/  179]
train() client id: f_00007-4-0 loss: 0.720501  [   32/  179]
train() client id: f_00007-4-1 loss: 0.895792  [   64/  179]
train() client id: f_00007-4-2 loss: 0.502027  [   96/  179]
train() client id: f_00007-4-3 loss: 0.467545  [  128/  179]
train() client id: f_00007-4-4 loss: 0.585188  [  160/  179]
train() client id: f_00007-5-0 loss: 0.556475  [   32/  179]
train() client id: f_00007-5-1 loss: 0.566999  [   64/  179]
train() client id: f_00007-5-2 loss: 0.694759  [   96/  179]
train() client id: f_00007-5-3 loss: 0.766593  [  128/  179]
train() client id: f_00007-5-4 loss: 0.657723  [  160/  179]
train() client id: f_00007-6-0 loss: 0.548572  [   32/  179]
train() client id: f_00007-6-1 loss: 0.832877  [   64/  179]
train() client id: f_00007-6-2 loss: 0.579281  [   96/  179]
train() client id: f_00007-6-3 loss: 0.650572  [  128/  179]
train() client id: f_00007-6-4 loss: 0.489250  [  160/  179]
train() client id: f_00007-7-0 loss: 0.874542  [   32/  179]
train() client id: f_00007-7-1 loss: 0.553988  [   64/  179]
train() client id: f_00007-7-2 loss: 0.662677  [   96/  179]
train() client id: f_00007-7-3 loss: 0.505287  [  128/  179]
train() client id: f_00007-7-4 loss: 0.547046  [  160/  179]
train() client id: f_00007-8-0 loss: 0.720134  [   32/  179]
train() client id: f_00007-8-1 loss: 0.546422  [   64/  179]
train() client id: f_00007-8-2 loss: 0.450134  [   96/  179]
train() client id: f_00007-8-3 loss: 0.577588  [  128/  179]
train() client id: f_00007-8-4 loss: 0.887889  [  160/  179]
train() client id: f_00007-9-0 loss: 0.651832  [   32/  179]
train() client id: f_00007-9-1 loss: 0.559650  [   64/  179]
train() client id: f_00007-9-2 loss: 0.688653  [   96/  179]
train() client id: f_00007-9-3 loss: 0.674652  [  128/  179]
train() client id: f_00007-9-4 loss: 0.526781  [  160/  179]
train() client id: f_00008-0-0 loss: 0.665679  [   32/  130]
train() client id: f_00008-0-1 loss: 0.704816  [   64/  130]
train() client id: f_00008-0-2 loss: 0.712796  [   96/  130]
train() client id: f_00008-0-3 loss: 0.764304  [  128/  130]
train() client id: f_00008-1-0 loss: 0.650269  [   32/  130]
train() client id: f_00008-1-1 loss: 0.797329  [   64/  130]
train() client id: f_00008-1-2 loss: 0.658425  [   96/  130]
train() client id: f_00008-1-3 loss: 0.709910  [  128/  130]
train() client id: f_00008-2-0 loss: 0.799294  [   32/  130]
train() client id: f_00008-2-1 loss: 0.665505  [   64/  130]
train() client id: f_00008-2-2 loss: 0.755642  [   96/  130]
train() client id: f_00008-2-3 loss: 0.617574  [  128/  130]
train() client id: f_00008-3-0 loss: 0.579568  [   32/  130]
train() client id: f_00008-3-1 loss: 0.912928  [   64/  130]
train() client id: f_00008-3-2 loss: 0.583178  [   96/  130]
train() client id: f_00008-3-3 loss: 0.768747  [  128/  130]
train() client id: f_00008-4-0 loss: 0.628653  [   32/  130]
train() client id: f_00008-4-1 loss: 0.780479  [   64/  130]
train() client id: f_00008-4-2 loss: 0.609229  [   96/  130]
train() client id: f_00008-4-3 loss: 0.820588  [  128/  130]
train() client id: f_00008-5-0 loss: 0.797881  [   32/  130]
train() client id: f_00008-5-1 loss: 0.703959  [   64/  130]
train() client id: f_00008-5-2 loss: 0.695406  [   96/  130]
train() client id: f_00008-5-3 loss: 0.633743  [  128/  130]
train() client id: f_00008-6-0 loss: 0.670065  [   32/  130]
train() client id: f_00008-6-1 loss: 0.730747  [   64/  130]
train() client id: f_00008-6-2 loss: 0.715773  [   96/  130]
train() client id: f_00008-6-3 loss: 0.689853  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708833  [   32/  130]
train() client id: f_00008-7-1 loss: 0.604195  [   64/  130]
train() client id: f_00008-7-2 loss: 0.828465  [   96/  130]
train() client id: f_00008-7-3 loss: 0.657473  [  128/  130]
train() client id: f_00008-8-0 loss: 0.700320  [   32/  130]
train() client id: f_00008-8-1 loss: 0.731975  [   64/  130]
train() client id: f_00008-8-2 loss: 0.645697  [   96/  130]
train() client id: f_00008-8-3 loss: 0.749442  [  128/  130]
train() client id: f_00008-9-0 loss: 0.738318  [   32/  130]
train() client id: f_00008-9-1 loss: 0.658592  [   64/  130]
train() client id: f_00008-9-2 loss: 0.620459  [   96/  130]
train() client id: f_00008-9-3 loss: 0.762559  [  128/  130]
train() client id: f_00009-0-0 loss: 0.845055  [   32/  118]
train() client id: f_00009-0-1 loss: 0.926352  [   64/  118]
train() client id: f_00009-0-2 loss: 0.710651  [   96/  118]
train() client id: f_00009-1-0 loss: 0.978577  [   32/  118]
train() client id: f_00009-1-1 loss: 0.777628  [   64/  118]
train() client id: f_00009-1-2 loss: 0.638588  [   96/  118]
train() client id: f_00009-2-0 loss: 0.678729  [   32/  118]
train() client id: f_00009-2-1 loss: 0.802475  [   64/  118]
train() client id: f_00009-2-2 loss: 0.868917  [   96/  118]
train() client id: f_00009-3-0 loss: 0.822012  [   32/  118]
train() client id: f_00009-3-1 loss: 0.611752  [   64/  118]
train() client id: f_00009-3-2 loss: 0.949751  [   96/  118]
train() client id: f_00009-4-0 loss: 0.669340  [   32/  118]
train() client id: f_00009-4-1 loss: 0.692593  [   64/  118]
train() client id: f_00009-4-2 loss: 0.812935  [   96/  118]
train() client id: f_00009-5-0 loss: 0.789046  [   32/  118]
train() client id: f_00009-5-1 loss: 0.637297  [   64/  118]
train() client id: f_00009-5-2 loss: 0.816715  [   96/  118]
train() client id: f_00009-6-0 loss: 0.810828  [   32/  118]
train() client id: f_00009-6-1 loss: 0.733058  [   64/  118]
train() client id: f_00009-6-2 loss: 0.726058  [   96/  118]
train() client id: f_00009-7-0 loss: 0.719470  [   32/  118]
train() client id: f_00009-7-1 loss: 0.782493  [   64/  118]
train() client id: f_00009-7-2 loss: 0.612984  [   96/  118]
train() client id: f_00009-8-0 loss: 0.707321  [   32/  118]
train() client id: f_00009-8-1 loss: 0.891028  [   64/  118]
train() client id: f_00009-8-2 loss: 0.706400  [   96/  118]
train() client id: f_00009-9-0 loss: 0.691183  [   32/  118]
train() client id: f_00009-9-1 loss: 0.805223  [   64/  118]
train() client id: f_00009-9-2 loss: 0.644961  [   96/  118]
At round 61 accuracy: 0.6472148541114059
At round 61 training accuracy: 0.5888665325285044
At round 61 training loss: 0.827669865661837
update_location
xs = [  -3.9056584     4.20031788  325.00902392   18.81129433    0.97929623
    3.95640986 -287.44319194 -266.32485185  309.66397685 -252.06087855]
ys = [ 317.5879595   300.55583871    1.32061395 -287.45517586  279.35018685
  262.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [332.98253135 316.78297753 340.04795198 304.93334177 296.71111525
 281.22398726 304.3525573  284.48128755 325.88410291 271.20231996]
dists_bs = [222.7209014  218.80818669 529.31422911 501.42557522 204.59148124
 199.34071762 210.22394361 196.66744272 509.59168784 187.56031626]
uav_gains = [1.52568368e-12 1.97196846e-12 1.37954414e-12 2.43484695e-12
 2.84867986e-12 3.89882946e-12 2.46132727e-12 3.64447161e-12
 1.69955459e-12 4.80856961e-12]
bs_gains = [2.94824353e-11 3.09824743e-11 2.61153645e-12 3.03889670e-12
 3.73946054e-11 4.02184414e-11 3.46564429e-11 4.17679507e-11
 2.90450111e-12 4.76979163e-11]
Round 62
-------------------------------
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.82115899 5.66246847 2.78706489 1.03447434 6.52759612 3.14171865
 1.26660636 3.89259813 2.86603349 2.54747095]
obj_prev = 32.54719040436046
eta_min = 1.2218214947720352e-33	eta_max = 0.9394406220974982
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 7.459395796309552	eta = 0.9090909090909091
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 18.0305957827678	eta = 0.37609788314465736
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 12.220288063290287	eta = 0.5549189078534793
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.203657475325056	eta = 0.605272779953424
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140690513111194	eta = 0.6086937697223753
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140415786273087	eta = 0.6087087803393881
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.14041578100363	eta = 0.6087087806273096
eta = 0.6087087806273096
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.040963   0.08615235 0.0403128  0.01397944 0.09948158 0.04746507
 0.01755557 0.05819347 0.04226342 0.03836218]
ene_total = [1.12030085 1.67073395 1.1295553  0.56372656 1.90151444 0.97618988
 0.62570552 1.29997444 1.04280632 0.80990852]
ti_comp = [1.2940368  1.43579767 1.28232047 1.33779388 1.43906047 1.44025443
 1.33863419 1.36510734 1.35070396 1.44291406]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [2.56544140e-06 1.93862966e-05 2.49009332e-06 9.54048993e-08
 2.97132334e-05 3.22198749e-06 1.88712718e-07 6.60950921e-06
 2.58614832e-06 1.69476237e-06]
ene_total = [0.39332206 0.14201894 0.41411524 0.31561678 0.13641131 0.13382205
 0.31412702 0.26725561 0.29274776 0.12907455]
optimize_network iter = 0 obj = 2.5385113123879783
eta = 0.6087087806273096
freqs = [15827603.24540961 30001562.15823117 15718689.94891933  5224809.85098799
 34564766.11963843 16478014.31267369  6557270.26566169 21314613.85120587
 15644958.54527248 13293299.23888385]
eta_min = 0.6087087806273109	eta_max = 0.7286862650795968
af = 0.0009590243179496368	bf = 0.9928211124535382	zeta = 0.0010549267497446007	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.92556679e-07 3.72210798e-06 4.78090085e-07 1.83174406e-08
 5.70484736e-06 6.18611464e-07 3.62322484e-08 1.26900498e-06
 4.96532343e-07 3.25389044e-07]
ene_total = [1.6927798  0.61008177 1.78228252 1.35847294 0.58530795 0.57579846
 1.35205494 1.14991453 1.25988629 0.5554585 ]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 1 obj = 3.660319648720293
eta = 0.7286862650795968
freqs = [15746566.89412167 28283149.87566576 15718689.94891934  5104500.91249742
 32549667.43739824 15511225.99197442  6404139.63058966 20604377.17176979
 15207342.34336579 12502384.68143362]
eta_min = 0.7286862650796062	eta_max = 0.7286862650795745
af = 0.0008653145558439159	bf = 0.9928211124535382	zeta = 0.0009518460114283075	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.87525871e-07 3.30793358e-06 4.78090085e-07 1.74835808e-08
 5.05906076e-06 5.48151319e-07 3.45597587e-08 1.18584348e-06
 4.69143123e-07 2.87821399e-07]
ene_total = [1.69277942 0.61005013 1.78228252 1.35847287 0.58525862 0.57579307
 1.35205481 1.14990818 1.25988419 0.55545563]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 2 obj = 3.6603196487199923
eta = 0.7286862650795745
freqs = [15746566.89412155 28283149.87566597 15718689.9489192   5104500.91249741
 32549667.4373985  15511225.99197455  6404139.63058965 20604377.17176981
 15207342.34336578 12502384.68143372]
Done!
At round 62 energy consumption: 3.660319648720293
At round 62 eta: 0.7286862650795745
At round 62 local rounds: 10.364211052331338
At round 62 global rounds: 25.59678869996595
At round 62 a_n: 6.602214497187386
gradient difference: 0.4810906946659088
train() client id: f_00000-0-0 loss: 1.057873  [   32/  126]
train() client id: f_00000-0-1 loss: 1.381554  [   64/  126]
train() client id: f_00000-0-2 loss: 1.119374  [   96/  126]
train() client id: f_00000-1-0 loss: 0.836213  [   32/  126]
train() client id: f_00000-1-1 loss: 1.261088  [   64/  126]
train() client id: f_00000-1-2 loss: 0.935154  [   96/  126]
train() client id: f_00000-2-0 loss: 1.132201  [   32/  126]
train() client id: f_00000-2-1 loss: 0.824181  [   64/  126]
train() client id: f_00000-2-2 loss: 1.050559  [   96/  126]
train() client id: f_00000-3-0 loss: 0.980327  [   32/  126]
train() client id: f_00000-3-1 loss: 0.834392  [   64/  126]
train() client id: f_00000-3-2 loss: 0.960564  [   96/  126]
train() client id: f_00000-4-0 loss: 0.987333  [   32/  126]
train() client id: f_00000-4-1 loss: 1.035406  [   64/  126]
train() client id: f_00000-4-2 loss: 0.730760  [   96/  126]
train() client id: f_00000-5-0 loss: 0.824975  [   32/  126]
train() client id: f_00000-5-1 loss: 0.868639  [   64/  126]
train() client id: f_00000-5-2 loss: 0.982439  [   96/  126]
train() client id: f_00000-6-0 loss: 0.852696  [   32/  126]
train() client id: f_00000-6-1 loss: 0.934575  [   64/  126]
train() client id: f_00000-6-2 loss: 0.824994  [   96/  126]
train() client id: f_00000-7-0 loss: 0.962371  [   32/  126]
train() client id: f_00000-7-1 loss: 0.905261  [   64/  126]
train() client id: f_00000-7-2 loss: 0.816482  [   96/  126]
train() client id: f_00000-8-0 loss: 0.961056  [   32/  126]
train() client id: f_00000-8-1 loss: 0.732419  [   64/  126]
train() client id: f_00000-8-2 loss: 0.853206  [   96/  126]
train() client id: f_00000-9-0 loss: 0.899509  [   32/  126]
train() client id: f_00000-9-1 loss: 0.779589  [   64/  126]
train() client id: f_00000-9-2 loss: 0.830583  [   96/  126]
train() client id: f_00001-0-0 loss: 0.472114  [   32/  265]
train() client id: f_00001-0-1 loss: 0.550129  [   64/  265]
train() client id: f_00001-0-2 loss: 0.470627  [   96/  265]
train() client id: f_00001-0-3 loss: 0.499936  [  128/  265]
train() client id: f_00001-0-4 loss: 0.484936  [  160/  265]
train() client id: f_00001-0-5 loss: 0.686880  [  192/  265]
train() client id: f_00001-0-6 loss: 0.501783  [  224/  265]
train() client id: f_00001-0-7 loss: 0.565790  [  256/  265]
train() client id: f_00001-1-0 loss: 0.596405  [   32/  265]
train() client id: f_00001-1-1 loss: 0.475835  [   64/  265]
train() client id: f_00001-1-2 loss: 0.657719  [   96/  265]
train() client id: f_00001-1-3 loss: 0.580247  [  128/  265]
train() client id: f_00001-1-4 loss: 0.497350  [  160/  265]
train() client id: f_00001-1-5 loss: 0.494820  [  192/  265]
train() client id: f_00001-1-6 loss: 0.457619  [  224/  265]
train() client id: f_00001-1-7 loss: 0.415942  [  256/  265]
train() client id: f_00001-2-0 loss: 0.499580  [   32/  265]
train() client id: f_00001-2-1 loss: 0.429920  [   64/  265]
train() client id: f_00001-2-2 loss: 0.474545  [   96/  265]
train() client id: f_00001-2-3 loss: 0.714554  [  128/  265]
train() client id: f_00001-2-4 loss: 0.439971  [  160/  265]
train() client id: f_00001-2-5 loss: 0.467576  [  192/  265]
train() client id: f_00001-2-6 loss: 0.612892  [  224/  265]
train() client id: f_00001-2-7 loss: 0.503126  [  256/  265]
train() client id: f_00001-3-0 loss: 0.525340  [   32/  265]
train() client id: f_00001-3-1 loss: 0.502553  [   64/  265]
train() client id: f_00001-3-2 loss: 0.534272  [   96/  265]
train() client id: f_00001-3-3 loss: 0.556566  [  128/  265]
train() client id: f_00001-3-4 loss: 0.560737  [  160/  265]
train() client id: f_00001-3-5 loss: 0.498146  [  192/  265]
train() client id: f_00001-3-6 loss: 0.464220  [  224/  265]
train() client id: f_00001-3-7 loss: 0.456526  [  256/  265]
train() client id: f_00001-4-0 loss: 0.426443  [   32/  265]
train() client id: f_00001-4-1 loss: 0.520964  [   64/  265]
train() client id: f_00001-4-2 loss: 0.517854  [   96/  265]
train() client id: f_00001-4-3 loss: 0.505439  [  128/  265]
train() client id: f_00001-4-4 loss: 0.559826  [  160/  265]
train() client id: f_00001-4-5 loss: 0.433916  [  192/  265]
train() client id: f_00001-4-6 loss: 0.506034  [  224/  265]
train() client id: f_00001-4-7 loss: 0.486281  [  256/  265]
train() client id: f_00001-5-0 loss: 0.616883  [   32/  265]
train() client id: f_00001-5-1 loss: 0.538911  [   64/  265]
train() client id: f_00001-5-2 loss: 0.536784  [   96/  265]
train() client id: f_00001-5-3 loss: 0.427165  [  128/  265]
train() client id: f_00001-5-4 loss: 0.639673  [  160/  265]
train() client id: f_00001-5-5 loss: 0.423611  [  192/  265]
train() client id: f_00001-5-6 loss: 0.445285  [  224/  265]
train() client id: f_00001-5-7 loss: 0.434810  [  256/  265]
train() client id: f_00001-6-0 loss: 0.421175  [   32/  265]
train() client id: f_00001-6-1 loss: 0.453669  [   64/  265]
train() client id: f_00001-6-2 loss: 0.467168  [   96/  265]
train() client id: f_00001-6-3 loss: 0.470758  [  128/  265]
train() client id: f_00001-6-4 loss: 0.722182  [  160/  265]
train() client id: f_00001-6-5 loss: 0.410687  [  192/  265]
train() client id: f_00001-6-6 loss: 0.479944  [  224/  265]
train() client id: f_00001-6-7 loss: 0.651454  [  256/  265]
train() client id: f_00001-7-0 loss: 0.480816  [   32/  265]
train() client id: f_00001-7-1 loss: 0.482848  [   64/  265]
train() client id: f_00001-7-2 loss: 0.630534  [   96/  265]
train() client id: f_00001-7-3 loss: 0.433286  [  128/  265]
train() client id: f_00001-7-4 loss: 0.427497  [  160/  265]
train() client id: f_00001-7-5 loss: 0.475864  [  192/  265]
train() client id: f_00001-7-6 loss: 0.504251  [  224/  265]
train() client id: f_00001-7-7 loss: 0.608151  [  256/  265]
train() client id: f_00001-8-0 loss: 0.639074  [   32/  265]
train() client id: f_00001-8-1 loss: 0.688151  [   64/  265]
train() client id: f_00001-8-2 loss: 0.379425  [   96/  265]
train() client id: f_00001-8-3 loss: 0.542637  [  128/  265]
train() client id: f_00001-8-4 loss: 0.469876  [  160/  265]
train() client id: f_00001-8-5 loss: 0.416890  [  192/  265]
train() client id: f_00001-8-6 loss: 0.411412  [  224/  265]
train() client id: f_00001-8-7 loss: 0.433705  [  256/  265]
train() client id: f_00001-9-0 loss: 0.462941  [   32/  265]
train() client id: f_00001-9-1 loss: 0.538637  [   64/  265]
train() client id: f_00001-9-2 loss: 0.442469  [   96/  265]
train() client id: f_00001-9-3 loss: 0.460641  [  128/  265]
train() client id: f_00001-9-4 loss: 0.542040  [  160/  265]
train() client id: f_00001-9-5 loss: 0.575277  [  192/  265]
train() client id: f_00001-9-6 loss: 0.473820  [  224/  265]
train() client id: f_00001-9-7 loss: 0.547072  [  256/  265]
train() client id: f_00002-0-0 loss: 1.235855  [   32/  124]
train() client id: f_00002-0-1 loss: 0.957333  [   64/  124]
train() client id: f_00002-0-2 loss: 1.188033  [   96/  124]
train() client id: f_00002-1-0 loss: 1.076223  [   32/  124]
train() client id: f_00002-1-1 loss: 1.028203  [   64/  124]
train() client id: f_00002-1-2 loss: 1.077398  [   96/  124]
train() client id: f_00002-2-0 loss: 0.974955  [   32/  124]
train() client id: f_00002-2-1 loss: 1.063662  [   64/  124]
train() client id: f_00002-2-2 loss: 1.210909  [   96/  124]
train() client id: f_00002-3-0 loss: 1.234192  [   32/  124]
train() client id: f_00002-3-1 loss: 1.034546  [   64/  124]
train() client id: f_00002-3-2 loss: 0.991545  [   96/  124]
train() client id: f_00002-4-0 loss: 1.234211  [   32/  124]
train() client id: f_00002-4-1 loss: 0.934565  [   64/  124]
train() client id: f_00002-4-2 loss: 0.914385  [   96/  124]
train() client id: f_00002-5-0 loss: 0.811454  [   32/  124]
train() client id: f_00002-5-1 loss: 1.274246  [   64/  124]
train() client id: f_00002-5-2 loss: 1.089832  [   96/  124]
train() client id: f_00002-6-0 loss: 0.880940  [   32/  124]
train() client id: f_00002-6-1 loss: 1.108443  [   64/  124]
train() client id: f_00002-6-2 loss: 1.093781  [   96/  124]
train() client id: f_00002-7-0 loss: 0.813169  [   32/  124]
train() client id: f_00002-7-1 loss: 0.953897  [   64/  124]
train() client id: f_00002-7-2 loss: 1.038826  [   96/  124]
train() client id: f_00002-8-0 loss: 0.969421  [   32/  124]
train() client id: f_00002-8-1 loss: 0.994529  [   64/  124]
train() client id: f_00002-8-2 loss: 0.894386  [   96/  124]
train() client id: f_00002-9-0 loss: 0.846184  [   32/  124]
train() client id: f_00002-9-1 loss: 1.146573  [   64/  124]
train() client id: f_00002-9-2 loss: 0.947824  [   96/  124]
train() client id: f_00003-0-0 loss: 0.598779  [   32/   43]
train() client id: f_00003-1-0 loss: 0.564336  [   32/   43]
train() client id: f_00003-2-0 loss: 0.562303  [   32/   43]
train() client id: f_00003-3-0 loss: 0.673420  [   32/   43]
train() client id: f_00003-4-0 loss: 0.391666  [   32/   43]
train() client id: f_00003-5-0 loss: 0.562549  [   32/   43]
train() client id: f_00003-6-0 loss: 0.659584  [   32/   43]
train() client id: f_00003-7-0 loss: 0.736312  [   32/   43]
train() client id: f_00003-8-0 loss: 0.612227  [   32/   43]
train() client id: f_00003-9-0 loss: 0.551316  [   32/   43]
train() client id: f_00004-0-0 loss: 0.747787  [   32/  306]
train() client id: f_00004-0-1 loss: 0.731722  [   64/  306]
train() client id: f_00004-0-2 loss: 0.645344  [   96/  306]
train() client id: f_00004-0-3 loss: 0.686788  [  128/  306]
train() client id: f_00004-0-4 loss: 0.736401  [  160/  306]
train() client id: f_00004-0-5 loss: 0.793396  [  192/  306]
train() client id: f_00004-0-6 loss: 0.632952  [  224/  306]
train() client id: f_00004-0-7 loss: 0.720076  [  256/  306]
train() client id: f_00004-0-8 loss: 0.689901  [  288/  306]
train() client id: f_00004-1-0 loss: 0.604932  [   32/  306]
train() client id: f_00004-1-1 loss: 0.905632  [   64/  306]
train() client id: f_00004-1-2 loss: 0.675107  [   96/  306]
train() client id: f_00004-1-3 loss: 0.637222  [  128/  306]
train() client id: f_00004-1-4 loss: 0.817929  [  160/  306]
train() client id: f_00004-1-5 loss: 0.690620  [  192/  306]
train() client id: f_00004-1-6 loss: 0.817935  [  224/  306]
train() client id: f_00004-1-7 loss: 0.755313  [  256/  306]
train() client id: f_00004-1-8 loss: 0.560110  [  288/  306]
train() client id: f_00004-2-0 loss: 0.574061  [   32/  306]
train() client id: f_00004-2-1 loss: 0.757404  [   64/  306]
train() client id: f_00004-2-2 loss: 0.751666  [   96/  306]
train() client id: f_00004-2-3 loss: 0.723259  [  128/  306]
train() client id: f_00004-2-4 loss: 0.568386  [  160/  306]
train() client id: f_00004-2-5 loss: 0.795746  [  192/  306]
train() client id: f_00004-2-6 loss: 0.778559  [  224/  306]
train() client id: f_00004-2-7 loss: 0.714079  [  256/  306]
train() client id: f_00004-2-8 loss: 0.788588  [  288/  306]
train() client id: f_00004-3-0 loss: 0.716254  [   32/  306]
train() client id: f_00004-3-1 loss: 0.624873  [   64/  306]
train() client id: f_00004-3-2 loss: 0.847302  [   96/  306]
train() client id: f_00004-3-3 loss: 0.660341  [  128/  306]
train() client id: f_00004-3-4 loss: 0.663393  [  160/  306]
train() client id: f_00004-3-5 loss: 0.780077  [  192/  306]
train() client id: f_00004-3-6 loss: 0.850107  [  224/  306]
train() client id: f_00004-3-7 loss: 0.793088  [  256/  306]
train() client id: f_00004-3-8 loss: 0.537512  [  288/  306]
train() client id: f_00004-4-0 loss: 0.650845  [   32/  306]
train() client id: f_00004-4-1 loss: 0.847918  [   64/  306]
train() client id: f_00004-4-2 loss: 0.899073  [   96/  306]
train() client id: f_00004-4-3 loss: 0.640211  [  128/  306]
train() client id: f_00004-4-4 loss: 0.742462  [  160/  306]
train() client id: f_00004-4-5 loss: 0.666868  [  192/  306]
train() client id: f_00004-4-6 loss: 0.689140  [  224/  306]
train() client id: f_00004-4-7 loss: 0.739296  [  256/  306]
train() client id: f_00004-4-8 loss: 0.668566  [  288/  306]
train() client id: f_00004-5-0 loss: 0.714626  [   32/  306]
train() client id: f_00004-5-1 loss: 0.761123  [   64/  306]
train() client id: f_00004-5-2 loss: 0.747331  [   96/  306]
train() client id: f_00004-5-3 loss: 0.690173  [  128/  306]
train() client id: f_00004-5-4 loss: 0.630090  [  160/  306]
train() client id: f_00004-5-5 loss: 0.721189  [  192/  306]
train() client id: f_00004-5-6 loss: 0.757872  [  224/  306]
train() client id: f_00004-5-7 loss: 0.798546  [  256/  306]
train() client id: f_00004-5-8 loss: 0.658090  [  288/  306]
train() client id: f_00004-6-0 loss: 0.825541  [   32/  306]
train() client id: f_00004-6-1 loss: 0.656227  [   64/  306]
train() client id: f_00004-6-2 loss: 0.832565  [   96/  306]
train() client id: f_00004-6-3 loss: 0.822817  [  128/  306]
train() client id: f_00004-6-4 loss: 0.661762  [  160/  306]
train() client id: f_00004-6-5 loss: 0.724760  [  192/  306]
train() client id: f_00004-6-6 loss: 0.681579  [  224/  306]
train() client id: f_00004-6-7 loss: 0.628929  [  256/  306]
train() client id: f_00004-6-8 loss: 0.804131  [  288/  306]
train() client id: f_00004-7-0 loss: 0.875417  [   32/  306]
train() client id: f_00004-7-1 loss: 0.670423  [   64/  306]
train() client id: f_00004-7-2 loss: 0.656654  [   96/  306]
train() client id: f_00004-7-3 loss: 0.867503  [  128/  306]
train() client id: f_00004-7-4 loss: 0.719512  [  160/  306]
train() client id: f_00004-7-5 loss: 0.649582  [  192/  306]
train() client id: f_00004-7-6 loss: 0.667612  [  224/  306]
train() client id: f_00004-7-7 loss: 0.698839  [  256/  306]
train() client id: f_00004-7-8 loss: 0.777310  [  288/  306]
train() client id: f_00004-8-0 loss: 0.757582  [   32/  306]
train() client id: f_00004-8-1 loss: 0.585960  [   64/  306]
train() client id: f_00004-8-2 loss: 0.641819  [   96/  306]
train() client id: f_00004-8-3 loss: 0.748256  [  128/  306]
train() client id: f_00004-8-4 loss: 0.765978  [  160/  306]
train() client id: f_00004-8-5 loss: 0.743066  [  192/  306]
train() client id: f_00004-8-6 loss: 0.766986  [  224/  306]
train() client id: f_00004-8-7 loss: 0.943141  [  256/  306]
train() client id: f_00004-8-8 loss: 0.733738  [  288/  306]
train() client id: f_00004-9-0 loss: 0.669517  [   32/  306]
train() client id: f_00004-9-1 loss: 0.838278  [   64/  306]
train() client id: f_00004-9-2 loss: 0.819876  [   96/  306]
train() client id: f_00004-9-3 loss: 0.824122  [  128/  306]
train() client id: f_00004-9-4 loss: 0.650747  [  160/  306]
train() client id: f_00004-9-5 loss: 0.711890  [  192/  306]
train() client id: f_00004-9-6 loss: 0.700254  [  224/  306]
train() client id: f_00004-9-7 loss: 0.768342  [  256/  306]
train() client id: f_00004-9-8 loss: 0.704095  [  288/  306]
train() client id: f_00005-0-0 loss: 0.392788  [   32/  146]
train() client id: f_00005-0-1 loss: 0.658841  [   64/  146]
train() client id: f_00005-0-2 loss: 0.113152  [   96/  146]
train() client id: f_00005-0-3 loss: 0.194069  [  128/  146]
train() client id: f_00005-1-0 loss: 0.188258  [   32/  146]
train() client id: f_00005-1-1 loss: 0.541056  [   64/  146]
train() client id: f_00005-1-2 loss: 0.290231  [   96/  146]
train() client id: f_00005-1-3 loss: 0.214331  [  128/  146]
train() client id: f_00005-2-0 loss: 0.529736  [   32/  146]
train() client id: f_00005-2-1 loss: 0.445332  [   64/  146]
train() client id: f_00005-2-2 loss: 0.248451  [   96/  146]
train() client id: f_00005-2-3 loss: 0.181456  [  128/  146]
train() client id: f_00005-3-0 loss: 0.528753  [   32/  146]
train() client id: f_00005-3-1 loss: 0.374839  [   64/  146]
train() client id: f_00005-3-2 loss: 0.469766  [   96/  146]
train() client id: f_00005-3-3 loss: 0.312027  [  128/  146]
train() client id: f_00005-4-0 loss: 0.401467  [   32/  146]
train() client id: f_00005-4-1 loss: 0.454949  [   64/  146]
train() client id: f_00005-4-2 loss: 0.426733  [   96/  146]
train() client id: f_00005-4-3 loss: 0.225063  [  128/  146]
train() client id: f_00005-5-0 loss: 0.221298  [   32/  146]
train() client id: f_00005-5-1 loss: 0.147112  [   64/  146]
train() client id: f_00005-5-2 loss: 0.589176  [   96/  146]
train() client id: f_00005-5-3 loss: 0.508469  [  128/  146]
train() client id: f_00005-6-0 loss: 0.182201  [   32/  146]
train() client id: f_00005-6-1 loss: 0.319091  [   64/  146]
train() client id: f_00005-6-2 loss: 0.659054  [   96/  146]
train() client id: f_00005-6-3 loss: 0.453871  [  128/  146]
train() client id: f_00005-7-0 loss: 0.223903  [   32/  146]
train() client id: f_00005-7-1 loss: 0.351058  [   64/  146]
train() client id: f_00005-7-2 loss: 0.618519  [   96/  146]
train() client id: f_00005-7-3 loss: 0.493294  [  128/  146]
train() client id: f_00005-8-0 loss: 0.435957  [   32/  146]
train() client id: f_00005-8-1 loss: 0.495240  [   64/  146]
train() client id: f_00005-8-2 loss: 0.308078  [   96/  146]
train() client id: f_00005-8-3 loss: 0.416236  [  128/  146]
train() client id: f_00005-9-0 loss: 0.413121  [   32/  146]
train() client id: f_00005-9-1 loss: 0.434156  [   64/  146]
train() client id: f_00005-9-2 loss: 0.316720  [   96/  146]
train() client id: f_00005-9-3 loss: 0.384580  [  128/  146]
train() client id: f_00006-0-0 loss: 0.471802  [   32/   54]
train() client id: f_00006-1-0 loss: 0.446083  [   32/   54]
train() client id: f_00006-2-0 loss: 0.446035  [   32/   54]
train() client id: f_00006-3-0 loss: 0.452443  [   32/   54]
train() client id: f_00006-4-0 loss: 0.482378  [   32/   54]
train() client id: f_00006-5-0 loss: 0.483917  [   32/   54]
train() client id: f_00006-6-0 loss: 0.456109  [   32/   54]
train() client id: f_00006-7-0 loss: 0.393100  [   32/   54]
train() client id: f_00006-8-0 loss: 0.454142  [   32/   54]
train() client id: f_00006-9-0 loss: 0.497021  [   32/   54]
train() client id: f_00007-0-0 loss: 0.866182  [   32/  179]
train() client id: f_00007-0-1 loss: 0.495549  [   64/  179]
train() client id: f_00007-0-2 loss: 0.508072  [   96/  179]
train() client id: f_00007-0-3 loss: 0.689726  [  128/  179]
train() client id: f_00007-0-4 loss: 0.559258  [  160/  179]
train() client id: f_00007-1-0 loss: 0.611275  [   32/  179]
train() client id: f_00007-1-1 loss: 0.694950  [   64/  179]
train() client id: f_00007-1-2 loss: 0.603589  [   96/  179]
train() client id: f_00007-1-3 loss: 0.659538  [  128/  179]
train() client id: f_00007-1-4 loss: 0.706614  [  160/  179]
train() client id: f_00007-2-0 loss: 0.678791  [   32/  179]
train() client id: f_00007-2-1 loss: 0.502725  [   64/  179]
train() client id: f_00007-2-2 loss: 0.618035  [   96/  179]
train() client id: f_00007-2-3 loss: 0.741523  [  128/  179]
train() client id: f_00007-2-4 loss: 0.660843  [  160/  179]
train() client id: f_00007-3-0 loss: 0.466734  [   32/  179]
train() client id: f_00007-3-1 loss: 0.678679  [   64/  179]
train() client id: f_00007-3-2 loss: 0.717960  [   96/  179]
train() client id: f_00007-3-3 loss: 0.621559  [  128/  179]
train() client id: f_00007-3-4 loss: 0.762099  [  160/  179]
train() client id: f_00007-4-0 loss: 0.548872  [   32/  179]
train() client id: f_00007-4-1 loss: 0.579201  [   64/  179]
train() client id: f_00007-4-2 loss: 0.693456  [   96/  179]
train() client id: f_00007-4-3 loss: 0.765837  [  128/  179]
train() client id: f_00007-4-4 loss: 0.641727  [  160/  179]
train() client id: f_00007-5-0 loss: 0.761800  [   32/  179]
train() client id: f_00007-5-1 loss: 0.509532  [   64/  179]
train() client id: f_00007-5-2 loss: 0.581364  [   96/  179]
train() client id: f_00007-5-3 loss: 0.633930  [  128/  179]
train() client id: f_00007-5-4 loss: 0.648929  [  160/  179]
train() client id: f_00007-6-0 loss: 0.539002  [   32/  179]
train() client id: f_00007-6-1 loss: 0.458287  [   64/  179]
train() client id: f_00007-6-2 loss: 0.572374  [   96/  179]
train() client id: f_00007-6-3 loss: 0.676051  [  128/  179]
train() client id: f_00007-6-4 loss: 0.648984  [  160/  179]
train() client id: f_00007-7-0 loss: 0.655899  [   32/  179]
train() client id: f_00007-7-1 loss: 0.453561  [   64/  179]
train() client id: f_00007-7-2 loss: 0.782179  [   96/  179]
train() client id: f_00007-7-3 loss: 0.611547  [  128/  179]
train() client id: f_00007-7-4 loss: 0.702744  [  160/  179]
train() client id: f_00007-8-0 loss: 0.682375  [   32/  179]
train() client id: f_00007-8-1 loss: 0.596222  [   64/  179]
train() client id: f_00007-8-2 loss: 0.552652  [   96/  179]
train() client id: f_00007-8-3 loss: 0.823218  [  128/  179]
train() client id: f_00007-8-4 loss: 0.435497  [  160/  179]
train() client id: f_00007-9-0 loss: 0.475638  [   32/  179]
train() client id: f_00007-9-1 loss: 0.644420  [   64/  179]
train() client id: f_00007-9-2 loss: 0.447392  [   96/  179]
train() client id: f_00007-9-3 loss: 0.742256  [  128/  179]
train() client id: f_00007-9-4 loss: 0.738247  [  160/  179]
train() client id: f_00008-0-0 loss: 0.617931  [   32/  130]
train() client id: f_00008-0-1 loss: 0.694895  [   64/  130]
train() client id: f_00008-0-2 loss: 0.702377  [   96/  130]
train() client id: f_00008-0-3 loss: 0.588184  [  128/  130]
train() client id: f_00008-1-0 loss: 0.649695  [   32/  130]
train() client id: f_00008-1-1 loss: 0.684588  [   64/  130]
train() client id: f_00008-1-2 loss: 0.608641  [   96/  130]
train() client id: f_00008-1-3 loss: 0.660145  [  128/  130]
train() client id: f_00008-2-0 loss: 0.533299  [   32/  130]
train() client id: f_00008-2-1 loss: 0.700420  [   64/  130]
train() client id: f_00008-2-2 loss: 0.696942  [   96/  130]
train() client id: f_00008-2-3 loss: 0.604866  [  128/  130]
train() client id: f_00008-3-0 loss: 0.615040  [   32/  130]
train() client id: f_00008-3-1 loss: 0.687123  [   64/  130]
train() client id: f_00008-3-2 loss: 0.641060  [   96/  130]
train() client id: f_00008-3-3 loss: 0.654674  [  128/  130]
train() client id: f_00008-4-0 loss: 0.600229  [   32/  130]
train() client id: f_00008-4-1 loss: 0.780022  [   64/  130]
train() client id: f_00008-4-2 loss: 0.631858  [   96/  130]
train() client id: f_00008-4-3 loss: 0.602784  [  128/  130]
train() client id: f_00008-5-0 loss: 0.689254  [   32/  130]
train() client id: f_00008-5-1 loss: 0.546701  [   64/  130]
train() client id: f_00008-5-2 loss: 0.771354  [   96/  130]
train() client id: f_00008-5-3 loss: 0.577498  [  128/  130]
train() client id: f_00008-6-0 loss: 0.710552  [   32/  130]
train() client id: f_00008-6-1 loss: 0.646608  [   64/  130]
train() client id: f_00008-6-2 loss: 0.574024  [   96/  130]
train() client id: f_00008-6-3 loss: 0.657499  [  128/  130]
train() client id: f_00008-7-0 loss: 0.666952  [   32/  130]
train() client id: f_00008-7-1 loss: 0.707096  [   64/  130]
train() client id: f_00008-7-2 loss: 0.655294  [   96/  130]
train() client id: f_00008-7-3 loss: 0.588955  [  128/  130]
train() client id: f_00008-8-0 loss: 0.558509  [   32/  130]
train() client id: f_00008-8-1 loss: 0.609486  [   64/  130]
train() client id: f_00008-8-2 loss: 0.778316  [   96/  130]
train() client id: f_00008-8-3 loss: 0.665516  [  128/  130]
train() client id: f_00008-9-0 loss: 0.676809  [   32/  130]
train() client id: f_00008-9-1 loss: 0.676983  [   64/  130]
train() client id: f_00008-9-2 loss: 0.515346  [   96/  130]
train() client id: f_00008-9-3 loss: 0.709855  [  128/  130]
train() client id: f_00009-0-0 loss: 1.160089  [   32/  118]
train() client id: f_00009-0-1 loss: 1.121232  [   64/  118]
train() client id: f_00009-0-2 loss: 0.980976  [   96/  118]
train() client id: f_00009-1-0 loss: 1.083888  [   32/  118]
train() client id: f_00009-1-1 loss: 1.156223  [   64/  118]
train() client id: f_00009-1-2 loss: 0.967530  [   96/  118]
train() client id: f_00009-2-0 loss: 1.086610  [   32/  118]
train() client id: f_00009-2-1 loss: 0.954172  [   64/  118]
train() client id: f_00009-2-2 loss: 0.882536  [   96/  118]
train() client id: f_00009-3-0 loss: 1.111768  [   32/  118]
train() client id: f_00009-3-1 loss: 0.943605  [   64/  118]
train() client id: f_00009-3-2 loss: 0.899526  [   96/  118]
train() client id: f_00009-4-0 loss: 0.937003  [   32/  118]
train() client id: f_00009-4-1 loss: 0.969589  [   64/  118]
train() client id: f_00009-4-2 loss: 1.131795  [   96/  118]
train() client id: f_00009-5-0 loss: 0.954926  [   32/  118]
train() client id: f_00009-5-1 loss: 1.023686  [   64/  118]
train() client id: f_00009-5-2 loss: 0.862791  [   96/  118]
train() client id: f_00009-6-0 loss: 1.052540  [   32/  118]
train() client id: f_00009-6-1 loss: 0.781026  [   64/  118]
train() client id: f_00009-6-2 loss: 0.976057  [   96/  118]
train() client id: f_00009-7-0 loss: 0.965585  [   32/  118]
train() client id: f_00009-7-1 loss: 0.763052  [   64/  118]
train() client id: f_00009-7-2 loss: 1.012087  [   96/  118]
train() client id: f_00009-8-0 loss: 0.850408  [   32/  118]
train() client id: f_00009-8-1 loss: 0.992733  [   64/  118]
train() client id: f_00009-8-2 loss: 1.100017  [   96/  118]
train() client id: f_00009-9-0 loss: 1.064001  [   32/  118]
train() client id: f_00009-9-1 loss: 0.721865  [   64/  118]
train() client id: f_00009-9-2 loss: 0.957260  [   96/  118]
At round 62 accuracy: 0.6472148541114059
At round 62 training accuracy: 0.590878604963112
At round 62 training loss: 0.8147314776073272
update_location
xs = [  -3.9056584     4.20031788  330.00902392   18.81129433    0.97929623
    3.95640986 -292.44319194 -271.32485185  314.66397685 -257.06087855]
ys = [ 322.5879595   305.55583871    1.32061395 -292.45517586  284.35018685
  267.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [337.75471245 321.53073452 344.82995793 309.65124686 301.42327014
 285.90220796 309.07913397 289.16751458 330.63890923 275.85559109]
dists_bs = [225.95459613 221.71508958 534.03992676 506.04066956 207.18633175
 201.59331142 212.94022257 199.03851774 514.34942217 189.66939928]
uav_gains = [1.42435425e-12 1.82215782e-12 1.29334638e-12 2.23367367e-12
 2.60101956e-12 3.53945382e-12 2.25679855e-12 3.31121347e-12
 1.57981843e-12 4.36146341e-12]
bs_gains = [2.83161877e-11 2.98584584e-11 2.54734430e-12 2.96193078e-12
 3.60979885e-11 3.89727411e-11 3.34327819e-11 4.03896532e-11
 2.82989921e-12 4.62276411e-11]
Round 63
-------------------------------
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.68727091 5.38361967 2.65494812 0.98808132 6.20602153 2.98709652
 1.20875503 3.70447299 2.72587058 2.42213493]
obj_prev = 30.968271608412685
eta_min = 2.604478585360512e-35	eta_max = 0.9403513878285256
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 7.091465885945381	eta = 0.9090909090909091
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 17.42929397319813	eta = 0.36988229006606876
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 11.714987188776163	eta = 0.5503025368408222
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.719694765497792	eta = 0.601396523881516
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657792616593055	eta = 0.6048895302207599
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519519314839	eta = 0.6049050304207853
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519513957864	eta = 0.6049050307248393
eta = 0.6049050307248393
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.04148026 0.08724024 0.04082185 0.01415596 0.10073778 0.04806443
 0.01777726 0.05892831 0.0427971  0.0388466 ]
ene_total = [1.07662235 1.59233833 1.08542302 0.54531731 1.81230098 0.92997575
 0.60440513 1.2459445  0.99375704 0.77143509]
ti_comp = [1.37728495 1.52626582 1.36543421 1.42198973 1.52960948 1.53088406
 1.42284286 1.45043831 1.44004221 1.53358086]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [2.35156460e-06 1.78143569e-05 2.28042807e-06 8.76809665e-08
 2.73083858e-05 2.96119390e-06 1.73444402e-07 6.07929826e-06
 2.36250370e-06 1.55784967e-06]
ene_total = [0.38350781 0.13481247 0.40330974 0.30876628 0.12938372 0.12684698
 0.3073421  0.26132755 0.2786378  0.12231706]
optimize_network iter = 0 obj = 2.4562515021459133
eta = 0.6049050307248393
freqs = [15058707.65852771 28579633.50514324 14948302.02718542  4977519.32111269
 32929248.53672925 15698259.84497274  6247090.26876788 20313966.31628257
 14859667.46433555 12665323.49976712]
eta_min = 0.6049050307248401	eta_max = 0.735169781083726
af = 0.0008271317221307269	bf = 0.9700368260492027	zeta = 0.0009098448943437997	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.45862869e-07 3.37764920e-06 4.32375195e-07 1.66245432e-08
 5.17774221e-06 5.61450199e-07 3.28855174e-08 1.15265103e-06
 4.47936952e-07 2.95372418e-07]
ene_total = [1.66660366 0.58485258 1.75266791 1.34190688 0.5607002  0.55110835
 1.3357123  1.1353834  1.21083327 0.53150371]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 1 obj = 3.6637216068534983
eta = 0.735169781083726
freqs = [14977014.44528698 26790042.71364152 14948302.02718542  4855109.38680831
 30831466.09028835 14691709.46586062  6091289.81349057 19586300.25463695
 14387143.5153923  11842221.20609098]
eta_min = 0.7351697810837274	eta_max = 0.735169781083714
af = 0.0007390421746511915	bf = 0.9700368260492027	zeta = 0.0008129463921163107	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.41038396e-07 2.96789166e-06 4.32375195e-07 1.58169176e-08
 4.53905178e-06 4.91759633e-07 3.12656630e-08 1.07155188e-06
 4.19901921e-07 2.58228221e-07]
ene_total = [1.66660331 0.58482282 1.75266791 1.34190682 0.56065381 0.55110329
 1.33571218 1.13537751 1.21083123 0.53150101]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 2 obj = 3.6637216068533323
eta = 0.735169781083714
freqs = [14977014.44528692 26790042.71364162 14948302.02718534  4855109.38680831
 30831466.09028849 14691709.46586069  6091289.81349056 19586300.25463695
 14387143.51539229 11842221.20609103]
Done!
At round 63 energy consumption: 3.6637216068534983
At round 63 eta: 0.735169781083714
At round 63 local rounds: 10.074148866251232
At round 63 global rounds: 24.929989199134315
At round 63 a_n: 6.259668650218071
gradient difference: 0.4665254056453705
train() client id: f_00000-0-0 loss: 1.234607  [   32/  126]
train() client id: f_00000-0-1 loss: 1.181619  [   64/  126]
train() client id: f_00000-0-2 loss: 0.958496  [   96/  126]
train() client id: f_00000-1-0 loss: 0.897930  [   32/  126]
train() client id: f_00000-1-1 loss: 1.211504  [   64/  126]
train() client id: f_00000-1-2 loss: 0.902456  [   96/  126]
train() client id: f_00000-2-0 loss: 1.077978  [   32/  126]
train() client id: f_00000-2-1 loss: 0.882898  [   64/  126]
train() client id: f_00000-2-2 loss: 1.076065  [   96/  126]
train() client id: f_00000-3-0 loss: 0.979508  [   32/  126]
train() client id: f_00000-3-1 loss: 0.869767  [   64/  126]
train() client id: f_00000-3-2 loss: 1.045676  [   96/  126]
train() client id: f_00000-4-0 loss: 0.915955  [   32/  126]
train() client id: f_00000-4-1 loss: 0.983843  [   64/  126]
train() client id: f_00000-4-2 loss: 0.965322  [   96/  126]
train() client id: f_00000-5-0 loss: 0.866442  [   32/  126]
train() client id: f_00000-5-1 loss: 0.786260  [   64/  126]
train() client id: f_00000-5-2 loss: 1.192054  [   96/  126]
train() client id: f_00000-6-0 loss: 0.764134  [   32/  126]
train() client id: f_00000-6-1 loss: 0.884812  [   64/  126]
train() client id: f_00000-6-2 loss: 1.028316  [   96/  126]
train() client id: f_00000-7-0 loss: 0.801496  [   32/  126]
train() client id: f_00000-7-1 loss: 0.891654  [   64/  126]
train() client id: f_00000-7-2 loss: 0.896716  [   96/  126]
train() client id: f_00000-8-0 loss: 0.976972  [   32/  126]
train() client id: f_00000-8-1 loss: 0.960724  [   64/  126]
train() client id: f_00000-8-2 loss: 0.796292  [   96/  126]
train() client id: f_00000-9-0 loss: 0.810394  [   32/  126]
train() client id: f_00000-9-1 loss: 0.855532  [   64/  126]
train() client id: f_00000-9-2 loss: 0.915768  [   96/  126]
train() client id: f_00001-0-0 loss: 0.395185  [   32/  265]
train() client id: f_00001-0-1 loss: 0.430555  [   64/  265]
train() client id: f_00001-0-2 loss: 0.394252  [   96/  265]
train() client id: f_00001-0-3 loss: 0.360456  [  128/  265]
train() client id: f_00001-0-4 loss: 0.332406  [  160/  265]
train() client id: f_00001-0-5 loss: 0.417165  [  192/  265]
train() client id: f_00001-0-6 loss: 0.489809  [  224/  265]
train() client id: f_00001-0-7 loss: 0.296860  [  256/  265]
train() client id: f_00001-1-0 loss: 0.327084  [   32/  265]
train() client id: f_00001-1-1 loss: 0.377646  [   64/  265]
train() client id: f_00001-1-2 loss: 0.376361  [   96/  265]
train() client id: f_00001-1-3 loss: 0.433742  [  128/  265]
train() client id: f_00001-1-4 loss: 0.498972  [  160/  265]
train() client id: f_00001-1-5 loss: 0.338520  [  192/  265]
train() client id: f_00001-1-6 loss: 0.448450  [  224/  265]
train() client id: f_00001-1-7 loss: 0.308023  [  256/  265]
train() client id: f_00001-2-0 loss: 0.319983  [   32/  265]
train() client id: f_00001-2-1 loss: 0.351991  [   64/  265]
train() client id: f_00001-2-2 loss: 0.427684  [   96/  265]
train() client id: f_00001-2-3 loss: 0.359210  [  128/  265]
train() client id: f_00001-2-4 loss: 0.594414  [  160/  265]
train() client id: f_00001-2-5 loss: 0.296795  [  192/  265]
train() client id: f_00001-2-6 loss: 0.350982  [  224/  265]
train() client id: f_00001-2-7 loss: 0.378685  [  256/  265]
train() client id: f_00001-3-0 loss: 0.355046  [   32/  265]
train() client id: f_00001-3-1 loss: 0.410808  [   64/  265]
train() client id: f_00001-3-2 loss: 0.385929  [   96/  265]
train() client id: f_00001-3-3 loss: 0.342671  [  128/  265]
train() client id: f_00001-3-4 loss: 0.475720  [  160/  265]
train() client id: f_00001-3-5 loss: 0.387590  [  192/  265]
train() client id: f_00001-3-6 loss: 0.339134  [  224/  265]
train() client id: f_00001-3-7 loss: 0.339851  [  256/  265]
train() client id: f_00001-4-0 loss: 0.270490  [   32/  265]
train() client id: f_00001-4-1 loss: 0.394325  [   64/  265]
train() client id: f_00001-4-2 loss: 0.428392  [   96/  265]
train() client id: f_00001-4-3 loss: 0.384573  [  128/  265]
train() client id: f_00001-4-4 loss: 0.536936  [  160/  265]
train() client id: f_00001-4-5 loss: 0.260472  [  192/  265]
train() client id: f_00001-4-6 loss: 0.389638  [  224/  265]
train() client id: f_00001-4-7 loss: 0.354792  [  256/  265]
train() client id: f_00001-5-0 loss: 0.427280  [   32/  265]
train() client id: f_00001-5-1 loss: 0.352446  [   64/  265]
train() client id: f_00001-5-2 loss: 0.290916  [   96/  265]
train() client id: f_00001-5-3 loss: 0.443756  [  128/  265]
train() client id: f_00001-5-4 loss: 0.538905  [  160/  265]
train() client id: f_00001-5-5 loss: 0.303499  [  192/  265]
train() client id: f_00001-5-6 loss: 0.314864  [  224/  265]
train() client id: f_00001-5-7 loss: 0.296163  [  256/  265]
train() client id: f_00001-6-0 loss: 0.353217  [   32/  265]
train() client id: f_00001-6-1 loss: 0.402525  [   64/  265]
train() client id: f_00001-6-2 loss: 0.274584  [   96/  265]
train() client id: f_00001-6-3 loss: 0.353618  [  128/  265]
train() client id: f_00001-6-4 loss: 0.285867  [  160/  265]
train() client id: f_00001-6-5 loss: 0.404907  [  192/  265]
train() client id: f_00001-6-6 loss: 0.512013  [  224/  265]
train() client id: f_00001-6-7 loss: 0.403370  [  256/  265]
train() client id: f_00001-7-0 loss: 0.283623  [   32/  265]
train() client id: f_00001-7-1 loss: 0.352515  [   64/  265]
train() client id: f_00001-7-2 loss: 0.290593  [   96/  265]
train() client id: f_00001-7-3 loss: 0.486299  [  128/  265]
train() client id: f_00001-7-4 loss: 0.396510  [  160/  265]
train() client id: f_00001-7-5 loss: 0.280038  [  192/  265]
train() client id: f_00001-7-6 loss: 0.484686  [  224/  265]
train() client id: f_00001-7-7 loss: 0.369525  [  256/  265]
train() client id: f_00001-8-0 loss: 0.428121  [   32/  265]
train() client id: f_00001-8-1 loss: 0.370169  [   64/  265]
train() client id: f_00001-8-2 loss: 0.356934  [   96/  265]
train() client id: f_00001-8-3 loss: 0.345146  [  128/  265]
train() client id: f_00001-8-4 loss: 0.446398  [  160/  265]
train() client id: f_00001-8-5 loss: 0.380172  [  192/  265]
train() client id: f_00001-8-6 loss: 0.333419  [  224/  265]
train() client id: f_00001-8-7 loss: 0.271676  [  256/  265]
train() client id: f_00001-9-0 loss: 0.305750  [   32/  265]
train() client id: f_00001-9-1 loss: 0.367624  [   64/  265]
train() client id: f_00001-9-2 loss: 0.421245  [   96/  265]
train() client id: f_00001-9-3 loss: 0.333644  [  128/  265]
train() client id: f_00001-9-4 loss: 0.402857  [  160/  265]
train() client id: f_00001-9-5 loss: 0.411632  [  192/  265]
train() client id: f_00001-9-6 loss: 0.267641  [  224/  265]
train() client id: f_00001-9-7 loss: 0.481548  [  256/  265]
train() client id: f_00002-0-0 loss: 1.295196  [   32/  124]
train() client id: f_00002-0-1 loss: 1.058455  [   64/  124]
train() client id: f_00002-0-2 loss: 1.161839  [   96/  124]
train() client id: f_00002-1-0 loss: 1.151094  [   32/  124]
train() client id: f_00002-1-1 loss: 1.101183  [   64/  124]
train() client id: f_00002-1-2 loss: 1.229055  [   96/  124]
train() client id: f_00002-2-0 loss: 1.021606  [   32/  124]
train() client id: f_00002-2-1 loss: 1.257464  [   64/  124]
train() client id: f_00002-2-2 loss: 1.052992  [   96/  124]
train() client id: f_00002-3-0 loss: 1.167593  [   32/  124]
train() client id: f_00002-3-1 loss: 1.083678  [   64/  124]
train() client id: f_00002-3-2 loss: 1.101387  [   96/  124]
train() client id: f_00002-4-0 loss: 1.207990  [   32/  124]
train() client id: f_00002-4-1 loss: 1.186198  [   64/  124]
train() client id: f_00002-4-2 loss: 0.978020  [   96/  124]
train() client id: f_00002-5-0 loss: 1.037613  [   32/  124]
train() client id: f_00002-5-1 loss: 1.040972  [   64/  124]
train() client id: f_00002-5-2 loss: 1.123391  [   96/  124]
train() client id: f_00002-6-0 loss: 1.095387  [   32/  124]
train() client id: f_00002-6-1 loss: 0.927521  [   64/  124]
train() client id: f_00002-6-2 loss: 1.018298  [   96/  124]
train() client id: f_00002-7-0 loss: 0.969149  [   32/  124]
train() client id: f_00002-7-1 loss: 1.243518  [   64/  124]
train() client id: f_00002-7-2 loss: 0.866796  [   96/  124]
train() client id: f_00002-8-0 loss: 0.875041  [   32/  124]
train() client id: f_00002-8-1 loss: 0.938791  [   64/  124]
train() client id: f_00002-8-2 loss: 1.285994  [   96/  124]
train() client id: f_00002-9-0 loss: 1.000275  [   32/  124]
train() client id: f_00002-9-1 loss: 0.998813  [   64/  124]
train() client id: f_00002-9-2 loss: 0.962293  [   96/  124]
train() client id: f_00003-0-0 loss: 0.861618  [   32/   43]
train() client id: f_00003-1-0 loss: 0.782809  [   32/   43]
train() client id: f_00003-2-0 loss: 0.674720  [   32/   43]
train() client id: f_00003-3-0 loss: 0.694021  [   32/   43]
train() client id: f_00003-4-0 loss: 0.532234  [   32/   43]
train() client id: f_00003-5-0 loss: 0.718761  [   32/   43]
train() client id: f_00003-6-0 loss: 0.523234  [   32/   43]
train() client id: f_00003-7-0 loss: 0.704286  [   32/   43]
train() client id: f_00003-8-0 loss: 0.673325  [   32/   43]
train() client id: f_00003-9-0 loss: 0.667991  [   32/   43]
train() client id: f_00004-0-0 loss: 0.860566  [   32/  306]
train() client id: f_00004-0-1 loss: 1.044421  [   64/  306]
train() client id: f_00004-0-2 loss: 0.847668  [   96/  306]
train() client id: f_00004-0-3 loss: 0.837603  [  128/  306]
train() client id: f_00004-0-4 loss: 0.906339  [  160/  306]
train() client id: f_00004-0-5 loss: 0.687779  [  192/  306]
train() client id: f_00004-0-6 loss: 0.817347  [  224/  306]
train() client id: f_00004-0-7 loss: 0.770917  [  256/  306]
train() client id: f_00004-0-8 loss: 0.825101  [  288/  306]
train() client id: f_00004-1-0 loss: 0.876241  [   32/  306]
train() client id: f_00004-1-1 loss: 0.891907  [   64/  306]
train() client id: f_00004-1-2 loss: 0.779085  [   96/  306]
train() client id: f_00004-1-3 loss: 0.938071  [  128/  306]
train() client id: f_00004-1-4 loss: 0.893270  [  160/  306]
train() client id: f_00004-1-5 loss: 0.897275  [  192/  306]
train() client id: f_00004-1-6 loss: 0.881645  [  224/  306]
train() client id: f_00004-1-7 loss: 0.801816  [  256/  306]
train() client id: f_00004-1-8 loss: 0.763213  [  288/  306]
train() client id: f_00004-2-0 loss: 0.951435  [   32/  306]
train() client id: f_00004-2-1 loss: 0.791643  [   64/  306]
train() client id: f_00004-2-2 loss: 0.936022  [   96/  306]
train() client id: f_00004-2-3 loss: 0.791028  [  128/  306]
train() client id: f_00004-2-4 loss: 0.925095  [  160/  306]
train() client id: f_00004-2-5 loss: 0.729163  [  192/  306]
train() client id: f_00004-2-6 loss: 0.783064  [  224/  306]
train() client id: f_00004-2-7 loss: 0.874851  [  256/  306]
train() client id: f_00004-2-8 loss: 0.847585  [  288/  306]
train() client id: f_00004-3-0 loss: 0.730564  [   32/  306]
train() client id: f_00004-3-1 loss: 0.822967  [   64/  306]
train() client id: f_00004-3-2 loss: 0.756026  [   96/  306]
train() client id: f_00004-3-3 loss: 0.752757  [  128/  306]
train() client id: f_00004-3-4 loss: 0.786198  [  160/  306]
train() client id: f_00004-3-5 loss: 0.881036  [  192/  306]
train() client id: f_00004-3-6 loss: 0.943134  [  224/  306]
train() client id: f_00004-3-7 loss: 0.912901  [  256/  306]
train() client id: f_00004-3-8 loss: 0.919926  [  288/  306]
train() client id: f_00004-4-0 loss: 0.866018  [   32/  306]
train() client id: f_00004-4-1 loss: 0.866190  [   64/  306]
train() client id: f_00004-4-2 loss: 1.014275  [   96/  306]
train() client id: f_00004-4-3 loss: 0.765715  [  128/  306]
train() client id: f_00004-4-4 loss: 0.850737  [  160/  306]
train() client id: f_00004-4-5 loss: 0.841008  [  192/  306]
train() client id: f_00004-4-6 loss: 0.752759  [  224/  306]
train() client id: f_00004-4-7 loss: 0.832160  [  256/  306]
train() client id: f_00004-4-8 loss: 0.909207  [  288/  306]
train() client id: f_00004-5-0 loss: 0.825064  [   32/  306]
train() client id: f_00004-5-1 loss: 0.792722  [   64/  306]
train() client id: f_00004-5-2 loss: 0.723117  [   96/  306]
train() client id: f_00004-5-3 loss: 0.817943  [  128/  306]
train() client id: f_00004-5-4 loss: 0.899197  [  160/  306]
train() client id: f_00004-5-5 loss: 0.917444  [  192/  306]
train() client id: f_00004-5-6 loss: 0.908109  [  224/  306]
train() client id: f_00004-5-7 loss: 0.876542  [  256/  306]
train() client id: f_00004-5-8 loss: 0.831728  [  288/  306]
train() client id: f_00004-6-0 loss: 0.826918  [   32/  306]
train() client id: f_00004-6-1 loss: 0.941408  [   64/  306]
train() client id: f_00004-6-2 loss: 0.823042  [   96/  306]
train() client id: f_00004-6-3 loss: 0.960892  [  128/  306]
train() client id: f_00004-6-4 loss: 0.791554  [  160/  306]
train() client id: f_00004-6-5 loss: 0.674905  [  192/  306]
train() client id: f_00004-6-6 loss: 0.856214  [  224/  306]
train() client id: f_00004-6-7 loss: 0.841281  [  256/  306]
train() client id: f_00004-6-8 loss: 0.914101  [  288/  306]
train() client id: f_00004-7-0 loss: 0.897512  [   32/  306]
train() client id: f_00004-7-1 loss: 0.859709  [   64/  306]
train() client id: f_00004-7-2 loss: 0.820379  [   96/  306]
train() client id: f_00004-7-3 loss: 0.872543  [  128/  306]
train() client id: f_00004-7-4 loss: 0.804736  [  160/  306]
train() client id: f_00004-7-5 loss: 0.872896  [  192/  306]
train() client id: f_00004-7-6 loss: 0.745623  [  224/  306]
train() client id: f_00004-7-7 loss: 0.743815  [  256/  306]
train() client id: f_00004-7-8 loss: 1.050702  [  288/  306]
train() client id: f_00004-8-0 loss: 0.823338  [   32/  306]
train() client id: f_00004-8-1 loss: 0.692295  [   64/  306]
train() client id: f_00004-8-2 loss: 0.918725  [   96/  306]
train() client id: f_00004-8-3 loss: 0.808526  [  128/  306]
train() client id: f_00004-8-4 loss: 0.924529  [  160/  306]
train() client id: f_00004-8-5 loss: 0.911447  [  192/  306]
train() client id: f_00004-8-6 loss: 0.781416  [  224/  306]
train() client id: f_00004-8-7 loss: 1.056096  [  256/  306]
train() client id: f_00004-8-8 loss: 0.790786  [  288/  306]
train() client id: f_00004-9-0 loss: 0.874972  [   32/  306]
train() client id: f_00004-9-1 loss: 0.786243  [   64/  306]
train() client id: f_00004-9-2 loss: 0.784033  [   96/  306]
train() client id: f_00004-9-3 loss: 1.001088  [  128/  306]
train() client id: f_00004-9-4 loss: 0.830755  [  160/  306]
train() client id: f_00004-9-5 loss: 0.854254  [  192/  306]
train() client id: f_00004-9-6 loss: 0.797450  [  224/  306]
train() client id: f_00004-9-7 loss: 0.911907  [  256/  306]
train() client id: f_00004-9-8 loss: 0.750321  [  288/  306]
train() client id: f_00005-0-0 loss: 0.898428  [   32/  146]
train() client id: f_00005-0-1 loss: 0.737253  [   64/  146]
train() client id: f_00005-0-2 loss: 0.528338  [   96/  146]
train() client id: f_00005-0-3 loss: 0.462284  [  128/  146]
train() client id: f_00005-1-0 loss: 0.501292  [   32/  146]
train() client id: f_00005-1-1 loss: 0.938807  [   64/  146]
train() client id: f_00005-1-2 loss: 0.532429  [   96/  146]
train() client id: f_00005-1-3 loss: 0.640509  [  128/  146]
train() client id: f_00005-2-0 loss: 0.570962  [   32/  146]
train() client id: f_00005-2-1 loss: 0.515925  [   64/  146]
train() client id: f_00005-2-2 loss: 0.925982  [   96/  146]
train() client id: f_00005-2-3 loss: 0.598185  [  128/  146]
train() client id: f_00005-3-0 loss: 0.285550  [   32/  146]
train() client id: f_00005-3-1 loss: 0.553035  [   64/  146]
train() client id: f_00005-3-2 loss: 0.600651  [   96/  146]
train() client id: f_00005-3-3 loss: 0.828700  [  128/  146]
train() client id: f_00005-4-0 loss: 0.367367  [   32/  146]
train() client id: f_00005-4-1 loss: 0.926004  [   64/  146]
train() client id: f_00005-4-2 loss: 0.869119  [   96/  146]
train() client id: f_00005-4-3 loss: 0.517630  [  128/  146]
train() client id: f_00005-5-0 loss: 0.486596  [   32/  146]
train() client id: f_00005-5-1 loss: 0.900465  [   64/  146]
train() client id: f_00005-5-2 loss: 0.743260  [   96/  146]
train() client id: f_00005-5-3 loss: 0.369084  [  128/  146]
train() client id: f_00005-6-0 loss: 0.420782  [   32/  146]
train() client id: f_00005-6-1 loss: 0.567168  [   64/  146]
train() client id: f_00005-6-2 loss: 0.685898  [   96/  146]
train() client id: f_00005-6-3 loss: 0.651887  [  128/  146]
train() client id: f_00005-7-0 loss: 0.533452  [   32/  146]
train() client id: f_00005-7-1 loss: 0.897717  [   64/  146]
train() client id: f_00005-7-2 loss: 0.569919  [   96/  146]
train() client id: f_00005-7-3 loss: 0.451376  [  128/  146]
train() client id: f_00005-8-0 loss: 0.583181  [   32/  146]
train() client id: f_00005-8-1 loss: 0.772279  [   64/  146]
train() client id: f_00005-8-2 loss: 0.496190  [   96/  146]
train() client id: f_00005-8-3 loss: 0.707028  [  128/  146]
train() client id: f_00005-9-0 loss: 0.719236  [   32/  146]
train() client id: f_00005-9-1 loss: 0.743895  [   64/  146]
train() client id: f_00005-9-2 loss: 0.785602  [   96/  146]
train() client id: f_00005-9-3 loss: 0.306339  [  128/  146]
train() client id: f_00006-0-0 loss: 0.425133  [   32/   54]
train() client id: f_00006-1-0 loss: 0.450557  [   32/   54]
train() client id: f_00006-2-0 loss: 0.515539  [   32/   54]
train() client id: f_00006-3-0 loss: 0.442306  [   32/   54]
train() client id: f_00006-4-0 loss: 0.455048  [   32/   54]
train() client id: f_00006-5-0 loss: 0.476037  [   32/   54]
train() client id: f_00006-6-0 loss: 0.508073  [   32/   54]
train() client id: f_00006-7-0 loss: 0.467979  [   32/   54]
train() client id: f_00006-8-0 loss: 0.393422  [   32/   54]
train() client id: f_00006-9-0 loss: 0.414655  [   32/   54]
train() client id: f_00007-0-0 loss: 0.338997  [   32/  179]
train() client id: f_00007-0-1 loss: 0.462702  [   64/  179]
train() client id: f_00007-0-2 loss: 0.473253  [   96/  179]
train() client id: f_00007-0-3 loss: 0.739186  [  128/  179]
train() client id: f_00007-0-4 loss: 0.324635  [  160/  179]
train() client id: f_00007-1-0 loss: 0.351410  [   32/  179]
train() client id: f_00007-1-1 loss: 0.281780  [   64/  179]
train() client id: f_00007-1-2 loss: 0.592061  [   96/  179]
train() client id: f_00007-1-3 loss: 0.561869  [  128/  179]
train() client id: f_00007-1-4 loss: 0.509375  [  160/  179]
train() client id: f_00007-2-0 loss: 0.401444  [   32/  179]
train() client id: f_00007-2-1 loss: 0.408236  [   64/  179]
train() client id: f_00007-2-2 loss: 0.392729  [   96/  179]
train() client id: f_00007-2-3 loss: 0.544872  [  128/  179]
train() client id: f_00007-2-4 loss: 0.451301  [  160/  179]
train() client id: f_00007-3-0 loss: 0.670456  [   32/  179]
train() client id: f_00007-3-1 loss: 0.514438  [   64/  179]
train() client id: f_00007-3-2 loss: 0.388114  [   96/  179]
train() client id: f_00007-3-3 loss: 0.445165  [  128/  179]
train() client id: f_00007-3-4 loss: 0.263307  [  160/  179]
train() client id: f_00007-4-0 loss: 0.681492  [   32/  179]
train() client id: f_00007-4-1 loss: 0.255719  [   64/  179]
train() client id: f_00007-4-2 loss: 0.405173  [   96/  179]
train() client id: f_00007-4-3 loss: 0.385685  [  128/  179]
train() client id: f_00007-4-4 loss: 0.325448  [  160/  179]
train() client id: f_00007-5-0 loss: 0.402321  [   32/  179]
train() client id: f_00007-5-1 loss: 0.366781  [   64/  179]
train() client id: f_00007-5-2 loss: 0.410310  [   96/  179]
train() client id: f_00007-5-3 loss: 0.248452  [  128/  179]
train() client id: f_00007-5-4 loss: 0.623490  [  160/  179]
train() client id: f_00007-6-0 loss: 0.429575  [   32/  179]
train() client id: f_00007-6-1 loss: 0.460446  [   64/  179]
train() client id: f_00007-6-2 loss: 0.348983  [   96/  179]
train() client id: f_00007-6-3 loss: 0.412420  [  128/  179]
train() client id: f_00007-6-4 loss: 0.373557  [  160/  179]
train() client id: f_00007-7-0 loss: 0.643974  [   32/  179]
train() client id: f_00007-7-1 loss: 0.346675  [   64/  179]
train() client id: f_00007-7-2 loss: 0.291100  [   96/  179]
train() client id: f_00007-7-3 loss: 0.285775  [  128/  179]
train() client id: f_00007-7-4 loss: 0.510123  [  160/  179]
train() client id: f_00007-8-0 loss: 0.627401  [   32/  179]
train() client id: f_00007-8-1 loss: 0.461668  [   64/  179]
train() client id: f_00007-8-2 loss: 0.254550  [   96/  179]
train() client id: f_00007-8-3 loss: 0.447012  [  128/  179]
train() client id: f_00007-8-4 loss: 0.368331  [  160/  179]
train() client id: f_00007-9-0 loss: 0.711663  [   32/  179]
train() client id: f_00007-9-1 loss: 0.272999  [   64/  179]
train() client id: f_00007-9-2 loss: 0.491267  [   96/  179]
train() client id: f_00007-9-3 loss: 0.214750  [  128/  179]
train() client id: f_00007-9-4 loss: 0.349947  [  160/  179]
train() client id: f_00008-0-0 loss: 0.765217  [   32/  130]
train() client id: f_00008-0-1 loss: 0.521163  [   64/  130]
train() client id: f_00008-0-2 loss: 0.820406  [   96/  130]
train() client id: f_00008-0-3 loss: 0.646104  [  128/  130]
train() client id: f_00008-1-0 loss: 0.786198  [   32/  130]
train() client id: f_00008-1-1 loss: 0.638792  [   64/  130]
train() client id: f_00008-1-2 loss: 0.730292  [   96/  130]
train() client id: f_00008-1-3 loss: 0.651919  [  128/  130]
train() client id: f_00008-2-0 loss: 0.694979  [   32/  130]
train() client id: f_00008-2-1 loss: 0.696584  [   64/  130]
train() client id: f_00008-2-2 loss: 0.705051  [   96/  130]
train() client id: f_00008-2-3 loss: 0.710272  [  128/  130]
train() client id: f_00008-3-0 loss: 0.692472  [   32/  130]
train() client id: f_00008-3-1 loss: 0.750400  [   64/  130]
train() client id: f_00008-3-2 loss: 0.667404  [   96/  130]
train() client id: f_00008-3-3 loss: 0.700626  [  128/  130]
train() client id: f_00008-4-0 loss: 0.612356  [   32/  130]
train() client id: f_00008-4-1 loss: 0.740190  [   64/  130]
train() client id: f_00008-4-2 loss: 0.771072  [   96/  130]
train() client id: f_00008-4-3 loss: 0.685199  [  128/  130]
train() client id: f_00008-5-0 loss: 0.639578  [   32/  130]
train() client id: f_00008-5-1 loss: 0.698007  [   64/  130]
train() client id: f_00008-5-2 loss: 0.804164  [   96/  130]
train() client id: f_00008-5-3 loss: 0.670228  [  128/  130]
train() client id: f_00008-6-0 loss: 0.659373  [   32/  130]
train() client id: f_00008-6-1 loss: 0.761770  [   64/  130]
train() client id: f_00008-6-2 loss: 0.758799  [   96/  130]
train() client id: f_00008-6-3 loss: 0.631691  [  128/  130]
train() client id: f_00008-7-0 loss: 0.631054  [   32/  130]
train() client id: f_00008-7-1 loss: 0.718605  [   64/  130]
train() client id: f_00008-7-2 loss: 0.635746  [   96/  130]
train() client id: f_00008-7-3 loss: 0.808330  [  128/  130]
train() client id: f_00008-8-0 loss: 0.764140  [   32/  130]
train() client id: f_00008-8-1 loss: 0.632205  [   64/  130]
train() client id: f_00008-8-2 loss: 0.752055  [   96/  130]
train() client id: f_00008-8-3 loss: 0.640339  [  128/  130]
train() client id: f_00008-9-0 loss: 0.675117  [   32/  130]
train() client id: f_00008-9-1 loss: 0.605625  [   64/  130]
train() client id: f_00008-9-2 loss: 0.786251  [   96/  130]
train() client id: f_00008-9-3 loss: 0.693822  [  128/  130]
train() client id: f_00009-0-0 loss: 0.865996  [   32/  118]
train() client id: f_00009-0-1 loss: 0.990523  [   64/  118]
train() client id: f_00009-0-2 loss: 0.868438  [   96/  118]
train() client id: f_00009-1-0 loss: 0.981428  [   32/  118]
train() client id: f_00009-1-1 loss: 0.897599  [   64/  118]
train() client id: f_00009-1-2 loss: 0.934376  [   96/  118]
train() client id: f_00009-2-0 loss: 0.964741  [   32/  118]
train() client id: f_00009-2-1 loss: 0.875968  [   64/  118]
train() client id: f_00009-2-2 loss: 0.773143  [   96/  118]
train() client id: f_00009-3-0 loss: 0.757566  [   32/  118]
train() client id: f_00009-3-1 loss: 0.974223  [   64/  118]
train() client id: f_00009-3-2 loss: 0.890661  [   96/  118]
train() client id: f_00009-4-0 loss: 0.940320  [   32/  118]
train() client id: f_00009-4-1 loss: 0.775323  [   64/  118]
train() client id: f_00009-4-2 loss: 0.701347  [   96/  118]
train() client id: f_00009-5-0 loss: 0.701430  [   32/  118]
train() client id: f_00009-5-1 loss: 1.018520  [   64/  118]
train() client id: f_00009-5-2 loss: 0.851378  [   96/  118]
train() client id: f_00009-6-0 loss: 0.827883  [   32/  118]
train() client id: f_00009-6-1 loss: 0.608571  [   64/  118]
train() client id: f_00009-6-2 loss: 1.000786  [   96/  118]
train() client id: f_00009-7-0 loss: 0.857097  [   32/  118]
train() client id: f_00009-7-1 loss: 0.782427  [   64/  118]
train() client id: f_00009-7-2 loss: 0.901388  [   96/  118]
train() client id: f_00009-8-0 loss: 0.987211  [   32/  118]
train() client id: f_00009-8-1 loss: 0.752737  [   64/  118]
train() client id: f_00009-8-2 loss: 0.631216  [   96/  118]
train() client id: f_00009-9-0 loss: 0.820671  [   32/  118]
train() client id: f_00009-9-1 loss: 0.810727  [   64/  118]
train() client id: f_00009-9-2 loss: 0.781112  [   96/  118]
At round 63 accuracy: 0.6472148541114059
At round 63 training accuracy: 0.5915492957746479
At round 63 training loss: 0.8222181672429827
update_location
xs = [  -3.9056584     4.20031788  335.00902392   18.81129433    0.97929623
    3.95640986 -297.44319194 -276.32485185  319.66397685 -262.06087855]
ys = [ 327.5879595   310.55583871    1.32061395 -297.45517586  289.35018685
  272.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [342.53339308 326.28602732 349.61806322 314.37787206 306.14455678
 290.59114581 313.81418542 293.86408424 335.40084685 280.5207941 ]
dists_bs = [229.25173742 224.69565936 538.77057605 510.6630112  209.86823946
 203.94363122 215.73819853 201.50578171 519.11171038 191.88561647]
uav_gains = [1.33357588e-12 1.68888635e-12 1.21580973e-12 2.05498399e-12
 2.38086830e-12 3.21721303e-12 2.07518341e-12 3.01317070e-12
 1.47290941e-12 3.95629760e-12]
bs_gains = [2.71905946e-11 2.87626551e-11 2.48521091e-12 2.88747186e-12
 3.48211629e-11 3.77281641e-11 3.22328236e-11 3.90201606e-11
 2.75780638e-12 4.47481726e-11]
Round 64
-------------------------------
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.55289596 5.10473648 2.52232289 0.94134025 5.8844195  2.83245373
 1.15055847 3.5161317  2.58559463 2.29678183]
obj_prev = 29.387235422694697
eta_min = 3.6403482068472025e-37	eta_max = 0.941453835955468
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 6.723535975581212	eta = 0.909090909090909
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 16.80493378903063	eta = 0.36372088751318643
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 11.200979803524296	eta = 0.5456938178232738
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.229583039555175	eta = 0.5975126658351408
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168941452928603	eta = 0.601075879986136
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671162537121	eta = 0.6010918570034193
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671157124352	eta = 0.6010918573233796
eta = 0.6010918573233796
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.04200208 0.08833771 0.04133538 0.01433404 0.10200505 0.04866908
 0.01800089 0.05966962 0.04333548 0.03933528]
ene_total = [1.03172726 1.51359867 1.04004204 0.52610415 1.72269629 0.88362426
 0.58229031 1.19121994 0.94449752 0.7328707 ]
ti_comp = [1.47039382 1.62666926 1.45842809 1.51593046 1.63009164 1.63144497
 1.51679335 1.54542978 1.53931646 1.63417731]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [2.14202768e-06 1.62824548e-05 2.07527568e-06 8.00990907e-08
 2.49643934e-05 2.70703886e-06 1.58456196e-07 5.55957146e-06
 2.14661920e-06 1.42438847e-06]
ene_total = [0.37266401 0.1276584  0.3914396  0.30117553 0.12242423 0.11995133
 0.29982271 0.25497116 0.26451064 0.11564362]
optimize_network iter = 0 obj = 2.3702612355707346
eta = 0.6010918573233796
freqs = [14282595.45433634 27152940.78877741 14171209.70092077  4727803.67422734
 31288134.07679844 14915942.00582812  5933864.17259814 19305186.95139418
 14076208.69920771 12035193.86722137]
eta_min = 0.6010918573233806	eta_max = 0.7423534842131878
af = 0.0007076189579367038	bf = 0.9451563237723394	zeta = 0.0007783808537303743	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [4.01088465e-07 3.04884240e-06 3.88589348e-07 1.49983222e-08
 4.67451020e-06 5.06885170e-07 2.96704627e-08 1.04101362e-06
 4.01948215e-07 2.66712607e-07]
ene_total = [1.63512592 0.5592525  1.71751674 1.3215506  0.53579916 0.52619371
 1.31561006 1.11849968 1.16054979 0.50736327]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 1 obj = 3.6691462360745932
eta = 0.7423534842131878
freqs = [14200721.70636506 25299122.79358197 14171209.70092077  4604073.01677737
 29115814.90637482 13873547.37129865  5776388.64214124 18564736.76689159
 13570968.78990086 11183104.37494726]
eta_min = 0.7423534842131893	eta_max = 0.7423534842131851
af = 0.0006257175094953287	bf = 0.9451563237723394	zeta = 0.0006882892604448616	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [3.96503235e-07 2.64674534e-06 3.88589348e-07 1.42235570e-08
 4.04794575e-06 4.38513790e-07 2.81165434e-08 9.62688933e-07
 3.73611652e-07 2.30283129e-07]
ene_total = [1.63512561 0.55922482 1.71751674 1.32155055 0.53575602 0.526189
 1.31560995 1.11849429 1.16054784 0.50736076]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 2 obj = 3.669146236074556
eta = 0.7423534842131851
freqs = [14200721.70636504 25299122.79358199 14171209.70092075  4604073.01677736
 29115814.90637484 13873547.37129866  5776388.64214123 18564736.76689158
 13570968.78990086 11183104.37494727]
Done!
At round 64 energy consumption: 3.6691462360745932
At round 64 eta: 0.7423534842131851
At round 64 local rounds: 9.755733857653984
At round 64 global rounds: 24.295568799375985
At round 64 a_n: 5.917122803248752
gradient difference: 0.49760228395462036
train() client id: f_00000-0-0 loss: 1.152376  [   32/  126]
train() client id: f_00000-0-1 loss: 0.954377  [   64/  126]
train() client id: f_00000-0-2 loss: 1.210273  [   96/  126]
train() client id: f_00000-1-0 loss: 1.003305  [   32/  126]
train() client id: f_00000-1-1 loss: 1.152392  [   64/  126]
train() client id: f_00000-1-2 loss: 0.944992  [   96/  126]
train() client id: f_00000-2-0 loss: 1.038800  [   32/  126]
train() client id: f_00000-2-1 loss: 0.941280  [   64/  126]
train() client id: f_00000-2-2 loss: 0.947794  [   96/  126]
train() client id: f_00000-3-0 loss: 0.934267  [   32/  126]
train() client id: f_00000-3-1 loss: 1.007045  [   64/  126]
train() client id: f_00000-3-2 loss: 0.919587  [   96/  126]
train() client id: f_00000-4-0 loss: 0.938120  [   32/  126]
train() client id: f_00000-4-1 loss: 0.937582  [   64/  126]
train() client id: f_00000-4-2 loss: 0.836663  [   96/  126]
train() client id: f_00000-5-0 loss: 0.971194  [   32/  126]
train() client id: f_00000-5-1 loss: 0.852239  [   64/  126]
train() client id: f_00000-5-2 loss: 0.897192  [   96/  126]
train() client id: f_00000-6-0 loss: 0.849772  [   32/  126]
train() client id: f_00000-6-1 loss: 0.797670  [   64/  126]
train() client id: f_00000-6-2 loss: 0.747579  [   96/  126]
train() client id: f_00000-7-0 loss: 0.936472  [   32/  126]
train() client id: f_00000-7-1 loss: 0.759495  [   64/  126]
train() client id: f_00000-7-2 loss: 0.881785  [   96/  126]
train() client id: f_00000-8-0 loss: 0.774806  [   32/  126]
train() client id: f_00000-8-1 loss: 0.831896  [   64/  126]
train() client id: f_00000-8-2 loss: 0.813148  [   96/  126]
train() client id: f_00001-0-0 loss: 0.369656  [   32/  265]
train() client id: f_00001-0-1 loss: 0.319392  [   64/  265]
train() client id: f_00001-0-2 loss: 0.502588  [   96/  265]
train() client id: f_00001-0-3 loss: 0.410702  [  128/  265]
train() client id: f_00001-0-4 loss: 0.315424  [  160/  265]
train() client id: f_00001-0-5 loss: 0.518286  [  192/  265]
train() client id: f_00001-0-6 loss: 0.333865  [  224/  265]
train() client id: f_00001-0-7 loss: 0.397240  [  256/  265]
train() client id: f_00001-1-0 loss: 0.383348  [   32/  265]
train() client id: f_00001-1-1 loss: 0.442650  [   64/  265]
train() client id: f_00001-1-2 loss: 0.302319  [   96/  265]
train() client id: f_00001-1-3 loss: 0.441106  [  128/  265]
train() client id: f_00001-1-4 loss: 0.328786  [  160/  265]
train() client id: f_00001-1-5 loss: 0.373886  [  192/  265]
train() client id: f_00001-1-6 loss: 0.456285  [  224/  265]
train() client id: f_00001-1-7 loss: 0.344651  [  256/  265]
train() client id: f_00001-2-0 loss: 0.363264  [   32/  265]
train() client id: f_00001-2-1 loss: 0.309526  [   64/  265]
train() client id: f_00001-2-2 loss: 0.402785  [   96/  265]
train() client id: f_00001-2-3 loss: 0.352076  [  128/  265]
train() client id: f_00001-2-4 loss: 0.305772  [  160/  265]
train() client id: f_00001-2-5 loss: 0.325930  [  192/  265]
train() client id: f_00001-2-6 loss: 0.420525  [  224/  265]
train() client id: f_00001-2-7 loss: 0.449843  [  256/  265]
train() client id: f_00001-3-0 loss: 0.509346  [   32/  265]
train() client id: f_00001-3-1 loss: 0.307599  [   64/  265]
train() client id: f_00001-3-2 loss: 0.400481  [   96/  265]
train() client id: f_00001-3-3 loss: 0.370939  [  128/  265]
train() client id: f_00001-3-4 loss: 0.342068  [  160/  265]
train() client id: f_00001-3-5 loss: 0.372897  [  192/  265]
train() client id: f_00001-3-6 loss: 0.386528  [  224/  265]
train() client id: f_00001-3-7 loss: 0.266745  [  256/  265]
train() client id: f_00001-4-0 loss: 0.585842  [   32/  265]
train() client id: f_00001-4-1 loss: 0.359207  [   64/  265]
train() client id: f_00001-4-2 loss: 0.249218  [   96/  265]
train() client id: f_00001-4-3 loss: 0.390151  [  128/  265]
train() client id: f_00001-4-4 loss: 0.345107  [  160/  265]
train() client id: f_00001-4-5 loss: 0.277581  [  192/  265]
train() client id: f_00001-4-6 loss: 0.432703  [  224/  265]
train() client id: f_00001-4-7 loss: 0.273012  [  256/  265]
train() client id: f_00001-5-0 loss: 0.337786  [   32/  265]
train() client id: f_00001-5-1 loss: 0.301684  [   64/  265]
train() client id: f_00001-5-2 loss: 0.453160  [   96/  265]
train() client id: f_00001-5-3 loss: 0.255351  [  128/  265]
train() client id: f_00001-5-4 loss: 0.359114  [  160/  265]
train() client id: f_00001-5-5 loss: 0.290800  [  192/  265]
train() client id: f_00001-5-6 loss: 0.420885  [  224/  265]
train() client id: f_00001-5-7 loss: 0.467633  [  256/  265]
train() client id: f_00001-6-0 loss: 0.297946  [   32/  265]
train() client id: f_00001-6-1 loss: 0.262387  [   64/  265]
train() client id: f_00001-6-2 loss: 0.333767  [   96/  265]
train() client id: f_00001-6-3 loss: 0.460603  [  128/  265]
train() client id: f_00001-6-4 loss: 0.325642  [  160/  265]
train() client id: f_00001-6-5 loss: 0.409846  [  192/  265]
train() client id: f_00001-6-6 loss: 0.415266  [  224/  265]
train() client id: f_00001-6-7 loss: 0.340588  [  256/  265]
train() client id: f_00001-7-0 loss: 0.281815  [   32/  265]
train() client id: f_00001-7-1 loss: 0.347698  [   64/  265]
train() client id: f_00001-7-2 loss: 0.437180  [   96/  265]
train() client id: f_00001-7-3 loss: 0.465383  [  128/  265]
train() client id: f_00001-7-4 loss: 0.347643  [  160/  265]
train() client id: f_00001-7-5 loss: 0.310981  [  192/  265]
train() client id: f_00001-7-6 loss: 0.286111  [  224/  265]
train() client id: f_00001-7-7 loss: 0.353535  [  256/  265]
train() client id: f_00001-8-0 loss: 0.450383  [   32/  265]
train() client id: f_00001-8-1 loss: 0.441795  [   64/  265]
train() client id: f_00001-8-2 loss: 0.306545  [   96/  265]
train() client id: f_00001-8-3 loss: 0.321995  [  128/  265]
train() client id: f_00001-8-4 loss: 0.309764  [  160/  265]
train() client id: f_00001-8-5 loss: 0.340596  [  192/  265]
train() client id: f_00001-8-6 loss: 0.355162  [  224/  265]
train() client id: f_00001-8-7 loss: 0.275352  [  256/  265]
train() client id: f_00002-0-0 loss: 0.853956  [   32/  124]
train() client id: f_00002-0-1 loss: 1.169849  [   64/  124]
train() client id: f_00002-0-2 loss: 1.147923  [   96/  124]
train() client id: f_00002-1-0 loss: 1.212391  [   32/  124]
train() client id: f_00002-1-1 loss: 0.886216  [   64/  124]
train() client id: f_00002-1-2 loss: 1.023937  [   96/  124]
train() client id: f_00002-2-0 loss: 1.059111  [   32/  124]
train() client id: f_00002-2-1 loss: 0.973206  [   64/  124]
train() client id: f_00002-2-2 loss: 1.159155  [   96/  124]
train() client id: f_00002-3-0 loss: 0.949842  [   32/  124]
train() client id: f_00002-3-1 loss: 1.008663  [   64/  124]
train() client id: f_00002-3-2 loss: 1.110184  [   96/  124]
train() client id: f_00002-4-0 loss: 1.039445  [   32/  124]
train() client id: f_00002-4-1 loss: 0.970647  [   64/  124]
train() client id: f_00002-4-2 loss: 1.093652  [   96/  124]
train() client id: f_00002-5-0 loss: 1.035904  [   32/  124]
train() client id: f_00002-5-1 loss: 1.060915  [   64/  124]
train() client id: f_00002-5-2 loss: 0.871101  [   96/  124]
train() client id: f_00002-6-0 loss: 0.946580  [   32/  124]
train() client id: f_00002-6-1 loss: 1.046722  [   64/  124]
train() client id: f_00002-6-2 loss: 0.896191  [   96/  124]
train() client id: f_00002-7-0 loss: 0.969290  [   32/  124]
train() client id: f_00002-7-1 loss: 0.889980  [   64/  124]
train() client id: f_00002-7-2 loss: 1.112299  [   96/  124]
train() client id: f_00002-8-0 loss: 0.948696  [   32/  124]
train() client id: f_00002-8-1 loss: 1.107252  [   64/  124]
train() client id: f_00002-8-2 loss: 0.952444  [   96/  124]
train() client id: f_00003-0-0 loss: 0.601193  [   32/   43]
train() client id: f_00003-1-0 loss: 0.698121  [   32/   43]
train() client id: f_00003-2-0 loss: 0.820496  [   32/   43]
train() client id: f_00003-3-0 loss: 0.814391  [   32/   43]
train() client id: f_00003-4-0 loss: 0.695649  [   32/   43]
train() client id: f_00003-5-0 loss: 0.854931  [   32/   43]
train() client id: f_00003-6-0 loss: 0.556157  [   32/   43]
train() client id: f_00003-7-0 loss: 0.991700  [   32/   43]
train() client id: f_00003-8-0 loss: 0.680326  [   32/   43]
train() client id: f_00004-0-0 loss: 0.944579  [   32/  306]
train() client id: f_00004-0-1 loss: 0.870434  [   64/  306]
train() client id: f_00004-0-2 loss: 0.982205  [   96/  306]
train() client id: f_00004-0-3 loss: 0.882740  [  128/  306]
train() client id: f_00004-0-4 loss: 1.063783  [  160/  306]
train() client id: f_00004-0-5 loss: 0.890225  [  192/  306]
train() client id: f_00004-0-6 loss: 0.866792  [  224/  306]
train() client id: f_00004-0-7 loss: 1.024492  [  256/  306]
train() client id: f_00004-0-8 loss: 0.997610  [  288/  306]
train() client id: f_00004-1-0 loss: 1.028870  [   32/  306]
train() client id: f_00004-1-1 loss: 0.897007  [   64/  306]
train() client id: f_00004-1-2 loss: 0.877858  [   96/  306]
train() client id: f_00004-1-3 loss: 0.912926  [  128/  306]
train() client id: f_00004-1-4 loss: 1.006186  [  160/  306]
train() client id: f_00004-1-5 loss: 0.934563  [  192/  306]
train() client id: f_00004-1-6 loss: 0.886126  [  224/  306]
train() client id: f_00004-1-7 loss: 1.037376  [  256/  306]
train() client id: f_00004-1-8 loss: 0.959733  [  288/  306]
train() client id: f_00004-2-0 loss: 0.930446  [   32/  306]
train() client id: f_00004-2-1 loss: 0.960887  [   64/  306]
train() client id: f_00004-2-2 loss: 1.009760  [   96/  306]
train() client id: f_00004-2-3 loss: 0.853744  [  128/  306]
train() client id: f_00004-2-4 loss: 0.964615  [  160/  306]
train() client id: f_00004-2-5 loss: 0.863758  [  192/  306]
train() client id: f_00004-2-6 loss: 0.991925  [  224/  306]
train() client id: f_00004-2-7 loss: 0.904986  [  256/  306]
train() client id: f_00004-2-8 loss: 0.944621  [  288/  306]
train() client id: f_00004-3-0 loss: 0.814248  [   32/  306]
train() client id: f_00004-3-1 loss: 1.038468  [   64/  306]
train() client id: f_00004-3-2 loss: 0.718570  [   96/  306]
train() client id: f_00004-3-3 loss: 0.908172  [  128/  306]
train() client id: f_00004-3-4 loss: 1.085439  [  160/  306]
train() client id: f_00004-3-5 loss: 0.979968  [  192/  306]
train() client id: f_00004-3-6 loss: 0.996908  [  224/  306]
train() client id: f_00004-3-7 loss: 0.835379  [  256/  306]
train() client id: f_00004-3-8 loss: 0.983117  [  288/  306]
train() client id: f_00004-4-0 loss: 0.867914  [   32/  306]
train() client id: f_00004-4-1 loss: 0.869161  [   64/  306]
train() client id: f_00004-4-2 loss: 1.034104  [   96/  306]
train() client id: f_00004-4-3 loss: 1.045305  [  128/  306]
train() client id: f_00004-4-4 loss: 0.937122  [  160/  306]
train() client id: f_00004-4-5 loss: 0.932323  [  192/  306]
train() client id: f_00004-4-6 loss: 0.851486  [  224/  306]
train() client id: f_00004-4-7 loss: 0.850961  [  256/  306]
train() client id: f_00004-4-8 loss: 0.976509  [  288/  306]
train() client id: f_00004-5-0 loss: 0.960510  [   32/  306]
train() client id: f_00004-5-1 loss: 0.965809  [   64/  306]
train() client id: f_00004-5-2 loss: 0.881711  [   96/  306]
train() client id: f_00004-5-3 loss: 1.038528  [  128/  306]
train() client id: f_00004-5-4 loss: 0.891719  [  160/  306]
train() client id: f_00004-5-5 loss: 0.893781  [  192/  306]
train() client id: f_00004-5-6 loss: 0.943696  [  224/  306]
train() client id: f_00004-5-7 loss: 0.882491  [  256/  306]
train() client id: f_00004-5-8 loss: 0.892227  [  288/  306]
train() client id: f_00004-6-0 loss: 0.881413  [   32/  306]
train() client id: f_00004-6-1 loss: 0.983780  [   64/  306]
train() client id: f_00004-6-2 loss: 0.959967  [   96/  306]
train() client id: f_00004-6-3 loss: 0.953114  [  128/  306]
train() client id: f_00004-6-4 loss: 0.839465  [  160/  306]
train() client id: f_00004-6-5 loss: 1.061152  [  192/  306]
train() client id: f_00004-6-6 loss: 0.816534  [  224/  306]
train() client id: f_00004-6-7 loss: 0.884186  [  256/  306]
train() client id: f_00004-6-8 loss: 0.973529  [  288/  306]
train() client id: f_00004-7-0 loss: 0.890718  [   32/  306]
train() client id: f_00004-7-1 loss: 1.009100  [   64/  306]
train() client id: f_00004-7-2 loss: 0.882058  [   96/  306]
train() client id: f_00004-7-3 loss: 0.910175  [  128/  306]
train() client id: f_00004-7-4 loss: 0.841165  [  160/  306]
train() client id: f_00004-7-5 loss: 0.921773  [  192/  306]
train() client id: f_00004-7-6 loss: 1.075674  [  224/  306]
train() client id: f_00004-7-7 loss: 0.948285  [  256/  306]
train() client id: f_00004-7-8 loss: 0.847694  [  288/  306]
train() client id: f_00004-8-0 loss: 0.980809  [   32/  306]
train() client id: f_00004-8-1 loss: 0.891591  [   64/  306]
train() client id: f_00004-8-2 loss: 0.898920  [   96/  306]
train() client id: f_00004-8-3 loss: 0.990700  [  128/  306]
train() client id: f_00004-8-4 loss: 1.016469  [  160/  306]
train() client id: f_00004-8-5 loss: 0.886309  [  192/  306]
train() client id: f_00004-8-6 loss: 0.782516  [  224/  306]
train() client id: f_00004-8-7 loss: 0.949397  [  256/  306]
train() client id: f_00004-8-8 loss: 0.899268  [  288/  306]
train() client id: f_00005-0-0 loss: 0.848678  [   32/  146]
train() client id: f_00005-0-1 loss: 1.076502  [   64/  146]
train() client id: f_00005-0-2 loss: 0.955299  [   96/  146]
train() client id: f_00005-0-3 loss: 0.626046  [  128/  146]
train() client id: f_00005-1-0 loss: 0.838837  [   32/  146]
train() client id: f_00005-1-1 loss: 0.821238  [   64/  146]
train() client id: f_00005-1-2 loss: 0.804906  [   96/  146]
train() client id: f_00005-1-3 loss: 1.033928  [  128/  146]
train() client id: f_00005-2-0 loss: 0.941728  [   32/  146]
train() client id: f_00005-2-1 loss: 0.969353  [   64/  146]
train() client id: f_00005-2-2 loss: 0.874714  [   96/  146]
train() client id: f_00005-2-3 loss: 0.896776  [  128/  146]
train() client id: f_00005-3-0 loss: 0.965369  [   32/  146]
train() client id: f_00005-3-1 loss: 0.966726  [   64/  146]
train() client id: f_00005-3-2 loss: 0.775427  [   96/  146]
train() client id: f_00005-3-3 loss: 0.651638  [  128/  146]
train() client id: f_00005-4-0 loss: 1.014099  [   32/  146]
train() client id: f_00005-4-1 loss: 0.689904  [   64/  146]
train() client id: f_00005-4-2 loss: 0.806547  [   96/  146]
train() client id: f_00005-4-3 loss: 0.919288  [  128/  146]
train() client id: f_00005-5-0 loss: 0.769099  [   32/  146]
train() client id: f_00005-5-1 loss: 1.127710  [   64/  146]
train() client id: f_00005-5-2 loss: 0.930082  [   96/  146]
train() client id: f_00005-5-3 loss: 0.851414  [  128/  146]
train() client id: f_00005-6-0 loss: 0.684673  [   32/  146]
train() client id: f_00005-6-1 loss: 0.729145  [   64/  146]
train() client id: f_00005-6-2 loss: 1.105753  [   96/  146]
train() client id: f_00005-6-3 loss: 1.145255  [  128/  146]
train() client id: f_00005-7-0 loss: 0.771925  [   32/  146]
train() client id: f_00005-7-1 loss: 0.900697  [   64/  146]
train() client id: f_00005-7-2 loss: 1.064162  [   96/  146]
train() client id: f_00005-7-3 loss: 0.763030  [  128/  146]
train() client id: f_00005-8-0 loss: 0.778025  [   32/  146]
train() client id: f_00005-8-1 loss: 0.812757  [   64/  146]
train() client id: f_00005-8-2 loss: 0.872273  [   96/  146]
train() client id: f_00005-8-3 loss: 1.065593  [  128/  146]
train() client id: f_00006-0-0 loss: 0.505851  [   32/   54]
train() client id: f_00006-1-0 loss: 0.527017  [   32/   54]
train() client id: f_00006-2-0 loss: 0.540070  [   32/   54]
train() client id: f_00006-3-0 loss: 0.436940  [   32/   54]
train() client id: f_00006-4-0 loss: 0.478139  [   32/   54]
train() client id: f_00006-5-0 loss: 0.563534  [   32/   54]
train() client id: f_00006-6-0 loss: 0.525854  [   32/   54]
train() client id: f_00006-7-0 loss: 0.552633  [   32/   54]
train() client id: f_00006-8-0 loss: 0.432095  [   32/   54]
train() client id: f_00007-0-0 loss: 0.519931  [   32/  179]
train() client id: f_00007-0-1 loss: 0.903256  [   64/  179]
train() client id: f_00007-0-2 loss: 0.575017  [   96/  179]
train() client id: f_00007-0-3 loss: 0.829459  [  128/  179]
train() client id: f_00007-0-4 loss: 0.487520  [  160/  179]
train() client id: f_00007-1-0 loss: 0.482611  [   32/  179]
train() client id: f_00007-1-1 loss: 0.779427  [   64/  179]
train() client id: f_00007-1-2 loss: 0.543549  [   96/  179]
train() client id: f_00007-1-3 loss: 0.610586  [  128/  179]
train() client id: f_00007-1-4 loss: 0.569833  [  160/  179]
train() client id: f_00007-2-0 loss: 0.562696  [   32/  179]
train() client id: f_00007-2-1 loss: 0.642514  [   64/  179]
train() client id: f_00007-2-2 loss: 0.718598  [   96/  179]
train() client id: f_00007-2-3 loss: 0.546530  [  128/  179]
train() client id: f_00007-2-4 loss: 0.646405  [  160/  179]
train() client id: f_00007-3-0 loss: 0.419126  [   32/  179]
train() client id: f_00007-3-1 loss: 0.801786  [   64/  179]
train() client id: f_00007-3-2 loss: 0.663645  [   96/  179]
train() client id: f_00007-3-3 loss: 0.476289  [  128/  179]
train() client id: f_00007-3-4 loss: 0.725268  [  160/  179]
train() client id: f_00007-4-0 loss: 0.706483  [   32/  179]
train() client id: f_00007-4-1 loss: 0.570840  [   64/  179]
train() client id: f_00007-4-2 loss: 0.576032  [   96/  179]
train() client id: f_00007-4-3 loss: 0.651845  [  128/  179]
train() client id: f_00007-4-4 loss: 0.585321  [  160/  179]
train() client id: f_00007-5-0 loss: 0.573198  [   32/  179]
train() client id: f_00007-5-1 loss: 0.586536  [   64/  179]
train() client id: f_00007-5-2 loss: 0.657464  [   96/  179]
train() client id: f_00007-5-3 loss: 0.654638  [  128/  179]
train() client id: f_00007-5-4 loss: 0.546357  [  160/  179]
train() client id: f_00007-6-0 loss: 0.691429  [   32/  179]
train() client id: f_00007-6-1 loss: 0.534651  [   64/  179]
train() client id: f_00007-6-2 loss: 0.735982  [   96/  179]
train() client id: f_00007-6-3 loss: 0.451978  [  128/  179]
train() client id: f_00007-6-4 loss: 0.486641  [  160/  179]
train() client id: f_00007-7-0 loss: 0.550396  [   32/  179]
train() client id: f_00007-7-1 loss: 0.453326  [   64/  179]
train() client id: f_00007-7-2 loss: 0.529689  [   96/  179]
train() client id: f_00007-7-3 loss: 0.704882  [  128/  179]
train() client id: f_00007-7-4 loss: 0.727625  [  160/  179]
train() client id: f_00007-8-0 loss: 0.546802  [   32/  179]
train() client id: f_00007-8-1 loss: 0.625393  [   64/  179]
train() client id: f_00007-8-2 loss: 0.658482  [   96/  179]
train() client id: f_00007-8-3 loss: 0.635471  [  128/  179]
train() client id: f_00007-8-4 loss: 0.558540  [  160/  179]
train() client id: f_00008-0-0 loss: 0.727152  [   32/  130]
train() client id: f_00008-0-1 loss: 0.640512  [   64/  130]
train() client id: f_00008-0-2 loss: 0.729359  [   96/  130]
train() client id: f_00008-0-3 loss: 0.708286  [  128/  130]
train() client id: f_00008-1-0 loss: 0.588339  [   32/  130]
train() client id: f_00008-1-1 loss: 0.787871  [   64/  130]
train() client id: f_00008-1-2 loss: 0.756682  [   96/  130]
train() client id: f_00008-1-3 loss: 0.667409  [  128/  130]
train() client id: f_00008-2-0 loss: 0.655555  [   32/  130]
train() client id: f_00008-2-1 loss: 0.789928  [   64/  130]
train() client id: f_00008-2-2 loss: 0.698095  [   96/  130]
train() client id: f_00008-2-3 loss: 0.657361  [  128/  130]
train() client id: f_00008-3-0 loss: 0.743047  [   32/  130]
train() client id: f_00008-3-1 loss: 0.657857  [   64/  130]
train() client id: f_00008-3-2 loss: 0.769774  [   96/  130]
train() client id: f_00008-3-3 loss: 0.603908  [  128/  130]
train() client id: f_00008-4-0 loss: 0.732980  [   32/  130]
train() client id: f_00008-4-1 loss: 0.761590  [   64/  130]
train() client id: f_00008-4-2 loss: 0.562733  [   96/  130]
train() client id: f_00008-4-3 loss: 0.744631  [  128/  130]
train() client id: f_00008-5-0 loss: 0.584428  [   32/  130]
train() client id: f_00008-5-1 loss: 0.880596  [   64/  130]
train() client id: f_00008-5-2 loss: 0.634254  [   96/  130]
train() client id: f_00008-5-3 loss: 0.712733  [  128/  130]
train() client id: f_00008-6-0 loss: 0.796074  [   32/  130]
train() client id: f_00008-6-1 loss: 0.742983  [   64/  130]
train() client id: f_00008-6-2 loss: 0.659132  [   96/  130]
train() client id: f_00008-6-3 loss: 0.598846  [  128/  130]
train() client id: f_00008-7-0 loss: 0.712313  [   32/  130]
train() client id: f_00008-7-1 loss: 0.736455  [   64/  130]
train() client id: f_00008-7-2 loss: 0.664680  [   96/  130]
train() client id: f_00008-7-3 loss: 0.686669  [  128/  130]
train() client id: f_00008-8-0 loss: 0.752606  [   32/  130]
train() client id: f_00008-8-1 loss: 0.599173  [   64/  130]
train() client id: f_00008-8-2 loss: 0.622579  [   96/  130]
train() client id: f_00008-8-3 loss: 0.799710  [  128/  130]
train() client id: f_00009-0-0 loss: 0.942954  [   32/  118]
train() client id: f_00009-0-1 loss: 1.062369  [   64/  118]
train() client id: f_00009-0-2 loss: 0.978150  [   96/  118]
train() client id: f_00009-1-0 loss: 0.996832  [   32/  118]
train() client id: f_00009-1-1 loss: 0.835593  [   64/  118]
train() client id: f_00009-1-2 loss: 0.996823  [   96/  118]
train() client id: f_00009-2-0 loss: 0.839762  [   32/  118]
train() client id: f_00009-2-1 loss: 0.950283  [   64/  118]
train() client id: f_00009-2-2 loss: 1.037795  [   96/  118]
train() client id: f_00009-3-0 loss: 0.915432  [   32/  118]
train() client id: f_00009-3-1 loss: 1.036276  [   64/  118]
train() client id: f_00009-3-2 loss: 0.724029  [   96/  118]
train() client id: f_00009-4-0 loss: 0.940323  [   32/  118]
train() client id: f_00009-4-1 loss: 0.762383  [   64/  118]
train() client id: f_00009-4-2 loss: 0.974710  [   96/  118]
train() client id: f_00009-5-0 loss: 0.957282  [   32/  118]
train() client id: f_00009-5-1 loss: 0.705282  [   64/  118]
train() client id: f_00009-5-2 loss: 0.900301  [   96/  118]
train() client id: f_00009-6-0 loss: 0.814416  [   32/  118]
train() client id: f_00009-6-1 loss: 0.768114  [   64/  118]
train() client id: f_00009-6-2 loss: 0.872604  [   96/  118]
train() client id: f_00009-7-0 loss: 0.728187  [   32/  118]
train() client id: f_00009-7-1 loss: 0.960335  [   64/  118]
train() client id: f_00009-7-2 loss: 0.830783  [   96/  118]
train() client id: f_00009-8-0 loss: 0.884843  [   32/  118]
train() client id: f_00009-8-1 loss: 0.915263  [   64/  118]
train() client id: f_00009-8-2 loss: 0.814082  [   96/  118]
At round 64 accuracy: 0.6472148541114059
At round 64 training accuracy: 0.5942320590207915
At round 64 training loss: 0.8214003949216447
update_location
xs = [  -3.9056584     4.20031788  340.00902392   18.81129433    0.97929623
    3.95640986 -302.44319194 -281.32485185  324.66397685 -267.06087855]
ys = [ 332.5879595   315.55583871    1.32061395 -302.45517586  294.35018685
  277.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [347.31830497 331.04853121 354.41202063 319.11282989 310.87455913
 295.29029028 318.55733376 298.57050846 340.16961628 285.19734345]
dists_bs = [232.60962728 227.74700376 543.50604767 515.29240511 212.63391028
 206.38833839 218.61473469 204.06574573 523.87842829 194.20530012]
uav_gains = [1.25202968e-12 1.57013546e-12 1.14585755e-12 1.89621938e-12
 2.18534457e-12 2.92930714e-12 1.91387845e-12 2.74742002e-12
 1.37723251e-12 3.59110754e-12]
bs_gains = [2.61057735e-11 2.76966092e-11 2.42505626e-12 2.81542271e-12
 3.35678104e-11 3.64901503e-11 3.10593012e-11 3.76649830e-11
 2.68811993e-12 4.32676259e-11]
Round 65
-------------------------------
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.41801623 4.82581662 2.38917526 0.89421619 5.56278746 2.67778749
 1.09198131 3.3275343  2.44520381 2.17140866]
obj_prev = 27.80392733965525
eta_min = 3.100306852884529e-39	eta_max = 0.942754072368237
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 6.3556060652170405	eta = 0.9090909090909091
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 16.15583264303265	eta = 0.3576308212219315
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 10.677811898063974	eta = 0.5411055889361986
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.733010612728588	eta = 0.5936317061131898
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673834452832171	eta = 0.597263031926322
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568210715082	eta = 0.5972794701806058
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568205281397	eta = 0.5972794705161003
eta = 0.5972794705161003
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.04252711 0.08944194 0.04185208 0.01451322 0.10328012 0.04927744
 0.0182259  0.0604155  0.04387718 0.03982698]
ene_total = [0.98560292 1.43449508 0.99341101 0.50602748 1.63267669 0.83712064
 0.55929955 1.13571028 0.89502304 0.69420151]
ti_comp = [1.57505947 1.73869087 1.56299493 1.62132055 1.74218994 1.74362015
 1.62219032 1.65177999 1.65020898 1.7463865 ]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.93768734e-06 1.47930885e-05 1.87549493e-06 7.26831216e-08
 2.26850246e-05 2.45991889e-06 1.43794955e-07 5.05150367e-06
 1.93873362e-06 1.29458708e-06]
ene_total = [0.36078585 0.12055314 0.37851121 0.29278761 0.11552795 0.1131294
 0.29151072 0.24810709 0.25036961 0.10904772]
optimize_network iter = 0 obj = 2.280330304694935
eta = 0.5972794705161003
freqs = [13500159.94622198 25721057.81080511 13388423.45249819  4475740.44373556
 29640890.63331935 14130785.41729783  5617683.65648641 18287997.60708139
 13294430.26761132 11402680.79036755]
eta_min = 0.5972794705161009	eta_max = 0.7502409697515174
af = 0.0005999271736932453	bf = 0.9180266431472464	zeta = 0.0006599198910625699	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.58346965e-07 2.73576559e-06 3.46845386e-07 1.34416814e-08
 4.19526387e-06 4.54926060e-07 2.65927760e-08 9.34201802e-07
 3.58540459e-07 2.39414968e-07]
ene_total = [1.59815174 0.53325669 1.67667707 1.29702355 0.51057679 0.50102436
 1.29136322 1.09882803 1.10901597 0.48300462]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 1 obj = 3.676253139763781
eta = 0.7502409697515174
freqs = [13418605.79030925 23811832.48313907 13388423.4524982   4351530.1809025
 27404358.81401067 13057508.37108007  5459605.33203652 17539930.50999551
 12759371.1105045  10525653.53083322]
eta_min = 0.7502409697515208	eta_max = 0.7502409697515149
af = 0.00052469220506583	bf = 0.9180266431472464	zeta = 0.000577161425572413	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.54030510e-07 2.34469776e-06 3.46845386e-07 1.27059696e-08
 3.58604780e-06 3.88444377e-07 2.51172226e-08 8.59338214e-07
 3.30260967e-07 2.04002507e-07]
ene_total = [1.59815146 0.53323124 1.67667707 1.2970235  0.51053714 0.50102003
 1.29136313 1.09882316 1.10901413 0.48300231]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 2 obj = 3.6762531397637437
eta = 0.7502409697515149
freqs = [13418605.79030923 23811832.48313908 13388423.45249818  4351530.1809025
 27404358.81401069 13057508.37108008  5459605.33203651 17539930.5099955
 12759371.11050449 10525653.53083322]
Done!
At round 65 energy consumption: 3.676253139763781
At round 65 eta: 0.7502409697515149
At round 65 local rounds: 9.409653576790653
At round 65 global rounds: 23.691326785509258
At round 65 a_n: 5.574576956279433
gradient difference: 0.5934411287307739
train() client id: f_00000-0-0 loss: 0.944871  [   32/  126]
train() client id: f_00000-0-1 loss: 1.155664  [   64/  126]
train() client id: f_00000-0-2 loss: 0.878851  [   96/  126]
train() client id: f_00000-1-0 loss: 0.957728  [   32/  126]
train() client id: f_00000-1-1 loss: 1.055003  [   64/  126]
train() client id: f_00000-1-2 loss: 0.954046  [   96/  126]
train() client id: f_00000-2-0 loss: 0.941547  [   32/  126]
train() client id: f_00000-2-1 loss: 0.867975  [   64/  126]
train() client id: f_00000-2-2 loss: 0.802942  [   96/  126]
train() client id: f_00000-3-0 loss: 0.844323  [   32/  126]
train() client id: f_00000-3-1 loss: 0.950035  [   64/  126]
train() client id: f_00000-3-2 loss: 0.966045  [   96/  126]
train() client id: f_00000-4-0 loss: 0.860257  [   32/  126]
train() client id: f_00000-4-1 loss: 0.913650  [   64/  126]
train() client id: f_00000-4-2 loss: 0.959816  [   96/  126]
train() client id: f_00000-5-0 loss: 0.942117  [   32/  126]
train() client id: f_00000-5-1 loss: 0.912044  [   64/  126]
train() client id: f_00000-5-2 loss: 0.749084  [   96/  126]
train() client id: f_00000-6-0 loss: 0.839148  [   32/  126]
train() client id: f_00000-6-1 loss: 0.700316  [   64/  126]
train() client id: f_00000-6-2 loss: 1.075725  [   96/  126]
train() client id: f_00000-7-0 loss: 0.865507  [   32/  126]
train() client id: f_00000-7-1 loss: 0.763400  [   64/  126]
train() client id: f_00000-7-2 loss: 0.902341  [   96/  126]
train() client id: f_00000-8-0 loss: 0.863146  [   32/  126]
train() client id: f_00000-8-1 loss: 0.817841  [   64/  126]
train() client id: f_00000-8-2 loss: 0.784581  [   96/  126]
train() client id: f_00001-0-0 loss: 0.640956  [   32/  265]
train() client id: f_00001-0-1 loss: 0.495439  [   64/  265]
train() client id: f_00001-0-2 loss: 0.506329  [   96/  265]
train() client id: f_00001-0-3 loss: 0.447963  [  128/  265]
train() client id: f_00001-0-4 loss: 0.606649  [  160/  265]
train() client id: f_00001-0-5 loss: 0.625397  [  192/  265]
train() client id: f_00001-0-6 loss: 0.606035  [  224/  265]
train() client id: f_00001-0-7 loss: 0.472588  [  256/  265]
train() client id: f_00001-1-0 loss: 0.438000  [   32/  265]
train() client id: f_00001-1-1 loss: 0.562629  [   64/  265]
train() client id: f_00001-1-2 loss: 0.571957  [   96/  265]
train() client id: f_00001-1-3 loss: 0.472394  [  128/  265]
train() client id: f_00001-1-4 loss: 0.480012  [  160/  265]
train() client id: f_00001-1-5 loss: 0.658877  [  192/  265]
train() client id: f_00001-1-6 loss: 0.686042  [  224/  265]
train() client id: f_00001-1-7 loss: 0.475328  [  256/  265]
train() client id: f_00001-2-0 loss: 0.453594  [   32/  265]
train() client id: f_00001-2-1 loss: 0.535753  [   64/  265]
train() client id: f_00001-2-2 loss: 0.570826  [   96/  265]
train() client id: f_00001-2-3 loss: 0.612745  [  128/  265]
train() client id: f_00001-2-4 loss: 0.544048  [  160/  265]
train() client id: f_00001-2-5 loss: 0.630614  [  192/  265]
train() client id: f_00001-2-6 loss: 0.452012  [  224/  265]
train() client id: f_00001-2-7 loss: 0.572603  [  256/  265]
train() client id: f_00001-3-0 loss: 0.808861  [   32/  265]
train() client id: f_00001-3-1 loss: 0.498815  [   64/  265]
train() client id: f_00001-3-2 loss: 0.458139  [   96/  265]
train() client id: f_00001-3-3 loss: 0.535775  [  128/  265]
train() client id: f_00001-3-4 loss: 0.473902  [  160/  265]
train() client id: f_00001-3-5 loss: 0.497106  [  192/  265]
train() client id: f_00001-3-6 loss: 0.446115  [  224/  265]
train() client id: f_00001-3-7 loss: 0.525863  [  256/  265]
train() client id: f_00001-4-0 loss: 0.574231  [   32/  265]
train() client id: f_00001-4-1 loss: 0.520520  [   64/  265]
train() client id: f_00001-4-2 loss: 0.574727  [   96/  265]
train() client id: f_00001-4-3 loss: 0.530138  [  128/  265]
train() client id: f_00001-4-4 loss: 0.591506  [  160/  265]
train() client id: f_00001-4-5 loss: 0.529813  [  192/  265]
train() client id: f_00001-4-6 loss: 0.504834  [  224/  265]
train() client id: f_00001-4-7 loss: 0.457152  [  256/  265]
train() client id: f_00001-5-0 loss: 0.529372  [   32/  265]
train() client id: f_00001-5-1 loss: 0.526363  [   64/  265]
train() client id: f_00001-5-2 loss: 0.607023  [   96/  265]
train() client id: f_00001-5-3 loss: 0.516987  [  128/  265]
train() client id: f_00001-5-4 loss: 0.518256  [  160/  265]
train() client id: f_00001-5-5 loss: 0.593054  [  192/  265]
train() client id: f_00001-5-6 loss: 0.480094  [  224/  265]
train() client id: f_00001-5-7 loss: 0.600621  [  256/  265]
train() client id: f_00001-6-0 loss: 0.590730  [   32/  265]
train() client id: f_00001-6-1 loss: 0.531137  [   64/  265]
train() client id: f_00001-6-2 loss: 0.506399  [   96/  265]
train() client id: f_00001-6-3 loss: 0.654218  [  128/  265]
train() client id: f_00001-6-4 loss: 0.452437  [  160/  265]
train() client id: f_00001-6-5 loss: 0.580877  [  192/  265]
train() client id: f_00001-6-6 loss: 0.508123  [  224/  265]
train() client id: f_00001-6-7 loss: 0.557799  [  256/  265]
train() client id: f_00001-7-0 loss: 0.615178  [   32/  265]
train() client id: f_00001-7-1 loss: 0.621844  [   64/  265]
train() client id: f_00001-7-2 loss: 0.602788  [   96/  265]
train() client id: f_00001-7-3 loss: 0.437637  [  128/  265]
train() client id: f_00001-7-4 loss: 0.455957  [  160/  265]
train() client id: f_00001-7-5 loss: 0.625008  [  192/  265]
train() client id: f_00001-7-6 loss: 0.589164  [  224/  265]
train() client id: f_00001-7-7 loss: 0.456046  [  256/  265]
train() client id: f_00001-8-0 loss: 0.469125  [   32/  265]
train() client id: f_00001-8-1 loss: 0.589700  [   64/  265]
train() client id: f_00001-8-2 loss: 0.573241  [   96/  265]
train() client id: f_00001-8-3 loss: 0.660881  [  128/  265]
train() client id: f_00001-8-4 loss: 0.469722  [  160/  265]
train() client id: f_00001-8-5 loss: 0.618338  [  192/  265]
train() client id: f_00001-8-6 loss: 0.538211  [  224/  265]
train() client id: f_00001-8-7 loss: 0.473095  [  256/  265]
train() client id: f_00002-0-0 loss: 1.064821  [   32/  124]
train() client id: f_00002-0-1 loss: 1.130817  [   64/  124]
train() client id: f_00002-0-2 loss: 0.943331  [   96/  124]
train() client id: f_00002-1-0 loss: 1.008026  [   32/  124]
train() client id: f_00002-1-1 loss: 1.027401  [   64/  124]
train() client id: f_00002-1-2 loss: 1.142285  [   96/  124]
train() client id: f_00002-2-0 loss: 1.171947  [   32/  124]
train() client id: f_00002-2-1 loss: 0.851272  [   64/  124]
train() client id: f_00002-2-2 loss: 1.016043  [   96/  124]
train() client id: f_00002-3-0 loss: 1.077796  [   32/  124]
train() client id: f_00002-3-1 loss: 1.094224  [   64/  124]
train() client id: f_00002-3-2 loss: 0.963510  [   96/  124]
train() client id: f_00002-4-0 loss: 1.000765  [   32/  124]
train() client id: f_00002-4-1 loss: 1.043566  [   64/  124]
train() client id: f_00002-4-2 loss: 0.997127  [   96/  124]
train() client id: f_00002-5-0 loss: 1.126823  [   32/  124]
train() client id: f_00002-5-1 loss: 1.005638  [   64/  124]
train() client id: f_00002-5-2 loss: 1.034731  [   96/  124]
train() client id: f_00002-6-0 loss: 0.908520  [   32/  124]
train() client id: f_00002-6-1 loss: 1.332865  [   64/  124]
train() client id: f_00002-6-2 loss: 0.879216  [   96/  124]
train() client id: f_00002-7-0 loss: 1.033779  [   32/  124]
train() client id: f_00002-7-1 loss: 0.981629  [   64/  124]
train() client id: f_00002-7-2 loss: 1.012789  [   96/  124]
train() client id: f_00002-8-0 loss: 1.038142  [   32/  124]
train() client id: f_00002-8-1 loss: 0.881840  [   64/  124]
train() client id: f_00002-8-2 loss: 1.053447  [   96/  124]
train() client id: f_00003-0-0 loss: 0.709357  [   32/   43]
train() client id: f_00003-1-0 loss: 0.822904  [   32/   43]
train() client id: f_00003-2-0 loss: 0.771855  [   32/   43]
train() client id: f_00003-3-0 loss: 0.575401  [   32/   43]
train() client id: f_00003-4-0 loss: 0.517482  [   32/   43]
train() client id: f_00003-5-0 loss: 0.678982  [   32/   43]
train() client id: f_00003-6-0 loss: 0.620965  [   32/   43]
train() client id: f_00003-7-0 loss: 0.591678  [   32/   43]
train() client id: f_00003-8-0 loss: 0.609691  [   32/   43]
train() client id: f_00004-0-0 loss: 0.762939  [   32/  306]
train() client id: f_00004-0-1 loss: 0.783220  [   64/  306]
train() client id: f_00004-0-2 loss: 0.865859  [   96/  306]
train() client id: f_00004-0-3 loss: 0.651563  [  128/  306]
train() client id: f_00004-0-4 loss: 0.839737  [  160/  306]
train() client id: f_00004-0-5 loss: 0.965546  [  192/  306]
train() client id: f_00004-0-6 loss: 0.996979  [  224/  306]
train() client id: f_00004-0-7 loss: 0.870941  [  256/  306]
train() client id: f_00004-0-8 loss: 0.854329  [  288/  306]
train() client id: f_00004-1-0 loss: 0.824964  [   32/  306]
train() client id: f_00004-1-1 loss: 0.751296  [   64/  306]
train() client id: f_00004-1-2 loss: 0.711084  [   96/  306]
train() client id: f_00004-1-3 loss: 0.850390  [  128/  306]
train() client id: f_00004-1-4 loss: 0.923209  [  160/  306]
train() client id: f_00004-1-5 loss: 0.922481  [  192/  306]
train() client id: f_00004-1-6 loss: 1.039806  [  224/  306]
train() client id: f_00004-1-7 loss: 0.813556  [  256/  306]
train() client id: f_00004-1-8 loss: 0.654130  [  288/  306]
train() client id: f_00004-2-0 loss: 0.935054  [   32/  306]
train() client id: f_00004-2-1 loss: 0.802043  [   64/  306]
train() client id: f_00004-2-2 loss: 0.804439  [   96/  306]
train() client id: f_00004-2-3 loss: 0.860118  [  128/  306]
train() client id: f_00004-2-4 loss: 0.771780  [  160/  306]
train() client id: f_00004-2-5 loss: 0.897804  [  192/  306]
train() client id: f_00004-2-6 loss: 0.903456  [  224/  306]
train() client id: f_00004-2-7 loss: 0.807752  [  256/  306]
train() client id: f_00004-2-8 loss: 0.793387  [  288/  306]
train() client id: f_00004-3-0 loss: 0.799235  [   32/  306]
train() client id: f_00004-3-1 loss: 0.686043  [   64/  306]
train() client id: f_00004-3-2 loss: 0.827265  [   96/  306]
train() client id: f_00004-3-3 loss: 0.887853  [  128/  306]
train() client id: f_00004-3-4 loss: 0.766742  [  160/  306]
train() client id: f_00004-3-5 loss: 0.899091  [  192/  306]
train() client id: f_00004-3-6 loss: 0.763804  [  224/  306]
train() client id: f_00004-3-7 loss: 0.883897  [  256/  306]
train() client id: f_00004-3-8 loss: 0.923269  [  288/  306]
train() client id: f_00004-4-0 loss: 0.875819  [   32/  306]
train() client id: f_00004-4-1 loss: 0.954111  [   64/  306]
train() client id: f_00004-4-2 loss: 0.793695  [   96/  306]
train() client id: f_00004-4-3 loss: 0.696941  [  128/  306]
train() client id: f_00004-4-4 loss: 0.769714  [  160/  306]
train() client id: f_00004-4-5 loss: 0.889600  [  192/  306]
train() client id: f_00004-4-6 loss: 0.972773  [  224/  306]
train() client id: f_00004-4-7 loss: 0.903022  [  256/  306]
train() client id: f_00004-4-8 loss: 0.687206  [  288/  306]
train() client id: f_00004-5-0 loss: 0.872683  [   32/  306]
train() client id: f_00004-5-1 loss: 0.950214  [   64/  306]
train() client id: f_00004-5-2 loss: 0.690709  [   96/  306]
train() client id: f_00004-5-3 loss: 0.799496  [  128/  306]
train() client id: f_00004-5-4 loss: 0.962246  [  160/  306]
train() client id: f_00004-5-5 loss: 0.777418  [  192/  306]
train() client id: f_00004-5-6 loss: 0.995747  [  224/  306]
train() client id: f_00004-5-7 loss: 0.747290  [  256/  306]
train() client id: f_00004-5-8 loss: 0.760757  [  288/  306]
train() client id: f_00004-6-0 loss: 0.782627  [   32/  306]
train() client id: f_00004-6-1 loss: 0.828016  [   64/  306]
train() client id: f_00004-6-2 loss: 0.818587  [   96/  306]
train() client id: f_00004-6-3 loss: 0.787041  [  128/  306]
train() client id: f_00004-6-4 loss: 0.918959  [  160/  306]
train() client id: f_00004-6-5 loss: 0.860394  [  192/  306]
train() client id: f_00004-6-6 loss: 0.861581  [  224/  306]
train() client id: f_00004-6-7 loss: 0.826654  [  256/  306]
train() client id: f_00004-6-8 loss: 0.802562  [  288/  306]
train() client id: f_00004-7-0 loss: 0.893588  [   32/  306]
train() client id: f_00004-7-1 loss: 0.847030  [   64/  306]
train() client id: f_00004-7-2 loss: 0.836609  [   96/  306]
train() client id: f_00004-7-3 loss: 0.878658  [  128/  306]
train() client id: f_00004-7-4 loss: 0.771889  [  160/  306]
train() client id: f_00004-7-5 loss: 0.804971  [  192/  306]
train() client id: f_00004-7-6 loss: 0.825410  [  224/  306]
train() client id: f_00004-7-7 loss: 0.850528  [  256/  306]
train() client id: f_00004-7-8 loss: 0.839064  [  288/  306]
train() client id: f_00004-8-0 loss: 0.973225  [   32/  306]
train() client id: f_00004-8-1 loss: 0.863280  [   64/  306]
train() client id: f_00004-8-2 loss: 0.830750  [   96/  306]
train() client id: f_00004-8-3 loss: 0.762227  [  128/  306]
train() client id: f_00004-8-4 loss: 0.755557  [  160/  306]
train() client id: f_00004-8-5 loss: 0.869263  [  192/  306]
train() client id: f_00004-8-6 loss: 0.740533  [  224/  306]
train() client id: f_00004-8-7 loss: 0.963730  [  256/  306]
train() client id: f_00004-8-8 loss: 0.744495  [  288/  306]
train() client id: f_00005-0-0 loss: 0.207357  [   32/  146]
train() client id: f_00005-0-1 loss: 0.281743  [   64/  146]
train() client id: f_00005-0-2 loss: 0.861201  [   96/  146]
train() client id: f_00005-0-3 loss: 0.370781  [  128/  146]
train() client id: f_00005-1-0 loss: 0.456203  [   32/  146]
train() client id: f_00005-1-1 loss: 0.485975  [   64/  146]
train() client id: f_00005-1-2 loss: 0.565131  [   96/  146]
train() client id: f_00005-1-3 loss: 0.381094  [  128/  146]
train() client id: f_00005-2-0 loss: 0.206896  [   32/  146]
train() client id: f_00005-2-1 loss: 0.813152  [   64/  146]
train() client id: f_00005-2-2 loss: 0.344394  [   96/  146]
train() client id: f_00005-2-3 loss: 0.501946  [  128/  146]
train() client id: f_00005-3-0 loss: 0.341281  [   32/  146]
train() client id: f_00005-3-1 loss: 0.269507  [   64/  146]
train() client id: f_00005-3-2 loss: 0.438080  [   96/  146]
train() client id: f_00005-3-3 loss: 0.494346  [  128/  146]
train() client id: f_00005-4-0 loss: 0.522519  [   32/  146]
train() client id: f_00005-4-1 loss: 0.276611  [   64/  146]
train() client id: f_00005-4-2 loss: 0.472185  [   96/  146]
train() client id: f_00005-4-3 loss: 0.426619  [  128/  146]
train() client id: f_00005-5-0 loss: 0.493998  [   32/  146]
train() client id: f_00005-5-1 loss: 0.194092  [   64/  146]
train() client id: f_00005-5-2 loss: 0.615542  [   96/  146]
train() client id: f_00005-5-3 loss: 0.385458  [  128/  146]
train() client id: f_00005-6-0 loss: 0.312084  [   32/  146]
train() client id: f_00005-6-1 loss: 0.633672  [   64/  146]
train() client id: f_00005-6-2 loss: 0.586956  [   96/  146]
train() client id: f_00005-6-3 loss: 0.413123  [  128/  146]
train() client id: f_00005-7-0 loss: 0.218977  [   32/  146]
train() client id: f_00005-7-1 loss: 0.454416  [   64/  146]
train() client id: f_00005-7-2 loss: 0.603664  [   96/  146]
train() client id: f_00005-7-3 loss: 0.556826  [  128/  146]
train() client id: f_00005-8-0 loss: 0.480288  [   32/  146]
train() client id: f_00005-8-1 loss: 0.318146  [   64/  146]
train() client id: f_00005-8-2 loss: 0.521255  [   96/  146]
train() client id: f_00005-8-3 loss: 0.295782  [  128/  146]
train() client id: f_00006-0-0 loss: 0.610936  [   32/   54]
train() client id: f_00006-1-0 loss: 0.553961  [   32/   54]
train() client id: f_00006-2-0 loss: 0.522189  [   32/   54]
train() client id: f_00006-3-0 loss: 0.556988  [   32/   54]
train() client id: f_00006-4-0 loss: 0.610970  [   32/   54]
train() client id: f_00006-5-0 loss: 0.550724  [   32/   54]
train() client id: f_00006-6-0 loss: 0.508082  [   32/   54]
train() client id: f_00006-7-0 loss: 0.584167  [   32/   54]
train() client id: f_00006-8-0 loss: 0.560755  [   32/   54]
train() client id: f_00007-0-0 loss: 0.687953  [   32/  179]
train() client id: f_00007-0-1 loss: 0.731719  [   64/  179]
train() client id: f_00007-0-2 loss: 0.738571  [   96/  179]
train() client id: f_00007-0-3 loss: 0.665434  [  128/  179]
train() client id: f_00007-0-4 loss: 0.502428  [  160/  179]
train() client id: f_00007-1-0 loss: 0.551695  [   32/  179]
train() client id: f_00007-1-1 loss: 0.558540  [   64/  179]
train() client id: f_00007-1-2 loss: 0.617672  [   96/  179]
train() client id: f_00007-1-3 loss: 0.719057  [  128/  179]
train() client id: f_00007-1-4 loss: 0.907753  [  160/  179]
train() client id: f_00007-2-0 loss: 0.640764  [   32/  179]
train() client id: f_00007-2-1 loss: 0.582812  [   64/  179]
train() client id: f_00007-2-2 loss: 0.733541  [   96/  179]
train() client id: f_00007-2-3 loss: 0.656352  [  128/  179]
train() client id: f_00007-2-4 loss: 0.529105  [  160/  179]
train() client id: f_00007-3-0 loss: 0.430604  [   32/  179]
train() client id: f_00007-3-1 loss: 0.578155  [   64/  179]
train() client id: f_00007-3-2 loss: 0.861523  [   96/  179]
train() client id: f_00007-3-3 loss: 0.618697  [  128/  179]
train() client id: f_00007-3-4 loss: 0.680017  [  160/  179]
train() client id: f_00007-4-0 loss: 0.757163  [   32/  179]
train() client id: f_00007-4-1 loss: 0.856648  [   64/  179]
train() client id: f_00007-4-2 loss: 0.543152  [   96/  179]
train() client id: f_00007-4-3 loss: 0.610184  [  128/  179]
train() client id: f_00007-4-4 loss: 0.468745  [  160/  179]
train() client id: f_00007-5-0 loss: 0.704536  [   32/  179]
train() client id: f_00007-5-1 loss: 0.698766  [   64/  179]
train() client id: f_00007-5-2 loss: 0.641107  [   96/  179]
train() client id: f_00007-5-3 loss: 0.595764  [  128/  179]
train() client id: f_00007-5-4 loss: 0.461615  [  160/  179]
train() client id: f_00007-6-0 loss: 0.647199  [   32/  179]
train() client id: f_00007-6-1 loss: 0.632512  [   64/  179]
train() client id: f_00007-6-2 loss: 0.520314  [   96/  179]
train() client id: f_00007-6-3 loss: 0.653523  [  128/  179]
train() client id: f_00007-6-4 loss: 0.827087  [  160/  179]
train() client id: f_00007-7-0 loss: 0.552974  [   32/  179]
train() client id: f_00007-7-1 loss: 0.602028  [   64/  179]
train() client id: f_00007-7-2 loss: 0.579093  [   96/  179]
train() client id: f_00007-7-3 loss: 0.780880  [  128/  179]
train() client id: f_00007-7-4 loss: 0.672325  [  160/  179]
train() client id: f_00007-8-0 loss: 0.608635  [   32/  179]
train() client id: f_00007-8-1 loss: 0.472021  [   64/  179]
train() client id: f_00007-8-2 loss: 0.704678  [   96/  179]
train() client id: f_00007-8-3 loss: 0.744730  [  128/  179]
train() client id: f_00007-8-4 loss: 0.514998  [  160/  179]
train() client id: f_00008-0-0 loss: 0.639579  [   32/  130]
train() client id: f_00008-0-1 loss: 0.564792  [   64/  130]
train() client id: f_00008-0-2 loss: 0.666392  [   96/  130]
train() client id: f_00008-0-3 loss: 0.649551  [  128/  130]
train() client id: f_00008-1-0 loss: 0.775772  [   32/  130]
train() client id: f_00008-1-1 loss: 0.622416  [   64/  130]
train() client id: f_00008-1-2 loss: 0.541685  [   96/  130]
train() client id: f_00008-1-3 loss: 0.591028  [  128/  130]
train() client id: f_00008-2-0 loss: 0.637921  [   32/  130]
train() client id: f_00008-2-1 loss: 0.484893  [   64/  130]
train() client id: f_00008-2-2 loss: 0.729944  [   96/  130]
train() client id: f_00008-2-3 loss: 0.659860  [  128/  130]
train() client id: f_00008-3-0 loss: 0.651321  [   32/  130]
train() client id: f_00008-3-1 loss: 0.685943  [   64/  130]
train() client id: f_00008-3-2 loss: 0.654776  [   96/  130]
train() client id: f_00008-3-3 loss: 0.557450  [  128/  130]
train() client id: f_00008-4-0 loss: 0.644792  [   32/  130]
train() client id: f_00008-4-1 loss: 0.691033  [   64/  130]
train() client id: f_00008-4-2 loss: 0.569262  [   96/  130]
train() client id: f_00008-4-3 loss: 0.613932  [  128/  130]
train() client id: f_00008-5-0 loss: 0.713283  [   32/  130]
train() client id: f_00008-5-1 loss: 0.679276  [   64/  130]
train() client id: f_00008-5-2 loss: 0.554511  [   96/  130]
train() client id: f_00008-5-3 loss: 0.578165  [  128/  130]
train() client id: f_00008-6-0 loss: 0.531911  [   32/  130]
train() client id: f_00008-6-1 loss: 0.736442  [   64/  130]
train() client id: f_00008-6-2 loss: 0.630510  [   96/  130]
train() client id: f_00008-6-3 loss: 0.609223  [  128/  130]
train() client id: f_00008-7-0 loss: 0.714803  [   32/  130]
train() client id: f_00008-7-1 loss: 0.620954  [   64/  130]
train() client id: f_00008-7-2 loss: 0.575618  [   96/  130]
train() client id: f_00008-7-3 loss: 0.637640  [  128/  130]
train() client id: f_00008-8-0 loss: 0.519509  [   32/  130]
train() client id: f_00008-8-1 loss: 0.748468  [   64/  130]
train() client id: f_00008-8-2 loss: 0.619826  [   96/  130]
train() client id: f_00008-8-3 loss: 0.668344  [  128/  130]
train() client id: f_00009-0-0 loss: 1.056401  [   32/  118]
train() client id: f_00009-0-1 loss: 1.175913  [   64/  118]
train() client id: f_00009-0-2 loss: 0.870504  [   96/  118]
train() client id: f_00009-1-0 loss: 0.879869  [   32/  118]
train() client id: f_00009-1-1 loss: 1.236926  [   64/  118]
train() client id: f_00009-1-2 loss: 0.870264  [   96/  118]
train() client id: f_00009-2-0 loss: 0.924030  [   32/  118]
train() client id: f_00009-2-1 loss: 1.053177  [   64/  118]
train() client id: f_00009-2-2 loss: 0.975365  [   96/  118]
train() client id: f_00009-3-0 loss: 0.893442  [   32/  118]
train() client id: f_00009-3-1 loss: 1.025612  [   64/  118]
train() client id: f_00009-3-2 loss: 0.882652  [   96/  118]
train() client id: f_00009-4-0 loss: 0.810256  [   32/  118]
train() client id: f_00009-4-1 loss: 0.977680  [   64/  118]
train() client id: f_00009-4-2 loss: 0.924796  [   96/  118]
train() client id: f_00009-5-0 loss: 0.917080  [   32/  118]
train() client id: f_00009-5-1 loss: 0.953404  [   64/  118]
train() client id: f_00009-5-2 loss: 0.856707  [   96/  118]
train() client id: f_00009-6-0 loss: 0.892346  [   32/  118]
train() client id: f_00009-6-1 loss: 0.954877  [   64/  118]
train() client id: f_00009-6-2 loss: 0.868112  [   96/  118]
train() client id: f_00009-7-0 loss: 0.942516  [   32/  118]
train() client id: f_00009-7-1 loss: 0.860135  [   64/  118]
train() client id: f_00009-7-2 loss: 0.817662  [   96/  118]
train() client id: f_00009-8-0 loss: 0.844771  [   32/  118]
train() client id: f_00009-8-1 loss: 0.886856  [   64/  118]
train() client id: f_00009-8-2 loss: 0.936939  [   96/  118]
At round 65 accuracy: 0.6472148541114059
At round 65 training accuracy: 0.590878604963112
At round 65 training loss: 0.8195743862735246
update_location
xs = [  -3.9056584     4.20031788  345.00902392   18.81129433    0.97929623
    3.95640986 -307.44319194 -286.32485185  329.66397685 -272.06087855]
ys = [ 337.5879595   320.55583871    1.32061395 -307.45517586  299.35018685
  282.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [352.10919409 335.81793937 359.21159587 323.85575486 315.61288533
 299.99916173 323.30822262 303.28632848 344.94493416 289.88469   ]
dists_bs = [236.02567297 230.86631653 548.24621667 519.92866292 215.48011896
 208.92411955 221.5667713  206.71496584 528.64945606 196.62478831]
uav_gains = [1.17856502e-12 1.46410888e-12 1.08255453e-12 1.75503525e-12
 2.01172458e-12 2.67278145e-12 1.77049497e-12 2.51097274e-12
 1.29138642e-12 3.26345136e-12]
bs_gains = [2.50615657e-11 2.66614938e-11 2.36680394e-12 2.74569035e-12
 3.23410344e-11 3.52635523e-11 2.99144570e-11 3.63289386e-11
 2.62074203e-12 4.17933260e-11]
Round 66
-------------------------------
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.28261697 4.54685792 2.2554939  0.84667744 5.24112296 2.52309512
 1.03299146 3.13864151 2.30469629 2.04601254]
obj_prev = 26.218206122007363
eta_min = 1.468265776943342e-41	eta_max = 0.944257265961695
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 5.987676154852869	eta = 0.9090909090909091
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 15.480452461148737	eta = 0.3516267998379439
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 10.14509351030873	eta = 0.5365492149900848
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.229709829328627	eta = 0.5897630651031099
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.172211188857027	eta = 0.5934601642807859
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195028532138	eta = 0.5934770457345999
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195027990423	eta = 0.5934770460851201
eta = 0.5934770460851201
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.04305412 0.09055032 0.04237072 0.01469307 0.10456    0.0498881
 0.01845176 0.06116418 0.04442091 0.04032052]
ene_total = [0.9382438  1.35500986 0.94553393 0.48503672 1.54222126 0.79045116
 0.53538055 1.0793304  0.84532808 0.65541452]
ti_comp = [1.69339463 1.86443235 1.68124436 1.74028295 1.86800619 1.86951147
 1.74115692 1.77160865 1.77482088 1.87231038]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [1.73943290e-06 1.33492469e-05 1.68196045e-06 6.54603068e-08
 2.04748185e-05 2.22031557e-06 1.29514340e-07 4.55654761e-06
 1.73913543e-06 1.16869917e-06]
ene_total = [0.34787422 0.11349245 0.36453489 0.28355414 0.10868941 0.10637493
 0.28235657 0.24065936 0.23621585 0.10252241]
optimize_network iter = 0 obj = 2.186274239551174
eta = 0.5934770460851201
freqs = [12712369.24332196 24283617.19415748 12600999.04046834  4221460.42224169
 27987058.04437446 13342550.5059045   5298707.90102926 17262328.31372014
 12514196.1057277  10767584.65528894]
eta_min = 0.5934770460851205	eta_max = 0.7588338965075888
af = 0.0005034964568992424	bf = 0.8885079592597985	zeta = 0.0005538461025891666	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.17745128e-07 2.43852935e-06 3.07246539e-07 1.19577441e-08
 3.74016947e-06 4.05588772e-07 2.36586018e-08 8.32352201e-07
 3.17690788e-07 2.13488239e-07]
ene_total = [1.55552023 0.5068411  1.6300255  1.26798066 0.48500598 0.47557109
 1.26262219 1.07594073 1.05621167 0.45839627]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 1 obj = 3.6847138773635306
eta = 0.7588338965075888
freqs = [12631650.06371988 22329634.8077465  12600999.04046835  4097654.84679903
 25698769.41757561 12244375.3475877   5141153.96101874 16512221.31849963
 11952921.87158274  9870499.99002256]
eta_min = 0.7588338965075906	eta_max = 0.7588338965075868
af = 0.00043529367837054833	bf = 0.8885079592597985	zeta = 0.0004788230462076032	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.13722794e-07 2.06188515e-06 3.07246539e-07 1.12666437e-08
 3.15356247e-06 3.41571386e-07 2.22725702e-08 7.61586768e-07
 2.89832359e-07 1.79397204e-07]
ene_total = [1.55551998 0.506818   1.6300255  1.26798062 0.48497001 0.47556716
 1.2626221  1.0759364  1.05620996 0.45839418]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 2 obj = 3.6847138773635004
eta = 0.7588338965075868
freqs = [12631650.06371987 22329634.80774652 12600999.04046834  4097654.84679903
 25698769.41757564 12244375.34758771  5141153.96101873 16512221.31849963
 11952921.87158274  9870499.99002257]
Done!
At round 66 energy consumption: 3.6847138773635306
At round 66 eta: 0.7588338965075868
At round 66 local rounds: 9.036737525084137
At round 66 global rounds: 23.115093189100694
At round 66 a_n: 5.2320311093101175
gradient difference: 0.49800434708595276
train() client id: f_00000-0-0 loss: 1.087326  [   32/  126]
train() client id: f_00000-0-1 loss: 1.316937  [   64/  126]
train() client id: f_00000-0-2 loss: 1.163503  [   96/  126]
train() client id: f_00000-1-0 loss: 1.071203  [   32/  126]
train() client id: f_00000-1-1 loss: 1.152214  [   64/  126]
train() client id: f_00000-1-2 loss: 1.086280  [   96/  126]
train() client id: f_00000-2-0 loss: 1.205410  [   32/  126]
train() client id: f_00000-2-1 loss: 0.877522  [   64/  126]
train() client id: f_00000-2-2 loss: 0.943452  [   96/  126]
train() client id: f_00000-3-0 loss: 1.128636  [   32/  126]
train() client id: f_00000-3-1 loss: 0.874461  [   64/  126]
train() client id: f_00000-3-2 loss: 0.948559  [   96/  126]
train() client id: f_00000-4-0 loss: 0.994338  [   32/  126]
train() client id: f_00000-4-1 loss: 0.948698  [   64/  126]
train() client id: f_00000-4-2 loss: 0.986923  [   96/  126]
train() client id: f_00000-5-0 loss: 1.028317  [   32/  126]
train() client id: f_00000-5-1 loss: 0.825621  [   64/  126]
train() client id: f_00000-5-2 loss: 0.829228  [   96/  126]
train() client id: f_00000-6-0 loss: 0.964863  [   32/  126]
train() client id: f_00000-6-1 loss: 0.846045  [   64/  126]
train() client id: f_00000-6-2 loss: 0.997458  [   96/  126]
train() client id: f_00000-7-0 loss: 0.946948  [   32/  126]
train() client id: f_00000-7-1 loss: 0.916299  [   64/  126]
train() client id: f_00000-7-2 loss: 1.022251  [   96/  126]
train() client id: f_00000-8-0 loss: 0.882346  [   32/  126]
train() client id: f_00000-8-1 loss: 0.939916  [   64/  126]
train() client id: f_00000-8-2 loss: 0.833588  [   96/  126]
train() client id: f_00001-0-0 loss: 0.604594  [   32/  265]
train() client id: f_00001-0-1 loss: 0.476438  [   64/  265]
train() client id: f_00001-0-2 loss: 0.402978  [   96/  265]
train() client id: f_00001-0-3 loss: 0.499203  [  128/  265]
train() client id: f_00001-0-4 loss: 0.497423  [  160/  265]
train() client id: f_00001-0-5 loss: 0.407266  [  192/  265]
train() client id: f_00001-0-6 loss: 0.466079  [  224/  265]
train() client id: f_00001-0-7 loss: 0.530204  [  256/  265]
train() client id: f_00001-1-0 loss: 0.481206  [   32/  265]
train() client id: f_00001-1-1 loss: 0.472801  [   64/  265]
train() client id: f_00001-1-2 loss: 0.461501  [   96/  265]
train() client id: f_00001-1-3 loss: 0.538121  [  128/  265]
train() client id: f_00001-1-4 loss: 0.514099  [  160/  265]
train() client id: f_00001-1-5 loss: 0.465092  [  192/  265]
train() client id: f_00001-1-6 loss: 0.459414  [  224/  265]
train() client id: f_00001-1-7 loss: 0.398687  [  256/  265]
train() client id: f_00001-2-0 loss: 0.437496  [   32/  265]
train() client id: f_00001-2-1 loss: 0.481254  [   64/  265]
train() client id: f_00001-2-2 loss: 0.452185  [   96/  265]
train() client id: f_00001-2-3 loss: 0.577745  [  128/  265]
train() client id: f_00001-2-4 loss: 0.462778  [  160/  265]
train() client id: f_00001-2-5 loss: 0.555349  [  192/  265]
train() client id: f_00001-2-6 loss: 0.430763  [  224/  265]
train() client id: f_00001-2-7 loss: 0.427892  [  256/  265]
train() client id: f_00001-3-0 loss: 0.661163  [   32/  265]
train() client id: f_00001-3-1 loss: 0.463182  [   64/  265]
train() client id: f_00001-3-2 loss: 0.374212  [   96/  265]
train() client id: f_00001-3-3 loss: 0.595595  [  128/  265]
train() client id: f_00001-3-4 loss: 0.478491  [  160/  265]
train() client id: f_00001-3-5 loss: 0.400232  [  192/  265]
train() client id: f_00001-3-6 loss: 0.444146  [  224/  265]
train() client id: f_00001-3-7 loss: 0.394777  [  256/  265]
train() client id: f_00001-4-0 loss: 0.449408  [   32/  265]
train() client id: f_00001-4-1 loss: 0.432324  [   64/  265]
train() client id: f_00001-4-2 loss: 0.390310  [   96/  265]
train() client id: f_00001-4-3 loss: 0.450665  [  128/  265]
train() client id: f_00001-4-4 loss: 0.557564  [  160/  265]
train() client id: f_00001-4-5 loss: 0.608427  [  192/  265]
train() client id: f_00001-4-6 loss: 0.398521  [  224/  265]
train() client id: f_00001-4-7 loss: 0.513451  [  256/  265]
train() client id: f_00001-5-0 loss: 0.532315  [   32/  265]
train() client id: f_00001-5-1 loss: 0.357909  [   64/  265]
train() client id: f_00001-5-2 loss: 0.526712  [   96/  265]
train() client id: f_00001-5-3 loss: 0.507436  [  128/  265]
train() client id: f_00001-5-4 loss: 0.380431  [  160/  265]
train() client id: f_00001-5-5 loss: 0.470607  [  192/  265]
train() client id: f_00001-5-6 loss: 0.490624  [  224/  265]
train() client id: f_00001-5-7 loss: 0.507443  [  256/  265]
train() client id: f_00001-6-0 loss: 0.496650  [   32/  265]
train() client id: f_00001-6-1 loss: 0.452309  [   64/  265]
train() client id: f_00001-6-2 loss: 0.529519  [   96/  265]
train() client id: f_00001-6-3 loss: 0.441916  [  128/  265]
train() client id: f_00001-6-4 loss: 0.461400  [  160/  265]
train() client id: f_00001-6-5 loss: 0.522880  [  192/  265]
train() client id: f_00001-6-6 loss: 0.451539  [  224/  265]
train() client id: f_00001-6-7 loss: 0.414210  [  256/  265]
train() client id: f_00001-7-0 loss: 0.623970  [   32/  265]
train() client id: f_00001-7-1 loss: 0.382922  [   64/  265]
train() client id: f_00001-7-2 loss: 0.540703  [   96/  265]
train() client id: f_00001-7-3 loss: 0.476593  [  128/  265]
train() client id: f_00001-7-4 loss: 0.456803  [  160/  265]
train() client id: f_00001-7-5 loss: 0.455671  [  192/  265]
train() client id: f_00001-7-6 loss: 0.390096  [  224/  265]
train() client id: f_00001-7-7 loss: 0.448869  [  256/  265]
train() client id: f_00001-8-0 loss: 0.537049  [   32/  265]
train() client id: f_00001-8-1 loss: 0.454286  [   64/  265]
train() client id: f_00001-8-2 loss: 0.504019  [   96/  265]
train() client id: f_00001-8-3 loss: 0.371986  [  128/  265]
train() client id: f_00001-8-4 loss: 0.723690  [  160/  265]
train() client id: f_00001-8-5 loss: 0.424812  [  192/  265]
train() client id: f_00001-8-6 loss: 0.393455  [  224/  265]
train() client id: f_00001-8-7 loss: 0.374142  [  256/  265]
train() client id: f_00002-0-0 loss: 1.253363  [   32/  124]
train() client id: f_00002-0-1 loss: 1.098699  [   64/  124]
train() client id: f_00002-0-2 loss: 0.832500  [   96/  124]
train() client id: f_00002-1-0 loss: 0.965375  [   32/  124]
train() client id: f_00002-1-1 loss: 0.964890  [   64/  124]
train() client id: f_00002-1-2 loss: 0.915688  [   96/  124]
train() client id: f_00002-2-0 loss: 1.065901  [   32/  124]
train() client id: f_00002-2-1 loss: 0.888483  [   64/  124]
train() client id: f_00002-2-2 loss: 1.046057  [   96/  124]
train() client id: f_00002-3-0 loss: 1.252448  [   32/  124]
train() client id: f_00002-3-1 loss: 0.944124  [   64/  124]
train() client id: f_00002-3-2 loss: 0.656594  [   96/  124]
train() client id: f_00002-4-0 loss: 1.002660  [   32/  124]
train() client id: f_00002-4-1 loss: 0.849070  [   64/  124]
train() client id: f_00002-4-2 loss: 0.861630  [   96/  124]
train() client id: f_00002-5-0 loss: 0.758251  [   32/  124]
train() client id: f_00002-5-1 loss: 0.940394  [   64/  124]
train() client id: f_00002-5-2 loss: 0.937279  [   96/  124]
train() client id: f_00002-6-0 loss: 0.978434  [   32/  124]
train() client id: f_00002-6-1 loss: 0.734311  [   64/  124]
train() client id: f_00002-6-2 loss: 0.921518  [   96/  124]
train() client id: f_00002-7-0 loss: 0.906629  [   32/  124]
train() client id: f_00002-7-1 loss: 0.862376  [   64/  124]
train() client id: f_00002-7-2 loss: 0.842966  [   96/  124]
train() client id: f_00002-8-0 loss: 1.083192  [   32/  124]
train() client id: f_00002-8-1 loss: 0.863837  [   64/  124]
train() client id: f_00002-8-2 loss: 0.805615  [   96/  124]
train() client id: f_00003-0-0 loss: 0.524772  [   32/   43]
train() client id: f_00003-1-0 loss: 0.681879  [   32/   43]
train() client id: f_00003-2-0 loss: 0.653626  [   32/   43]
train() client id: f_00003-3-0 loss: 0.577677  [   32/   43]
train() client id: f_00003-4-0 loss: 0.747047  [   32/   43]
train() client id: f_00003-5-0 loss: 0.652523  [   32/   43]
train() client id: f_00003-6-0 loss: 0.554468  [   32/   43]
train() client id: f_00003-7-0 loss: 0.507404  [   32/   43]
train() client id: f_00003-8-0 loss: 0.619744  [   32/   43]
train() client id: f_00004-0-0 loss: 0.968719  [   32/  306]
train() client id: f_00004-0-1 loss: 0.856625  [   64/  306]
train() client id: f_00004-0-2 loss: 0.822562  [   96/  306]
train() client id: f_00004-0-3 loss: 0.886339  [  128/  306]
train() client id: f_00004-0-4 loss: 0.874548  [  160/  306]
train() client id: f_00004-0-5 loss: 0.795565  [  192/  306]
train() client id: f_00004-0-6 loss: 0.777603  [  224/  306]
train() client id: f_00004-0-7 loss: 0.860238  [  256/  306]
train() client id: f_00004-0-8 loss: 1.101639  [  288/  306]
train() client id: f_00004-1-0 loss: 0.909189  [   32/  306]
train() client id: f_00004-1-1 loss: 0.921736  [   64/  306]
train() client id: f_00004-1-2 loss: 0.759898  [   96/  306]
train() client id: f_00004-1-3 loss: 0.785268  [  128/  306]
train() client id: f_00004-1-4 loss: 0.898572  [  160/  306]
train() client id: f_00004-1-5 loss: 0.900130  [  192/  306]
train() client id: f_00004-1-6 loss: 0.800489  [  224/  306]
train() client id: f_00004-1-7 loss: 0.868554  [  256/  306]
train() client id: f_00004-1-8 loss: 0.906751  [  288/  306]
train() client id: f_00004-2-0 loss: 0.847305  [   32/  306]
train() client id: f_00004-2-1 loss: 0.895749  [   64/  306]
train() client id: f_00004-2-2 loss: 0.809897  [   96/  306]
train() client id: f_00004-2-3 loss: 0.973824  [  128/  306]
train() client id: f_00004-2-4 loss: 0.755709  [  160/  306]
train() client id: f_00004-2-5 loss: 0.994164  [  192/  306]
train() client id: f_00004-2-6 loss: 0.874685  [  224/  306]
train() client id: f_00004-2-7 loss: 0.867855  [  256/  306]
train() client id: f_00004-2-8 loss: 0.902908  [  288/  306]
train() client id: f_00004-3-0 loss: 0.844824  [   32/  306]
train() client id: f_00004-3-1 loss: 0.775275  [   64/  306]
train() client id: f_00004-3-2 loss: 0.939025  [   96/  306]
train() client id: f_00004-3-3 loss: 0.840720  [  128/  306]
train() client id: f_00004-3-4 loss: 1.016696  [  160/  306]
train() client id: f_00004-3-5 loss: 0.787910  [  192/  306]
train() client id: f_00004-3-6 loss: 0.854339  [  224/  306]
train() client id: f_00004-3-7 loss: 0.834208  [  256/  306]
train() client id: f_00004-3-8 loss: 0.894189  [  288/  306]
train() client id: f_00004-4-0 loss: 0.858944  [   32/  306]
train() client id: f_00004-4-1 loss: 0.862028  [   64/  306]
train() client id: f_00004-4-2 loss: 0.890995  [   96/  306]
train() client id: f_00004-4-3 loss: 0.800481  [  128/  306]
train() client id: f_00004-4-4 loss: 0.751234  [  160/  306]
train() client id: f_00004-4-5 loss: 0.751204  [  192/  306]
train() client id: f_00004-4-6 loss: 0.842162  [  224/  306]
train() client id: f_00004-4-7 loss: 0.954918  [  256/  306]
train() client id: f_00004-4-8 loss: 1.100286  [  288/  306]
train() client id: f_00004-5-0 loss: 0.967710  [   32/  306]
train() client id: f_00004-5-1 loss: 0.781097  [   64/  306]
train() client id: f_00004-5-2 loss: 1.040395  [   96/  306]
train() client id: f_00004-5-3 loss: 0.912858  [  128/  306]
train() client id: f_00004-5-4 loss: 0.804447  [  160/  306]
train() client id: f_00004-5-5 loss: 0.849426  [  192/  306]
train() client id: f_00004-5-6 loss: 0.803659  [  224/  306]
train() client id: f_00004-5-7 loss: 0.886591  [  256/  306]
train() client id: f_00004-5-8 loss: 0.796394  [  288/  306]
train() client id: f_00004-6-0 loss: 0.886135  [   32/  306]
train() client id: f_00004-6-1 loss: 0.846638  [   64/  306]
train() client id: f_00004-6-2 loss: 0.875483  [   96/  306]
train() client id: f_00004-6-3 loss: 0.809558  [  128/  306]
train() client id: f_00004-6-4 loss: 0.867611  [  160/  306]
train() client id: f_00004-6-5 loss: 1.055329  [  192/  306]
train() client id: f_00004-6-6 loss: 0.877197  [  224/  306]
train() client id: f_00004-6-7 loss: 0.902526  [  256/  306]
train() client id: f_00004-6-8 loss: 0.780229  [  288/  306]
train() client id: f_00004-7-0 loss: 0.856845  [   32/  306]
train() client id: f_00004-7-1 loss: 0.779084  [   64/  306]
train() client id: f_00004-7-2 loss: 0.911645  [   96/  306]
train() client id: f_00004-7-3 loss: 0.864230  [  128/  306]
train() client id: f_00004-7-4 loss: 0.908766  [  160/  306]
train() client id: f_00004-7-5 loss: 0.818401  [  192/  306]
train() client id: f_00004-7-6 loss: 1.099443  [  224/  306]
train() client id: f_00004-7-7 loss: 0.860336  [  256/  306]
train() client id: f_00004-7-8 loss: 0.822453  [  288/  306]
train() client id: f_00004-8-0 loss: 0.914091  [   32/  306]
train() client id: f_00004-8-1 loss: 0.750359  [   64/  306]
train() client id: f_00004-8-2 loss: 0.953540  [   96/  306]
train() client id: f_00004-8-3 loss: 0.882898  [  128/  306]
train() client id: f_00004-8-4 loss: 0.871149  [  160/  306]
train() client id: f_00004-8-5 loss: 0.850118  [  192/  306]
train() client id: f_00004-8-6 loss: 0.819513  [  224/  306]
train() client id: f_00004-8-7 loss: 0.982987  [  256/  306]
train() client id: f_00004-8-8 loss: 0.862739  [  288/  306]
train() client id: f_00005-0-0 loss: 1.254160  [   32/  146]
train() client id: f_00005-0-1 loss: 0.660480  [   64/  146]
train() client id: f_00005-0-2 loss: 0.472241  [   96/  146]
train() client id: f_00005-0-3 loss: 0.617623  [  128/  146]
train() client id: f_00005-1-0 loss: 0.682415  [   32/  146]
train() client id: f_00005-1-1 loss: 0.823843  [   64/  146]
train() client id: f_00005-1-2 loss: 0.676852  [   96/  146]
train() client id: f_00005-1-3 loss: 0.876026  [  128/  146]
train() client id: f_00005-2-0 loss: 0.598545  [   32/  146]
train() client id: f_00005-2-1 loss: 1.025522  [   64/  146]
train() client id: f_00005-2-2 loss: 0.754979  [   96/  146]
train() client id: f_00005-2-3 loss: 0.740967  [  128/  146]
train() client id: f_00005-3-0 loss: 0.663147  [   32/  146]
train() client id: f_00005-3-1 loss: 0.567702  [   64/  146]
train() client id: f_00005-3-2 loss: 0.698549  [   96/  146]
train() client id: f_00005-3-3 loss: 0.919730  [  128/  146]
train() client id: f_00005-4-0 loss: 0.722781  [   32/  146]
train() client id: f_00005-4-1 loss: 0.739336  [   64/  146]
train() client id: f_00005-4-2 loss: 0.736284  [   96/  146]
train() client id: f_00005-4-3 loss: 0.696073  [  128/  146]
train() client id: f_00005-5-0 loss: 0.740788  [   32/  146]
train() client id: f_00005-5-1 loss: 0.816661  [   64/  146]
train() client id: f_00005-5-2 loss: 0.813472  [   96/  146]
train() client id: f_00005-5-3 loss: 0.792213  [  128/  146]
train() client id: f_00005-6-0 loss: 0.814972  [   32/  146]
train() client id: f_00005-6-1 loss: 0.798369  [   64/  146]
train() client id: f_00005-6-2 loss: 0.732889  [   96/  146]
train() client id: f_00005-6-3 loss: 0.741537  [  128/  146]
train() client id: f_00005-7-0 loss: 0.498684  [   32/  146]
train() client id: f_00005-7-1 loss: 1.034201  [   64/  146]
train() client id: f_00005-7-2 loss: 0.827680  [   96/  146]
train() client id: f_00005-7-3 loss: 0.728214  [  128/  146]
train() client id: f_00005-8-0 loss: 0.891715  [   32/  146]
train() client id: f_00005-8-1 loss: 0.647784  [   64/  146]
train() client id: f_00005-8-2 loss: 0.917569  [   96/  146]
train() client id: f_00005-8-3 loss: 0.689189  [  128/  146]
train() client id: f_00006-0-0 loss: 0.464725  [   32/   54]
train() client id: f_00006-1-0 loss: 0.536520  [   32/   54]
train() client id: f_00006-2-0 loss: 0.531708  [   32/   54]
train() client id: f_00006-3-0 loss: 0.449842  [   32/   54]
train() client id: f_00006-4-0 loss: 0.466416  [   32/   54]
train() client id: f_00006-5-0 loss: 0.529154  [   32/   54]
train() client id: f_00006-6-0 loss: 0.530039  [   32/   54]
train() client id: f_00006-7-0 loss: 0.510175  [   32/   54]
train() client id: f_00006-8-0 loss: 0.449272  [   32/   54]
train() client id: f_00007-0-0 loss: 0.550795  [   32/  179]
train() client id: f_00007-0-1 loss: 0.573535  [   64/  179]
train() client id: f_00007-0-2 loss: 0.645576  [   96/  179]
train() client id: f_00007-0-3 loss: 1.033613  [  128/  179]
train() client id: f_00007-0-4 loss: 0.650438  [  160/  179]
train() client id: f_00007-1-0 loss: 0.719275  [   32/  179]
train() client id: f_00007-1-1 loss: 0.832022  [   64/  179]
train() client id: f_00007-1-2 loss: 0.525892  [   96/  179]
train() client id: f_00007-1-3 loss: 0.560225  [  128/  179]
train() client id: f_00007-1-4 loss: 0.585538  [  160/  179]
train() client id: f_00007-2-0 loss: 0.666225  [   32/  179]
train() client id: f_00007-2-1 loss: 0.718729  [   64/  179]
train() client id: f_00007-2-2 loss: 0.603101  [   96/  179]
train() client id: f_00007-2-3 loss: 0.718146  [  128/  179]
train() client id: f_00007-2-4 loss: 0.705935  [  160/  179]
train() client id: f_00007-3-0 loss: 0.573364  [   32/  179]
train() client id: f_00007-3-1 loss: 0.861048  [   64/  179]
train() client id: f_00007-3-2 loss: 0.578207  [   96/  179]
train() client id: f_00007-3-3 loss: 0.657136  [  128/  179]
train() client id: f_00007-3-4 loss: 0.642168  [  160/  179]
train() client id: f_00007-4-0 loss: 0.559711  [   32/  179]
train() client id: f_00007-4-1 loss: 0.792940  [   64/  179]
train() client id: f_00007-4-2 loss: 0.775468  [   96/  179]
train() client id: f_00007-4-3 loss: 0.586726  [  128/  179]
train() client id: f_00007-4-4 loss: 0.575222  [  160/  179]
train() client id: f_00007-5-0 loss: 0.581055  [   32/  179]
train() client id: f_00007-5-1 loss: 0.724494  [   64/  179]
train() client id: f_00007-5-2 loss: 0.603610  [   96/  179]
train() client id: f_00007-5-3 loss: 0.587857  [  128/  179]
train() client id: f_00007-5-4 loss: 0.583924  [  160/  179]
train() client id: f_00007-6-0 loss: 0.709793  [   32/  179]
train() client id: f_00007-6-1 loss: 0.596377  [   64/  179]
train() client id: f_00007-6-2 loss: 0.604195  [   96/  179]
train() client id: f_00007-6-3 loss: 0.568469  [  128/  179]
train() client id: f_00007-6-4 loss: 0.652795  [  160/  179]
train() client id: f_00007-7-0 loss: 0.783583  [   32/  179]
train() client id: f_00007-7-1 loss: 0.602624  [   64/  179]
train() client id: f_00007-7-2 loss: 0.715923  [   96/  179]
train() client id: f_00007-7-3 loss: 0.560214  [  128/  179]
train() client id: f_00007-7-4 loss: 0.500352  [  160/  179]
train() client id: f_00007-8-0 loss: 0.641536  [   32/  179]
train() client id: f_00007-8-1 loss: 0.582496  [   64/  179]
train() client id: f_00007-8-2 loss: 0.446765  [   96/  179]
train() client id: f_00007-8-3 loss: 0.697542  [  128/  179]
train() client id: f_00007-8-4 loss: 0.747925  [  160/  179]
train() client id: f_00008-0-0 loss: 0.644028  [   32/  130]
train() client id: f_00008-0-1 loss: 0.758055  [   64/  130]
train() client id: f_00008-0-2 loss: 0.627270  [   96/  130]
train() client id: f_00008-0-3 loss: 0.719129  [  128/  130]
train() client id: f_00008-1-0 loss: 0.795604  [   32/  130]
train() client id: f_00008-1-1 loss: 0.563115  [   64/  130]
train() client id: f_00008-1-2 loss: 0.746234  [   96/  130]
train() client id: f_00008-1-3 loss: 0.631874  [  128/  130]
train() client id: f_00008-2-0 loss: 0.609358  [   32/  130]
train() client id: f_00008-2-1 loss: 0.723293  [   64/  130]
train() client id: f_00008-2-2 loss: 0.657566  [   96/  130]
train() client id: f_00008-2-3 loss: 0.755084  [  128/  130]
train() client id: f_00008-3-0 loss: 0.661446  [   32/  130]
train() client id: f_00008-3-1 loss: 0.702899  [   64/  130]
train() client id: f_00008-3-2 loss: 0.689942  [   96/  130]
train() client id: f_00008-3-3 loss: 0.679423  [  128/  130]
train() client id: f_00008-4-0 loss: 0.667309  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671966  [   64/  130]
train() client id: f_00008-4-2 loss: 0.806145  [   96/  130]
train() client id: f_00008-4-3 loss: 0.611548  [  128/  130]
train() client id: f_00008-5-0 loss: 0.829764  [   32/  130]
train() client id: f_00008-5-1 loss: 0.660364  [   64/  130]
train() client id: f_00008-5-2 loss: 0.576819  [   96/  130]
train() client id: f_00008-5-3 loss: 0.681692  [  128/  130]
train() client id: f_00008-6-0 loss: 0.732130  [   32/  130]
train() client id: f_00008-6-1 loss: 0.616637  [   64/  130]
train() client id: f_00008-6-2 loss: 0.662163  [   96/  130]
train() client id: f_00008-6-3 loss: 0.689008  [  128/  130]
train() client id: f_00008-7-0 loss: 0.764603  [   32/  130]
train() client id: f_00008-7-1 loss: 0.680813  [   64/  130]
train() client id: f_00008-7-2 loss: 0.563839  [   96/  130]
train() client id: f_00008-7-3 loss: 0.708816  [  128/  130]
train() client id: f_00008-8-0 loss: 0.645831  [   32/  130]
train() client id: f_00008-8-1 loss: 0.735738  [   64/  130]
train() client id: f_00008-8-2 loss: 0.724376  [   96/  130]
train() client id: f_00008-8-3 loss: 0.649425  [  128/  130]
train() client id: f_00009-0-0 loss: 0.814645  [   32/  118]
train() client id: f_00009-0-1 loss: 1.121605  [   64/  118]
train() client id: f_00009-0-2 loss: 0.891935  [   96/  118]
train() client id: f_00009-1-0 loss: 1.014325  [   32/  118]
train() client id: f_00009-1-1 loss: 1.020966  [   64/  118]
train() client id: f_00009-1-2 loss: 0.672785  [   96/  118]
train() client id: f_00009-2-0 loss: 0.725105  [   32/  118]
train() client id: f_00009-2-1 loss: 0.998742  [   64/  118]
train() client id: f_00009-2-2 loss: 0.760822  [   96/  118]
train() client id: f_00009-3-0 loss: 0.888396  [   32/  118]
train() client id: f_00009-3-1 loss: 0.917728  [   64/  118]
train() client id: f_00009-3-2 loss: 0.725443  [   96/  118]
train() client id: f_00009-4-0 loss: 0.840352  [   32/  118]
train() client id: f_00009-4-1 loss: 0.676842  [   64/  118]
train() client id: f_00009-4-2 loss: 0.958943  [   96/  118]
train() client id: f_00009-5-0 loss: 0.737177  [   32/  118]
train() client id: f_00009-5-1 loss: 0.724227  [   64/  118]
train() client id: f_00009-5-2 loss: 0.716207  [   96/  118]
train() client id: f_00009-6-0 loss: 0.890167  [   32/  118]
train() client id: f_00009-6-1 loss: 0.746167  [   64/  118]
train() client id: f_00009-6-2 loss: 0.645436  [   96/  118]
train() client id: f_00009-7-0 loss: 0.793591  [   32/  118]
train() client id: f_00009-7-1 loss: 0.933399  [   64/  118]
train() client id: f_00009-7-2 loss: 0.563565  [   96/  118]
train() client id: f_00009-8-0 loss: 0.874239  [   32/  118]
train() client id: f_00009-8-1 loss: 0.720947  [   64/  118]
train() client id: f_00009-8-2 loss: 0.666881  [   96/  118]
At round 66 accuracy: 0.6472148541114059
At round 66 training accuracy: 0.5922199865861838
At round 66 training loss: 0.8189788727468288
update_location
xs = [  -3.9056584     4.20031788  350.00902392   18.81129433    0.97929623
    3.95640986 -312.44319194 -291.32485185  334.66397685 -277.06087855]
ys = [ 342.5879595   325.55583871    1.32061395 -312.45517586  304.35018685
  287.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [356.90581973 340.59396176 364.01656672 328.606302   320.35916602
 304.71730923 328.06651571 308.01111272 349.72653227 294.58231834]
dists_bs = [239.49738599 234.05088014 552.99096225 524.57160263 218.40371686
 211.54769968 224.59133123 209.45005519 533.42467806 199.14044332]
uav_gains = [1.11218118e-12 1.36922033e-12 1.02508951e-12 1.62931179e-12
 1.85748367e-12 2.44465100e-12 1.64287124e-12 2.30088160e-12
 1.21414591e-12 2.97056458e-12]
bs_gains = [2.40575783e-11 2.56581480e-11 2.31038093e-12 2.67818585e-12
 3.11433994e-11 3.40526430e-11 2.88000814e-11 3.50161782e-11
 2.55557949e-12 4.03317968e-11]
Round 67
-------------------------------
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.14668611 4.26785824 2.12126966 0.79869584 4.91942362 2.36837405
 0.97356032 2.94941557 2.16407019 1.92059068]
obj_prev = 24.629944275071566
eta_min = 3.445448803028318e-44	eta_max = 0.9459678027745403
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 5.6197462444887005	eta = 0.9090909090909091
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 14.777404826319295	eta = 0.34572107093955495
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 9.602497695143875	eta = 0.5320345168992939
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.719456153128892	eta = 0.585915008062652
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663852506476184	eta = 0.5896753457475884
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663598265580141	eta = 0.5896926502882287
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.66359826021894	eta = 0.5896926506531418
eta = 0.5896926506531418
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.04358199 0.09166053 0.04289021 0.01487322 0.10584197 0.05049976
 0.01867799 0.06191409 0.04496554 0.04081488]
ene_total = [0.88965002 1.27512756 0.89641868 0.46309031 1.45131187 0.74360323
 0.51049018 1.02200221 0.79540655 0.61649765]
ti_comp = [1.82806619 2.00655166 1.81584028 1.87549598 2.01019846 2.01177704
 1.87637173 1.90759375 1.91580954 2.01460717]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [1.54816501e-06 1.19543613e-05 1.49554405e-06 5.84605015e-08
 1.83390085e-05 1.98878720e-06 1.15673257e-07 4.07639963e-06
 1.54815642e-06 1.04701937e-06]
ene_total = [0.33393439 0.10647147 0.34952357 0.27343533 0.10190267 0.09968125
 0.27231935 0.23255711 0.22204852 0.09606041]
optimize_network iter = 0 obj = 2.0879340736516
eta = 0.5896926506531418
freqs = [11920243.00940963 22840310.78845544 11810016.9072595   3965142.48672103
 26326248.17150466 12551033.48678927  4977157.22105609 16228322.10506473
 11735389.30408466 10129735.66511689]
eta_min = 0.589692650653142	eta_max = 0.7681327803953306
af = 0.00041776276143721885	bf = 0.8564740523773019	zeta = 0.00045953903758094077	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.79380519e-07 2.15727370e-06 2.69884586e-07 1.05497316e-08
 3.30944163e-06 3.58894819e-07 2.08742959e-08 7.35623554e-07
 2.79378970e-07 1.88944211e-07]
ene_total = [1.50710005 0.47998245 1.57746246 1.23411502 0.45906057 0.44980564
 1.22907549 1.04942654 1.00211652 0.43350782]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 1 obj = 3.694218537284266
eta = 0.7681327803953306
freqs = [11840881.82931954 20853986.15252445 11810016.9072595   3842652.168879
 24000712.97449805 11434929.97175277  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340382]
eta_min = 0.7681327803953335	eta_max = 0.7681327803953281
af = 0.0003568274848523184	bf = 0.8564740523773019	zeta = 0.00039251023333755023	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.75672850e-07 1.79837142e-06 2.69884586e-07 9.90799918e-09
 2.75058500e-06 2.97903243e-07 1.95873319e-08 6.69522327e-07
 2.52301484e-07 1.56472000e-07]
ene_total = [1.50709984 0.47996179 1.57746246 1.23411499 0.4590284  0.44980213
 1.22907542 1.04942273 1.00211496 0.43350595]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 2 obj = 3.6942185372842253
eta = 0.7681327803953281
freqs = [11840881.82931953 20853986.15252446 11810016.90725948  3842652.168879
 24000712.97449807 11434929.97175278  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340384]
Done!
At round 67 energy consumption: 3.694218537284266
At round 67 eta: 0.7681327803953281
At round 67 local rounds: 8.637912231455728
At round 67 global rounds: 22.564772710133866
At round 67 a_n: 4.8894852623407985
gradient difference: 0.5140773057937622
train() client id: f_00000-0-0 loss: 0.978645  [   32/  126]
train() client id: f_00000-0-1 loss: 1.005491  [   64/  126]
train() client id: f_00000-0-2 loss: 1.290265  [   96/  126]
train() client id: f_00000-1-0 loss: 1.109648  [   32/  126]
train() client id: f_00000-1-1 loss: 1.016379  [   64/  126]
train() client id: f_00000-1-2 loss: 1.018143  [   96/  126]
train() client id: f_00000-2-0 loss: 0.861252  [   32/  126]
train() client id: f_00000-2-1 loss: 1.152099  [   64/  126]
train() client id: f_00000-2-2 loss: 1.045821  [   96/  126]
train() client id: f_00000-3-0 loss: 0.884256  [   32/  126]
train() client id: f_00000-3-1 loss: 1.032569  [   64/  126]
train() client id: f_00000-3-2 loss: 0.947194  [   96/  126]
train() client id: f_00000-4-0 loss: 0.942103  [   32/  126]
train() client id: f_00000-4-1 loss: 0.914299  [   64/  126]
train() client id: f_00000-4-2 loss: 0.875020  [   96/  126]
train() client id: f_00000-5-0 loss: 0.833108  [   32/  126]
train() client id: f_00000-5-1 loss: 0.804341  [   64/  126]
train() client id: f_00000-5-2 loss: 0.844958  [   96/  126]
train() client id: f_00000-6-0 loss: 0.829471  [   32/  126]
train() client id: f_00000-6-1 loss: 1.003111  [   64/  126]
train() client id: f_00000-6-2 loss: 0.905588  [   96/  126]
train() client id: f_00000-7-0 loss: 0.901470  [   32/  126]
train() client id: f_00000-7-1 loss: 0.889345  [   64/  126]
train() client id: f_00000-7-2 loss: 0.943324  [   96/  126]
train() client id: f_00001-0-0 loss: 0.684359  [   32/  265]
train() client id: f_00001-0-1 loss: 0.587607  [   64/  265]
train() client id: f_00001-0-2 loss: 0.588803  [   96/  265]
train() client id: f_00001-0-3 loss: 0.575031  [  128/  265]
train() client id: f_00001-0-4 loss: 0.484346  [  160/  265]
train() client id: f_00001-0-5 loss: 0.447578  [  192/  265]
train() client id: f_00001-0-6 loss: 0.474868  [  224/  265]
train() client id: f_00001-0-7 loss: 0.428045  [  256/  265]
train() client id: f_00001-1-0 loss: 0.641646  [   32/  265]
train() client id: f_00001-1-1 loss: 0.486429  [   64/  265]
train() client id: f_00001-1-2 loss: 0.522967  [   96/  265]
train() client id: f_00001-1-3 loss: 0.552175  [  128/  265]
train() client id: f_00001-1-4 loss: 0.642687  [  160/  265]
train() client id: f_00001-1-5 loss: 0.454845  [  192/  265]
train() client id: f_00001-1-6 loss: 0.523589  [  224/  265]
train() client id: f_00001-1-7 loss: 0.450118  [  256/  265]
train() client id: f_00001-2-0 loss: 0.679729  [   32/  265]
train() client id: f_00001-2-1 loss: 0.533420  [   64/  265]
train() client id: f_00001-2-2 loss: 0.541619  [   96/  265]
train() client id: f_00001-2-3 loss: 0.496344  [  128/  265]
train() client id: f_00001-2-4 loss: 0.443444  [  160/  265]
train() client id: f_00001-2-5 loss: 0.487874  [  192/  265]
train() client id: f_00001-2-6 loss: 0.567288  [  224/  265]
train() client id: f_00001-2-7 loss: 0.425636  [  256/  265]
train() client id: f_00001-3-0 loss: 0.470866  [   32/  265]
train() client id: f_00001-3-1 loss: 0.513668  [   64/  265]
train() client id: f_00001-3-2 loss: 0.615457  [   96/  265]
train() client id: f_00001-3-3 loss: 0.409927  [  128/  265]
train() client id: f_00001-3-4 loss: 0.526164  [  160/  265]
train() client id: f_00001-3-5 loss: 0.497382  [  192/  265]
train() client id: f_00001-3-6 loss: 0.459743  [  224/  265]
train() client id: f_00001-3-7 loss: 0.744989  [  256/  265]
train() client id: f_00001-4-0 loss: 0.548717  [   32/  265]
train() client id: f_00001-4-1 loss: 0.735936  [   64/  265]
train() client id: f_00001-4-2 loss: 0.557086  [   96/  265]
train() client id: f_00001-4-3 loss: 0.489839  [  128/  265]
train() client id: f_00001-4-4 loss: 0.426004  [  160/  265]
train() client id: f_00001-4-5 loss: 0.475673  [  192/  265]
train() client id: f_00001-4-6 loss: 0.491435  [  224/  265]
train() client id: f_00001-4-7 loss: 0.440238  [  256/  265]
train() client id: f_00001-5-0 loss: 0.505752  [   32/  265]
train() client id: f_00001-5-1 loss: 0.666579  [   64/  265]
train() client id: f_00001-5-2 loss: 0.678703  [   96/  265]
train() client id: f_00001-5-3 loss: 0.500964  [  128/  265]
train() client id: f_00001-5-4 loss: 0.500419  [  160/  265]
train() client id: f_00001-5-5 loss: 0.434870  [  192/  265]
train() client id: f_00001-5-6 loss: 0.419297  [  224/  265]
train() client id: f_00001-5-7 loss: 0.506328  [  256/  265]
train() client id: f_00001-6-0 loss: 0.645559  [   32/  265]
train() client id: f_00001-6-1 loss: 0.478821  [   64/  265]
train() client id: f_00001-6-2 loss: 0.567504  [   96/  265]
train() client id: f_00001-6-3 loss: 0.577328  [  128/  265]
train() client id: f_00001-6-4 loss: 0.525156  [  160/  265]
train() client id: f_00001-6-5 loss: 0.470046  [  192/  265]
train() client id: f_00001-6-6 loss: 0.468370  [  224/  265]
train() client id: f_00001-6-7 loss: 0.481374  [  256/  265]
train() client id: f_00001-7-0 loss: 0.499136  [   32/  265]
train() client id: f_00001-7-1 loss: 0.481297  [   64/  265]
train() client id: f_00001-7-2 loss: 0.523212  [   96/  265]
train() client id: f_00001-7-3 loss: 0.602991  [  128/  265]
train() client id: f_00001-7-4 loss: 0.671172  [  160/  265]
train() client id: f_00001-7-5 loss: 0.472693  [  192/  265]
train() client id: f_00001-7-6 loss: 0.442791  [  224/  265]
train() client id: f_00001-7-7 loss: 0.496734  [  256/  265]
train() client id: f_00002-0-0 loss: 1.184380  [   32/  124]
train() client id: f_00002-0-1 loss: 1.247481  [   64/  124]
train() client id: f_00002-0-2 loss: 1.110211  [   96/  124]
train() client id: f_00002-1-0 loss: 0.967274  [   32/  124]
train() client id: f_00002-1-1 loss: 1.102750  [   64/  124]
train() client id: f_00002-1-2 loss: 1.241535  [   96/  124]
train() client id: f_00002-2-0 loss: 1.256780  [   32/  124]
train() client id: f_00002-2-1 loss: 1.165947  [   64/  124]
train() client id: f_00002-2-2 loss: 1.020162  [   96/  124]
train() client id: f_00002-3-0 loss: 1.032740  [   32/  124]
train() client id: f_00002-3-1 loss: 1.003162  [   64/  124]
train() client id: f_00002-3-2 loss: 1.186179  [   96/  124]
train() client id: f_00002-4-0 loss: 1.240418  [   32/  124]
train() client id: f_00002-4-1 loss: 0.938922  [   64/  124]
train() client id: f_00002-4-2 loss: 0.923256  [   96/  124]
train() client id: f_00002-5-0 loss: 1.154756  [   32/  124]
train() client id: f_00002-5-1 loss: 0.978116  [   64/  124]
train() client id: f_00002-5-2 loss: 1.012805  [   96/  124]
train() client id: f_00002-6-0 loss: 1.137933  [   32/  124]
train() client id: f_00002-6-1 loss: 0.914240  [   64/  124]
train() client id: f_00002-6-2 loss: 0.939961  [   96/  124]
train() client id: f_00002-7-0 loss: 1.167531  [   32/  124]
train() client id: f_00002-7-1 loss: 0.946275  [   64/  124]
train() client id: f_00002-7-2 loss: 1.010198  [   96/  124]
train() client id: f_00003-0-0 loss: 0.882793  [   32/   43]
train() client id: f_00003-1-0 loss: 0.541133  [   32/   43]
train() client id: f_00003-2-0 loss: 0.624170  [   32/   43]
train() client id: f_00003-3-0 loss: 0.709369  [   32/   43]
train() client id: f_00003-4-0 loss: 0.951564  [   32/   43]
train() client id: f_00003-5-0 loss: 0.575917  [   32/   43]
train() client id: f_00003-6-0 loss: 0.496840  [   32/   43]
train() client id: f_00003-7-0 loss: 0.559805  [   32/   43]
train() client id: f_00004-0-0 loss: 0.889559  [   32/  306]
train() client id: f_00004-0-1 loss: 0.737671  [   64/  306]
train() client id: f_00004-0-2 loss: 0.791145  [   96/  306]
train() client id: f_00004-0-3 loss: 0.677961  [  128/  306]
train() client id: f_00004-0-4 loss: 0.900665  [  160/  306]
train() client id: f_00004-0-5 loss: 0.867255  [  192/  306]
train() client id: f_00004-0-6 loss: 0.972926  [  224/  306]
train() client id: f_00004-0-7 loss: 0.734711  [  256/  306]
train() client id: f_00004-0-8 loss: 0.760793  [  288/  306]
train() client id: f_00004-1-0 loss: 0.773174  [   32/  306]
train() client id: f_00004-1-1 loss: 0.766420  [   64/  306]
train() client id: f_00004-1-2 loss: 0.874525  [   96/  306]
train() client id: f_00004-1-3 loss: 0.746331  [  128/  306]
train() client id: f_00004-1-4 loss: 0.807142  [  160/  306]
train() client id: f_00004-1-5 loss: 0.929121  [  192/  306]
train() client id: f_00004-1-6 loss: 0.716043  [  224/  306]
train() client id: f_00004-1-7 loss: 0.773348  [  256/  306]
train() client id: f_00004-1-8 loss: 0.837732  [  288/  306]
train() client id: f_00004-2-0 loss: 0.821023  [   32/  306]
train() client id: f_00004-2-1 loss: 0.868947  [   64/  306]
train() client id: f_00004-2-2 loss: 0.745538  [   96/  306]
train() client id: f_00004-2-3 loss: 0.707487  [  128/  306]
train() client id: f_00004-2-4 loss: 0.818388  [  160/  306]
train() client id: f_00004-2-5 loss: 0.784348  [  192/  306]
train() client id: f_00004-2-6 loss: 0.906310  [  224/  306]
train() client id: f_00004-2-7 loss: 0.782618  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939003  [  288/  306]
train() client id: f_00004-3-0 loss: 0.724893  [   32/  306]
train() client id: f_00004-3-1 loss: 0.923642  [   64/  306]
train() client id: f_00004-3-2 loss: 0.723174  [   96/  306]
train() client id: f_00004-3-3 loss: 0.820289  [  128/  306]
train() client id: f_00004-3-4 loss: 0.829627  [  160/  306]
train() client id: f_00004-3-5 loss: 0.808977  [  192/  306]
train() client id: f_00004-3-6 loss: 0.889181  [  224/  306]
train() client id: f_00004-3-7 loss: 0.865319  [  256/  306]
train() client id: f_00004-3-8 loss: 0.808637  [  288/  306]
train() client id: f_00004-4-0 loss: 0.916541  [   32/  306]
train() client id: f_00004-4-1 loss: 0.854259  [   64/  306]
train() client id: f_00004-4-2 loss: 0.790292  [   96/  306]
train() client id: f_00004-4-3 loss: 0.856996  [  128/  306]
train() client id: f_00004-4-4 loss: 0.731544  [  160/  306]
train() client id: f_00004-4-5 loss: 0.833764  [  192/  306]
train() client id: f_00004-4-6 loss: 0.704587  [  224/  306]
train() client id: f_00004-4-7 loss: 0.887297  [  256/  306]
train() client id: f_00004-4-8 loss: 0.834489  [  288/  306]
train() client id: f_00004-5-0 loss: 0.915209  [   32/  306]
train() client id: f_00004-5-1 loss: 0.967264  [   64/  306]
train() client id: f_00004-5-2 loss: 0.766799  [   96/  306]
train() client id: f_00004-5-3 loss: 0.754583  [  128/  306]
train() client id: f_00004-5-4 loss: 0.699655  [  160/  306]
train() client id: f_00004-5-5 loss: 0.802383  [  192/  306]
train() client id: f_00004-5-6 loss: 0.894644  [  224/  306]
train() client id: f_00004-5-7 loss: 0.829534  [  256/  306]
train() client id: f_00004-5-8 loss: 0.814649  [  288/  306]
train() client id: f_00004-6-0 loss: 0.777813  [   32/  306]
train() client id: f_00004-6-1 loss: 0.815035  [   64/  306]
train() client id: f_00004-6-2 loss: 0.800248  [   96/  306]
train() client id: f_00004-6-3 loss: 0.767512  [  128/  306]
train() client id: f_00004-6-4 loss: 0.888441  [  160/  306]
train() client id: f_00004-6-5 loss: 0.768624  [  192/  306]
train() client id: f_00004-6-6 loss: 0.943685  [  224/  306]
train() client id: f_00004-6-7 loss: 0.777781  [  256/  306]
train() client id: f_00004-6-8 loss: 0.871485  [  288/  306]
train() client id: f_00004-7-0 loss: 0.779271  [   32/  306]
train() client id: f_00004-7-1 loss: 0.777212  [   64/  306]
train() client id: f_00004-7-2 loss: 0.868465  [   96/  306]
train() client id: f_00004-7-3 loss: 0.837126  [  128/  306]
train() client id: f_00004-7-4 loss: 0.778557  [  160/  306]
train() client id: f_00004-7-5 loss: 1.008260  [  192/  306]
train() client id: f_00004-7-6 loss: 0.842963  [  224/  306]
train() client id: f_00004-7-7 loss: 0.766011  [  256/  306]
train() client id: f_00004-7-8 loss: 0.792149  [  288/  306]
train() client id: f_00005-0-0 loss: 0.459053  [   32/  146]
train() client id: f_00005-0-1 loss: 0.618459  [   64/  146]
train() client id: f_00005-0-2 loss: 0.743346  [   96/  146]
train() client id: f_00005-0-3 loss: 0.679548  [  128/  146]
train() client id: f_00005-1-0 loss: 0.798013  [   32/  146]
train() client id: f_00005-1-1 loss: 0.541787  [   64/  146]
train() client id: f_00005-1-2 loss: 0.587564  [   96/  146]
train() client id: f_00005-1-3 loss: 0.652676  [  128/  146]
train() client id: f_00005-2-0 loss: 0.728165  [   32/  146]
train() client id: f_00005-2-1 loss: 0.759954  [   64/  146]
train() client id: f_00005-2-2 loss: 0.531278  [   96/  146]
train() client id: f_00005-2-3 loss: 0.736996  [  128/  146]
train() client id: f_00005-3-0 loss: 0.595201  [   32/  146]
train() client id: f_00005-3-1 loss: 0.733779  [   64/  146]
train() client id: f_00005-3-2 loss: 0.615840  [   96/  146]
train() client id: f_00005-3-3 loss: 0.781090  [  128/  146]
train() client id: f_00005-4-0 loss: 0.778054  [   32/  146]
train() client id: f_00005-4-1 loss: 0.672113  [   64/  146]
train() client id: f_00005-4-2 loss: 0.847946  [   96/  146]
train() client id: f_00005-4-3 loss: 0.493380  [  128/  146]
train() client id: f_00005-5-0 loss: 0.998317  [   32/  146]
train() client id: f_00005-5-1 loss: 0.588626  [   64/  146]
train() client id: f_00005-5-2 loss: 0.567072  [   96/  146]
train() client id: f_00005-5-3 loss: 0.508355  [  128/  146]
train() client id: f_00005-6-0 loss: 1.048448  [   32/  146]
train() client id: f_00005-6-1 loss: 0.697345  [   64/  146]
train() client id: f_00005-6-2 loss: 0.628441  [   96/  146]
train() client id: f_00005-6-3 loss: 0.558641  [  128/  146]
train() client id: f_00005-7-0 loss: 0.445087  [   32/  146]
train() client id: f_00005-7-1 loss: 0.940023  [   64/  146]
train() client id: f_00005-7-2 loss: 0.824686  [   96/  146]
train() client id: f_00005-7-3 loss: 0.697588  [  128/  146]
train() client id: f_00006-0-0 loss: 0.512721  [   32/   54]
train() client id: f_00006-1-0 loss: 0.558258  [   32/   54]
train() client id: f_00006-2-0 loss: 0.529704  [   32/   54]
train() client id: f_00006-3-0 loss: 0.571928  [   32/   54]
train() client id: f_00006-4-0 loss: 0.532217  [   32/   54]
train() client id: f_00006-5-0 loss: 0.496401  [   32/   54]
train() client id: f_00006-6-0 loss: 0.461836  [   32/   54]
train() client id: f_00006-7-0 loss: 0.563414  [   32/   54]
train() client id: f_00007-0-0 loss: 0.430093  [   32/  179]
train() client id: f_00007-0-1 loss: 0.490594  [   64/  179]
train() client id: f_00007-0-2 loss: 0.583039  [   96/  179]
train() client id: f_00007-0-3 loss: 0.605306  [  128/  179]
train() client id: f_00007-0-4 loss: 0.444867  [  160/  179]
train() client id: f_00007-1-0 loss: 0.514106  [   32/  179]
train() client id: f_00007-1-1 loss: 0.414222  [   64/  179]
train() client id: f_00007-1-2 loss: 0.314501  [   96/  179]
train() client id: f_00007-1-3 loss: 0.626473  [  128/  179]
train() client id: f_00007-1-4 loss: 0.608479  [  160/  179]
train() client id: f_00007-2-0 loss: 0.568684  [   32/  179]
train() client id: f_00007-2-1 loss: 0.400699  [   64/  179]
train() client id: f_00007-2-2 loss: 0.538277  [   96/  179]
train() client id: f_00007-2-3 loss: 0.339255  [  128/  179]
train() client id: f_00007-2-4 loss: 0.454681  [  160/  179]
train() client id: f_00007-3-0 loss: 0.425823  [   32/  179]
train() client id: f_00007-3-1 loss: 0.457530  [   64/  179]
train() client id: f_00007-3-2 loss: 0.483683  [   96/  179]
train() client id: f_00007-3-3 loss: 0.406487  [  128/  179]
train() client id: f_00007-3-4 loss: 0.524991  [  160/  179]
train() client id: f_00007-4-0 loss: 0.534106  [   32/  179]
train() client id: f_00007-4-1 loss: 0.626159  [   64/  179]
train() client id: f_00007-4-2 loss: 0.454994  [   96/  179]
train() client id: f_00007-4-3 loss: 0.472451  [  128/  179]
train() client id: f_00007-4-4 loss: 0.278567  [  160/  179]
train() client id: f_00007-5-0 loss: 0.313043  [   32/  179]
train() client id: f_00007-5-1 loss: 0.536719  [   64/  179]
train() client id: f_00007-5-2 loss: 0.280131  [   96/  179]
train() client id: f_00007-5-3 loss: 0.590007  [  128/  179]
train() client id: f_00007-5-4 loss: 0.315135  [  160/  179]
train() client id: f_00007-6-0 loss: 0.763102  [   32/  179]
train() client id: f_00007-6-1 loss: 0.294820  [   64/  179]
train() client id: f_00007-6-2 loss: 0.476038  [   96/  179]
train() client id: f_00007-6-3 loss: 0.391164  [  128/  179]
train() client id: f_00007-6-4 loss: 0.340358  [  160/  179]
train() client id: f_00007-7-0 loss: 0.569854  [   32/  179]
train() client id: f_00007-7-1 loss: 0.580349  [   64/  179]
train() client id: f_00007-7-2 loss: 0.425739  [   96/  179]
train() client id: f_00007-7-3 loss: 0.399885  [  128/  179]
train() client id: f_00007-7-4 loss: 0.309377  [  160/  179]
train() client id: f_00008-0-0 loss: 0.810274  [   32/  130]
train() client id: f_00008-0-1 loss: 0.668311  [   64/  130]
train() client id: f_00008-0-2 loss: 0.676586  [   96/  130]
train() client id: f_00008-0-3 loss: 0.654644  [  128/  130]
train() client id: f_00008-1-0 loss: 0.703856  [   32/  130]
train() client id: f_00008-1-1 loss: 0.735399  [   64/  130]
train() client id: f_00008-1-2 loss: 0.743258  [   96/  130]
train() client id: f_00008-1-3 loss: 0.596251  [  128/  130]
train() client id: f_00008-2-0 loss: 0.693356  [   32/  130]
train() client id: f_00008-2-1 loss: 0.669043  [   64/  130]
train() client id: f_00008-2-2 loss: 0.734748  [   96/  130]
train() client id: f_00008-2-3 loss: 0.717726  [  128/  130]
train() client id: f_00008-3-0 loss: 0.692949  [   32/  130]
train() client id: f_00008-3-1 loss: 0.660930  [   64/  130]
train() client id: f_00008-3-2 loss: 0.707183  [   96/  130]
train() client id: f_00008-3-3 loss: 0.755156  [  128/  130]
train() client id: f_00008-4-0 loss: 0.812437  [   32/  130]
train() client id: f_00008-4-1 loss: 0.639708  [   64/  130]
train() client id: f_00008-4-2 loss: 0.630130  [   96/  130]
train() client id: f_00008-4-3 loss: 0.745773  [  128/  130]
train() client id: f_00008-5-0 loss: 0.618039  [   32/  130]
train() client id: f_00008-5-1 loss: 0.724364  [   64/  130]
train() client id: f_00008-5-2 loss: 0.663987  [   96/  130]
train() client id: f_00008-5-3 loss: 0.758967  [  128/  130]
train() client id: f_00008-6-0 loss: 0.773484  [   32/  130]
train() client id: f_00008-6-1 loss: 0.738864  [   64/  130]
train() client id: f_00008-6-2 loss: 0.528312  [   96/  130]
train() client id: f_00008-6-3 loss: 0.742223  [  128/  130]
train() client id: f_00008-7-0 loss: 0.777755  [   32/  130]
train() client id: f_00008-7-1 loss: 0.746411  [   64/  130]
train() client id: f_00008-7-2 loss: 0.554301  [   96/  130]
train() client id: f_00008-7-3 loss: 0.708870  [  128/  130]
train() client id: f_00009-0-0 loss: 1.145278  [   32/  118]
train() client id: f_00009-0-1 loss: 1.144664  [   64/  118]
train() client id: f_00009-0-2 loss: 1.041336  [   96/  118]
train() client id: f_00009-1-0 loss: 1.070643  [   32/  118]
train() client id: f_00009-1-1 loss: 0.766557  [   64/  118]
train() client id: f_00009-1-2 loss: 1.300021  [   96/  118]
train() client id: f_00009-2-0 loss: 1.086622  [   32/  118]
train() client id: f_00009-2-1 loss: 1.087305  [   64/  118]
train() client id: f_00009-2-2 loss: 0.947137  [   96/  118]
train() client id: f_00009-3-0 loss: 0.985390  [   32/  118]
train() client id: f_00009-3-1 loss: 1.198405  [   64/  118]
train() client id: f_00009-3-2 loss: 0.910676  [   96/  118]
train() client id: f_00009-4-0 loss: 1.099309  [   32/  118]
train() client id: f_00009-4-1 loss: 0.823242  [   64/  118]
train() client id: f_00009-4-2 loss: 1.004314  [   96/  118]
train() client id: f_00009-5-0 loss: 0.852328  [   32/  118]
train() client id: f_00009-5-1 loss: 1.078408  [   64/  118]
train() client id: f_00009-5-2 loss: 1.008117  [   96/  118]
train() client id: f_00009-6-0 loss: 0.857758  [   32/  118]
train() client id: f_00009-6-1 loss: 0.861965  [   64/  118]
train() client id: f_00009-6-2 loss: 1.122137  [   96/  118]
train() client id: f_00009-7-0 loss: 0.871828  [   32/  118]
train() client id: f_00009-7-1 loss: 0.974903  [   64/  118]
train() client id: f_00009-7-2 loss: 1.022305  [   96/  118]
At round 67 accuracy: 0.6472148541114059
At round 67 training accuracy: 0.5875251509054326
At round 67 training loss: 0.8320255411316184
update_location
xs = [  -3.9056584     4.20031788  355.00902392   18.81129433    0.97929623
    3.95640986 -317.44319194 -296.32485185  339.66397685 -282.06087855]
ys = [ 347.5879595   330.55583871    1.32061395 -317.45517586  309.35018685
  292.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [361.70795368 345.37632399 368.8267223  333.36414546 325.11305283
 309.44430849 332.83189548 312.74445491 354.51415648 299.28974434]
dists_bs = [243.02238063 237.29806759 557.74016761 529.22104837 221.40163822
 214.25585347 227.68552432 212.26769452 538.20398264 201.74866778]
uav_gains = [1.05200963e-12 1.28407742e-12 9.72759726e-13 1.51715294e-12
 1.72031827e-12 2.24199922e-12 1.52907249e-12 2.11432101e-12
 1.14444349e-12 2.70950730e-12]
bs_gains = [2.30932218e-11 2.46871139e-11 2.25571746e-12 2.61282415e-12
 2.99769719e-11 3.28611333e-11 2.77175544e-11 3.37302187e-11
 2.49254349e-12 3.88887688e-11]
Round 68
-------------------------------
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.01021389 3.98881553 1.98649518 0.75024676 4.59768714 2.21362175
 0.91366287 2.75982094 2.02332361 1.79514037]
obj_prev = 23.039028040351884
eta_min = nan	eta_max = nan
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 5.251816334124527	eta = 0.9090909090909091
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 14.045450908523552	eta = 0.3399234753417846
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 9.049757744866326	eta = 0.5275697560275713
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.202066092537866	eta = 0.5820946127112314
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148578542585753	eta = 0.5859155017793716
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332307464996	eta = 0.5859332076078638
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332302200442	eta = 0.5859332079864292
eta = 0.5859332079864292
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.04410974 0.09277049 0.04340959 0.01505332 0.10712366 0.05111129
 0.01890417 0.06266384 0.04551005 0.04130912]
ene_total = [0.83982608 1.19483484 0.84607581 0.44015542 1.35993302 0.69656538
 0.48459427 0.96365584 0.74525191 0.57743974]
ti_comp = [1.98249026 2.16845803 1.97019612 2.03038779 2.17217608 2.17382626
 2.03126319 2.06316582 2.07658364 2.17668634]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [1.36477616e-06 1.06122506e-05 1.31709789e-06 5.17153298e-08
 1.62834394e-05 1.76595985e-06 1.02334201e-07 3.61295660e-06
 1.36616489e-06 9.29878567e-07]
ene_total = [0.31897471 0.09948487 0.33349161 0.26239966 0.0951614  0.09304137
 0.26136655 0.22373596 0.20786507 0.08965418]
optimize_network iter = 0 obj = 1.9851753783046104
eta = 0.5859332079864292
freqs = [11124831.81396149 21390888.52316821 11016564.88463251  3707007.23591558
 24658143.18962516 11756065.47830876  4653305.14511856 15186331.26526243
 10957914.22550441  9488993.01846851]
eta_min = 0.5859332079864303	eta_max = 0.7781376768963073
af = 0.0003421552139497174	bf = 0.8218123015887785	zeta = 0.00037637073534468917	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.43339611e-07 1.89216446e-06 2.34838575e-07 9.22084412e-09
 2.90333750e-06 3.14870668e-07 1.82461896e-08 6.44190219e-07
 2.43587222e-07 1.65797363e-07]
ene_total = [1.45278485 0.4526582  1.5189076  1.19515856 0.43271528 0.42370069
 1.19045077 1.01889828 0.94670988 0.40830993]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 1 obj = 3.7044806936267416
eta = 0.7781376768963073
freqs = [11047352.84864209 19386314.72442644 11016564.8846325   3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166095
 10357782.55225607  8569597.11605592]
eta_min = 0.7781376768963095	eta_max = 0.7781376768963063
af = 0.0002885807068502459	bf = 0.8218123015887785	zeta = 0.0003174387775352705	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.39961934e-07 1.55414576e-06 2.34838575e-07 8.63230839e-09
 2.37709741e-06 2.57436357e-07 1.70659737e-08 5.83230415e-07
 2.17636778e-07 1.35225369e-07]
ene_total = [1.45278467 0.45264002 1.5189076  1.19515853 0.43268697 0.4236976
 1.19045071 1.018895   0.94670848 0.40830829]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 2 obj = 3.7044806936267247
eta = 0.7781376768963063
freqs = [11047352.84864208 19386314.72442644 11016564.88463249  3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166094
 10357782.55225607  8569597.11605592]
Done!
At round 68 energy consumption: 3.7044806936267416
At round 68 eta: 0.7781376768963063
At round 68 local rounds: 8.214162669625793
At round 68 global rounds: 22.038375844715002
At round 68 a_n: 4.546939415371483
gradient difference: 0.5891221761703491
train() client id: f_00000-0-0 loss: 0.622136  [   32/  126]
train() client id: f_00000-0-1 loss: 0.838429  [   64/  126]
train() client id: f_00000-0-2 loss: 0.899066  [   96/  126]
train() client id: f_00000-1-0 loss: 0.922489  [   32/  126]
train() client id: f_00000-1-1 loss: 0.847842  [   64/  126]
train() client id: f_00000-1-2 loss: 0.659419  [   96/  126]
train() client id: f_00000-2-0 loss: 0.919570  [   32/  126]
train() client id: f_00000-2-1 loss: 0.723577  [   64/  126]
train() client id: f_00000-2-2 loss: 0.736652  [   96/  126]
train() client id: f_00000-3-0 loss: 0.883821  [   32/  126]
train() client id: f_00000-3-1 loss: 0.880557  [   64/  126]
train() client id: f_00000-3-2 loss: 0.717686  [   96/  126]
train() client id: f_00000-4-0 loss: 0.685320  [   32/  126]
train() client id: f_00000-4-1 loss: 0.755396  [   64/  126]
train() client id: f_00000-4-2 loss: 0.847979  [   96/  126]
train() client id: f_00000-5-0 loss: 0.804286  [   32/  126]
train() client id: f_00000-5-1 loss: 0.860504  [   64/  126]
train() client id: f_00000-5-2 loss: 0.710319  [   96/  126]
train() client id: f_00000-6-0 loss: 0.797224  [   32/  126]
train() client id: f_00000-6-1 loss: 0.806305  [   64/  126]
train() client id: f_00000-6-2 loss: 0.809195  [   96/  126]
train() client id: f_00000-7-0 loss: 0.727058  [   32/  126]
train() client id: f_00000-7-1 loss: 0.883816  [   64/  126]
train() client id: f_00000-7-2 loss: 0.810320  [   96/  126]
train() client id: f_00001-0-0 loss: 0.477950  [   32/  265]
train() client id: f_00001-0-1 loss: 0.460315  [   64/  265]
train() client id: f_00001-0-2 loss: 0.553423  [   96/  265]
train() client id: f_00001-0-3 loss: 0.501671  [  128/  265]
train() client id: f_00001-0-4 loss: 0.534381  [  160/  265]
train() client id: f_00001-0-5 loss: 0.638326  [  192/  265]
train() client id: f_00001-0-6 loss: 0.480642  [  224/  265]
train() client id: f_00001-0-7 loss: 0.413390  [  256/  265]
train() client id: f_00001-1-0 loss: 0.491140  [   32/  265]
train() client id: f_00001-1-1 loss: 0.392919  [   64/  265]
train() client id: f_00001-1-2 loss: 0.536726  [   96/  265]
train() client id: f_00001-1-3 loss: 0.511237  [  128/  265]
train() client id: f_00001-1-4 loss: 0.559035  [  160/  265]
train() client id: f_00001-1-5 loss: 0.491864  [  192/  265]
train() client id: f_00001-1-6 loss: 0.583867  [  224/  265]
train() client id: f_00001-1-7 loss: 0.470398  [  256/  265]
train() client id: f_00001-2-0 loss: 0.440336  [   32/  265]
train() client id: f_00001-2-1 loss: 0.484441  [   64/  265]
train() client id: f_00001-2-2 loss: 0.469163  [   96/  265]
train() client id: f_00001-2-3 loss: 0.412761  [  128/  265]
train() client id: f_00001-2-4 loss: 0.565937  [  160/  265]
train() client id: f_00001-2-5 loss: 0.506611  [  192/  265]
train() client id: f_00001-2-6 loss: 0.578005  [  224/  265]
train() client id: f_00001-2-7 loss: 0.568966  [  256/  265]
train() client id: f_00001-3-0 loss: 0.397600  [   32/  265]
train() client id: f_00001-3-1 loss: 0.510545  [   64/  265]
train() client id: f_00001-3-2 loss: 0.538844  [   96/  265]
train() client id: f_00001-3-3 loss: 0.420462  [  128/  265]
train() client id: f_00001-3-4 loss: 0.444625  [  160/  265]
train() client id: f_00001-3-5 loss: 0.493764  [  192/  265]
train() client id: f_00001-3-6 loss: 0.584868  [  224/  265]
train() client id: f_00001-3-7 loss: 0.582936  [  256/  265]
train() client id: f_00001-4-0 loss: 0.489148  [   32/  265]
train() client id: f_00001-4-1 loss: 0.453267  [   64/  265]
train() client id: f_00001-4-2 loss: 0.470987  [   96/  265]
train() client id: f_00001-4-3 loss: 0.529517  [  128/  265]
train() client id: f_00001-4-4 loss: 0.524350  [  160/  265]
train() client id: f_00001-4-5 loss: 0.479970  [  192/  265]
train() client id: f_00001-4-6 loss: 0.511946  [  224/  265]
train() client id: f_00001-4-7 loss: 0.550079  [  256/  265]
train() client id: f_00001-5-0 loss: 0.501885  [   32/  265]
train() client id: f_00001-5-1 loss: 0.586770  [   64/  265]
train() client id: f_00001-5-2 loss: 0.380515  [   96/  265]
train() client id: f_00001-5-3 loss: 0.478081  [  128/  265]
train() client id: f_00001-5-4 loss: 0.487473  [  160/  265]
train() client id: f_00001-5-5 loss: 0.413924  [  192/  265]
train() client id: f_00001-5-6 loss: 0.565697  [  224/  265]
train() client id: f_00001-5-7 loss: 0.500583  [  256/  265]
train() client id: f_00001-6-0 loss: 0.538836  [   32/  265]
train() client id: f_00001-6-1 loss: 0.547025  [   64/  265]
train() client id: f_00001-6-2 loss: 0.445895  [   96/  265]
train() client id: f_00001-6-3 loss: 0.555792  [  128/  265]
train() client id: f_00001-6-4 loss: 0.469105  [  160/  265]
train() client id: f_00001-6-5 loss: 0.490518  [  192/  265]
train() client id: f_00001-6-6 loss: 0.430294  [  224/  265]
train() client id: f_00001-6-7 loss: 0.477877  [  256/  265]
train() client id: f_00001-7-0 loss: 0.459716  [   32/  265]
train() client id: f_00001-7-1 loss: 0.446682  [   64/  265]
train() client id: f_00001-7-2 loss: 0.402630  [   96/  265]
train() client id: f_00001-7-3 loss: 0.606560  [  128/  265]
train() client id: f_00001-7-4 loss: 0.756407  [  160/  265]
train() client id: f_00001-7-5 loss: 0.468023  [  192/  265]
train() client id: f_00001-7-6 loss: 0.466068  [  224/  265]
train() client id: f_00001-7-7 loss: 0.412803  [  256/  265]
train() client id: f_00002-0-0 loss: 1.167835  [   32/  124]
train() client id: f_00002-0-1 loss: 1.150501  [   64/  124]
train() client id: f_00002-0-2 loss: 1.252902  [   96/  124]
train() client id: f_00002-1-0 loss: 1.108672  [   32/  124]
train() client id: f_00002-1-1 loss: 1.256513  [   64/  124]
train() client id: f_00002-1-2 loss: 0.955946  [   96/  124]
train() client id: f_00002-2-0 loss: 0.846176  [   32/  124]
train() client id: f_00002-2-1 loss: 1.244139  [   64/  124]
train() client id: f_00002-2-2 loss: 1.046079  [   96/  124]
train() client id: f_00002-3-0 loss: 1.259963  [   32/  124]
train() client id: f_00002-3-1 loss: 0.896646  [   64/  124]
train() client id: f_00002-3-2 loss: 1.144540  [   96/  124]
train() client id: f_00002-4-0 loss: 0.977779  [   32/  124]
train() client id: f_00002-4-1 loss: 0.960316  [   64/  124]
train() client id: f_00002-4-2 loss: 1.081343  [   96/  124]
train() client id: f_00002-5-0 loss: 0.804252  [   32/  124]
train() client id: f_00002-5-1 loss: 1.195029  [   64/  124]
train() client id: f_00002-5-2 loss: 1.017842  [   96/  124]
train() client id: f_00002-6-0 loss: 0.903366  [   32/  124]
train() client id: f_00002-6-1 loss: 1.096833  [   64/  124]
train() client id: f_00002-6-2 loss: 0.835781  [   96/  124]
train() client id: f_00002-7-0 loss: 0.831905  [   32/  124]
train() client id: f_00002-7-1 loss: 0.963506  [   64/  124]
train() client id: f_00002-7-2 loss: 1.174964  [   96/  124]
train() client id: f_00003-0-0 loss: 0.699780  [   32/   43]
train() client id: f_00003-1-0 loss: 0.843375  [   32/   43]
train() client id: f_00003-2-0 loss: 0.808532  [   32/   43]
train() client id: f_00003-3-0 loss: 0.732537  [   32/   43]
train() client id: f_00003-4-0 loss: 0.651397  [   32/   43]
train() client id: f_00003-5-0 loss: 0.716469  [   32/   43]
train() client id: f_00003-6-0 loss: 0.805520  [   32/   43]
train() client id: f_00003-7-0 loss: 0.806069  [   32/   43]
train() client id: f_00004-0-0 loss: 0.823753  [   32/  306]
train() client id: f_00004-0-1 loss: 0.900129  [   64/  306]
train() client id: f_00004-0-2 loss: 0.809663  [   96/  306]
train() client id: f_00004-0-3 loss: 0.703039  [  128/  306]
train() client id: f_00004-0-4 loss: 0.969964  [  160/  306]
train() client id: f_00004-0-5 loss: 0.729211  [  192/  306]
train() client id: f_00004-0-6 loss: 0.807905  [  224/  306]
train() client id: f_00004-0-7 loss: 1.021483  [  256/  306]
train() client id: f_00004-0-8 loss: 0.762628  [  288/  306]
train() client id: f_00004-1-0 loss: 0.779243  [   32/  306]
train() client id: f_00004-1-1 loss: 0.841887  [   64/  306]
train() client id: f_00004-1-2 loss: 0.845791  [   96/  306]
train() client id: f_00004-1-3 loss: 0.674456  [  128/  306]
train() client id: f_00004-1-4 loss: 0.918273  [  160/  306]
train() client id: f_00004-1-5 loss: 0.994418  [  192/  306]
train() client id: f_00004-1-6 loss: 0.843070  [  224/  306]
train() client id: f_00004-1-7 loss: 0.813388  [  256/  306]
train() client id: f_00004-1-8 loss: 0.929820  [  288/  306]
train() client id: f_00004-2-0 loss: 0.919544  [   32/  306]
train() client id: f_00004-2-1 loss: 1.013317  [   64/  306]
train() client id: f_00004-2-2 loss: 0.941662  [   96/  306]
train() client id: f_00004-2-3 loss: 0.769996  [  128/  306]
train() client id: f_00004-2-4 loss: 0.815066  [  160/  306]
train() client id: f_00004-2-5 loss: 0.766407  [  192/  306]
train() client id: f_00004-2-6 loss: 0.789544  [  224/  306]
train() client id: f_00004-2-7 loss: 0.919428  [  256/  306]
train() client id: f_00004-2-8 loss: 0.853604  [  288/  306]
train() client id: f_00004-3-0 loss: 0.978618  [   32/  306]
train() client id: f_00004-3-1 loss: 0.727513  [   64/  306]
train() client id: f_00004-3-2 loss: 0.909995  [   96/  306]
train() client id: f_00004-3-3 loss: 0.823098  [  128/  306]
train() client id: f_00004-3-4 loss: 0.861493  [  160/  306]
train() client id: f_00004-3-5 loss: 0.852073  [  192/  306]
train() client id: f_00004-3-6 loss: 0.900096  [  224/  306]
train() client id: f_00004-3-7 loss: 0.713840  [  256/  306]
train() client id: f_00004-3-8 loss: 0.913152  [  288/  306]
train() client id: f_00004-4-0 loss: 0.770719  [   32/  306]
train() client id: f_00004-4-1 loss: 0.803262  [   64/  306]
train() client id: f_00004-4-2 loss: 0.794737  [   96/  306]
train() client id: f_00004-4-3 loss: 0.965489  [  128/  306]
train() client id: f_00004-4-4 loss: 0.901489  [  160/  306]
train() client id: f_00004-4-5 loss: 0.848354  [  192/  306]
train() client id: f_00004-4-6 loss: 0.851309  [  224/  306]
train() client id: f_00004-4-7 loss: 0.784917  [  256/  306]
train() client id: f_00004-4-8 loss: 0.891537  [  288/  306]
train() client id: f_00004-5-0 loss: 0.835362  [   32/  306]
train() client id: f_00004-5-1 loss: 0.864671  [   64/  306]
train() client id: f_00004-5-2 loss: 0.885980  [   96/  306]
train() client id: f_00004-5-3 loss: 0.827862  [  128/  306]
train() client id: f_00004-5-4 loss: 0.768390  [  160/  306]
train() client id: f_00004-5-5 loss: 0.893102  [  192/  306]
train() client id: f_00004-5-6 loss: 0.866201  [  224/  306]
train() client id: f_00004-5-7 loss: 0.999944  [  256/  306]
train() client id: f_00004-5-8 loss: 0.753515  [  288/  306]
train() client id: f_00004-6-0 loss: 0.911471  [   32/  306]
train() client id: f_00004-6-1 loss: 0.861431  [   64/  306]
train() client id: f_00004-6-2 loss: 0.774534  [   96/  306]
train() client id: f_00004-6-3 loss: 0.840533  [  128/  306]
train() client id: f_00004-6-4 loss: 0.884103  [  160/  306]
train() client id: f_00004-6-5 loss: 0.785672  [  192/  306]
train() client id: f_00004-6-6 loss: 0.846671  [  224/  306]
train() client id: f_00004-6-7 loss: 0.909731  [  256/  306]
train() client id: f_00004-6-8 loss: 0.871631  [  288/  306]
train() client id: f_00004-7-0 loss: 0.868680  [   32/  306]
train() client id: f_00004-7-1 loss: 0.865666  [   64/  306]
train() client id: f_00004-7-2 loss: 0.850009  [   96/  306]
train() client id: f_00004-7-3 loss: 0.925620  [  128/  306]
train() client id: f_00004-7-4 loss: 0.773937  [  160/  306]
train() client id: f_00004-7-5 loss: 0.783485  [  192/  306]
train() client id: f_00004-7-6 loss: 0.776819  [  224/  306]
train() client id: f_00004-7-7 loss: 0.970834  [  256/  306]
train() client id: f_00004-7-8 loss: 0.829899  [  288/  306]
train() client id: f_00005-0-0 loss: 0.487168  [   32/  146]
train() client id: f_00005-0-1 loss: 0.506755  [   64/  146]
train() client id: f_00005-0-2 loss: 0.308983  [   96/  146]
train() client id: f_00005-0-3 loss: 1.067241  [  128/  146]
train() client id: f_00005-1-0 loss: 0.710378  [   32/  146]
train() client id: f_00005-1-1 loss: 0.400421  [   64/  146]
train() client id: f_00005-1-2 loss: 0.602001  [   96/  146]
train() client id: f_00005-1-3 loss: 0.712090  [  128/  146]
train() client id: f_00005-2-0 loss: 0.554241  [   32/  146]
train() client id: f_00005-2-1 loss: 0.805074  [   64/  146]
train() client id: f_00005-2-2 loss: 0.477753  [   96/  146]
train() client id: f_00005-2-3 loss: 0.447659  [  128/  146]
train() client id: f_00005-3-0 loss: 0.347989  [   32/  146]
train() client id: f_00005-3-1 loss: 0.613105  [   64/  146]
train() client id: f_00005-3-2 loss: 0.755219  [   96/  146]
train() client id: f_00005-3-3 loss: 0.739018  [  128/  146]
train() client id: f_00005-4-0 loss: 0.476428  [   32/  146]
train() client id: f_00005-4-1 loss: 0.705529  [   64/  146]
train() client id: f_00005-4-2 loss: 0.630474  [   96/  146]
train() client id: f_00005-4-3 loss: 0.409492  [  128/  146]
train() client id: f_00005-5-0 loss: 0.546345  [   32/  146]
train() client id: f_00005-5-1 loss: 0.693878  [   64/  146]
train() client id: f_00005-5-2 loss: 0.740852  [   96/  146]
train() client id: f_00005-5-3 loss: 0.492867  [  128/  146]
train() client id: f_00005-6-0 loss: 0.595830  [   32/  146]
train() client id: f_00005-6-1 loss: 0.544034  [   64/  146]
train() client id: f_00005-6-2 loss: 0.626823  [   96/  146]
train() client id: f_00005-6-3 loss: 0.522146  [  128/  146]
train() client id: f_00005-7-0 loss: 0.389807  [   32/  146]
train() client id: f_00005-7-1 loss: 0.636242  [   64/  146]
train() client id: f_00005-7-2 loss: 0.722709  [   96/  146]
train() client id: f_00005-7-3 loss: 0.630692  [  128/  146]
train() client id: f_00006-0-0 loss: 0.404354  [   32/   54]
train() client id: f_00006-1-0 loss: 0.433675  [   32/   54]
train() client id: f_00006-2-0 loss: 0.466445  [   32/   54]
train() client id: f_00006-3-0 loss: 0.413267  [   32/   54]
train() client id: f_00006-4-0 loss: 0.382267  [   32/   54]
train() client id: f_00006-5-0 loss: 0.386804  [   32/   54]
train() client id: f_00006-6-0 loss: 0.484291  [   32/   54]
train() client id: f_00006-7-0 loss: 0.380290  [   32/   54]
train() client id: f_00007-0-0 loss: 0.833412  [   32/  179]
train() client id: f_00007-0-1 loss: 0.644384  [   64/  179]
train() client id: f_00007-0-2 loss: 0.734716  [   96/  179]
train() client id: f_00007-0-3 loss: 0.779352  [  128/  179]
train() client id: f_00007-0-4 loss: 0.691531  [  160/  179]
train() client id: f_00007-1-0 loss: 0.966826  [   32/  179]
train() client id: f_00007-1-1 loss: 0.792457  [   64/  179]
train() client id: f_00007-1-2 loss: 0.670371  [   96/  179]
train() client id: f_00007-1-3 loss: 0.688216  [  128/  179]
train() client id: f_00007-1-4 loss: 0.677068  [  160/  179]
train() client id: f_00007-2-0 loss: 0.749746  [   32/  179]
train() client id: f_00007-2-1 loss: 0.638493  [   64/  179]
train() client id: f_00007-2-2 loss: 0.742180  [   96/  179]
train() client id: f_00007-2-3 loss: 0.882836  [  128/  179]
train() client id: f_00007-2-4 loss: 0.589093  [  160/  179]
train() client id: f_00007-3-0 loss: 0.926513  [   32/  179]
train() client id: f_00007-3-1 loss: 0.787422  [   64/  179]
train() client id: f_00007-3-2 loss: 0.639141  [   96/  179]
train() client id: f_00007-3-3 loss: 0.747419  [  128/  179]
train() client id: f_00007-3-4 loss: 0.634430  [  160/  179]
train() client id: f_00007-4-0 loss: 0.649998  [   32/  179]
train() client id: f_00007-4-1 loss: 0.558234  [   64/  179]
train() client id: f_00007-4-2 loss: 0.749387  [   96/  179]
train() client id: f_00007-4-3 loss: 0.907589  [  128/  179]
train() client id: f_00007-4-4 loss: 0.747902  [  160/  179]
train() client id: f_00007-5-0 loss: 0.589337  [   32/  179]
train() client id: f_00007-5-1 loss: 0.556900  [   64/  179]
train() client id: f_00007-5-2 loss: 0.811682  [   96/  179]
train() client id: f_00007-5-3 loss: 0.779604  [  128/  179]
train() client id: f_00007-5-4 loss: 0.778094  [  160/  179]
train() client id: f_00007-6-0 loss: 0.715679  [   32/  179]
train() client id: f_00007-6-1 loss: 0.598284  [   64/  179]
train() client id: f_00007-6-2 loss: 0.634796  [   96/  179]
train() client id: f_00007-6-3 loss: 0.585751  [  128/  179]
train() client id: f_00007-6-4 loss: 0.994688  [  160/  179]
train() client id: f_00007-7-0 loss: 0.714695  [   32/  179]
train() client id: f_00007-7-1 loss: 0.673103  [   64/  179]
train() client id: f_00007-7-2 loss: 0.675333  [   96/  179]
train() client id: f_00007-7-3 loss: 0.850638  [  128/  179]
train() client id: f_00007-7-4 loss: 0.735871  [  160/  179]
train() client id: f_00008-0-0 loss: 0.657188  [   32/  130]
train() client id: f_00008-0-1 loss: 0.756732  [   64/  130]
train() client id: f_00008-0-2 loss: 0.673271  [   96/  130]
train() client id: f_00008-0-3 loss: 0.648247  [  128/  130]
train() client id: f_00008-1-0 loss: 0.629904  [   32/  130]
train() client id: f_00008-1-1 loss: 0.614552  [   64/  130]
train() client id: f_00008-1-2 loss: 0.739126  [   96/  130]
train() client id: f_00008-1-3 loss: 0.719947  [  128/  130]
train() client id: f_00008-2-0 loss: 0.520222  [   32/  130]
train() client id: f_00008-2-1 loss: 0.696748  [   64/  130]
train() client id: f_00008-2-2 loss: 0.714369  [   96/  130]
train() client id: f_00008-2-3 loss: 0.795988  [  128/  130]
train() client id: f_00008-3-0 loss: 0.729372  [   32/  130]
train() client id: f_00008-3-1 loss: 0.649617  [   64/  130]
train() client id: f_00008-3-2 loss: 0.742450  [   96/  130]
train() client id: f_00008-3-3 loss: 0.582297  [  128/  130]
train() client id: f_00008-4-0 loss: 0.724198  [   32/  130]
train() client id: f_00008-4-1 loss: 0.716246  [   64/  130]
train() client id: f_00008-4-2 loss: 0.623251  [   96/  130]
train() client id: f_00008-4-3 loss: 0.671814  [  128/  130]
train() client id: f_00008-5-0 loss: 0.688135  [   32/  130]
train() client id: f_00008-5-1 loss: 0.655929  [   64/  130]
train() client id: f_00008-5-2 loss: 0.836872  [   96/  130]
train() client id: f_00008-5-3 loss: 0.530887  [  128/  130]
train() client id: f_00008-6-0 loss: 0.702529  [   32/  130]
train() client id: f_00008-6-1 loss: 0.653118  [   64/  130]
train() client id: f_00008-6-2 loss: 0.620993  [   96/  130]
train() client id: f_00008-6-3 loss: 0.735592  [  128/  130]
train() client id: f_00008-7-0 loss: 0.627233  [   32/  130]
train() client id: f_00008-7-1 loss: 0.699979  [   64/  130]
train() client id: f_00008-7-2 loss: 0.723293  [   96/  130]
train() client id: f_00008-7-3 loss: 0.658736  [  128/  130]
train() client id: f_00009-0-0 loss: 0.949169  [   32/  118]
train() client id: f_00009-0-1 loss: 0.923158  [   64/  118]
train() client id: f_00009-0-2 loss: 0.854336  [   96/  118]
train() client id: f_00009-1-0 loss: 0.852266  [   32/  118]
train() client id: f_00009-1-1 loss: 0.834100  [   64/  118]
train() client id: f_00009-1-2 loss: 0.931145  [   96/  118]
train() client id: f_00009-2-0 loss: 0.771685  [   32/  118]
train() client id: f_00009-2-1 loss: 1.021302  [   64/  118]
train() client id: f_00009-2-2 loss: 0.673346  [   96/  118]
train() client id: f_00009-3-0 loss: 0.748287  [   32/  118]
train() client id: f_00009-3-1 loss: 0.933774  [   64/  118]
train() client id: f_00009-3-2 loss: 0.775322  [   96/  118]
train() client id: f_00009-4-0 loss: 1.007917  [   32/  118]
train() client id: f_00009-4-1 loss: 0.702433  [   64/  118]
train() client id: f_00009-4-2 loss: 0.733639  [   96/  118]
train() client id: f_00009-5-0 loss: 0.880926  [   32/  118]
train() client id: f_00009-5-1 loss: 0.727205  [   64/  118]
train() client id: f_00009-5-2 loss: 0.811127  [   96/  118]
train() client id: f_00009-6-0 loss: 0.750189  [   32/  118]
train() client id: f_00009-6-1 loss: 0.753325  [   64/  118]
train() client id: f_00009-6-2 loss: 0.823722  [   96/  118]
train() client id: f_00009-7-0 loss: 0.729724  [   32/  118]
train() client id: f_00009-7-1 loss: 0.720552  [   64/  118]
train() client id: f_00009-7-2 loss: 0.767119  [   96/  118]
At round 68 accuracy: 0.6445623342175066
At round 68 training accuracy: 0.5922199865861838
At round 68 training loss: 0.8114779361755841
update_location
xs = [  -3.9056584     4.20031788  360.00902392   18.81129433    0.97929623
    3.95640986 -322.44319194 -301.32485185  344.66397685 -287.06087855]
ys = [ 352.5879595   335.55583871    1.32061395 -322.45517586  314.35018685
  297.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [366.51537942 350.16476631 373.64186238 338.12897722 329.87421692
 314.17975995 337.60406184 317.48597228 359.3075659  304.00651284]
dists_bs = [246.59837202 240.60534339 562.4937198  533.87683018 224.47090518
 217.04541519 230.84655055 215.16464081 542.987262   204.44591885]
uav_gains = [9.97297579e-13 1.20746354e-12 9.24956572e-13 1.41687733e-12
 1.59815306e-12 2.06205032e-12 1.42738234e-12 1.94864202e-12
 1.08135149e-12 2.47729216e-12]
bs_gains = [2.21677447e-11 2.37486723e-11 2.20274684e-12 2.54952386e-12
 2.88433648e-11 3.16921979e-11 2.66678863e-11 3.24739829e-11
 2.43154935e-12 3.74692015e-11]
Round 69
-------------------------------
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.8731924  3.7097278  1.85116443 0.70130907 4.2759113  2.0588358
 0.85327763 2.56982483 1.88245466 1.66965903]
obj_prev = 21.445356959883735
eta_min = 1.2228941532952692e-50	eta_max = 0.9500254409851863
af = 4.439896748873053	bf = 0.784423253509249	zeta = 4.883886423760359	eta = 0.9090909090909091
af = 4.439896748873053	bf = 0.784423253509249	zeta = 13.283496720526747	eta = 0.3342415662294824
af = 4.439896748873053	bf = 0.784423253509249	zeta = 8.486662978303045	eta = 0.5231616667498248
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.677394178566005	eta = 0.5783077754778437
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626245838238497	eta = 0.5821864181995182
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626008957353865	eta = 0.5822045022110287
eta = 0.5822045022110287
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.04463654 0.09387843 0.04392802 0.0152331  0.10840302 0.0517217
 0.01912994 0.06341222 0.04605357 0.04180247]
ene_total = [0.78877956 1.11412037 0.79451743 0.41620728 1.26807169 0.64932725
 0.45766691 0.90423052 0.69485736 0.53823058]
ti_comp = [2.16111529 2.35459485 2.14875789 2.20941898 2.35838257 2.36010268
 2.21029221 2.24279005 2.26158603 2.36299155]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [1.19013336e-06 9.32706721e-06 1.14743992e-06 4.52573450e-08
 1.43144819e-05 1.55251797e-06 8.95615827e-08 3.16826659e-06
 1.19355847e-06 8.17638912e-07]
ene_total = [0.30300549 0.09252695 0.3164538  0.25042331 0.088459   0.08644808
 0.24947344 0.214139   0.19366152 0.08329608]
optimize_network iter = 0 obj = 1.8778866839980972
eta = 0.5822045022110287
freqs = [10327199.39136077 19935155.93888518 10221724.11669465  3447309.92299876
 22982492.36545908 10957510.88478662  4327469.55320673 14136905.79686903
 10181697.58740329  8845243.50076053]
eta_min = 0.5822045022110292	eta_max = 0.7888487440198617
af = 0.000276093861788328	bf = 0.784423253509249	zeta = 0.00030370324796716086	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.09696425e-07 1.64338949e-06 2.02174023e-07 7.97415130e-09
 2.52215070e-06 2.73547049e-07 1.57803692e-08 5.58235069e-07
 2.10299915e-07 1.44064491e-07]
ene_total = [1.3924886  0.42484648 1.45429544 1.1508812  0.40594569 0.39722982
 1.14651401 0.98399884 0.88997085 0.38277432]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 1 obj = 3.715240812383447
eta = 0.7888487440198617
freqs = [10252122.85547994 17928004.42999087 10221724.11669466  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117568
  9570237.57402436  7925070.10389261]
eta_min = 0.7888487440198628	eta_max = 0.7888487440198605
af = 0.00022982602808494754	bf = 0.784423253509249	zeta = 0.0002528086308934423	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.06658611e-07 1.32912288e-06 2.02174023e-07 7.44162632e-09
 2.03296858e-06 2.20155092e-07 1.47125432e-08 5.02791340e-07
 1.85799332e-07 1.15649455e-07]
ene_total = [1.39248845 0.42483077 1.45429544 1.15088117 0.40592122 0.39722715
 1.14651395 0.98399607 0.88996963 0.38277289]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 2 obj = 3.715240812383425
eta = 0.7888487440198605
freqs = [10252122.85547993 17928004.42999088 10221724.11669465  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117567
  9570237.57402436  7925070.10389262]
Done!
At round 69 energy consumption: 3.715240812383447
At round 69 eta: 0.7888487440198605
At round 69 local rounds: 7.766500565656417
At round 69 global rounds: 21.53403916195109
At round 69 a_n: 4.204393568402164
gradient difference: 0.5832789540290833
train() client id: f_00000-0-0 loss: 1.049243  [   32/  126]
train() client id: f_00000-0-1 loss: 1.191550  [   64/  126]
train() client id: f_00000-0-2 loss: 1.102489  [   96/  126]
train() client id: f_00000-1-0 loss: 1.038237  [   32/  126]
train() client id: f_00000-1-1 loss: 1.187944  [   64/  126]
train() client id: f_00000-1-2 loss: 0.992980  [   96/  126]
train() client id: f_00000-2-0 loss: 0.948526  [   32/  126]
train() client id: f_00000-2-1 loss: 1.022051  [   64/  126]
train() client id: f_00000-2-2 loss: 1.053892  [   96/  126]
train() client id: f_00000-3-0 loss: 0.837050  [   32/  126]
train() client id: f_00000-3-1 loss: 0.911474  [   64/  126]
train() client id: f_00000-3-2 loss: 1.227912  [   96/  126]
train() client id: f_00000-4-0 loss: 0.817081  [   32/  126]
train() client id: f_00000-4-1 loss: 0.985051  [   64/  126]
train() client id: f_00000-4-2 loss: 0.963048  [   96/  126]
train() client id: f_00000-5-0 loss: 0.730266  [   32/  126]
train() client id: f_00000-5-1 loss: 0.998885  [   64/  126]
train() client id: f_00000-5-2 loss: 0.868408  [   96/  126]
train() client id: f_00000-6-0 loss: 0.844915  [   32/  126]
train() client id: f_00000-6-1 loss: 1.021080  [   64/  126]
train() client id: f_00000-6-2 loss: 0.712111  [   96/  126]
train() client id: f_00001-0-0 loss: 0.540938  [   32/  265]
train() client id: f_00001-0-1 loss: 0.580548  [   64/  265]
train() client id: f_00001-0-2 loss: 0.541839  [   96/  265]
train() client id: f_00001-0-3 loss: 0.545324  [  128/  265]
train() client id: f_00001-0-4 loss: 0.548209  [  160/  265]
train() client id: f_00001-0-5 loss: 0.587528  [  192/  265]
train() client id: f_00001-0-6 loss: 0.556758  [  224/  265]
train() client id: f_00001-0-7 loss: 0.520464  [  256/  265]
train() client id: f_00001-1-0 loss: 0.645968  [   32/  265]
train() client id: f_00001-1-1 loss: 0.566043  [   64/  265]
train() client id: f_00001-1-2 loss: 0.693817  [   96/  265]
train() client id: f_00001-1-3 loss: 0.521609  [  128/  265]
train() client id: f_00001-1-4 loss: 0.474689  [  160/  265]
train() client id: f_00001-1-5 loss: 0.512451  [  192/  265]
train() client id: f_00001-1-6 loss: 0.478896  [  224/  265]
train() client id: f_00001-1-7 loss: 0.484046  [  256/  265]
train() client id: f_00001-2-0 loss: 0.485144  [   32/  265]
train() client id: f_00001-2-1 loss: 0.645137  [   64/  265]
train() client id: f_00001-2-2 loss: 0.490765  [   96/  265]
train() client id: f_00001-2-3 loss: 0.514206  [  128/  265]
train() client id: f_00001-2-4 loss: 0.462667  [  160/  265]
train() client id: f_00001-2-5 loss: 0.479615  [  192/  265]
train() client id: f_00001-2-6 loss: 0.780583  [  224/  265]
train() client id: f_00001-2-7 loss: 0.498565  [  256/  265]
train() client id: f_00001-3-0 loss: 0.430404  [   32/  265]
train() client id: f_00001-3-1 loss: 0.504700  [   64/  265]
train() client id: f_00001-3-2 loss: 0.593988  [   96/  265]
train() client id: f_00001-3-3 loss: 0.462699  [  128/  265]
train() client id: f_00001-3-4 loss: 0.488028  [  160/  265]
train() client id: f_00001-3-5 loss: 0.634035  [  192/  265]
train() client id: f_00001-3-6 loss: 0.689441  [  224/  265]
train() client id: f_00001-3-7 loss: 0.472602  [  256/  265]
train() client id: f_00001-4-0 loss: 0.531572  [   32/  265]
train() client id: f_00001-4-1 loss: 0.463187  [   64/  265]
train() client id: f_00001-4-2 loss: 0.565649  [   96/  265]
train() client id: f_00001-4-3 loss: 0.646805  [  128/  265]
train() client id: f_00001-4-4 loss: 0.641065  [  160/  265]
train() client id: f_00001-4-5 loss: 0.622671  [  192/  265]
train() client id: f_00001-4-6 loss: 0.443258  [  224/  265]
train() client id: f_00001-4-7 loss: 0.439621  [  256/  265]
train() client id: f_00001-5-0 loss: 0.512820  [   32/  265]
train() client id: f_00001-5-1 loss: 0.446750  [   64/  265]
train() client id: f_00001-5-2 loss: 0.720113  [   96/  265]
train() client id: f_00001-5-3 loss: 0.563594  [  128/  265]
train() client id: f_00001-5-4 loss: 0.518955  [  160/  265]
train() client id: f_00001-5-5 loss: 0.450680  [  192/  265]
train() client id: f_00001-5-6 loss: 0.455362  [  224/  265]
train() client id: f_00001-5-7 loss: 0.693989  [  256/  265]
train() client id: f_00001-6-0 loss: 0.556565  [   32/  265]
train() client id: f_00001-6-1 loss: 0.504060  [   64/  265]
train() client id: f_00001-6-2 loss: 0.572777  [   96/  265]
train() client id: f_00001-6-3 loss: 0.535425  [  128/  265]
train() client id: f_00001-6-4 loss: 0.617568  [  160/  265]
train() client id: f_00001-6-5 loss: 0.563560  [  192/  265]
train() client id: f_00001-6-6 loss: 0.546945  [  224/  265]
train() client id: f_00001-6-7 loss: 0.461403  [  256/  265]
train() client id: f_00002-0-0 loss: 0.781664  [   32/  124]
train() client id: f_00002-0-1 loss: 0.620073  [   64/  124]
train() client id: f_00002-0-2 loss: 0.899440  [   96/  124]
train() client id: f_00002-1-0 loss: 0.882258  [   32/  124]
train() client id: f_00002-1-1 loss: 0.671646  [   64/  124]
train() client id: f_00002-1-2 loss: 0.733095  [   96/  124]
train() client id: f_00002-2-0 loss: 0.843519  [   32/  124]
train() client id: f_00002-2-1 loss: 0.619791  [   64/  124]
train() client id: f_00002-2-2 loss: 0.730961  [   96/  124]
train() client id: f_00002-3-0 loss: 0.806036  [   32/  124]
train() client id: f_00002-3-1 loss: 0.838100  [   64/  124]
train() client id: f_00002-3-2 loss: 0.807334  [   96/  124]
train() client id: f_00002-4-0 loss: 0.720643  [   32/  124]
train() client id: f_00002-4-1 loss: 0.652285  [   64/  124]
train() client id: f_00002-4-2 loss: 0.865855  [   96/  124]
train() client id: f_00002-5-0 loss: 0.699292  [   32/  124]
train() client id: f_00002-5-1 loss: 0.776967  [   64/  124]
train() client id: f_00002-5-2 loss: 0.618088  [   96/  124]
train() client id: f_00002-6-0 loss: 0.775464  [   32/  124]
train() client id: f_00002-6-1 loss: 0.700137  [   64/  124]
train() client id: f_00002-6-2 loss: 0.680304  [   96/  124]
train() client id: f_00003-0-0 loss: 0.394284  [   32/   43]
train() client id: f_00003-1-0 loss: 0.516499  [   32/   43]
train() client id: f_00003-2-0 loss: 0.508910  [   32/   43]
train() client id: f_00003-3-0 loss: 0.569536  [   32/   43]
train() client id: f_00003-4-0 loss: 0.657297  [   32/   43]
train() client id: f_00003-5-0 loss: 0.511459  [   32/   43]
train() client id: f_00003-6-0 loss: 0.416367  [   32/   43]
train() client id: f_00004-0-0 loss: 0.900922  [   32/  306]
train() client id: f_00004-0-1 loss: 0.848746  [   64/  306]
train() client id: f_00004-0-2 loss: 0.926347  [   96/  306]
train() client id: f_00004-0-3 loss: 0.864637  [  128/  306]
train() client id: f_00004-0-4 loss: 0.756828  [  160/  306]
train() client id: f_00004-0-5 loss: 0.929061  [  192/  306]
train() client id: f_00004-0-6 loss: 0.882826  [  224/  306]
train() client id: f_00004-0-7 loss: 0.800700  [  256/  306]
train() client id: f_00004-0-8 loss: 0.728166  [  288/  306]
train() client id: f_00004-1-0 loss: 0.788662  [   32/  306]
train() client id: f_00004-1-1 loss: 0.669226  [   64/  306]
train() client id: f_00004-1-2 loss: 0.889181  [   96/  306]
train() client id: f_00004-1-3 loss: 0.767709  [  128/  306]
train() client id: f_00004-1-4 loss: 0.951201  [  160/  306]
train() client id: f_00004-1-5 loss: 0.916867  [  192/  306]
train() client id: f_00004-1-6 loss: 0.879083  [  224/  306]
train() client id: f_00004-1-7 loss: 0.924420  [  256/  306]
train() client id: f_00004-1-8 loss: 0.839357  [  288/  306]
train() client id: f_00004-2-0 loss: 0.782925  [   32/  306]
train() client id: f_00004-2-1 loss: 0.928929  [   64/  306]
train() client id: f_00004-2-2 loss: 0.793116  [   96/  306]
train() client id: f_00004-2-3 loss: 0.780248  [  128/  306]
train() client id: f_00004-2-4 loss: 0.825298  [  160/  306]
train() client id: f_00004-2-5 loss: 0.864945  [  192/  306]
train() client id: f_00004-2-6 loss: 0.969640  [  224/  306]
train() client id: f_00004-2-7 loss: 0.693699  [  256/  306]
train() client id: f_00004-2-8 loss: 0.934573  [  288/  306]
train() client id: f_00004-3-0 loss: 0.892028  [   32/  306]
train() client id: f_00004-3-1 loss: 0.681853  [   64/  306]
train() client id: f_00004-3-2 loss: 0.787900  [   96/  306]
train() client id: f_00004-3-3 loss: 0.912595  [  128/  306]
train() client id: f_00004-3-4 loss: 0.772823  [  160/  306]
train() client id: f_00004-3-5 loss: 0.834567  [  192/  306]
train() client id: f_00004-3-6 loss: 0.876367  [  224/  306]
train() client id: f_00004-3-7 loss: 0.980293  [  256/  306]
train() client id: f_00004-3-8 loss: 0.740926  [  288/  306]
train() client id: f_00004-4-0 loss: 0.836986  [   32/  306]
train() client id: f_00004-4-1 loss: 0.813420  [   64/  306]
train() client id: f_00004-4-2 loss: 0.848421  [   96/  306]
train() client id: f_00004-4-3 loss: 0.803828  [  128/  306]
train() client id: f_00004-4-4 loss: 0.934682  [  160/  306]
train() client id: f_00004-4-5 loss: 0.752580  [  192/  306]
train() client id: f_00004-4-6 loss: 0.771778  [  224/  306]
train() client id: f_00004-4-7 loss: 0.804363  [  256/  306]
train() client id: f_00004-4-8 loss: 0.889613  [  288/  306]
train() client id: f_00004-5-0 loss: 0.804300  [   32/  306]
train() client id: f_00004-5-1 loss: 0.729450  [   64/  306]
train() client id: f_00004-5-2 loss: 0.744425  [   96/  306]
train() client id: f_00004-5-3 loss: 0.820680  [  128/  306]
train() client id: f_00004-5-4 loss: 0.817761  [  160/  306]
train() client id: f_00004-5-5 loss: 0.998825  [  192/  306]
train() client id: f_00004-5-6 loss: 0.897542  [  224/  306]
train() client id: f_00004-5-7 loss: 0.773823  [  256/  306]
train() client id: f_00004-5-8 loss: 0.847532  [  288/  306]
train() client id: f_00004-6-0 loss: 0.855462  [   32/  306]
train() client id: f_00004-6-1 loss: 0.810095  [   64/  306]
train() client id: f_00004-6-2 loss: 0.712404  [   96/  306]
train() client id: f_00004-6-3 loss: 0.881207  [  128/  306]
train() client id: f_00004-6-4 loss: 0.754576  [  160/  306]
train() client id: f_00004-6-5 loss: 0.905559  [  192/  306]
train() client id: f_00004-6-6 loss: 0.955336  [  224/  306]
train() client id: f_00004-6-7 loss: 0.831698  [  256/  306]
train() client id: f_00004-6-8 loss: 0.833086  [  288/  306]
train() client id: f_00005-0-0 loss: 0.305545  [   32/  146]
train() client id: f_00005-0-1 loss: 0.682653  [   64/  146]
train() client id: f_00005-0-2 loss: 0.851743  [   96/  146]
train() client id: f_00005-0-3 loss: 0.493508  [  128/  146]
train() client id: f_00005-1-0 loss: 0.647779  [   32/  146]
train() client id: f_00005-1-1 loss: 0.432071  [   64/  146]
train() client id: f_00005-1-2 loss: 0.584629  [   96/  146]
train() client id: f_00005-1-3 loss: 0.558568  [  128/  146]
train() client id: f_00005-2-0 loss: 0.310731  [   32/  146]
train() client id: f_00005-2-1 loss: 0.431778  [   64/  146]
train() client id: f_00005-2-2 loss: 0.535195  [   96/  146]
train() client id: f_00005-2-3 loss: 0.777661  [  128/  146]
train() client id: f_00005-3-0 loss: 0.315036  [   32/  146]
train() client id: f_00005-3-1 loss: 0.684393  [   64/  146]
train() client id: f_00005-3-2 loss: 0.488477  [   96/  146]
train() client id: f_00005-3-3 loss: 0.512985  [  128/  146]
train() client id: f_00005-4-0 loss: 0.713041  [   32/  146]
train() client id: f_00005-4-1 loss: 0.585721  [   64/  146]
train() client id: f_00005-4-2 loss: 0.749021  [   96/  146]
train() client id: f_00005-4-3 loss: 0.264700  [  128/  146]
train() client id: f_00005-5-0 loss: 0.494280  [   32/  146]
train() client id: f_00005-5-1 loss: 0.581581  [   64/  146]
train() client id: f_00005-5-2 loss: 0.454762  [   96/  146]
train() client id: f_00005-5-3 loss: 0.641342  [  128/  146]
train() client id: f_00005-6-0 loss: 0.424032  [   32/  146]
train() client id: f_00005-6-1 loss: 0.678182  [   64/  146]
train() client id: f_00005-6-2 loss: 0.699734  [   96/  146]
train() client id: f_00005-6-3 loss: 0.534292  [  128/  146]
train() client id: f_00006-0-0 loss: 0.576756  [   32/   54]
train() client id: f_00006-1-0 loss: 0.559339  [   32/   54]
train() client id: f_00006-2-0 loss: 0.489563  [   32/   54]
train() client id: f_00006-3-0 loss: 0.504497  [   32/   54]
train() client id: f_00006-4-0 loss: 0.584662  [   32/   54]
train() client id: f_00006-5-0 loss: 0.598994  [   32/   54]
train() client id: f_00006-6-0 loss: 0.536963  [   32/   54]
train() client id: f_00007-0-0 loss: 0.423226  [   32/  179]
train() client id: f_00007-0-1 loss: 0.668184  [   64/  179]
train() client id: f_00007-0-2 loss: 0.657655  [   96/  179]
train() client id: f_00007-0-3 loss: 0.589782  [  128/  179]
train() client id: f_00007-0-4 loss: 0.487103  [  160/  179]
train() client id: f_00007-1-0 loss: 0.541162  [   32/  179]
train() client id: f_00007-1-1 loss: 0.627777  [   64/  179]
train() client id: f_00007-1-2 loss: 0.665129  [   96/  179]
train() client id: f_00007-1-3 loss: 0.495899  [  128/  179]
train() client id: f_00007-1-4 loss: 0.609498  [  160/  179]
train() client id: f_00007-2-0 loss: 0.492498  [   32/  179]
train() client id: f_00007-2-1 loss: 0.420314  [   64/  179]
train() client id: f_00007-2-2 loss: 0.617750  [   96/  179]
train() client id: f_00007-2-3 loss: 0.495437  [  128/  179]
train() client id: f_00007-2-4 loss: 0.889681  [  160/  179]
train() client id: f_00007-3-0 loss: 0.434794  [   32/  179]
train() client id: f_00007-3-1 loss: 0.601505  [   64/  179]
train() client id: f_00007-3-2 loss: 0.636501  [   96/  179]
train() client id: f_00007-3-3 loss: 0.487839  [  128/  179]
train() client id: f_00007-3-4 loss: 0.655297  [  160/  179]
train() client id: f_00007-4-0 loss: 0.599910  [   32/  179]
train() client id: f_00007-4-1 loss: 0.530922  [   64/  179]
train() client id: f_00007-4-2 loss: 0.577885  [   96/  179]
train() client id: f_00007-4-3 loss: 0.577402  [  128/  179]
train() client id: f_00007-4-4 loss: 0.528473  [  160/  179]
train() client id: f_00007-5-0 loss: 0.416534  [   32/  179]
train() client id: f_00007-5-1 loss: 0.575321  [   64/  179]
train() client id: f_00007-5-2 loss: 0.571389  [   96/  179]
train() client id: f_00007-5-3 loss: 0.519232  [  128/  179]
train() client id: f_00007-5-4 loss: 0.669258  [  160/  179]
train() client id: f_00007-6-0 loss: 0.546218  [   32/  179]
train() client id: f_00007-6-1 loss: 0.442180  [   64/  179]
train() client id: f_00007-6-2 loss: 0.530576  [   96/  179]
train() client id: f_00007-6-3 loss: 0.640516  [  128/  179]
train() client id: f_00007-6-4 loss: 0.592730  [  160/  179]
train() client id: f_00008-0-0 loss: 0.770027  [   32/  130]
train() client id: f_00008-0-1 loss: 0.776757  [   64/  130]
train() client id: f_00008-0-2 loss: 0.780267  [   96/  130]
train() client id: f_00008-0-3 loss: 0.687043  [  128/  130]
train() client id: f_00008-1-0 loss: 0.778472  [   32/  130]
train() client id: f_00008-1-1 loss: 0.623578  [   64/  130]
train() client id: f_00008-1-2 loss: 0.725537  [   96/  130]
train() client id: f_00008-1-3 loss: 0.817467  [  128/  130]
train() client id: f_00008-2-0 loss: 0.734752  [   32/  130]
train() client id: f_00008-2-1 loss: 0.675058  [   64/  130]
train() client id: f_00008-2-2 loss: 0.761736  [   96/  130]
train() client id: f_00008-2-3 loss: 0.845305  [  128/  130]
train() client id: f_00008-3-0 loss: 0.771459  [   32/  130]
train() client id: f_00008-3-1 loss: 0.759507  [   64/  130]
train() client id: f_00008-3-2 loss: 0.719032  [   96/  130]
train() client id: f_00008-3-3 loss: 0.748221  [  128/  130]
train() client id: f_00008-4-0 loss: 0.773219  [   32/  130]
train() client id: f_00008-4-1 loss: 0.760510  [   64/  130]
train() client id: f_00008-4-2 loss: 0.724986  [   96/  130]
train() client id: f_00008-4-3 loss: 0.752142  [  128/  130]
train() client id: f_00008-5-0 loss: 0.796935  [   32/  130]
train() client id: f_00008-5-1 loss: 0.728261  [   64/  130]
train() client id: f_00008-5-2 loss: 0.767118  [   96/  130]
train() client id: f_00008-5-3 loss: 0.693893  [  128/  130]
train() client id: f_00008-6-0 loss: 0.768463  [   32/  130]
train() client id: f_00008-6-1 loss: 0.816261  [   64/  130]
train() client id: f_00008-6-2 loss: 0.707537  [   96/  130]
train() client id: f_00008-6-3 loss: 0.728953  [  128/  130]
train() client id: f_00009-0-0 loss: 0.861314  [   32/  118]
train() client id: f_00009-0-1 loss: 0.926712  [   64/  118]
train() client id: f_00009-0-2 loss: 0.775131  [   96/  118]
train() client id: f_00009-1-0 loss: 0.801838  [   32/  118]
train() client id: f_00009-1-1 loss: 0.960830  [   64/  118]
train() client id: f_00009-1-2 loss: 0.755517  [   96/  118]
train() client id: f_00009-2-0 loss: 0.760854  [   32/  118]
train() client id: f_00009-2-1 loss: 0.831805  [   64/  118]
train() client id: f_00009-2-2 loss: 0.902271  [   96/  118]
train() client id: f_00009-3-0 loss: 0.801866  [   32/  118]
train() client id: f_00009-3-1 loss: 0.794529  [   64/  118]
train() client id: f_00009-3-2 loss: 0.719859  [   96/  118]
train() client id: f_00009-4-0 loss: 0.819918  [   32/  118]
train() client id: f_00009-4-1 loss: 0.877451  [   64/  118]
train() client id: f_00009-4-2 loss: 0.897277  [   96/  118]
train() client id: f_00009-5-0 loss: 0.806938  [   32/  118]
train() client id: f_00009-5-1 loss: 0.863301  [   64/  118]
train() client id: f_00009-5-2 loss: 0.656948  [   96/  118]
train() client id: f_00009-6-0 loss: 0.755264  [   32/  118]
train() client id: f_00009-6-1 loss: 0.850448  [   64/  118]
train() client id: f_00009-6-2 loss: 0.737762  [   96/  118]
At round 69 accuracy: 0.6472148541114059
At round 69 training accuracy: 0.5848423876592891
At round 69 training loss: 0.8329833601377179
update_location
xs = [  -3.9056584     4.20031788  365.00902392   18.81129433    0.97929623
    3.95640986 -327.44319194 -306.32485185  349.66397685 -292.06087855]
ys = [ 357.5879595   340.55583871    1.32061395 -327.45517586  319.35018685
  302.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [371.32789142 354.95904264 378.4617967  342.90050597 334.64234768
 318.92328712 342.382731   322.23530396 364.10653205 308.73219566]
dists_bs = [250.22317375 243.97026388 567.25150952 538.53878371 227.60863152
 219.91328691 234.07170231 218.13773441 547.77441202 207.22872031]
uav_gains = [9.47393004e-13 1.13831941e-12 8.81152900e-13 1.32700416e-12
 1.48913730e-12 1.90221719e-12 1.33628878e-12 1.80140553e-12
 1.02406515e-12 2.27098709e-12]
bs_gains = [2.12802637e-11 2.28428757e-11 2.15140531e-12 2.48820711e-12
 2.77437810e-11 3.05485056e-11 2.56517576e-11 3.12498438e-11
 2.37251633e-12 3.60773175e-11]
Round 70
-------------------------------
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.73561523 3.43059311 1.71527241 0.65186493 3.95409396 1.90401388
 0.79238643 2.37939759 1.74146138 1.54414415]
obj_prev = 19.848843077614966
eta_min = 1.1829730818327539e-54	eta_max = 0.9523787254678187
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 4.515956513396188	eta = 0.9090909090909091
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 12.490584420921488	eta = 0.32868077856324013
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.913053448586266	eta = 0.518815529157309
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.145329232998529	eta = 0.5745592509885677
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096743771047384	eta = 0.5784927770574487
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096517585671171	eta = 0.5785112152005008
eta = 0.5785112152005008
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.04516167 0.09498287 0.04444481 0.01541231 0.10967833 0.05233018
 0.019355   0.06415824 0.04659537 0.04229426]
ene_total = [0.7365201  1.0329747  0.74175636 0.39122836 1.17571715 0.60187953
 0.42968967 0.84367489 0.64421599 0.49886084]
ti_comp = [2.36984321 2.57086067 2.35742536 2.41850329 2.57471655 2.57650502
 2.41937284 2.45238669 2.47671475 2.57942159]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.02506317e-06 8.10324115e-06 9.87341466e-07 3.91192234e-08
 1.24389480e-05 1.34919525e-06 7.74201308e-08 2.74447586e-06
 1.03075701e-06 7.10688982e-07]
ene_total = [0.28603811 0.08559175 0.29842467 0.23748935 0.08178872 0.07989408
 0.23662235 0.2037173  0.17943272 0.0769784 ]
optimize_network iter = 0 obj = 1.7659774351917215
eta = 0.5785112152005008
freqs = [ 9528407.84474552 18472970.62942295  9426557.92578857  3186333.13230265
 21299107.60121082 10155265.18397391  4000003.44569053 13080775.34726803
  9406688.61857887  8198399.6009876 ]
eta_min = 0.578511215200502	eta_max = 0.8002666857305597
af = 0.00021898789766085596	bf = 0.7442198316471329	zeta = 0.00024088668742694157	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.78511656e-07 1.41115497e-06 1.71942534e-07 6.81249461e-09
 2.16620522e-06 2.34958278e-07 1.34824820e-08 4.77942182e-07
 1.79503221e-07 1.23764340e-07]
ene_total = [1.32614126 0.39652609 1.38357144 1.10108858 0.3787282  0.37036751
 1.09706736 0.9444053  0.83187829 0.35687372]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 1 obj = 3.726268206640733
eta = 0.8002666857305597
freqs = [ 9456245.97160665 16480383.53188894  9426557.92578857  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052094
  8790118.03381017  7285272.29487025]
eta_min = 0.800266685730563	eta_max = 0.8002666857305585
af = 0.00017982620690873804	bf = 0.7442198316471329	zeta = 0.00019780882759961186	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.75818035e-07 1.12314508e-06 1.71942534e-07 6.33773081e-09
 1.71795736e-06 1.86032089e-07 1.25305500e-08 4.28277123e-07
 1.56742989e-07 9.77302347e-08]
ene_total = [1.32614114 0.39651277 1.38357144 1.10108855 0.37870747 0.37036524
 1.09706732 0.944403   0.83187723 0.35687251]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 2 obj = 3.7262682066407105
eta = 0.8002666857305585
freqs = [ 9456245.97160664 16480383.53188894  9426557.92578855  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052093
  8790118.03381016  7285272.29487025]
Done!
At round 70 energy consumption: 3.726268206640733
At round 70 eta: 0.8002666857305585
At round 70 local rounds: 7.295939587071601
At round 70 global rounds: 21.05003656390746
At round 70 a_n: 3.8618477214328486
gradient difference: 0.573125422000885
train() client id: f_00000-0-0 loss: 1.062262  [   32/  126]
train() client id: f_00000-0-1 loss: 1.443966  [   64/  126]
train() client id: f_00000-0-2 loss: 1.392720  [   96/  126]
train() client id: f_00000-1-0 loss: 1.472837  [   32/  126]
train() client id: f_00000-1-1 loss: 1.169699  [   64/  126]
train() client id: f_00000-1-2 loss: 1.092586  [   96/  126]
train() client id: f_00000-2-0 loss: 1.208863  [   32/  126]
train() client id: f_00000-2-1 loss: 1.029214  [   64/  126]
train() client id: f_00000-2-2 loss: 1.080510  [   96/  126]
train() client id: f_00000-3-0 loss: 1.205370  [   32/  126]
train() client id: f_00000-3-1 loss: 1.017439  [   64/  126]
train() client id: f_00000-3-2 loss: 1.185699  [   96/  126]
train() client id: f_00000-4-0 loss: 0.974984  [   32/  126]
train() client id: f_00000-4-1 loss: 1.075678  [   64/  126]
train() client id: f_00000-4-2 loss: 1.031461  [   96/  126]
train() client id: f_00000-5-0 loss: 0.976562  [   32/  126]
train() client id: f_00000-5-1 loss: 1.092109  [   64/  126]
train() client id: f_00000-5-2 loss: 1.091359  [   96/  126]
train() client id: f_00000-6-0 loss: 1.134417  [   32/  126]
train() client id: f_00000-6-1 loss: 0.952255  [   64/  126]
train() client id: f_00000-6-2 loss: 1.027493  [   96/  126]
train() client id: f_00001-0-0 loss: 0.601457  [   32/  265]
train() client id: f_00001-0-1 loss: 0.513429  [   64/  265]
train() client id: f_00001-0-2 loss: 0.550122  [   96/  265]
train() client id: f_00001-0-3 loss: 0.525051  [  128/  265]
train() client id: f_00001-0-4 loss: 0.665540  [  160/  265]
train() client id: f_00001-0-5 loss: 0.435540  [  192/  265]
train() client id: f_00001-0-6 loss: 0.525338  [  224/  265]
train() client id: f_00001-0-7 loss: 0.558575  [  256/  265]
train() client id: f_00001-1-0 loss: 0.541202  [   32/  265]
train() client id: f_00001-1-1 loss: 0.514643  [   64/  265]
train() client id: f_00001-1-2 loss: 0.538635  [   96/  265]
train() client id: f_00001-1-3 loss: 0.469189  [  128/  265]
train() client id: f_00001-1-4 loss: 0.513777  [  160/  265]
train() client id: f_00001-1-5 loss: 0.604538  [  192/  265]
train() client id: f_00001-1-6 loss: 0.527151  [  224/  265]
train() client id: f_00001-1-7 loss: 0.658367  [  256/  265]
train() client id: f_00001-2-0 loss: 0.568881  [   32/  265]
train() client id: f_00001-2-1 loss: 0.591706  [   64/  265]
train() client id: f_00001-2-2 loss: 0.579032  [   96/  265]
train() client id: f_00001-2-3 loss: 0.683205  [  128/  265]
train() client id: f_00001-2-4 loss: 0.493468  [  160/  265]
train() client id: f_00001-2-5 loss: 0.519125  [  192/  265]
train() client id: f_00001-2-6 loss: 0.458837  [  224/  265]
train() client id: f_00001-2-7 loss: 0.432923  [  256/  265]
train() client id: f_00001-3-0 loss: 0.630931  [   32/  265]
train() client id: f_00001-3-1 loss: 0.455655  [   64/  265]
train() client id: f_00001-3-2 loss: 0.614484  [   96/  265]
train() client id: f_00001-3-3 loss: 0.442286  [  128/  265]
train() client id: f_00001-3-4 loss: 0.598223  [  160/  265]
train() client id: f_00001-3-5 loss: 0.498800  [  192/  265]
train() client id: f_00001-3-6 loss: 0.559287  [  224/  265]
train() client id: f_00001-3-7 loss: 0.547360  [  256/  265]
train() client id: f_00001-4-0 loss: 0.420518  [   32/  265]
train() client id: f_00001-4-1 loss: 0.541554  [   64/  265]
train() client id: f_00001-4-2 loss: 0.607608  [   96/  265]
train() client id: f_00001-4-3 loss: 0.485134  [  128/  265]
train() client id: f_00001-4-4 loss: 0.693589  [  160/  265]
train() client id: f_00001-4-5 loss: 0.551557  [  192/  265]
train() client id: f_00001-4-6 loss: 0.517137  [  224/  265]
train() client id: f_00001-4-7 loss: 0.482487  [  256/  265]
train() client id: f_00001-5-0 loss: 0.592394  [   32/  265]
train() client id: f_00001-5-1 loss: 0.521054  [   64/  265]
train() client id: f_00001-5-2 loss: 0.671542  [   96/  265]
train() client id: f_00001-5-3 loss: 0.502615  [  128/  265]
train() client id: f_00001-5-4 loss: 0.539664  [  160/  265]
train() client id: f_00001-5-5 loss: 0.445802  [  192/  265]
train() client id: f_00001-5-6 loss: 0.553204  [  224/  265]
train() client id: f_00001-5-7 loss: 0.459992  [  256/  265]
train() client id: f_00001-6-0 loss: 0.617745  [   32/  265]
train() client id: f_00001-6-1 loss: 0.633728  [   64/  265]
train() client id: f_00001-6-2 loss: 0.457892  [   96/  265]
train() client id: f_00001-6-3 loss: 0.640961  [  128/  265]
train() client id: f_00001-6-4 loss: 0.509682  [  160/  265]
train() client id: f_00001-6-5 loss: 0.512657  [  192/  265]
train() client id: f_00001-6-6 loss: 0.496159  [  224/  265]
train() client id: f_00001-6-7 loss: 0.507966  [  256/  265]
train() client id: f_00002-0-0 loss: 1.118497  [   32/  124]
train() client id: f_00002-0-1 loss: 0.783614  [   64/  124]
train() client id: f_00002-0-2 loss: 1.179410  [   96/  124]
train() client id: f_00002-1-0 loss: 1.127617  [   32/  124]
train() client id: f_00002-1-1 loss: 1.096619  [   64/  124]
train() client id: f_00002-1-2 loss: 0.932953  [   96/  124]
train() client id: f_00002-2-0 loss: 1.143796  [   32/  124]
train() client id: f_00002-2-1 loss: 1.064419  [   64/  124]
train() client id: f_00002-2-2 loss: 1.088598  [   96/  124]
train() client id: f_00002-3-0 loss: 0.873671  [   32/  124]
train() client id: f_00002-3-1 loss: 1.185986  [   64/  124]
train() client id: f_00002-3-2 loss: 1.029786  [   96/  124]
train() client id: f_00002-4-0 loss: 1.141065  [   32/  124]
train() client id: f_00002-4-1 loss: 0.829490  [   64/  124]
train() client id: f_00002-4-2 loss: 1.000635  [   96/  124]
train() client id: f_00002-5-0 loss: 0.993216  [   32/  124]
train() client id: f_00002-5-1 loss: 0.950691  [   64/  124]
train() client id: f_00002-5-2 loss: 1.044110  [   96/  124]
train() client id: f_00002-6-0 loss: 1.036794  [   32/  124]
train() client id: f_00002-6-1 loss: 1.080631  [   64/  124]
train() client id: f_00002-6-2 loss: 0.803038  [   96/  124]
train() client id: f_00003-0-0 loss: 0.661614  [   32/   43]
train() client id: f_00003-1-0 loss: 0.730980  [   32/   43]
train() client id: f_00003-2-0 loss: 0.691048  [   32/   43]
train() client id: f_00003-3-0 loss: 0.760396  [   32/   43]
train() client id: f_00003-4-0 loss: 0.644720  [   32/   43]
train() client id: f_00003-5-0 loss: 0.737829  [   32/   43]
train() client id: f_00003-6-0 loss: 0.679202  [   32/   43]
train() client id: f_00004-0-0 loss: 0.977564  [   32/  306]
train() client id: f_00004-0-1 loss: 0.834811  [   64/  306]
train() client id: f_00004-0-2 loss: 0.808134  [   96/  306]
train() client id: f_00004-0-3 loss: 0.784666  [  128/  306]
train() client id: f_00004-0-4 loss: 0.997773  [  160/  306]
train() client id: f_00004-0-5 loss: 0.970880  [  192/  306]
train() client id: f_00004-0-6 loss: 0.788366  [  224/  306]
train() client id: f_00004-0-7 loss: 0.876387  [  256/  306]
train() client id: f_00004-0-8 loss: 0.848092  [  288/  306]
train() client id: f_00004-1-0 loss: 0.943705  [   32/  306]
train() client id: f_00004-1-1 loss: 0.846700  [   64/  306]
train() client id: f_00004-1-2 loss: 0.804516  [   96/  306]
train() client id: f_00004-1-3 loss: 0.920990  [  128/  306]
train() client id: f_00004-1-4 loss: 0.902564  [  160/  306]
train() client id: f_00004-1-5 loss: 0.783879  [  192/  306]
train() client id: f_00004-1-6 loss: 0.824151  [  224/  306]
train() client id: f_00004-1-7 loss: 0.790862  [  256/  306]
train() client id: f_00004-1-8 loss: 1.097245  [  288/  306]
train() client id: f_00004-2-0 loss: 0.817895  [   32/  306]
train() client id: f_00004-2-1 loss: 0.943046  [   64/  306]
train() client id: f_00004-2-2 loss: 0.752132  [   96/  306]
train() client id: f_00004-2-3 loss: 0.899705  [  128/  306]
train() client id: f_00004-2-4 loss: 0.861475  [  160/  306]
train() client id: f_00004-2-5 loss: 1.046671  [  192/  306]
train() client id: f_00004-2-6 loss: 0.815702  [  224/  306]
train() client id: f_00004-2-7 loss: 0.866522  [  256/  306]
train() client id: f_00004-2-8 loss: 0.810259  [  288/  306]
train() client id: f_00004-3-0 loss: 0.811856  [   32/  306]
train() client id: f_00004-3-1 loss: 0.884161  [   64/  306]
train() client id: f_00004-3-2 loss: 0.912684  [   96/  306]
train() client id: f_00004-3-3 loss: 0.840888  [  128/  306]
train() client id: f_00004-3-4 loss: 0.822851  [  160/  306]
train() client id: f_00004-3-5 loss: 0.866334  [  192/  306]
train() client id: f_00004-3-6 loss: 0.897195  [  224/  306]
train() client id: f_00004-3-7 loss: 0.815240  [  256/  306]
train() client id: f_00004-3-8 loss: 0.905654  [  288/  306]
train() client id: f_00004-4-0 loss: 0.762775  [   32/  306]
train() client id: f_00004-4-1 loss: 0.921598  [   64/  306]
train() client id: f_00004-4-2 loss: 0.933424  [   96/  306]
train() client id: f_00004-4-3 loss: 0.777741  [  128/  306]
train() client id: f_00004-4-4 loss: 0.793141  [  160/  306]
train() client id: f_00004-4-5 loss: 0.874861  [  192/  306]
train() client id: f_00004-4-6 loss: 0.947618  [  224/  306]
train() client id: f_00004-4-7 loss: 0.923906  [  256/  306]
train() client id: f_00004-4-8 loss: 0.833199  [  288/  306]
train() client id: f_00004-5-0 loss: 0.850733  [   32/  306]
train() client id: f_00004-5-1 loss: 0.811879  [   64/  306]
train() client id: f_00004-5-2 loss: 0.825288  [   96/  306]
train() client id: f_00004-5-3 loss: 0.883318  [  128/  306]
train() client id: f_00004-5-4 loss: 0.886331  [  160/  306]
train() client id: f_00004-5-5 loss: 0.878507  [  192/  306]
train() client id: f_00004-5-6 loss: 0.695540  [  224/  306]
train() client id: f_00004-5-7 loss: 0.887289  [  256/  306]
train() client id: f_00004-5-8 loss: 0.984940  [  288/  306]
train() client id: f_00004-6-0 loss: 0.822739  [   32/  306]
train() client id: f_00004-6-1 loss: 1.009870  [   64/  306]
train() client id: f_00004-6-2 loss: 0.862202  [   96/  306]
train() client id: f_00004-6-3 loss: 1.006368  [  128/  306]
train() client id: f_00004-6-4 loss: 0.753444  [  160/  306]
train() client id: f_00004-6-5 loss: 0.778388  [  192/  306]
train() client id: f_00004-6-6 loss: 1.002827  [  224/  306]
train() client id: f_00004-6-7 loss: 0.753633  [  256/  306]
train() client id: f_00004-6-8 loss: 0.811280  [  288/  306]
train() client id: f_00005-0-0 loss: 0.749013  [   32/  146]
train() client id: f_00005-0-1 loss: 0.528065  [   64/  146]
train() client id: f_00005-0-2 loss: 0.517653  [   96/  146]
train() client id: f_00005-0-3 loss: 0.763896  [  128/  146]
train() client id: f_00005-1-0 loss: 0.624187  [   32/  146]
train() client id: f_00005-1-1 loss: 0.676258  [   64/  146]
train() client id: f_00005-1-2 loss: 0.535376  [   96/  146]
train() client id: f_00005-1-3 loss: 0.831069  [  128/  146]
train() client id: f_00005-2-0 loss: 0.644412  [   32/  146]
train() client id: f_00005-2-1 loss: 0.444445  [   64/  146]
train() client id: f_00005-2-2 loss: 0.676187  [   96/  146]
train() client id: f_00005-2-3 loss: 0.831849  [  128/  146]
train() client id: f_00005-3-0 loss: 0.514496  [   32/  146]
train() client id: f_00005-3-1 loss: 0.642820  [   64/  146]
train() client id: f_00005-3-2 loss: 0.507522  [   96/  146]
train() client id: f_00005-3-3 loss: 0.730125  [  128/  146]
train() client id: f_00005-4-0 loss: 0.729686  [   32/  146]
train() client id: f_00005-4-1 loss: 0.453171  [   64/  146]
train() client id: f_00005-4-2 loss: 0.796439  [   96/  146]
train() client id: f_00005-4-3 loss: 0.624694  [  128/  146]
train() client id: f_00005-5-0 loss: 0.633669  [   32/  146]
train() client id: f_00005-5-1 loss: 0.768021  [   64/  146]
train() client id: f_00005-5-2 loss: 0.750111  [   96/  146]
train() client id: f_00005-5-3 loss: 0.639393  [  128/  146]
train() client id: f_00005-6-0 loss: 0.814113  [   32/  146]
train() client id: f_00005-6-1 loss: 0.567826  [   64/  146]
train() client id: f_00005-6-2 loss: 0.390678  [   96/  146]
train() client id: f_00005-6-3 loss: 0.825109  [  128/  146]
train() client id: f_00006-0-0 loss: 0.494573  [   32/   54]
train() client id: f_00006-1-0 loss: 0.470050  [   32/   54]
train() client id: f_00006-2-0 loss: 0.486079  [   32/   54]
train() client id: f_00006-3-0 loss: 0.550384  [   32/   54]
train() client id: f_00006-4-0 loss: 0.554540  [   32/   54]
train() client id: f_00006-5-0 loss: 0.495377  [   32/   54]
train() client id: f_00006-6-0 loss: 0.489477  [   32/   54]
train() client id: f_00007-0-0 loss: 0.477936  [   32/  179]
train() client id: f_00007-0-1 loss: 0.574424  [   64/  179]
train() client id: f_00007-0-2 loss: 0.627427  [   96/  179]
train() client id: f_00007-0-3 loss: 0.577684  [  128/  179]
train() client id: f_00007-0-4 loss: 0.339883  [  160/  179]
train() client id: f_00007-1-0 loss: 0.354412  [   32/  179]
train() client id: f_00007-1-1 loss: 0.517274  [   64/  179]
train() client id: f_00007-1-2 loss: 0.494568  [   96/  179]
train() client id: f_00007-1-3 loss: 0.662024  [  128/  179]
train() client id: f_00007-1-4 loss: 0.638362  [  160/  179]
train() client id: f_00007-2-0 loss: 0.718155  [   32/  179]
train() client id: f_00007-2-1 loss: 0.357083  [   64/  179]
train() client id: f_00007-2-2 loss: 0.615007  [   96/  179]
train() client id: f_00007-2-3 loss: 0.468745  [  128/  179]
train() client id: f_00007-2-4 loss: 0.554654  [  160/  179]
train() client id: f_00007-3-0 loss: 0.480920  [   32/  179]
train() client id: f_00007-3-1 loss: 0.547197  [   64/  179]
train() client id: f_00007-3-2 loss: 0.688043  [   96/  179]
train() client id: f_00007-3-3 loss: 0.415332  [  128/  179]
train() client id: f_00007-3-4 loss: 0.416819  [  160/  179]
train() client id: f_00007-4-0 loss: 0.330365  [   32/  179]
train() client id: f_00007-4-1 loss: 0.588874  [   64/  179]
train() client id: f_00007-4-2 loss: 0.497485  [   96/  179]
train() client id: f_00007-4-3 loss: 0.572410  [  128/  179]
train() client id: f_00007-4-4 loss: 0.425164  [  160/  179]
train() client id: f_00007-5-0 loss: 0.387472  [   32/  179]
train() client id: f_00007-5-1 loss: 0.441594  [   64/  179]
train() client id: f_00007-5-2 loss: 0.459280  [   96/  179]
train() client id: f_00007-5-3 loss: 0.697511  [  128/  179]
train() client id: f_00007-5-4 loss: 0.455134  [  160/  179]
train() client id: f_00007-6-0 loss: 0.605581  [   32/  179]
train() client id: f_00007-6-1 loss: 0.392664  [   64/  179]
train() client id: f_00007-6-2 loss: 0.595293  [   96/  179]
train() client id: f_00007-6-3 loss: 0.356846  [  128/  179]
train() client id: f_00007-6-4 loss: 0.498296  [  160/  179]
train() client id: f_00008-0-0 loss: 0.736093  [   32/  130]
train() client id: f_00008-0-1 loss: 0.675866  [   64/  130]
train() client id: f_00008-0-2 loss: 0.879649  [   96/  130]
train() client id: f_00008-0-3 loss: 0.641215  [  128/  130]
train() client id: f_00008-1-0 loss: 0.651900  [   32/  130]
train() client id: f_00008-1-1 loss: 0.825121  [   64/  130]
train() client id: f_00008-1-2 loss: 0.699426  [   96/  130]
train() client id: f_00008-1-3 loss: 0.783691  [  128/  130]
train() client id: f_00008-2-0 loss: 0.767718  [   32/  130]
train() client id: f_00008-2-1 loss: 0.718247  [   64/  130]
train() client id: f_00008-2-2 loss: 0.704106  [   96/  130]
train() client id: f_00008-2-3 loss: 0.774406  [  128/  130]
train() client id: f_00008-3-0 loss: 0.712325  [   32/  130]
train() client id: f_00008-3-1 loss: 0.831202  [   64/  130]
train() client id: f_00008-3-2 loss: 0.830456  [   96/  130]
train() client id: f_00008-3-3 loss: 0.591020  [  128/  130]
train() client id: f_00008-4-0 loss: 0.686599  [   32/  130]
train() client id: f_00008-4-1 loss: 0.871193  [   64/  130]
train() client id: f_00008-4-2 loss: 0.726462  [   96/  130]
train() client id: f_00008-4-3 loss: 0.681419  [  128/  130]
train() client id: f_00008-5-0 loss: 0.694220  [   32/  130]
train() client id: f_00008-5-1 loss: 0.841157  [   64/  130]
train() client id: f_00008-5-2 loss: 0.610729  [   96/  130]
train() client id: f_00008-5-3 loss: 0.819006  [  128/  130]
train() client id: f_00008-6-0 loss: 0.699698  [   32/  130]
train() client id: f_00008-6-1 loss: 0.700263  [   64/  130]
train() client id: f_00008-6-2 loss: 0.689965  [   96/  130]
train() client id: f_00008-6-3 loss: 0.877730  [  128/  130]
train() client id: f_00009-0-0 loss: 0.854406  [   32/  118]
train() client id: f_00009-0-1 loss: 1.147474  [   64/  118]
train() client id: f_00009-0-2 loss: 0.830788  [   96/  118]
train() client id: f_00009-1-0 loss: 0.769425  [   32/  118]
train() client id: f_00009-1-1 loss: 0.883789  [   64/  118]
train() client id: f_00009-1-2 loss: 1.026971  [   96/  118]
train() client id: f_00009-2-0 loss: 0.922264  [   32/  118]
train() client id: f_00009-2-1 loss: 0.788787  [   64/  118]
train() client id: f_00009-2-2 loss: 1.036427  [   96/  118]
train() client id: f_00009-3-0 loss: 0.807283  [   32/  118]
train() client id: f_00009-3-1 loss: 0.971929  [   64/  118]
train() client id: f_00009-3-2 loss: 0.881217  [   96/  118]
train() client id: f_00009-4-0 loss: 0.905183  [   32/  118]
train() client id: f_00009-4-1 loss: 0.918609  [   64/  118]
train() client id: f_00009-4-2 loss: 0.878751  [   96/  118]
train() client id: f_00009-5-0 loss: 0.994871  [   32/  118]
train() client id: f_00009-5-1 loss: 0.849624  [   64/  118]
train() client id: f_00009-5-2 loss: 0.729082  [   96/  118]
train() client id: f_00009-6-0 loss: 0.972069  [   32/  118]
train() client id: f_00009-6-1 loss: 0.714502  [   64/  118]
train() client id: f_00009-6-2 loss: 0.886824  [   96/  118]
At round 70 accuracy: 0.6445623342175066
At round 70 training accuracy: 0.5835010060362174
At round 70 training loss: 0.8272509355902532
update_location
xs = [  -3.9056584     4.20031788  370.00902392   18.81129433    0.97929623
    3.95640986 -332.44319194 -311.32485185  354.66397685 -297.06087855]
ys = [ 362.5879595   345.55583871    1.32061395 -332.45517586  324.35018685
  307.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [376.14529445 359.75891974 383.2863444  347.67845598 339.41715149
 323.67453495 347.16763445 326.99210944 368.91083807 313.46638962]
dists_bs = [253.89469525 247.39047687 572.01343104 543.20675007 230.81202527
 222.85644543 237.35836564 221.18390469 552.5653321  210.09367269]
uav_gains = [9.01731222e-13 1.07572493e-12 8.40891865e-13 1.24623615e-12
 1.39163386e-12 1.76012882e-12 1.25446710e-12 1.67039793e-12
 9.71887012e-13 2.08779148e-12]
bs_gains = [2.04297913e-11 2.19695797e-11 2.10163190e-12 2.42879938e-12
 2.66790571e-11 2.94322543e-11 2.46695578e-11 3.00596713e-11
 2.31536738e-12 3.47166460e-11]
Round 71
-------------------------------
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.59747704 3.1514096  1.57881481 0.60189948 3.63223305 1.7491537
 0.73097414 2.18851291 1.6003418  1.41859333]
obj_prev = 18.24940985510058
eta_min = 2.2153155362897003e-59	eta_max = 0.954951968660827
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 4.14802660303202	eta = 0.909090909090909
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 11.665880506041221	eta = 0.323244634087488
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 7.328813927254386	eta = 0.5145352730897301
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.605790167817666	eta = 0.5708527185521312
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.559990536518958	eta = 0.5748382188192438
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.55977636966569	eta = 0.5748569864243457
eta = 0.5748569864243457
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.04568455 0.09608258 0.0449594  0.01559076 0.11094819 0.05293607
 0.01957909 0.06490107 0.04713485 0.04278394]
ene_total = [0.68305847 0.95139002 0.68780539 0.3652074  1.08286069 0.55421391
 0.40065058 0.78194702 0.59332082 0.45932206]
ti_comp = [2.6166745  2.82525406 2.60419719 2.66565241 2.82917673 2.83103203
 2.66651706 2.69997525 2.72996789 2.83397532]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [8.70339434e-07 6.94542833e-06 8.37517236e-07 3.33330274e-08
 1.06640102e-05 1.15676577e-06 6.59734165e-08 2.34377520e-06
 8.78195905e-07 6.09439114e-07]
ene_total = [0.26808419 0.07867313 0.27941784 0.2235868  0.07514369 0.07337205
 0.22280168 0.19243014 0.16517258 0.0706935 ]
optimize_network iter = 0 obj = 1.6493755938248962
eta = 0.5748569864243457
freqs = [ 8729505.68014144 17004237.96072038  8632103.28335727  2924379.60099508
 19607858.17168588  9349252.32733673  3671285.85880158 12018826.22316111
  8632858.47902665  7548397.32371458]
eta_min = 0.5748569864243458	eta_max = 0.8123930873707683
af = 0.00017023437323749214	bf = 0.7011262629418353	zeta = 0.00018725781056124136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.49832217e-07 1.19568169e-06 1.44181752e-07 5.73840643e-09
 1.83584959e-06 1.99141592e-07 1.13575726e-08 4.03489744e-07
 1.51184738e-07 1.04917243e-07]
ene_total = [1.25368474 0.36767638 1.30668851 1.04561886 0.35103995 0.34308906
 1.04194602 0.8998312  0.77241077 0.33058187]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 1 obj = 3.7373617634764136
eta = 0.8123930873707683
freqs = [ 8660759.82156696 15044717.2754607   8632103.28335727  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144383
  8017957.73562188  6650755.80408876]
eta_min = 0.8123930873707704	eta_max = 0.8123930873707671
af = 0.0001378387179910577	bf = 0.7011262629418353	zeta = 0.0001516225897901635	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.47481618e-07 9.35985870e-07 1.44181752e-07 5.32207417e-09
 1.43171779e-06 1.55028805e-07 1.05228684e-08 3.59748455e-07
 1.30414606e-07 8.14478020e-08]
ene_total = [1.25368464 0.36766535 1.30668851 1.04561884 0.35102278 0.34308719
 1.04194599 0.89982934 0.77240989 0.33058088]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 2 obj = 3.7373617634763896
eta = 0.8123930873707671
freqs = [ 8660759.82156695 15044717.2754607   8632103.28335726  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144382
  8017957.73562187  6650755.80408876]
Done!
At round 71 energy consumption: 3.7373617634764136
At round 71 eta: 0.8123930873707671
At round 71 local rounds: 6.803476774877765
At round 71 global rounds: 20.584783723108377
At round 71 a_n: 3.5193018744635296
gradient difference: 0.6669342517852783
train() client id: f_00000-0-0 loss: 1.031304  [   32/  126]
train() client id: f_00000-0-1 loss: 0.941470  [   64/  126]
train() client id: f_00000-0-2 loss: 1.007474  [   96/  126]
train() client id: f_00000-1-0 loss: 0.793064  [   32/  126]
train() client id: f_00000-1-1 loss: 0.984059  [   64/  126]
train() client id: f_00000-1-2 loss: 0.931496  [   96/  126]
train() client id: f_00000-2-0 loss: 1.002041  [   32/  126]
train() client id: f_00000-2-1 loss: 0.723576  [   64/  126]
train() client id: f_00000-2-2 loss: 0.834913  [   96/  126]
train() client id: f_00000-3-0 loss: 1.067527  [   32/  126]
train() client id: f_00000-3-1 loss: 0.795218  [   64/  126]
train() client id: f_00000-3-2 loss: 0.726502  [   96/  126]
train() client id: f_00000-4-0 loss: 0.840602  [   32/  126]
train() client id: f_00000-4-1 loss: 0.856485  [   64/  126]
train() client id: f_00000-4-2 loss: 0.782787  [   96/  126]
train() client id: f_00000-5-0 loss: 0.898660  [   32/  126]
train() client id: f_00000-5-1 loss: 0.708248  [   64/  126]
train() client id: f_00000-5-2 loss: 0.904927  [   96/  126]
train() client id: f_00001-0-0 loss: 0.533894  [   32/  265]
train() client id: f_00001-0-1 loss: 0.536361  [   64/  265]
train() client id: f_00001-0-2 loss: 0.536694  [   96/  265]
train() client id: f_00001-0-3 loss: 0.586131  [  128/  265]
train() client id: f_00001-0-4 loss: 0.694393  [  160/  265]
train() client id: f_00001-0-5 loss: 0.569956  [  192/  265]
train() client id: f_00001-0-6 loss: 0.443812  [  224/  265]
train() client id: f_00001-0-7 loss: 0.455694  [  256/  265]
train() client id: f_00001-1-0 loss: 0.488068  [   32/  265]
train() client id: f_00001-1-1 loss: 0.613952  [   64/  265]
train() client id: f_00001-1-2 loss: 0.614730  [   96/  265]
train() client id: f_00001-1-3 loss: 0.452465  [  128/  265]
train() client id: f_00001-1-4 loss: 0.538251  [  160/  265]
train() client id: f_00001-1-5 loss: 0.526774  [  192/  265]
train() client id: f_00001-1-6 loss: 0.577626  [  224/  265]
train() client id: f_00001-1-7 loss: 0.567001  [  256/  265]
train() client id: f_00001-2-0 loss: 0.519924  [   32/  265]
train() client id: f_00001-2-1 loss: 0.524308  [   64/  265]
train() client id: f_00001-2-2 loss: 0.561881  [   96/  265]
train() client id: f_00001-2-3 loss: 0.529073  [  128/  265]
train() client id: f_00001-2-4 loss: 0.664142  [  160/  265]
train() client id: f_00001-2-5 loss: 0.578282  [  192/  265]
train() client id: f_00001-2-6 loss: 0.452929  [  224/  265]
train() client id: f_00001-2-7 loss: 0.515543  [  256/  265]
train() client id: f_00001-3-0 loss: 0.457494  [   32/  265]
train() client id: f_00001-3-1 loss: 0.562593  [   64/  265]
train() client id: f_00001-3-2 loss: 0.537680  [   96/  265]
train() client id: f_00001-3-3 loss: 0.597093  [  128/  265]
train() client id: f_00001-3-4 loss: 0.447125  [  160/  265]
train() client id: f_00001-3-5 loss: 0.518722  [  192/  265]
train() client id: f_00001-3-6 loss: 0.499714  [  224/  265]
train() client id: f_00001-3-7 loss: 0.660932  [  256/  265]
train() client id: f_00001-4-0 loss: 0.678690  [   32/  265]
train() client id: f_00001-4-1 loss: 0.441540  [   64/  265]
train() client id: f_00001-4-2 loss: 0.562337  [   96/  265]
train() client id: f_00001-4-3 loss: 0.451380  [  128/  265]
train() client id: f_00001-4-4 loss: 0.543431  [  160/  265]
train() client id: f_00001-4-5 loss: 0.503431  [  192/  265]
train() client id: f_00001-4-6 loss: 0.540661  [  224/  265]
train() client id: f_00001-4-7 loss: 0.517347  [  256/  265]
train() client id: f_00001-5-0 loss: 0.518504  [   32/  265]
train() client id: f_00001-5-1 loss: 0.503325  [   64/  265]
train() client id: f_00001-5-2 loss: 0.683638  [   96/  265]
train() client id: f_00001-5-3 loss: 0.550610  [  128/  265]
train() client id: f_00001-5-4 loss: 0.554363  [  160/  265]
train() client id: f_00001-5-5 loss: 0.537375  [  192/  265]
train() client id: f_00001-5-6 loss: 0.484213  [  224/  265]
train() client id: f_00001-5-7 loss: 0.495566  [  256/  265]
train() client id: f_00002-0-0 loss: 1.177424  [   32/  124]
train() client id: f_00002-0-1 loss: 1.245600  [   64/  124]
train() client id: f_00002-0-2 loss: 1.101332  [   96/  124]
train() client id: f_00002-1-0 loss: 1.171119  [   32/  124]
train() client id: f_00002-1-1 loss: 1.186239  [   64/  124]
train() client id: f_00002-1-2 loss: 1.137773  [   96/  124]
train() client id: f_00002-2-0 loss: 1.125545  [   32/  124]
train() client id: f_00002-2-1 loss: 1.057532  [   64/  124]
train() client id: f_00002-2-2 loss: 1.296863  [   96/  124]
train() client id: f_00002-3-0 loss: 1.045911  [   32/  124]
train() client id: f_00002-3-1 loss: 1.235857  [   64/  124]
train() client id: f_00002-3-2 loss: 1.055675  [   96/  124]
train() client id: f_00002-4-0 loss: 1.152752  [   32/  124]
train() client id: f_00002-4-1 loss: 1.234655  [   64/  124]
train() client id: f_00002-4-2 loss: 1.207750  [   96/  124]
train() client id: f_00002-5-0 loss: 1.074611  [   32/  124]
train() client id: f_00002-5-1 loss: 1.232649  [   64/  124]
train() client id: f_00002-5-2 loss: 1.081019  [   96/  124]
train() client id: f_00003-0-0 loss: 0.627191  [   32/   43]
train() client id: f_00003-1-0 loss: 0.670745  [   32/   43]
train() client id: f_00003-2-0 loss: 0.571633  [   32/   43]
train() client id: f_00003-3-0 loss: 0.775967  [   32/   43]
train() client id: f_00003-4-0 loss: 0.696739  [   32/   43]
train() client id: f_00003-5-0 loss: 0.641183  [   32/   43]
train() client id: f_00004-0-0 loss: 0.936568  [   32/  306]
train() client id: f_00004-0-1 loss: 0.692188  [   64/  306]
train() client id: f_00004-0-2 loss: 0.853859  [   96/  306]
train() client id: f_00004-0-3 loss: 0.886291  [  128/  306]
train() client id: f_00004-0-4 loss: 0.749396  [  160/  306]
train() client id: f_00004-0-5 loss: 0.976939  [  192/  306]
train() client id: f_00004-0-6 loss: 0.828355  [  224/  306]
train() client id: f_00004-0-7 loss: 0.948135  [  256/  306]
train() client id: f_00004-0-8 loss: 0.869296  [  288/  306]
train() client id: f_00004-1-0 loss: 0.856615  [   32/  306]
train() client id: f_00004-1-1 loss: 0.829107  [   64/  306]
train() client id: f_00004-1-2 loss: 0.751666  [   96/  306]
train() client id: f_00004-1-3 loss: 0.933853  [  128/  306]
train() client id: f_00004-1-4 loss: 0.940741  [  160/  306]
train() client id: f_00004-1-5 loss: 0.852287  [  192/  306]
train() client id: f_00004-1-6 loss: 0.940838  [  224/  306]
train() client id: f_00004-1-7 loss: 0.811182  [  256/  306]
train() client id: f_00004-1-8 loss: 0.747185  [  288/  306]
train() client id: f_00004-2-0 loss: 0.877466  [   32/  306]
train() client id: f_00004-2-1 loss: 0.833389  [   64/  306]
train() client id: f_00004-2-2 loss: 0.761360  [   96/  306]
train() client id: f_00004-2-3 loss: 0.845357  [  128/  306]
train() client id: f_00004-2-4 loss: 0.923917  [  160/  306]
train() client id: f_00004-2-5 loss: 0.875104  [  192/  306]
train() client id: f_00004-2-6 loss: 0.887649  [  224/  306]
train() client id: f_00004-2-7 loss: 0.882313  [  256/  306]
train() client id: f_00004-2-8 loss: 0.949160  [  288/  306]
train() client id: f_00004-3-0 loss: 1.013818  [   32/  306]
train() client id: f_00004-3-1 loss: 0.803540  [   64/  306]
train() client id: f_00004-3-2 loss: 0.847902  [   96/  306]
train() client id: f_00004-3-3 loss: 0.848830  [  128/  306]
train() client id: f_00004-3-4 loss: 0.923929  [  160/  306]
train() client id: f_00004-3-5 loss: 0.915170  [  192/  306]
train() client id: f_00004-3-6 loss: 0.812931  [  224/  306]
train() client id: f_00004-3-7 loss: 0.772076  [  256/  306]
train() client id: f_00004-3-8 loss: 0.871829  [  288/  306]
train() client id: f_00004-4-0 loss: 0.961572  [   32/  306]
train() client id: f_00004-4-1 loss: 1.062613  [   64/  306]
train() client id: f_00004-4-2 loss: 0.818424  [   96/  306]
train() client id: f_00004-4-3 loss: 0.883553  [  128/  306]
train() client id: f_00004-4-4 loss: 0.841723  [  160/  306]
train() client id: f_00004-4-5 loss: 0.778964  [  192/  306]
train() client id: f_00004-4-6 loss: 0.874622  [  224/  306]
train() client id: f_00004-4-7 loss: 0.826123  [  256/  306]
train() client id: f_00004-4-8 loss: 0.694338  [  288/  306]
train() client id: f_00004-5-0 loss: 0.826467  [   32/  306]
train() client id: f_00004-5-1 loss: 0.830015  [   64/  306]
train() client id: f_00004-5-2 loss: 0.868481  [   96/  306]
train() client id: f_00004-5-3 loss: 0.853148  [  128/  306]
train() client id: f_00004-5-4 loss: 0.775283  [  160/  306]
train() client id: f_00004-5-5 loss: 0.950143  [  192/  306]
train() client id: f_00004-5-6 loss: 0.859166  [  224/  306]
train() client id: f_00004-5-7 loss: 0.957566  [  256/  306]
train() client id: f_00004-5-8 loss: 0.862722  [  288/  306]
train() client id: f_00005-0-0 loss: 0.536376  [   32/  146]
train() client id: f_00005-0-1 loss: 0.444162  [   64/  146]
train() client id: f_00005-0-2 loss: 0.524783  [   96/  146]
train() client id: f_00005-0-3 loss: 0.456837  [  128/  146]
train() client id: f_00005-1-0 loss: 0.391312  [   32/  146]
train() client id: f_00005-1-1 loss: 0.372549  [   64/  146]
train() client id: f_00005-1-2 loss: 0.632582  [   96/  146]
train() client id: f_00005-1-3 loss: 0.575992  [  128/  146]
train() client id: f_00005-2-0 loss: 0.582790  [   32/  146]
train() client id: f_00005-2-1 loss: 0.608707  [   64/  146]
train() client id: f_00005-2-2 loss: 0.580748  [   96/  146]
train() client id: f_00005-2-3 loss: 0.244775  [  128/  146]
train() client id: f_00005-3-0 loss: 0.651106  [   32/  146]
train() client id: f_00005-3-1 loss: 0.458954  [   64/  146]
train() client id: f_00005-3-2 loss: 0.288206  [   96/  146]
train() client id: f_00005-3-3 loss: 0.290870  [  128/  146]
train() client id: f_00005-4-0 loss: 0.514744  [   32/  146]
train() client id: f_00005-4-1 loss: 0.492378  [   64/  146]
train() client id: f_00005-4-2 loss: 0.864141  [   96/  146]
train() client id: f_00005-4-3 loss: 0.317969  [  128/  146]
train() client id: f_00005-5-0 loss: 0.292200  [   32/  146]
train() client id: f_00005-5-1 loss: 0.407811  [   64/  146]
train() client id: f_00005-5-2 loss: 0.691157  [   96/  146]
train() client id: f_00005-5-3 loss: 0.552679  [  128/  146]
train() client id: f_00006-0-0 loss: 0.523031  [   32/   54]
train() client id: f_00006-1-0 loss: 0.510881  [   32/   54]
train() client id: f_00006-2-0 loss: 0.527571  [   32/   54]
train() client id: f_00006-3-0 loss: 0.486205  [   32/   54]
train() client id: f_00006-4-0 loss: 0.544145  [   32/   54]
train() client id: f_00006-5-0 loss: 0.453974  [   32/   54]
train() client id: f_00007-0-0 loss: 0.492781  [   32/  179]
train() client id: f_00007-0-1 loss: 0.439880  [   64/  179]
train() client id: f_00007-0-2 loss: 0.826856  [   96/  179]
train() client id: f_00007-0-3 loss: 0.415963  [  128/  179]
train() client id: f_00007-0-4 loss: 0.723542  [  160/  179]
train() client id: f_00007-1-0 loss: 0.451996  [   32/  179]
train() client id: f_00007-1-1 loss: 0.500977  [   64/  179]
train() client id: f_00007-1-2 loss: 0.559284  [   96/  179]
train() client id: f_00007-1-3 loss: 0.793763  [  128/  179]
train() client id: f_00007-1-4 loss: 0.638059  [  160/  179]
train() client id: f_00007-2-0 loss: 0.534818  [   32/  179]
train() client id: f_00007-2-1 loss: 0.404636  [   64/  179]
train() client id: f_00007-2-2 loss: 0.762251  [   96/  179]
train() client id: f_00007-2-3 loss: 0.500119  [  128/  179]
train() client id: f_00007-2-4 loss: 0.623178  [  160/  179]
train() client id: f_00007-3-0 loss: 0.492002  [   32/  179]
train() client id: f_00007-3-1 loss: 0.550913  [   64/  179]
train() client id: f_00007-3-2 loss: 0.474528  [   96/  179]
train() client id: f_00007-3-3 loss: 0.395385  [  128/  179]
train() client id: f_00007-3-4 loss: 0.669539  [  160/  179]
train() client id: f_00007-4-0 loss: 0.417212  [   32/  179]
train() client id: f_00007-4-1 loss: 0.408538  [   64/  179]
train() client id: f_00007-4-2 loss: 0.531417  [   96/  179]
train() client id: f_00007-4-3 loss: 0.640264  [  128/  179]
train() client id: f_00007-4-4 loss: 0.740715  [  160/  179]
train() client id: f_00007-5-0 loss: 0.838828  [   32/  179]
train() client id: f_00007-5-1 loss: 0.357914  [   64/  179]
train() client id: f_00007-5-2 loss: 0.385366  [   96/  179]
train() client id: f_00007-5-3 loss: 0.692053  [  128/  179]
train() client id: f_00007-5-4 loss: 0.387650  [  160/  179]
train() client id: f_00008-0-0 loss: 0.649547  [   32/  130]
train() client id: f_00008-0-1 loss: 0.646515  [   64/  130]
train() client id: f_00008-0-2 loss: 0.629350  [   96/  130]
train() client id: f_00008-0-3 loss: 0.534375  [  128/  130]
train() client id: f_00008-1-0 loss: 0.545458  [   32/  130]
train() client id: f_00008-1-1 loss: 0.678998  [   64/  130]
train() client id: f_00008-1-2 loss: 0.654709  [   96/  130]
train() client id: f_00008-1-3 loss: 0.562780  [  128/  130]
train() client id: f_00008-2-0 loss: 0.604425  [   32/  130]
train() client id: f_00008-2-1 loss: 0.561593  [   64/  130]
train() client id: f_00008-2-2 loss: 0.674206  [   96/  130]
train() client id: f_00008-2-3 loss: 0.618213  [  128/  130]
train() client id: f_00008-3-0 loss: 0.493229  [   32/  130]
train() client id: f_00008-3-1 loss: 0.633208  [   64/  130]
train() client id: f_00008-3-2 loss: 0.773521  [   96/  130]
train() client id: f_00008-3-3 loss: 0.521144  [  128/  130]
train() client id: f_00008-4-0 loss: 0.566142  [   32/  130]
train() client id: f_00008-4-1 loss: 0.624740  [   64/  130]
train() client id: f_00008-4-2 loss: 0.669215  [   96/  130]
train() client id: f_00008-4-3 loss: 0.610486  [  128/  130]
train() client id: f_00008-5-0 loss: 0.579359  [   32/  130]
train() client id: f_00008-5-1 loss: 0.639270  [   64/  130]
train() client id: f_00008-5-2 loss: 0.619599  [   96/  130]
train() client id: f_00008-5-3 loss: 0.642061  [  128/  130]
train() client id: f_00009-0-0 loss: 0.901013  [   32/  118]
train() client id: f_00009-0-1 loss: 0.867436  [   64/  118]
train() client id: f_00009-0-2 loss: 0.890990  [   96/  118]
train() client id: f_00009-1-0 loss: 0.904871  [   32/  118]
train() client id: f_00009-1-1 loss: 0.847045  [   64/  118]
train() client id: f_00009-1-2 loss: 0.838293  [   96/  118]
train() client id: f_00009-2-0 loss: 0.963471  [   32/  118]
train() client id: f_00009-2-1 loss: 0.671715  [   64/  118]
train() client id: f_00009-2-2 loss: 0.977753  [   96/  118]
train() client id: f_00009-3-0 loss: 0.790955  [   32/  118]
train() client id: f_00009-3-1 loss: 0.794382  [   64/  118]
train() client id: f_00009-3-2 loss: 0.806848  [   96/  118]
train() client id: f_00009-4-0 loss: 0.862857  [   32/  118]
train() client id: f_00009-4-1 loss: 0.960800  [   64/  118]
train() client id: f_00009-4-2 loss: 0.695810  [   96/  118]
train() client id: f_00009-5-0 loss: 0.759998  [   32/  118]
train() client id: f_00009-5-1 loss: 0.910934  [   64/  118]
train() client id: f_00009-5-2 loss: 0.715890  [   96/  118]
At round 71 accuracy: 0.6419098143236074
At round 71 training accuracy: 0.5888665325285044
At round 71 training loss: 0.829069375883219
update_location
xs = [  -3.9056584     4.20031788  375.00902392   18.81129433    0.97929623
    3.95640986 -337.44319194 -316.32485185  359.66397685 -302.06087855]
ys = [ 367.5879595   350.55583871    1.32061395 -337.45517586  329.35018685
  312.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [380.96740298 364.56417641 388.11533343 352.46256611 344.19835066
 328.43316837 351.95851791 331.75606724 373.72027804 318.20871485]
dists_bs = [257.61093895 250.86372084 576.77938202 547.88057556 234.07839046
 225.87194774 240.70402087 224.30017435 557.35992501 213.03746171]
uav_gains = [8.59823011e-13 1.01888229e-12 8.03777175e-13 1.17344140e-12
 1.30420376e-12 1.63364151e-12 1.18076150e-12 1.55363346e-12
 9.24212893e-13 1.92508765e-12]
bs_gains = [1.96152589e-11 2.11284714e-11 2.05336828e-12 2.37122932e-12
 2.56497054e-11 2.83452084e-11 2.37214220e-11 2.89048802e-11
 2.26002902e-12 3.33900711e-11]
Round 72
-------------------------------
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.45877326 2.87217542 1.44178771 0.55140047 3.31032656 1.59425308
 0.66902829 1.99714791 1.45909395 1.29300424]
obj_prev = 16.646990883599326
eta_min = 4.971520837874105e-65	eta_max = 0.9577477121135591
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 3.7800966926678474	eta = 0.9090909090909091
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 10.808661784694127	eta = 0.3179349680138224
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.733867507222867	eta = 0.5103236045412168
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.058721545799979	eta = 0.5671908690326205
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015928897442357	eta = 0.5712254245973457
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015728044345185	eta = 0.5712444966689002
eta = 0.5712444966689002
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.04620474 0.09717663 0.04547133 0.01576828 0.11221151 0.05353882
 0.01980203 0.06564007 0.04767156 0.04327111]
ene_total = [0.62840586 0.86936    0.63267678 0.33813836 0.98949538 0.50632297
 0.37054313 0.71901402 0.54216497 0.41960659]
ti_comp = [2.91272976 3.128895   2.90019242 2.96199725 3.13288317 3.13480384
 2.96285607 2.99669538 3.03246495 3.13777295]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [7.26673296e-07 5.85846172e-06 6.98617374e-07 2.79295552e-08
 8.99712661e-06 9.76035873e-07 5.52825571e-08 1.96834785e-06
 7.36319715e-07 5.14317079e-07]
ene_total = [0.24915505 0.07176489 0.2594457  0.20870969 0.06851708 0.06687473
 0.20800497 0.18024477 0.15087435 0.06443384]
optimize_network iter = 0 obj = 1.52802506642978
eta = 0.5712444966689002
freqs = [ 7931518.36924844 15528906.23802998  7839364.44612375  2661765.47781446
 17908664.85509679  8539421.8514933   3341713.30591816 10952075.17012927
  7860199.06137994  6895193.77630764]
eta_min = 0.5712444966689012	eta_max = 0.8252306516812821
af = 0.0001292173845627228	bf = 0.6550768027395515	zeta = 0.00014213912301899509	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.23691134e-07 9.97201600e-07 1.18915578e-07 4.75404610e-09
 1.53145134e-06 1.66136535e-07 9.40995383e-09 3.35043518e-07
 1.25333105e-07 8.75447921e-08]
ene_total = [1.17506927 0.33827729 1.22360405 0.98433881 0.32285882 0.31537062
 0.98101428 0.85002707 0.71154662 0.30387348]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 1 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
eta_min = 0.8252306516812807	eta_max = 0.8252306516812821
af = 0.00010312038831994186	bf = 0.6550768027395515	zeta = 0.00011343242715193606	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.21677013e-07 7.67354369e-07 1.18915578e-07 4.39574606e-09
 1.17380585e-06 1.27096235e-07 8.69166314e-09 2.97251805e-07
 1.06754438e-07 6.67767395e-08]
ene_total = [1.17506919 0.33826839 1.22360405 0.98433879 0.32284498 0.31536911
 0.98101425 0.85002561 0.7115459  0.30387267]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 2 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
Done!
At round 72 energy consumption: 3.7483495686155184
At round 72 eta: 0.8252306516812821
At round 72 local rounds: 6.290079613751617
At round 72 global rounds: 20.136836970093626
At round 72 a_n: 3.176756027494214
gradient difference: 0.8502238988876343
train() client id: f_00000-0-0 loss: 0.843260  [   32/  126]
train() client id: f_00000-0-1 loss: 0.863573  [   64/  126]
train() client id: f_00000-0-2 loss: 0.818483  [   96/  126]
train() client id: f_00000-1-0 loss: 0.758569  [   32/  126]
train() client id: f_00000-1-1 loss: 0.719084  [   64/  126]
train() client id: f_00000-1-2 loss: 0.912138  [   96/  126]
train() client id: f_00000-2-0 loss: 0.725694  [   32/  126]
train() client id: f_00000-2-1 loss: 0.752210  [   64/  126]
train() client id: f_00000-2-2 loss: 0.837659  [   96/  126]
train() client id: f_00000-3-0 loss: 0.903737  [   32/  126]
train() client id: f_00000-3-1 loss: 0.812185  [   64/  126]
train() client id: f_00000-3-2 loss: 0.877619  [   96/  126]
train() client id: f_00000-4-0 loss: 0.843569  [   32/  126]
train() client id: f_00000-4-1 loss: 0.823478  [   64/  126]
train() client id: f_00000-4-2 loss: 0.811041  [   96/  126]
train() client id: f_00000-5-0 loss: 0.828797  [   32/  126]
train() client id: f_00000-5-1 loss: 0.955994  [   64/  126]
train() client id: f_00000-5-2 loss: 0.797814  [   96/  126]
train() client id: f_00001-0-0 loss: 0.507437  [   32/  265]
train() client id: f_00001-0-1 loss: 0.602288  [   64/  265]
train() client id: f_00001-0-2 loss: 0.552139  [   96/  265]
train() client id: f_00001-0-3 loss: 0.423170  [  128/  265]
train() client id: f_00001-0-4 loss: 0.457115  [  160/  265]
train() client id: f_00001-0-5 loss: 0.473927  [  192/  265]
train() client id: f_00001-0-6 loss: 0.566860  [  224/  265]
train() client id: f_00001-0-7 loss: 0.466482  [  256/  265]
train() client id: f_00001-1-0 loss: 0.506815  [   32/  265]
train() client id: f_00001-1-1 loss: 0.511400  [   64/  265]
train() client id: f_00001-1-2 loss: 0.509540  [   96/  265]
train() client id: f_00001-1-3 loss: 0.481478  [  128/  265]
train() client id: f_00001-1-4 loss: 0.495341  [  160/  265]
train() client id: f_00001-1-5 loss: 0.469190  [  192/  265]
train() client id: f_00001-1-6 loss: 0.442153  [  224/  265]
train() client id: f_00001-1-7 loss: 0.571222  [  256/  265]
train() client id: f_00001-2-0 loss: 0.519697  [   32/  265]
train() client id: f_00001-2-1 loss: 0.502971  [   64/  265]
train() client id: f_00001-2-2 loss: 0.407825  [   96/  265]
train() client id: f_00001-2-3 loss: 0.456659  [  128/  265]
train() client id: f_00001-2-4 loss: 0.471701  [  160/  265]
train() client id: f_00001-2-5 loss: 0.539503  [  192/  265]
train() client id: f_00001-2-6 loss: 0.582505  [  224/  265]
train() client id: f_00001-2-7 loss: 0.515261  [  256/  265]
train() client id: f_00001-3-0 loss: 0.489679  [   32/  265]
train() client id: f_00001-3-1 loss: 0.454260  [   64/  265]
train() client id: f_00001-3-2 loss: 0.437878  [   96/  265]
train() client id: f_00001-3-3 loss: 0.453778  [  128/  265]
train() client id: f_00001-3-4 loss: 0.480057  [  160/  265]
train() client id: f_00001-3-5 loss: 0.473544  [  192/  265]
train() client id: f_00001-3-6 loss: 0.578242  [  224/  265]
train() client id: f_00001-3-7 loss: 0.614372  [  256/  265]
train() client id: f_00001-4-0 loss: 0.433251  [   32/  265]
train() client id: f_00001-4-1 loss: 0.455571  [   64/  265]
train() client id: f_00001-4-2 loss: 0.482790  [   96/  265]
train() client id: f_00001-4-3 loss: 0.486664  [  128/  265]
train() client id: f_00001-4-4 loss: 0.585827  [  160/  265]
train() client id: f_00001-4-5 loss: 0.395531  [  192/  265]
train() client id: f_00001-4-6 loss: 0.566426  [  224/  265]
train() client id: f_00001-4-7 loss: 0.572600  [  256/  265]
train() client id: f_00001-5-0 loss: 0.602953  [   32/  265]
train() client id: f_00001-5-1 loss: 0.460959  [   64/  265]
train() client id: f_00001-5-2 loss: 0.469297  [   96/  265]
train() client id: f_00001-5-3 loss: 0.444848  [  128/  265]
train() client id: f_00001-5-4 loss: 0.545921  [  160/  265]
train() client id: f_00001-5-5 loss: 0.445867  [  192/  265]
train() client id: f_00001-5-6 loss: 0.444688  [  224/  265]
train() client id: f_00001-5-7 loss: 0.481814  [  256/  265]
train() client id: f_00002-0-0 loss: 1.185892  [   32/  124]
train() client id: f_00002-0-1 loss: 1.075938  [   64/  124]
train() client id: f_00002-0-2 loss: 1.147385  [   96/  124]
train() client id: f_00002-1-0 loss: 1.164501  [   32/  124]
train() client id: f_00002-1-1 loss: 0.988188  [   64/  124]
train() client id: f_00002-1-2 loss: 0.953232  [   96/  124]
train() client id: f_00002-2-0 loss: 0.919527  [   32/  124]
train() client id: f_00002-2-1 loss: 1.142798  [   64/  124]
train() client id: f_00002-2-2 loss: 0.911608  [   96/  124]
train() client id: f_00002-3-0 loss: 0.975258  [   32/  124]
train() client id: f_00002-3-1 loss: 0.942213  [   64/  124]
train() client id: f_00002-3-2 loss: 1.120970  [   96/  124]
train() client id: f_00002-4-0 loss: 1.105252  [   32/  124]
train() client id: f_00002-4-1 loss: 0.904608  [   64/  124]
train() client id: f_00002-4-2 loss: 0.886718  [   96/  124]
train() client id: f_00002-5-0 loss: 0.981094  [   32/  124]
train() client id: f_00002-5-1 loss: 0.983272  [   64/  124]
train() client id: f_00002-5-2 loss: 1.077508  [   96/  124]
train() client id: f_00003-0-0 loss: 0.622259  [   32/   43]
train() client id: f_00003-1-0 loss: 0.793611  [   32/   43]
train() client id: f_00003-2-0 loss: 0.319175  [   32/   43]
train() client id: f_00003-3-0 loss: 0.631489  [   32/   43]
train() client id: f_00003-4-0 loss: 0.657991  [   32/   43]
train() client id: f_00003-5-0 loss: 0.680673  [   32/   43]
train() client id: f_00004-0-0 loss: 0.765210  [   32/  306]
train() client id: f_00004-0-1 loss: 0.842417  [   64/  306]
train() client id: f_00004-0-2 loss: 0.747964  [   96/  306]
train() client id: f_00004-0-3 loss: 0.930204  [  128/  306]
train() client id: f_00004-0-4 loss: 0.695059  [  160/  306]
train() client id: f_00004-0-5 loss: 0.769920  [  192/  306]
train() client id: f_00004-0-6 loss: 1.027057  [  224/  306]
train() client id: f_00004-0-7 loss: 0.722719  [  256/  306]
train() client id: f_00004-0-8 loss: 0.884097  [  288/  306]
train() client id: f_00004-1-0 loss: 0.853401  [   32/  306]
train() client id: f_00004-1-1 loss: 0.957535  [   64/  306]
train() client id: f_00004-1-2 loss: 0.945086  [   96/  306]
train() client id: f_00004-1-3 loss: 0.836193  [  128/  306]
train() client id: f_00004-1-4 loss: 0.626006  [  160/  306]
train() client id: f_00004-1-5 loss: 0.617863  [  192/  306]
train() client id: f_00004-1-6 loss: 0.920088  [  224/  306]
train() client id: f_00004-1-7 loss: 0.674751  [  256/  306]
train() client id: f_00004-1-8 loss: 0.837644  [  288/  306]
train() client id: f_00004-2-0 loss: 0.813631  [   32/  306]
train() client id: f_00004-2-1 loss: 0.781983  [   64/  306]
train() client id: f_00004-2-2 loss: 0.777823  [   96/  306]
train() client id: f_00004-2-3 loss: 0.666559  [  128/  306]
train() client id: f_00004-2-4 loss: 0.791285  [  160/  306]
train() client id: f_00004-2-5 loss: 0.802586  [  192/  306]
train() client id: f_00004-2-6 loss: 0.807126  [  224/  306]
train() client id: f_00004-2-7 loss: 0.870366  [  256/  306]
train() client id: f_00004-2-8 loss: 0.949493  [  288/  306]
train() client id: f_00004-3-0 loss: 0.879571  [   32/  306]
train() client id: f_00004-3-1 loss: 0.636104  [   64/  306]
train() client id: f_00004-3-2 loss: 0.845927  [   96/  306]
train() client id: f_00004-3-3 loss: 0.901240  [  128/  306]
train() client id: f_00004-3-4 loss: 0.857278  [  160/  306]
train() client id: f_00004-3-5 loss: 0.807230  [  192/  306]
train() client id: f_00004-3-6 loss: 0.816804  [  224/  306]
train() client id: f_00004-3-7 loss: 0.798963  [  256/  306]
train() client id: f_00004-3-8 loss: 0.796826  [  288/  306]
train() client id: f_00004-4-0 loss: 0.896134  [   32/  306]
train() client id: f_00004-4-1 loss: 0.871342  [   64/  306]
train() client id: f_00004-4-2 loss: 0.753003  [   96/  306]
train() client id: f_00004-4-3 loss: 0.773666  [  128/  306]
train() client id: f_00004-4-4 loss: 0.852919  [  160/  306]
train() client id: f_00004-4-5 loss: 0.769113  [  192/  306]
train() client id: f_00004-4-6 loss: 0.775015  [  224/  306]
train() client id: f_00004-4-7 loss: 0.777395  [  256/  306]
train() client id: f_00004-4-8 loss: 0.873504  [  288/  306]
train() client id: f_00004-5-0 loss: 0.875142  [   32/  306]
train() client id: f_00004-5-1 loss: 0.861236  [   64/  306]
train() client id: f_00004-5-2 loss: 0.832927  [   96/  306]
train() client id: f_00004-5-3 loss: 0.824597  [  128/  306]
train() client id: f_00004-5-4 loss: 0.840264  [  160/  306]
train() client id: f_00004-5-5 loss: 0.806185  [  192/  306]
train() client id: f_00004-5-6 loss: 0.884774  [  224/  306]
train() client id: f_00004-5-7 loss: 0.705305  [  256/  306]
train() client id: f_00004-5-8 loss: 0.697083  [  288/  306]
train() client id: f_00005-0-0 loss: 0.386519  [   32/  146]
train() client id: f_00005-0-1 loss: 0.358753  [   64/  146]
train() client id: f_00005-0-2 loss: 0.638390  [   96/  146]
train() client id: f_00005-0-3 loss: 0.922848  [  128/  146]
train() client id: f_00005-1-0 loss: 0.578139  [   32/  146]
train() client id: f_00005-1-1 loss: 0.637040  [   64/  146]
train() client id: f_00005-1-2 loss: 0.465133  [   96/  146]
train() client id: f_00005-1-3 loss: 0.538560  [  128/  146]
train() client id: f_00005-2-0 loss: 0.603082  [   32/  146]
train() client id: f_00005-2-1 loss: 0.587433  [   64/  146]
train() client id: f_00005-2-2 loss: 0.595918  [   96/  146]
train() client id: f_00005-2-3 loss: 0.576862  [  128/  146]
train() client id: f_00005-3-0 loss: 0.687312  [   32/  146]
train() client id: f_00005-3-1 loss: 0.587753  [   64/  146]
train() client id: f_00005-3-2 loss: 0.408246  [   96/  146]
train() client id: f_00005-3-3 loss: 0.656227  [  128/  146]
train() client id: f_00005-4-0 loss: 0.481739  [   32/  146]
train() client id: f_00005-4-1 loss: 0.578048  [   64/  146]
train() client id: f_00005-4-2 loss: 0.543744  [   96/  146]
train() client id: f_00005-4-3 loss: 0.586215  [  128/  146]
train() client id: f_00005-5-0 loss: 0.469199  [   32/  146]
train() client id: f_00005-5-1 loss: 0.567354  [   64/  146]
train() client id: f_00005-5-2 loss: 0.605101  [   96/  146]
train() client id: f_00005-5-3 loss: 0.716221  [  128/  146]
train() client id: f_00006-0-0 loss: 0.485284  [   32/   54]
train() client id: f_00006-1-0 loss: 0.547631  [   32/   54]
train() client id: f_00006-2-0 loss: 0.544480  [   32/   54]
train() client id: f_00006-3-0 loss: 0.500284  [   32/   54]
train() client id: f_00006-4-0 loss: 0.531204  [   32/   54]
train() client id: f_00006-5-0 loss: 0.551535  [   32/   54]
train() client id: f_00007-0-0 loss: 1.069720  [   32/  179]
train() client id: f_00007-0-1 loss: 0.700106  [   64/  179]
train() client id: f_00007-0-2 loss: 0.723749  [   96/  179]
train() client id: f_00007-0-3 loss: 0.684167  [  128/  179]
train() client id: f_00007-0-4 loss: 0.774330  [  160/  179]
train() client id: f_00007-1-0 loss: 0.819702  [   32/  179]
train() client id: f_00007-1-1 loss: 0.776547  [   64/  179]
train() client id: f_00007-1-2 loss: 0.762507  [   96/  179]
train() client id: f_00007-1-3 loss: 0.724976  [  128/  179]
train() client id: f_00007-1-4 loss: 0.899852  [  160/  179]
train() client id: f_00007-2-0 loss: 1.015845  [   32/  179]
train() client id: f_00007-2-1 loss: 0.717143  [   64/  179]
train() client id: f_00007-2-2 loss: 0.687363  [   96/  179]
train() client id: f_00007-2-3 loss: 0.867084  [  128/  179]
train() client id: f_00007-2-4 loss: 0.674215  [  160/  179]
train() client id: f_00007-3-0 loss: 0.718568  [   32/  179]
train() client id: f_00007-3-1 loss: 0.728526  [   64/  179]
train() client id: f_00007-3-2 loss: 0.782938  [   96/  179]
train() client id: f_00007-3-3 loss: 0.976840  [  128/  179]
train() client id: f_00007-3-4 loss: 0.806636  [  160/  179]
train() client id: f_00007-4-0 loss: 0.744748  [   32/  179]
train() client id: f_00007-4-1 loss: 0.910419  [   64/  179]
train() client id: f_00007-4-2 loss: 0.716532  [   96/  179]
train() client id: f_00007-4-3 loss: 0.912544  [  128/  179]
train() client id: f_00007-4-4 loss: 0.647648  [  160/  179]
train() client id: f_00007-5-0 loss: 0.804066  [   32/  179]
train() client id: f_00007-5-1 loss: 0.829753  [   64/  179]
train() client id: f_00007-5-2 loss: 0.798192  [   96/  179]
train() client id: f_00007-5-3 loss: 0.878595  [  128/  179]
train() client id: f_00007-5-4 loss: 0.650049  [  160/  179]
train() client id: f_00008-0-0 loss: 0.715243  [   32/  130]
train() client id: f_00008-0-1 loss: 0.688144  [   64/  130]
train() client id: f_00008-0-2 loss: 0.685066  [   96/  130]
train() client id: f_00008-0-3 loss: 0.666130  [  128/  130]
train() client id: f_00008-1-0 loss: 0.806702  [   32/  130]
train() client id: f_00008-1-1 loss: 0.612091  [   64/  130]
train() client id: f_00008-1-2 loss: 0.747967  [   96/  130]
train() client id: f_00008-1-3 loss: 0.588197  [  128/  130]
train() client id: f_00008-2-0 loss: 0.610992  [   32/  130]
train() client id: f_00008-2-1 loss: 0.599224  [   64/  130]
train() client id: f_00008-2-2 loss: 0.767672  [   96/  130]
train() client id: f_00008-2-3 loss: 0.740113  [  128/  130]
train() client id: f_00008-3-0 loss: 0.743919  [   32/  130]
train() client id: f_00008-3-1 loss: 0.557631  [   64/  130]
train() client id: f_00008-3-2 loss: 0.733162  [   96/  130]
train() client id: f_00008-3-3 loss: 0.709269  [  128/  130]
train() client id: f_00008-4-0 loss: 0.623693  [   32/  130]
train() client id: f_00008-4-1 loss: 0.709865  [   64/  130]
train() client id: f_00008-4-2 loss: 0.696250  [   96/  130]
train() client id: f_00008-4-3 loss: 0.723383  [  128/  130]
train() client id: f_00008-5-0 loss: 0.618535  [   32/  130]
train() client id: f_00008-5-1 loss: 0.672796  [   64/  130]
train() client id: f_00008-5-2 loss: 0.706738  [   96/  130]
train() client id: f_00008-5-3 loss: 0.763911  [  128/  130]
train() client id: f_00009-0-0 loss: 0.806105  [   32/  118]
train() client id: f_00009-0-1 loss: 1.128735  [   64/  118]
train() client id: f_00009-0-2 loss: 0.961317  [   96/  118]
train() client id: f_00009-1-0 loss: 0.994713  [   32/  118]
train() client id: f_00009-1-1 loss: 0.959200  [   64/  118]
train() client id: f_00009-1-2 loss: 0.839664  [   96/  118]
train() client id: f_00009-2-0 loss: 0.834808  [   32/  118]
train() client id: f_00009-2-1 loss: 0.906454  [   64/  118]
train() client id: f_00009-2-2 loss: 0.933560  [   96/  118]
train() client id: f_00009-3-0 loss: 0.976320  [   32/  118]
train() client id: f_00009-3-1 loss: 0.982816  [   64/  118]
train() client id: f_00009-3-2 loss: 0.863159  [   96/  118]
train() client id: f_00009-4-0 loss: 0.885031  [   32/  118]
train() client id: f_00009-4-1 loss: 0.814914  [   64/  118]
train() client id: f_00009-4-2 loss: 0.986251  [   96/  118]
train() client id: f_00009-5-0 loss: 0.846779  [   32/  118]
train() client id: f_00009-5-1 loss: 0.908831  [   64/  118]
train() client id: f_00009-5-2 loss: 0.887111  [   96/  118]
At round 72 accuracy: 0.6472148541114059
At round 72 training accuracy: 0.5855130784708249
At round 72 training loss: 0.830922431544239
update_location
xs = [  -3.9056584     4.20031788  380.00902392   18.81129433    0.97929623
    3.95640986 -342.44319194 -321.32485185  364.66397685 -307.06087855]
ys = [ 372.5879595   355.55583871    1.32061395 -342.45517586  334.35018685
  317.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [385.79404056 369.37460269 392.94860005 357.25258889 348.98568232
 333.19887093 356.75514046 336.52687362 378.53465625 322.95881315]
dists_bs = [261.36999725 254.38782364 581.5492634  552.56011151 237.40512789
 228.95693543 244.1062424  227.48366256 562.15809678 216.05686492]
uav_gains = [8.21244196e-13 9.67100320e-13 7.69464620e-13 1.10763527e-12
 1.22558846e-12 1.52083801e-12 1.11416668e-12 1.44934733e-12
 8.80519312e-13 1.78047100e-12]
bs_gains = [1.88355381e-11 2.03190964e-11 2.00655865e-12 2.31542863e-12
 2.46559544e-11 2.72887370e-11 2.28072648e-11 2.77864773e-11
 2.20643109e-12 3.20998859e-11]
Round 73
-------------------------------
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.3194998  2.59288881 1.30418738 0.50035792 2.98837254 1.43930991
 0.6065387  1.80528297 1.31771582 1.16737463]
obj_prev = 15.041528473465261
eta_min = 6.730086322824428e-72	eta_max = 0.960768447983313
af = 3.101969802094253	bf = 0.606014338290774	zeta = 3.4121667823036788	eta = 0.9090909090909091
af = 3.101969802094253	bf = 0.606014338290774	zeta = 9.918300016635595	eta = 0.3127521648761819
af = 3.101969802094253	bf = 0.606014338290774	zeta = 6.128169132936568	eta = 0.5061821458912337
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.504089108696306	eta = 0.5635755055624424
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464521898505789	eta = 0.5676562121459247
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464335618649047	eta = 0.5676755636142928
eta = 0.5676755636142928
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.0467219  0.09826431 0.04598028 0.01594477 0.11346746 0.05413807
 0.02002367 0.06637476 0.04820513 0.04375543]
ene_total = [0.57257327 0.78687954 0.57638183 0.31001941 0.89561579 0.45820012
 0.33936518 0.65485131 0.49074166 0.3797075 ]
ti_comp = [3.27393193 3.49770683 3.26133269 3.32347001 3.50175933 3.50374398
 3.32432233 3.35848843 3.40012884 3.50673809]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [5.94705746e-07 4.84730784e-06 5.71221622e-07 2.29377929e-08
 7.44597339e-06 8.07836791e-07 4.54051187e-08 1.62032140e-06
 6.05576618e-07 4.25764209e-07]
ene_total = [0.22926124 0.06486083 0.2385191  0.19285604 0.06190211 0.060395
 0.19222992 0.16713595 0.13653078 0.05819209]
optimize_network iter = 0 obj = 1.401883055865988
eta = 0.5676755636142928
freqs = [ 7135441.14428652 14046961.66554712  7049308.38877608  2398814.24536511
 16201493.85610097  7725745.89318307  3011692.04068683  9881641.69424418
  7088721.37557428  6238764.68717198]
eta_min = 0.5676755636142934	eta_max = 0.8387833537635655
af = 9.530769728321942e-05	bf = 0.606014338290774	zeta = 0.00010483846701154136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.00107717e-07 8.15954657e-07 9.61545994e-08 3.86115336e-09
 1.25339196e-06 1.35984388e-07 7.64311227e-09 2.72751152e-07
 1.01937628e-07 7.16695331e-08]
ene_total = [1.09025033 0.30830925 1.13427738 0.91713951 0.29416335 0.28718908
 0.91416128 0.79477943 0.64926391 0.27672413]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 1 obj = 3.7590876976936305
eta = 0.8387833537635655
freqs = [ 7074978.59491438 12213971.70314103  7049308.38877607  2303188.06336389
 14058067.6277585   6696837.19701407  2890083.78088009  9285155.45638792
  6499537.15984087  5399630.45377395]
eta_min = 0.8387833537635758	eta_max = 0.8387833537635625
af = 7.4931901688053e-05	bf = 0.606014338290774	zeta = 8.242509185685831e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [9.84183688e-08 6.16900397e-07 9.61545994e-08 3.55944769e-09
 9.43687147e-07 1.02175717e-07 7.03833579e-09 2.40816781e-07
 8.56965960e-08 5.36865431e-08]
ene_total = [1.09025027 0.30830229 1.13427738 0.9171395  0.29415253 0.2871879
 0.91416126 0.79477832 0.64926334 0.2767235 ]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 2 obj = 3.759087697693561
eta = 0.8387833537635625
freqs = [ 7074978.59491436 12213971.70314105  7049308.38877605  2303188.06336388
 14058067.62775852  6696837.19701408  2890083.78088009  9285155.45638791
  6499537.15984087  5399630.45377396]
Done!
At round 73 energy consumption: 3.7590876976936305
At round 73 eta: 0.8387833537635625
At round 73 local rounds: 5.756677690695127
At round 73 global rounds: 19.70488843214887
At round 73 a_n: 2.834210180524895
gradient difference: 0.6788832545280457
train() client id: f_00000-0-0 loss: 0.990368  [   32/  126]
train() client id: f_00000-0-1 loss: 1.125029  [   64/  126]
train() client id: f_00000-0-2 loss: 1.291311  [   96/  126]
train() client id: f_00000-1-0 loss: 1.004256  [   32/  126]
train() client id: f_00000-1-1 loss: 1.086869  [   64/  126]
train() client id: f_00000-1-2 loss: 0.900296  [   96/  126]
train() client id: f_00000-2-0 loss: 0.996783  [   32/  126]
train() client id: f_00000-2-1 loss: 1.093213  [   64/  126]
train() client id: f_00000-2-2 loss: 0.919526  [   96/  126]
train() client id: f_00000-3-0 loss: 0.833095  [   32/  126]
train() client id: f_00000-3-1 loss: 1.002491  [   64/  126]
train() client id: f_00000-3-2 loss: 1.016940  [   96/  126]
train() client id: f_00000-4-0 loss: 1.070089  [   32/  126]
train() client id: f_00000-4-1 loss: 0.939440  [   64/  126]
train() client id: f_00000-4-2 loss: 0.980013  [   96/  126]
train() client id: f_00001-0-0 loss: 0.523408  [   32/  265]
train() client id: f_00001-0-1 loss: 0.505715  [   64/  265]
train() client id: f_00001-0-2 loss: 0.557277  [   96/  265]
train() client id: f_00001-0-3 loss: 0.487173  [  128/  265]
train() client id: f_00001-0-4 loss: 0.613092  [  160/  265]
train() client id: f_00001-0-5 loss: 0.557516  [  192/  265]
train() client id: f_00001-0-6 loss: 0.428017  [  224/  265]
train() client id: f_00001-0-7 loss: 0.487939  [  256/  265]
train() client id: f_00001-1-0 loss: 0.443377  [   32/  265]
train() client id: f_00001-1-1 loss: 0.451055  [   64/  265]
train() client id: f_00001-1-2 loss: 0.553911  [   96/  265]
train() client id: f_00001-1-3 loss: 0.566531  [  128/  265]
train() client id: f_00001-1-4 loss: 0.523875  [  160/  265]
train() client id: f_00001-1-5 loss: 0.455839  [  192/  265]
train() client id: f_00001-1-6 loss: 0.636596  [  224/  265]
train() client id: f_00001-1-7 loss: 0.534852  [  256/  265]
train() client id: f_00001-2-0 loss: 0.452387  [   32/  265]
train() client id: f_00001-2-1 loss: 0.448197  [   64/  265]
train() client id: f_00001-2-2 loss: 0.538375  [   96/  265]
train() client id: f_00001-2-3 loss: 0.509804  [  128/  265]
train() client id: f_00001-2-4 loss: 0.550474  [  160/  265]
train() client id: f_00001-2-5 loss: 0.555457  [  192/  265]
train() client id: f_00001-2-6 loss: 0.495404  [  224/  265]
train() client id: f_00001-2-7 loss: 0.537498  [  256/  265]
train() client id: f_00001-3-0 loss: 0.470649  [   32/  265]
train() client id: f_00001-3-1 loss: 0.652025  [   64/  265]
train() client id: f_00001-3-2 loss: 0.596809  [   96/  265]
train() client id: f_00001-3-3 loss: 0.391424  [  128/  265]
train() client id: f_00001-3-4 loss: 0.566280  [  160/  265]
train() client id: f_00001-3-5 loss: 0.470322  [  192/  265]
train() client id: f_00001-3-6 loss: 0.456123  [  224/  265]
train() client id: f_00001-3-7 loss: 0.532408  [  256/  265]
train() client id: f_00001-4-0 loss: 0.530119  [   32/  265]
train() client id: f_00001-4-1 loss: 0.525081  [   64/  265]
train() client id: f_00001-4-2 loss: 0.540973  [   96/  265]
train() client id: f_00001-4-3 loss: 0.591300  [  128/  265]
train() client id: f_00001-4-4 loss: 0.500768  [  160/  265]
train() client id: f_00001-4-5 loss: 0.578941  [  192/  265]
train() client id: f_00001-4-6 loss: 0.424316  [  224/  265]
train() client id: f_00001-4-7 loss: 0.447179  [  256/  265]
train() client id: f_00002-0-0 loss: 0.825439  [   32/  124]
train() client id: f_00002-0-1 loss: 0.979679  [   64/  124]
train() client id: f_00002-0-2 loss: 0.817454  [   96/  124]
train() client id: f_00002-1-0 loss: 0.893177  [   32/  124]
train() client id: f_00002-1-1 loss: 0.929479  [   64/  124]
train() client id: f_00002-1-2 loss: 0.898310  [   96/  124]
train() client id: f_00002-2-0 loss: 0.799338  [   32/  124]
train() client id: f_00002-2-1 loss: 0.850015  [   64/  124]
train() client id: f_00002-2-2 loss: 0.954035  [   96/  124]
train() client id: f_00002-3-0 loss: 0.885205  [   32/  124]
train() client id: f_00002-3-1 loss: 0.913570  [   64/  124]
train() client id: f_00002-3-2 loss: 0.736527  [   96/  124]
train() client id: f_00002-4-0 loss: 0.833483  [   32/  124]
train() client id: f_00002-4-1 loss: 0.797003  [   64/  124]
train() client id: f_00002-4-2 loss: 0.914392  [   96/  124]
train() client id: f_00003-0-0 loss: 0.591915  [   32/   43]
train() client id: f_00003-1-0 loss: 0.461048  [   32/   43]
train() client id: f_00003-2-0 loss: 0.616510  [   32/   43]
train() client id: f_00003-3-0 loss: 0.543930  [   32/   43]
train() client id: f_00003-4-0 loss: 0.629188  [   32/   43]
train() client id: f_00004-0-0 loss: 0.792414  [   32/  306]
train() client id: f_00004-0-1 loss: 0.947196  [   64/  306]
train() client id: f_00004-0-2 loss: 1.057090  [   96/  306]
train() client id: f_00004-0-3 loss: 1.069909  [  128/  306]
train() client id: f_00004-0-4 loss: 0.883102  [  160/  306]
train() client id: f_00004-0-5 loss: 1.086582  [  192/  306]
train() client id: f_00004-0-6 loss: 0.903783  [  224/  306]
train() client id: f_00004-0-7 loss: 0.975798  [  256/  306]
train() client id: f_00004-0-8 loss: 1.010045  [  288/  306]
train() client id: f_00004-1-0 loss: 0.853713  [   32/  306]
train() client id: f_00004-1-1 loss: 0.855490  [   64/  306]
train() client id: f_00004-1-2 loss: 0.830826  [   96/  306]
train() client id: f_00004-1-3 loss: 0.896571  [  128/  306]
train() client id: f_00004-1-4 loss: 0.859719  [  160/  306]
train() client id: f_00004-1-5 loss: 1.060493  [  192/  306]
train() client id: f_00004-1-6 loss: 1.078038  [  224/  306]
train() client id: f_00004-1-7 loss: 1.000782  [  256/  306]
train() client id: f_00004-1-8 loss: 1.126976  [  288/  306]
train() client id: f_00004-2-0 loss: 0.981396  [   32/  306]
train() client id: f_00004-2-1 loss: 0.876642  [   64/  306]
train() client id: f_00004-2-2 loss: 0.911524  [   96/  306]
train() client id: f_00004-2-3 loss: 1.080233  [  128/  306]
train() client id: f_00004-2-4 loss: 0.928595  [  160/  306]
train() client id: f_00004-2-5 loss: 1.091730  [  192/  306]
train() client id: f_00004-2-6 loss: 0.998405  [  224/  306]
train() client id: f_00004-2-7 loss: 0.829966  [  256/  306]
train() client id: f_00004-2-8 loss: 0.903022  [  288/  306]
train() client id: f_00004-3-0 loss: 1.090194  [   32/  306]
train() client id: f_00004-3-1 loss: 0.873539  [   64/  306]
train() client id: f_00004-3-2 loss: 0.971006  [   96/  306]
train() client id: f_00004-3-3 loss: 0.800905  [  128/  306]
train() client id: f_00004-3-4 loss: 1.147670  [  160/  306]
train() client id: f_00004-3-5 loss: 1.089132  [  192/  306]
train() client id: f_00004-3-6 loss: 0.850610  [  224/  306]
train() client id: f_00004-3-7 loss: 0.967759  [  256/  306]
train() client id: f_00004-3-8 loss: 0.866165  [  288/  306]
train() client id: f_00004-4-0 loss: 0.968018  [   32/  306]
train() client id: f_00004-4-1 loss: 1.105784  [   64/  306]
train() client id: f_00004-4-2 loss: 0.857022  [   96/  306]
train() client id: f_00004-4-3 loss: 1.044964  [  128/  306]
train() client id: f_00004-4-4 loss: 1.015650  [  160/  306]
train() client id: f_00004-4-5 loss: 0.855134  [  192/  306]
train() client id: f_00004-4-6 loss: 0.871876  [  224/  306]
train() client id: f_00004-4-7 loss: 0.894184  [  256/  306]
train() client id: f_00004-4-8 loss: 1.029388  [  288/  306]
train() client id: f_00005-0-0 loss: 0.304872  [   32/  146]
train() client id: f_00005-0-1 loss: 0.756600  [   64/  146]
train() client id: f_00005-0-2 loss: 0.357100  [   96/  146]
train() client id: f_00005-0-3 loss: 0.436657  [  128/  146]
train() client id: f_00005-1-0 loss: 0.542234  [   32/  146]
train() client id: f_00005-1-1 loss: 0.559695  [   64/  146]
train() client id: f_00005-1-2 loss: 0.420503  [   96/  146]
train() client id: f_00005-1-3 loss: 0.658108  [  128/  146]
train() client id: f_00005-2-0 loss: 0.499388  [   32/  146]
train() client id: f_00005-2-1 loss: 0.603443  [   64/  146]
train() client id: f_00005-2-2 loss: 0.517156  [   96/  146]
train() client id: f_00005-2-3 loss: 0.523006  [  128/  146]
train() client id: f_00005-3-0 loss: 0.526116  [   32/  146]
train() client id: f_00005-3-1 loss: 0.326531  [   64/  146]
train() client id: f_00005-3-2 loss: 0.450170  [   96/  146]
train() client id: f_00005-3-3 loss: 0.619188  [  128/  146]
train() client id: f_00005-4-0 loss: 0.224792  [   32/  146]
train() client id: f_00005-4-1 loss: 0.660225  [   64/  146]
train() client id: f_00005-4-2 loss: 0.463080  [   96/  146]
train() client id: f_00005-4-3 loss: 0.624375  [  128/  146]
train() client id: f_00006-0-0 loss: 0.412702  [   32/   54]
train() client id: f_00006-1-0 loss: 0.406731  [   32/   54]
train() client id: f_00006-2-0 loss: 0.474509  [   32/   54]
train() client id: f_00006-3-0 loss: 0.530396  [   32/   54]
train() client id: f_00006-4-0 loss: 0.473365  [   32/   54]
train() client id: f_00007-0-0 loss: 0.598896  [   32/  179]
train() client id: f_00007-0-1 loss: 0.967601  [   64/  179]
train() client id: f_00007-0-2 loss: 0.585019  [   96/  179]
train() client id: f_00007-0-3 loss: 0.874609  [  128/  179]
train() client id: f_00007-0-4 loss: 0.562403  [  160/  179]
train() client id: f_00007-1-0 loss: 0.589866  [   32/  179]
train() client id: f_00007-1-1 loss: 0.609713  [   64/  179]
train() client id: f_00007-1-2 loss: 0.741847  [   96/  179]
train() client id: f_00007-1-3 loss: 1.025116  [  128/  179]
train() client id: f_00007-1-4 loss: 0.685937  [  160/  179]
train() client id: f_00007-2-0 loss: 0.648571  [   32/  179]
train() client id: f_00007-2-1 loss: 0.596204  [   64/  179]
train() client id: f_00007-2-2 loss: 0.694103  [   96/  179]
train() client id: f_00007-2-3 loss: 1.016032  [  128/  179]
train() client id: f_00007-2-4 loss: 0.717051  [  160/  179]
train() client id: f_00007-3-0 loss: 0.697160  [   32/  179]
train() client id: f_00007-3-1 loss: 0.695494  [   64/  179]
train() client id: f_00007-3-2 loss: 0.810829  [   96/  179]
train() client id: f_00007-3-3 loss: 0.570873  [  128/  179]
train() client id: f_00007-3-4 loss: 0.961746  [  160/  179]
train() client id: f_00007-4-0 loss: 0.687772  [   32/  179]
train() client id: f_00007-4-1 loss: 0.781433  [   64/  179]
train() client id: f_00007-4-2 loss: 0.669874  [   96/  179]
train() client id: f_00007-4-3 loss: 0.650144  [  128/  179]
train() client id: f_00007-4-4 loss: 0.644158  [  160/  179]
train() client id: f_00008-0-0 loss: 0.737511  [   32/  130]
train() client id: f_00008-0-1 loss: 0.622181  [   64/  130]
train() client id: f_00008-0-2 loss: 0.650918  [   96/  130]
train() client id: f_00008-0-3 loss: 0.703134  [  128/  130]
train() client id: f_00008-1-0 loss: 0.603505  [   32/  130]
train() client id: f_00008-1-1 loss: 0.725428  [   64/  130]
train() client id: f_00008-1-2 loss: 0.739119  [   96/  130]
train() client id: f_00008-1-3 loss: 0.683710  [  128/  130]
train() client id: f_00008-2-0 loss: 0.839052  [   32/  130]
train() client id: f_00008-2-1 loss: 0.623746  [   64/  130]
train() client id: f_00008-2-2 loss: 0.582929  [   96/  130]
train() client id: f_00008-2-3 loss: 0.708960  [  128/  130]
train() client id: f_00008-3-0 loss: 0.584154  [   32/  130]
train() client id: f_00008-3-1 loss: 0.786635  [   64/  130]
train() client id: f_00008-3-2 loss: 0.805897  [   96/  130]
train() client id: f_00008-3-3 loss: 0.583723  [  128/  130]
train() client id: f_00008-4-0 loss: 0.707907  [   32/  130]
train() client id: f_00008-4-1 loss: 0.644408  [   64/  130]
train() client id: f_00008-4-2 loss: 0.749597  [   96/  130]
train() client id: f_00008-4-3 loss: 0.635159  [  128/  130]
train() client id: f_00009-0-0 loss: 0.964444  [   32/  118]
train() client id: f_00009-0-1 loss: 0.912307  [   64/  118]
train() client id: f_00009-0-2 loss: 0.971856  [   96/  118]
train() client id: f_00009-1-0 loss: 0.937284  [   32/  118]
train() client id: f_00009-1-1 loss: 1.034378  [   64/  118]
train() client id: f_00009-1-2 loss: 0.806171  [   96/  118]
train() client id: f_00009-2-0 loss: 0.929527  [   32/  118]
train() client id: f_00009-2-1 loss: 0.985141  [   64/  118]
train() client id: f_00009-2-2 loss: 0.737496  [   96/  118]
train() client id: f_00009-3-0 loss: 0.944970  [   32/  118]
train() client id: f_00009-3-1 loss: 0.874117  [   64/  118]
train() client id: f_00009-3-2 loss: 0.750752  [   96/  118]
train() client id: f_00009-4-0 loss: 0.814273  [   32/  118]
train() client id: f_00009-4-1 loss: 0.866861  [   64/  118]
train() client id: f_00009-4-2 loss: 0.927990  [   96/  118]
At round 73 accuracy: 0.6472148541114059
At round 73 training accuracy: 0.5935613682092555
At round 73 training loss: 0.8162456766782813
update_location
xs = [  -3.9056584     4.20031788  385.00902392   18.81129433    0.97929623
    3.95640986 -347.44319194 -326.32485185  369.66397685 -312.06087855]
ys = [ 377.5879595   360.55583871    1.32061395 -347.45517586  339.35018685
  322.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [390.6250393  374.18999919 397.78598834 362.04828963 353.77889752
 337.97134361 361.5572737  341.30424139 383.35378667 327.71634652]
dists_bs = [265.17004933 257.96070089 586.32297926 557.24521406 240.78973528
 232.10863791 247.56269812 230.73158702 566.95975654 219.14875693]
uav_gains = [7.85626588e-13 9.19780429e-13 7.37654771e-13 1.04796295e-12
 1.15469134e-12 1.42001853e-12 1.05381008e-12 1.35598266e-12
 8.40352469e-13 1.65176343e-12]
bs_gains = [1.80894587e-11 1.95408815e-11 1.96114959e-12 2.26133192e-12
 2.36977859e-11 2.62638529e-11 2.19268126e-11 2.67051078e-11
 2.15450662e-12 3.08478466e-11]
Round 74
-------------------------------
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.17965282 2.31354803 1.16601007 0.44876366 2.6663691  1.28432213
 0.54349708 1.6129016  1.17620535 1.04170236]
obj_prev = 13.43297219536657
eta_min = 1.985153234470665e-80	eta_max = 0.9640166902686983
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 3.044236871939507	eta = 0.9090909090909091
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 8.994246026442418	eta = 0.3076953929504865
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 5.5116993178884695	eta = 0.5021115822515108
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.941875446838901	eta = 0.5600076519876258
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905748712654075	eta = 0.5641316397354368
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905578223300182	eta = 0.5641512456685993
eta = 0.5641512456685993
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.04723579 0.09934512 0.04648602 0.01612015 0.11471549 0.05473354
 0.02024391 0.06710481 0.04873534 0.04423669]
ene_total = [0.51557111 0.70394459 0.51893062 0.28085198 0.80121777 0.40983951
 0.30711799 0.58944182 0.4390443  0.33961853]
ti_comp = [3.7239081  3.95531799 3.71124405 3.77370601 3.95943373 3.96148105
 3.77455137 3.80899893 3.85658756 3.96449942]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [4.75002260e-07 3.91702877e-06 4.55835322e-07 1.83844731e-08
 6.01838646e-06 6.53018300e-07 3.63942310e-08 1.30172529e-06
 4.86413493e-07 3.44232011e-07]
ene_total = [0.20841223 0.05795483 0.2166472  0.17602698 0.05529214 0.05392593
 0.17547739 0.15308527 0.12213433 0.05196115]
optimize_network iter = 0 obj = 1.2709174591621142
eta = 0.5641512456685993
freqs = [ 6342233.67091708 12558423.2934157   6262861.66135149  2135851.43012001
 14486350.74522172  6908216.2161498   2681631.30754287  8808720.47508642
  6318453.65727492  5579101.94228267]
eta_min = 0.5641512456686001	eta_max = 0.8530565293483942
af = 6.786276083478417e-05	bf = 0.5538889445570291	zeta = 7.464903691826259e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.90879612e-08 6.52185990e-07 7.58966627e-08 3.06101806e-09
 1.00206242e-06 1.08727664e-07 6.05964597e-09 2.16737493e-07
 8.09879336e-08 5.73146914e-08]
ene_total = [0.99918599 0.27775315 1.03866771 0.84393208 0.2649327  0.25852208
 0.84129665 0.73390845 0.58554042 0.2491103 ]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 1 obj = 3.769458367807797
eta = 0.8530565293483942
freqs = [ 6286609.35895016 10821081.10112869  6262861.66135149  2047669.00897199
 12455063.70509777  5933111.71394756  2569503.70041527  8257350.63294458
  5754226.36383451  4783979.0224429 ]
eta_min = 0.8530565293483974	eta_max = 0.8530565293483936
af = 5.2542085376783516e-05	bf = 0.5538889445570291	zeta = 5.7796293914461874e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.77067690e-08 4.84219790e-07 7.58966627e-08 2.81347673e-09
 7.40744934e-07 8.01997961e-08 5.56349384e-09 1.90453887e-07
 6.71695741e-08 4.21420732e-08]
ene_total = [0.99918595 0.27774791 1.03866771 0.84393207 0.26492456 0.25852119
 0.84129664 0.73390763 0.58553999 0.24910983]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 2 obj = 3.7694583678077827
eta = 0.8530565293483936
freqs = [ 6286609.35895015 10821081.10112868  6262861.66135148  2047669.00897199
 12455063.70509776  5933111.71394756  2569503.70041527  8257350.63294457
  5754226.3638345   4783979.0224429 ]
Done!
At round 74 energy consumption: 3.769458367807797
At round 74 eta: 0.8530565293483936
At round 74 local rounds: 5.2041580597357475
At round 74 global rounds: 19.287758537054202
At round 74 a_n: 2.4916643335555797
gradient difference: 0.9510065913200378
train() client id: f_00000-0-0 loss: 0.816324  [   32/  126]
train() client id: f_00000-0-1 loss: 0.980035  [   64/  126]
train() client id: f_00000-0-2 loss: 0.988767  [   96/  126]
train() client id: f_00000-1-0 loss: 0.818931  [   32/  126]
train() client id: f_00000-1-1 loss: 0.803585  [   64/  126]
train() client id: f_00000-1-2 loss: 0.956326  [   96/  126]
train() client id: f_00000-2-0 loss: 0.783248  [   32/  126]
train() client id: f_00000-2-1 loss: 0.833527  [   64/  126]
train() client id: f_00000-2-2 loss: 1.014919  [   96/  126]
train() client id: f_00000-3-0 loss: 0.807742  [   32/  126]
train() client id: f_00000-3-1 loss: 0.954639  [   64/  126]
train() client id: f_00000-3-2 loss: 0.709845  [   96/  126]
train() client id: f_00000-4-0 loss: 0.943265  [   32/  126]
train() client id: f_00000-4-1 loss: 0.776518  [   64/  126]
train() client id: f_00000-4-2 loss: 0.928596  [   96/  126]
train() client id: f_00001-0-0 loss: 0.625079  [   32/  265]
train() client id: f_00001-0-1 loss: 0.554371  [   64/  265]
train() client id: f_00001-0-2 loss: 0.642381  [   96/  265]
train() client id: f_00001-0-3 loss: 0.674491  [  128/  265]
train() client id: f_00001-0-4 loss: 0.542994  [  160/  265]
train() client id: f_00001-0-5 loss: 0.646797  [  192/  265]
train() client id: f_00001-0-6 loss: 0.508723  [  224/  265]
train() client id: f_00001-0-7 loss: 0.613977  [  256/  265]
train() client id: f_00001-1-0 loss: 0.593839  [   32/  265]
train() client id: f_00001-1-1 loss: 0.715102  [   64/  265]
train() client id: f_00001-1-2 loss: 0.507455  [   96/  265]
train() client id: f_00001-1-3 loss: 0.589132  [  128/  265]
train() client id: f_00001-1-4 loss: 0.587828  [  160/  265]
train() client id: f_00001-1-5 loss: 0.656044  [  192/  265]
train() client id: f_00001-1-6 loss: 0.504619  [  224/  265]
train() client id: f_00001-1-7 loss: 0.585534  [  256/  265]
train() client id: f_00001-2-0 loss: 0.731029  [   32/  265]
train() client id: f_00001-2-1 loss: 0.617542  [   64/  265]
train() client id: f_00001-2-2 loss: 0.590017  [   96/  265]
train() client id: f_00001-2-3 loss: 0.549950  [  128/  265]
train() client id: f_00001-2-4 loss: 0.586986  [  160/  265]
train() client id: f_00001-2-5 loss: 0.517684  [  192/  265]
train() client id: f_00001-2-6 loss: 0.603410  [  224/  265]
train() client id: f_00001-2-7 loss: 0.594160  [  256/  265]
train() client id: f_00001-3-0 loss: 0.523044  [   32/  265]
train() client id: f_00001-3-1 loss: 0.573175  [   64/  265]
train() client id: f_00001-3-2 loss: 0.609095  [   96/  265]
train() client id: f_00001-3-3 loss: 0.527676  [  128/  265]
train() client id: f_00001-3-4 loss: 0.520830  [  160/  265]
train() client id: f_00001-3-5 loss: 0.582615  [  192/  265]
train() client id: f_00001-3-6 loss: 0.587196  [  224/  265]
train() client id: f_00001-3-7 loss: 0.806035  [  256/  265]
train() client id: f_00001-4-0 loss: 0.791569  [   32/  265]
train() client id: f_00001-4-1 loss: 0.498065  [   64/  265]
train() client id: f_00001-4-2 loss: 0.575676  [   96/  265]
train() client id: f_00001-4-3 loss: 0.541606  [  128/  265]
train() client id: f_00001-4-4 loss: 0.598852  [  160/  265]
train() client id: f_00001-4-5 loss: 0.578825  [  192/  265]
train() client id: f_00001-4-6 loss: 0.548852  [  224/  265]
train() client id: f_00001-4-7 loss: 0.653675  [  256/  265]
train() client id: f_00002-0-0 loss: 0.924188  [   32/  124]
train() client id: f_00002-0-1 loss: 0.959153  [   64/  124]
train() client id: f_00002-0-2 loss: 0.862398  [   96/  124]
train() client id: f_00002-1-0 loss: 0.894701  [   32/  124]
train() client id: f_00002-1-1 loss: 0.890406  [   64/  124]
train() client id: f_00002-1-2 loss: 0.848875  [   96/  124]
train() client id: f_00002-2-0 loss: 0.747466  [   32/  124]
train() client id: f_00002-2-1 loss: 0.896654  [   64/  124]
train() client id: f_00002-2-2 loss: 0.895130  [   96/  124]
train() client id: f_00002-3-0 loss: 0.780245  [   32/  124]
train() client id: f_00002-3-1 loss: 0.990924  [   64/  124]
train() client id: f_00002-3-2 loss: 0.928194  [   96/  124]
train() client id: f_00002-4-0 loss: 0.943352  [   32/  124]
train() client id: f_00002-4-1 loss: 0.738894  [   64/  124]
train() client id: f_00002-4-2 loss: 0.900751  [   96/  124]
train() client id: f_00003-0-0 loss: 0.475137  [   32/   43]
train() client id: f_00003-1-0 loss: 0.507510  [   32/   43]
train() client id: f_00003-2-0 loss: 0.499038  [   32/   43]
train() client id: f_00003-3-0 loss: 0.693744  [   32/   43]
train() client id: f_00003-4-0 loss: 0.328703  [   32/   43]
train() client id: f_00004-0-0 loss: 0.716431  [   32/  306]
train() client id: f_00004-0-1 loss: 0.778328  [   64/  306]
train() client id: f_00004-0-2 loss: 0.847018  [   96/  306]
train() client id: f_00004-0-3 loss: 0.707364  [  128/  306]
train() client id: f_00004-0-4 loss: 0.720092  [  160/  306]
train() client id: f_00004-0-5 loss: 0.861552  [  192/  306]
train() client id: f_00004-0-6 loss: 0.889980  [  224/  306]
train() client id: f_00004-0-7 loss: 0.703639  [  256/  306]
train() client id: f_00004-0-8 loss: 0.811876  [  288/  306]
train() client id: f_00004-1-0 loss: 0.625199  [   32/  306]
train() client id: f_00004-1-1 loss: 0.739017  [   64/  306]
train() client id: f_00004-1-2 loss: 0.893869  [   96/  306]
train() client id: f_00004-1-3 loss: 0.884003  [  128/  306]
train() client id: f_00004-1-4 loss: 0.834541  [  160/  306]
train() client id: f_00004-1-5 loss: 0.753506  [  192/  306]
train() client id: f_00004-1-6 loss: 0.819232  [  224/  306]
train() client id: f_00004-1-7 loss: 0.753642  [  256/  306]
train() client id: f_00004-1-8 loss: 0.690168  [  288/  306]
train() client id: f_00004-2-0 loss: 0.796828  [   32/  306]
train() client id: f_00004-2-1 loss: 0.867071  [   64/  306]
train() client id: f_00004-2-2 loss: 0.771976  [   96/  306]
train() client id: f_00004-2-3 loss: 0.826982  [  128/  306]
train() client id: f_00004-2-4 loss: 0.810367  [  160/  306]
train() client id: f_00004-2-5 loss: 0.809504  [  192/  306]
train() client id: f_00004-2-6 loss: 0.744577  [  224/  306]
train() client id: f_00004-2-7 loss: 0.612538  [  256/  306]
train() client id: f_00004-2-8 loss: 0.829762  [  288/  306]
train() client id: f_00004-3-0 loss: 0.730061  [   32/  306]
train() client id: f_00004-3-1 loss: 0.782879  [   64/  306]
train() client id: f_00004-3-2 loss: 0.857526  [   96/  306]
train() client id: f_00004-3-3 loss: 0.777914  [  128/  306]
train() client id: f_00004-3-4 loss: 0.811932  [  160/  306]
train() client id: f_00004-3-5 loss: 0.727205  [  192/  306]
train() client id: f_00004-3-6 loss: 0.761522  [  224/  306]
train() client id: f_00004-3-7 loss: 0.933758  [  256/  306]
train() client id: f_00004-3-8 loss: 0.664424  [  288/  306]
train() client id: f_00004-4-0 loss: 0.760438  [   32/  306]
train() client id: f_00004-4-1 loss: 0.840369  [   64/  306]
train() client id: f_00004-4-2 loss: 0.714000  [   96/  306]
train() client id: f_00004-4-3 loss: 0.894418  [  128/  306]
train() client id: f_00004-4-4 loss: 0.788075  [  160/  306]
train() client id: f_00004-4-5 loss: 0.705112  [  192/  306]
train() client id: f_00004-4-6 loss: 0.872099  [  224/  306]
train() client id: f_00004-4-7 loss: 0.717480  [  256/  306]
train() client id: f_00004-4-8 loss: 0.730119  [  288/  306]
train() client id: f_00005-0-0 loss: 0.813073  [   32/  146]
train() client id: f_00005-0-1 loss: 0.454380  [   64/  146]
train() client id: f_00005-0-2 loss: 0.600639  [   96/  146]
train() client id: f_00005-0-3 loss: 0.501367  [  128/  146]
train() client id: f_00005-1-0 loss: 0.796605  [   32/  146]
train() client id: f_00005-1-1 loss: 0.437748  [   64/  146]
train() client id: f_00005-1-2 loss: 0.479449  [   96/  146]
train() client id: f_00005-1-3 loss: 0.569499  [  128/  146]
train() client id: f_00005-2-0 loss: 0.421641  [   32/  146]
train() client id: f_00005-2-1 loss: 0.489935  [   64/  146]
train() client id: f_00005-2-2 loss: 0.746991  [   96/  146]
train() client id: f_00005-2-3 loss: 0.663398  [  128/  146]
train() client id: f_00005-3-0 loss: 0.604630  [   32/  146]
train() client id: f_00005-3-1 loss: 0.508511  [   64/  146]
train() client id: f_00005-3-2 loss: 0.458291  [   96/  146]
train() client id: f_00005-3-3 loss: 0.620326  [  128/  146]
train() client id: f_00005-4-0 loss: 0.636954  [   32/  146]
train() client id: f_00005-4-1 loss: 0.751251  [   64/  146]
train() client id: f_00005-4-2 loss: 0.405247  [   96/  146]
train() client id: f_00005-4-3 loss: 0.284958  [  128/  146]
train() client id: f_00006-0-0 loss: 0.566653  [   32/   54]
train() client id: f_00006-1-0 loss: 0.574935  [   32/   54]
train() client id: f_00006-2-0 loss: 0.574389  [   32/   54]
train() client id: f_00006-3-0 loss: 0.517973  [   32/   54]
train() client id: f_00006-4-0 loss: 0.468087  [   32/   54]
train() client id: f_00007-0-0 loss: 0.830159  [   32/  179]
train() client id: f_00007-0-1 loss: 0.662252  [   64/  179]
train() client id: f_00007-0-2 loss: 0.682876  [   96/  179]
train() client id: f_00007-0-3 loss: 0.738872  [  128/  179]
train() client id: f_00007-0-4 loss: 0.677511  [  160/  179]
train() client id: f_00007-1-0 loss: 0.984234  [   32/  179]
train() client id: f_00007-1-1 loss: 0.555221  [   64/  179]
train() client id: f_00007-1-2 loss: 0.510888  [   96/  179]
train() client id: f_00007-1-3 loss: 0.772929  [  128/  179]
train() client id: f_00007-1-4 loss: 0.661508  [  160/  179]
train() client id: f_00007-2-0 loss: 0.741881  [   32/  179]
train() client id: f_00007-2-1 loss: 0.689223  [   64/  179]
train() client id: f_00007-2-2 loss: 0.505873  [   96/  179]
train() client id: f_00007-2-3 loss: 0.910212  [  128/  179]
train() client id: f_00007-2-4 loss: 0.682633  [  160/  179]
train() client id: f_00007-3-0 loss: 0.727521  [   32/  179]
train() client id: f_00007-3-1 loss: 0.675518  [   64/  179]
train() client id: f_00007-3-2 loss: 0.738499  [   96/  179]
train() client id: f_00007-3-3 loss: 0.541865  [  128/  179]
train() client id: f_00007-3-4 loss: 0.695328  [  160/  179]
train() client id: f_00007-4-0 loss: 0.672610  [   32/  179]
train() client id: f_00007-4-1 loss: 0.586741  [   64/  179]
train() client id: f_00007-4-2 loss: 0.659997  [   96/  179]
train() client id: f_00007-4-3 loss: 0.886201  [  128/  179]
train() client id: f_00007-4-4 loss: 0.662615  [  160/  179]
train() client id: f_00008-0-0 loss: 0.684994  [   32/  130]
train() client id: f_00008-0-1 loss: 0.862263  [   64/  130]
train() client id: f_00008-0-2 loss: 0.787269  [   96/  130]
train() client id: f_00008-0-3 loss: 0.656758  [  128/  130]
train() client id: f_00008-1-0 loss: 0.869119  [   32/  130]
train() client id: f_00008-1-1 loss: 0.716024  [   64/  130]
train() client id: f_00008-1-2 loss: 0.731403  [   96/  130]
train() client id: f_00008-1-3 loss: 0.660383  [  128/  130]
train() client id: f_00008-2-0 loss: 0.651999  [   32/  130]
train() client id: f_00008-2-1 loss: 0.760513  [   64/  130]
train() client id: f_00008-2-2 loss: 0.827463  [   96/  130]
train() client id: f_00008-2-3 loss: 0.753619  [  128/  130]
train() client id: f_00008-3-0 loss: 0.849558  [   32/  130]
train() client id: f_00008-3-1 loss: 0.707604  [   64/  130]
train() client id: f_00008-3-2 loss: 0.737584  [   96/  130]
train() client id: f_00008-3-3 loss: 0.692969  [  128/  130]
train() client id: f_00008-4-0 loss: 0.582421  [   32/  130]
train() client id: f_00008-4-1 loss: 0.786594  [   64/  130]
train() client id: f_00008-4-2 loss: 0.841614  [   96/  130]
train() client id: f_00008-4-3 loss: 0.765939  [  128/  130]
train() client id: f_00009-0-0 loss: 0.917924  [   32/  118]
train() client id: f_00009-0-1 loss: 0.889991  [   64/  118]
train() client id: f_00009-0-2 loss: 0.519894  [   96/  118]
train() client id: f_00009-1-0 loss: 0.784083  [   32/  118]
train() client id: f_00009-1-1 loss: 0.830540  [   64/  118]
train() client id: f_00009-1-2 loss: 0.699013  [   96/  118]
train() client id: f_00009-2-0 loss: 0.862925  [   32/  118]
train() client id: f_00009-2-1 loss: 0.710169  [   64/  118]
train() client id: f_00009-2-2 loss: 0.607639  [   96/  118]
train() client id: f_00009-3-0 loss: 0.733181  [   32/  118]
train() client id: f_00009-3-1 loss: 0.685631  [   64/  118]
train() client id: f_00009-3-2 loss: 0.750177  [   96/  118]
train() client id: f_00009-4-0 loss: 0.928731  [   32/  118]
train() client id: f_00009-4-1 loss: 0.549055  [   64/  118]
train() client id: f_00009-4-2 loss: 0.738362  [   96/  118]
At round 74 accuracy: 0.6472148541114059
At round 74 training accuracy: 0.5888665325285044
At round 74 training loss: 0.8280068441340147
update_location
xs = [  -3.9056584     4.20031788  390.00902392   18.81129433    0.97929623
    3.95640986 -352.44319194 -331.32485185  374.66397685 -317.06087855]
ys = [ 382.5879595   365.55583871    1.32061395 -352.45517586  344.35018685
  327.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [395.46023937 379.01017649 402.62734974 366.84944567 358.57776033
 342.75030358 366.36470093 346.08789882 388.17749229 332.48099579]
dists_bs = [269.00935793 261.58035399 591.10043668 561.93574397 244.22980671
 235.32437464 251.07114812 234.04126509 571.76481643 222.31011324]
uav_gains = [7.52650140e-13 8.76404134e-13 7.08086688e-13 9.93683329e-13
 1.09055942e-12 1.32968634e-12 9.98935416e-13 1.27217390e-12
 8.03318596e-13 1.53701405e-12]
bs_gains = [1.73758236e-11 1.87931569e-11 1.91709000e-12 2.20887654e-12
 2.27749704e-11 2.52712502e-11 2.10796318e-11 2.56610996e-11
 2.10419163e-12 2.96352278e-11]
Round 75
-------------------------------
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.03922851 2.03415137 1.02725189 0.39661097 2.34431442 1.12928775
 0.47989662 1.41999007 1.0345605  0.91598534]
obj_prev = 11.821277437641127
eta_min = 2.632614097881737e-91	eta_max = 0.9674950324086027
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 2.6763069615753383	eta = 0.9090909090909091
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 8.03601399311375	eta = 0.3027628287842397
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.8844582536009025	eta = 0.4981118073659859
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.372075946270912	eta = 0.5564876636646818
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3396007499684774	eta = 0.5606521126909028
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3394472210647415	eta = 0.5606719484671788
eta = 0.5606719484671788
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.04774628 0.10041876 0.0469884  0.01629436 0.11595525 0.05532505
 0.02046269 0.06783003 0.04926203 0.04471477]
ene_total = [0.45740881 0.62055196 0.46033181 0.25063986 0.70629813 0.3612359
 0.2738053  0.5227749  0.38706651 0.29933403]
ti_comp = [4.2992849  4.53835717 4.28655232 4.34933896 4.54253515 4.54464388
 4.35017711 4.38486951 4.43846938 4.54768586]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [3.68049423e-07 3.07275006e-06 3.52887078e-07 1.42937379e-08
 4.72231164e-06 5.12443289e-07 2.82979099e-08 1.01445480e-06
 3.79271804e-07 2.70179295e-07]
ene_total = [0.18661622 0.05104089 0.19383747 0.15822588 0.04868069 0.04746084
 0.1577506  0.13808028 0.10767732 0.0457342 ]
optimize_network iter = 0 obj = 1.1351043989055445
eta = 0.5606719484671788
freqs = [ 5552816.27091243 11063338.14405196  5480908.36118572  1873200.15342277
 12763274.63448044  6086841.3557449   2351937.65383736  7734555.17253756
  5549439.33843933  4916211.22498621]
eta_min = 0.5606719484671795	eta_max = 0.8680569114774389
af = 4.622705191075305e-05	bf = 0.4986564559927847	zeta = 5.084975710182836e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [6.06251059e-08 5.06143430e-07 5.81275641e-08 2.35446469e-09
 7.77859235e-07 8.44096653e-08 4.66123208e-09 1.67101009e-07
 6.24736566e-08 4.45039370e-08]
ene_total = [0.90183473 0.24659032 0.93673252 0.76464342 0.23514662 0.22934794
 0.76234626 0.66726472 0.52035367 0.22100929]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 1 obj = 3.7793676563367
eta = 0.8680569114774389
freqs = [ 5502474.32221998  9444523.43544286  5480908.36118572  1793229.33817215
 10870816.12174629  5178356.71698614  2250264.85743831  7233513.5913764
  5018771.87691086  4175523.98380345]
eta_min = 0.8680569114774714	eta_max = 0.8680569114774374
af = 3.523192231617816e-05	bf = 0.4986564559927847	zeta = 3.8755114547795976e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [5.95308319e-08 3.68859801e-07 5.81275641e-08 2.15772197e-09
 5.64288349e-07 6.10930969e-08 4.26693859e-09 1.46152750e-07
 5.10967904e-08 3.21040135e-08]
ene_total = [0.9018347  0.24658655 0.93673252 0.76464342 0.23514077 0.2293473
 0.76234625 0.66726414 0.52035336 0.22100895]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 2 obj = 3.779367656336656
eta = 0.8680569114774374
freqs = [ 5502474.32221997  9444523.43544286  5480908.3611857   1793229.33817215
 10870816.12174629  5178356.71698613  2250264.85743831  7233513.59137638
  5018771.87691085  4175523.98380344]
Done!
At round 75 energy consumption: 3.7793676563367
At round 75 eta: 0.8680569114774374
At round 75 local rounds: 4.633363424333758
At round 75 global rounds: 18.884386908447258
At round 75 a_n: 2.1491184865862607
gradient difference: 0.7825359106063843
train() client id: f_00000-0-0 loss: 0.789281  [   32/  126]
train() client id: f_00000-0-1 loss: 0.989279  [   64/  126]
train() client id: f_00000-0-2 loss: 0.957950  [   96/  126]
train() client id: f_00000-1-0 loss: 1.021271  [   32/  126]
train() client id: f_00000-1-1 loss: 0.962526  [   64/  126]
train() client id: f_00000-1-2 loss: 0.755491  [   96/  126]
train() client id: f_00000-2-0 loss: 0.931818  [   32/  126]
train() client id: f_00000-2-1 loss: 0.902102  [   64/  126]
train() client id: f_00000-2-2 loss: 0.787962  [   96/  126]
train() client id: f_00000-3-0 loss: 0.898386  [   32/  126]
train() client id: f_00000-3-1 loss: 0.753593  [   64/  126]
train() client id: f_00000-3-2 loss: 0.840912  [   96/  126]
train() client id: f_00001-0-0 loss: 0.510034  [   32/  265]
train() client id: f_00001-0-1 loss: 0.400518  [   64/  265]
train() client id: f_00001-0-2 loss: 0.505047  [   96/  265]
train() client id: f_00001-0-3 loss: 0.413966  [  128/  265]
train() client id: f_00001-0-4 loss: 0.523884  [  160/  265]
train() client id: f_00001-0-5 loss: 0.494238  [  192/  265]
train() client id: f_00001-0-6 loss: 0.483231  [  224/  265]
train() client id: f_00001-0-7 loss: 0.516804  [  256/  265]
train() client id: f_00001-1-0 loss: 0.667488  [   32/  265]
train() client id: f_00001-1-1 loss: 0.470008  [   64/  265]
train() client id: f_00001-1-2 loss: 0.428458  [   96/  265]
train() client id: f_00001-1-3 loss: 0.360559  [  128/  265]
train() client id: f_00001-1-4 loss: 0.424915  [  160/  265]
train() client id: f_00001-1-5 loss: 0.444247  [  192/  265]
train() client id: f_00001-1-6 loss: 0.464437  [  224/  265]
train() client id: f_00001-1-7 loss: 0.537657  [  256/  265]
train() client id: f_00001-2-0 loss: 0.403416  [   32/  265]
train() client id: f_00001-2-1 loss: 0.495185  [   64/  265]
train() client id: f_00001-2-2 loss: 0.461124  [   96/  265]
train() client id: f_00001-2-3 loss: 0.518095  [  128/  265]
train() client id: f_00001-2-4 loss: 0.450426  [  160/  265]
train() client id: f_00001-2-5 loss: 0.388477  [  192/  265]
train() client id: f_00001-2-6 loss: 0.479324  [  224/  265]
train() client id: f_00001-2-7 loss: 0.383870  [  256/  265]
train() client id: f_00001-3-0 loss: 0.364053  [   32/  265]
train() client id: f_00001-3-1 loss: 0.597557  [   64/  265]
train() client id: f_00001-3-2 loss: 0.383597  [   96/  265]
train() client id: f_00001-3-3 loss: 0.512280  [  128/  265]
train() client id: f_00001-3-4 loss: 0.487615  [  160/  265]
train() client id: f_00001-3-5 loss: 0.515912  [  192/  265]
train() client id: f_00001-3-6 loss: 0.429799  [  224/  265]
train() client id: f_00001-3-7 loss: 0.455576  [  256/  265]
train() client id: f_00002-0-0 loss: 0.914897  [   32/  124]
train() client id: f_00002-0-1 loss: 0.909559  [   64/  124]
train() client id: f_00002-0-2 loss: 0.923394  [   96/  124]
train() client id: f_00002-1-0 loss: 1.013984  [   32/  124]
train() client id: f_00002-1-1 loss: 0.798808  [   64/  124]
train() client id: f_00002-1-2 loss: 1.037092  [   96/  124]
train() client id: f_00002-2-0 loss: 0.927021  [   32/  124]
train() client id: f_00002-2-1 loss: 0.986835  [   64/  124]
train() client id: f_00002-2-2 loss: 0.961548  [   96/  124]
train() client id: f_00002-3-0 loss: 0.745458  [   32/  124]
train() client id: f_00002-3-1 loss: 0.991292  [   64/  124]
train() client id: f_00002-3-2 loss: 0.968252  [   96/  124]
train() client id: f_00003-0-0 loss: 1.112967  [   32/   43]
train() client id: f_00003-1-0 loss: 0.793948  [   32/   43]
train() client id: f_00003-2-0 loss: 0.860081  [   32/   43]
train() client id: f_00003-3-0 loss: 0.781062  [   32/   43]
train() client id: f_00004-0-0 loss: 0.921871  [   32/  306]
train() client id: f_00004-0-1 loss: 0.846192  [   64/  306]
train() client id: f_00004-0-2 loss: 0.856824  [   96/  306]
train() client id: f_00004-0-3 loss: 0.914987  [  128/  306]
train() client id: f_00004-0-4 loss: 0.943400  [  160/  306]
train() client id: f_00004-0-5 loss: 0.861905  [  192/  306]
train() client id: f_00004-0-6 loss: 0.929296  [  224/  306]
train() client id: f_00004-0-7 loss: 0.937328  [  256/  306]
train() client id: f_00004-0-8 loss: 0.951963  [  288/  306]
train() client id: f_00004-1-0 loss: 0.894849  [   32/  306]
train() client id: f_00004-1-1 loss: 0.879845  [   64/  306]
train() client id: f_00004-1-2 loss: 0.838592  [   96/  306]
train() client id: f_00004-1-3 loss: 0.924059  [  128/  306]
train() client id: f_00004-1-4 loss: 0.813116  [  160/  306]
train() client id: f_00004-1-5 loss: 0.940999  [  192/  306]
train() client id: f_00004-1-6 loss: 0.876443  [  224/  306]
train() client id: f_00004-1-7 loss: 1.036944  [  256/  306]
train() client id: f_00004-1-8 loss: 0.894329  [  288/  306]
train() client id: f_00004-2-0 loss: 0.753322  [   32/  306]
train() client id: f_00004-2-1 loss: 1.028852  [   64/  306]
train() client id: f_00004-2-2 loss: 0.945868  [   96/  306]
train() client id: f_00004-2-3 loss: 0.872300  [  128/  306]
train() client id: f_00004-2-4 loss: 0.940820  [  160/  306]
train() client id: f_00004-2-5 loss: 0.841373  [  192/  306]
train() client id: f_00004-2-6 loss: 0.941220  [  224/  306]
train() client id: f_00004-2-7 loss: 0.714592  [  256/  306]
train() client id: f_00004-2-8 loss: 1.073281  [  288/  306]
train() client id: f_00004-3-0 loss: 0.993955  [   32/  306]
train() client id: f_00004-3-1 loss: 0.896514  [   64/  306]
train() client id: f_00004-3-2 loss: 0.929809  [   96/  306]
train() client id: f_00004-3-3 loss: 0.899123  [  128/  306]
train() client id: f_00004-3-4 loss: 0.974883  [  160/  306]
train() client id: f_00004-3-5 loss: 0.976250  [  192/  306]
train() client id: f_00004-3-6 loss: 0.793811  [  224/  306]
train() client id: f_00004-3-7 loss: 0.855008  [  256/  306]
train() client id: f_00004-3-8 loss: 0.775639  [  288/  306]
train() client id: f_00005-0-0 loss: 0.476179  [   32/  146]
train() client id: f_00005-0-1 loss: 0.908852  [   64/  146]
train() client id: f_00005-0-2 loss: 0.784684  [   96/  146]
train() client id: f_00005-0-3 loss: 1.169520  [  128/  146]
train() client id: f_00005-1-0 loss: 0.977246  [   32/  146]
train() client id: f_00005-1-1 loss: 0.818458  [   64/  146]
train() client id: f_00005-1-2 loss: 0.723997  [   96/  146]
train() client id: f_00005-1-3 loss: 0.633735  [  128/  146]
train() client id: f_00005-2-0 loss: 0.818650  [   32/  146]
train() client id: f_00005-2-1 loss: 0.774376  [   64/  146]
train() client id: f_00005-2-2 loss: 0.961902  [   96/  146]
train() client id: f_00005-2-3 loss: 1.004837  [  128/  146]
train() client id: f_00005-3-0 loss: 0.644563  [   32/  146]
train() client id: f_00005-3-1 loss: 0.946016  [   64/  146]
train() client id: f_00005-3-2 loss: 1.059026  [   96/  146]
train() client id: f_00005-3-3 loss: 0.809381  [  128/  146]
train() client id: f_00006-0-0 loss: 0.494724  [   32/   54]
train() client id: f_00006-1-0 loss: 0.479014  [   32/   54]
train() client id: f_00006-2-0 loss: 0.528508  [   32/   54]
train() client id: f_00006-3-0 loss: 0.602669  [   32/   54]
train() client id: f_00007-0-0 loss: 0.607587  [   32/  179]
train() client id: f_00007-0-1 loss: 0.598424  [   64/  179]
train() client id: f_00007-0-2 loss: 1.057918  [   96/  179]
train() client id: f_00007-0-3 loss: 0.482138  [  128/  179]
train() client id: f_00007-0-4 loss: 0.570133  [  160/  179]
train() client id: f_00007-1-0 loss: 0.456942  [   32/  179]
train() client id: f_00007-1-1 loss: 0.660677  [   64/  179]
train() client id: f_00007-1-2 loss: 0.673936  [   96/  179]
train() client id: f_00007-1-3 loss: 0.699255  [  128/  179]
train() client id: f_00007-1-4 loss: 0.536154  [  160/  179]
train() client id: f_00007-2-0 loss: 0.745081  [   32/  179]
train() client id: f_00007-2-1 loss: 0.554245  [   64/  179]
train() client id: f_00007-2-2 loss: 0.503524  [   96/  179]
train() client id: f_00007-2-3 loss: 0.837140  [  128/  179]
train() client id: f_00007-2-4 loss: 0.621797  [  160/  179]
train() client id: f_00007-3-0 loss: 0.527222  [   32/  179]
train() client id: f_00007-3-1 loss: 0.754000  [   64/  179]
train() client id: f_00007-3-2 loss: 0.763141  [   96/  179]
train() client id: f_00007-3-3 loss: 0.484992  [  128/  179]
train() client id: f_00007-3-4 loss: 0.720822  [  160/  179]
train() client id: f_00008-0-0 loss: 0.649060  [   32/  130]
train() client id: f_00008-0-1 loss: 0.731555  [   64/  130]
train() client id: f_00008-0-2 loss: 0.799172  [   96/  130]
train() client id: f_00008-0-3 loss: 0.647077  [  128/  130]
train() client id: f_00008-1-0 loss: 0.663016  [   32/  130]
train() client id: f_00008-1-1 loss: 0.743909  [   64/  130]
train() client id: f_00008-1-2 loss: 0.759907  [   96/  130]
train() client id: f_00008-1-3 loss: 0.667487  [  128/  130]
train() client id: f_00008-2-0 loss: 0.680552  [   32/  130]
train() client id: f_00008-2-1 loss: 0.813964  [   64/  130]
train() client id: f_00008-2-2 loss: 0.668774  [   96/  130]
train() client id: f_00008-2-3 loss: 0.616598  [  128/  130]
train() client id: f_00008-3-0 loss: 0.672665  [   32/  130]
train() client id: f_00008-3-1 loss: 0.739583  [   64/  130]
train() client id: f_00008-3-2 loss: 0.698553  [   96/  130]
train() client id: f_00008-3-3 loss: 0.728424  [  128/  130]
train() client id: f_00009-0-0 loss: 0.962151  [   32/  118]
train() client id: f_00009-0-1 loss: 0.851970  [   64/  118]
train() client id: f_00009-0-2 loss: 0.725517  [   96/  118]
train() client id: f_00009-1-0 loss: 0.743594  [   32/  118]
train() client id: f_00009-1-1 loss: 0.655403  [   64/  118]
train() client id: f_00009-1-2 loss: 0.969053  [   96/  118]
train() client id: f_00009-2-0 loss: 0.847786  [   32/  118]
train() client id: f_00009-2-1 loss: 0.809607  [   64/  118]
train() client id: f_00009-2-2 loss: 0.694828  [   96/  118]
train() client id: f_00009-3-0 loss: 0.750900  [   32/  118]
train() client id: f_00009-3-1 loss: 0.947144  [   64/  118]
train() client id: f_00009-3-2 loss: 0.732860  [   96/  118]
At round 75 accuracy: 0.649867374005305
At round 75 training accuracy: 0.5888665325285044
At round 75 training loss: 0.8110463125076958
update_location
xs = [  -3.9056584     4.20031788  395.00902392   18.81129433    0.97929623
    3.95640986 -357.44319194 -336.32485185  379.66397685 -322.06087855]
ys = [ 387.5879595   370.55583871    1.32061395 -357.45517586  349.35018685
  332.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [400.29948853 383.83495447 407.47254263 371.65584557 363.38204698
 347.53548324 371.17721644 350.87758866 393.00560465 337.25245937]
dists_bs = [272.88626614 265.24486796 595.88154568 566.63156646 247.72303154
 238.60155659 254.62944319 237.41011412 576.57319143 225.53801284]
uav_gains = [7.22036186e-13 8.36522048e-13 6.80532523e-13 9.44154307e-13
 1.03236608e-12 1.24853056e-12 9.48887642e-13 1.19672866e-12
 7.69075581e-13 1.43449114e-12]
bs_gains = [1.66934225e-11 1.80751751e-11 1.87433090e-12 2.15800250e-12
 2.18870991e-11 2.43113414e-11 2.02651559e-11 2.46545045e-11
 2.05542502e-12 2.84628764e-11]
Round 76
-------------------------------
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.89822293 1.75469717 0.88790864 0.34389424 2.02220671 0.97420485
 0.41573163 1.22653712 0.89277918 0.79022156]
obj_prev = 10.206404031521155
eta_min = 1.1840901563596855e-105	eta_max = 0.9712061927430241
af = 2.098524592010151	bf = 0.440277105517973	zeta = 2.308377051211166	eta = 0.9090909090909091
af = 2.098524592010151	bf = 0.440277105517973	zeta = 7.043166478768837	eta = 0.29795186559000353
af = 2.098524592010151	bf = 0.440277105517973	zeta = 4.246460455912228	eta = 0.49418206381468455
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.794695110359663	eta = 0.5530153361415251
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.766078122653466	eta = 0.557217488237231
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.765942672970741	eta = 0.5572375296819754
eta = 0.5572375296819754
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.0482533  0.10148511 0.04748737 0.0164674  0.11718658 0.05591255
 0.02067998 0.06855032 0.04978515 0.0451896 ]
ene_total = [0.39809468 0.53669908 0.4005926  0.21938841 0.6108545  0.31238457
 0.23943255 0.45484532 0.3348021  0.25884885]
ti_comp = [5.06004778 5.30681242 5.04724229 5.11036031 5.31105174 5.31322071
 5.11119117 5.14610005 5.20576198 5.31628572]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [2.74253247e-07 2.31963433e-06 2.62727860e-07 1.06868990e-08
 3.56576398e-06 3.86983308e-07 2.11585747e-08 7.60242285e-07
 2.84584225e-07 2.04069817e-07]
ene_total = [0.16388001 0.0441132  0.1700956  0.13945761 0.04206153 0.04099331
 0.13905437 0.12211358 0.09315205 0.0395047 ]
optimize_network iter = 0 obj = 0.9944259543834252
eta = 0.5572375296819754
freqs = [ 4768067.39444     9561776.65897993  4704288.96039803  1611177.51564226
 11032332.74977597  5261643.95832219  2023010.29856296  6660414.59689484
  4781734.99264716  4250109.82064523]
eta_min = 0.5572375296819757	eta_max = 0.8837926289249317
af = 2.9732682841315033e-05	bf = 0.440277105517973	zeta = 3.2705951125446536e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.47003242e-08 3.78075401e-07 4.28218103e-08 1.74184938e-09
 5.81181107e-07 6.30741094e-08 3.44861970e-09 1.23911301e-07
 4.63841622e-08 3.32611812e-08]
ene_total = [0.79815367 0.21480247 0.82842621 0.67921232 0.20478538 0.19964562
 0.67724819 0.59472538 0.45368094 0.19239915]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 1 obj = 3.7887429688072136
eta = 0.8837926289249317
freqs = [4723436.47190019 8085223.78253195 4704288.96039802 1540154.03017325
 9306387.77331266 4433073.47420022 1932725.03801711 6214669.51457857
 4293583.32001691 3574670.85257318]
eta_min = 0.8837926289249345	eta_max = 0.8837926289249313
af = 2.229825334963228e-05	bf = 0.440277105517973	zeta = 2.452807868459551e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.38674166e-08 2.70324445e-07 4.28218103e-08 1.59166668e-09
 4.13560520e-07 4.47731935e-08 3.14767049e-09 1.07880876e-07
 3.73971494e-08 2.35293258e-08]
ene_total = [0.79815365 0.21479992 0.82842621 0.67921232 0.20478142 0.19964519
 0.67724818 0.594725   0.45368072 0.19239892]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 2 obj = 3.788742968807199
eta = 0.8837926289249313
freqs = [4723436.47190018 8085223.78253194 4704288.96039801 1540154.03017325
 9306387.77331265 4433073.47420021 1932725.03801711 6214669.51457855
 4293583.3200169  3574670.85257317]
Done!
At round 76 energy consumption: 3.7887429688072136
At round 76 eta: 0.8837926289249313
At round 76 local rounds: 4.045092362532633
At round 76 global rounds: 18.49382243745926
At round 76 a_n: 1.8065726396169453
gradient difference: 0.9526262283325195
train() client id: f_00000-0-0 loss: 0.726152  [   32/  126]
train() client id: f_00000-0-1 loss: 0.924934  [   64/  126]
train() client id: f_00000-0-2 loss: 0.899302  [   96/  126]
train() client id: f_00000-1-0 loss: 0.791772  [   32/  126]
train() client id: f_00000-1-1 loss: 0.847046  [   64/  126]
train() client id: f_00000-1-2 loss: 0.995031  [   96/  126]
train() client id: f_00000-2-0 loss: 0.941046  [   32/  126]
train() client id: f_00000-2-1 loss: 0.957251  [   64/  126]
train() client id: f_00000-2-2 loss: 0.841188  [   96/  126]
train() client id: f_00000-3-0 loss: 0.858241  [   32/  126]
train() client id: f_00000-3-1 loss: 0.823230  [   64/  126]
train() client id: f_00000-3-2 loss: 0.956027  [   96/  126]
train() client id: f_00001-0-0 loss: 0.501527  [   32/  265]
train() client id: f_00001-0-1 loss: 0.431165  [   64/  265]
train() client id: f_00001-0-2 loss: 0.551047  [   96/  265]
train() client id: f_00001-0-3 loss: 0.482920  [  128/  265]
train() client id: f_00001-0-4 loss: 0.537975  [  160/  265]
train() client id: f_00001-0-5 loss: 0.565862  [  192/  265]
train() client id: f_00001-0-6 loss: 0.541890  [  224/  265]
train() client id: f_00001-0-7 loss: 0.509006  [  256/  265]
train() client id: f_00001-1-0 loss: 0.644606  [   32/  265]
train() client id: f_00001-1-1 loss: 0.511691  [   64/  265]
train() client id: f_00001-1-2 loss: 0.462719  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482850  [  128/  265]
train() client id: f_00001-1-4 loss: 0.455831  [  160/  265]
train() client id: f_00001-1-5 loss: 0.647087  [  192/  265]
train() client id: f_00001-1-6 loss: 0.422071  [  224/  265]
train() client id: f_00001-1-7 loss: 0.456949  [  256/  265]
train() client id: f_00001-2-0 loss: 0.467941  [   32/  265]
train() client id: f_00001-2-1 loss: 0.413724  [   64/  265]
train() client id: f_00001-2-2 loss: 0.510159  [   96/  265]
train() client id: f_00001-2-3 loss: 0.606681  [  128/  265]
train() client id: f_00001-2-4 loss: 0.556452  [  160/  265]
train() client id: f_00001-2-5 loss: 0.551301  [  192/  265]
train() client id: f_00001-2-6 loss: 0.494583  [  224/  265]
train() client id: f_00001-2-7 loss: 0.553329  [  256/  265]
train() client id: f_00001-3-0 loss: 0.532295  [   32/  265]
train() client id: f_00001-3-1 loss: 0.499477  [   64/  265]
train() client id: f_00001-3-2 loss: 0.539375  [   96/  265]
train() client id: f_00001-3-3 loss: 0.533995  [  128/  265]
train() client id: f_00001-3-4 loss: 0.676565  [  160/  265]
train() client id: f_00001-3-5 loss: 0.436162  [  192/  265]
train() client id: f_00001-3-6 loss: 0.503054  [  224/  265]
train() client id: f_00001-3-7 loss: 0.444703  [  256/  265]
train() client id: f_00002-0-0 loss: 1.054541  [   32/  124]
train() client id: f_00002-0-1 loss: 0.821433  [   64/  124]
train() client id: f_00002-0-2 loss: 1.167519  [   96/  124]
train() client id: f_00002-1-0 loss: 0.900315  [   32/  124]
train() client id: f_00002-1-1 loss: 1.021273  [   64/  124]
train() client id: f_00002-1-2 loss: 0.890619  [   96/  124]
train() client id: f_00002-2-0 loss: 0.943970  [   32/  124]
train() client id: f_00002-2-1 loss: 0.872162  [   64/  124]
train() client id: f_00002-2-2 loss: 0.892432  [   96/  124]
train() client id: f_00002-3-0 loss: 0.913336  [   32/  124]
train() client id: f_00002-3-1 loss: 0.778378  [   64/  124]
train() client id: f_00002-3-2 loss: 0.877738  [   96/  124]
train() client id: f_00003-0-0 loss: 0.650425  [   32/   43]
train() client id: f_00003-1-0 loss: 0.698552  [   32/   43]
train() client id: f_00003-2-0 loss: 0.620392  [   32/   43]
train() client id: f_00003-3-0 loss: 0.543401  [   32/   43]
train() client id: f_00004-0-0 loss: 0.835283  [   32/  306]
train() client id: f_00004-0-1 loss: 1.038159  [   64/  306]
train() client id: f_00004-0-2 loss: 0.894404  [   96/  306]
train() client id: f_00004-0-3 loss: 1.034156  [  128/  306]
train() client id: f_00004-0-4 loss: 0.942292  [  160/  306]
train() client id: f_00004-0-5 loss: 0.925399  [  192/  306]
train() client id: f_00004-0-6 loss: 0.901927  [  224/  306]
train() client id: f_00004-0-7 loss: 1.033312  [  256/  306]
train() client id: f_00004-0-8 loss: 1.023669  [  288/  306]
train() client id: f_00004-1-0 loss: 0.974469  [   32/  306]
train() client id: f_00004-1-1 loss: 1.033362  [   64/  306]
train() client id: f_00004-1-2 loss: 1.071639  [   96/  306]
train() client id: f_00004-1-3 loss: 0.829931  [  128/  306]
train() client id: f_00004-1-4 loss: 1.039419  [  160/  306]
train() client id: f_00004-1-5 loss: 0.884971  [  192/  306]
train() client id: f_00004-1-6 loss: 0.911384  [  224/  306]
train() client id: f_00004-1-7 loss: 0.971493  [  256/  306]
train() client id: f_00004-1-8 loss: 0.947911  [  288/  306]
train() client id: f_00004-2-0 loss: 1.078645  [   32/  306]
train() client id: f_00004-2-1 loss: 0.948245  [   64/  306]
train() client id: f_00004-2-2 loss: 1.001066  [   96/  306]
train() client id: f_00004-2-3 loss: 0.954240  [  128/  306]
train() client id: f_00004-2-4 loss: 0.985757  [  160/  306]
train() client id: f_00004-2-5 loss: 1.002786  [  192/  306]
train() client id: f_00004-2-6 loss: 0.954726  [  224/  306]
train() client id: f_00004-2-7 loss: 0.950538  [  256/  306]
train() client id: f_00004-2-8 loss: 0.768803  [  288/  306]
train() client id: f_00004-3-0 loss: 0.914612  [   32/  306]
train() client id: f_00004-3-1 loss: 0.956911  [   64/  306]
train() client id: f_00004-3-2 loss: 1.082323  [   96/  306]
train() client id: f_00004-3-3 loss: 0.905451  [  128/  306]
train() client id: f_00004-3-4 loss: 0.983932  [  160/  306]
train() client id: f_00004-3-5 loss: 0.954505  [  192/  306]
train() client id: f_00004-3-6 loss: 0.882935  [  224/  306]
train() client id: f_00004-3-7 loss: 0.823048  [  256/  306]
train() client id: f_00004-3-8 loss: 1.042978  [  288/  306]
train() client id: f_00005-0-0 loss: 1.018370  [   32/  146]
train() client id: f_00005-0-1 loss: 0.758305  [   64/  146]
train() client id: f_00005-0-2 loss: 0.583051  [   96/  146]
train() client id: f_00005-0-3 loss: 0.464947  [  128/  146]
train() client id: f_00005-1-0 loss: 0.819344  [   32/  146]
train() client id: f_00005-1-1 loss: 0.860039  [   64/  146]
train() client id: f_00005-1-2 loss: 0.517351  [   96/  146]
train() client id: f_00005-1-3 loss: 0.715200  [  128/  146]
train() client id: f_00005-2-0 loss: 0.577583  [   32/  146]
train() client id: f_00005-2-1 loss: 0.804797  [   64/  146]
train() client id: f_00005-2-2 loss: 0.506741  [   96/  146]
train() client id: f_00005-2-3 loss: 0.982987  [  128/  146]
train() client id: f_00005-3-0 loss: 0.633313  [   32/  146]
train() client id: f_00005-3-1 loss: 0.974614  [   64/  146]
train() client id: f_00005-3-2 loss: 0.436541  [   96/  146]
train() client id: f_00005-3-3 loss: 0.774896  [  128/  146]
train() client id: f_00006-0-0 loss: 0.447873  [   32/   54]
train() client id: f_00006-1-0 loss: 0.495461  [   32/   54]
train() client id: f_00006-2-0 loss: 0.507940  [   32/   54]
train() client id: f_00006-3-0 loss: 0.459705  [   32/   54]
train() client id: f_00007-0-0 loss: 0.432867  [   32/  179]
train() client id: f_00007-0-1 loss: 0.642470  [   64/  179]
train() client id: f_00007-0-2 loss: 0.445447  [   96/  179]
train() client id: f_00007-0-3 loss: 0.571930  [  128/  179]
train() client id: f_00007-0-4 loss: 0.681655  [  160/  179]
train() client id: f_00007-1-0 loss: 0.648363  [   32/  179]
train() client id: f_00007-1-1 loss: 0.631206  [   64/  179]
train() client id: f_00007-1-2 loss: 0.429211  [   96/  179]
train() client id: f_00007-1-3 loss: 0.627426  [  128/  179]
train() client id: f_00007-1-4 loss: 0.427026  [  160/  179]
train() client id: f_00007-2-0 loss: 0.494136  [   32/  179]
train() client id: f_00007-2-1 loss: 0.605858  [   64/  179]
train() client id: f_00007-2-2 loss: 0.727599  [   96/  179]
train() client id: f_00007-2-3 loss: 0.433756  [  128/  179]
train() client id: f_00007-2-4 loss: 0.629411  [  160/  179]
train() client id: f_00007-3-0 loss: 0.640530  [   32/  179]
train() client id: f_00007-3-1 loss: 0.463487  [   64/  179]
train() client id: f_00007-3-2 loss: 0.651959  [   96/  179]
train() client id: f_00007-3-3 loss: 0.361117  [  128/  179]
train() client id: f_00007-3-4 loss: 0.592395  [  160/  179]
train() client id: f_00008-0-0 loss: 0.711720  [   32/  130]
train() client id: f_00008-0-1 loss: 0.658802  [   64/  130]
train() client id: f_00008-0-2 loss: 0.763311  [   96/  130]
train() client id: f_00008-0-3 loss: 0.683999  [  128/  130]
train() client id: f_00008-1-0 loss: 0.781304  [   32/  130]
train() client id: f_00008-1-1 loss: 0.593370  [   64/  130]
train() client id: f_00008-1-2 loss: 0.653562  [   96/  130]
train() client id: f_00008-1-3 loss: 0.742364  [  128/  130]
train() client id: f_00008-2-0 loss: 0.580451  [   32/  130]
train() client id: f_00008-2-1 loss: 0.714542  [   64/  130]
train() client id: f_00008-2-2 loss: 0.727407  [   96/  130]
train() client id: f_00008-2-3 loss: 0.782610  [  128/  130]
train() client id: f_00008-3-0 loss: 0.747697  [   32/  130]
train() client id: f_00008-3-1 loss: 0.709095  [   64/  130]
train() client id: f_00008-3-2 loss: 0.693540  [   96/  130]
train() client id: f_00008-3-3 loss: 0.660686  [  128/  130]
train() client id: f_00009-0-0 loss: 0.977273  [   32/  118]
train() client id: f_00009-0-1 loss: 1.025589  [   64/  118]
train() client id: f_00009-0-2 loss: 0.578438  [   96/  118]
train() client id: f_00009-1-0 loss: 0.917529  [   32/  118]
train() client id: f_00009-1-1 loss: 0.879473  [   64/  118]
train() client id: f_00009-1-2 loss: 0.783912  [   96/  118]
train() client id: f_00009-2-0 loss: 0.997521  [   32/  118]
train() client id: f_00009-2-1 loss: 0.658684  [   64/  118]
train() client id: f_00009-2-2 loss: 0.838646  [   96/  118]
train() client id: f_00009-3-0 loss: 0.797532  [   32/  118]
train() client id: f_00009-3-1 loss: 0.915082  [   64/  118]
train() client id: f_00009-3-2 loss: 0.834841  [   96/  118]
At round 76 accuracy: 0.6472148541114059
At round 76 training accuracy: 0.5881958417169685
At round 76 training loss: 0.8205776083748891
update_location
xs = [  -3.9056584     4.20031788  400.00902392   18.81129433    0.97929623
    3.95640986 -362.44319194 -341.32485185  384.66397685 -327.06087855]
ys = [ 392.5879595   375.55583871    1.32061395 -362.45517586  354.35018685
  337.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [405.14264168 388.66416179 412.32143194 376.46728849 368.19154517
 352.32662917 375.99462486 355.67306722 397.83796332 342.03045206]
dists_bs = [276.79919408 268.95240911 600.66621906 571.33255103 251.26719289
 241.93768684 258.23552284 240.83565102 581.38479929 228.82963973]
uav_gains = [6.93541635e-13 7.99744288e-13 6.54792901e-13 8.98819590e-13
 9.79395095e-13 1.17540781e-12 9.03099454e-13 1.12860915e-12
 7.37325739e-13 1.34266847e-12]
bs_gains = [1.60410424e-11 1.73861274e-11 1.83282543e-12 2.10865230e-12
 2.10336130e-11 2.33842916e-11 1.94827084e-11 2.36851376e-11
 2.00814839e-12 2.73312636e-11]
Round 77
-------------------------------
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.7566319  1.47518382 0.74797579 0.2906086  1.70004424 0.81907155
 0.35099723 1.03253353 0.75085927 0.66440907]
obj_prev = 8.588314982517188
eta_min = nan	eta_max = nan
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 1.9404471408469977	eta = 0.9090909090909091
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 6.015300613417023	eta = 0.2932593013524188
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.5977300389560636	eta = 0.49032107362544536
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.2097433156161213	eta = 0.5495900082517465
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1851865241926465	eta = 0.5538271752429276
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1850702185308974	eta = 0.5538473987330524
eta = 0.5538473987330524
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.04875685 0.10254417 0.04798293 0.01663924 0.1184095  0.05649604
 0.02089579 0.06926569 0.05030469 0.04566118]
ene_total = [0.33763567 0.45238391 0.33971862 0.18710392 0.51488509 0.26328129
 0.20400615 0.38565213 0.28224513 0.2181583 ]
ti_comp = [6.11168561 6.36617567 6.09880235 6.16226391 6.37047552 6.37270362
 6.16308753 6.19819213 6.2639569  6.37579113]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [1.93938935e-07 1.66286030e-06 1.85631348e-07 7.58228597e-09
 2.55679530e-06 2.77515029e-07 1.50127409e-08 5.40635308e-07
 2.02771968e-07 1.46370394e-07]
ene_total = [0.14020895 0.03716612 0.14542564 0.1197279  0.03542864 0.0345172
 0.11939442 0.10518191 0.0785509  0.03326647]
optimize_network iter = 0 obj = 0.848868136074443
eta = 0.5538473987330524
freqs = [3988822.08036756 8053828.5629115  3933799.78027688 1350091.76188237
 9293615.50887199 4432658.36750978 1695237.49554636 5587571.86903341
 4015408.34280281 3580824.62742287]
eta_min = 0.5538473987330529	eta_max = 0.9002731777032497
af = 1.770021082250513e-05	bf = 0.3787142685879881	zeta = 1.9470231904755644e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.12834926e-08 2.68229162e-07 2.99434299e-08 1.22306739e-09
 4.12426142e-07 4.47648089e-08 2.42164354e-09 8.72076598e-08
 3.27083129e-08 2.36104067e-08]
ene_total = [0.68809723 0.18237166 0.71369919 0.58758591 0.17382975 0.1693947
 0.58594921 0.51618999 0.38549919 0.16325868]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 1 obj = 3.7975304096863765
eta = 0.9002731777032497
freqs = [3950315.32823697 6744042.83153091 3933799.78027688 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677077 5201849.38696053
 3579043.95899813 2981797.2351148 ]
eta_min = 0.8902239180004755	eta_max = 0.9002731777032479
af = 1.3057149041253039e-05	bf = 0.3787142685879881	zeta = 1.4362863945378344e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.06824073e-08 1.88079677e-07 2.99434299e-08 1.11439939e-09
 2.87746454e-07 3.11514510e-08 2.20390950e-09 7.55829668e-08
 2.59856015e-08 1.63716925e-08]
ene_total = [0.68809721 0.18237007 0.71369919 0.58758591 0.17382727 0.16939443
 0.58594921 0.51618975 0.38549905 0.16325854]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 2 obj = 3.7975304096863085
eta = 0.9002731777032479
freqs = [3950315.32823696 6744042.83153091 3933799.78027686 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677076 5201849.38696052
 3579043.95899813 2981797.2351148 ]
Done!
At round 77 energy consumption: 3.7975304096863765
At round 77 eta: 0.9002731777032479
At round 77 local rounds: 3.4401009464594003
At round 77 global rounds: 18.115213119307235
At round 77 a_n: 1.4640267926476263
gradient difference: 0.909521222114563
train() client id: f_00000-0-0 loss: 0.935666  [   32/  126]
train() client id: f_00000-0-1 loss: 0.799882  [   64/  126]
train() client id: f_00000-0-2 loss: 1.022378  [   96/  126]
train() client id: f_00000-1-0 loss: 0.796227  [   32/  126]
train() client id: f_00000-1-1 loss: 0.786592  [   64/  126]
train() client id: f_00000-1-2 loss: 0.893540  [   96/  126]
train() client id: f_00000-2-0 loss: 0.957099  [   32/  126]
train() client id: f_00000-2-1 loss: 0.781558  [   64/  126]
train() client id: f_00000-2-2 loss: 0.885419  [   96/  126]
train() client id: f_00001-0-0 loss: 0.602703  [   32/  265]
train() client id: f_00001-0-1 loss: 0.596986  [   64/  265]
train() client id: f_00001-0-2 loss: 0.592201  [   96/  265]
train() client id: f_00001-0-3 loss: 0.627514  [  128/  265]
train() client id: f_00001-0-4 loss: 0.640766  [  160/  265]
train() client id: f_00001-0-5 loss: 0.528954  [  192/  265]
train() client id: f_00001-0-6 loss: 0.534775  [  224/  265]
train() client id: f_00001-0-7 loss: 0.659889  [  256/  265]
train() client id: f_00001-1-0 loss: 0.550605  [   32/  265]
train() client id: f_00001-1-1 loss: 0.560118  [   64/  265]
train() client id: f_00001-1-2 loss: 0.639849  [   96/  265]
train() client id: f_00001-1-3 loss: 0.510668  [  128/  265]
train() client id: f_00001-1-4 loss: 0.674991  [  160/  265]
train() client id: f_00001-1-5 loss: 0.630305  [  192/  265]
train() client id: f_00001-1-6 loss: 0.727043  [  224/  265]
train() client id: f_00001-1-7 loss: 0.547868  [  256/  265]
train() client id: f_00001-2-0 loss: 0.520108  [   32/  265]
train() client id: f_00001-2-1 loss: 0.631917  [   64/  265]
train() client id: f_00001-2-2 loss: 0.575740  [   96/  265]
train() client id: f_00001-2-3 loss: 0.662761  [  128/  265]
train() client id: f_00001-2-4 loss: 0.676650  [  160/  265]
train() client id: f_00001-2-5 loss: 0.651818  [  192/  265]
train() client id: f_00001-2-6 loss: 0.596054  [  224/  265]
train() client id: f_00001-2-7 loss: 0.507301  [  256/  265]
train() client id: f_00002-0-0 loss: 0.998701  [   32/  124]
train() client id: f_00002-0-1 loss: 0.944463  [   64/  124]
train() client id: f_00002-0-2 loss: 0.935713  [   96/  124]
train() client id: f_00002-1-0 loss: 0.924642  [   32/  124]
train() client id: f_00002-1-1 loss: 1.051844  [   64/  124]
train() client id: f_00002-1-2 loss: 0.774673  [   96/  124]
train() client id: f_00002-2-0 loss: 1.078958  [   32/  124]
train() client id: f_00002-2-1 loss: 1.253869  [   64/  124]
train() client id: f_00002-2-2 loss: 0.651245  [   96/  124]
train() client id: f_00003-0-0 loss: 0.705076  [   32/   43]
train() client id: f_00003-1-0 loss: 0.495274  [   32/   43]
train() client id: f_00003-2-0 loss: 0.778160  [   32/   43]
train() client id: f_00004-0-0 loss: 0.727515  [   32/  306]
train() client id: f_00004-0-1 loss: 0.720147  [   64/  306]
train() client id: f_00004-0-2 loss: 0.646422  [   96/  306]
train() client id: f_00004-0-3 loss: 0.764449  [  128/  306]
train() client id: f_00004-0-4 loss: 0.631790  [  160/  306]
train() client id: f_00004-0-5 loss: 0.700517  [  192/  306]
train() client id: f_00004-0-6 loss: 0.876060  [  224/  306]
train() client id: f_00004-0-7 loss: 0.702277  [  256/  306]
train() client id: f_00004-0-8 loss: 0.792108  [  288/  306]
train() client id: f_00004-1-0 loss: 0.712025  [   32/  306]
train() client id: f_00004-1-1 loss: 0.781551  [   64/  306]
train() client id: f_00004-1-2 loss: 0.770527  [   96/  306]
train() client id: f_00004-1-3 loss: 0.733831  [  128/  306]
train() client id: f_00004-1-4 loss: 0.801040  [  160/  306]
train() client id: f_00004-1-5 loss: 0.749692  [  192/  306]
train() client id: f_00004-1-6 loss: 0.698977  [  224/  306]
train() client id: f_00004-1-7 loss: 0.776785  [  256/  306]
train() client id: f_00004-1-8 loss: 0.734949  [  288/  306]
train() client id: f_00004-2-0 loss: 0.749561  [   32/  306]
train() client id: f_00004-2-1 loss: 0.739255  [   64/  306]
train() client id: f_00004-2-2 loss: 0.833932  [   96/  306]
train() client id: f_00004-2-3 loss: 0.730102  [  128/  306]
train() client id: f_00004-2-4 loss: 0.757632  [  160/  306]
train() client id: f_00004-2-5 loss: 0.647514  [  192/  306]
train() client id: f_00004-2-6 loss: 0.714401  [  224/  306]
train() client id: f_00004-2-7 loss: 0.816616  [  256/  306]
train() client id: f_00004-2-8 loss: 0.721070  [  288/  306]
train() client id: f_00005-0-0 loss: 0.625648  [   32/  146]
train() client id: f_00005-0-1 loss: 0.702833  [   64/  146]
train() client id: f_00005-0-2 loss: 0.446860  [   96/  146]
train() client id: f_00005-0-3 loss: 0.362693  [  128/  146]
train() client id: f_00005-1-0 loss: 0.588313  [   32/  146]
train() client id: f_00005-1-1 loss: 0.494876  [   64/  146]
train() client id: f_00005-1-2 loss: 0.590887  [   96/  146]
train() client id: f_00005-1-3 loss: 0.578239  [  128/  146]
train() client id: f_00005-2-0 loss: 0.393646  [   32/  146]
train() client id: f_00005-2-1 loss: 0.693114  [   64/  146]
train() client id: f_00005-2-2 loss: 0.508248  [   96/  146]
train() client id: f_00005-2-3 loss: 0.743770  [  128/  146]
train() client id: f_00006-0-0 loss: 0.555515  [   32/   54]
train() client id: f_00006-1-0 loss: 0.433229  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508825  [   32/   54]
train() client id: f_00007-0-0 loss: 0.834099  [   32/  179]
train() client id: f_00007-0-1 loss: 0.569525  [   64/  179]
train() client id: f_00007-0-2 loss: 0.677518  [   96/  179]
train() client id: f_00007-0-3 loss: 0.493801  [  128/  179]
train() client id: f_00007-0-4 loss: 0.578327  [  160/  179]
train() client id: f_00007-1-0 loss: 0.597609  [   32/  179]
train() client id: f_00007-1-1 loss: 0.515716  [   64/  179]
train() client id: f_00007-1-2 loss: 0.526567  [   96/  179]
train() client id: f_00007-1-3 loss: 0.705808  [  128/  179]
train() client id: f_00007-1-4 loss: 0.690896  [  160/  179]
train() client id: f_00007-2-0 loss: 0.437896  [   32/  179]
train() client id: f_00007-2-1 loss: 0.587734  [   64/  179]
train() client id: f_00007-2-2 loss: 0.705794  [   96/  179]
train() client id: f_00007-2-3 loss: 0.584449  [  128/  179]
train() client id: f_00007-2-4 loss: 0.714189  [  160/  179]
train() client id: f_00008-0-0 loss: 0.759660  [   32/  130]
train() client id: f_00008-0-1 loss: 0.663997  [   64/  130]
train() client id: f_00008-0-2 loss: 0.860858  [   96/  130]
train() client id: f_00008-0-3 loss: 0.760217  [  128/  130]
train() client id: f_00008-1-0 loss: 0.787838  [   32/  130]
train() client id: f_00008-1-1 loss: 0.688873  [   64/  130]
train() client id: f_00008-1-2 loss: 0.788216  [   96/  130]
train() client id: f_00008-1-3 loss: 0.764283  [  128/  130]
train() client id: f_00008-2-0 loss: 0.709178  [   32/  130]
train() client id: f_00008-2-1 loss: 0.889937  [   64/  130]
train() client id: f_00008-2-2 loss: 0.654821  [   96/  130]
train() client id: f_00008-2-3 loss: 0.759194  [  128/  130]
train() client id: f_00009-0-0 loss: 0.909913  [   32/  118]
train() client id: f_00009-0-1 loss: 0.840589  [   64/  118]
train() client id: f_00009-0-2 loss: 0.876411  [   96/  118]
train() client id: f_00009-1-0 loss: 0.744617  [   32/  118]
train() client id: f_00009-1-1 loss: 0.963931  [   64/  118]
train() client id: f_00009-1-2 loss: 0.846163  [   96/  118]
train() client id: f_00009-2-0 loss: 0.866771  [   32/  118]
train() client id: f_00009-2-1 loss: 0.754454  [   64/  118]
train() client id: f_00009-2-2 loss: 0.834848  [   96/  118]
At round 77 accuracy: 0.6472148541114059
At round 77 training accuracy: 0.5855130784708249
At round 77 training loss: 0.8268093280014497
update_location
xs = [  -3.9056584     4.20031788  405.00902392   18.81129433    0.97929623
    3.95640986 -367.44319194 -346.32485185  389.66397685 -332.06087855]
ys = [ 397.5879595   380.55583871    1.32061395 -367.45517586  359.35018685
  342.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [409.98956048 393.49763537 417.17388877 381.28358352 373.00605331
 357.12350123 380.8167405  360.47410346 402.67441541 346.81470401]
dists_bs = [280.74663567 272.7012225  605.45437232 576.0385713  254.86016576
 245.33036058 261.88741317 244.31549137 586.1995604  232.18228357]
uav_gains = [6.66953995e-13 7.65732155e-13 6.30692965e-13 8.57197052e-13
 9.31026172e-13 1.10932372e-12 8.61079374e-13 1.06691429e-12
 7.07809580e-13 1.26020837e-12]
bs_gains = [1.54174772e-11 1.67251587e-11 1.79252867e-12 2.06077086e-12
 2.02138293e-11 2.24900517e-11 1.87315249e-11 2.27526125e-11
 1.96230594e-12 2.62405338e-11]
Round 78
-------------------------------
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.61445089 1.1956097  0.60744834 0.2367497  1.37782529 0.66388603
 0.28568902 0.83797174 0.60879864 0.53854598]
obj_prev = 6.96697533321086
eta_min = nan	eta_max = nan
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 1.572517230482826	eta = 0.909090909090909
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 4.952035710772682	eta = 0.2886815043580717
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.9382966582345253	eta = 0.48652715668257346
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.6172340285407683	eta = 0.5462106571408896
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.596934549245399	eta = 0.5504802264023727
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.5968383965379944	eta = 0.5505006089430082
eta = 0.5505006089430082
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.049257   0.10359608 0.04847515 0.01680993 0.11962415 0.05707558
 0.02111014 0.06997622 0.05082072 0.04612957]
ene_total = [0.27603738 0.36760475 0.27771398 0.15379303 0.4183885  0.21392216
 0.16753295 0.31519772 0.22938986 0.17725807]
ti_comp = [7.65842292 7.92067487 7.64545672 7.70927839 7.9250345  7.92732068
 7.71009494 7.74538131 7.81728173 7.93043026]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.27351857e-07 1.10760673e-06 1.21795299e-07 4.99516971e-09
 1.70346924e-06 1.84917516e-07 9.89086254e-09 3.56981594e-07
 1.34242758e-07 9.75494579e-08]
ene_total = [0.11560694 0.03019425 0.11983004 0.09904284 0.02877625 0.02802669
 0.0987769  0.0872852  0.06386639 0.02701362]
optimize_network iter = 0 obj = 0.6984191265921221
eta = 0.5505006089430082
freqs = [3215871.18363491 6539599.20133967 3170192.94839821 1090240.14955988
 7547232.16704463 3599928.48687781 1368993.7919406  4517286.87955715
 3250536.39445521 2908390.39625094]
eta_min = 0.5505006089430086	eta_max = 0.917509374892285
af = 9.439588067529733e-06	bf = 0.3139333373586834	zeta = 1.0383546874282708e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [2.03340174e-08 1.76849361e-07 1.94468128e-08 7.97568798e-10
 2.71989541e-07 2.95254116e-08 1.57925432e-09 5.69985401e-08
 2.14342739e-08 1.55755277e-08]
ene_total = [0.57161603 0.14928029 0.59249717 0.48971657 0.14226095 0.1385753
 0.48840159 0.43157626 0.31578514 0.13356732]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 1 obj = 3.4532738111991463
eta = 0.909090909090909
freqs = [3106837.34109527 5412726.23484139 3089168.32072196 1019314.72951421
 6232384.57566306 2969191.46464435 1279276.24484534 4129943.16102444
 2848016.20722417 2394904.72079761]
eta_min = 0.9009430532547689	eta_max = 0.9090909090909057
af = 6.772388892494084e-06	bf = 0.3139333373586834	zeta = 7.449627781743493e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 2 obj = 3.4532738111990233
eta = 0.9090909090909057
freqs = [3106837.34109525 5412726.23484139 3089168.32072194 1019314.7295142
 6232384.57566307 2969191.46464435 1279276.24484533 4129943.16102442
 2848016.20722416 2394904.72079761]
Done!
At round 78 energy consumption: 3.4532738111991463
At round 78 eta: 0.9090909090909057
At round 78 local rounds: 3.120939520577876
At round 78 global rounds: 16.104294719123292
At round 78 a_n: 1.1214809456783108
gradient difference: 1.0017642974853516
train() client id: f_00000-0-0 loss: 1.086850  [   32/  126]
train() client id: f_00000-0-1 loss: 0.728481  [   64/  126]
train() client id: f_00000-0-2 loss: 0.851157  [   96/  126]
train() client id: f_00000-1-0 loss: 0.919918  [   32/  126]
train() client id: f_00000-1-1 loss: 0.763240  [   64/  126]
train() client id: f_00000-1-2 loss: 0.781515  [   96/  126]
train() client id: f_00000-2-0 loss: 0.930803  [   32/  126]
train() client id: f_00000-2-1 loss: 0.921452  [   64/  126]
train() client id: f_00000-2-2 loss: 0.867064  [   96/  126]
train() client id: f_00001-0-0 loss: 0.586143  [   32/  265]
train() client id: f_00001-0-1 loss: 0.639145  [   64/  265]
train() client id: f_00001-0-2 loss: 0.851432  [   96/  265]
train() client id: f_00001-0-3 loss: 0.622782  [  128/  265]
train() client id: f_00001-0-4 loss: 0.566258  [  160/  265]
train() client id: f_00001-0-5 loss: 0.507111  [  192/  265]
train() client id: f_00001-0-6 loss: 0.556482  [  224/  265]
train() client id: f_00001-0-7 loss: 0.524659  [  256/  265]
train() client id: f_00001-1-0 loss: 0.560074  [   32/  265]
train() client id: f_00001-1-1 loss: 0.682274  [   64/  265]
train() client id: f_00001-1-2 loss: 0.553593  [   96/  265]
train() client id: f_00001-1-3 loss: 0.571906  [  128/  265]
train() client id: f_00001-1-4 loss: 0.596941  [  160/  265]
train() client id: f_00001-1-5 loss: 0.602067  [  192/  265]
train() client id: f_00001-1-6 loss: 0.567319  [  224/  265]
train() client id: f_00001-1-7 loss: 0.740353  [  256/  265]
train() client id: f_00001-2-0 loss: 0.726066  [   32/  265]
train() client id: f_00001-2-1 loss: 0.591698  [   64/  265]
train() client id: f_00001-2-2 loss: 0.623028  [   96/  265]
train() client id: f_00001-2-3 loss: 0.555343  [  128/  265]
train() client id: f_00001-2-4 loss: 0.580460  [  160/  265]
train() client id: f_00001-2-5 loss: 0.637259  [  192/  265]
train() client id: f_00001-2-6 loss: 0.572008  [  224/  265]
train() client id: f_00001-2-7 loss: 0.649837  [  256/  265]
train() client id: f_00002-0-0 loss: 1.371522  [   32/  124]
train() client id: f_00002-0-1 loss: 1.279338  [   64/  124]
train() client id: f_00002-0-2 loss: 1.347057  [   96/  124]
train() client id: f_00002-1-0 loss: 1.148440  [   32/  124]
train() client id: f_00002-1-1 loss: 1.372643  [   64/  124]
train() client id: f_00002-1-2 loss: 1.385287  [   96/  124]
train() client id: f_00002-2-0 loss: 1.260224  [   32/  124]
train() client id: f_00002-2-1 loss: 1.235160  [   64/  124]
train() client id: f_00002-2-2 loss: 1.357572  [   96/  124]
train() client id: f_00003-0-0 loss: 0.492735  [   32/   43]
train() client id: f_00003-1-0 loss: 0.691793  [   32/   43]
train() client id: f_00003-2-0 loss: 0.421226  [   32/   43]
train() client id: f_00004-0-0 loss: 0.872069  [   32/  306]
train() client id: f_00004-0-1 loss: 0.771933  [   64/  306]
train() client id: f_00004-0-2 loss: 0.861856  [   96/  306]
train() client id: f_00004-0-3 loss: 0.898628  [  128/  306]
train() client id: f_00004-0-4 loss: 0.683506  [  160/  306]
train() client id: f_00004-0-5 loss: 0.912697  [  192/  306]
train() client id: f_00004-0-6 loss: 0.787928  [  224/  306]
train() client id: f_00004-0-7 loss: 0.919177  [  256/  306]
train() client id: f_00004-0-8 loss: 0.910870  [  288/  306]
train() client id: f_00004-1-0 loss: 0.766158  [   32/  306]
train() client id: f_00004-1-1 loss: 0.851022  [   64/  306]
train() client id: f_00004-1-2 loss: 0.852008  [   96/  306]
train() client id: f_00004-1-3 loss: 0.755241  [  128/  306]
train() client id: f_00004-1-4 loss: 0.843794  [  160/  306]
train() client id: f_00004-1-5 loss: 0.804745  [  192/  306]
train() client id: f_00004-1-6 loss: 0.923225  [  224/  306]
train() client id: f_00004-1-7 loss: 1.056851  [  256/  306]
train() client id: f_00004-1-8 loss: 0.866045  [  288/  306]
train() client id: f_00004-2-0 loss: 0.725360  [   32/  306]
train() client id: f_00004-2-1 loss: 0.770466  [   64/  306]
train() client id: f_00004-2-2 loss: 0.944778  [   96/  306]
train() client id: f_00004-2-3 loss: 0.847098  [  128/  306]
train() client id: f_00004-2-4 loss: 1.023007  [  160/  306]
train() client id: f_00004-2-5 loss: 0.875498  [  192/  306]
train() client id: f_00004-2-6 loss: 0.867954  [  224/  306]
train() client id: f_00004-2-7 loss: 0.829016  [  256/  306]
train() client id: f_00004-2-8 loss: 0.834562  [  288/  306]
train() client id: f_00005-0-0 loss: 0.583695  [   32/  146]
train() client id: f_00005-0-1 loss: 0.522054  [   64/  146]
train() client id: f_00005-0-2 loss: 0.381624  [   96/  146]
train() client id: f_00005-0-3 loss: 0.408006  [  128/  146]
train() client id: f_00005-1-0 loss: 0.466087  [   32/  146]
train() client id: f_00005-1-1 loss: 0.367808  [   64/  146]
train() client id: f_00005-1-2 loss: 0.445493  [   96/  146]
train() client id: f_00005-1-3 loss: 0.390036  [  128/  146]
train() client id: f_00005-2-0 loss: 0.446711  [   32/  146]
train() client id: f_00005-2-1 loss: 0.436372  [   64/  146]
train() client id: f_00005-2-2 loss: 0.322010  [   96/  146]
train() client id: f_00005-2-3 loss: 0.483474  [  128/  146]
train() client id: f_00006-0-0 loss: 0.425848  [   32/   54]
train() client id: f_00006-1-0 loss: 0.474366  [   32/   54]
train() client id: f_00006-2-0 loss: 0.529052  [   32/   54]
train() client id: f_00007-0-0 loss: 0.388175  [   32/  179]
train() client id: f_00007-0-1 loss: 0.618223  [   64/  179]
train() client id: f_00007-0-2 loss: 0.552325  [   96/  179]
train() client id: f_00007-0-3 loss: 0.592428  [  128/  179]
train() client id: f_00007-0-4 loss: 0.710127  [  160/  179]
train() client id: f_00007-1-0 loss: 0.775833  [   32/  179]
train() client id: f_00007-1-1 loss: 0.655951  [   64/  179]
train() client id: f_00007-1-2 loss: 0.455712  [   96/  179]
train() client id: f_00007-1-3 loss: 0.387962  [  128/  179]
train() client id: f_00007-1-4 loss: 0.517005  [  160/  179]
train() client id: f_00007-2-0 loss: 0.496671  [   32/  179]
train() client id: f_00007-2-1 loss: 0.723113  [   64/  179]
train() client id: f_00007-2-2 loss: 0.396948  [   96/  179]
train() client id: f_00007-2-3 loss: 0.481540  [  128/  179]
train() client id: f_00007-2-4 loss: 0.448877  [  160/  179]
train() client id: f_00008-0-0 loss: 0.606000  [   32/  130]
train() client id: f_00008-0-1 loss: 0.732841  [   64/  130]
train() client id: f_00008-0-2 loss: 0.750583  [   96/  130]
train() client id: f_00008-0-3 loss: 0.855452  [  128/  130]
train() client id: f_00008-1-0 loss: 0.673501  [   32/  130]
train() client id: f_00008-1-1 loss: 0.799105  [   64/  130]
train() client id: f_00008-1-2 loss: 0.707009  [   96/  130]
train() client id: f_00008-1-3 loss: 0.771578  [  128/  130]
train() client id: f_00008-2-0 loss: 0.745467  [   32/  130]
train() client id: f_00008-2-1 loss: 0.763804  [   64/  130]
train() client id: f_00008-2-2 loss: 0.709042  [   96/  130]
train() client id: f_00008-2-3 loss: 0.734843  [  128/  130]
train() client id: f_00009-0-0 loss: 0.617779  [   32/  118]
train() client id: f_00009-0-1 loss: 0.743503  [   64/  118]
train() client id: f_00009-0-2 loss: 0.753727  [   96/  118]
train() client id: f_00009-1-0 loss: 0.825102  [   32/  118]
train() client id: f_00009-1-1 loss: 0.691049  [   64/  118]
train() client id: f_00009-1-2 loss: 0.568765  [   96/  118]
train() client id: f_00009-2-0 loss: 0.597340  [   32/  118]
train() client id: f_00009-2-1 loss: 0.790174  [   64/  118]
train() client id: f_00009-2-2 loss: 0.783540  [   96/  118]
At round 78 accuracy: 0.6472148541114059
At round 78 training accuracy: 0.5861837692823608
At round 78 training loss: 0.8344625691139749
update_location
xs = [  -3.9056584     4.20031788  410.00902392   18.81129433    0.97929623
    3.95640986 -372.44319194 -351.32485185  394.66397685 -337.06087855]
ys = [ 402.5879595   385.55583871    1.32061395 -372.45517586  364.35018685
  347.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [414.84011293 398.33521992 422.02979008 386.10454908 377.82537987
 361.92587174 385.64338677 365.28047824 407.51481519 351.60495973]
dists_bs = [284.72715542 276.48962935 610.24592354 580.74950485 258.49991481
 248.77726449 265.58322443 247.84734786 591.0173977  235.59333944]
uav_gains = [6.42087105e-13 7.34190954e-13 6.08078996e-13 8.18868515e-13
 8.86722082e-13 1.04941536e-12 8.22401288e-13 1.01086288e-12
 6.80300468e-13 1.18594342e-12]
bs_gains = [1.48215354e-11 1.60913809e-11 1.75339758e-12 2.01430537e-12
 1.94269648e-11 2.16283883e-11 1.80107706e-11 2.18563739e-11
 1.91784428e-12 2.51905507e-11]
Round 79
-------------------------------
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.47167495 0.91597325 0.46632083 0.18231339 1.05554823 0.50864652
 0.2198029  0.64284551 0.46659513 0.41263047]
obj_prev = 5.342351171013073
eta_min = 2.6352711032412936e-201	eta_max = 0.9959613407486563
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 1.2045873201186579	eta = 0.909090909090909
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 3.853002462740199	eta = 0.28421455540603213
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.268192123899988	eta = 0.48279833546161194
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0171814832442236	eta = 0.5428759836545998
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0013314548574064	eta = 0.5471754212767701
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0012564070127876	eta = 0.5471959405544855
eta = 0.5471959405544855
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.04975385 0.10464104 0.04896411 0.01697949 0.12083078 0.05765129
 0.02132308 0.07068206 0.05133334 0.04659488]
ene_total = [0.21330398 0.28236012 0.21458127 0.11946231 0.32136361 0.16430363
 0.13001973 0.24348682 0.17623076 0.13614417]
ti_comp = [10.15400802 10.42406192 10.14095348 10.20515528 10.42848067 10.43082397
 10.20596501 10.24142523 10.31948805 10.43395522]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [7.46595073e-08 6.59040563e-07 7.13437549e-08 2.93774855e-09
 1.01384295e-06 1.10070217e-07 5.81730210e-09 2.10419888e-07
 7.93893490e-08 5.80759835e-08]
ene_total = [0.09007648 0.02319242 0.09330975 0.07740842 0.02209888 0.02151627
 0.07720788 0.06842579 0.04909128 0.02074061]
optimize_network iter = 0 obj = 0.5430677938747402
eta = 0.5471959405544855
freqs = [2449961.18756652 5019206.37355413 2414176.71155869  831907.42340227
 5793307.05286581 2763505.9294842  1044638.0630291  3450792.08881117
 2487203.73595765 2232848.20767479]
eta_min = 0.5471959405544858	eta_max = 0.9355133020148048
af = 4.251199729612188e-06	bf = 0.24590073836681936	zeta = 4.6763197025734075e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [1.18016929e-08 1.04176878e-07 1.12775602e-08 4.64380326e-10
 1.60261749e-07 1.73991893e-08 9.19561561e-10 3.32618174e-08
 1.25493558e-08 9.18027656e-09]
ene_total = [0.44865618 0.11551099 0.46476064 0.3855593  0.11006058 0.10716806
 0.3845604  0.34081604 0.24451523 0.10330516]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 1 obj = 2.7049111966856447
eta = 0.909090909090909
freqs = [2249723.37868762 4098369.93259797 2230673.32764145  745935.59638761
 4722117.45608519 2250428.94068191  936334.90519755 3043809.99590222
 2120401.76543593 1816031.36876833]
eta_min = 0.9090909090910358	eta_max = 0.909090909090905
af = 2.9326740829202082e-06	bf = 0.24590073836681936	zeta = 3.2259414912122293e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 2 obj = 2.7049111966855257
eta = 0.909090909090905
freqs = [2249723.37868761 4098369.93259797 2230673.32764143  745935.59638761
 4722117.4560852  2250428.94068191  936334.90519755 3043809.99590221
 2120401.76543593 1816031.36876833]
Done!
At round 79 energy consumption: 2.7049111966856447
At round 79 eta: 0.909090909090905
At round 79 local rounds: 3.120939520577902
At round 79 global rounds: 12.336290402460858
At round 79 a_n: 0.7789350987089918
gradient difference: 0.8903557658195496
train() client id: f_00000-0-0 loss: 0.872583  [   32/  126]
train() client id: f_00000-0-1 loss: 1.089447  [   64/  126]
train() client id: f_00000-0-2 loss: 1.092176  [   96/  126]
train() client id: f_00000-1-0 loss: 0.785132  [   32/  126]
train() client id: f_00000-1-1 loss: 0.972035  [   64/  126]
train() client id: f_00000-1-2 loss: 0.989524  [   96/  126]
train() client id: f_00000-2-0 loss: 0.955874  [   32/  126]
train() client id: f_00000-2-1 loss: 1.078292  [   64/  126]
train() client id: f_00000-2-2 loss: 0.785124  [   96/  126]
train() client id: f_00001-0-0 loss: 0.481894  [   32/  265]
train() client id: f_00001-0-1 loss: 0.579805  [   64/  265]
train() client id: f_00001-0-2 loss: 0.545876  [   96/  265]
train() client id: f_00001-0-3 loss: 0.478539  [  128/  265]
train() client id: f_00001-0-4 loss: 0.570602  [  160/  265]
train() client id: f_00001-0-5 loss: 0.393624  [  192/  265]
train() client id: f_00001-0-6 loss: 0.419628  [  224/  265]
train() client id: f_00001-0-7 loss: 0.390386  [  256/  265]
train() client id: f_00001-1-0 loss: 0.496444  [   32/  265]
train() client id: f_00001-1-1 loss: 0.434789  [   64/  265]
train() client id: f_00001-1-2 loss: 0.397675  [   96/  265]
train() client id: f_00001-1-3 loss: 0.494036  [  128/  265]
train() client id: f_00001-1-4 loss: 0.383129  [  160/  265]
train() client id: f_00001-1-5 loss: 0.512180  [  192/  265]
train() client id: f_00001-1-6 loss: 0.551111  [  224/  265]
train() client id: f_00001-1-7 loss: 0.547793  [  256/  265]
train() client id: f_00001-2-0 loss: 0.417731  [   32/  265]
train() client id: f_00001-2-1 loss: 0.456303  [   64/  265]
train() client id: f_00001-2-2 loss: 0.395751  [   96/  265]
train() client id: f_00001-2-3 loss: 0.489151  [  128/  265]
train() client id: f_00001-2-4 loss: 0.563698  [  160/  265]
train() client id: f_00001-2-5 loss: 0.690999  [  192/  265]
train() client id: f_00001-2-6 loss: 0.412013  [  224/  265]
train() client id: f_00001-2-7 loss: 0.414266  [  256/  265]
train() client id: f_00002-0-0 loss: 0.645072  [   32/  124]
train() client id: f_00002-0-1 loss: 0.736478  [   64/  124]
train() client id: f_00002-0-2 loss: 0.796042  [   96/  124]
train() client id: f_00002-1-0 loss: 0.872799  [   32/  124]
train() client id: f_00002-1-1 loss: 0.531729  [   64/  124]
train() client id: f_00002-1-2 loss: 0.942044  [   96/  124]
train() client id: f_00002-2-0 loss: 0.920750  [   32/  124]
train() client id: f_00002-2-1 loss: 0.655667  [   64/  124]
train() client id: f_00002-2-2 loss: 0.955195  [   96/  124]
train() client id: f_00003-0-0 loss: 0.693206  [   32/   43]
train() client id: f_00003-1-0 loss: 0.810171  [   32/   43]
train() client id: f_00003-2-0 loss: 0.642944  [   32/   43]
train() client id: f_00004-0-0 loss: 0.882071  [   32/  306]
train() client id: f_00004-0-1 loss: 0.709183  [   64/  306]
train() client id: f_00004-0-2 loss: 0.856200  [   96/  306]
train() client id: f_00004-0-3 loss: 0.730337  [  128/  306]
train() client id: f_00004-0-4 loss: 0.883710  [  160/  306]
train() client id: f_00004-0-5 loss: 0.799504  [  192/  306]
train() client id: f_00004-0-6 loss: 0.884543  [  224/  306]
train() client id: f_00004-0-7 loss: 0.948664  [  256/  306]
train() client id: f_00004-0-8 loss: 0.961795  [  288/  306]
train() client id: f_00004-1-0 loss: 0.807758  [   32/  306]
train() client id: f_00004-1-1 loss: 0.884043  [   64/  306]
train() client id: f_00004-1-2 loss: 1.012413  [   96/  306]
train() client id: f_00004-1-3 loss: 0.853077  [  128/  306]
train() client id: f_00004-1-4 loss: 0.800044  [  160/  306]
train() client id: f_00004-1-5 loss: 0.851448  [  192/  306]
train() client id: f_00004-1-6 loss: 0.843455  [  224/  306]
train() client id: f_00004-1-7 loss: 0.777805  [  256/  306]
train() client id: f_00004-1-8 loss: 0.973036  [  288/  306]
train() client id: f_00004-2-0 loss: 0.833652  [   32/  306]
train() client id: f_00004-2-1 loss: 0.789503  [   64/  306]
train() client id: f_00004-2-2 loss: 0.921700  [   96/  306]
train() client id: f_00004-2-3 loss: 0.966938  [  128/  306]
train() client id: f_00004-2-4 loss: 0.850055  [  160/  306]
train() client id: f_00004-2-5 loss: 0.876783  [  192/  306]
train() client id: f_00004-2-6 loss: 0.908630  [  224/  306]
train() client id: f_00004-2-7 loss: 0.796739  [  256/  306]
train() client id: f_00004-2-8 loss: 0.741494  [  288/  306]
train() client id: f_00005-0-0 loss: 0.442167  [   32/  146]
train() client id: f_00005-0-1 loss: 0.659252  [   64/  146]
train() client id: f_00005-0-2 loss: 0.773217  [   96/  146]
train() client id: f_00005-0-3 loss: 0.463925  [  128/  146]
train() client id: f_00005-1-0 loss: 0.593546  [   32/  146]
train() client id: f_00005-1-1 loss: 0.483951  [   64/  146]
train() client id: f_00005-1-2 loss: 0.808698  [   96/  146]
train() client id: f_00005-1-3 loss: 0.481358  [  128/  146]
train() client id: f_00005-2-0 loss: 0.679558  [   32/  146]
train() client id: f_00005-2-1 loss: 0.524654  [   64/  146]
train() client id: f_00005-2-2 loss: 0.832177  [   96/  146]
train() client id: f_00005-2-3 loss: 0.490006  [  128/  146]
train() client id: f_00006-0-0 loss: 0.495972  [   32/   54]
train() client id: f_00006-1-0 loss: 0.520384  [   32/   54]
train() client id: f_00006-2-0 loss: 0.399448  [   32/   54]
train() client id: f_00007-0-0 loss: 0.757244  [   32/  179]
train() client id: f_00007-0-1 loss: 0.639772  [   64/  179]
train() client id: f_00007-0-2 loss: 0.689516  [   96/  179]
train() client id: f_00007-0-3 loss: 0.702272  [  128/  179]
train() client id: f_00007-0-4 loss: 0.820661  [  160/  179]
train() client id: f_00007-1-0 loss: 0.763798  [   32/  179]
train() client id: f_00007-1-1 loss: 0.594864  [   64/  179]
train() client id: f_00007-1-2 loss: 0.800974  [   96/  179]
train() client id: f_00007-1-3 loss: 0.616126  [  128/  179]
train() client id: f_00007-1-4 loss: 0.912629  [  160/  179]
train() client id: f_00007-2-0 loss: 0.781431  [   32/  179]
train() client id: f_00007-2-1 loss: 0.932358  [   64/  179]
train() client id: f_00007-2-2 loss: 0.611092  [   96/  179]
train() client id: f_00007-2-3 loss: 0.657924  [  128/  179]
train() client id: f_00007-2-4 loss: 0.697441  [  160/  179]
train() client id: f_00008-0-0 loss: 0.725888  [   32/  130]
train() client id: f_00008-0-1 loss: 0.707453  [   64/  130]
train() client id: f_00008-0-2 loss: 0.775691  [   96/  130]
train() client id: f_00008-0-3 loss: 0.752713  [  128/  130]
train() client id: f_00008-1-0 loss: 0.761981  [   32/  130]
train() client id: f_00008-1-1 loss: 0.729147  [   64/  130]
train() client id: f_00008-1-2 loss: 0.682941  [   96/  130]
train() client id: f_00008-1-3 loss: 0.784752  [  128/  130]
train() client id: f_00008-2-0 loss: 0.744010  [   32/  130]
train() client id: f_00008-2-1 loss: 0.625546  [   64/  130]
train() client id: f_00008-2-2 loss: 0.777330  [   96/  130]
train() client id: f_00008-2-3 loss: 0.850275  [  128/  130]
train() client id: f_00009-0-0 loss: 0.790581  [   32/  118]
train() client id: f_00009-0-1 loss: 0.616311  [   64/  118]
train() client id: f_00009-0-2 loss: 0.634882  [   96/  118]
train() client id: f_00009-1-0 loss: 0.556872  [   32/  118]
train() client id: f_00009-1-1 loss: 0.713878  [   64/  118]
train() client id: f_00009-1-2 loss: 0.602354  [   96/  118]
train() client id: f_00009-2-0 loss: 0.642090  [   32/  118]
train() client id: f_00009-2-1 loss: 0.690390  [   64/  118]
train() client id: f_00009-2-2 loss: 0.549230  [   96/  118]
At round 79 accuracy: 0.6472148541114059
At round 79 training accuracy: 0.5888665325285044
At round 79 training loss: 0.8243125274770751
update_location
xs = [  -3.9056584     4.20031788  415.00902392   18.81129433    0.97929623
    3.95640986 -377.44319194 -356.32485185  399.66397685 -342.06087855]
ys = [ 407.5879595   390.55583871    1.32061395 -377.45517586  369.35018685
  352.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [419.69417305 403.17676746 426.88901831 390.93001238 382.6493428
 366.73352471 390.47439568 370.09198357 412.35902362 356.40097712]
dists_bs = [288.73938531 280.31602439 615.04079331 585.46523308 262.18449197
 252.27617572 269.32114847 251.42902847 595.83823656 239.06030698]
uav_gains = [6.18777499e-13 7.04863808e-13 5.86815526e-13 7.83470858e-13
 8.46017284e-13 9.94934833e-13 7.86695341e-13 9.59778197e-13
 6.54600040e-13 1.11885798e-12]
bs_gains = [1.42520458e-11 1.54838835e-11 1.71539091e-12 1.96920521e-12
 1.86721568e-11 2.07989120e-11 1.73195580e-11 2.09957270e-11
 1.87471239e-12 2.41809390e-11]
Round 80
-------------------------------
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.32829867 0.63627291 0.3245873  0.12729558 0.73321141 0.35335129
 0.15333479 0.44714953 0.32424655 0.28666076]
obj_prev = 3.7144087846743608
eta_min = 2.74115185868049e-289	eta_max = 0.9972962516128097
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 0.8366574097544858	eta = 0.9090909090909091
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 2.7178337538132036	eta = 0.2798543671643672
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.5874476546409413	eta = 0.4791324255686323
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.4095987989406396	eta = 0.539584487304448
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.398385343651186	eta = 0.5439113393776205
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.39833229498431	eta = 0.5439319737944585
eta = 0.5439319737944585
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.05024753 0.10567934 0.04944995 0.01714797 0.12202973 0.05822333
 0.02153466 0.0713834  0.05184269 0.04705721]
ene_total = [0.14943821 0.1966487  0.15032166 0.08411788 0.22380943 0.11442236
 0.09147288 0.17052575 0.12276251 0.0948129 ]
ti_comp = [14.85036606 15.12826574 14.83721764 14.90182223 15.13274303 15.13514254
 14.90262547 14.93825677 15.02250443 15.13829514]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [3.59542268e-08 3.22308656e-07 3.43299174e-08 1.41918577e-09
 4.95954233e-07 5.38515457e-08 2.81040133e-09 1.01876001e-07
 3.85885102e-08 2.84187337e-08]
ene_total = [0.06361869 0.01615569 0.06586436 0.05483025 0.01539129 0.01498072
 0.05469306 0.04860763 0.03421856 0.01444223]
optimize_network iter = 0 obj = 0.38280246883118135
eta = 0.5439319737944585
freqs = [1691794.45393543 3492777.65574166 1666416.00768811  575364.79907189
 4031976.3853505  1923448.44978197  722512.19806255 2389281.50597988
 1725501.02801081 1554244.18120876]
eta_min = 0.5439319737944591	eta_max = 0.9542982433752891
af = 1.4269447035111872e-06	bf = 0.17458309747545425	zeta = 1.569639173862306e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [5.62757246e-09 5.04479022e-08 5.37333478e-09 2.22131623e-10
 7.76269894e-08 8.42886922e-09 4.39885336e-10 1.59456795e-08
 6.03989174e-09 4.44811354e-09]
ene_total = [0.3191587  0.08104669 0.33042464 0.27506954 0.07721065 0.07515409
 0.2743813  0.24385151 0.17166562 0.07245282]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 1 obj = 1.9204150651233343
eta = 0.9090909090909091
freqs = [1477274.39950027 2813265.88968522 1461042.34829177  494588.56272364
 3243585.97051685 1546333.09614372  620927.40517717 2031588.97688467
 1431594.99214584 1248440.38657298]
eta_min = 0.9090909090909186	eta_max = 0.9090909090909067
af = 9.470450164325744e-07	bf = 0.17458309747545425	zeta = 1.0417495180758319e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 2 obj = 1.9204150651232854
eta = 0.9090909090909067
freqs = [1477274.39950027 2813265.88968522 1461042.34829176  494588.56272364
 3243585.97051684 1546333.09614371  620927.40517717 2031588.97688466
 1431594.99214584 1248440.38657297]
Done!
At round 80 energy consumption: 1.9204150651233343
At round 80 eta: 0.9090909090909067
At round 80 local rounds: 3.120939520577836
At round 80 global rounds: 8.568286085798688
At round 80 a_n: 0.43638925173967635
gradient difference: 0.9624425768852234
train() client id: f_00000-0-0 loss: 1.119808  [   32/  126]
train() client id: f_00000-0-1 loss: 1.134627  [   64/  126]
train() client id: f_00000-0-2 loss: 1.208763  [   96/  126]
train() client id: f_00000-1-0 loss: 1.019268  [   32/  126]
train() client id: f_00000-1-1 loss: 1.195604  [   64/  126]
train() client id: f_00000-1-2 loss: 0.995845  [   96/  126]
train() client id: f_00000-2-0 loss: 0.999381  [   32/  126]
train() client id: f_00000-2-1 loss: 1.065631  [   64/  126]
train() client id: f_00000-2-2 loss: 1.050934  [   96/  126]
train() client id: f_00001-0-0 loss: 0.547774  [   32/  265]
train() client id: f_00001-0-1 loss: 0.638920  [   64/  265]
train() client id: f_00001-0-2 loss: 0.662561  [   96/  265]
train() client id: f_00001-0-3 loss: 0.541893  [  128/  265]
train() client id: f_00001-0-4 loss: 0.464027  [  160/  265]
train() client id: f_00001-0-5 loss: 0.666280  [  192/  265]
train() client id: f_00001-0-6 loss: 0.510122  [  224/  265]
train() client id: f_00001-0-7 loss: 0.543173  [  256/  265]
train() client id: f_00001-1-0 loss: 0.590774  [   32/  265]
train() client id: f_00001-1-1 loss: 0.685058  [   64/  265]
train() client id: f_00001-1-2 loss: 0.532680  [   96/  265]
train() client id: f_00001-1-3 loss: 0.550801  [  128/  265]
train() client id: f_00001-1-4 loss: 0.533290  [  160/  265]
train() client id: f_00001-1-5 loss: 0.561687  [  192/  265]
train() client id: f_00001-1-6 loss: 0.473648  [  224/  265]
train() client id: f_00001-1-7 loss: 0.663960  [  256/  265]
train() client id: f_00001-2-0 loss: 0.598723  [   32/  265]
train() client id: f_00001-2-1 loss: 0.540480  [   64/  265]
train() client id: f_00001-2-2 loss: 0.664041  [   96/  265]
train() client id: f_00001-2-3 loss: 0.656738  [  128/  265]
train() client id: f_00001-2-4 loss: 0.560191  [  160/  265]
train() client id: f_00001-2-5 loss: 0.498715  [  192/  265]
train() client id: f_00001-2-6 loss: 0.536252  [  224/  265]
train() client id: f_00001-2-7 loss: 0.545922  [  256/  265]
train() client id: f_00002-0-0 loss: 0.630038  [   32/  124]
train() client id: f_00002-0-1 loss: 0.671146  [   64/  124]
train() client id: f_00002-0-2 loss: 0.897954  [   96/  124]
train() client id: f_00002-1-0 loss: 0.786928  [   32/  124]
train() client id: f_00002-1-1 loss: 0.635060  [   64/  124]
train() client id: f_00002-1-2 loss: 0.708552  [   96/  124]
train() client id: f_00002-2-0 loss: 0.688759  [   32/  124]
train() client id: f_00002-2-1 loss: 0.682475  [   64/  124]
train() client id: f_00002-2-2 loss: 0.855337  [   96/  124]
train() client id: f_00003-0-0 loss: 0.498935  [   32/   43]
train() client id: f_00003-1-0 loss: 0.672330  [   32/   43]
train() client id: f_00003-2-0 loss: 0.663084  [   32/   43]
train() client id: f_00004-0-0 loss: 0.931620  [   32/  306]
train() client id: f_00004-0-1 loss: 1.050055  [   64/  306]
train() client id: f_00004-0-2 loss: 0.889892  [   96/  306]
train() client id: f_00004-0-3 loss: 1.116661  [  128/  306]
train() client id: f_00004-0-4 loss: 1.084203  [  160/  306]
train() client id: f_00004-0-5 loss: 0.907843  [  192/  306]
train() client id: f_00004-0-6 loss: 0.915849  [  224/  306]
train() client id: f_00004-0-7 loss: 0.788264  [  256/  306]
train() client id: f_00004-0-8 loss: 1.084819  [  288/  306]
train() client id: f_00004-1-0 loss: 1.091519  [   32/  306]
train() client id: f_00004-1-1 loss: 0.924784  [   64/  306]
train() client id: f_00004-1-2 loss: 0.984930  [   96/  306]
train() client id: f_00004-1-3 loss: 1.028847  [  128/  306]
train() client id: f_00004-1-4 loss: 0.921771  [  160/  306]
train() client id: f_00004-1-5 loss: 0.936716  [  192/  306]
train() client id: f_00004-1-6 loss: 0.953050  [  224/  306]
train() client id: f_00004-1-7 loss: 1.043749  [  256/  306]
train() client id: f_00004-1-8 loss: 0.865585  [  288/  306]
train() client id: f_00004-2-0 loss: 0.989308  [   32/  306]
train() client id: f_00004-2-1 loss: 0.947210  [   64/  306]
train() client id: f_00004-2-2 loss: 0.905029  [   96/  306]
train() client id: f_00004-2-3 loss: 1.081471  [  128/  306]
train() client id: f_00004-2-4 loss: 1.001247  [  160/  306]
train() client id: f_00004-2-5 loss: 0.740851  [  192/  306]
train() client id: f_00004-2-6 loss: 0.901316  [  224/  306]
train() client id: f_00004-2-7 loss: 1.148628  [  256/  306]
train() client id: f_00004-2-8 loss: 1.045264  [  288/  306]
train() client id: f_00005-0-0 loss: 0.531992  [   32/  146]
train() client id: f_00005-0-1 loss: 0.808269  [   64/  146]
train() client id: f_00005-0-2 loss: 0.729400  [   96/  146]
train() client id: f_00005-0-3 loss: 1.142437  [  128/  146]
train() client id: f_00005-1-0 loss: 0.964216  [   32/  146]
train() client id: f_00005-1-1 loss: 0.810422  [   64/  146]
train() client id: f_00005-1-2 loss: 0.611384  [   96/  146]
train() client id: f_00005-1-3 loss: 0.659708  [  128/  146]
train() client id: f_00005-2-0 loss: 0.821393  [   32/  146]
train() client id: f_00005-2-1 loss: 0.540521  [   64/  146]
train() client id: f_00005-2-2 loss: 0.748902  [   96/  146]
train() client id: f_00005-2-3 loss: 0.872058  [  128/  146]
train() client id: f_00006-0-0 loss: 0.568148  [   32/   54]
train() client id: f_00006-1-0 loss: 0.542513  [   32/   54]
train() client id: f_00006-2-0 loss: 0.552186  [   32/   54]
train() client id: f_00007-0-0 loss: 0.674116  [   32/  179]
train() client id: f_00007-0-1 loss: 0.974482  [   64/  179]
train() client id: f_00007-0-2 loss: 0.525425  [   96/  179]
train() client id: f_00007-0-3 loss: 0.765839  [  128/  179]
train() client id: f_00007-0-4 loss: 0.608693  [  160/  179]
train() client id: f_00007-1-0 loss: 0.807087  [   32/  179]
train() client id: f_00007-1-1 loss: 0.596370  [   64/  179]
train() client id: f_00007-1-2 loss: 0.795972  [   96/  179]
train() client id: f_00007-1-3 loss: 0.689270  [  128/  179]
train() client id: f_00007-1-4 loss: 0.703691  [  160/  179]
train() client id: f_00007-2-0 loss: 0.591661  [   32/  179]
train() client id: f_00007-2-1 loss: 0.810404  [   64/  179]
train() client id: f_00007-2-2 loss: 0.760372  [   96/  179]
train() client id: f_00007-2-3 loss: 0.669302  [  128/  179]
train() client id: f_00007-2-4 loss: 0.648827  [  160/  179]
train() client id: f_00008-0-0 loss: 0.601688  [   32/  130]
train() client id: f_00008-0-1 loss: 0.785241  [   64/  130]
train() client id: f_00008-0-2 loss: 0.659022  [   96/  130]
train() client id: f_00008-0-3 loss: 0.648909  [  128/  130]
train() client id: f_00008-1-0 loss: 0.572134  [   32/  130]
train() client id: f_00008-1-1 loss: 0.625438  [   64/  130]
train() client id: f_00008-1-2 loss: 0.736939  [   96/  130]
train() client id: f_00008-1-3 loss: 0.775565  [  128/  130]
train() client id: f_00008-2-0 loss: 0.601526  [   32/  130]
train() client id: f_00008-2-1 loss: 0.652141  [   64/  130]
train() client id: f_00008-2-2 loss: 0.742430  [   96/  130]
train() client id: f_00008-2-3 loss: 0.719303  [  128/  130]
train() client id: f_00009-0-0 loss: 0.811683  [   32/  118]
train() client id: f_00009-0-1 loss: 0.916344  [   64/  118]
train() client id: f_00009-0-2 loss: 0.886606  [   96/  118]
train() client id: f_00009-1-0 loss: 0.886859  [   32/  118]
train() client id: f_00009-1-1 loss: 1.118003  [   64/  118]
train() client id: f_00009-1-2 loss: 0.707773  [   96/  118]
train() client id: f_00009-2-0 loss: 0.687637  [   32/  118]
train() client id: f_00009-2-1 loss: 1.025354  [   64/  118]
train() client id: f_00009-2-2 loss: 0.918156  [   96/  118]
At round 80 accuracy: 0.6472148541114059
At round 80 training accuracy: 0.5935613682092555
At round 80 training loss: 0.806499514175113
update_location
xs = [  -3.9056584     4.20031788  420.00902392   18.81129433    0.97929623
    3.95640986 -382.44319194 -361.32485185  404.66397685 -347.06087855]
ys = [ 412.5879595   395.55583871    1.32061395 -382.45517586  374.35018685
  357.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [424.55162052 408.02213691 431.75146113 395.7598089  387.47776893
 371.54625506 395.30960727 374.90842191 417.20690806 361.20252668]
dists_bs = [292.78202169 284.1788731  619.83890462 590.18564105 265.91203375
 255.82496036 273.09945612 255.05843424 600.66200473 242.58078894]
uav_gains = [5.96881287e-13 6.77526367e-13 5.66782867e-13 7.50688319e-13
 8.08508001e-13 9.45234363e-13 7.53640053e-13 9.13074203e-13
 6.30534284e-13 1.05807042e-12]
bs_gains = [1.37078628e-11 1.49017434e-11 1.67846913e-12 1.92542185e-12
 1.79484814e-11 2.00011019e-11 1.66569601e-11 2.01698633e-11
 1.83286144e-12 2.32111230e-11]
Round 81
-------------------------------
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.18431614 0.35650715 0.18224127 0.07169206 0.41081325 0.19799863
 0.08628049 0.25087914 0.1817507  0.16063512]
obj_prev = 2.0831139657376503
eta_min = 0.0	eta_max = inf
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.4687274993903175	eta = 0.909090909090909
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 1.5461570538938256	eta = 0.2755967820109389
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.8960917211906734	eta = 0.47552711230325256
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7944965020417399	eta = 0.5363345306638816
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7881017357421507	eta = 0.5406864230991465
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7880715228657817	eta = 0.54070715179138
eta = 0.54070715179138
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.05073821 0.10671132 0.04993285 0.01731542 0.12322138 0.0587919
 0.02174495 0.07208048 0.05234895 0.04751674]
ene_total = [0.08444145 0.1104692  0.08493497 0.04776518 0.12572503 0.06427522
 0.05189808 0.09632167 0.06897994 0.05326078]
ti_comp = [26.93025964 27.2160528  26.91701177 26.98204373 27.22058811 27.22304297
 26.98284085 27.01864488 27.10909704 27.22621665]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.12565281e-08 1.02532430e-07 1.07395299e-08 4.45686613e-10
 1.57813146e-07 1.71379604e-08 8.82632352e-10 3.20631268e-08
 1.22003941e-08 9.04576573e-09]
ene_total = [0.03623342 0.00907936 0.03749214 0.03131323 0.00864849 0.00841512
 0.03123749 0.02783567 0.01924149 0.00811357]
optimize_network iter = 0 obj = 0.21760996983692812
eta = 0.54070715179138
freqs = [ 942029.79379861 1960448.1887988   927533.22358696  320869.35975109
 2263385.64084244 1079818.64242409  402940.3153214  1333902.54699206
  965523.69021721  872628.40406785]
eta_min = 0.5407071517913803	eta_max = 0.9738786232505544
af = 2.5132315156122307e-07	bf = 0.0999465483120546	zeta = 2.764554667173454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.74483827e-09 1.58932227e-08 1.66470005e-09 6.90844506e-11
 2.44621090e-08 2.65650021e-09 1.36814006e-10 4.97000232e-09
 1.89114391e-09 1.40215510e-09]
ene_total = [0.1830592  0.0458705  0.18941855 0.15820137 0.04369346 0.04251496
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 1 obj = 1.0994121997748156
eta = 0.909090909090909
freqs = [ 782640.11306705 1557441.83511423  772253.41470056  264366.82466173
 1796869.89992426  856934.27544998  331943.42290144 1092625.69941991
  779732.80435494  692177.58267536]
eta_min = 0.9090909090909166	eta_max = 0.9090909090908771
af = 1.605687647826223e-07	bf = 0.0999465483120546	zeta = 1.7662564126088454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 2 obj = 1.09941219977443
eta = 0.9090909090908771
freqs = [ 782640.11306704 1557441.83511425  772253.41470055  264366.82466173
 1796869.89992429  856934.27544999  331943.42290144 1092625.69941991
  779732.80435495  692177.58267537]
Done!
At round 81 energy consumption: 1.0994121997748156
At round 81 eta: 0.9090909090908771
At round 81 local rounds: 3.120939520578907
At round 81 global rounds: 4.80028176913475
At round 81 a_n: 0.09384340477035735
gradient difference: 0.9276318550109863
train() client id: f_00000-0-0 loss: 0.926736  [   32/  126]
train() client id: f_00000-0-1 loss: 0.866591  [   64/  126]
train() client id: f_00000-0-2 loss: 0.826061  [   96/  126]
train() client id: f_00000-1-0 loss: 0.841867  [   32/  126]
train() client id: f_00000-1-1 loss: 0.946041  [   64/  126]
train() client id: f_00000-1-2 loss: 0.811531  [   96/  126]
train() client id: f_00000-2-0 loss: 0.836319  [   32/  126]
train() client id: f_00000-2-1 loss: 1.036642  [   64/  126]
train() client id: f_00000-2-2 loss: 0.915643  [   96/  126]
train() client id: f_00001-0-0 loss: 0.567178  [   32/  265]
train() client id: f_00001-0-1 loss: 0.515574  [   64/  265]
train() client id: f_00001-0-2 loss: 0.487356  [   96/  265]
train() client id: f_00001-0-3 loss: 0.632920  [  128/  265]
train() client id: f_00001-0-4 loss: 0.551245  [  160/  265]
train() client id: f_00001-0-5 loss: 0.572204  [  192/  265]
train() client id: f_00001-0-6 loss: 0.681856  [  224/  265]
train() client id: f_00001-0-7 loss: 0.624712  [  256/  265]
train() client id: f_00001-1-0 loss: 0.661778  [   32/  265]
train() client id: f_00001-1-1 loss: 0.464511  [   64/  265]
train() client id: f_00001-1-2 loss: 0.597773  [   96/  265]
train() client id: f_00001-1-3 loss: 0.627913  [  128/  265]
train() client id: f_00001-1-4 loss: 0.563040  [  160/  265]
train() client id: f_00001-1-5 loss: 0.664047  [  192/  265]
train() client id: f_00001-1-6 loss: 0.477314  [  224/  265]
train() client id: f_00001-1-7 loss: 0.520142  [  256/  265]
train() client id: f_00001-2-0 loss: 0.591505  [   32/  265]
train() client id: f_00001-2-1 loss: 0.527725  [   64/  265]
train() client id: f_00001-2-2 loss: 0.674422  [   96/  265]
train() client id: f_00001-2-3 loss: 0.597787  [  128/  265]
train() client id: f_00001-2-4 loss: 0.522581  [  160/  265]
train() client id: f_00001-2-5 loss: 0.457742  [  192/  265]
train() client id: f_00001-2-6 loss: 0.535230  [  224/  265]
train() client id: f_00001-2-7 loss: 0.587647  [  256/  265]
train() client id: f_00002-0-0 loss: 0.864687  [   32/  124]
train() client id: f_00002-0-1 loss: 0.807608  [   64/  124]
train() client id: f_00002-0-2 loss: 0.770068  [   96/  124]
train() client id: f_00002-1-0 loss: 0.516462  [   32/  124]
train() client id: f_00002-1-1 loss: 0.726505  [   64/  124]
train() client id: f_00002-1-2 loss: 1.099735  [   96/  124]
train() client id: f_00002-2-0 loss: 0.713672  [   32/  124]
train() client id: f_00002-2-1 loss: 0.666014  [   64/  124]
train() client id: f_00002-2-2 loss: 0.909165  [   96/  124]
train() client id: f_00003-0-0 loss: 0.678907  [   32/   43]
train() client id: f_00003-1-0 loss: 0.299299  [   32/   43]
train() client id: f_00003-2-0 loss: 0.542549  [   32/   43]
train() client id: f_00004-0-0 loss: 0.766167  [   32/  306]
train() client id: f_00004-0-1 loss: 0.711417  [   64/  306]
train() client id: f_00004-0-2 loss: 0.810960  [   96/  306]
train() client id: f_00004-0-3 loss: 0.622922  [  128/  306]
train() client id: f_00004-0-4 loss: 0.619097  [  160/  306]
train() client id: f_00004-0-5 loss: 0.634903  [  192/  306]
train() client id: f_00004-0-6 loss: 0.641857  [  224/  306]
train() client id: f_00004-0-7 loss: 0.714425  [  256/  306]
train() client id: f_00004-0-8 loss: 0.804530  [  288/  306]
train() client id: f_00004-1-0 loss: 0.730005  [   32/  306]
train() client id: f_00004-1-1 loss: 0.637058  [   64/  306]
train() client id: f_00004-1-2 loss: 0.664649  [   96/  306]
train() client id: f_00004-1-3 loss: 0.816091  [  128/  306]
train() client id: f_00004-1-4 loss: 0.654493  [  160/  306]
train() client id: f_00004-1-5 loss: 0.700643  [  192/  306]
train() client id: f_00004-1-6 loss: 0.650763  [  224/  306]
train() client id: f_00004-1-7 loss: 0.613498  [  256/  306]
train() client id: f_00004-1-8 loss: 0.875656  [  288/  306]
train() client id: f_00004-2-0 loss: 0.770981  [   32/  306]
train() client id: f_00004-2-1 loss: 0.740881  [   64/  306]
train() client id: f_00004-2-2 loss: 0.796317  [   96/  306]
train() client id: f_00004-2-3 loss: 0.630375  [  128/  306]
train() client id: f_00004-2-4 loss: 0.654242  [  160/  306]
train() client id: f_00004-2-5 loss: 0.668962  [  192/  306]
train() client id: f_00004-2-6 loss: 0.783138  [  224/  306]
train() client id: f_00004-2-7 loss: 0.703283  [  256/  306]
train() client id: f_00004-2-8 loss: 0.820612  [  288/  306]
train() client id: f_00005-0-0 loss: 0.524309  [   32/  146]
train() client id: f_00005-0-1 loss: 0.823712  [   64/  146]
train() client id: f_00005-0-2 loss: 0.458063  [   96/  146]
train() client id: f_00005-0-3 loss: 0.664510  [  128/  146]
train() client id: f_00005-1-0 loss: 0.217110  [   32/  146]
train() client id: f_00005-1-1 loss: 0.534977  [   64/  146]
train() client id: f_00005-1-2 loss: 0.704088  [   96/  146]
train() client id: f_00005-1-3 loss: 0.791807  [  128/  146]
train() client id: f_00005-2-0 loss: 0.237082  [   32/  146]
train() client id: f_00005-2-1 loss: 0.570524  [   64/  146]
train() client id: f_00005-2-2 loss: 0.797934  [   96/  146]
train() client id: f_00005-2-3 loss: 0.688712  [  128/  146]
train() client id: f_00006-0-0 loss: 0.421037  [   32/   54]
train() client id: f_00006-1-0 loss: 0.466771  [   32/   54]
train() client id: f_00006-2-0 loss: 0.453082  [   32/   54]
train() client id: f_00007-0-0 loss: 0.659109  [   32/  179]
train() client id: f_00007-0-1 loss: 0.697568  [   64/  179]
train() client id: f_00007-0-2 loss: 0.691060  [   96/  179]
train() client id: f_00007-0-3 loss: 0.641943  [  128/  179]
train() client id: f_00007-0-4 loss: 0.938335  [  160/  179]
train() client id: f_00007-1-0 loss: 0.620295  [   32/  179]
train() client id: f_00007-1-1 loss: 0.588050  [   64/  179]
train() client id: f_00007-1-2 loss: 0.699344  [   96/  179]
train() client id: f_00007-1-3 loss: 0.899702  [  128/  179]
train() client id: f_00007-1-4 loss: 0.719207  [  160/  179]
train() client id: f_00007-2-0 loss: 0.707757  [   32/  179]
train() client id: f_00007-2-1 loss: 0.731180  [   64/  179]
train() client id: f_00007-2-2 loss: 0.674473  [   96/  179]
train() client id: f_00007-2-3 loss: 0.842155  [  128/  179]
train() client id: f_00007-2-4 loss: 0.716478  [  160/  179]
train() client id: f_00008-0-0 loss: 0.773921  [   32/  130]
train() client id: f_00008-0-1 loss: 0.613652  [   64/  130]
train() client id: f_00008-0-2 loss: 0.646453  [   96/  130]
train() client id: f_00008-0-3 loss: 0.671436  [  128/  130]
train() client id: f_00008-1-0 loss: 0.672916  [   32/  130]
train() client id: f_00008-1-1 loss: 0.608250  [   64/  130]
train() client id: f_00008-1-2 loss: 0.714221  [   96/  130]
train() client id: f_00008-1-3 loss: 0.701417  [  128/  130]
train() client id: f_00008-2-0 loss: 0.641171  [   32/  130]
train() client id: f_00008-2-1 loss: 0.570147  [   64/  130]
train() client id: f_00008-2-2 loss: 0.600803  [   96/  130]
train() client id: f_00008-2-3 loss: 0.858905  [  128/  130]
train() client id: f_00009-0-0 loss: 0.703246  [   32/  118]
train() client id: f_00009-0-1 loss: 0.843390  [   64/  118]
train() client id: f_00009-0-2 loss: 0.624097  [   96/  118]
train() client id: f_00009-1-0 loss: 0.736560  [   32/  118]
train() client id: f_00009-1-1 loss: 0.753352  [   64/  118]
train() client id: f_00009-1-2 loss: 0.721308  [   96/  118]
train() client id: f_00009-2-0 loss: 0.612990  [   32/  118]
train() client id: f_00009-2-1 loss: 0.652866  [   64/  118]
train() client id: f_00009-2-2 loss: 0.726246  [   96/  118]
At round 81 accuracy: 0.6472148541114059
At round 81 training accuracy: 0.5902079141515761
At round 81 training loss: 0.8080781114604778
update_location
xs = [  -3.9056584     4.20031788  425.00902392   18.81129433    0.97929623
    3.95640986 -387.44319194 -366.32485185  409.66397685 -352.06087855]
ys = [ 417.5879595   400.55583871    1.32061395 -387.45517586  379.35018685
  362.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [429.4123404  412.87119371 436.61701116 400.5937819  392.31049346
 376.36386803 400.1488692  379.72960556 422.05834182 366.00939068]
dists_bs = [296.85382231 288.07670906 624.64018276 594.91061737 269.68075861
 259.42157167 276.91649437 258.73355676 605.48863217 246.15248922]
uav_gains = [5.76271498e-13 6.51982263e-13 5.47875000e-13 7.20245856e-13
 7.73843588e-13 8.99752934e-13 7.22955533e-13 8.70243248e-13
 6.07950194e-13 1.00281652e-12]
bs_gains = [1.31878706e-11 1.43440329e-11 1.64259431e-12 1.88290873e-12
 1.72549694e-11 1.92343292e-11 1.60220239e-11 1.93778844e-11
 1.79224469e-12 2.22803616e-11]
Round 82
-------------------------------
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.03972093 0.07667447 0.03927574 0.01549837 0.08835219 0.04258691
 0.01863559 0.05403005 0.03910533 0.03455187]
obj_prev = 0.44843144567355053
eta_min = 0.0	eta_max = inf
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.10079758902614538	eta = 0.9090909090909091
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.3375882886973583	eta = 0.2714376502678347
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.19414841544868844	eta = 0.47198001400206313
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17188140883090305	eta = 0.5331243935293785
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17048248758463935	eta = 0.5374990307814271
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17047588964991173	eta = 0.5375198336265012
eta = 0.5375198336265012
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.05122607 0.10773737 0.05041296 0.01748191 0.12440617 0.05935719
 0.02195403 0.07277355 0.0528523  0.04797362]
ene_total = [0.01831375 0.02382031 0.0184197  0.01040877 0.02710943 0.01385921
 0.01130017 0.020882   0.01487805 0.0114845 ]
ti_comp = [127.24870803 127.54244623 127.23535515 127.30084038 127.54703912
 127.54954855 127.30163179 127.33761381 127.43428871 127.5527431 ]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [5.18855480e-10 4.80473758e-09 4.94641987e-10 2.06055571e-11
 7.39716068e-09 8.03419283e-10 4.08088986e-11 1.48554980e-09
 5.68197076e-10 4.24138750e-10]
ene_total = [0.00791929 0.00195895 0.00819024 0.00686146 0.00186576 0.00181484
 0.0068454  0.00611528 0.00415361 0.00175002]
optimize_network iter = 0 obj = 0.04747485012945179
eta = 0.5375198336265012
freqs = [201283.26753271 422358.89209453 198109.08366522  68663.77799187
 487687.42285783 232682.88526082  86228.39443656 285750.39151138
 207370.78109251 188054.06037245]
eta_min = 0.537519833626502	eta_max = 0.9942699445819028
af = 2.5030539272411267e-09	bf = 0.021956175032555886	zeta = 2.7533593199652398e-09	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [7.96601739e-11 7.37674065e-10 7.59426628e-11 3.16358278e-12
 1.13569024e-09 1.23349415e-10 6.26541318e-12 2.28077296e-10
 8.72356169e-11 6.51182610e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 1 obj = 0.24151792697303298
eta = 0.909090909090909
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775164]
eta_min = 0.909090909090918	eta_max = 0.9684934797724077
af = 1.5403473503468153e-09	bf = 0.021956175032555886	zeta = 1.694382085381497e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 2 obj = 0.24151792697305713
eta = 0.909090909090918
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775163]
Done!
At round 82 energy consumption: 0.24151792697303298
At round 82 eta: 0.909090909090918
At round 82 local rounds: 3.1209395205774326
At round 82 global rounds: 1.0322774524740326
At round 82 a_n: -0.2487024421989581
gradient difference: 0.9015123248100281
train() client id: f_00000-0-0 loss: 0.944906  [   32/  126]
train() client id: f_00000-0-1 loss: 0.813115  [   64/  126]
train() client id: f_00000-0-2 loss: 1.019813  [   96/  126]
train() client id: f_00000-1-0 loss: 1.092509  [   32/  126]
train() client id: f_00000-1-1 loss: 0.745150  [   64/  126]
train() client id: f_00000-1-2 loss: 0.941403  [   96/  126]
train() client id: f_00000-2-0 loss: 0.781555  [   32/  126]
train() client id: f_00000-2-1 loss: 0.906948  [   64/  126]
train() client id: f_00000-2-2 loss: 0.964315  [   96/  126]
train() client id: f_00001-0-0 loss: 0.544416  [   32/  265]
train() client id: f_00001-0-1 loss: 0.537305  [   64/  265]
train() client id: f_00001-0-2 loss: 0.588366  [   96/  265]
train() client id: f_00001-0-3 loss: 0.419288  [  128/  265]
train() client id: f_00001-0-4 loss: 0.554579  [  160/  265]
train() client id: f_00001-0-5 loss: 0.434850  [  192/  265]
train() client id: f_00001-0-6 loss: 0.563758  [  224/  265]
train() client id: f_00001-0-7 loss: 0.535441  [  256/  265]
train() client id: f_00001-1-0 loss: 0.511038  [   32/  265]
train() client id: f_00001-1-1 loss: 0.475567  [   64/  265]
train() client id: f_00001-1-2 loss: 0.491204  [   96/  265]
train() client id: f_00001-1-3 loss: 0.569369  [  128/  265]
train() client id: f_00001-1-4 loss: 0.599540  [  160/  265]
train() client id: f_00001-1-5 loss: 0.551625  [  192/  265]
train() client id: f_00001-1-6 loss: 0.501353  [  224/  265]
train() client id: f_00001-1-7 loss: 0.484155  [  256/  265]
train() client id: f_00001-2-0 loss: 0.477880  [   32/  265]
train() client id: f_00001-2-1 loss: 0.612139  [   64/  265]
train() client id: f_00001-2-2 loss: 0.547655  [   96/  265]
train() client id: f_00001-2-3 loss: 0.579591  [  128/  265]
train() client id: f_00001-2-4 loss: 0.422122  [  160/  265]
train() client id: f_00001-2-5 loss: 0.583502  [  192/  265]
train() client id: f_00001-2-6 loss: 0.465269  [  224/  265]
train() client id: f_00001-2-7 loss: 0.527262  [  256/  265]
train() client id: f_00002-0-0 loss: 0.737532  [   32/  124]
train() client id: f_00002-0-1 loss: 0.664693  [   64/  124]
train() client id: f_00002-0-2 loss: 0.702992  [   96/  124]
train() client id: f_00002-1-0 loss: 0.756700  [   32/  124]
train() client id: f_00002-1-1 loss: 0.645794  [   64/  124]
train() client id: f_00002-1-2 loss: 0.639933  [   96/  124]
train() client id: f_00002-2-0 loss: 0.751800  [   32/  124]
train() client id: f_00002-2-1 loss: 0.729898  [   64/  124]
train() client id: f_00002-2-2 loss: 0.608658  [   96/  124]
train() client id: f_00003-0-0 loss: 0.683343  [   32/   43]
train() client id: f_00003-1-0 loss: 0.409389  [   32/   43]
train() client id: f_00003-2-0 loss: 0.463482  [   32/   43]
train() client id: f_00004-0-0 loss: 0.855800  [   32/  306]
train() client id: f_00004-0-1 loss: 0.786112  [   64/  306]
train() client id: f_00004-0-2 loss: 0.653571  [   96/  306]
train() client id: f_00004-0-3 loss: 0.813365  [  128/  306]
train() client id: f_00004-0-4 loss: 0.866141  [  160/  306]
train() client id: f_00004-0-5 loss: 0.868498  [  192/  306]
train() client id: f_00004-0-6 loss: 0.711842  [  224/  306]
train() client id: f_00004-0-7 loss: 0.945635  [  256/  306]
train() client id: f_00004-0-8 loss: 0.652224  [  288/  306]
train() client id: f_00004-1-0 loss: 0.951071  [   32/  306]
train() client id: f_00004-1-1 loss: 0.585964  [   64/  306]
train() client id: f_00004-1-2 loss: 0.765166  [   96/  306]
train() client id: f_00004-1-3 loss: 0.864159  [  128/  306]
train() client id: f_00004-1-4 loss: 0.807331  [  160/  306]
train() client id: f_00004-1-5 loss: 0.875735  [  192/  306]
train() client id: f_00004-1-6 loss: 0.823700  [  224/  306]
train() client id: f_00004-1-7 loss: 0.693360  [  256/  306]
train() client id: f_00004-1-8 loss: 0.772444  [  288/  306]
train() client id: f_00004-2-0 loss: 0.847118  [   32/  306]
train() client id: f_00004-2-1 loss: 0.856504  [   64/  306]
train() client id: f_00004-2-2 loss: 0.654030  [   96/  306]
train() client id: f_00004-2-3 loss: 0.889365  [  128/  306]
train() client id: f_00004-2-4 loss: 0.857885  [  160/  306]
train() client id: f_00004-2-5 loss: 0.722356  [  192/  306]
train() client id: f_00004-2-6 loss: 0.705785  [  224/  306]
train() client id: f_00004-2-7 loss: 0.826105  [  256/  306]
train() client id: f_00004-2-8 loss: 0.743275  [  288/  306]
train() client id: f_00005-0-0 loss: 0.355856  [   32/  146]
train() client id: f_00005-0-1 loss: 0.330060  [   64/  146]
train() client id: f_00005-0-2 loss: 0.738004  [   96/  146]
train() client id: f_00005-0-3 loss: 0.677064  [  128/  146]
train() client id: f_00005-1-0 loss: 0.366600  [   32/  146]
train() client id: f_00005-1-1 loss: 0.569836  [   64/  146]
train() client id: f_00005-1-2 loss: 0.563227  [   96/  146]
train() client id: f_00005-1-3 loss: 0.703160  [  128/  146]
train() client id: f_00005-2-0 loss: 0.693473  [   32/  146]
train() client id: f_00005-2-1 loss: 0.465630  [   64/  146]
train() client id: f_00005-2-2 loss: 0.489907  [   96/  146]
train() client id: f_00005-2-3 loss: 0.363990  [  128/  146]
train() client id: f_00006-0-0 loss: 0.555786  [   32/   54]
train() client id: f_00006-1-0 loss: 0.563540  [   32/   54]
train() client id: f_00006-2-0 loss: 0.554920  [   32/   54]
train() client id: f_00007-0-0 loss: 0.654448  [   32/  179]
train() client id: f_00007-0-1 loss: 0.621795  [   64/  179]
train() client id: f_00007-0-2 loss: 0.515373  [   96/  179]
train() client id: f_00007-0-3 loss: 0.794300  [  128/  179]
train() client id: f_00007-0-4 loss: 0.650157  [  160/  179]
train() client id: f_00007-1-0 loss: 0.768423  [   32/  179]
train() client id: f_00007-1-1 loss: 0.579343  [   64/  179]
train() client id: f_00007-1-2 loss: 0.763488  [   96/  179]
train() client id: f_00007-1-3 loss: 0.624502  [  128/  179]
train() client id: f_00007-1-4 loss: 0.662890  [  160/  179]
train() client id: f_00007-2-0 loss: 0.596729  [   32/  179]
train() client id: f_00007-2-1 loss: 0.659925  [   64/  179]
train() client id: f_00007-2-2 loss: 0.723086  [   96/  179]
train() client id: f_00007-2-3 loss: 0.893231  [  128/  179]
train() client id: f_00007-2-4 loss: 0.530738  [  160/  179]
train() client id: f_00008-0-0 loss: 0.603274  [   32/  130]
train() client id: f_00008-0-1 loss: 0.525320  [   64/  130]
train() client id: f_00008-0-2 loss: 0.644924  [   96/  130]
train() client id: f_00008-0-3 loss: 0.590025  [  128/  130]
train() client id: f_00008-1-0 loss: 0.555516  [   32/  130]
train() client id: f_00008-1-1 loss: 0.546492  [   64/  130]
train() client id: f_00008-1-2 loss: 0.682939  [   96/  130]
train() client id: f_00008-1-3 loss: 0.597965  [  128/  130]
train() client id: f_00008-2-0 loss: 0.502125  [   32/  130]
train() client id: f_00008-2-1 loss: 0.585597  [   64/  130]
train() client id: f_00008-2-2 loss: 0.626172  [   96/  130]
train() client id: f_00008-2-3 loss: 0.686040  [  128/  130]
train() client id: f_00009-0-0 loss: 0.658873  [   32/  118]
train() client id: f_00009-0-1 loss: 1.053537  [   64/  118]
train() client id: f_00009-0-2 loss: 0.821995  [   96/  118]
train() client id: f_00009-1-0 loss: 0.882292  [   32/  118]
train() client id: f_00009-1-1 loss: 0.665928  [   64/  118]
train() client id: f_00009-1-2 loss: 0.726357  [   96/  118]
train() client id: f_00009-2-0 loss: 0.662867  [   32/  118]
train() client id: f_00009-2-1 loss: 0.690670  [   64/  118]
train() client id: f_00009-2-2 loss: 0.804243  [   96/  118]
At round 82 accuracy: 0.6472148541114059
At round 82 training accuracy: 0.5875251509054326
At round 82 training loss: 0.8275273943694277
Done!
