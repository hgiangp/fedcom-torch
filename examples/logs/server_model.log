v = 22.697160301531774
a_0 = 39.83011693861099	a_alpha = -0.5113081611742347
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
gradient difference: 2.0123987197875977
At round 0 accuracy: 0.41114058355437666
At round 0 training accuracy: 0.39101274312541917
At round 0 training loss: 1.0800984192306733
gradient difference: 0.18125352263450623
At round 1 accuracy: 0.5994694960212201
At round 1 training accuracy: 0.579476861167002
At round 1 training loss: 0.963020946127317
gradient difference: 0.2669667601585388
At round 2 accuracy: 0.6312997347480106
At round 2 training accuracy: 0.630449362843729
At round 2 training loss: 0.9108123518189531
gradient difference: 0.23045487701892853
At round 3 accuracy: 0.6392572944297082
At round 3 training accuracy: 0.636485580147552
At round 3 training loss: 0.8902976674057791
gradient difference: 0.1761840134859085
At round 4 accuracy: 0.636604774535809
At round 4 training accuracy: 0.6371562709590879
At round 4 training loss: 0.8780294994095664
gradient difference: 0.18968649208545685
At round 5 accuracy: 0.636604774535809
At round 5 training accuracy: 0.6391683433936955
At round 5 training loss: 0.8728483371571516
gradient difference: 0.24110284447669983
At round 6 accuracy: 0.636604774535809
At round 6 training accuracy: 0.6411804158283032
At round 6 training loss: 0.8676047979974739
gradient difference: 0.15431971848011017
At round 7 accuracy: 0.636604774535809
At round 7 training accuracy: 0.6411804158283032
At round 7 training loss: 0.8615433482210454
gradient difference: 0.585593581199646
At round 8 accuracy: 0.6339522546419099
At round 8 training accuracy: 0.641851106639839
At round 8 training loss: 0.869003831612672
gradient difference: 0.2482854127883911
At round 9 accuracy: 0.6392572944297082
At round 9 training accuracy: 0.641851106639839
At round 9 training loss: 0.8651646365431082
gradient difference: 0.1513996124267578
At round 10 accuracy: 0.6392572944297082
At round 10 training accuracy: 0.641851106639839
At round 10 training loss: 0.8692373530254767
gradient difference: 0.17144742608070374
At round 11 accuracy: 0.6392572944297082
At round 11 training accuracy: 0.6438631790744467
At round 11 training loss: 0.8606562955785113
gradient difference: 0.19302067160606384
At round 12 accuracy: 0.6392572944297082
At round 12 training accuracy: 0.6438631790744467
At round 12 training loss: 0.8599914465594003
gradient difference: 0.3517153561115265
At round 13 accuracy: 0.6419098143236074
At round 13 training accuracy: 0.6445338698859826
At round 13 training loss: 0.8615496329038755
gradient difference: 0.168033629655838
At round 14 accuracy: 0.6419098143236074
At round 14 training accuracy: 0.6458752515090543
At round 14 training loss: 0.8645447813613519
gradient difference: 0.15723364055156708
At round 15 accuracy: 0.6392572944297082
At round 15 training accuracy: 0.6465459423205903
At round 15 training loss: 0.8646624993141738
gradient difference: 0.3069106638431549
At round 16 accuracy: 0.6392572944297082
At round 16 training accuracy: 0.6458752515090543
At round 16 training loss: 0.8550660000196361
gradient difference: 0.22396370768547058
At round 17 accuracy: 0.6392572944297082
At round 17 training accuracy: 0.6472166331321261
At round 17 training loss: 0.8660598285332444
gradient difference: 0.25928404927253723
At round 18 accuracy: 0.636604774535809
At round 18 training accuracy: 0.6472166331321261
At round 18 training loss: 0.8546000071132546
gradient difference: 0.20556119084358215
At round 19 accuracy: 0.636604774535809
At round 19 training accuracy: 0.647887323943662
At round 19 training loss: 0.8585976738916067
gradient difference: 0.23623785376548767
At round 20 accuracy: 0.636604774535809
At round 20 training accuracy: 0.6492287055667337
At round 20 training loss: 0.8544307446988828
gradient difference: 0.18068620562553406
At round 21 accuracy: 0.636604774535809
At round 21 training accuracy: 0.6492287055667337
At round 21 training loss: 0.8600761795962956
gradient difference: 0.18146829307079315
At round 22 accuracy: 0.6339522546419099
At round 22 training accuracy: 0.6492287055667337
At round 22 training loss: 0.8566900305342804
gradient difference: 0.32650628685951233
At round 23 accuracy: 0.6339522546419099
At round 23 training accuracy: 0.6492287055667337
At round 23 training loss: 0.8526502629634173
gradient difference: 0.2514481544494629
At round 24 accuracy: 0.636604774535809
At round 24 training accuracy: 0.6485580147551978
At round 24 training loss: 0.8574904395689755
gradient difference: 0.20760223269462585
At round 25 accuracy: 0.6339522546419099
At round 25 training accuracy: 0.6485580147551978
At round 25 training loss: 0.8604654021281866
gradient difference: 0.16117236018180847
At round 26 accuracy: 0.6339522546419099
At round 26 training accuracy: 0.647887323943662
At round 26 training loss: 0.8706659473374967
gradient difference: 0.2050890475511551
At round 27 accuracy: 0.6339522546419099
At round 27 training accuracy: 0.647887323943662
At round 27 training loss: 0.8589715674533975
gradient difference: 0.17262421548366547
At round 28 accuracy: 0.6339522546419099
At round 28 training accuracy: 0.647887323943662
At round 28 training loss: 0.869287515861443
gradient difference: 0.20807310938835144
At round 29 accuracy: 0.6339522546419099
At round 29 training accuracy: 0.6485580147551978
At round 29 training loss: 0.8555983120646754
gradient difference: 0.227626770734787
At round 30 accuracy: 0.6339522546419099
At round 30 training accuracy: 0.6485580147551978
At round 30 training loss: 0.8616021862414424
gradient difference: 0.2321152687072754
At round 31 accuracy: 0.6339522546419099
At round 31 training accuracy: 0.6485580147551978
At round 31 training loss: 0.8573355386507955
gradient difference: 0.19065876305103302
At round 32 accuracy: 0.6339522546419099
At round 32 training accuracy: 0.6492287055667337
At round 32 training loss: 0.8631352770674094
gradient difference: 0.5254876017570496
At round 33 accuracy: 0.6339522546419099
At round 33 training accuracy: 0.6492287055667337
At round 33 training loss: 0.8648723292130012
gradient difference: 0.351250559091568
At round 34 accuracy: 0.6339522546419099
At round 34 training accuracy: 0.6492287055667337
At round 34 training loss: 0.8585891133558378
gradient difference: 0.16421805322170258
At round 35 accuracy: 0.6339522546419099
At round 35 training accuracy: 0.6492287055667337
At round 35 training loss: 0.8665513915961524
gradient difference: 0.16956397891044617
At round 36 accuracy: 0.6339522546419099
At round 36 training accuracy: 0.6492287055667337
At round 36 training loss: 0.8571943154501768
gradient difference: 0.19903503358364105
At round 37 accuracy: 0.6312997347480106
At round 37 training accuracy: 0.6492287055667337
At round 37 training loss: 0.8686005232806123
gradient difference: 0.2107863426208496
At round 38 accuracy: 0.6312997347480106
At round 38 training accuracy: 0.6498993963782697
At round 38 training loss: 0.8587457447773263
gradient difference: 0.16075918078422546
At round 39 accuracy: 0.6312997347480106
At round 39 training accuracy: 0.6498993963782697
At round 39 training loss: 0.8633447099635014
gradient difference: 0.2661139965057373
At round 40 accuracy: 0.6312997347480106
At round 40 training accuracy: 0.6498993963782697
At round 40 training loss: 0.8531424262423
gradient difference: 0.2868598401546478
At round 41 accuracy: 0.6312997347480106
At round 41 training accuracy: 0.6498993963782697
At round 41 training loss: 0.8542390497966161
gradient difference: 0.39895784854888916
At round 42 accuracy: 0.6339522546419099
At round 42 training accuracy: 0.6498993963782697
At round 42 training loss: 0.8470141315004476
gradient difference: 0.5920247435569763
At round 43 accuracy: 0.6339522546419099
At round 43 training accuracy: 0.6498993963782697
At round 43 training loss: 0.8623818606207078
gradient difference: 0.2585207223892212
At round 44 accuracy: 0.6339522546419099
At round 44 training accuracy: 0.6498993963782697
At round 44 training loss: 0.8628499816803048
gradient difference: 0.20778140425682068
At round 45 accuracy: 0.6339522546419099
At round 45 training accuracy: 0.6498993963782697
At round 45 training loss: 0.8703897088244186
gradient difference: 0.2475883662700653
At round 46 accuracy: 0.6339522546419099
At round 46 training accuracy: 0.6498993963782697
At round 46 training loss: 0.8597473031449437
gradient difference: 0.2168005257844925
At round 47 accuracy: 0.6339522546419099
At round 47 training accuracy: 0.6498993963782697
At round 47 training loss: 0.8573753049638093
gradient difference: 0.21127431094646454
At round 48 accuracy: 0.6339522546419099
At round 48 training accuracy: 0.6498993963782697
At round 48 training loss: 0.8590528575876039
gradient difference: 0.3245159387588501
At round 49 accuracy: 0.6339522546419099
At round 49 training accuracy: 0.6498993963782697
At round 49 training loss: 0.8630329762479484
gradient difference: 0.26177260279655457
At round 50 accuracy: 0.6339522546419099
At round 50 training accuracy: 0.6492287055667337
At round 50 training loss: 0.8557304455571881
gradient difference: 0.2399725466966629
At round 51 accuracy: 0.6339522546419099
At round 51 training accuracy: 0.6492287055667337
At round 51 training loss: 0.8718474743074237
gradient difference: 0.19367744028568268
At round 52 accuracy: 0.6339522546419099
At round 52 training accuracy: 0.6498993963782697
At round 52 training loss: 0.8610343025066306
gradient difference: 0.2739875912666321
At round 53 accuracy: 0.6339522546419099
At round 53 training accuracy: 0.6492287055667337
At round 53 training loss: 0.8582347371326544
gradient difference: 0.2889544367790222
At round 54 accuracy: 0.6339522546419099
At round 54 training accuracy: 0.6492287055667337
At round 54 training loss: 0.8727813064994288
gradient difference: 0.2462078332901001
At round 55 accuracy: 0.6339522546419099
At round 55 training accuracy: 0.6492287055667337
At round 55 training loss: 0.8673451576374426
gradient difference: 0.21324467658996582
At round 56 accuracy: 0.6339522546419099
At round 56 training accuracy: 0.6492287055667337
At round 56 training loss: 0.8713267916695924
gradient difference: 0.21601566672325134
At round 57 accuracy: 0.6339522546419099
At round 57 training accuracy: 0.6492287055667337
At round 57 training loss: 0.8633831897187636
gradient difference: 0.1950967013835907
At round 58 accuracy: 0.6339522546419099
At round 58 training accuracy: 0.6492287055667337
At round 58 training loss: 0.8683071477794373
gradient difference: 0.16057629883289337
At round 59 accuracy: 0.6339522546419099
At round 59 training accuracy: 0.6498993963782697
At round 59 training loss: 0.8518940708122699
gradient difference: 0.1980309933423996
At round 60 accuracy: 0.6339522546419099
At round 60 training accuracy: 0.6492287055667337
At round 60 training loss: 0.8546179965255033
gradient difference: 0.23972824215888977
At round 61 accuracy: 0.6339522546419099
At round 61 training accuracy: 0.6492287055667337
At round 61 training loss: 0.8551909620737531
gradient difference: 0.23194542527198792
At round 62 accuracy: 0.6339522546419099
At round 62 training accuracy: 0.6492287055667337
At round 62 training loss: 0.8524935306308967
gradient difference: 0.26152414083480835
At round 63 accuracy: 0.6339522546419099
At round 63 training accuracy: 0.6492287055667337
At round 63 training loss: 0.8538948345845266
gradient difference: 0.32301968336105347
At round 64 accuracy: 0.636604774535809
At round 64 training accuracy: 0.6498993963782697
At round 64 training loss: 0.8566578556254436
gradient difference: 0.2431321144104004
At round 65 accuracy: 0.636604774535809
At round 65 training accuracy: 0.6498993963782697
At round 65 training loss: 0.848562139106344
gradient difference: 0.22963306307792664
At round 66 accuracy: 0.636604774535809
At round 66 training accuracy: 0.6492287055667337
At round 66 training loss: 0.8759058503679888
gradient difference: 0.2787010371685028
At round 67 accuracy: 0.636604774535809
At round 67 training accuracy: 0.6498993963782697
At round 67 training loss: 0.8626664014587158
gradient difference: 0.2633567154407501
At round 68 accuracy: 0.636604774535809
At round 68 training accuracy: 0.6492287055667337
At round 68 training loss: 0.8714927424736194
gradient difference: 0.19248782098293304
At round 69 accuracy: 0.636604774535809
At round 69 training accuracy: 0.6492287055667337
At round 69 training loss: 0.8504812105451862
gradient difference: 0.28053727746009827
At round 70 accuracy: 0.636604774535809
At round 70 training accuracy: 0.6498993963782697
At round 70 training loss: 0.8561478467865401
gradient difference: 0.2738996148109436
At round 71 accuracy: 0.636604774535809
At round 71 training accuracy: 0.6505700871898055
At round 71 training loss: 0.8605752786693492
gradient difference: 0.4092217981815338
At round 72 accuracy: 0.636604774535809
At round 72 training accuracy: 0.6498993963782697
At round 72 training loss: 0.8640221545538397
gradient difference: 0.18181273341178894
At round 73 accuracy: 0.636604774535809
At round 73 training accuracy: 0.6498993963782697
At round 73 training loss: 0.8593431343397286
gradient difference: 0.2747056484222412
At round 74 accuracy: 0.636604774535809
At round 74 training accuracy: 0.6492287055667337
At round 74 training loss: 0.8640552539587447
gradient difference: 0.1822570115327835
At round 75 accuracy: 0.636604774535809
At round 75 training accuracy: 0.6505700871898055
At round 75 training loss: 0.8555174014875524
gradient difference: 0.7046614289283752
At round 76 accuracy: 0.636604774535809
At round 76 training accuracy: 0.6505700871898055
At round 76 training loss: 0.8525011416363357
gradient difference: 0.329612135887146
At round 77 accuracy: 0.636604774535809
At round 77 training accuracy: 0.6498993963782697
At round 77 training loss: 0.8686620125408384
gradient difference: 0.22699053585529327
At round 78 accuracy: 0.636604774535809
At round 78 training accuracy: 0.6505700871898055
At round 78 training loss: 0.876589286008218
gradient difference: 0.21028895676136017
At round 79 accuracy: 0.636604774535809
At round 79 training accuracy: 0.6505700871898055
At round 79 training loss: 0.8649059808636681
