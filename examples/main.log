v = 22.697160301531774
a = 91.65029380654791
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 83
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 129
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 85
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 80
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 235
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 100
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 93
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 89
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 86
BaseFederated generated!
num_samples = [ 83 129  85  80 235 100 146  93  89  86]
msize = 502400
xs = [  6.0943416  -20.79968212  15.00902392  18.81129433 -39.02070377
 -26.04359014   2.55680806  -6.32485185  -0.33602315 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  22.54482414   9.35018685
 -17.18584926   7.37501568 -19.17765202  17.56900603  -0.99851822]
dists_uav = [101.71763524 103.31800857 101.12870423 104.22156154 107.7499017
 104.75505717 100.304178   102.01855757 101.5321766  101.44984286]
dists_bs = [239.94522511 221.81113273 257.42563646 246.58750669 214.31339109
 243.15306433 244.18139867 257.20860434 235.14255819 236.47461703]
uav_gains = [9.58312886e-11 9.21631668e-11 9.72326274e-11 9.01785356e-11
 8.29761259e-11 8.90347214e-11 9.92432040e-11 9.51261523e-11
 9.62695134e-11 9.64649612e-11]
bs_gains = [2.39320649e-11 2.98222725e-11 1.96548195e-11 2.21704798e-11
 3.28364367e-11 2.30584863e-11 2.27876148e-11 1.97012919e-11
 2.53260008e-11 2.49285719e-11]
SystemModel __init__!
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.25032247 0.38905541 0.25635434 0.24127467 0.70874434 0.30159334
 0.44032627 0.2804818  0.26841807 0.25937027]
ene_total = [23.76183224 36.76263384 24.31869245 22.9307867  66.34214127 28.71212373
 41.34792207 26.55465407 25.62334201 24.78863831]
obj_prev = 321.1427666854388
t_co_uav = [0.06351163 0.06396501 0.06334461 0.06422068 0.06521719 0.06437154
 0.0631106  0.06359693 0.06345904 0.0634357 ]
t_co_bs = [0.08476871 0.08051968 0.08895491 0.08634835 0.0787878  0.08552995
 0.08577462 0.08890235 0.0836346  0.08394849]
difference = [-0.02125708 -0.01655467 -0.0256103  -0.02212767 -0.01357061 -0.0211584
 -0.02266402 -0.02530541 -0.02017555 -0.0205128 ]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 16.896171935691484	bf = 66.9234999628387	zeta = 33.79234387138297	eta = 0.5
af = 16.896171935691484	bf = 66.9234999628387	zeta = 157.27006780463867	eta = 0.1074341238072076
af = 16.896171935691484	bf = 66.9234999628387	zeta = 117.20903882473088	eta = 0.1441541719402482
af = 16.896171935691484	bf = 66.9234999628387	zeta = 116.43361171638361	eta = 0.14511421304054584
af = 16.896171935691484	bf = 66.9234999628387	zeta = 116.4331772754501	eta = 0.14511475449750555
af = 16.896171935691484	bf = 66.9234999628387	zeta = 116.43317727531253	eta = 0.14511475449767702
eta_opt = 0.14511475449767702
solve_bound_eta tau = 65.0
solve_bound_eta tau = 37.5
solve_bound_eta tau = 23.75
solve_bound_eta tau = 16.875
solve_bound_eta tau = 13.4375
solve_bound_eta tau = 15.15625
solve_bound_eta tau = 16.015625
solve_bound_eta tau = 16.4453125
solve_bound_eta tau = 16.23046875
solve_bound_eta tau = 16.337890625
solve_bound_eta tau = 16.3916015625
solve_bound_eta tau = 16.41845703125
solve_bound_eta tau = 16.405029296875
solve_bound_eta tau = 16.4117431640625
solve_bound_eta tau = 16.40838623046875
solve_bound_eta tau = 16.410064697265625
solve_bound_eta tau = 16.409225463867188
initialize_feasible_solution eta = 0.14511475449767702, tau = 16.409225463867188
ti_comp = [0.0262303  0.04076758 0.02686236 0.02528222 0.07426652 0.03160278
 0.04614005 0.02939058 0.02812647 0.02717839]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [ 9.62102767 13.00293018  9.67088294  9.5954013  16.40860404 12.55753137
 11.71251303  9.96897901 11.98164868 11.91365904]
optimize_network_fake eta = 0.14511475449767702	tau = 21	t_min = 16.409225463867188
solve_bound_eta tau = 21
eta_min = 0.02304542908779013	eta_max = 0.5582623390540764
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 135.1693754855319	eta = 0.5
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 107.07697150841277	eta = 0.6311785511925502
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 102.4684402827174	eta = 0.6595658873726896
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 102.27911495535675	eta = 0.6607867869434109
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 102.27877445458762	eta = 0.6607889867977833
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 102.27877445348349	eta = 0.6607889868049167
eta = 0.6607889868049167
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.02252116 0.03500277 0.02306384 0.02170714 0.06376474 0.02713393
 0.03961554 0.02523456 0.0241492  0.02333518]
ene_total = [ 7.80091752 11.63281654  7.94302935  7.60013827 19.35712339  9.64213153
 12.40875492  8.53634495  8.78448685  8.57303114]
ti_comp = [0.03770453 0.02069648 0.03787155 0.03699548 0.02242836 0.01568622
 0.03810556 0.03761923 0.01758156 0.01726767]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00139861 0.01742711 0.00148895 0.00130083 0.08971345 0.0141323
 0.007453   0.00197642 0.00793057 0.00741783]
ene_total = [ 1.60789749  5.28631548  1.62317555  1.60232245 20.24811869  4.70667053
  2.85572206  1.72954873  3.38063131  3.28076434]
optimize_network_iter = 0 obj = 46.321166643300465
eta = 0.5582623390540764
freqs = [4.20188589e+08 1.18974237e+09 4.28415846e+08 4.12763225e+08
 2.00000000e+09 1.21686254e+09 7.31348244e+08 4.71881315e+08
 9.66256019e+08 9.50658428e+08]
solve_bound_eta tau = 21
eta_min = 0.12023805694888506	eta_max = 0.5582623390540764
af = 23.621260790044964	bf = 6.6923499962838715	zeta = 47.24252158008993	eta = 0.5
af = 23.621260790044964	bf = 6.6923499962838715	zeta = 46.13072062834945	eta = 0.5120505482745182
af = 23.621260790044964	bf = 6.6923499962838715	zeta = 46.11710092286252	eta = 0.5122017715197431
af = 23.621260790044964	bf = 6.6923499962838715	zeta = 46.11709881193507	eta = 0.5122017949648602
eta = 0.5122017949648602
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00160521 0.02000145 0.0017089  0.00149299 0.10296599 0.01621993
 0.00855396 0.00226837 0.00910207 0.0085136 ]
ene_total = [ 1.49488897  5.2708356   1.51123173  1.48712663 20.8261441   4.6544777
  2.79292444  1.62109004  3.28152361  3.17685599]
ti_comp = [0.04825847 0.03125042 0.04842549 0.04754942 0.0329823  0.02624015
 0.0486595  0.04817316 0.0281355  0.0278216 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00129076 0.01155621 0.00137679 0.00119052 0.06271906 0.00763528
 0.00691005 0.0018222  0.00468186 0.00432005]
ene_total = [ 1.43580759  3.68409525  1.44883289  1.43029653 13.26432284  3.04154534
  2.48405717  1.53726076  2.45102893  2.38894761]
optimize_network_iter = 1 obj = 33.1661949001213
eta = 0.5122017949648602
freqs = [3.76790986e+08 9.04336036e+08 3.84539407e+08 3.68587592e+08
 1.56092800e+09 8.34890627e+08 6.57326577e+08 4.22935087e+08
 6.92996914e+08 6.77192651e+08]
solve_bound_eta tau = 21
eta_min = 0.17966330823878449	eta_max = 0.5122017949648601
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 28.357373370854216	eta = 0.5
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 33.04053339266102	eta = 0.42913007840776823
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 32.73504185028158	eta = 0.4331348269012567
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 32.73396608414873	eta = 0.43314906140545767
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 32.733966070641806	eta = 0.43314906158418676
eta = 0.43314906158418676
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00161418 0.01445178 0.00172176 0.00148882 0.07843423 0.00954841
 0.00864146 0.00227878 0.00585497 0.0054025 ]
ene_total = [ 1.28786192  3.63847908  1.30255587  1.27905862 13.95536594  2.92669408
  2.41757224  1.39669631  2.29888159  2.23080041]
ti_comp = [0.06637197 0.04936391 0.06653899 0.06566292 0.0510958  0.04435365
 0.06677299 0.06628666 0.046249   0.0459351 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00133457 0.00905787 0.0014262  0.00122097 0.0511104  0.00522658
 0.00717683 0.00188223 0.00338876 0.00309943]
ene_total = [1.24265397 2.76637357 1.25476876 1.23575138 9.53756066 2.22792556
 2.18076633 1.33258074 1.90013761 1.8584325 ]
optimize_network_iter = 2 obj = 25.536951077960452
eta = 0.43314906158418676
freqs = [3.42606356e+08 7.15949056e+08 3.49981224e+08 3.33788844e+08
 1.26004102e+09 6.17692935e+08 5.99037501e+08 3.84378247e+08
 5.27217330e+08 5.12927255e+08]
solve_bound_eta tau = 21
eta_min = 0.2753370547044945	eta_max = 0.4331490615841859
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 18.605336527371843	eta = 0.5
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 26.2809365498845	eta = 0.35397019607837393
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 25.31395118638227	eta = 0.36749175169028236
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 25.303744389596346	eta = 0.36763998720721824
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 25.303743193437224	eta = 0.3676400045863041
eta = 0.3676400045863041
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00159613 0.0108331  0.00170572 0.00146027 0.06112741 0.00625092
 0.00858341 0.00225112 0.00405292 0.00370688]
ene_total = [ 1.15183051  2.73708314  1.16529308  1.14241624 10.00132391  2.1455867
  2.15871043  1.24799779  1.79955233  1.75394906]
ti_comp = [0.08138218 0.06437413 0.0815492  0.08067313 0.06610601 0.05936386
 0.0817832  0.08129687 0.06125921 0.06094531]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00151856 0.00911177 0.00162433 0.00138378 0.05223696 0.00499128
 0.00818439 0.00214071 0.00330433 0.00301211]
ene_total = [1.14058828 2.48760427 1.15349689 1.13133126 8.71279876 1.96302221
 2.10088022 1.23199483 1.69105633 1.6532533 ]
optimize_network_iter = 3 obj = 23.266026344810328
eta = 0.3676400045863041
freqs = [3.34177743e+08 6.56609403e+08 3.41529296e+08 3.24930007e+08
 1.16481152e+09 5.51958707e+08 5.84948270e+08 3.74833016e+08
 4.76044278e+08 4.62367048e+08]
solve_bound_eta tau = 21
eta_min = 0.34596780694746343	eta_max = 0.367640004586301
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 16.029872095788594	eta = 0.5
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 24.495760640500148	eta = 0.3271968633887929
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 23.255810921595604	eta = 0.3446422949909495
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 23.239593104525053	eta = 0.34488280461045095
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 23.239590128237733	eta = 0.34488284877948805
eta = 0.34488284877948805
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00161553 0.00969363 0.00172805 0.00147215 0.0555727  0.00531001
 0.00870703 0.00227741 0.00351534 0.00320446]
ene_total = [1.11453334 2.48259241 1.12793843 1.10439385 8.87680456 1.93942202
 2.10101736 1.20832271 1.66183294 1.62273251]
ti_comp = [0.08634797 0.06933992 0.08651499 0.08563892 0.0710718  0.06432965
 0.08674899 0.08626266 0.066225   0.0659111 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00160986 0.00937259 0.00172239 0.0014655  0.05393444 0.00507265
 0.00868134 0.00226913 0.0033743  0.00307352]
ene_total = [1.11558697 2.44172384 1.12901577 1.10529359 8.66195812 1.90937726
 2.10090272 1.20916775 1.64482549 1.60707532]
optimize_network_iter = 4 obj = 22.924926840287487
eta = 0.34596780694746343
freqs = [3.34083505e+08 6.46599638e+08 3.41473205e+08 3.24674269e+08
 1.14921057e+09 5.40278715e+08 5.84948270e+08 3.74704702e+08
 4.67086268e+08 4.53491272e+08]
solve_bound_eta tau = 21
eta_min = 0.3459678069474702	eta_max = 0.3459678069474627
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 15.642011449570198	eta = 0.5
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 24.226916127123708	eta = 0.32282299916946267
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 22.941090251059364	eta = 0.34091691542096364
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 22.92376106991469	eta = 0.3411746310272551
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 22.923757681402567	eta = 0.3411746814585321
eta = 0.3411746814585321
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00163102 0.00949578 0.00174503 0.00148476 0.05464335 0.00513932
 0.00879545 0.00229896 0.00341865 0.00311391]
ene_total = [1.11041438 2.44109724 1.12395124 1.09993192 8.69755803 1.90476126
 2.10149175 1.2045198  1.63902878 1.60100328]
ti_comp = [0.08634797 0.06933992 0.08651499 0.08563892 0.0710718  0.06432965
 0.08674899 0.08626266 0.066225   0.0659111 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00160986 0.00937259 0.00172239 0.0014655  0.05393444 0.00507265
 0.00868134 0.00226913 0.0033743  0.00307352]
ene_total = [1.11558697 2.44172384 1.12901577 1.10529359 8.66195812 1.90937726
 2.10090272 1.20916775 1.64482549 1.60707532]
optimize_network_iter = 5 obj = 22.924926840287576
eta = 0.3459678069474702
freqs = [3.34083505e+08 6.46599638e+08 3.41473205e+08 3.24674269e+08
 1.14921057e+09 5.40278715e+08 5.84948270e+08 3.74704702e+08
 4.67086268e+08 4.53491272e+08]
Done!
optimize_network_fake num_local_rounds = 34.75594133176583	num_global_rounds = 140.1311659886241
iter = 0	num_local_rounds = 34.75594133176583	num_global_rounds = 140.1311659886241
Round 1
-------------------------------
gradient difference: 1.5741688013076782
At round 0 accuracy: 0.397212543554007
At round 0 training accuracy: 0.41563055062166965
At round 0 training loss: 1.1098039958015076
Round 2
-------------------------------
gradient difference: 0.36927393078804016
At round 1 accuracy: 0.5017421602787456
At round 1 training accuracy: 0.4706927175843694
At round 1 training loss: 1.0235904712964439
Round 3
-------------------------------
gradient difference: 0.3576163947582245
At round 2 accuracy: 0.519163763066202
At round 2 training accuracy: 0.5337477797513321
At round 2 training loss: 0.9556367215935898
Round 4
-------------------------------
gradient difference: 0.3145295977592468
At round 3 accuracy: 0.554006968641115
At round 3 training accuracy: 0.5621669626998224
At round 3 training loss: 0.9190475660793664
Round 5
-------------------------------
gradient difference: 0.35037845373153687
At round 4 accuracy: 0.5958188153310104
At round 4 training accuracy: 0.5861456483126111
At round 4 training loss: 0.8860840592231181
Round 6
-------------------------------
gradient difference: 0.3830379843711853
At round 5 accuracy: 0.6236933797909407
At round 5 training accuracy: 0.5950266429840142
At round 5 training loss: 0.8919200598995067
Round 7
-------------------------------
gradient difference: 0.30467313528060913
At round 6 accuracy: 0.6341463414634146
At round 6 training accuracy: 0.6021314387211367
At round 6 training loss: 0.8576844278588458
Round 8
-------------------------------
gradient difference: 0.4123110771179199
At round 7 accuracy: 0.6376306620209059
At round 7 training accuracy: 0.6198934280639432
At round 7 training loss: 0.8426974217588817
Round 9
-------------------------------
gradient difference: 0.2877056300640106
At round 8 accuracy: 0.6341463414634146
At round 8 training accuracy: 0.6252220248667851
At round 8 training loss: 0.8402722004957633
Round 10
-------------------------------
gradient difference: 0.3010636270046234
At round 9 accuracy: 0.6376306620209059
At round 9 training accuracy: 0.6314387211367674
At round 9 training loss: 0.8290817912659166
Round 11
-------------------------------
gradient difference: 0.4236535131931305
At round 10 accuracy: 0.6445993031358885
At round 10 training accuracy: 0.6314387211367674
At round 10 training loss: 0.8334159678453662
Round 12
-------------------------------
gradient difference: 0.31168994307518005
At round 11 accuracy: 0.6515679442508711
At round 11 training accuracy: 0.6305506216696269
At round 11 training loss: 0.8475723122645229
Round 13
-------------------------------
gradient difference: 0.4194268584251404
At round 12 accuracy: 0.6585365853658537
At round 12 training accuracy: 0.6323268206039077
At round 12 training loss: 0.8215785438901771
Round 14
-------------------------------
gradient difference: 0.2884611487388611
At round 13 accuracy: 0.6655052264808362
At round 13 training accuracy: 0.6367673179396093
At round 13 training loss: 0.8104847031090212
Round 15
-------------------------------
gradient difference: 0.3283955454826355
At round 14 accuracy: 0.6794425087108014
At round 14 training accuracy: 0.6385435168738899
At round 14 training loss: 0.8101269413116611
Round 16
-------------------------------
gradient difference: 0.40158089995384216
At round 15 accuracy: 0.6829268292682927
At round 15 training accuracy: 0.6429840142095915
At round 15 training loss: 0.8114374130344502
Round 17
-------------------------------
gradient difference: 0.5345473885536194
At round 16 accuracy: 0.6829268292682927
At round 16 training accuracy: 0.6456483126110124
At round 16 training loss: 0.8098671216358883
Round 18
-------------------------------
gradient difference: 0.3028518855571747
At round 17 accuracy: 0.6829268292682927
At round 17 training accuracy: 0.6483126110124334
At round 17 training loss: 0.8110229023628029
Round 19
-------------------------------
gradient difference: 0.29598692059516907
At round 18 accuracy: 0.6829268292682927
At round 18 training accuracy: 0.6474245115452931
At round 18 training loss: 0.8027234875956024
Round 20
-------------------------------
gradient difference: 0.7750457525253296
At round 19 accuracy: 0.6829268292682927
At round 19 training accuracy: 0.650088809946714
At round 19 training loss: 0.8052149942988401
Round 21
-------------------------------
gradient difference: 0.35583919286727905
At round 20 accuracy: 0.6829268292682927
At round 20 training accuracy: 0.650088809946714
At round 20 training loss: 0.8003305739642089
Round 22
-------------------------------
gradient difference: 0.28613919019699097
At round 21 accuracy: 0.686411149825784
At round 21 training accuracy: 0.6509769094138543
At round 21 training loss: 0.8022530408451513
Round 23
-------------------------------
gradient difference: 0.39670827984809875
At round 22 accuracy: 0.6829268292682927
At round 22 training accuracy: 0.6518650088809946
At round 22 training loss: 0.8135595602289398
Round 24
-------------------------------
gradient difference: 0.3068327307701111
At round 23 accuracy: 0.6794425087108014
At round 23 training accuracy: 0.6571936056838366
At round 23 training loss: 0.8050967081986443
Round 25
-------------------------------
gradient difference: 0.32568299770355225
At round 24 accuracy: 0.6794425087108014
At round 24 training accuracy: 0.6580817051509769
At round 24 training loss: 0.8076192177091039
Round 26
-------------------------------
gradient difference: 0.5897510051727295
At round 25 accuracy: 0.6829268292682927
At round 25 training accuracy: 0.6607460035523979
At round 25 training loss: 0.7985214556921924
Round 27
-------------------------------
gradient difference: 0.33896178007125854
At round 26 accuracy: 0.6829268292682927
At round 26 training accuracy: 0.6616341030195382
At round 26 training loss: 0.8119624784687184
Round 28
-------------------------------
gradient difference: 0.24915413558483124
At round 27 accuracy: 0.6829268292682927
At round 27 training accuracy: 0.6616341030195382
At round 27 training loss: 0.8278013284009915
Round 29
-------------------------------
gradient difference: 0.2652168869972229
At round 28 accuracy: 0.686411149825784
At round 28 training accuracy: 0.6607460035523979
At round 28 training loss: 0.8002783205713073
Round 30
-------------------------------
gradient difference: 0.37594467401504517
At round 29 accuracy: 0.686411149825784
At round 29 training accuracy: 0.6598579040852576
At round 29 training loss: 0.8002214594272695
Round 31
-------------------------------
gradient difference: 0.3279697000980377
At round 30 accuracy: 0.686411149825784
At round 30 training accuracy: 0.6634103019538188
At round 30 training loss: 0.8090583963905251
Round 32
-------------------------------
gradient difference: 0.28849369287490845
At round 31 accuracy: 0.686411149825784
At round 31 training accuracy: 0.6625222024866785
At round 31 training loss: 0.7881319501977072
Round 33
-------------------------------
gradient difference: 0.4837307929992676
At round 32 accuracy: 0.686411149825784
At round 32 training accuracy: 0.6642984014209592
At round 32 training loss: 0.7938230301977492
Round 34
-------------------------------
gradient difference: 0.3293401598930359
At round 33 accuracy: 0.686411149825784
At round 33 training accuracy: 0.6651865008880995
At round 33 training loss: 0.799372850733662
Done!
update_location
xs = [-193.9056584   179.20031788  215.00902392   18.81129433  160.97929623
  173.95640986    2.55680806   -6.32485185  199.66397685  -17.06087855]
ys = [  17.5879595    15.55583871    1.32061395 -177.45517586    9.35018685
  -17.18584926 -192.62498432  180.82234798   17.56900603  199.00148178]
dists_uav = [218.88065397 205.80266773 237.12997361 204.55855942 189.74129705
 201.3856647  217.0505053  206.72862714 223.99636967 223.36665671]
dists_bs = [158.54328877 388.43314193 426.93274397 402.22813029 374.59571286
 398.37918701 408.25794442 168.77560647 406.39588262 159.75242474]
uav_gains = [1.28428224e-11 1.56663855e-11 9.45055194e-12 1.59558146e-11
 1.97708036e-11 1.67131081e-11 1.32162457e-11 1.54535768e-11
 1.18323052e-11 1.19541205e-11]
bs_gains = [7.63624879e-11 6.21167189e-12 4.76746578e-12 5.63340860e-12
 6.87572087e-12 5.78713336e-12 5.40352395e-12 6.40954187e-11
 5.47313357e-12 7.47551644e-11]
ene_coms = [0.00995035 0.01241431 0.01086944 0.00938423 0.01200473 0.01271513
 0.00987191 0.00946377 0.01296162 0.00665102]
ene_comp = [0.25032247 0.38905541 0.25635434 0.24127467 0.70874434 0.30159334
 0.44032627 0.2804818  0.26841807 0.25937027]
ene_total = [24.0950311  37.16648206 24.73852305 23.20501241 66.72410513 29.09743753
 41.67757189 26.84201758 26.04902159 24.62720147]
obj_prev = 324.22240383938123
t_co_uav = [0.09950354 0.09429604 0.10869441 0.09384235 0.08882407 0.09271155
 0.09871914 0.09463774 0.10181268 0.10151857]
t_co_bs = [0.06624248 0.12414308 0.13610068 0.12833018 0.12004727 0.12715127
 0.13019398 0.06851171 0.12961621 0.0665102 ]
difference = [ 0.03326106 -0.02984704 -0.02740626 -0.03448783 -0.0312232  -0.03443972
 -0.03147484  0.02612603 -0.02780353  0.03500836]
decs_opt = [0 1 0 1 1 1 1 0 0 0]
af = 16.896171935691484	bf = 85.7273397848762	zeta = 33.79234387138297	eta = 0.5
af = 16.896171935691484	bf = 85.7273397848762	zeta = 194.87774744871365	eta = 0.08670139180533212
af = 16.896171935691484	bf = 85.7273397848762	zeta = 139.1037938946951	eta = 0.12146449397693848
af = 16.896171935691484	bf = 85.7273397848762	zeta = 138.12386804695794	eta = 0.12232622916371917
af = 16.896171935691484	bf = 85.7273397848762	zeta = 138.12338811540417	eta = 0.12232665420554611
af = 16.896171935691484	bf = 85.7273397848762	zeta = 138.12338811528798	eta = 0.12232665420564902
eta_opt = 0.12232665420564902
solve_bound_eta tau = 65.0
solve_bound_eta tau = 37.5
solve_bound_eta tau = 23.75
solve_bound_eta tau = 16.875
solve_bound_eta tau = 20.3125
solve_bound_eta tau = 18.59375
solve_bound_eta tau = 17.734375
solve_bound_eta tau = 17.3046875
solve_bound_eta tau = 17.51953125
solve_bound_eta tau = 17.626953125
solve_bound_eta tau = 17.6806640625
solve_bound_eta tau = 17.70751953125
solve_bound_eta tau = 17.720947265625
solve_bound_eta tau = 17.7142333984375
solve_bound_eta tau = 17.71759033203125
solve_bound_eta tau = 17.715911865234375
initialize_feasible_solution eta = 0.12232665420564902, tau = 17.715911865234375
ti_comp = [0.02855175 0.04437561 0.02923974 0.02751976 0.08083928 0.03439969
 0.05022355 0.03199172 0.03061573 0.02958374]
ti_coms = [0.06624248 0.09429604 0.13610068 0.09384235 0.08882407 0.09271155
 0.09871914 0.06851171 0.12961621 0.0665102 ]
t_total = [ 9.89880612 14.48066848 17.26553267 12.6731345  17.71695114 13.27348388
 15.55321473 10.49498504 16.7320836  10.03452797]
optimize_network_fake eta = 0.12232665420564902	tau = 23	t_min = 17.715911865234375
solve_bound_eta tau = 23
eta_min = 0.044510918911557486	eta_max = 0.36863311860438985
af = 67.58468774276595	bf = 9.74119077868692	zeta = 135.1693754855319	eta = 0.5
af = 67.58468774276595	bf = 9.74119077868692	zeta = 113.17465307321886	eta = 0.5971715919380086
af = 67.58468774276595	bf = 9.74119077868692	zeta = 110.67871228770245	eta = 0.6106385441771652
af = 67.58468774276595	bf = 9.74119077868692	zeta = 110.63586924745336	eta = 0.6108750100892042
af = 67.58468774276595	bf = 9.74119077868692	zeta = 110.63585623156978	eta = 0.610875081956303
eta = 0.610875081956303
ene_coms = [0.00995035 0.01241431 0.01086944 0.00938423 0.01200473 0.01271513
 0.00987191 0.00946377 0.01296162 0.00665102]
ene_comp = [0.02679046 0.04163819 0.02743601 0.02582213 0.07585251 0.03227766
 0.04712539 0.03001823 0.02872712 0.02775879]
ene_total = [ 8.65353584 12.7309425   9.02205456  8.29212829 20.69294743 10.59711748
 13.42453113  9.29916539  9.81891724  8.10451637]
ti_comp = [0.05894045 0.03430091 0.04974958 0.06460164 0.03839673 0.03129272
 0.05972485 0.06380625 0.02882779 0.09193379]
ti_coms = [0.09950354 0.12414308 0.10869441 0.09384235 0.12004727 0.12715127
 0.09871914 0.09463774 0.12961621 0.0665102 ]
t_total = [23. 23. 23. 23. 23. 23. 23. 23. 23. 23.]
ene_coms = [0.00995035 0.01241431 0.01086944 0.00938423 0.01200473 0.01271513
 0.00987191 0.00946377 0.01296162 0.00665102]
ene_comp = [0.00287175 0.03183441 0.00432929 0.00214053 0.1535869  0.01781773
 0.01522255 0.00344716 0.01480084 0.00131306]
ene_total = [ 1.86127809  6.42321962  2.2062742   1.67295484 24.03756326  4.43220136
  3.64275543  1.87417303  4.03004649  1.15607921]
optimize_network_iter = 0 obj = 51.33654554423736
eta = 0.36863311860438985
freqs = [4.60172784e+08 1.22896749e+09 5.58323436e+08 4.04671613e+08
 2.00000000e+09 1.04427048e+09 7.98829567e+08 4.76295003e+08
 1.00886964e+09 3.05688860e+08]
solve_bound_eta tau = 23
eta_min = 0.3655484527824722	eta_max = 0.3686331186043896
af = 22.717497785207623	bf = 9.74119077868692	zeta = 45.434995570415246	eta = 0.5
af = 22.717497785207623	bf = 9.74119077868692	zeta = 50.97552063576077	eta = 0.44565504190791244
af = 22.717497785207623	bf = 9.74119077868692	zeta = 50.69352747633599	eta = 0.4481340896194741
af = 22.717497785207623	bf = 9.74119077868692	zeta = 50.692895273994566	eta = 0.44813967839910873
af = 22.717497785207623	bf = 9.74119077868692	zeta = 50.69289527079337	eta = 0.4481396784274082
eta = 0.4481396784274082
ene_coms = [0.00995035 0.01241431 0.01086944 0.00938423 0.01200473 0.01271513
 0.00987191 0.00946377 0.01296162 0.00665102]
ene_comp = [0.00230974 0.0256043  0.00348203 0.00172162 0.12352939 0.01433073
 0.01224344 0.00277254 0.01190426 0.00105609]
ene_total = [ 2.03609644  6.31394699  2.38342375  1.84440705 22.50885093  4.49164592
  3.67281162  2.03214742  4.1296059   1.27995926]
ti_comp = [0.05894045 0.03430091 0.04974958 0.06460164 0.03839673 0.03129272
 0.05972485 0.06380625 0.02882779 0.09193379]
ti_coms = [0.09950354 0.12414308 0.10869441 0.09384235 0.12004727 0.12715127
 0.09871914 0.09463774 0.12961621 0.0665102 ]
t_total = [23. 23. 23. 23. 23. 23. 23. 23. 23. 23.]
ene_coms = [0.00995035 0.01241431 0.01086944 0.00938423 0.01200473 0.01271513
 0.00987191 0.00946377 0.01296162 0.00665102]
ene_comp = [0.00287175 0.03183441 0.00432929 0.00214053 0.1535869  0.01781773
 0.01522255 0.00344716 0.01480084 0.00131306]
ene_total = [ 1.86127809  6.42321962  2.2062742   1.67295484 24.03756326  4.43220136
  3.64275543  1.87417303  4.03004649  1.15607921]
optimize_network_iter = 1 obj = 51.336545544237296
eta = 0.3686331186043896
freqs = [4.60172784e+08 1.22896749e+09 5.58323436e+08 4.04671613e+08
 2.00000000e+09 1.04427048e+09 7.98829567e+08 4.76295003e+08
 1.00886964e+09 3.05688860e+08]
Done!
optimize_network_fake num_local_rounds = 32.67806413663097	num_global_rounds = 145.16170630293234
iter = 1	num_local_rounds = 32.67806413663097	num_global_rounds = 145.16170630293234
Round 1
-------------------------------
gradient difference: 0.27242377400398254
At round 0 accuracy: 0.686411149825784
At round 0 training accuracy: 0.6651865008880995
At round 0 training loss: 0.8289074500827499
Round 2
-------------------------------
gradient difference: 0.2767952084541321
At round 1 accuracy: 0.686411149825784
At round 1 training accuracy: 0.6651865008880995
At round 1 training loss: 0.7964401986939604
Round 3
-------------------------------
gradient difference: 0.6640928983688354
At round 2 accuracy: 0.686411149825784
At round 2 training accuracy: 0.6660746003552398
At round 2 training loss: 0.8257529857778528
Round 4
-------------------------------
gradient difference: 0.7239192724227905
At round 3 accuracy: 0.6898954703832753
At round 3 training accuracy: 0.6660746003552398
At round 3 training loss: 0.8037580731091608
Round 5
-------------------------------
gradient difference: 0.3562278747558594
At round 4 accuracy: 0.686411149825784
At round 4 training accuracy: 0.6660746003552398
At round 4 training loss: 0.7960808561524833
Round 6
-------------------------------
gradient difference: 0.2750494182109833
At round 5 accuracy: 0.6898954703832753
At round 5 training accuracy: 0.6651865008880995
At round 5 training loss: 0.7981074853552658
Round 7
-------------------------------
gradient difference: 0.35183653235435486
At round 6 accuracy: 0.6898954703832753
At round 6 training accuracy: 0.6642984014209592
At round 6 training loss: 0.8036226534740663
Round 8
-------------------------------
gradient difference: 0.3153029680252075
At round 7 accuracy: 0.686411149825784
At round 7 training accuracy: 0.6642984014209592
At round 7 training loss: 0.8080735024733082
Round 9
-------------------------------
gradient difference: 0.2652113735675812
At round 8 accuracy: 0.6898954703832753
At round 8 training accuracy: 0.6642984014209592
At round 8 training loss: 0.7935041922751688
Round 10
-------------------------------
gradient difference: 0.4263916015625
At round 9 accuracy: 0.6898954703832753
At round 9 training accuracy: 0.6642984014209592
At round 9 training loss: 0.787757121949788
Round 11
-------------------------------
gradient difference: 0.26110464334487915
At round 10 accuracy: 0.6898954703832753
At round 10 training accuracy: 0.6642984014209592
At round 10 training loss: 0.7873694988491444
Round 12
-------------------------------
gradient difference: 0.2651013731956482
At round 11 accuracy: 0.6898954703832753
At round 11 training accuracy: 0.6651865008880995
At round 11 training loss: 0.8203250611215265
Round 13
-------------------------------
gradient difference: 0.3593471646308899
At round 12 accuracy: 0.6933797909407665
At round 12 training accuracy: 0.6660746003552398
At round 12 training loss: 0.8101980091127209
Round 14
-------------------------------
gradient difference: 0.23532576858997345
At round 13 accuracy: 0.6933797909407665
At round 13 training accuracy: 0.6660746003552398
At round 13 training loss: 0.8083077090589994
Round 15
-------------------------------
gradient difference: 0.4433055818080902
At round 14 accuracy: 0.686411149825784
At round 14 training accuracy: 0.6660746003552398
At round 14 training loss: 0.8225515377105095
Round 16
-------------------------------
gradient difference: 0.35275593400001526
At round 15 accuracy: 0.686411149825784
At round 15 training accuracy: 0.6678507992895204
At round 15 training loss: 0.809289267849091
Round 17
-------------------------------
gradient difference: 0.43738991022109985
At round 16 accuracy: 0.6898954703832753
At round 16 training accuracy: 0.6678507992895204
At round 16 training loss: 0.8040667072924427
Round 18
-------------------------------
gradient difference: 0.331220805644989
At round 17 accuracy: 0.6898954703832753
At round 17 training accuracy: 0.6678507992895204
At round 17 training loss: 0.82072581424846
Round 19
-------------------------------
gradient difference: 0.28617677092552185
At round 18 accuracy: 0.6898954703832753
At round 18 training accuracy: 0.6687388987566607
At round 18 training loss: 0.8021804274690087
Round 20
-------------------------------
gradient difference: 0.3842594027519226
At round 19 accuracy: 0.6898954703832753
At round 19 training accuracy: 0.6669626998223801
At round 19 training loss: 0.8376695818193323
Round 21
-------------------------------
gradient difference: 0.33161461353302
At round 20 accuracy: 0.6898954703832753
At round 20 training accuracy: 0.6660746003552398
At round 20 training loss: 0.7971696450730676
Round 22
-------------------------------
gradient difference: 0.5139472484588623
At round 21 accuracy: 0.6898954703832753
At round 21 training accuracy: 0.6660746003552398
At round 21 training loss: 0.8003288957048285
Round 23
-------------------------------
gradient difference: 0.3330554962158203
At round 22 accuracy: 0.6898954703832753
At round 22 training accuracy: 0.6660746003552398
At round 22 training loss: 0.824984854933309
Round 24
-------------------------------
gradient difference: 0.401280015707016
At round 23 accuracy: 0.6898954703832753
At round 23 training accuracy: 0.6642984014209592
At round 23 training loss: 0.8011952422965094
Round 25
-------------------------------
gradient difference: 0.2703552544116974
At round 24 accuracy: 0.6898954703832753
At round 24 training accuracy: 0.6642984014209592
At round 24 training loss: 0.8002063406213386
Round 26
-------------------------------
gradient difference: 0.3024442791938782
At round 25 accuracy: 0.6898954703832753
At round 25 training accuracy: 0.6651865008880995
At round 25 training loss: 0.7926918732113761
Round 27
-------------------------------
gradient difference: 0.2842898964881897
At round 26 accuracy: 0.6898954703832753
At round 26 training accuracy: 0.6634103019538188
At round 26 training loss: 0.8082189507910386
Round 28
-------------------------------
gradient difference: 0.4878113269805908
At round 27 accuracy: 0.6898954703832753
At round 27 training accuracy: 0.6642984014209592
At round 27 training loss: 0.7983166322976686
Round 29
-------------------------------
gradient difference: 0.34992486238479614
At round 28 accuracy: 0.6898954703832753
At round 28 training accuracy: 0.6651865008880995
At round 28 training loss: 0.7948277449139376
Round 30
-------------------------------
gradient difference: 0.34494227170944214
At round 29 accuracy: 0.6898954703832753
At round 29 training accuracy: 0.6669626998223801
At round 29 training loss: 0.8010417660216367
Round 31
-------------------------------
gradient difference: 0.4310327172279358
At round 30 accuracy: 0.686411149825784
At round 30 training accuracy: 0.6669626998223801
At round 30 training loss: 0.7878159896756785
Round 32
-------------------------------
gradient difference: 0.4156022071838379
At round 31 accuracy: 0.686411149825784
At round 31 training accuracy: 0.6678507992895204
At round 31 training loss: 0.8001476594078746
Done!
update_location
xs = [-193.9056584   -20.79968212  415.00902392   18.81129433  160.97929623
  -26.04359014  202.55680806 -206.32485185  399.66397685  182.93912145]
ys = [ 217.5879595    15.55583871    1.32061395 -377.45517586 -190.64981315
  -17.18584926 -192.62498432  180.82234798   17.56900603  199.00148178]
dists_uav = [308.12972021 103.31800857 426.88901831 390.93001238 268.81533637
 104.75505717 296.87311275 292.00456507 412.35902362 288.21573848]
dists_bs = [ 46.59568879 221.81113273 615.04079331 585.46523308 496.5701092
 243.15306433 526.96989706  31.86135715 595.83823656 358.74292438]
uav_gains = [2.29592653e-12 9.21631668e-11 5.86815526e-13 7.83470858e-13
 5.05517205e-12 8.90347214e-11 2.83966812e-12 3.12707259e-12
 6.54600040e-13 3.37589768e-12]
bs_gains = [2.35464817e-09 2.98222725e-11 1.71539091e-12 1.96920521e-12
 3.12283098e-12 2.30584863e-11 2.64419711e-12 6.82580893e-09
 1.87471239e-12 7.76069690e-12]
Done!
