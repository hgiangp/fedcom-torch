{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho = 1.4\ti = 30\tn = 100\n",
      "eta = 0.7923059592313171\tgamma_cv are 2.2569648939844305 and 283.4573208203013\n",
      "rho = 1.4\ti = 20\tn = 100\n",
      "eta = 0.7923059592313171\tgamma_cv are 3.3991440465523146 and 282.3151416677334\n",
      "rho = 1.1\ti = 30\tn = 100\n",
      "eta = 0.8717807197295375\tgamma_cv are 1.3245768685607398 and 362.31178676780286\n",
      "rho = 1.05\ti = 30\tn = 100\n",
      "eta = 0.8831721020676158\tgamma_cv are 1.1986615349025511 and 379.7537194174784\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def func(rho, i, n): \n",
    "    print(f\"rho = {rho}\\ti = {i}\\tn = {n}\")\n",
    "    epsilon_0 = 5*1e-3 \n",
    "    lr = 5*1e-3 \n",
    "    \n",
    "    eta = 1 - (2* rho**2 * math.log(1/epsilon_0))/n  \n",
    "    a = rho  \n",
    "    b = -2\n",
    "    c = 2/i * math.log2(n/(n - 2*(rho**2)*math.log(1/epsilon_0)))\n",
    "\n",
    "    # calculate the discriminant\n",
    "    d = (b**2) - (4*a*c)\n",
    "\n",
    "    # find two solutions\n",
    "    sol1 = (-b-math.sqrt(d))/(2*a)\n",
    "    sol2 = (-b+math.sqrt(d))/(2*a)\n",
    "    \n",
    "    # print('The solution are {0} and {1}'.format(sol1,sol2))\n",
    "    \n",
    "    gamma_cv1 = sol1/lr\n",
    "    gamma_cv2 = sol2/lr\n",
    "\n",
    "    print('eta = {0}\\tgamma_cv are {1} and {2}'.format(eta,gamma_cv1,gamma_cv2))\n",
    "\n",
    "func(rho=1.4, i=30, n=100)\n",
    "func(rho=1.4, i=20, n=100)\n",
    "func(rho=1.1, i=30, n=100)\n",
    "func(rho=1.05, i=30, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 200.0 y = 172.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "a1 = -0.5; b1 = 272 \n",
    "a2 = 1; b2 = -28 \n",
    "\n",
    "x = (b2 - b1)/(a1 - a2)\n",
    "y = a1 * x + b1 \n",
    "\n",
    "print(f\"x = {x} y = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import copy \n",
    "\n",
    "from torch import nn \n",
    "\n",
    "model = nn.Linear(1, 1)\n",
    "model_cp = copy.deepcopy(model)\n",
    "\n",
    "model.to('cuda:0')\n",
    "\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Ti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.device_count()\n",
    "\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# check whether the current device on cuda \n",
    "print(next(model.parameters()).is_cuda)\n",
    "\n",
    "model.to('cpu')\n",
    "# check whether the current device on cude \n",
    "print(next(model.parameters()).is_cuda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0')\n",
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy \n",
    "import torch \n",
    "\n",
    "x = torch.ones((1, ), device=torch.device(\"cuda\", 0))\n",
    "print(x)\n",
    "y = deepcopy(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0f27780933b868be0d4b886deae0378dd92430bbe3edc59532a3cc155579b80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
